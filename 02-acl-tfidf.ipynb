{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (2020.12.5)\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Filtering I (BM25 Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n",
      "Thus+,+over+the+past+few+years+,+along+with+advances+in+the+use+of+learning+and+statistical+methods+for+acquisition+of+full+parsers+(+Collins+,+1997+;+Charniak+,+1997a+;+Charniak+,+1997b+;+Ratnaparkhi+,+1997+)+,+significant+progress+has+been+made+on+the+use+of+statistical+learning+methods+to+recognize+shallow+parsing+patterns+syntactic+phrases+or+words+that+participate+in+a+syntactic+relationship+(+Church+,+1988+;+Ramshaw+and+Marcus+,+1995+;+Argamon+et+al.+,+1998+;+Cardie+and+Pierce+,+1998+;+Munoz+et+al.+,+1999+;+Punyakanok+and+Roth+,+2001+;+Buchholz+et+al.+,+1999+;+Tjong+Kim+Sang+and+Buchholz+,+2000+)+.\n",
      "Number of retrieved documents: 10\n",
      "Over the past fifteen years there has been significant progress in the field of statistical parsing . Much of the work has focussed on supervised methods , where by ' ' supervised ' ' we mean that the training data consists of sentences and their associated syntactic trees ( for example , Charniak 1997 , Collins 1999 , Roark and Johnson 1999 ) . There are a number of treebank corpora , of which the Penn Treebank , based largely on ' ' Wall Street Journal ' ' text , and available from the Linguistic Data Consortium , is the most widely used . More impressively , there has been recent work by Dan Klein and Chris Manning on machine learning procedures ( Klein & Manning 2002 , Klein & Manning 2004 ) that infer parsers in an unsupervised fashion from raw text annotated with part - of- speech tags . These systems attempt to do what a child does when it learns his / her language . They infer structure from distributional patterns in a more or less unannotated sequence of tokens . The Klein - Manning procedures do not yet perform as well on held out test data as the supervised systems , but they are quickly converging on supervised results . What is particularly notable about the Klein - Manning grammar induction procedures is that they do what Chomsky and others have argued is impossible : They induce a grammar using general statistical methods which have few , if any , built - in assumptions that are specific to language . There has also been significant progress in the realm of hand - built parsers that adopt certain grammatical frameworks . For example , the PARC LFG parser , developed by Ron Kaplan and colleagues over the past twenty years , with intensive manual labor , performs at a level comparable to current statistical parsers on Penn Treebank data ( Riezler et al . 2002 ) . Surprisingly , one approach to grammar that is not represented in work on robust parsing is the Principles and Parameters model , or what has evolved over the last ten years into the ' ' Minimalist Program ' ' ( MP ) . At the risk of being accused of nomenclatural solecism , we will simply refer to it as ' ' Principles and Parameters ' ' ( P&P ) . They are not even up to the task of parsing arbitrary Dr. Seuss books , let alone the ' ' Wall Street Journal ' ' . For example , Fong 's English parser , which is arguably the best piece of work of this kind , handles , in its current incarnation , just the example sentences in Lasnik and Uriagereka 's textbook ( 1988 ) . Crucially , there has been , to our knowledge , no interest in developing a broad - coverage P&P parser . Even more puzzling is the lack of any serious attempt to build a P&P - style parser that is able to learn from unannotated input ( as Klein and Manning 's systems do ) . It seems to us that if the claims on behalf of P&P approaches are to be taken seriously , it is an obvious requirement that someone provide a computational learner that incorporates P&P mechanisms , and uses it to demonstrate learning of the grammar of a natural language . With this in mind , we offer the following challenge to the community . THE CHALLENGE . We challenge someone to produce , by May of 2008 , a working P&P parser that can be trained in a supervised fashion on a standard treebank , such as the Penn Treebank , and perform in a range comparable to state - of - the - art statistical parsers . Let us now lay out what we feel would be the minimal requirements for meeting this challenge . First , the system must use P&P in a non - trivial way . So for example , using a standard machine learning algorithm to extract a statistical parser like those in existence , supplemented by a transducer that maps Penn Tree Bank structures into P&P annotations would not satisfy the challenge . For a system to qualify , it would have to be the case that the P&P component is an integral part of the learning mechanism . Removing the P&P component must seriously degrade performance . Second , the particular choice of parameters and their possible settings , must conform to some recognized version of a P&P theory proposed in the literature . We recognize that it may be necessary to augment the set of accepted parameters with additional conditions that are motivated by particular problems in learning grammatical structures . However , the greater the number of ad hoc principles and parameters that are used , the less adequate the implemented system will be . Third , we recognize that the assumptions about syntactic structure used by the Penn Treebank ( and other treebanks ) differ in many details from those of the P&P tradition . In particular , P&P work generally assumes a far more articulated syntactic structure than is generally employed in other frameworks . The P&P parser that we are asking for can produce this more articulated structure , without being penalized for such re - analysis of the data . In fact many robust parsers map Penn Tree Bank analyses into alternative formal representations . All that is required is that the designer of the parser also produce code that converts from the P&P parser 's syntactic structures back into appropriate treebank structures , so that proper evaluation of its output is possible . It is worth pointing out that our challenge allows the P&P model a considerable , if illicit advantage . We are only asking for supervised grammar induction , when , in fact , unsupervised learning on the basis of parameterized principles would be the more reasonable test of the model 's viability . P&P advocates have long eschewed the existence of negative evidence in the learning process , and insisted that grammar induction takes place with very little external data . In allowing supervised learning we have liberalized the conditions of the acquisition problem well beyond the stringent restriction invoked in the P&P literature . POSSIBLE OBJECTIONS . We outline here some possible objections to our challenge that might be raised by proponents of P&P approaches to grammar . These objections are anticipated , in part , on the basis of the experience of one of the authors with a prior debate on the scientific foundations of Minimalism . Objection 1 : You 're confusing performance and competence . Grammars are models of competence , parsers are models of performance . Reply : No we are not . Please note that we have been careful to impose no requirements on algorithms for learning or for the resulting parser . Thus we make no claims and set no expectations on mode of implementation ( i.e. performance ) . The only requirement we insist on is the obvious condition that the learning method and the resulting parser make non- trivial use of the assumptions of the P&P approach to syntax . Again , this is the point of the challenge , given that P&P makes very strong claims about how these grammatical assumptions are essential to language learning . Objection 2 : There have been very few people working in P&P parsing . So naturally progress has been slow . Reply : Certainly this is true . The obvious question here is why this has been the case . Positing a theory that makes such far reaching claims about mechanisms underlying language learning would seem to us to commit the adherents of such a theory to the task of demonstrating its viability through implementation of a large scale model . Why , in the past quarter century of work on this topic , has no one attempted this ? Objection 3 : The MP is a research program , not a fully developed theory . Therefore it ca n't be expected to yield a model that can be implemented as a broad coverage processing device . Reply : This is a remarkable dodge . The MP has been around at least since the early 1990s . Chomsky sketched this view in ' ' Some Notes on Economy of Derivation and Representation ' ' in 1991 , and then presented a detailed account in his book ' ' The Minimalist Program ' ' in 1995 . Much subsequent theoretical work has been done within the MP . The P&P model was explicitly proposed in Lectures on Government and Binding in 1981 . The antecedents for this general approach to grammar and language acquisition go back to ' ' Syntactic Structures ' ' ( 1957 ) and ' ' Aspects of the Theory of Syntax ' ' ( 1964 ) . The P&P view replaced a model of an innate language faculty consisting of a grammar evaluation metric applied to a set of grammars generated by a universal schema of grammar . The idea of a language- specific device / set of constraints as the basis of language learning has , therefore , been at the center of this line of research for close to fifty years . Surely it is long past time to ask for some substantive evidence in the way of a robust grammar induction system that this view of grammar induction is computationally viable . Most other major theoretical models of grammar have succeeded in yielding robust parsers in far less time , despite the fact that they do not , as far we know , make analogous claims about the nature of language learning . Objection 4 : It would be trivially easy to devise a procedure to convert Penn treebank structures into MP representations and then use machine learning methods to extract a grammar that generates the latter for Penn Treebank test data . Why bother ? Reply : This is exactly the case that we exclude as not satisfying the challenge . The P&P approach makes claims not only about the nature of syntactic representation but the way in which grammar is acquired . Because it purports to be first and foremost a learning theory , it is necessary to show that this model can yield a robust grammar learning device in order for the framework to sustain any credibility . So far , it has not done this . EPILOGUE . We will close by noting that we are in no way suggesting that the construction of a trainable wide - coverage P&P - based parser is impossible . Since no one has attempted this , and no one has even sketched a proposal for how to do it , we simply do not know if it is possible . As scientists , we do not speculate about the impossibility of something of which we have no knowledge . In fact , we would be delighted if someone succeeds in meeting our challenge . Such success would convince us that the P&P enterprise is , after all , a testable theory with genuine scientific content . Richard Sproat , Department of Linguistics Department of Electrical and Computer Engineering Beckman Institute University of Illinois at Urbana - Champaign . Shalom Lappin , Department of Computer Science King 's College , London . REFERENCES . Berwick , Robert . Locality Principles and the Acquisition of Syntactic Knowledge . PhD Dissertation , MIT . Charniak , Eugene . Statistical parsing with a context - free grammar and word statistics ' ' , Proceedings of the Fourteenth National Conference on Artificial Intelligence AAAI Press / MIT Press , Menlo Park . Collins , Michael . Head - Driven Statistical Models for Natural Language Parsing . PhD Dissertation , University of Pennsylvania . Fong , Sandiway . Computational Properties of Principled - Based Grammatical Theories , AI Laboratory , MIT . Fong , Sandiway . Computation with Probes and Goals : A Parsing Perspective . ' ' In Di Sciullo , A. M. and R. Delmonte ( Eds . ) UG and External Systems . Amsterdam : John Benjamins . Klein , Dan and Manning , Christopher . Natural Language Grammar Induction using a Constituent - Context Model . ' ' In Thomas G. Dietterich , Suzanna Becker , and Zoubin Ghahramani ( eds ) , Advances in Neural Information Processing Systems 14 ( NIPS 2001 ) . Cambridge , MA : MIT Press , vol . 1 , pp . 35 - 42 . Klein , Dan and Manning , Christopher . Corpus - Based Induction of Syntactic Structure : Models of Dependency and Constituency . ' ' Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( ACL 2004 ) . Lasnik , Howard and Uriagereka , Juan . A Course in GB Syntax : Lectures on Binding and Empty Categories . MIT Press . Roark , Brian and Johnson , Mark . Efficient probabilistic top - down and left - corner parsing . In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 421 - 428 . Riezler , Stefan ; Maxwell , John ; King , Tracy ; Kaplan , Ronald ; Crouch , Richard and Johnson , Mark . 2002 ' ' Parsing the Wall Street Journal using a lexical- functional grammar and discriminative estimation techniques . ' ' Proceedings 40th Meeting Association for Computational Linguistics , Philadelphia . Linguistic Field(s ) : Computational Linguistics Discipline of Linguistics Language Acquisition Syntax Text / Corpus Linguistics Tools . \" ... We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints . In particular , we develop two general approaches for an important subproblem - identifying phrase structure . The first is a Markovian appro ... \" . We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints . In particular , we develop two general approaches for an important subproblem - identifying phrase structure . The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state - observation dependencies . The second is an extension of constraint satisfaction formalisms . We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . 1 Introduction In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints . Consider , for example , the problem of chunking natural language sentences ... . ... algorithms that use general classifiers to yield the inference . Working within a concrete task allows us to compare ... . by Marcia Muñoz , Vasin Punyakanok , Dan Roth , Day Zimak - IN PROCEEDINGS OF EMNLP - WVLC&apos;99 . ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1999 . \" ... A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally . The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference . Two instantiations of this approach are studied and experimental results for Noun - Phrase ... \" . A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally . The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference . Two instantiations of this approach are studied and experimental results for Noun - Phrases ( NP ) and Subject - Verb ( SV ) phrases that compare favorably with the best published results are presented . In doing that , we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are bet- ter learned using open / close predictors than using inside / outside predictors . . .. to full - sentence parsers . Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summariza ... . Tile common practice for approaching this task is by tedious manual definition of possible pat - tern structures , often in the h)rm of re ... \" . Tile common practice for approaching this task is by tedious manual definition of possible pat - tern structures , often in the h)rm of regular expres - sions or finite automata . This paper presents a novel memory - based learning method that recognizes shal - low patterns in new text based on a bracketed train - ing corpus . The training data are stored as - is , in efficient suttix - tree data structures . Generalization is performed on - line at recognition time by compar - ing subsequences of the new text to positive and negative evidence in the corIms . This way , no in - formation in tit ( ; training is lost , as can happen in other learning systems that construct a single gen - eralized model at the time of training . The paper presents experimental results for recognizing noun phrase , subject - verb and verb - object patterns in l ! ] n - glish . Since the learning approach enables easy port - ing to new domains , we plan to apply it to syntac - tic patterns in other languages and to sub - language patterns for information extraction . ... full parsing and instead to rely only on local information . These works have shown that it is possible to identify most instances of local syntactic patterns by rules that examine only the pattern itself and its nearby context . Often , the rules are applied ... . \" ... Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse . In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow p ... \" . Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse . We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts . \" ... In this paper , a memory - based parsing method is extended for handling compositional structures . The method is oriented for learning to parse any selected subset of target syntactic structures . It is local , yet can handle also compositional structures . In this paper , a memory - based parsing method is extended for handling compositional structures . The method is oriented for learning to parse any selected subset of target syntactic structures . It is local , yet can handle also compositional structures . . .. full parse of free - text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) . Shallow parsing tasks are often formulated as dividing the sentence into nonoverlapping seque ... . \" ... We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints . In particular , we develop two general approaches for an important subproblem- identifying phrase structure . The first is a Markovian approach t ... \" . We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints . In particular , we develop two general approaches for an important subproblem- identifying phrase structure . The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state - observation dependencies . The second is an extension of constraint satisfaction formalisms . We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . ... algorithms that use general classifiers to yield the inference . Working within a concrete task allows us to compare ... . by Young - Sook Hwang , So - Young Park , Hoo - Jung Chung , Yong - Jae Kwak , Hae - Chang Rim , 2001 . \" ... In this paper , we define the chunking problem as a classification of words and present a weighted probabilistic model for a text chunking . The proposed model exploits context features around the focus word . And to alleviate the sparse data problem , it integrates general features with specific feat ... \" . In this paper , we define the chunking problem as a classification of words and present a weighted probabilistic model for a text chunking . The proposed model exploits context features around the focus word . And to alleviate the sparse data problem , it integrates general features with specific features . In the training stage , we select useful features after measuring information gain ratio of each features and assign higher weight to more informative feature by adopting the information gain ratio . At the application time , we classify words into chunk labels while checking consistency of the begin and the end of a chunk . The experimental results show that the model combining general and specific features alleviates the sparse data problem . In addition , the weighted probabilistic model based on information gain ratio outperforms the non - weighted model . . .. distinctions that have to be taken intosaccount . For this reason , knowledge - poorsapproaches such as the distributional approachsare particularly suited for this task . Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \" scholarship of thirty years ago \" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German Wörterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \" where are we now ? \" and \" where do we want to be ? \" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \" computational lexicography \" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \" lemmatized searching . \" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \" Translation equivalents \" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word archê may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \" chapter 1 , section 1 \" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \" context \" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'čuk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \" that glory would know my old age \" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \" means . \" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \" The Latin Dependency Treebank in a Cultural Heritage Digital Library \" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \" Building a Dynamic Lexicon from a Digital Library \" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \" The Messianic Prophecy in Vergil 's Fourth Eclogue \" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \" Word - sense disambiguation using statistical methods \" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \" Foreword : Perspectives on the Digital Humanities \" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \" A Maximum - Entropy - Inspired Parser \" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \" Head - Driven Statistical Models for Natural Language Parsing \" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) Wörterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Berücksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \" Using bilingual materials to develop word sense disambiguation methods \" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Hajič 1999 Hajič , Jan. \" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevová . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \" The Sketch Engine \" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schnörch , and Petra Storjohann . \" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut für deutsche Sprache , Mannheim \" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \" Building a Large Annotated Corpus of English : The Penn Treebank \" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \" Finding Predominant Senses in Untagged Text \" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Hajič . \" Non - projective Dependency Parsing using Spanning Tree Algorithms \" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'čuk 1988 Mel'čuk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \" Wordnet : A Lexical Database \" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \" A Semantic Concordance \" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \" Fast and Accurate Sentence Alignment of Bilingual Corpora \" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \" A Systematic Comparison of Various Statistical Alignment Models \" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Schütz 1895 Schütz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . Natural Language Parsing as Statistical Pattern Recognition ( 1994 ) . Tools . by Joshua Goodman - IN PROCEEDINGS OF THE 34TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1996 . \" ... Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others . However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting th ... \" . Many different metrics exist for evaluating parsing results , including Viterbi , Crossing Brackets Rate , Zero Crossing Brackets Rate , and several others . However , most parsing algorithms , including the Viterbi algorithm , attempt to optimize the same metric , namely the probability of getting the correct labelled tree . By choosing a parsing algorithm appropriate for the evaluation metric , better performance can be achieved . We present two new algorithms : the \" Labelled Recall Algorithm , \" which maximizes the expected Labelled Recall Rate , and the \" Bracketed Recall Algorithm , \" which maximizes the Bracketed Recall Rate . Experimental results are given , showing that the two new algorithms have improved performance over the Viterbi algorithm on many criteria , especially the ones that they optimize . \" ... This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension . The paper proposes a simple informationtheoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel , incremental , probabi ... \" . This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension . This proposal subsumes and clarifies findings that high - constraint contexts can facilitate lexical processing , and connects these findings to well - known models of parallel constraint - based comprehension . In addition , the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension , including the reversal of locality - based difficulty patterns in syntactically constrained contexts , and conditions under which increased ambiguity facilitates processing . The paper examines a range of established results bearing on these predictions , and shows that they are largely consistent with the surprisal theory . . .. by Rebecca Hwa , Philip Resnik , Amy Weinberg , Clara Cabezas , Okan Kolak - Natural Language Engineering , 2005 . \" ... Broad coverage , high quality parsers are available for only a handful of languages . A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations ( also known as \" treebanking \" ) . However , syntactic annotation is a labor in ... \" . Broad coverage , high quality parsers are available for only a handful of languages . A prerequisite for developing broad coverage parsers for more languages is the annotation of text with the desired linguistic representations ( also known as \" treebanking \" ) . However , syntactic annotation is a labor intensive and time - consuming process , and it is difficult to find linguistically annotated text in sufficient quantities . In this article , we explore using parallel text to help solving the problem of creating syntactic annotation in more languages . The central idea is to annotate the English side of a parallel corpus , project the analysis to the second language , and then train a stochastic analyzer on the resulting noisy annotations . We discuss our background assumptions , describe an initial study on the \" projectability \" of syntactic relations , and then present two experiments in which stochastic parsers are developed with minimal human intervention via projection from English . 4 The parallel corpus is aligned at the word level using the GIZA++ implementation of the IBM statistical translation models ( Brown et al . ... . by Sameer Pradhan , Kadri Hacioglu , Valerie Krugler , Wayne Ward , James H. Martin , Daniel Jurafsky , 2005 . \" ... The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing - the process of assigning a WHO did WHAT to WHOM , WHEN , WHERE , WHY , HOW etc . structure to plain text . This process entails identifying groups of words in a sentence ... \" . The natural language processing community has recently experienced a growth of interest in domain independent shallow semantic parsing - the process of assigning a WHO did WHAT to WHOM , WHEN , WHERE , WHY , HOW etc . structure to plain text . This process entails identifying groups of words in a sentence that represent these semantic arguments and assigning specific labels to them . It could play a key role in NLP tasks like Information Extraction , Question Answering and Summarization . We propose a machine learning algorithm for semantic role parsing , extending the work of Gildea and Jurafsky ( 2002 ) , Surdeanu et al . ( 2003 ) and others . Our algorithm is based on Support Vector Machines which we show give large improvement in performance over earlier classifiers . We show performance improvements through a number of new features designed to improve generalization to unseen data , such as automatic clustering of verbs . We also report on various analytic studies examining which features are most important , comparing our classifier to other machine learning algorithms in the literature , and testing its generalization to new test set from different genre . On the task of assigning semantic labels to the PropBank ( Kingsbury , Palmer , & Marcus , 2002 ) corpus , our final system has a precision of 84 % and a recall of 75 % , which are the best results currently reported for this task . Finally , we explore a completely different architecture which does not requires a deep syntactic parse . We reformulate the task as a combined chunking and classification problem , thus allowing our algorithm to be applied to new languages or genres of text for which statistical syntactic parsers may not be available . \" ... Probabilistic Context - Free Grammars ( PCFGs ) and variations on them have recently become some of the most common formalisms for parsing . It is common with PCFGs to compute the inside and outside probabilities . When these probabilities are multiplied together and normalized , they produce the probabili ... \" . Probabilistic Context - Free Grammars ( PCFGs ) and variations on them have recently become some of the most common formalisms for parsing . It is common with PCFGs to compute the inside and outside probabilities . When these probabilities are multiplied together and normalized , they produce the probability that any given non - terminal covers any piece of the input sentence . The traditional use of these probabilities is to improve the probabilities of grammar rules . In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing . We give a framework for describing parsers . The framework generalizes the inside and outside values to semirings . It makes it easy to describe parsers that compute a wide variety of interesting quantities , including the inside and outside probabilities , as well as related quantities such as Viterbi probabilities and n - best lists . We also present three novel uses for the inside and outside probabilities . T .. rman and Weir , 1992 ) , and in current state - of - the - art systems , such as those of Charniak ( 1997 ) and Collins ( 1997 ) . One notable exception is Brill 's TransformationBased Error Driven system ( Brill , 1993 ) , which induces a set of transformations designed to maximize the Consistent ... . by Aoife Cahill , Michael Burke , Josef Van Genabith , Andy Way - In Proceedings of the 42nd Meeting of the ACL , 2004 . \" ... This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks . We extract LFG subcategorisation frames and paths linking LDD reentrancie ... \" . This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide - coverage , robust , probabilistic Lexical - Functional Grammar ( LFG ) resources acquired from treebanks . We extract LFG subcategorisation frames and paths linking LDD reentrancies from f - structures generated automatically for the Penn - II treebank trees and use them in an LDD resolution algorithm to parse new text . Unlike ( Collins , 1999 ; Johnson , 2002 ) , in our approach resolution of LDDs is done at f - structure ( attribute - value structure representations of basic predicate - argument or dependency structure ) without empty productions , traces and coindexation in CFG parse trees . ... tation . sal . , 2002 ) exploits configurational , categorial , PennII \" functional \" , local head and trace information to annotate nodes with LFG feature - structure equations . This partitions local subtrees of depth one ( corresponding to CFG rules ) into left and right contexts ( relative to head ) . The annotation al .. \" ... Interactive spoken dialogue provides many new challenges for natural language understanding systems . One of the most critical challenges is simply determining the speaker 's intended utterances : both segmenting a speaker 's turn into utterances and determining the intended words in each utterance . Eve ... \" . Interactive spoken dialogue provides many new challenges for natural language understanding systems . One of the most critical challenges is simply determining the speaker 's intended utterances : both segmenting a speaker 's turn into utterances and determining the intended words in each utterance . Even assuming perfect word recognition , the latter problem is complicated by the occurrence of speech repairs , which occur where speakers go back and change ( or repeat ) something they just said . The words that are replaced or repeated are no longer part of the intended utterance , and so need to be identified . Segmenting turns and resolving repairs are strongly intertwined with a third task : identifying discourse markers . Because of the interactions , and interactions with POS tagging and speech recognition , we need to address these tasks together and early on in the processing stream . This paper presents a statistical language model in which we redefine the speech recognition problem so that it includes the identification of POS tags , discourse markers , speech repairs and intonational phrases . By solving these simultaneously , we obtain better results on each task than addressing them separately . Our model is able to identify 72 % of turn - internal intonational boundaries with a precision of 71 % , 97 % of discourse markers with 96 % precision , and detect and correct 66 % of repairs with 74 % precision . . .. sking questions about its binary encoding . 3.3.3 Questions about Word Identities . Instead , we view the word identities as a further refinement of the POS tags . We start the clustering algorithm wit ... . ... ssible values of P w i c(w i i\\Gamman+1 ) are bucketed . If the last bucket has fewer than c min counts , we merge it with the preceding bucket . We use separate buckets for each n - gram model being interpolated . In performing this bucketing , we create an array containing how many n - grams occur for each value of P w i c(w i i\\Gamman+1 ) up to ... . \" ... Excellent results have been reported for DataOriented Parsing ( DOP ) of natural language texts ( Bod , 1993c ) . Unfortunately , existing algorithms are both computationally intensive and difficult to implement . Previous algorithms are expensive due to two factors : the exponential number of rules that mus ... \" . Excellent results have been reported for DataOriented Parsing ( DOP ) of natural language texts ( Bod , 1993c ) . Unfortunately , existing algorithms are both computationally intensive and difficult to implement . Previous algorithms are expensive due to two factors : the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm . In this paper we solve the first problem by a novel reduction of the DOP model toga small , equivalent probabilistic context - free grammar . We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents , rather than the probability of a correct parse tree . Using ithe optimizations , experiments yield a 97 % crossing brackets rate and 88 % zero crossing brackets rate . This differs significantly from the results reported by Bod , and is compara- ble to results from a duplication of Pereira and Schabes 's ( 1992 ) experiment on the same data . We show that Bod 's results are at least partially due to an extremely fortuitous choice of test data , and partially due to using cleaner data than other researchers . \" ... This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text - to - speech synthesizer . Text is first converted into a sequence of part - of - speech tags . Next a Markov model is used to give the most likely sequence of phrase breaks for the input part - of ... \" . This paper presents an algorithm for automatically assigning phrase breaks to unrestricted text for use in a text - to - speech synthesizer . Text is first converted into a sequence of part - of - speech tags . Next a Markov model is used to give the most likely sequence of phrase breaks for the input part - of - speech tags . In the Markov model , states represent types of phrase break and the transitions between states represent the likelihoods of sequences of phrase types occurring . The paper reports a variety of experiments investigating part - of - speech tag - sets , Markov model structure and smoothing . The best setup correctly identifies 79 % of breaks in the test corpus . © 1998 Academic Press Limited 1 . . .. cause syntactic parses themselves are unhelpful . These have been shown to significantly outperform rule - driven parsers . It is possible that a statistical parser could provide reliable parses and hence facilitate phrase break assignment . Reference ... \" ... We show how web mark - up can be used to improve unsupervised dependency parsing . Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints . Conversion procedures fall out of our ... \" . We show how web mark - up can be used to improve unsupervised dependency parsing . Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints . Conversion procedures fall out of our linguistic analysis of a newly available million - word hyper - text corpus . Web - scale experiments show that the DMV , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data . Our model , trained on a single blog , generalizes to 53.3 % accuracy out - of - domain , against the Brown corpus - nearly 10 % higher than the previous published best . The fact that web mark - up strongly correlates with syntactic structure may have broad applicability in NLP . \" ... We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames . Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as par ... \" . We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames . Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as paraphrasing , text entailment , and machine translation . We first apply chunking to raw corpora and then extract reliable chunks to ensure that high - quality predicate - argument structures are obtained from the chunks . Our experiments confirmed that we succeeded in acquiring highly reliable predicate - argument structures on a large scale . proposed an ensemble method ( Reichart and Rappoport , 2007 ) . They regarded parses as being of high quality if 20 different parsers agreed . They used an SVM regression approach on the basis of text - based and parse - based features . A method for acquiring reliable predicate - argument structures We acquire reliable predicate - argument str ... . \" ... It is well known that parsing accuracy suffers when a model is applied to out - of - domain data . It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) . Hence , an important task is to select appropriate domains . However , most ... \" . It is well known that parsing accuracy suffers when a model is applied to out - of - domain data . It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) . Hence , an important task is to select appropriate domains . However , most previous work on domain adaptation relied on the implicit assumption that domains are somehow given . As more and more data becomes available , automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive . This paper evaluates various ways to automatically acquire related training data for a given test set . The results show that an unsupervised technique based on topic models is effective - it outperforms random data selection on both languages examined , English and Dutch . Moreover , the technique works better than manually assigned labels gathered from meta - data that is available for English . ... y weighting trees in the WSJ according to their similarity to the subdomain . McClosky et al . ( 2010 ) coined the term multiple source domain adaptation . Similar to us , McClosky et al . ( 2010 ) regard a target domain as mixture of source domains , b .. by Joseph Le Roux , Jennifer Foster , Joachim Wagner , Rasul Samad , Zadeh Kaljahi , Anton Bryl . \" ... The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text . The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training . In the second submission , also a constituency parsing ... \" . The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text . The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training . In the second submission , also a constituency parsing system , the n - best lists of various parsing models are combined using an approximate sentence - level product model . The third system , the highest ranked system in the dependency parsing track , uses voting over dependency arcs to combine the output of three constituency parsing systems which have been converted to dependency trees . All systems make use of a data - normalisation component , a parser accuracy predictor and a genre classifier . ... WSJ data ( Petrov and Klein , 2007 ; Foster , 2010 ) . The parser uses the English signature list described in Attia et al ( 2010 ) to assign partof - speech tags to unknown words . \" ... Domain adaptation is an important task in order for NLP systems to work well in real applications . There has been extensive research on this topic . In this paper , we address two issues that are related to domain adaptation . The first question is how much genre variation will affect NLP systems ' per ... \" . Domain adaptation is an important task in order for NLP systems to work well in real applications . There has been extensive research on this topic . In this paper , we address two issues that are related to domain adaptation . The first question is how much genre variation will affect NLP systems ' performance . We investigate the effect of genre variation on the performance of three NLP tools , namely , word segmenter , POS tagger , and parser . We choose the Chinese Penn Treebank ( CTB ) as our corpus . The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist . To answer the question , we extend the parsing prediction model in ( Ravi et al . , 2008 ) to provide prediction for word segmentation and POS tagging as well . Our experiments show that the predicted scores are close to the real scores when tested on the CTB data . . .. our corpus . The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist . Our experiments show that the predicted scores are close to the real scores when tested on the CTB data . Keywords : genre variatio ... . ... poor et al . , 2009 ; Biber & Gray , 2010 ) , but the most interesting usages apply the divergence to a machine learning system . Despite the fact that authors have shown that a divergence ( Van Asch & Daelemans , 2010 ; Plank , 2011 ) or a linear combination of divergences ( McClosky , 2010 ) can be successfully used to link the sim ... . \" ... Current efforts in syntactic parsing are largely data - driven . These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures . Labeled data typically requires expert annotators which makes it both time consuming and costly to produce . Furthermo ... \" . Current efforts in syntactic parsing are largely data - driven . These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures . Labeled data typically requires expert annotators which makes it both time consuming and costly to produce . Furthermore , once training data has been created for one textual domain , portability to similar domains is limited . This domain - dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain . The simplest approach to this task is to assume that the target domain is essentially the same as the source domain . No additional knowledge about the target domain is included . A more realistic approach assumes that only raw text from the target domain is available . This assumption lends itself well to semi - supervised learning methods since these utilize both labeled and unlabeled examples . This dissertation focuses on a family of semi - supervised methods called self - training . Self - training creates semi - supervised learners from existing supervised learners with minimal effort . We first show results on self - training for constituency parsing within a single domain . While self - training has failed here in the past , we present a simple modification which allows it to succeed , producing state - of - the - art results for English constituency parsing . Next , we show how self - training is beneficial when parsing across domains and helps . \" ... We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences . We take two popular dependency parsers - one graph - based and one transition - based - and ... \" . We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences . We take two popular dependency parsers - one graph - based and one transition - based - and compare results for both . Results show that using semisupervised learning in the form of self - training and co - training yields only very modest improvements in parsing accuracy . We also try to use morphological information in a targeted way and fail to see any improvements . \" ... Current statistical parsers tend to perform well only on their training domain and nearby genres . While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains . When parsing document collectio ... \" . Current statistical parsers tend to perform well only on their training domain and nearby genres . While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains . When parsing document collections involving heterogeneous domains ( e.g. the web ) , the optimal parsing model for each document is typically not obvious . We study this problem as a new task - multiple source parser adaptation . Our system trains on corpora from many different domains . It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy . Given a specific target text , the resulting system proposes linear combinations of parsing models trained on the source corpora . Tested across six domains , our system outperforms all non - oracle baselines including the best domain - independent parsing model . Thus , we are able to demonstrate the value of customizing parsing models to specific domains . ... train models in many different domains but sidestep the problem of domain detection . Thus , our work is orthogonal to theirs . These works aim to predict the parser performance on a given target sentence . Ravi et al . ( 2008 ) frame this as a regression problem . Kawahara and Uchimoto ( 2008 ) treat ... . \" ... Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation . It has also been found to benefit retrieval of spoken or written docu - ments . At its ... \" . Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation . It has also been found to benefit retrieval of spoken or written docu - ments . At its base , however , classification assumes separability . This paper revisits an assump - tion that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions . Results on a very heterogeneous corpus of post-1990s American English reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study . The resulting model can provide a basis for more detailed analysis of sub - genres and the relation between genre and situations of language use , as well as a means to predict distributional properties of new genres . ... sub - field in its own right ( see for example ( Mehler et al . , 2010 ) ) . Review ARTICLE . Implicit memory in music and language . 1 Roxelyn and Richard Pepper Department of Communication Sciences and Disorders , Northwestern University , Evanston , IL , USA . 2 Department of Music , University of Arkansas , Fayetteville , AR , USA . 3 Department of Otolaryngology - Head and Neck Surgery , Feinberg School of Medicine , Northwestern University , Chicago , IL , USA . Research on music and language in recent decades has focused on their overlapping neurophysiological , perceptual , and cognitive underpinnings , ranging from the mechanism for encoding basic auditory cues to the mechanism for detecting violations in phrase structure . These overlaps have most often been identified in musicians with musical knowledge that was acquired explicitly , through formal training . In this paper , we review independent bodies of work in music and language that suggest an important role for implicitly acquired knowledge , implicit memory , and their associated neural structures in the acquisition of linguistic or musical grammar . These findings motivate potential new work that examines music and language comparatively in the context of the implicit memory system . Introduction . Music has been called the universal language of mankind ( Longfellow , 1835 ) reflecting longstanding curiosity on the relationship between music and language . Both share many traits including being perceived primarily through the auditory system , having similar acoustic attributes and reflecting analogous generative syntactic systems . This has led to decades of scientific research , exemplified by the papers included in this volume , exploring their overlapping neurophysiological , perceptual , and cognitive underpinnings . While using trained musicians has led to great strides in our understanding of how music is processed , it has obscured another important similarity between music and language : both may be acquired implicitly , without the aid of explicit instruction . In this paper , we review independent bodies of research exploring the role of implicitly acquired knowledge and associated neural structures in the acquisition of language and musical grammar . We first consider the role of implicit memory in language by looking at both natural and artificial language learning studies . The studies discussed in the Section \" Implicit Memory and Language \" show that the implicit memory system plays an important role in acquiring the grammar , or rules , of language at all levels of linguistics structure ( Table 1 ) . Similarly , implicit learning in music is found in the acquisition of rhythm , pitch , and melodic structures . The studies discussed in the Section \" Implicit Memory and Music \" suggest a potentially common learning mechanism shared by both music and language that allows for the acquisition of these complex systems without the need for instruction ( Table 1 ) . The studies we discuss below help us understand this mechanism by highlighting the fact that both music and language involve expectation and the tracking of dependencies between sequential elements . Neurally , there is a significant three - way overlap of the brain structures implicated in implicit memory and those involved in learning language and learning music . This convergence encourages new work that juxtaposes music and language in the context of the implicit memory system . Given the known relationship between dopamine and the implicit memory system , we may also consider more directly the genomic and molecular bases of music and language abilities . Implicit Memory . Implicit memory is generally defined as acquired knowledge that is not available to conscious access ( Schacter and Graf , 1986 ; Schacter , 1987 ) . This contrasts with explicit memory , which is characterized by knowledge that involves conscious recollection , recall , or recognition . The majority of behavioral evidence for an implicit memory system is based on experiments wherein experience leads to altered performance on some task without participants being aware of having learned anything . One type of implicit memory stems from perceptual learning , which involves changes to the perceptual system and to perceptual categories ( e.g. , phonemes , chords ) due to experience . For example , in one study ( Wade and Holt , 2005 ) , participants played a video game that involved navigating through a maze . A non - critical feature of the game was that certain non - speech auditory cues were associated with certain events . After playing the game , participants were better able to distinguish the sounds and reliably learned the sound - event patterns . Importantly , learning was qualitatively different , and in some cases better , than explicit training on these same patterns . While explicit attention has been shown to facilitate this sort of perceptual learning ( e.g. , Ranganath and Rainer , 2003 ) it is also well established that perceptual learning can be subliminal and implicit ( Goldstone , 1998 ; Seitz and Watanabe , 2003 ) . Another type of implicit memory involves the implicit learning of sequences ( e.g. , sentences , melodies ) . A commonly used paradigm to test implicit memory for sequences is the serial reaction time test ( SRTT ; Nissen and Bullemer , 1987 ) . In this test , participants are exposed to some stimuli ( e.g. , objects appearing sequentially at different points on a screen ) cuing participants to respond ( e.g. , by indicating where the stimuli appear ) as quickly as possible . While the sequences of stimuli appear to be random to the participant , embedded within the random sequences is a fixed pattern , repeatedly interspersed throughout the random sequences . Over the course of the experiment , response times and accuracy on the fixed sequences improves relative to the random sequences , presumably because the participants are learning this repeated sequence . Crucially , participants do not exhibit an improved ability to explicitly recall this repeated sequence as compared to recalling random non - repeated sequences . The fact that participants show implicit learning without explicit knowledge suggests that these memory systems can operate independently , and that people can learn about the sequencing of some stimuli without being explicitly aware of it . While implicit memory is relevant for both sequence learning and category learning and both sequence learning and category learning are relevant to language and music , we focus primarily on implicit memory in the context of sequence learning . A more specific kind of implicit sequence learning often discussed in the context of language and music is statistical learning ( e.g. , Saffran et al . , 1996a ) . Statistical learning involves the same basic idea that participants can learn sequences without explicit awareness , but adds an additional component of tracking statistics over these sequences 1 . For example , in a series of studies , Saffran et al . ( 1996a , b , 1999 ) showed that adults , children , and infants are able to track transitional probabilities between syllables and tones . Participants were exposed to seemingly random sequences of syllables obscuring consistent differences in the probability that certain syllables followed others ( see below for more details ) . Participants were sensitive to these differences in transitional probability , and subsequent work has explored what types of statistics and what types of dependencies can be implicitly tracked ( Knowlton and Squire , 1996 ; Aslin et al . , 1998 ; Gomez , 2002 ) . Finally , by virtue of the fact that dopamine receptors are found in the basal ganglia , and in particular , the striatum ( which includes the caudate , putamen , and nucleus accumbens ) , implicit memory has been associated with dopamine . Implicit Memory and Language . Language learning shares a number of important similarities with the learning of sensory - motor sequences , which have been classically associated with implicit memory and which , as will be discussed below , are also implicated in acquiring a musical system . As with the tasks used in implicit learning experiments ( e.g. , the SRTT ) , people are often unaware of , or unable to articulate many of the rules of their language ( Fodor , 1983 ) . People can also learn language without any explicit instruction ( Chomsky , 1957 ) . Finally , certain aspects of linguistic knowledge , namely the rules of combination , may be represented probabilistically or as information about the distributional relationships at different levels of linguistic structure ( e.g. , phonemes , morphemes , words , and sentences ; Redington and Chater , 1997 ) . This knowledge is generally not consciously accessible to speakers of a language and is similar in nature to the probabilistic knowledge acquired in implicit learning . Implicitly Learned Artificial Grammars . The use of implicitly learned distributional information for language learning has been demonstrated at many different levels of linguistic structure . For example , at the level of word segmentation , Saffran et al . ( 1996a ) exposed 8-month - old infants to a stream of running speech consisting of four three - syllable words without any breaks or pauses indicating word - hood . Thus , the only cue to word segmentation was the transitional probability between syllables , where within - word transitional probability of syllables was 1.0 and between - word transitional probability of syllables was 0.33 ( no word followed itself ) . Infants showed a significant ability to discriminate words from part - words ( formed by combining the final syllable from one word with the first two syllables of another ) . Adults performed similarly ( Saffran et al . , 1996b ) in what is argued to reflect implicit learning of word segmentation ( Evans et al . , 2009 ) . Importantly , this ability is suggested to be domain general as it also applies to tones ( Saffran et al . , 1999 and below in the discussion on implicit memory and music ) and visual stimuli ( Fiser and Aslin , 2001 ) . Analogous behavior is also found with respect to the acquisition of phonotactics . Phonotactics are the restrictions on where phonemes can occur in a word in a language ( e.g. , English prohibits ng starting a word or h ending one ) . In one study ( Onishi et al . , 2002 ) , adults briefly exposed to pseudo - words reflecting some non - English phonotactic generalization showed speeded repetition to words that adhered to the generalization as compared to words that did not . Another study on implicit phonotactic learning ( Dell et al . , 2000 ) found that when participants are tasked with repeating sets of words reflecting some phonotactic generalization , their speech errors tend to reflect these newly learned generalizations , as is true of one 's native language . The authors assessed the implicitness of learning using something they call the \" ask - tell technique . \" This involved asking all participants whether they had noticed anything about the words they were pronouncing ; the experimenters also told half the participants , explicitly , what the phonotactics would be before starting . Neither the uninformed nor informed participants were able to identify any regularities in the experimental materials . These results , in addition to the fact that the speech errors were not intentional , suggest that this learning is , in fact , implicit . Another important component of learning the phonology of a language , acquiring phonological rules , has been shown to relate to non - linguistic implicit learning as well ( Ettlinger et al . , in press ) . In this study , participants took both an artificial grammar learning experiment and a test of implicit learning . The artificial grammar learning task involved exposure to words that reflected a set of rules for forming plural and diminutive variants ( e.g. , dog , dogs , doggie , doggies ) . The test of implicit learning was a modified version of the Tower of London task ( Shallice , 1982 ) . In this task , participants were required to solve puzzles , increasing in difficulty , which involved virtually moving colored balls on three sticks to match a predetermined pattern . Embedded within the puzzles were repeated sequences of moves , and participants were asked to think through their moves before starting , to minimize the effects of motor coordination , unlike the SRTT . Implicit learning was measured by looking at the improvement in performance on the repeated sequences ( Phillips et al . , 1999 ) . Results showed a strong correlation between learning the artificial language and performance on the Tower of London task , suggesting that implicit memory and language learning are linked . In another set of experiments exploring the possible implicit learning of syntactic structure , Reber ( 1967 ) taught participants an artificial finite state grammar for sequences of letters ( Figure 1 ) . After exposure to strings of letters generated by the grammar , participants were asked to judge the grammaticality of novel sequences of letters . Participants were able to successfully distinguish what constituted a valid sequence without being able to explicitly describe the rules of the grammar . Figure 1 . Examples of finite state grammars used in language ( A ) and music ( B ) learning experiments . Participants can acquire grammars of this sort and identify valid versus invalid sequences without being explicitly aware of any specific aspects of the grammar for both music and language . In addition to these associative studies , more concrete evidence on the role of implicit memory in language learning is provided by recent imaging studies . Greater activation was found in inferior frontal gyrus for the statistical cue and statistical cue plus phonetic cue conditions as compared to the random condition . Additional activation was found in the superior temporal gyrus , which is associated with the processing of speech ( Geschwind , 1970 ) . This has led to the hypothesis that the basal ganglia , and therefore , presumably , implicit memory , is important for the integration of multiple information sources during the process of language learning ( Rodriguez - Fornells et al . , 2009 ) . Similarly , patients with early stage Huntington 's disease and striatal damage also do poorly on tasks of this sort ( De Diego - Balaguer et al . , 2008 ) . As with the SRTT and word segmentation , a fronto - striatal network is implicated in acquiring the finite state grammars described above . The ability of Parkinson 's patients with degeneration of the basal ganglia to learn artificial grammars is less clear , however , with conflicting evidence present ( Peigneux et al . , 1999b ; Witt et al . , 2002 ) . Activation of the caudate is also found in a study of syntactic processing ( Moro et al . , 2001 ) . In this latter study , participants were exposed to a version of Italian ( the participants ' native language ) where all content words were replaced with pseudo - words , with function words left intact , which served to eliminate any semantic component of processing . Syntactic ( word order ) , morphological ( determiner agreement ) , and phonotactic violations were juxtaposed using PET . Greater activation was also found in the left caudate , which is associated with implicit memory ( see above ) . This result had been replicated a number of times with different types of artificial syntactic grammar , with activation consistently found in Broca 's area and the caudate ( Forkstam et al . , 2006 ; Petersson et al . , in press ) . Implicit Learning and Natural Language . With respect to language learning in more ecologically valid settings , a few behavioral studies have shown a relationship between natural language processing and implicit learning . Misyak et al . ( 2010 ) created an implicit learning task that combined the SRTT and artificial grammar learning and showed that performance correlated with participants ' ability to comprehend complex English sentences . Evans et al . ( 2009 ) looked at children with specific language impairment , ages five to seven , and showed that these children also performed worse on the word - segmentation task from Saffran et al . ( 1996 ) as compared to a control group with the same non - verbal IQ . Furthermore , performance on the word - segmentation task correlated with vocabulary size within each participant group , suggesting implicit learning facilitates word learning . Finally , research looking into language processing in more realistic settings has also considered language processing in noise ( e.g. , Wong et al . , 2008 , 2009a , 2010 ; Harris et al . , 2009 ) . In particular , Conway et al . ( 2010 ) showed a relationship between an ability to perceive speech in noise and implicit sensory - motor sequence learning . Participants who were good at an SRTT - like task were similarly good at perceiving sentences embedded in noise when the last word in the sentence had high - predictability ( e.g. , Her entry should win first prize ) , even when controlling for working memory and intelligence . The correlation disappeared for sentences ending in low - predictability words ( e.g. , The arm is riding on the beach ) . This suggests that an important way in which implicit memory is related to language is through prediction and anticipation . A number of studies using eye - tracking ( see Kamide , 2008 for a review ) and event - related potentials ( ERPs ; see Van Berkum , 2008 for a review ) have shown that people make significant use of context to facilitate processing . For example , participants look more often at a picture of beer than a doll when hearing the beginning of the sentence the man will taste the ... ( Kamide et al . , 2003 ) . A violation of an anticipated sentence completion will also yield a specific ERP response , either N400 for semantic incongruency or P600 for syntactic . The same ERP response is elicited on encountering anomalies in predicted outcome for artificial grammars similar to Reber ( 1967 , above ; Friederici et al . , 2002 ) and music ( Patel et al . , 1998a ) . Additional neural evidence comes from functional imaging , showing a significant overlap in the brain regions associated with implicit memory and language . As mentioned above , Broca 's area has been implicated in implicit memory tasks , and Broca 's area has a longstanding association with language learning and language processing ( Embick et al . , 2000 ; Grodzinsky , 2000 ; Sahin et al . , 2009 ) . Thus , there is a wide range of similarities between language and implicit knowledge both in terms of their neural substrates ( the fronto - striatal system ) and in their cognitive underpinnings ( sequential knowledge , expectation ) . To conclude , there is extensive and convergent evidence for a close relationship between the cognitive and neurophysiological underpinnings of language learning and implicit memory . Language learning involves cognitive abilities that are generally learned implicitly , including tracking dependencies and developing expectations regarding adjacent linguistic structures . Language and implicit memory are also both supported by a set of neural structures including the anterior portion of the inferior frontal gyrus and the basal ganglia . As will be reviewed below , music shares many of these same associations with implicit memory and these shared associations are not restricted to musicians with formal musical training , but extend to everyday music listeners . Implicit Memory and Music . Although music is sometimes held to be the domain of specialists , its near - ubiquity in daily life , from mp3 players to Internet radio , cinema , and advertising , shows that affinity for music is widespread . Indeed , music has frequently been postulated by anthropologists to be a human universal , present in all known cultures ( Blacking , 1973 ; Zatorre and Peretz , 2001 ) . Although the ability to perform music skillfully is not evenly distributed and often relies on years of formal training , the ability to listen , process , and respond emotionally to music is shared across most of the population and seems to depend only on implicit exposure . For example , Bigand et al . ( 2005 ) showed that people with and without formal training responded largely interchangeably to non - vocal classical music . Other deep musical abilities in people without explicit training , such as the ability to perceive the relationship between a theme and its variations and to learn new compositional systems , are chronicled in Bigand and Poulin - Charronnat ( 2006 ) . With little to no explicit training , how is it possible for people to develop the ability to represent and respond appropriately to the complex syntactic structures of music ? Desain and Honing ( 1999 ) demonstrate that even a seemingly simple and near - universal ability like tapping to a beat depends on complex internal representations of harmonic and syntactic musical structures . Indeed , research summarized in Krumhansl ( 1990 ) shows that implicit exposure to Western tonal music is sufficient for listeners to develop internal representations of the pitch relationships that music theorists hold to underlie tonality . Given a tonal context , such as a scale or chord progression , listeners without formal training can accurately judge how well a given continuation fits the established tonality . One of the mechanisms by which passive exposure can ultimately yield sophisticated internal representations is statistical learning . Saffran et al . ( 1999 ) constructed long isochronous tone sequences out of 6 three - note \" figures \" repeated in random order , with no breaks or other indication of boundaries between the figures , and constrained so that the same figure never appeared twice in succession . When infants were exposed to this series of tones over a 20-min period , they were able to abstract the constituent three - note figures , despite the fact that nothing but the reduced transition probabilities between them delineated the figures in the continuous stream of the musical surface . The infants , it seemed , had carefully tracked continuation probabilities in the sequence , despite the fact that their exposure to it was entirely passive . This ability to track common outcomes in musical repertoires may seem arbitrary , but in fact has been held by music theorists and psychologists since Meyer ( 1956 ) to form the basis of affective responses to music ( see Huron and Margulis , 2010 for a summary ) . Continuations that are recognized , even implicitly , as unusual are thought to result in perceptions of special expressivity or esthetic charge . In this way , the ability to implicitly track statistics about continuations may form the fundamental scaffolding for the widespread ability to respond emotionally to music , even in the absence of formal training . Implicit memory for music also reveals itself in various well - documented priming effects . Priming is generally defined as an implicit memory effect in which exposure to a stimulus influences responses to later stimulus without awareness of or an ability to recall the specific prime ( Tulving et al . , 1982 ) . For example , Hutchins and Palmer ( 2008 ) showed that participants were more accurate in singing back the last tone of a short melody if that tone had appeared previously in the melody . Musical priming can also evidence itself in the form of faster and more accurate judgments about pitches or chords that are normative and expected given the tonal context . This kind of tonal priming has been documented in responses to melodic continuations ( Margulis and Levine , 2006 ) , and harmonic continuations ( Bigand and Pineau , 1997 ) by listeners with no formal training . fMRI studies have implicated suppressed activity in bilateral inferior frontal regions of the brain during harmonic priming ( Tillmann et al . , 2000 , 2003 ) . It has even been documented in children ( Schellenberg et al . , 2005 ) . Bharucha and Stoeckig(1986 , 1987 ) provide evidence that harmonic priming is cognitive ( based on the implicit abstraction of regularities in the musical environment ) rather than sensory ( based on psychoacoustic relationships ) in nature . Tillmann et al . ( 2000 ) propose a self - organizing network model that can account for the kind of implicit learning of tonal structure revealed by priming studies . These priming effects are also observed to reflect the acquisition of musical grammars implicitly learned in the same fashion as in the implicit language learning experiments above ( Figure 1 ; Tillmann and Poulin - Charronnat , 2010 ) . It is not only continuation statistics that listeners track implicitly . Duple and quadruple meters are more common than triple meters in Western music , and Brochard et al . ( 2003 ) confirmed that when presented with an ambiguous stimulus , listeners assume a binary division of the beat . Relatedly , the major mode is more common than the minor mode in Western music , and Huron ( 2006 ) confirmed that when presented with an ambiguous stimulus , listeners assume the major mode . It is clear that mere exposure , independent of formal training , or active use ( such as performance or participation ) is sufficient to engender highly structured and highly specific memory traces in ordinary listeners . Implicit memory for music emerges consistently in preference effects . Halpern and O'Connor ( 2000 ) showed that although explicit recognition memory for melodies deteriorated with age , implicit memory was retained , in the form of elevated preference ( the mere exposure effect first documented in Zajonc , 1968 ) . A battery of studies over the past several decades ( summarized nicely in Szpunar et al . , 2004 ) illustrate that listeners ' preference increases for music that has been encountered before . This effect is even stronger for music that is complex or ecologically valid ( Bornstein , 1989 ) . Similarly , Peretz et al . ( 1998 ) found that explicit recognition memory was more susceptible to decay over time than implicit memory measured by elevated preference . They concluded that , in contrast with explicit memory , implicit memory as manifested in affective judgments operates obligatorily , in an automatic and unconscious fashion . Wong et al . ( 2009b ) illustrate that passive exposure to the music from two cultures can result in the development of true bimusicals who approach both styles with affective and cognitive competence lacking in monomusicals of similar age and background . Wong et al . ( in press ) used structural equation modeling to investigate fMRI data from bimusical and monomusical listeners , finding more connectivity , and larger differentiation between the musical systems in bimusicals . These differences imply that even the implicit learning of multiple musical systems can result in fundamental changes to the way the brain approaches expressive sound . Electrophysiological evidence also supports this conclusion . Violations of expected harmonic , melodic , and rhythmic patterns result in a late positive component ( LPC ) characteristic of the detection of an incongruity , even when the participants lacked formal training and were unable to explicitly identify the surprises ( Besson and Faita , 1995 ) . The elicitation of ERP components related to syntactic violations in music seem to be independent of the task relevance of unexpected chords , and provides strong evidence for important implicit components to musical ability ( Koelsch et al . , 2000 ) . Patel et al . ( 1998b ) were the first to show that the P600 - a known marker of syntactic violations in language - extended to syntactic violations in music grammars that are abstracted implicitly by listeners . Generally , these responses have been found even when the musical exposure is entirely passive , as in Koelsch and Jentschke ( 2008 ) , when participants were watching a silent movie . Koelsch ( 2010 ) emphasizes that the early right anterior negativity ( ERAN ) that emerges in response to syntactic violations in music depends on the long - term extraction of statistical regularities in music , not from short - term exposure to particular sequences . Predictions based on these abstractions of musical syntax are thought to be localized in the premotor cortex and the inferior frontal gyrus ( particularly Broca 's area ) . There is also some evidence that the source of the ERP component responding to expectation violation may have origins in the right temporal - limbic areas , which is associated with affect and emotive processing ( James et al . , 2008 ) . The processing of syntactic violations in music has also been shown to interfere with the processing of syntactic violations in language , suggesting overlap for these two functions . So , implicit memory seems to play an important role in syntactic processing in both language and music . Implicit Memory in Language and Music . We have reviewed above independent sets of empirical studies implicating the role of the implicit memory system in music and language , summarized in Table 1 . In particular , we have discussed the fact that explicit training is not required for processing of language or music . It is important to note that these studies examined music or language alone . To ascertain common pathways in processing and/or representation , music and language should be examined in tandem . In terms of processing , studies could be conducted such as those performed by Patel and Slevc and colleagues ( Patel et al . , 1998b ; Slevc et al . , 2009 ) in which musical and linguistic stimuli were combined . However , it is preferable that everyday music listeners should be examined to ascertain that the results are not due to formal musical training alone or trained musicians possessing a genetic difference . Studies examining the dependence and independence of musical and linguistic functions sometimes yield conflicting results . In particular , the lesion literature favors independence while studies on neurologically normal subjects favor dependence . It is beyond the scope of this proposal to extensively discuss the nature of this debate , except to mention that a reconciliation has been proposed by imposing a distinction between representation and processing at least for syntax ( Patel , 2008 ) . In his Shared Syntactic Integration Resource Hypothesis , Patel ( 2003 ) postulates that while musical and linguistic syntactic representations are maintained separately , the processing of both musical and linguistic syntactic structures overlapped in neural resources . While the processing aspect of this hypothesis has much support ( Patel et al . , 1998b ) and is conceivably more feasible to test , representations are difficult to examine . However , neural repetition - suppression / enhancement paradigms have been used recently to examine mental representations in humans ( Grill - Spector et al . , 2006 ) and can potentially be used to test whether musical and linguistic representations overlap in neural regions . More specifically related to the implicit memory system , we believe such experiments could be conducted with both music and language studied side - by - side . Major divisions of the dopaminergic system contain neurons from the substantia nigra pars compacta and ventral tegmental area projecting to divisions of the striatum and prefrontal cortex , and other regions ( see Seamans and Yang , 2004 for a review ) . As discussed above , these brain regions are also associated with the implicit memory system . Future research into the role of the implicit memory system in music and language could employ similar methods to more directly examine their potentially shared molecular neurobiological mechanisms . Conflict of Interest Statement . Acknowledgments . The authors would like to thank Lionel Newman for assistance in editing this manuscript . Support provided by grant T32 NS047987 to Marc Ettlinger and NIH grants R01DC008333 , R21DC007468 , and NSF BCS-1125144 to Patrick C. M. Wong . Bigand , E. , Vieillard , S. , Madurell , F. , Marozeau , J. , and Dacquet , A. ( 2005 ) . Multidimensional scaling of emotional responses to music : the effect of musical expertise and of the duration of the excerpts . Cogn . Emot . Brochard , R. , Abecasis , D. , Potter , D. , Ragot , R. , and Drake , C. ( 2003 ) . The \" ticktock \" of our internal clock : direct brain evidence of subjective accents in isochronous sequences . Psychol . Sci . De Diego - Balaguer , R. , Couette , M. , Dolbeau , G. , Durr , A. , Youssov , K. , and Bachoud - Levi , A. C. ( 2008 ) . Striatal degeneration impairs language learning : evidence from Huntington 's disease . Brain 131 , 2870 - 2881 . de Vries , M. H. , Barth , A. C. R. , Maiworm , S. , Knecht , S. , Zwitserlood , P. , and Flöel , A. ( 2010a ) . Electrical stimulation of Broca 's area enhances implicit learning of an artificial grammar . J. Cogn . Neurosci . de Vries , M. H. , Ulte , C. , Zwitserlood , P. , Szymanski , B. , and Knecht , S. ( 2010b ) . Increasing dopamine levels in the brain improves feedback - based procedural learning in healthy participants : an artificial - grammar - learning experiment . Neuropsychologia 48 , 3193 - 3197 . DeKeyser , R. , and Larson - Hall , J. ( 2005 ) . \" What does the critical period really mean ? \" in Handbook of Bilingualism : Psycholinguistic Approaches , eds . J. F. Kroll , and A. M. B. De Groot ( New York , NY : Oxford University Press ) , 88 - 108 . Dell , G. S. , Reed , K. D. , Adams , D. R. , and Meyer , A. S. ( 2000 ) . Speech errors , phonotactic constraints , and implicit learning : a study of the role of experience in language production . J. Exp . Psychol . Learn . Mem . Cogn . Doyon , J. , Gaudreau , D. , Laforce , R. Jr. , Castonguay , M. , Bedard , P. J. , Bedard , F. , and Bouchard , J. P. ( 1997 ) . Role of the striatum , cerebellum , and frontal lobes in the learning of a visuomotor sequence . Brain Cogn . Doyon , J. , Laforce , R. Jr. , Bouchard , G. , Gaudreau , D. , Roy , J. , Poirier , M. , Bedard , P. J. , Bedard , F. , and Bouchard , J. P. ( 1998 ) . Role of the striatum , cerebellum and frontal lobes in the automatization of a repeated visuomotor sequence of movements . Neuropsychologia 36 , 625 - 641 . Ettlinger , M. , Bradlow , A. R. , and Wong , P. C. M. ( in press ) . \" The persistence and obliteration of opaque interactions , \" in Proceedings of the 45th Annual Meeting of the Chicago Linguistics Society , ed . R. Bochnak Chicago , IL : Chicago Linguistics Society . Evans , J. L. , Saffran , J. R. , and Robe - Torres , K. ( 2009 ) . Statistical learning in children with specific language impairment . J. Speech Lang . Hear . Res . Exner , C. , Koschack , J. , and Irle , E. ( 2002 ) . The differential role of premotor frontal cortex and basal ganglia in motor sequence learning : evidence from focal basal ganglia lesions . Learn . Mem . Friederici , A. D. , Steinhauer , K. , and Pfeifer , E. ( 2002 ) . Brain signatures of artificial language processing : evidence challenging the critical period hypothesis . Proc . Natl . Acad . Sci . U.S.A. 99 , 529 - 534 . Gabrieli , J. D. , Stebbins , G. T. , Singh , J. , Willingham , D. B. , and Goetz , C. G. ( 1997 ) . Intact mirror - tracing and impaired rotary - pursuit skill learning in patients with Huntington 's disease : evidence for dissociable memory systems in skill learning . Neuropsychology 11 , 272 - 281 . Jackson , G. M. , Jackson , S. R. , Harrison , J. , Henderson , L. , and Kennard , C. ( 1995 ) . Serial reaction time learning and Parkinson 's disease : evidence for a procedural learning deficit . Neuropsychologia 33 , 577 - 593 . James , C. E. , Britz , J. , Vuilleumier , P. , Hauert , C. A. , and Michel , C. M. ( 2008 ) . Early neuronal responses in right limbic structures mediate harmony incongruity processing in musical experts . Neuroimage 42 , 1597 - 1608 . Kamide , Y. , Scheepers , C. , and Altmann , G. T. ( 2003 ) . Integration of syntactic and semantic information in predictive processing : cross - linguistic evidence from German and English . J. Psycholinguist . Res . Knowlton , B. J. , and Squire , L. R. ( 1996 ) . Artificial grammar learning depends on implicit acquisition of both abstract and exemplar - specific information . J. Exp . Psychol . Learn . Mem . Cogn . Koechlin , E. , Danek , A. , Burnod , Y. , and Grafman , J. ( 2002 ) . Medial prefrontal and subcortical mechanisms underlying the acquisition of motor and cognitive action sequences in humans . Neuron 35 , 371 - 381 . Koelsch , S. ( 2010 ) . \" Unconscious memory representations underlying music - syntactic processing and processing of auditory oddballs , \" in Unconscious Memory Representations in Perception : Processes and Mechanisms in The Brain , eds . I. Cziglar , and I. Winkler ( Herndon , VA : John Benjamins Publishing Co. ) , 209 - 244 . Koelsch , S. , Gunter , T. , Friederici , A. D. , and Schroger , E. ( 2000 ) . Brain indices of music processing : \" nonmusicians \" are musical . J. Cogn . Neurosci . Lieberman , M. D. , Chang , G. Y. , Chiao , J. , Bookheiner , S. Y. , and Knowlton , B. J. ( 2004 ) . An event - related fMRI study of artificial grammar learning in a balanced chunk strength design . J. Cogn . Neurosci . McNab , F. , Varrone , A. , Farde , L. , Jucaite , A. , Bystritsky , P. , Forssberg , H. , and Klingberg , T. ( 2009 ) . Changes in cortical dopamine D1 receptor binding associated with cognitive training . Science 323 , 800 - 802 . Paradis , M. ( 1994 ) . \" Neurolinguistic aspects of implicit and explicit memory : implications for bilingualism , \" in Implicit and Explicit Learning of Second Languages , ed . N. Ellis ( London : Academic Press ) , 393 - 419 . Paradis , M. ( 2009 ) . Neurolinguistic Aspects of Bilingualism . Amsterdam : John Benjamins . Patel , A. D. ( 2003 ) . Language , music , syntax and the brain . Nat . Neurosci . Peigneux , P. , Maquet , P. , Van Der Linden , M. , Meulemans , T. , Degueldre , C. , Delfiore , G. , Luxen , A. , Cleeremans , A. , and Franck , G. ( 1999a ) . Left inferior frontal cortex is involved in probabilistic serial reaction time learning . Brain Cogn . Peigneux , P. , Meulemans , T. , Van Der Linden , M. , Salmon , E. , and Petit , H. ( 1999b ) . Exploration of implicit artificial grammar learning in Parkinson 's disease . Acta Neurol . Belg . Peretz , I. , Gaudreau , D. , and Bonnel , A. M. ( 1998 ) . Exposure effects on music preference and recognition . Mem . Cognit . Rauch , S. L. , Whalen , P. J. , Savage , C. R. , Curran , T. , Kendrick , A. , Brown , H. D. , Bush , G. , Breiter , H. C. , and Rosen , B. R. ( 1997 ) . Striatal recruitment during an implicit sequence learning task as measured by functional magnetic resonance imaging . Hum . Brain Mapp . Rodriguez - Fornells , A. , Cunillera , T. , Mestres - Misse , A. , and De Diego - Balaguer , R. ( 2009 ) . Neurophysiological mechanisms involved in language learning in adults . Philos . Trans . R. Soc . Lond . B Biol . Sci . Shohamy , D. , and Adcock , R. A. ( 2010 ) . Dopamine and adaptive memory . Trends Cogn . Sci . ( Regul . Ed . ) Shohamy , D. , Myers , C. E. , Hopkins , R. O. , Sage , J. , and Gluck , M. A. ( 2009 ) . Distinct hippocampal and basal ganglia contributions to probabilistic learning and reversal . J. Cogn . Neurosci . Slevc , L. R. , Rosenberg , J. C. , and Patel , A. D. ( 2009 ) . Making psycholinguistics musical : self - paced reading time evidence for shared processing of linguistic and musical syntax . Psychon . Bull . Rev. 16 , 374 - 381 . Squire , L. R. , and Knowlton , B. J. ( 2000 ) . \" The medial temporal lobe , the hippocampus , and the memory systems of the brain , \" in The New Cognitive Neurosciences , ed . M. S. Gazzaniga ( Cambridge , MA : MIT Press ) , 765 - 780 . Steriade , M. , Jones , E. G. , and Mccormick , D. A. ( 1997 ) . Thalamus . Amsterdam : Elsevier . Szpunar , K. K. , Schellenberg , E. G. , and Pliner , P. ( 2004 ) . Liking and memory for musical stimuli as a function of exposure . J. Exp . Psychol . Learn . Mem . Cogn . Vakil , E. , Kahan , S. , Huberman , M. , and Osimani , A. ( 2000 ) . Motor and non - motor sequence learning in patients with basal ganglia lesions : the case of serial reaction time ( SRT ) . Neuropsychologia 38 , 1 - 10 . Van Berkum , J. ( 2008 ) . Understanding sentences in context : what brain waves can tell us . Curr . Dir . Psychol . Sci . Wong , P. C. M. , Chan , A. H. D. , Roy , A. , and Margulis , E. H. ( in press ) . The bimusical brain is not two monomusical brains in one : evidence from musical affective processing . J. Cogn . Neurosci . 10.1162/jocn_a_00105 . [ Epub ahead of print]. Copyright : © 2011 Ettlinger , Margulis and Wong . This is an open - access article subject to a non - exclusive license between the authors and Frontiers Media SA , which permits use , distribution and reproduction in other forums , provided the original authors and source are credited and other Frontiers conditions are complied with . e - mail : pwong@northwestern.edu Tools . by Ted Briscoe , John Carroll - In Proceedings of the 5th ACL Conference on Applied Natural Language Processing , 1997 . \" ... We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora . Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English . An initial experiment , on a sample of 14 verb ... \" . We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora . Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English . An initial experiment , on a sample of 14 verbs which exhibit multiple complementation patterns , demonstrates that the technique achieves accuracy comparable to previous approaches , which are all limited to a highly restricted set of subcategorization classes . We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1 . ... rs ) . It is ' shallow ' in that no atof which thetempt is made to fully analyse unbounded dependencies . Furthe ... . \" ... We present a critical overview of the state - of - the - art in parser evaluation methodologies and metrics . A discussion of their relative strengths and weaknesses motivates a new --- and we claim more informative and generally applicable --- technique of measuring parser accuracy , based on the use of gramma ... \" . We present a critical overview of the state - of - the - art in parser evaluation methodologies and metrics . A discussion of their relative strengths and weaknesses motivates a new --- and we claim more informative and generally applicable --- technique of measuring parser accuracy , based on the use of grammatical relations . We conclude with some preliminary results of experiments in which we use this new scheme to evaluate a robust parser of English . \" ... Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages , domains and over time . Yet access to a comprehensive subcategorization lexicon is vital for successful parsing capable of recovering predicate - argument relations , and pr ... \" . Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages , domains and over time . Yet access to a comprehensive subcategorization lexicon is vital for successful parsing capable of recovering predicate - argument relations , and probabilistic parsers would greatly benefit from accurate information concerning the relative likelihood of different subcategorisation frames ( scfs ) of a given predicate . Acquisition of subcategorization lexicons from textual corpora has recently become increasingly popular . Although this work has met with some success , resulting lexicons indicate a need for greater accuracy . One significant source of error lies in the statistical filtering used for hypothesis selection , i.e. for removing noise from automatically acquired scfs . This thesis builds on earlier work in verbal subcategorization acquisition , taking as a starting point the problem with statistical filtering . Our investigation shows that statistical filters tend to work poorly because not only is the underlying distribution zipfian , but there is also very little correlation between conditional distribution of . by Michael A. Covington - In Proceedings of the 39th Annual ACM Southeast Conference , 2001 . \" ... Abstract - This paper presents a fundamental algorithm for parsing natural language sentences into dependency trees . Unlike phrase - structure ( constituency ) parsers , this algorithm operates one word at a time , attaching each word as soon as it can be attached , corresponding to properties claimed for ... \" . Abstract - This paper presents a fundamental algorithm for parsing natural language sentences into dependency trees . Unlike phrase - structure ( constituency ) parsers , this algorithm operates one word at a time , attaching each word as soon as it can be attached , corresponding to properties claimed for the parser in the human brain . Like phrasestructure parsing , its worst - case complexity is O(n 3 ) , but in human language , the worst case occurs only for small n. 1 Overview . This paper develops , from first principles , several variations on a fundamental algorithm for parsing natural language into dependency trees . This is an exposition of an algorithm that has been known , in some form , since the 1960s but is not presented systematically in the extant literature . Unlike phrase - structure ( constituency ) parsers , this algorithm operates one word at a time , attaching each word as soon as it can be attached . There is good evidence that the parsing process used by the human mind has these properties [ 1]. 2 Dependency grammar . 2.1 The key concept . There are two ways to describe sentence structure in natural language : by breaking up the sentence into constituents ( phrases ) , which are then broken into smaller constituents ( Fig . 1 ) , or by drawing links connecting . ... tuency grammar , NP and VP are atomic symbols not related to N and V , a fact all too seldom appreciated . ) Thus , constituency grammar as currently practiced is very close to being a notational variant of dependency grammar . Figure 4 shows interconversion of dependency and constituency trees . A bar over a .. s , adjectives and nouns ) by including separate rules for each pattern of possible complementation in English . Although ... . \" ... We describe an implemented system for robust domain - independent syntactic parsing of English , using a unification - based grammar of part - ofspeech and punctuation labels coupled with a probabilistic LR parser . We present evaluations of the system 's performance along several different dimensions ; ... \" . We describe an implemented system for robust domain - independent syntactic parsing of English , using a unification - based grammar of part - ofspeech and punctuation labels coupled with a probabilistic LR parser . We present evaluations of the system 's performance along several different dimensions ; these enable us to assess the contribution that each individual part is making to the success of the system as a whole , and thus prioririse the effort to be devoted to its further enhancement . Currently , the system is able to parse around 80 % of sentences in a substantial corpus of general text containing a number of distinct genres . On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83 % and 84 % respectively when evaluated against manually - disambiguated analyses . . .. bs , adjectives and nouns ) by including separate rules for each pattern of possible complementation in English . \" ... In this article I present a series of arguments that syntactic structures are built incrementally , in a strict left - to - right order . By assuming incremental structure building it becomes possible to explain the differences between the range of constituents available to different diagnostics of c ... \" . In this article I present a series of arguments that syntactic structures are built incrementally , in a strict left - to - right order . By assuming incremental structure building it becomes possible to explain the differences between the range of constituents available to different diagnostics of constituency , including movement , ellipsis , coordination , scope and binding . In an incremental derivation structure building creates new constituents , and in doing so may destroy existing constituents . The article presents detailed evidence for the prediction of incremental grammar , that a syntactic process may refer to only those constituents that are present at the point in the derivation when the process applies . Keywords : phrase structure , constituency , incrementality , coordination , binding , scope , ellipsis , movement . Introduction Tests of constituency are basic components of the syntactician 's toolbox . By investigating which strings of words can and can not be moved , deleted ... . In order to ach ... . by Sabine Buchholz - In Proceedings of the ESSLLI-98 Workshop on Automated Acquisition of Syntax and Parsing , 1998 . \" ... The automatic distinction between complements and adjuncts , i.e. between subcategorized and non - subcategorized constituents , is crucial for the automatic acquisition of subcategorization lexicons from corpora . In this paper we present memory - based learning experiments for the task of distinguishing ... \" . The automatic distinction between complements and adjuncts , i.e. between subcategorized and non - subcategorized constituents , is crucial for the automatic acquisition of subcategorization lexicons from corpora . In this paper we present memory - based learning experiments for the task of distinguishing complements from adjuncts . Data is extracted from the part - of - speech tagged and parsed version of the Wall Street Journal Corpus . Memory - based learning algorithms classify test instances by using the class of the most similar training instance . By providing the algorithm with different subsets of features in the data , we can explore the importance of different features . By using only syntactic information about the category itself and its neighboring constituents , we achieve an accuracy of 91.6 % for the complement adjunct distinction , which corresponds to 89.7 % correctly classified subcategorization frames . The error analysis shows that whereas at the level of constituents , PPs are most diff ... . ... formation . These contain about 50,000 sentences , totalling more than a million words . Thus complements are NPs without any func ... . \" ... We describe a recently developed corpus annotation scheme for evaluating parsers that avoids some of the shortcomings of current methods . The scheme encodes grammatical relations between heads and dependents , and has been used to mark up a new public - domain corpus of naturally occurring English text ... \" . We describe a recently developed corpus annotation scheme for evaluating parsers that avoids some of the shortcomings of current methods . The scheme encodes grammatical relations between heads and dependents , and has been used to mark up a new public - domain corpus of naturally occurring English text . We show how the corpus can be used to evaluate the accuracy of a robust parser , and relate the corpus to extant resources . by Susan P. Thompson , William Smith Colleges , Elissa L. Newport , 2007 . Research on word se ... \" . Research on word segmentation has shown that learners can use transitional probabilities between syllables to segment speech into word - like units ( Saffran , Aslin , & Newport , 1996 ) . In the present research , we combine and extend these two sets of findings , asking whether learners can use transitional probabilities between words ( or word classes ) to segment sentences into phrases , and use this phrasal information to fully acquire the syntax of a miniature language . Adult subjects were exposed to sentences from a miniature language . A pattern in the transitional probabilities between words - high within phrases , low at phrase boundaries - was created by adding syntactic properties that are widespread in natural languages : optional phrases , repeated phrases , moved phrases , different - sized form classes , or all four properties combined . All conditions outperformed controls in learning the language . The best learning occurred with all properties combined , despite the fact that this language was the most complex . These data address the important question of how language learning is successful in the face of the massive complexity of natural languages . In our experiments , learning got better , not worse , when properly structured complexity was added to a language . The results also show that the same type of statistical computation useful in word segmentation might be used as well in learning syntax , suggesting that the range of statistics needed for acquiring various types of structure in natural languages might be suitably small . Correspondence should be addressed to Susan P. Thompson , Department of Psychology , 205 . Morgan and Newport ( 1981 ) showed that the reference world of Moeser and Bregman ( 1972 ) was successful in facilitating the acquisition of complex aspects of syntax because it served to demarcate the ... Publications : Learning for Semantic Parsing . Semantic parsing is the process of mapping a natural - language sentence into a formal representation of its meaning . A shallow form of semantic representation is a case - role analysis ( a.k.a . a semantic role labeling ) , which identifies roles such as agent , patient , source , and destination . A deeper semantic analysis provides a representation of the sentence in predicate logic or other formal language which supports automated reasoning . We have developed methods for automatically learning semantic parsers from annotated corpora using inductive logic programming and other learning methods . We have explored learning semantic parsers for mapping natural - language sentences to case - role analyses , formal database queries , and formal command languages ( i.e. the Robocup coaching language for use in advice - taking learners ) . We have also explored methods for learning semantic lexicons , i.e. databases of words or phrases paired with one or more alternative formal meaning representations . Semantic lexicons can also be learned from semantically annotated sentences and are an important source of knowledge for semantic parsing . Learning for semantic parsing is part of our research on natural language learning . \" The fish trap exists because of the fish . Once you 've gotten the fish you can forget the trap . The rabbit snare exists because of the rabbit . Once you 've gotten the rabbit , you can forget the snare . Words exist because of meaning . Once you 've gotten the meaning , you can forget the words . Where can I find a man who has forgotten words so I can talk with him ? \" -- The Writings of Chuang Tzu , 4th century B.C. ( Original text in Chinese ) . Demos of learned natural - language database interfaces : . Tutorial on semantic parsing presented at ACL 2010 : . Using natural language to write programs is a touchstone problem for computational linguistics . We present an approach that learns to map natural - language descriptions of simple \" if - then \" rules to executable code . By training and testing on a large corpus of naturally - occurring programs ( called \" recipes \" ) and their natural language descriptions , we demonstrate the ability to effectively map language to code . We compare a number of semantic parsing approaches on the highly noisy training data collected from ordinary users , and find that loosely synchronous systems perform best . ML ID : 317 . Intelligent robots frequently need to understand requests from naive users through natural language . Previous approaches either can not account for language variation , e.g. , keyword search , or require gathering large annotated corpora , which can be expensive and can not adapt to new variation . We introduce a dialog agent for mobile robots that understands human instructions through semantic parsing , actively resolves ambiguities using a dialog manager , and incrementally learns from human - robot conversations by inducing training data from user paraphrases . Our dialog agent is implemented and tested both on a web interface with hundreds of users via Mechanical Turk and on a mobile robot over several days , tasked with understanding navigation and delivery requests through natural language in an office environment . In both contexts , We observe significant improvements in user satisfaction after learning from conversations . ML ID : 314 . Semantic Parsing using Distributional Semantics and Probabilistic Logic [ Details ] [ PDF ] [ Poster ] Islam Beltagy and Katrin Erk and Raymond Mooney In Proceedings of ACL 2014 Workshop on Semantic Parsing ( SP-2014 ) , 7 - -11 , Baltimore , MD , June 2014 . We propose a new approach to semantic parsing that is not constrained by a fixed formal ontology and purely logical inference . Instead , we use distributional semantics to generate only the relevant part of an on - the - fly ontology . Sentences and the on - the - fly ontology are represented in probabilistic logic . For inference , we use probabilistic logic frameworks like Markov Logic Networks ( MLN ) and Probabilistic Soft Logic ( PSL ) . This semantic parsing approach is evaluated on two tasks , Textual Entitlement ( RTE ) and Textual Similarity ( STS ) , both accomplished using inference in probabilistic logic . Experiments show the potential of the approach . ML ID : 301 . Grounded Language Learning Models for Ambiguous Supervision [ Details ] [ PDF ] [ Slides ] Joo Hyun Kim PhD Thesis , Department of Computer Science , University of Texas at Austin , December 2013 . Communicating with natural language interfaces is a long - standing , ultimate goal for artificial intelligence ( AI ) agents to pursue , eventually . One core issue toward this goal is \" grounded \" language learning , a process of learning the semantics of natural language with respect to relevant perceptual inputs . In order to ground the meanings of language in a real world situation , computational systems are trained with data in the form of natural language sentences paired with relevant but ambiguous perceptual contexts . With such ambiguous supervision , it is required to resolve the ambiguity between a natural language ( NL ) sentence and a corresponding set of possible logical meaning representations ( MR ) . In this thesis , we focus on devising effective models for simultaneously disambiguating such supervision and learning the underlying semantics of language to map NL sentences into proper logical MRs . We present probabilistic generative models for learning such correspondences along with a reranking model to improve the performance further . We perform evaluations on the RoboCup sportscasting corpus , proving that our model is more effective than those proposed by previous researchers . Next , we describe two PCFG induction models for grounded language learning that extend the previous grounded language learning model of Borschinger , Jones , and Johnson ( 2011 ) . Borschinger et al . 's approach works well in situations of limited ambiguity , such as in the sportscasting task . However , it does not scale well to highly ambiguous situations when there are large sets of potential meaning possibilities for each sentence , such as in the navigation instruction following task first studied by Chen and Mooney ( 2011 ) . The two models we present overcome such limitations by employing a learned semantic lexicon as a basic correspondence unit between NL and MR for PCFG rule generation . Finally , we present a method of adapting discriminative reranking to grounded language learning in order to improve the performance of our proposed generative models . Although such generative models are easy to implement and are intuitive , it is not always the case that generative models perform best , since they are maximizing the joint probability of data and model , rather than directly maximizing conditional probability . Because we do not have gold - standard references for training a secondary conditional reranker , we incorporate weak supervision of evaluations against the perceptual world during the process of improving model performance . All these approaches are evaluated on the two publicly available domains that have been actively used in many other grounded language learning studies . Our methods demonstrate consistently improved performance over those of previous studies in the domains with different languages ; this proves that our methods are language - independent and can be generally applied to other grounded learning problems as well . Further possible applications of the presented approaches include summarized machine translation tasks and learning from real perception data assisted by computer vision and robotics . ML ID : 291 . Adapting Discriminative Reranking to Grounded Language Learning [ Details ] [ PDF ] [ Slides ] Joohyun Kim and Raymond J. Mooney In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL-2013 ) , 218 - -227 , Sofia , Bulgaria , August 2013 . We adapt discriminative reranking to improve the performance of grounded language acquisition , specifically the task of learning to follow navigation instructions from observation . Unlike conventional reranking used in syntactic and semantic parsing , gold - standard reference trees are not naturally available in a grounded setting . Instead , we show how the weak supervision of response feedback ( e.g. successful task completion ) can be used as an alternative , experimentally demonstrating that its performance is comparable to training on gold - standard parse trees . ML ID : 286 . Generative Models of Grounded Language Learning with Ambiguous Supervision [ Details ] [ PDF ] [ Slides ] Joohyun Kim Technical Report , PhD proposal , Department of Computer Science , The University of Texas at Austin , June 2012 . \" Grounded \" language learning is the process of learning the semantics of natural language with respect to relevant perceptual inputs . Toward this goal , computational systems are trained with data in the form of natural language sentences paired with relevant but ambiguous perceptual contexts . With such ambiguous supervision , it is required to resolve the ambiguity between a natural language ( NL ) sentence and a corresponding set of possible logical meaning representations ( MR ) . My research focuses on devising effective models for simultaneously disambiguating such supervision and learning the underlying semantics of language to map NL sentences into proper logical forms . Specifically , I will present two probabilistic generative models for learning such correspondences . The models are applied to two publicly available datasets in two different domains , sportscasting and navigation , and compared with previous work on the same data . Evaluations are performed on the RoboCup sportscasting corpous , which show that it outperforms previous methods . Next , I present a PCFG induction model for grounded language learning that extends the model of Borschinger , Jones , and Johnson ( 2011 ) by utilizing a semantic lexicon . Our model overcomes such limitations by employing a semantic lexicon as the basic building block for PCFG rule generation . Our model also allows for novel combination of MR outputs when parsing novel test sentences . For future work , I propose to extend our PCFG induction model in several ways : improving the lexicon learning algorithm , discriminative re - ranking of top - k parses , and integrating the meaning representation language ( MRL ) grammar for extra structural information . The longer - term agenda includes applying our approach to summarized machine translation , using real perception data such as robot sensorimeter and images / videos , and joint learning with other natural language processing tasks . ML ID : 273 . \" Grounded \" language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts . Borschinger et al . ( 2011 ) introduced an approach to grounded language learning based on unsupervised PCFG induction . Their approach works well when each sentence potentially refers to one of a small set of possible meanings , such as in the sportscasting task . However , it does not scale to problems with a large set of potential meanings for each sentence , such as the navigation instruction following task studied by Chen and Mooney ( 2011 ) . This paper presents an enhancement of the PCFG approach that scales to such problems with highly - ambiguous supervision . Experimental results on the navigation task demonstrates the effectiveness of our approach . ML ID : 272 . Learning Language from Ambiguous Perceptual Context [ Details ] [ PDF ] [ Slides ] David L. Chen PhD Thesis , Department of Computer Science , University of Texas at Austin , May 2012 . Building a computer system that can understand human languages has been one of the long - standing goals of artificial intelligence . Currently , most state - of - the - art natural language processing ( NLP ) systems use statistical machine learning methods to extract linguistic knowledge from large , annotated corpora . However , constructing such corpora can be expensive and time - consuming due to the expertise it requires to annotate such data . In this thesis , we explore alternative ways of learning which do not rely on direct human supervision . In particular , we draw our inspirations from the fact that humans are able to learn language through exposure to linguistic inputs in the context of a rich , relevant , perceptual environment . We first present a system that learned to sportscast for RoboCup simulation games by observing how humans commentate a game . Using the simple assumption that people generally talk about events that have just occurred , we pair each textual comment with a set of events that it could be referring to . By applying an EM - like algorithm , the system simultaneously learns a grounded language model and aligns each description to the corresponding event . The system does not use any prior language knowledge and was able to learn to sportscast in both English and Korean . Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans . For the sportscasting task , while each comment could be aligned to one of several events , the level of ambiguity was low enough that we could enumerate all the possible alignments . However , it is not always possible to restrict the set of possible alignments to such limited numbers . Thus , we present another system that allows each sentence to be aligned to one of exponentially many connected subgraphs without explicitly enumerating them . The system first learns a lexicon and uses it to prune the nodes in the graph that are unrelated to the words in the sentence . By only observing how humans follow navigation instructions , the system was able to infer the corresponding hidden navigation plans and parse previously unseen instructions in new environments for both English and Chinese data . With the rise in popularity of crowdsourcing , we also present results on collecting additional training data using Amazon 's Mechanical Turk . Since our system only needs supervision in the form of language being used in relevant contexts , it is easy for virtually anyone to contribute to the training data . ML ID : 269 . Learning to Interpret Natural Language Navigation Instructions from Observations [ Details ] [ PDF ] [ Slides ] David L. Chen and Raymond J. Mooney In Proceedings of the 25th AAAI Conference on Artificial Intelligence ( AAAI-2011 ) , 859 - 865 , August 2011 . The ability to understand natural - language instructions is critical to building intelligent agents that interact with humans . We present a system that learns to transform natural - language navigation instructions into executable formal plans . Given no prior linguistic knowledge , the system learns by simply observing how humans follow navigation instructions . The system is evaluated in three complex virtual indoor environments with numerous objects and landmarks . A previously collected realistic corpus of complex English navigation instructions for these environments is used for training and testing data . By using a learned lexicon to refine inferred plans and a supervised learner to induce a semantic parser , the system is able to automatically learn to correctly interpret a reasonable fraction of the complex instructions in this corpus . ML ID : 264 . Generative Alignment and Semantic Parsing for Learning from Ambiguous Supervision [ Details ] [ PDF ] Joohyun Kim and Raymond J. Mooney In Proceedings of the 23rd International Conference on Computational Linguistics ( COLING 2010 ) , 543 - -551 , Beijing , China , August 2010 . We present a probabilistic generative model for learning semantic parsers from ambiguous supervision . Our approach learns from natural language sentences paired with world states consisting of multiple potential logical meaning representations . It disambiguates the meaning of each sentence while simultaneously learning a semantic parser that maps sentences into logical form . Compared to a previous generative model for semantic alignment , it also supports full semantic parsing . Experimental results on the Robocup sportscasting corpora in both English and Korean indicate that our approach produces more accurate semantic alignments than existing methods and also produces competitive semantic parsers and improved language generators . ML ID : 251 . Learning for Semantic Parsing Using Statistical Syntactic Parsing Techniques [ Details ] [ PDF ] [ Slides ] Ruifang Ge PhD Thesis , Department of Computer Science , University of Texas at Austin , Austin , TX , May 2010 . 165 pages . Natural language understanding is a sub - field of natural language processing , which builds automated systems to understand natural language . It is such an ambitious task that it sometimes is referred to as an AI - complete problem , implying that its difficulty is equivalent to solving the central artificial intelligence problem -- making computers as intelligent as people . Despite its complexity , natural language understanding continues to be a fundamental problem in natural language processing in terms of its theoretical and empirical importance . In recent years , startling progress has been made at different levels of natural language processing tasks , which provides great opportunity for deeper natural language understanding . In this thesis , we focus on the task of semantic parsing , which maps a natural language sentence into a complete , formal meaning representation in a meaning representation language . We present two novel state - of - the - art learned syntax - based semantic parsers using statistical syntactic parsing techniques , motivated by the following two reasons . First , the syntax - based semantic parsing is theoretically well - founded in computational semantics . Second , adopting a syntax - based approach allows us to directly leverage the enormous progress made in statistical syntactic parsing . The first semantic parser , SCISSOR , adopts an integrated syntactic - semantic parsing approach , in which a statistical syntactic parser is augmented with semantic parameters to produce a semantically - augmented parse tree ( SAPT ) . This integrated approach allows both syntactic and semantic information to be available during parsing time to obtain an accurate combined syntactic - semantic analysis . The performance of SCISSOR is further improved by using discriminative reranking for incorporating non - local features . The second semantic parser , SYNSEM , exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation . This pipeline approach allows semantic parsing to conveniently leverage the most recent progress in statistical syntactic parsing . SYNSEM also significantly improves results with limited training data , and is shown to be robust to syntactic errors . ML ID : 246 . Training a Multilingual Sportscaster : Using Perceptual Context to Learn Language [ Details ] [ PDF ] David L. Chen , Joohyun Kim , Raymond J. Mooney Journal of Artificial Intelligence Research , 37:397 - -435 , 2010 . We present a novel framework for learning to interpret and generate language using only perceptual context as supervision . We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language - specific prior knowledge . Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace . The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation . We also present a novel algorithm for learning which events are worth describing . Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain . ML ID : 240 . Learning Language from Perceptual Context [ Details ] [ PDF ] [ Slides ] David L. Chen December 2009 . Ph.D. proposal , Department of Computer Sciences , University of Texas at Austin . Most current natural language processing ( NLP ) systems are built using statistical learning algorithms trained on large annotated corpora which can be expensive and time - consuming to collect . In contrast , humans can learn language through exposure to linguistic input in the context of a rich , relevant , perceptual environment . I will first present a system we completed that can describe events in RoboCup 2D simulation games by learning only from sample language commentaries paired with traces of simulated activities without any language - specific prior knowledge . By applying an EM - like algorithm , the system was able to simultaneously learn a grounded language model as well as align the ambiguous training data . Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans . For future work , I am proposing to solve the more complex task of learning how to give and receive navigation instructions in a virtual environment . In this setting , each instruction corresponds to a navigation plan that is not directly observable . Since an exponential number of plans can all lead to the same observed actions , we have to learn from compact representations of all valid plans rather than enumerating all possible meanings as we did in the sportscasting task . Initially , the system will passively observe a human giving instruction to another human , and try to learn the correspondences between the instructions and the intended plan . After the system has a decent understanding of the language , it can then participate in the interactions to learn more directly by playing either the role of the instructor or the follower . ML ID : 239 . We present a new approach to learning a semantic parser ( a system that maps natural language sentences into logical form ) . Unlike previous methods , it exploits an existing syntactic parser to produce disambiguated parse trees that drive the compositional semantic interpretation . The resulting system produces improved results on standard corpora on natural language interfaces for database querying and simulated robot control . ML ID : 229 . A Dependency - based Word Subsequence Kernel [ Details ] [ PDF ] Rohit J. Kate In Proceedings of the conference on Empirical Methods in Natural Language Processing ( EMNLP-2008 ) , 400 - -409 , Waikiki , Honolulu , Hawaii , October 2008 . This paper introduces a new kernel which computes similarity between two natural language sentences as the number of paths shared by their dependency trees . The paper gives a very efficient algorithm to compute it . This kernel is also an improvement over the word subsequence kernel because it only counts linguistically meaningful word subsequences which are based on word dependencies . It overcomes some of the difficulties encountered by syntactic tree kernels as well . Experimental results demonstrate the advantage of this kernel over word subsequence and syntactic tree kernels . ML ID : 223 . Transforming Meaning Representation Grammars to Improve Semantic Parsing [ Details ] [ PDF ] Rohit J. Kate In Proceedings of the Twelfth Conference on Computational Natural Language Learning ( CoNLL-2008 ) , 33 - -40 , Manchester , UK , August 2008 . A semantic parser learning system learns to map natural language sentences into their domain - specific formal meaning representations , but if the constructs of the meaning representation language do not correspond well with the natural language then the system may not learn a good semantic parser . This paper presents approaches for automatically transforming a meaning representation grammar ( MRG ) to conform it better with the natural language semantics . It introduces grammar transformation operators and meaning representation macros which are applied in an error - driven manner to transform an MRG while training a semantic parser learning system . Experimental results show that the automatically transformed MRGs lead to better learned semantic parsers which perform comparable to the semantic parsers learned using manually engineered MRGs . ML ID : 222 . Learning to Sportscast : A Test of Grounded Language Acquisition [ Details ] [ PDF ] [ Slides ] [ Video ] David L. Chen and Raymond J. Mooney In Proceedings of the 25th International Conference on Machine Learning ( ICML ) , Helsinki , Finland , July 2008 . We present a novel commentator system that learns language from sportscasts of simulated soccer games . The system learns to parse and generate commentaries without any engineered knowledge about the English language . Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games . The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model . We also present a novel algorithm , Iterative Generation Strategy Learning ( IGSL ) , for deciding which events to comment on . Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries . ML ID : 219 . Learning for Semantic Parsing with Kernels under Various Forms of Supervision [ Details ] [ PDF ] [ Slides ] Rohit J. Kate PhD Thesis , Department of Computer Sciences , University of Texas at Austin , Austin , TX , August 2007 . 159 pages . Semantic parsing involves deep semantic analysis that maps natural language sentences to their formal executable meaning representations . This is a challenging problem and is critical for developing computing systems that understand natural language input . This thesis presents a new machine learning approach for semantic parsing based on string - kernel - based classification . It takes natural language sentences paired with their formal meaning representations as training data . For every production in the formal language grammar , a Support - Vector Machine ( SVM ) classifier is trained using string similarity as the kernel . Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these classifiers . This method does not use any hard - matching rules and unlike previous and other recent methods , does not use grammar rules for natural language , probabilistic or otherwise , which makes it more robust to noisy input . Besides being robust , this approach is also flexible and able to learn under a wide range of supervision , from extra to weaker forms of supervision . It can easily utilize extra supervision given in the form of syntactic parse trees for natural language sentences by using a syntactic tree kernel instead of a string kernel . Its learning algorithm can also take advantage of detailed supervision provided in the form of semantically augmented parse trees . A simple extension using transductive SVMs enables the system to do semi - supervised learning and improve its performance utilizing unannotated sentences which are usually easily available . Another extension involving EM - like retraining makes the system capable of learning under ambiguous supervision in which the correct meaning representation for each sentence is not explicitly given , but instead a set of possible meaning representations is given . This weaker and more general form of supervision is better representative of a natural training environment for a language - learning system requiring minimal human supervision . For a semantic parser to work well , conformity between natural language and meaning representation grammar is necessary . However meaning representation grammars are typically designed to best suit the application which will use the meaning representations with little consideration for how well they correspond to natural language semantics . We present approaches to automatically transform meaning representation grammars to make them more compatible with natural language semantics and hence more suitable for learning semantic parsers . Finally , we also show that ensembles of different semantic parser learning systems can obtain the best overall performance . ML ID : 215 . Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques [ Details ] [ PDF ] Yuk Wah Wong PhD Thesis , Department of Computer Sciences , University of Texas at Austin , Austin , TX , August 2007 . 188 pages . Also appears as Technical Report AI07 - 343 , Artificial Intelligence Lab , University of Texas at Austin , August 2007 . One of the main goals of natural language processing ( NLP ) is to build automated systems that can understand and generate human languages . This goal has so far remained elusive . Existing hand - crafted systems can provide in - depth analysis of domain sub - languages , but are often notoriously fragile and costly to build . Existing machine - learned systems are considerably more robust , but are limited to relatively shallow NLP tasks . In this thesis , we present novel statistical methods for robust natural language understanding and generation . We focus on two important sub - tasks , semantic parsing and tactical generation . The key idea is that both tasks can be treated as the translation between natural languages and formal meaning representation languages , and therefore , can be performed using state - of - the - art statistical machine translation techniques . Specifically , we use a technique called synchronous parsing , which has been extensively used in syntax - based machine translation , as the unifying framework for semantic parsing and tactical generation . The parsing and generation algorithms learn all of their linguistic knowledge from annotated corpora , and can handle natural - language sentences that are conceptually complex . A nice feature of our algorithms is that the semantic parsers and tactical generators share the same learned synchronous grammars . Moreover , charts are used as the unifying language - processing architecture for efficient parsing and generation . Therefore , the generators are said to be the inverse of the parsers , an elegant property that has been widely advocated . Furthermore , we show that our parsers and generators can handle formal meaning representation languages containing logical variables , including predicate logic . Our basic semantic parsing algorithm is called WASP . Most of the other parsing and generation algorithms presented in this thesis are extensions of WASP or its inverse . We demonstrate the effectiveness of our parsing and generation algorithms by performing experiments in two real - world , restricted domains . Experimental results show that our algorithms are more robust and accurate than the currently best systems that require similar supervision . Our work is also the first attempt to use the same automatically - learned grammar for both parsing and generation . Unlike previous systems that require manually - constructed grammars and lexicons , our systems require much less knowledge engineering and can be easily ported to other languages and domains . ML ID : 214 . Learning Language Semantics from Ambiguous Supervision [ Details ] [ PDF ] Rohit J. Kate and Raymond J. Mooney In Proceedings of the 22nd Conference on Artificial Intelligence ( AAAI-07 ) , 895 - 900 , Vancouver , Canada , July 2007 . This paper presents a method for learning a semantic parser from ambiguous supervision . Training data consists of natural language sentences annotated with multiple potential meaning representations , only one of which is correct . Such ambiguous supervision models the type of supervision that can be more naturally available to language - learning systems . Given such weak supervision , our approach produces a semantic parser that maps sentences into meaning representations . An existing semantic parsing learning system that can only learn from unambiguous supervision is augmented to handle ambiguous supervision . Experimental results show that the resulting system is able to cope up with ambiguities and learn accurate semantic parsers . ML ID : 200 . Learning Synchronous Grammars for Semantic Parsing with Lambda Calculus [ Details ] [ PDF ] Yuk Wah Wong and Raymond J. Mooney In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ( ACL-2007 ) , Prague , Czech Republic , June 2007 . This paper presents the first empirical results to our knowledge on learning synchronous grammars that generate logical forms . Using statistical machine translation techniques , a semantic parser based on a synchronous context - free grammar augmented with lambda - operators is learned given a set of training sentences and their correct logical forms . The resulting parser is shown to be the best - performing system so far in a database query domain . ML ID : 199 . We present a method for utilizing unannotated sentences to improve a semantic parser which maps natural language ( NL ) sentences into their formal meaning representations ( MRs ) . Given NL sentences annotated with their MRs , the initial supervised semantic parser learns the mapping by training Support Vector Machine ( SVM ) classifiers for every production in the MR grammar . Our new method applies the learned semantic parser to the unannotated sentences and collects unlabeled examples which are then used to retrain the classifiers using a variant of transductive SVMs . Experimental results show the improvements obtained over the purely supervised parser , particularly when the annotated training set is small . ML ID : 198 . This paper explores the use of statistical machine translation ( SMT ) methods for tactical natural language generation . We present results on using phrase - based SMT for learning to map meaning representations to natural language . Improved results are obtained by inverting a semantic parser that uses SMT methods to map sentences into meaning representations . Finally , we show that hybridizing these two approaches results in still more accurate generation systems . Automatic and human evaluation of generated sentences are presented across two domains and four languages . ML ID : 197 . Learning for Semantic Parsing [ Details ] [ PDF ] Raymond J. Mooney In A. Gelbukh , editors , Computational Linguistics and Intelligent Text Processing : Proceedings of the 8th International Conference ( CICLing 2007 ) , 311 - -324 , Mexico City , Mexico , February 2007 . Springer : Berlin , Germany . Invited paper . Semantic parsing is the task of mapping a natural language sentence into a complete , formal meaning representation . Over the past decade , we have developed a number of machine learning methods for inducing semantic parsers by training on a corpus of sentences paired with their meaning representations in a specified formal language . We have demonstrated these methods on the automated construction of natural - language interfaces to databases and robot command languages . This paper reviews our prior work on this topic and discusses directions for future research . ML ID : 196 . Association for Computational Linguistics . We present a new approach for mapping natural language sentences to their formal meaning representations using string - kernel - based classifiers . Our system learns these classifiers for every production in the formal language grammar . Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these string classifiers . Our experiments on two real - world data sets show that this approach compares favorably to other existing systems and is particularly robust to noise . ML ID : 191 . Discriminative Reranking for Semantic Parsing [ Details ] [ PDF ] Ruifang Ge and Raymond J. Mooney In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics ( COLING / ACL-06 ) , Sydney , Australia , July 2006 . Semantic parsing is the task of mapping natural language sentences to complete formal meaning representations . The performance of semantic parsing can be potentially improved by using discriminative reranking , which explores arbitrary global features . In this paper , we investigate discriminative reranking upon a baseline semantic parser , SCISSOR , where the composition of meaning representations is guided by syntax . We examine if features used for syntactic parsing can be adapted for semantic parsing by creating similar semantic features based on the mapping between syntax and semantics . We report experimental results on two real applications , an interpreter for coaching instructions in robotic soccer and a natural - language database interface . The results show that reranking can improve the performance on the coaching interpreter , but not on the database interface . ML ID : 190 . We present a novel statistical approach to semantic parsing , WASP , for constructing a complete , formal meaning representation of a sentence . A semantic parser is learned given a set of sentences annotated with their correct meaning representations . The main innovation of WASP is its use of state - of - the - art statistical machine translation techniques . A word alignment model is used for lexical acquisition , and the parsing model itself can be seen as a syntax - based translation model . We show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision , and shows better robustness to variations in task complexity and word order . ML ID : 187 . Learning Semantic Parsers Using Statistical Syntactic Parsing Techniques [ Details ] [ PDF ] Ruifang Ge 2006 . Most recent work on semantic analysis of natural language has focused on ' ' shallow ' ' semantics such as word - sense disambiguation and semantic role labeling . Our work addresses a more ambitious task we call semantic parsing where natural language sentences are mapped to complete formal meaning representations . We present our system Scissor based on a statistical parser that generates a semantically - augmented parse tree ( SAPT ) , in which each internal node has both a syntactic and semantic label . A compositional - semantics procedure is then used to map the augmented parse tree into a final meaning representation . Training the system requires sentences annotated with augmented parse trees . We evaluate the system in two domains , a natural - language database interface and an interpreter for coaching instructions in robotic soccer . We present experimental results demonstrating that Scissor produces more accurate semantic representations than several previous approaches on long sentences . In the future , we intend to pursue several directions in developing more accurate semantic parsing algorithms and automating the annotation process . This work will involve exploring alternative tree representations for better generalization in parsing . We also plan to apply discriminative reranking methods to semantic parsing , which allows exploring arbitrary , potentially correlated features not usable by the baseline learner . We also propose to design a method for automating the SAPT - generation process to alleviate the extra annotation work currently required for training Scissor . Finally , we will investigate the impact of different statistical syntactic parsers on semantic parsing using the automated SAPT - generation process . ML ID : 184 . A Kernel - based Approach to Learning Semantic Parsers [ Details ] [ PDF ] [ Slides ] Rohit J. Kate 2005 . Doctoral Dissertation Proposal , University of Texas at Austin . Semantic parsing involves deep semantic analysis that maps natural language sentences to their formal executable meaning representations . This is a challenging problem and is critical for developing user - friendly natural language interfaces to computing systems . Most of the research in natural language understanding , however , has mainly focused on shallow semantic analysis like case - role analysis or word sense disambiguation . The existing work in semantic parsing either lack the robustness of statistical methods or are applicable only to simple domains where semantic analysis is equivalent to filling a single semantic frame . In this proposal , we present a new approach to semantic parsing based on string - kernel - based classification . Our system takes natural language sentences paired with their formal meaning representations as training data . For every production in the formal language grammar , a Support - Vector Machine ( SVM ) classifier is trained using string similarity as the kernel . Each classifier then gives the probability of the production covering any given natural language string of words . These classifiers are further refined using EM - type iterations based on their performance on the training data . Meaning representations for novel natural language sentences are obtained by finding the most probable semantic parse using these classifiers . Our experiments on two real - world data sets that have deep meaning representations show that this approach compares favorably to other existing systems in terms of accuracy and coverage . For future work , we propose to extend this approach so that it will also exploit the knowledge of natural language syntax by using the existing syntactic parsers . We also intend to broaden the scope of application domains , for example , domains where the sentences are noisy as typical in speech , or domains where corpora available for training do not have natural language sentences aligned with their unique meaning representations . We aim to test our system on the task of complex relation extraction as well . Finally , we also plan to investigate ways to combine our semantic parser with some recently developed semantic parsers to form committees in order to get the best overall performance . ML ID : 181 . Learning for Semantic Parsing Using Statistical Machine Translation Techniques [ Details ] [ PDF ] Yuk Wah Wong 2005 . Doctoral Dissertation Proposal , University of Texas at Austin . Semantic parsing is the construction of a complete , formal , symbolic meaning representation of a sentence . While it is crucial to natural language understanding , the problem of semantic parsing has received relatively little attention from the machine learning community . Recent work on natural language understanding has mainly focused on shallow semantic analysis , such as word- sense disambiguation and semantic role labeling . Semantic parsing , on the other hand , involves deep semantic analysis in which word senses , semantic roles and other components are combined to produce useful meaning representations for a particular application domain ( e.g. database query ) . Prior research in machine learning for semantic parsing is mainly based on inductive logic programming or deterministic parsing , which lack some of the robustness that characterizes statistical learning . Existing statistical approaches to semantic parsing , however , are mostly concerned with relatively simple application domains in which a meaning representation is no more than a single semantic frame . In this proposal , we present a novel statistical approach to semantic parsing , WASP , which can handle meaning representations with a nested structure . The WASP algorithm learns a semantic parser given a set of sentences annotated with their correct meaning representations . The parsing model is based on the synchronous context - free grammar , where each rule maps a natural - language substring to its meaning representation . The main innovation of the algorithm is its use of state - of - the - art statistical machine translation techniques . A statistical word alignment model is used for lexical acquisition , and the parsing model itself can be seen as an instance of a syntax - based translation model . In initial evaluation on several real - world data sets , we show that WASP performs favorably in terms of both accuracy and coverage compared to existing learning methods requiring similar amount of supervision , and shows better robustness to variations in task complexity and word order . In future work , we intend to pursue several directions in developing accurate semantic parsers for a variety of application domains . This will involve exploiting prior knowledge about the natural - language syntax and the application domain . We also plan to construct a syntax - aware word - based alignment model for lexical acquisition . Finally , we will generalize the learning algorithm to handle context - dependent sentences and accept noisy training data . ML ID : 180 . A Statistical Semantic Parser that Integrates Syntax and Semantics [ Details ] [ PDF ] Ruifang Ge and Raymond J. Mooney In Proceedings of CoNLL-2005 , Ann Arbor , Michigan , June 2005 . We introduce a learning semantic parser , Scissor , that maps natural - language sentences to a detailed , formal , meaning - representation language . It first uses an integrated statistical parser to produce a semantically augmented parse tree , in which each non - terminal node has both a syntactic and a semantic label . A compositional - semantics procedure is then used to map the augmented parse tree into a final meaning representation . We evaluate the system in two domains , a natural - language database interface and an interpreter for coaching instructions in robotic soccer . We present experimental results demonstrating that Scissor produces more accurate semantic representations than several previous approaches . ML ID : 171 . Learning to Transform Natural to Formal Languages [ Details ] [ PDF ] [ Slides ] Rohit J. Kate , Yuk Wah Wong and Raymond J. Mooney In Proceedings of the Twentieth National Conference on Artificial Intelligence ( AAAI-05 ) , 1062 - 1068 , Pittsburgh , PA , July 2005 . This paper presents a method for inducing transformation rules that map natural - language sentences into a formal query or command language . The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non - terminal symbols in this grammar . The learned transformation rules incrementally map a natural - language sentence or its syntactic parse tree into a parse - tree for the target formal language . Experimental results are presented for two corpora , one which maps English instructions into an existing formal coaching language for simulated RoboCup soccer agents , and another which maps English U.S.-geography questions into a database query language . We show that our method performs overall better and faster than previous approaches in both domains . ML ID : 160 . Learning Transformation Rules for Semantic Parsing [ Details ] [ PDF ] Rohit J. Kate , Yuk Wah Wong , Ruifang Ge , and Raymond J. Mooney April 2004 . Unpublished Technical Report . This paper presents an approach for inducing transformation rules that map natural - language sentences into a formal semantic representation language . The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non - terminal symbols in this grammar . Patterns for the transformation rules are learned using an induction algorithm based on longest - common - subsequences previously developed for an information extraction system . Experimental results are presented on learning to map English coaching instructions for Robocup soccer into an existing formal language for coaching simulated robotic agents . ML ID : 140 . Learning Semantic Parsers : An Important But Under - Studied Problem [ Details ] [ PDF ] Raymond J. Mooney In Papers from the AAAI 2004 Spring Symposium on Language Learning : An Interdisciplinary Perspective , 39 - -44 , Stanford , CA , March 2004 . Computational systems that learn to transform natural - language sentences into semantic representations have important practical applications in building natural - language interfaces . They can also provide insight into important issues in human language acquisition . However , within AI , computational linguistics , and machine learning , there has been relatively little research on developing systems that learn such semantic parsers . This paper briefly reviews our own work in this area and presents semantic - parser acquistion as an important challenge problem for AI . ML ID : 138 . Integrating Top - down and Bottom - up Approaches in Inductive Logic Programming : Applications in Natural Language Processing and Relational Data Mining [ Details ] [ PDF ] Lappoon R. Tang PhD Thesis , Department of Computer Sciences , University of Texas , Austin , TX , August 2003 . Inductive Logic Programming ( ILP ) is the intersection of Machine Learning and Logic Programming in which the learner 's hypothesis space is the set of logic programs . There are two major ILP approaches : top - down and bottom - up . The former searches the hypothesis space from general to specific while the latter the other way round . Integrating both approaches has been demonstrated to be more effective . Integrated ILP systems were previously developed for two tasks : learning semantic parsers ( Chillin ) , and mining relational data ( Progol ) . Two new integrated ILP systems for these tasks that overcome limitations of existing methods will be presented . Cocktail is a new ILP algorithm for inducing semantic parsers . For this task , two features of a parse state , functional structure and context , provide important information for disambiguation . A bottom - up approach is more suitable for learning the former , while top - down is better for the latter . By allowing both approaches to induce program clauses and choosing the best combination of their results , Cocktail learns more effective parsers . Experimental results on learning natural - language interfaces for two databases demonstrate that it learns more accurate parsers than Chillin , the previous best method for this task . Beth is a new integrated ILP algorithm for relational data mining . The Inverse Entailment approach to ILP , implemented in the Progol and Aleph systems , starts with the construction of a bottom clause , the most specific hypothesis covering a seed example . When mining relational data with a large number of background facts , the bottom clause becomes intractably large , making learning very inefficient . A top - down approach heuristically guides the construction of clauses without building a bottom clause ; however , it wastes time exploring clauses that cover no positive examples . By using a top - down approach to heuristically guide the construction of generalizations of a bottom clause , Beth combines the strength of both approaches . Learning patterns for detecting potential terrorist activity is a current challenge problem for relational data mining . Experimental results on artificial data for this task with over half a million facts show that Beth is significantly more efficient at discovering such patterns than Aleph and m - Foil , two leading ILP systems . ML ID : 130 . Acquiring Word - Meaning Mappings for Natural Language Interfaces [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney Journal of Artificial Intelligence Research , 18:1 - 44 , 2003 . This paper focuses on a system , Wolfie ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with semantic representations . The lexicon learned consists of phrases paired with meaning representations . Wolfie is part of an integrated system that learns to parse representations such as logical database queries . Experimental results are presented demonstrating Wolfie 's ability to learn useful lexicons for a database interface in four different natural languages . The usefulness of the lexicons learned by Wolfie are compared to those acquired by a similar system developed by Siskind ( 1996 ) , with results favorable to Wolfie . A second set of experiments demonstrates Wolfie 's ability to scale to larger and more difficult , albeit artificially generated , corpora . In natural language acquisition , it is difficult to gather the annotated data needed for supervised learning ; however , unannotated data is fairly plentiful . Active learning methods ( Cohn , Atlas , & Ladner , 1994 ) attempt to select for annotation and training only the most informative examples , and therefore are potentially very useful in natural language applications . However , most results to date for active learning have only considered standard classification tasks . To reduce annotation effort while maintaining accuracy , we apply active learning to semantic lexicons . We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance . ML ID : 121 . Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing [ Details ] [ PDF ] Lappoon R. Tang and Raymond J. Mooney In Proceedings of the 12th European Conference on Machine Learning , 466 - 477 , Freiburg , Germany , 2001 . In this paper , we explored a learning approach which combines different learning methods in inductive logic programming ( ILP ) to allow a learner to produce more expressive hypothese than that of each individual learner . Such a learning approach may be useful when the performance of the task depends on solving a large amount of classification problems and each has its own characteristics which may or may not fit a particular learning method . The task of sematnic parser acquisition in two different domains was attempted and preliminary results demonstrated that such an approach is promising . ML ID : 107 . The development of natural language interfaces ( NLI 's ) for databases has been a challenging problem in natural language processing ( NLP ) since the 1970 's . The need for NLI 's has become more pronounced due to the widespread access to complex databases now available through the Internet . A challenging problem for empirical NLP is the automated acquisition of NLI 's from training examples . We present a method for integrating statistical and relational learning techniques for this task which exploits the strength of both approaches . Experimental results from three different domains suggest that such an approach is more robust than a previous purely logic - based approach . ML ID : 102 . Integrating Statistical and Relational Learning for Semantic Parsing : Applications to Learning Natural Language Interfaces for Databases [ Details ] [ PDF ] Lappoon R. Tang May 2000 . Ph.D. proposal , Department of Computer Sciences , University of Texas at Austin . The development of natural language interfaces ( NLIs ) for databases has been an interesting problem in natural language processing since the 70 's . The need for NLIs has become more pronounced given the widespread access to complex databases now available through the Internet . However , such systems are difficult to build and must be tailored to each application . A current research topic involves using machine learning methods to automate the development of NLI 's . This proposal presents a method for learning semantic parsers ( systems for mapping natural language to logical form ) that integrates logic - based and probabilistic methods in order to exploit the complementary strengths of these competing approaches . More precisely , an inductive logic programming ( ILP ) method , TABULATE , is developed for learning multiple models that are integrated via linear weighted combination to produce probabilistic models for statistical semantic parsing . Initial experimental results from three different domains suggest that an integration of statistical and logical approaches to semantic parsing can outperform a purely logical approach . Future research will further develop this integrated approach and demonstrate its ability to improve the automated development of NLI 's . ML ID : 99 . Learning for Semantic Interpretation : Scaling Up Without Dumbing Down [ Details ] [ PDF ] Raymond J. Mooney In Workshop Notes for the Workshop on Learning Language in Logic , 7 - 15 , Bled , Slovenia , 2000 . Most recent research in learning approaches to natural language have studied fairly ' ' low - level ' ' tasks such as morphology , part - of - speech tagging , and syntactic parsing . However , I believe that logical approaches may have the most relevance and impact at the level of semantic interpretation , where a logical representation of sentence meaning is important and useful . We have explored the use of inductive logic programming for learning parsers that map natural - language database queries into executable logical form . This work goes against the growing trend in computational linguistics of focusing on shallow but broad - coverage natural language tasks ( ' ' scaling up by dumbing down ' ' ) and instead concerns using logic - based learning to develop narrower , domain - specific systems that perform relatively deep processing . I first present a historical view of the shifting emphasis of research on various tasks in natural language processing and then briefly review our own work on learning for semantic interpretation . I will then attempt to encourage others to study such problems and explain why I believe logical approaches have the most to offer at the level of producing semantic interpretations of complete sentences . ML ID : 93 . Automatic Construction of Semantic Lexicons for Learning Natural Language Interfaces [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney In Proceedings of the Sixteenth National Conference on Artificial Intelligence ( AAAI-99 ) , 487 - 493 , Orlando , FL , July 1999 . This paper describes a system , Wolfie ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with semantic representations . The lexicon learned consists of words paired with meaning representations . Wolfie is part of an integrated system that learns to parse novel sentences into semantic representations , such as logical database queries . Experimental results are presented demonstrating Wolfie 's ability to learn useful lexicons for a database interface in four different natural languages . The lexicons learned by Wolfie are compared to those acquired by a competing system developed by Siskind . ML ID : 95 . Active Learning for Natural Language Parsing and Information Extraction [ Details ] [ PDF ] Cynthia A. Thompson , Mary Elaine Califf and Raymond J. Mooney In Proceedings of the Sixteenth International Conference on Machine Learning ( ICML-99 ) , 406 - 414 , Bled , Slovenia , June 1999 . In natural language acquisition , it is difficult to gather the annotated data needed for supervised learning ; however , unannotated data is fairly plentiful . Active learning methods attempt to select for annotation and training only the most informative examples , and therefore are potentially very useful in natural language applications . However , existing results for active learning have only considered standard classification tasks . To reduce annotation effort while maintaining accuracy , we apply active learning to two non - classification tasks in natural language processing : semantic parsing and information extraction . We show that active learning can significantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks . ML ID : 92 . Semantic Lexicon Acquisition for Learning Natural Language Interfaces [ Details ] [ PDF ] Cynthia Ann Thompson PhD Thesis , Department of Computer Sciences , University of Texas at Austin , Austin , TX , December 1998 . 101 pages . Also appears as Technical Report AI 99 - 278 , Artificial Intelligence Lab , University of Texas at Austin . A long - standing goal for the field of artificial intelligence is to enable computer understanding of human languages . A core requirement in reaching this goal is the ability to transform individual sentences into a form better suited for computer manipulation . This ability , called semantic parsing , requires several knowledge sources , such as a grammar , lexicon , and parsing mechanism . Building natural language parsing systems by hand is a tedious , error - prone undertaking . We build on previous research in automating the construction of such systems using machine learning techniques . The result is a combined system that learns semantic lexicons and semantic parsers from one common set of training examples . The input required is a corpus of sentence / representation pairs , where the representations are in the output format desired . A new system , Wolfie , learns semantic lexicons to be used as background knowledge by a previously developed parser acquisition system , Chill . The combined system is tested on a real world domain of answering database queries . We also compare this combination to a combination of Chill with a previously developed lexicon learner , demonstrating superior performance with our system . In addition , we show the ability of the system to learn to process natural languages other than English . Finally , we test the system on an alternate sentence representation , and on a set of large , artificial corpora with varying levels of ambiguity and synonymy . One difficulty in using machine learning methods for building natural language interfaces is building the required annotated corpus . Therefore , we also address this issue by using active learning to reduce the number of training examples required by both Wolfie and Chill . Experimental results show that the number of examples needed to reach a given level of performance can be significantly reduced with this method . ML ID : 90 . Semantic Lexicon Acquisition for Learning Natural Language Interfaces [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney In Proceedings of the Sixth Workshop on Very Large Corpora , Montreal , Quebec , Canada , August 1998 . Also available as TR AI 98 - 273 , Artificial Intelligence Lab , University of Texas at Austin , May 1998 . This paper describes a system , WOLFIE ( WOrd Learning From Interpreted Examples ) , that acquires a semantic lexicon from a corpus of sentences paired with representations of their meaning . The lexicon learned consists of words paired with meaning representations . WOLFIE is part of an integrated system that learns to parse novel sentences into semantic representations , such as logical database queries . Experimental results are presented demonstrating WOLFIE 's ability to learn useful lexicons for a database interface in four different natural languages . The lexicons learned by WOLFIE are compared to those acquired by a competing system developed by Siskind ( 1996 ) . ML ID : 89 . For most natural language processing tasks , a parser that maps sentences into a semantic representation is significantly more useful than a grammar or automata that simply recognizes syntactically well - formed strings . This paper reviews our work on using inductive logic programming methods to learn deterministic shift - reduce parsers that translate natural language into a semantic representation . We focus on the task of mapping database queries directly into executable logical form . An overview of the system is presented followed by recent experimental results on corpora of Spanish geography queries and English job - search queries . ML ID : 75 . An Inductive Logic Programming Method for Corpus - based Parser Construction [ Details ] [ PDF ] John M. Zelle and Raymond J. Mooney January 1997 . Unpublished Technical Note . Empirical methods for building natural language systems has become an important area of research in recent years . Most current approaches are based on propositional learning algorithms and have been applied to the problem of acquiring broad - coverage parsers for relatively shallow ( syntactic ) representations . This paper outlines an alternative empirical approach based on techniques from a subfield of machine learning known as Inductive Logic Programming ( ILP ) . ILP algorithms , which learn relational ( first - order ) rules , are used in a parser acquisition system called CHILL that learns rules to control the behavior of a traditional shift - reduce parser . Using this approach , CHILL is able to learn parsers for a variety of different types of analyses , from traditional syntax trees to more meaning - oriented case - role and database query forms . Experimental evidence shows that CHILL performs comparably to propositional learning systems on similar tasks , and is able to go beyond the broad - but - shallow paradigm and learn mappings directly from sentences into useful semantic representations . In a complete database - query application , parsers learned by CHILL outperform an existing hand - crafted system , demonstrating the promise of empricial techniques for automating the construction certain NLP systems . ML ID : 71 . Semantic Lexicon Acquisition for Learning Parsers [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney 1997 . Submitted for review . This paper describes a system , WOLFIE ( WOrd Learning From Interpreted Examples ) , that learns a semantic lexicon from a corpus of sentences paired with representations of their meaning . The lexicon learned consists of words paired with representations of their meaning , and allows for both synonymy and polysemy . WOLFIE is part of an integrated system that learns to parse novel sentences into their meaning representations . Experimental results are presented that demonstrate WOLFIE 's ability to learn useful lexicons for a realistic domain . The lexicons learned by WOLFIE are also compared to those learned by another lexical acquisition system , that of Siskind ( 1996 ) . ML ID : 69 . Inductive Logic Programming for Natural Language Processing [ Details ] [ PDF ] Raymond J. Mooney In Stephen Muggleton , editors , Inductive Logic Programming : Selected papers from the 6th International Workshop , 3 - 22 , Berlin , 1996 . Springer Verlag . This paper reviews our recent work on applying inductive logic programming to the construction of natural language processing systems . We have developed a system , CHILL , that learns a parser from a training corpus of parsed sentences by inducing heuristics that control an initial overly - general shift - reduce parser . CHILL learns syntactic parsers as well as ones that translate English database queries directly into executable logical form . The ATIS corpus of airline information queries was used to test the acquisition of syntactic parsers , and CHILL performed competitively with recent statistical methods . English queries to a small database on U.S. geography were used to test the acquisition of a complete natural language interface , and the parser that CHILL acquired was more accurate than an existing hand - coded system . The paper also includes a discussion of several issues this work has raised regarding the capabilities and testing of ILP systems as well as a summary of our current research directions . ML ID : 68 . Learning to Parse Database Queries using Inductive Logic Programming [ Details ] [ PDF ] John M. Zelle and Raymond J. Mooney In AAAI / IAAI , 1050 - 1055 , Portland , OR , August 1996 . AAAI Press / MIT Press . This paper presents recent work using the CHILL parser acquisition system to automate the construction of a natural - language interface for database queries . CHILL treats parser acquisition as the learning of search - control rules within a logic program representing a shift - reduce parser and uses techniques from Inductive Logic Programming to learn relational control knowledge . Starting with a general framework for constructing a suitable logical form , CHILL is able to train on a corpus comprising sentences paired with database queries and induce parsers that map subsequent sentences directly into executable queries . Experimental results with a complete database - query application for U.S. geography show that CHILL is able to learn parsers that outperform a pre - existing , hand - crafted counterpart . These results demonstrate the ability of a corpus - based system to produce more than purely syntactic representations . They also provide direct evidence of the utility of an empirical approach at the level of a complete natural language application . ML ID : 66 . Corpus - Based Lexical Acquisition For Semantic Parsing [ Details ] [ PDF ] Cynthia Thompson February 1996 . Ph.D. proposal . Building accurate and efficient natural language processing ( NLP ) systems is an important and difficult problem . There has been increasing interest in automating this process . The lexicon , or the mapping from words to meanings , is one component that is typically difficult to update and that changes from one domain to the next . Therefore , automating the acquisition of the lexicon is an important task in automating the acquisition of NLP systems . This proposal describes a system , WOLFIE ( WOrd Learning From Interpreted Examples ) , that learns a lexicon from input consisting of sentences paired with representations of their meanings . Preliminary experimental results show that this system can learn correct and useful mappings . The correctness is evaluated by comparing a known lexicon to one learned from the training input . The usefulness is evaluated by examining the effect of using the lexicon learned by WOLFIE to assist a parser acquisition system , where previously this lexicon had to be hand - built . Future work in the form of extensions to the algorithm , further evaluation , and possible applications is discussed . ML ID : 57 . Lexical Acquisition : A Novel Machine Learning Problem [ Details ] [ PDF ] Cynthia A. Thompson and Raymond J. Mooney Technical Report , Artificial Intelligence Lab , University of Texas at Austin , January 1996 . This paper defines a new machine learning problem to which standard machine learning algorithms can not easily be applied . The problem occurs in the domain of lexical acquisition . The ambiguous and synonymous nature of words causes the difficulty of using standard induction techniques to learn a lexicon . Additionally , negative examples are typically unavailable or difficult to construct in this domain . One approach to solve the lexical acquisition problem is presented , along with preliminary experimental results on an artificial corpus . Future work includes extending the algorithm and performing tests on a more realistic corpus . ML ID : 56 . Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers [ Details ] [ PDF ] John M. Zelle PhD Thesis , Department of Computer Sciences , The University of Texas at Austin , Austin , TX , 1995 . Designing computer systems to understand natural language input is a difficult task . In recent years there has been considerable interest in corpus - based methods for constructing natural language parsers . These empirical approaches replace hand - crafted grammars with linguistic models acquired through automated training over language corpora . A common thread among such methods to date is the use of propositional or probablistic representations for the learned knowledge . This dissertation presents an alternative approach based on techniques from a subfield of machine learning known as inductive logic programming ( ILP ) . ILP , which investigates the learning of relational ( first - order ) rules , provides an empirical method for acquiring knowledge within traditional , symbolic parsing frameworks . This dissertation details the architecture , implementation and evaluation of CHILL a computer system for acquiring natural language parsers by training over corpora of parsed text . CHILL treats language acquisition as the learning of search - control rules within a logic program that implements a shift - reduce parser . Control rules are induced using a novel ILP algorithm which handles difficult issues arising in the induction of search - control heuristics . Both the control - rule framework and the induction algorithm are crucial to CHILL 's success . The main advantage of CHILL over propositional counterparts is its flexibility in handling varied representations . CHILL has produced parsers for various analyses including case - role mapping , detailed syntactic parse trees , and a logical form suitable for expressing first - order database queries . All of these tasks are accomplished within the same framework , using a single , general learning method that can acquire new syntactic and semantic categories for resolving ambiguities . Experimental evidence from both aritificial and real - world corpora demonstrate that CHILL learns parsers as well or better than previous artificial neural network or probablistic approaches on comparable tasks . In the database query domain , which goes beyond the scope of previous empirical approaches , the learned parser outperforms an existing hand - crafted system . These results support the claim that ILP techniques as implemented in CHILL represent a viable alternative with significant potential advantages over neural - network , propositional , and probablistic approaches to empirical parser construction . ML ID : 48 . This paper presents results from recent experiments with CHILL , a corpus - based parser acquisition system . CHILL treats grammar acquisition as the learning of search - control rules within a logic program . Unlike many current corpus - based approaches that use propositional or probabilistic learning algorithms , CHILL uses techniques from inductive logic programming ( ILP ) to learn relational representations . The reported experiments compare CHILL 's performance to that of a more naive application of ILP to parser acquisition . The results show that ILP techniques , as employed in CHILL , are a viable alternative to propositional methods and that the control - rule framework is fundamental to CHILL 's success . ML ID : 47 . Acquisition of a Lexicon from Semantic Representations of Sentences [ Details ] [ PDF ] Cynthia A. Thompson In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics ( ACL-95 ) , 335 - 337 , Cambridge , MA , 1995 . A system , WOLFIE , that acquires a mapping of words to their semantic representation is presented and a preliminary evaluation is performed . Tree least general generalizations ( TLGGs ) of the representations of input sentences are performed to assist in determining the representations of individual words in the sentences . The best guess for a meaning of a word is the TLGG which overlaps with the highest percentage of sentence representations in which that word appears . Some promising experimental results on a non - artificial data set are presented . ML ID : 45 . Learning Semantic Grammars With Constructive Inductive Logic Programming [ Details ] [ PDF ] John M. Zelle and Raymond J. Mooney In Proceedings of the 11th National Conference on Artificial Intelligence , 817 - 822 , 1993 . Menlo Park , CA : AAAI Press . Automating the construction of semantic grammars is a difficult and interesting problem for machine learning . This paper shows how the semantic - grammar acquisition problem can be viewed as the learning of search - control heuristics in a logic program . Appropriate control rules are learned using a new first - order induction algorithm that automatically invents useful syntactic and semantic categories . Empirical results show that the learned parsers generalize well to novel sentences and out - perform previous approaches based on connectionist techniques . ML ID : 25 . Learning Search - Control Heuristics for Logic Programs : Applications to Speedup Learning and Language Acquisition [ Details ] [ PDF ] John M. Zelle March 1993 . Ph.D. proposal , Department of Computer Sciences , University of Texas at Austin . This paper presents a general framework , learning search - control heuristics for logic programs , which can be used to improve both the efficiency and accuracy of knowledge - based systems expressed as definite - clause logic programs . The approach combines techniques of explanation - based learning and recent advances in inductive logic programming to learn clause - selection heuristics that guide program execution . Two specific applications of this framework are detailed : dynamic optimization of Prolog programs ( improving efficiency ) and natural language acquisition ( improving accuracy ) . In the area of program optimization , a prototype system , DOLPHIN , is able to transform some intractable specifications into polynomial - time algorithms , and outperforms competing approaches in several benchmark speedup domains . A prototype language acquisition system , CHILL , is also described . It is capable of automatically acquiring semantic grammars , which uniformly incorprate syntactic and semantic constraints to parse sentences into case - role representations . Initial experiments show that this approach is able to construct accurate parsers which generalize well to novel sentences and significantly outperform previous approaches to learning case - role mapping based on connectionist techniques . Planned extensions of the general framework and the specific applications as well as plans for further evaluation are also discussed . Languages Main page Arabic Chinese French Russian Spanish IPCC web pages IPCC Home Working Group I Working Group II Working Group III NGGIP Data Distribution Centre . Constructing Change Fields . Because climate model results generally are not sufficiently accurate ( in terms of absolute values ) at regional scales to be used directly ( Mearns et al . , 1997 ) , mean differences between the control ( or current climate ) run and the future climate run usually are calculated and then combined with some baseline observed climate data set ( IPCC , 1994 ) . Conventionally , differences ( future climate minus control ) are used for temperature variables , and ratios ( future climate / control ) are used for other variables such as precipitation , solar radiation , relative humidity , and windspeed . Most impact applications consider one or more fixed time horizon(s ) in the future ( e.g. , the 2020s , the 2050s , and the 2080s have been chosen as 30-year time windows for storing change fields in the IPCC - DDC ) . Some other applications may require time - dependent information on changes , such as vegetation succession models that simulate transient changes in plant composition ( e.g. , VEMAP members , 1995 ) . Spatial Scale of Scenarios . Concern about this issue is raised in Chapters 4 and 5 . Several solutions have been adopted to obtain finer resolution information . Simple methods . Three major methods have been developed to produce higher resolution climate scenarios at the sub - GCM grid scale : regional climate modeling ( Giorgi and Mearns , 1991 , 1999 ; McGregor , 1997 ) , statistical downscaling ( von Storch et al . , 1993 ; Rummukainen , 1997 ; Wilby and Wigley , 1997 ) , and variable- and high - resolution GCM experiments ( Fox - Rabinovitz et al . All three methods are presented in Table 3 - 4 and discussed in detail in TAR WGI Chapter 10 , but we briefly review here the first two , since they have been most commonly applied to impact assessments . Both methods are dependent on large - scale circulation variables from GCMs . Large - scale circulation refers to the general behavior of the atmosphere at large ( i.e. , continental ) scales . Regional climate modeling . The basic strategy with regional models is to rely on the GCM to reproduce the large - scale circulation of the atmosphere and to use the regional model , run at a higher resolution , to simulate sub - GCM scale regional distributions of climate . Statistical methods . Statistical methods are much less computationally demanding than dynamic methods ; they offer an opportunity to produce ensembles of high - resolution climate scenarios ( for reviews , see von Storch , 1995 ; Wilby and Wigley , 1997 ) . , 1998 , 1999 , 2001 ; Sælthun et al . , 1998 ; Hay et al . , 1999 ; Brown et al . , 2000 ; Whetton et al . Mearns et al . ( 1999 , 2001 ) demonstrate that a high - resolution scenario results in agricultural impacts that differ from those produced with a coarser resolution GCM scenario ( discussed in Chapter 5 ) . Hay et al . ( 1999 ) found differences in runoff calculations , based on a GCM - scenario and a statistically downscaled scenario . It is the aim of this module to explore some of the aspects and challenges in Human Language Technologies ( HLT ) that are of relevance to Computer Assisted Language Learning ( CALL ) . Starting with a brief outline of some of the early attempts in HLT , using the example of Machine Translation ( MT ) , it will become apparent that experiences and results in this area had a direct bearing on some of the developments in CALL . CALL soon became a multi - disciplinary field of research , development and practice . Some researchers began to develop CALL applications that made use of Human Language Technologies , and a few such applications will be introduced in this module . The advantages and limitations of applying HLT to CALL will be discussed , using the example of parser - based CALL . This brief discussion will form the basis for first hypotheses about the nature of human - computer interaction ( HCI ) in parser - based CALL . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . Piklu Gupta : At this time of writing this module Piklu was a lecturer in German Linguistics at the University of Hull , UK . He is now working for Fraunhofer IPSI . Mathias Schulze : At this time of writing this module Mathias was a lecturer in German at UMIST , now merged with the University of Manchester , UK . He is now working at the University of Waterloo , Canada . His main research interest is in parser - based CALL and linguistics . He is an active member of the NLP SIG within the EUROCALL professional association and ICALL within the CALICO professional association . Graham Davies , ICT4LT Editor , Thames Valley University , UK . Graham has been interested in Machine Translation since 1976 . Human Language Technologies ( HLT ) is a relatively new term that embraces a wide range of areas of research and development in the sphere of what used to be called Language Technologies or Language Engineering . The aim of this module is to familiarise the student with key areas of HLT , including a range of Natural Language Processing ( NLP ) applications . NLP is a general term used to describe the use of computers to process information expressed in natural ( i.e. human ) languages . The term NLP is used in a number of different contexts in this document and is one of the most important branches of HLT . There is a Special Interest Group in Language Processing , NLP SIG , within the EUROCALL professional association , and a Special Interest Group in Intelligent Computer Assisted Language Instruction ( ICALL ) within the CALICO professional association . Both have similar aims , namely to further research in a number of areas that are mentioned in this module , such as : . Artificial Intelligence ( AI ) . Computational Linguistics . Corpus - Driven and Corpus Linguistics . Formal Linguistics . Machine Aided Translation ( MAT ) . Machine Translation ( MT ) . Natural Language Interfaces . Natural Language Processing ( NLP ) . Theoretical Linguistics . All of the above are areas of research that have produced results which have proven , are proving and will prove very useful in the field of Computer Assisted Language Learning . Of course , this module can not teach you everything there is to know about HLT . This is neither necessary nor possible . The two main authors of this module are living proof of that ; they both started off as language teachers and then got interested in HLT . A multilingual CD - ROM titled A world of understanding was produced in 1998 on behalf of the Information Society and Media Directorate General of the European Commission under its former name , DGXIII . The aim of the CD - ROM was to demonstrate the importance of HLT in helping to realise the benefits of the Multilingual Information Society , in particular forming a review and record of the Language Engineering Sector of the Fourth Framework Programme of the European Union ( 1994 - 98 ) . 1.1 Introduction to HLT . [ ... ] there is no doubt that the development of tools ( technology ) depends on language - it is difficult to imagine how any tool - from a chisel to a CAT scanner - could be built without communication , without language . What is less obvious is that the development and the evolution of language - its effectiveness in communicating faster , with more people , and with greater clarity - depends more and more on sophisticated tools . ( European Commission : Language and technology 1996:1 ) . Language and technology lists the following examples of language technology ( using an admittedly broad understanding of the term ) : . photocopier ( p. 10 ) . laser printer ( p. 11 ) . fax machine ( p. 12 ) . desktop publishing ( p. 13 ) . scanner , modem ( p. 15 ) . electronic mail ( p. 16 ) . machine translation ( p. 17 ) . translator 's workbench ( p. 18 ) . tape recorder , database search engines ( p. 19 ) . telephone ( p. 25 ) . Many of these are already being used in language learning and teaching . The field of human language technology covers a broad range of activities with the eventual goal of enabling people to communicate with machines using natural communication skills . Research and development activities include the coding , recognition , interpretation , translation , and generation of language . [ ... ] Advances in human language technology offer the promise of nearly universal access to online information and services . Since almost everyone speaks and understands a language , the development of spoken language systems will allow the average person to interact with computers without special skills or training , using common devices such as the telephone . These systems will combine spoken language understanding and generation to allow people to interact with computers using speech to obtain information on virtually any topic , to conduct business and to communicate with each other more effectively . [ Source : Foreword to ( Cole 1997 ) ] . Facilitating and supporting all aspects of human communication through machines has interested researchers for a number of centuries . The use of mechanical devices to overcome language barriers was proposed first in the seventeenth century . Then , suggestions for numerical codes to be used to mediate between languages were made by Leibnitz , Descartes and others ( v. Hutchins 1986:21 ) . The beginnings of what we describe today as Human Language Technologies are , of course , closely connected to the advent of computers . ( i ) Various games , e.g. chess , noughts and crosses , bridge , poker ( ii ) The learning of languages ( iii ) Translation of languages ( iv ) Cryptography ( v ) Mathematics . Of these ( i ) , ( iv ) , and to a lesser extent ( iii ) and ( v ) are good in that they require little contact with the outside world . For instance in order that the machine should be able to play games its only organs need be ' eyes ' capable of distinguishing the various positions on a specially made board , and means for announcing its own moves . Mathematics should preferably be resticted to branches where diagrams are not much used . Of the above possible fields the learning of languages would be the most impressive , since it is the most human of these activities . This field sees however to depend too much on sense organs and locomotion to be feasible . ( Turing 1948:9 ) . Later on , Machine Translation enjoyed a period of popularity with researchers and funding bodies in the United States and the Soviet Union : . From 1956 onwards , the dollars ( and roubles ) really started to flow . Between 1956 and 1959 , no less than twelve research groups became established at various US universities and private corporations and research centres . Although linguists , language teachers and computer users today may find these predictions ridiculous , it was the enthusiasm and the work during this time that form the basis of many developments in HLT today . Research and development in HLT is nowadays more rapidly transferred into commercial systems than was the case up until the 1980s . Indeed HLT is becoming increasingly pervasive in our everyday lives . Here are some examples : . Machine Translation ( Section 3 ): There are many online translation systems that can be accessed free of charge , causing headaches for teachers whose students thought that they could save themselves time and who were blissfully unaware of the unreliability of their output ( Section 3.2 ) . Speech synthesis ( Section 4.1 ): Satellite navigation ( satnav ) devices for motor vehicles use systems that read out road numbers , street names and directions for the driver , and their output is surprisingly good . Speech recognition ( Section 4.2 ): If you make a telephone call to a customer support service you may hear a telephone recording that asks you to say a word or short phrase so that you can be connected to the appropriate department . Other previously unexpected areas of use are emerging . It is now , for instance , common for mobile phones to have what is known as predictive text input to aid the writing of short text messages . Instead of having to press one of the nine keys a number of times to produce the correct letter in a word , software in the phone compares users ' key presses to a linguistic database to determine the correct ( or most likely ) word . Most Internet search engines also now incorporate some kind of linguistic technology to enable users to enter a query in natural language , for example \" What is meant by log - likelihood ratio ? \" is as acceptable a query as simply \" log - likelihood ratio \" . What are the possible benefits for language teaching and learning of using HLT ? Here are some examples : . Teachers might want to preprocess a text to highlight certain grammatical phenomena or patterns . This can easily be done with a word - processor . Teachers might use part - of - speech taggers ( see Section 5 ) which could save them the trouble of having to manually annotate a text . Parsers available either on the Web or for local use on PCs can generate a graphical representation of sentence structure that may be useful for grammatical analysis for more advanced learners . Machine Translation ( MT ) has been the dream of computer scientists since the 1940s . The student 's attention is drawn in particular to the following publications , which provide a very useful introduction to MT : . Hutchins ( 1999 ) \" The development and use of machine translation systems and computer - based translation tools \" . Paper given at the International Symposium on Machine Translation and Computer Language Information Processing , 26 - 28 June 1999 , Beijing , China . 3.1 Machine Translation : a brief history . Initial work on Machine Translation ( MT ) systems was typified by what we would now consider to be a naive approach to the \" problem \" of natural language translation . Successful decoding of encrypted messages by machines during World War II led some scientists , most notably Warren Weaver , to view the translation process as essentially analogous with decoding . The concept of Machine Translation in the modern age can be traced back to the 1940s . Warren Weaver , Director of the Natural Sciences Division of the Rockefeller Foundation , wrote to his friend Norbert Wiener on 4 March 1947 - short ly after the first computers and computer programs had been produced : . Recognising fully , even though necessarily vaguely , the semantic difficulties because of multiple meanings , etc . , I have wondered if it were unthinkable to design a computer which would translate . Even if it would translate only scientific material ( where the semantic difficulties are very notably less ) , and even if it did produce an inelegant ( but intelligible ) result , it would seem to me worth while . When I look at an article in Russian , I say \" This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode \" . Have you ever thought about this ? As a linguist and expert on computers , do you think it is worth thinking about ? Cited in Hutchins ( 1997 ) . Weaver was possibly chastened by Wiener 's pessimistic reply : . I frankly am afraid the boundaries of words in different languages are too vague and the emotional and international connotations are too extensive to make any quasi - mechanical translation scheme very hopeful . But Weaver remained undeterred and composed his famous 1949 Memorandum , titled simply \" Translation \" , which he sent to some 30 noteworthy minds of the time . It posited in more detail the need for and possibility of MT . Thus began the first era of MT research . A direct system would comprise a bilingual dictionary containing potential replacements or target language equivalents for each word in the source language . A restriction of such MT systems was therefore that they were unidirectional and could not accommodate many languages unlike the systems that followed . Rules for choosing correct replacements were incorporated but functioned on a basic level ; although there was some initial morphological analysis prior to dictionary lookup , subsequent local re - ordering and final generation of the target text , there was no scope for syntactic analysis let alone semantic analysis ! Inevitably this often led to poor quality output , which certainly contributed to the severe criticism of MT in the 1966 Automatic Language Processing Advisory Committee ( ALPAC ) report which stated that it saw little use for MT in the foreseeable future . The damning judgment of the ALPAC report effectively halted research funding for machine translation in the USA throughout the 1960s and 1970s . We can say that both technical constraints and the lack of a linguistic basis hampered MT systems . The system developed at Georgetown University , Washington DC , and first demonstrated at IBM in New York in 1954 had no clear separation of translation knowledge and processing algorithms , making modification of the system difficult . In the period following the ALPAC report the need was increasingly felt for an approach to MT system design which would avoid many of the pitfalls of 1 G systems . By this time opinion had shifted towards the view that linguistic developments should influence system design and development . Indeed it can be said that the second generation ( 2 G ) of \" indirect \" systems owed much to linguistic theories of the time . 2 G systems can be divided essentially into \" interlingual \" and \" transfer \" systems . We will look first of all at interlingual systems , or rather those claiming to adopt an interlingual approach . Although Warren Weaver had put forward the idea of an intermediary \" universal \" language as a possible route to machine translation in his 1947 letter to Norbert Wiener , linguistics was unable to offer any models to apply until the 1960s . By virtue of its introduction of the concept of \" deep structure \" , Noam Chomsky 's theory of transformational generative grammar appeared to offer a route towards \" universal \" semantic representations and thus appeared to provide a model for the structure of a so - called interlingua . An interlingua is not a natural language , rather it can be seen as a meaning representation which is independent of both the source and the target language of translation . An interlingua system maps from a language 's surface structure to the interlingua and vice versa . A truly interlingual approach to system design has obvious advantages , the most important of which is economy , since an interlingual representation can be applied for any language pair and facilitates addition of other language pairs without major additions to the system . The next section looks at \" transfer \" systems . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . ( For n languages the number of transfer modules required would be n ( n -1 ) or n ( n -1 ) /2 if the modules are reversible ) . An important advance in 2 G systems when compared to 1 G was the separation of algorithms ( software ) from linguistic data ( lingware ) . In a system such as the Georgetown model the program mixed language modelling , translation and the processing thereof in one program . This meant that the program was monolithic and it was easy to introduce errors when trying to rectify an existing shortcoming . The move towards separating software and lingware was hastened by parallel advances in both computational and linguistic techniques . The adoption of linguistic formalisms in the design of systems and the development of high level programming languages enabled MT workers to code in a more problem - oriented way . The development in programming languages meant that it was becoming ever easier to code rules for translation in a meaningful manner and arguably improved the quality of these rules . The declarative nature of linguistic description could now be far more explicitly reflected in the design of programs for MT . . Early MT systems were predominantly parser - based , one of the first steps in such a system being to parse and tag the source language : see Section 5 on Parsing and Tagging . More recent current approaches to MT rely less on formal linguistic descriptions than the transfer approach described above . Translation Memory ( TM ) systems are now in widespread commercial use : see below and Chapter 10 of Arnold et al . ( 1994 ) . Example - Based Machine Translation ( EBMT ) is a relatively new technology which aims to combine both traditional MT and more recent TM paradigms by reusing previous translations and applying various degrees of linguistic knowledge to convert fuzzy matches into exact ones : see the Wikipedia article on EBMT . However , some early definitions of EBMT refer to what is now known as TM and they often exclude the concept of fuzzy matches . Essentially , Google Translat e begins by examining and comparing massive corpora of texts on the Web that have already been translated by human beings . It looks for matches between source and target texts and uses complex statistical analysis routines to look for statistically significant patterns , i.e. it works out the rules of the interrelationships between source and target texts for itself . As more and more corpora are added to the Web this means that Google Translate will keep improving until it reaches a point where it will be very difficult to tell that a machine has done the translation . I remember early machine translation tools translating \" Wie geht es dir ? \" as \" How goes it you ? \" Now Google Translate gets it right : \" How are you ? \" Thus we have , in a sense , come full circle in that Weaver 's ideas of applying statistical techniques are seen as a fruitful basis for MT . . 3.2 Commercial MT packages . There are many automatic translation packages on the market - as well as free packages on the Web . While such packages may be useful for extracting the gist of a text they should not be seen as a serious replacement for the human translator . Some are not all that bad , producing translations that are half - intelligible , letting you know whether a text is worth having translated properly . See : . Professional human translators are making increasing use of Translation Memory ( TM ) packages . TM packages store texts that have previously been translated , together with their source texts , in a large database . Chunks of new texts to be translated are then matched against the translated texts in the database and suggested translations are offered to the human translator wherever a match is found . The human translator has to intervene regularly in this process of translation , making corrections and amendments as necessary . TM systems can save hours of time ( estimated at up to 80 % of a translator 's time ) , especially when translating texts that are repetitive or that use lots of standard phrases and sentence formulations . Producing updates of technical manuals is a typical application of TM systems . Examples of TM systems include : . An example of automatic translations can be found at the Newstran website . This site is extremely useful for locating newspapers in a wide range of languages . You can also locate selected newspapers that have been translated using a Machine Translation system . Another approach to translation is the stored phrase bank , for example LinguaWrite , which was aimed at the business user and contained a large database of equivalent phrases and sentences in different languages to facilitate the writing of business letters . LinguaWrite was programmed by Marco Bruzzone in the 1980s and marketed by Camsoft , but it is no longer available and has not been updated . David Sephton 's Tick - Tack ( Primrose Publishing ) adopted a similar approach , beginning as a package consisting of \" building blocks \" of language for business communication , but it now embraces other topics . 3.3 Just for fun . 3.3.1 Some apocryphal stuff . The following examples have often been cited as mistakes made by machine translation ( MT ) systems . Whether they are real examples or not can not be verified . Russian - English : In a technical text that had been translated from Russian into English the term water sheep kept appearing . When the Russian source text was checked it was found that it was actually referring to a hydraulic ram . Russian - English : Idioms are often a problem . Russian - English : Another example , similar to the one above , is where out of sight , out of mind ended up being translated as the equivalent of blind and stupid . MT systems do , however , often make mistakes . The Systran MT system , which has been used by the European Commission , translated the English phrase pregnant women and children into des femmes et enfants enceints , which implies that both the women and the children are pregnant . Although it is an interpretation of the original phrase that is theoretically possible , it is also clearly wrong . 3.3.2 Translations of nursery rhymes . Try using an online machine translator to translate a text from English into another language and then back again . The results are often amusing , especially if you are translating nursery rhymes ! ( i ) Bah , bah , black sheep translated into French and then back again into English , using Babel Fish . English source text : Bah , bah , black sheep , have you any wool ? Yes sir , yes sir , three bags full . One for the master , one for the dame , and one for the little boy who lives down the lane . French translation : Bah , bah , mouton noir , vous ont n'importe quelles laines ? Oui monsieur , oui monsieur , trois sacs complètement . Un pour le maître , un pour dame , et un pour le petit garçon qui vit en bas de la ruelle . And back into English again : Bah , bah , black sheep , have you n ' imports which wools ? Yes Sir , yes Sir , three bags completely . For the Master , for lady , and for the little boy who lives in bottom of the lane . ( ii ) Humpty Dumpty translated into Italian and then back again into English , using Babel Fish . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty si è seduto su una parete . Humpty Dumpty ha avuto una grande caduta . I cavalli di tutto il re e gli uomini di tutto il re non hanno potuto un Humpty ancora . And back into English again : Humpty Dumpty has been based on a wall . Humpty Dumpty has had a great fall . The horses of all the king and the men of all the king have not been able a Humpty still . ( iii ) Humpty Dumpty translated into Italian and then back again into English , using Google Translate . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty sedeva su un muro . Humpty Dumpty ha avuto un grande caduta . Tutti i cavalli del re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme . And back into English again : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . All the king 's horses and all the king 's men could not put Humpty together again . Now , the above is an interesting result ! Google Translate used to be a very unreliable MT tool . It drives language teachers mad , as their students often use it to do their homework , e.g. translating from a given text into a foreign language or drafting their own compositions and then translating them . Mistakes made by Google Translate used to be very easy to spot , but ( as indicated above in Section 3.1 ) Google changed its translation engine a few years ago and now uses a Statistical Machine Translation ( SMT ) approach . The Humpty Dumpty translation back into English from the Italian appears to indicate that Google Translate has matched the whole text and got it right . Clever ! 3.3.3 Translations of business and journalistic texts . ( i ) A business text , translated with Google Translate and Babel Fish . Google Translate was used to translate the following text from German into English : Die Handelskammer in Saarbrücken hat uns Ihre Anschrift zur Verfügung gestellt . Wir sind ein mittelgroßes Fachgeschäft in Stuttgart , und wir spezialisieren uns auf den Verkauf von Personalcomputern . This was rendered as : The Chamber of Commerce in Saarbrücken has provided us your address is available . We are a medium sized shop in Stuttgart , and we specialize in sales of personal computers . Babel Fish produced a better version : The Chamber of Commerce in Saarbruecken put your address to us at the disposal . We are a medium sized specialist shop in Stuttgart , and we specialize in the sale of personal computers . ( ii ) A journalistic text , translated with Google Translate and Babel Fish . Die deutsche Exportwirtschaft kämpft mit der weltweiten Konjunkturflaute und muss deshalb von den Zeiten zweistelligen Wachstums Abschied nehmen . [ Ludolf Wartenberg vom Bundesverband der Deutschen Industrie ] . This was rendered by Google Translate as : The German export economy is struggling with the global downturn and must therefore take the times of double - digit growth goodbye . [ Ludolf Wartenberg from the Federation of German Industry ] . The German export trade and industry fights with the world - wide recession and must take therefore from the times of two digit growth parting . [ Ludolf waiting mountain of the Federal association of the German industry . ] Computers are normally associated with two standard input devices , the keyboard and the mouse , and two standard output devices , the display screen and the printer . All these restrict language input and output . However , computer programs and hardware devices that enable the computer to handle human speech are now commonplace . All modern computers allow the user to plug in a microphone and record his / her own voice . A variety of other sources of sound recordings can also be used . Storing these sound files is not a problem anymore as a result of the immensely increased capacity and reduced cost of storage media and improved compression techniques that enable the size of sound files to be substantially reduced . For further information on the applications of sound recording and playback technology to CALL see Module 2.2 , Introduction to multimedia CALL . A range of computer software is available for speech analysis . Spoken input can be analysed according to a wide variety of parameters and the analysis can be represented graphically or numerically . Of course , graphic output is not immediately useful for the uninitiated viewer , and hence we are not arguing that this kind of graphical representation will prove useful to the language learner . On the other hand , specialists are well capable of interpreting this speech analysis data . The information we get from speech analysis has proven very valuable indeed for speech synthesis and speech recognition , which are dealt with in the following two sections . 4.1 Speech synthesis . Speech synthesis describes the process of generating human - like speech by computer . Producing natural sounding speech is a complex process in which one has to consider a range of factors that go beyond just converting characters to sounds because very often there is no one - to - one relation between them . The intonation of particular sentence types and the rhythm of particular utterances also have to be considered . Currently speech synthesis is far more advanced and more robust than speech recognition ( see Section 4.2 below ) . The naturalness of artificially produced utterances is now very impressive compared to what used to be produced by earlier speech synthesis systems in which the intonation and timing were far from natural and resulted in the production of monotonous , robot - like speech . Many people are now unaware that so - called talking dictionaries use speech synthesis software rather than recordings of human voices . In - car satellite navigation ( satnav ) systems can produce a range of different types of human voices , both male and female in a number of different languages , and \" talk \" to the car driver guiding him / her to a chosen destination . So far , however , speech synthesis has not been as widely used in CALL as speech recognition . This is probably due to the fact that language teachers ' requirements regarding the presentation of spoken language are very demanding . Anything that sounds artificial is likely to be rejected . Some language teachers even reject speakers whose regional accent is too far from what is considered standard or received pronunciation . There is , however , a category of speech synthesis technology known as Text To Speech ( TTS ) technology that is widely used for practical purposes . TTS software falls into the category of assistive technology , which has a vital role in improving accessiblity for a wide range of computer users with special needs , which is now governed by legislation in the UK . The Special Educational Needs and Disability Act ( SENDA ) of 2001 covers educational websites and obliges their designers \" to make reasonable adjustments to ensure that people who are disabled are not put at a substantial disadvantage compared to people who are not disabled . \" See JISC 's website on Disability Legislation and ICT in Further and Higher Education - Essentials . See the Glossary for definitions of assistive technology and accessiblity . TTS is important in making computers accessible to blind or partially sighted people as it enables them to \" read \" from the screen . TTS technology can be linked to any written input in a variety of languages , e.g. automatic pronunciation of words from an online dictionary , reading aloud of a text , etc . These are examples of TTS software : . Festival Speech Synthesis System : From the Centre for Speech Technology Research at the University of Edinburgh . Festival offers a general framework for building speech synthesis systems as well as including examples of various modules . Just for fun I entered the phrase \" Pas d'elle yeux Rhône que nous \" into a couple of French language synthesisers . It 's a nonsense sentence in French but it comes out sounding like a French person trying to pronounce a well - known expression in English . Try it ! There are also Web - based tools that enable you to create animated cartoons or movies incorporating TTS , for example : . Voki enables you to create and customise your own speaking cartoon character . You can choose the TTS option ( as in Graham Davies 's example on the right ) to give the character a voice , or you can record your own voice . ReadTheWords : A tool that works in much the same way as Voki , but without the option of recording one 's own voice . An excellent tool that helps people with hearing impairments to learn how to articulate is the CSLU Speech Toolkit . To what extent speech synthesis systems are suitable for CALL is a matter for further discussion . See the article by Handley & Hamel ( 2005 ) , who report on their progress towards the development of a benchmark for determining the adequacy of speech synthesis systems for use in CALL . The article mentions a Web - based package called FreeText , for advanced learners of French , the outcome of a project funded by the European Commission . 4.2 Speech recognition . Speech recognition describes the use of computers to recognise spoken words . Speech recognition has not reached such a high level of performance as speech synthesis ( see Section 4.1 above ) , but it has certainly become usable in CALL in recent years . EyeSpeak English is a typical example of the use of speech recognition software for helping students improve their English pronunciation . Speech recognition is a non - trivial task because the same spoken word does not produce entirely the same sound waves when uttered by different people or even when uttered by the same person on different occasions . The process is complex : the computer has to digitise the sound , transform it to discard unneeded information , and then try to match it with words stored in a dictionary . The most efficient speech recognition systems are speaker - dependent , i.e. they are trained to recognise a particular person 's speech and can then distinguish thousands of words uttered by that person . If one remembers that each of the parameters analysed could have been affected by some speaker - independent background noise or by some idiosyncratic pronunciation features of this particular speaker then it already becomes clear how difficult the interpretation of the analysis data is for a speech recognition program . The following information is taken from an article written by Norman Harris of DynEd , a publisher of CALL software incorporating ASR : . Speech recognition technology has finally come of age - at least for language training purposes for young adults and adults . The essence of real language is not in discrete single words - language students need to practice complete phrases and sentences in realistic contexts . Moreover , programs which were trained to accept a speaker 's individual pronunciation quirks were not ideally suited to helping students move toward more standard pronunciation . These technologies also failed if the speaker 's voice changed due to common colds , laryngitis and other throat ailments , rendering them useless until the speaker recovered or retrained the speech engine . The solution to these problems came with the development of continuous speech recognition engines that were speaker independent . These programs are able to deal with complete sentences spoken at a natural pace , not just isolated words . Such flexibility with regard to pronunciation paradigms means that today 's speaker - independent speech recognition programs are not ideal for direct pronunciation practice . Nonetheless , exercises which focus on fluency and word order , and with native speaker models which are heard immediately after a student 's utterance had been successfully recognized , have been shown to indirectly result in much improved pronunciation . Another trade off is that the greater flexibility and leniency which allows these programs to \" recognize \" sentences spoken by students with a wide variety of accents , also limits the accuracy of the programs , especially for similar sounding words and phrases . Some errors may be accepted as correct . Native speakers testing the \" understanding \" of programs \" tuned \" to the needs of non - native speakers may be bothered by this , but most teachers , after careful consideration of the different needs and psychologies of native speakers and learners , will accept the trade off . Students do not expect to be understood every time . If they are required occasionally to repeat a sentence which the program has not recognized or which the program has misinterpreted , there may be some small frustration , but language students are much more likely to take this in their stride than would native speakers . On the other hand , if the program does \" understand \" such students , however imperfect their pronunciation , they typically experience a huge sense of satisfaction , a feel good factor native speakers simply can not enjoy to anywhere near the same degree . The worst thing for a student is a program that is too demanding of perfection - such programs will quickly lead to student frustration or the kind of embarrassed , hesitant unwillingness to speak English typical of many classrooms . Even if we accept that accuracy needs to be responsive to proficiency in order to encourage students to speak , we must , as teachers , be concerned that errors do not become reinforced . A recent breakthrough is the implementation of apps such as Apple 's Siri on the iPhone 4S and Evi , which is available for the iPhone and the Android . These apps are quite impressive at recognising speech and providing answers to questions submitted by the user . Evi 's performance was tested by the author of this paragraph . \" She \" immediately provided correct answers to these questions submitted by voice input : . In which American state is Albuquerque ? In addition , Evi may link to relevant websites that provide further information . Text input is also accepted . In this section we outline the essentials of parsing , first of all by describing the components of a parsing system and then discussing different kinds of parser . We look at one linguistic phenomenon which causes problems for parsing and finally examine potential solutions to the difficulties raised by parsing . Put in simple terms , a parser is a program that maps strings of a language into its structures . The most basic components needed by a parser are a lexicon containing words that may be parsed and a grammar , consisting of rules which determine grammatical structures . The first parsers were developed for the analysis of programming languages ; obviously as artificial , regular languages they present fewer problems than a natural language . It is most useful to think of parsing as a search problem which has to be solved . It can be solved using an algorithm which can be defined as : . [ ... ] a formal procedure that always produces a correct or optimal result . An algorithm applies a step - by - step procedure that guarantees a specific outcome or solves a specific problem . The procedure of an algorithm performs a computation in a finite amount of time . Programmers specify the algorithm the program will follow when they develop a conventional program . ( Smith 1990 ) . Parsing algorithms define a procedure that looks for the optimum combination of grammatical rules that generate a tree structure for the input sentence . How might we define these grammatical rules in a concise way that is amenable to computer processing ? A useful construct for our purposes is a so - called context - free grammar ( CFG ) . A CFG consists of rules containing a single symbol on the left - hand side and one or more on the right - hand side . For example , the statement that a sentence can consist of : . a noun phrase and a verb phrase can be expressed by the following rewrite rule . S ® NP VP . This means that a sentence S can be ' rewritten ' as a noun phrase NP followed by a verb phrase VP which are in their turn defined in the grammar . A noun phrase , for example , can consist of a determiner DET and a noun N. These symbols are known as non - terminals and the words represented by these symbols are terminal symbols . Parsing algorithms can proceed top - down or bottom - up . In some cases , top - down and bottom - up algorithms can be combined . Below are simple descriptions of two parsing strategies . 5.1.1 Top Down ( depth first ) . Top down strategy works from non - terminal symbols : . S ® NP VP . and then breaks them down into constituents . The strategy assumes we have an S and tries to fit it in . If we choose to search depth first , then we proceed down one side of the tree at a time . The search will end successfully if it manages to break down the sentence into all its terminal symbols ( words ) . 5.1.2 Bottom up ( breadth first ) . A bottom up strategy looks at elements of an S and assigns categories to them to form larger constituents until we arrive at an S. If we choose to search breadth first , then we proceed consecutively through each layer and stop successfully once we have constructed a sentence . Let 's look now at one linguistic phenomenon which causes problems for parsers - that of so - called attachment ambiguity . Consider the following sentence : . The man saw the man in the park with a telescope . Parser output can be represented as a bracketed list or , more commonly , a tree structure . Here is the output of two possible parses for the sentence above . One way of dealing with the problem of sentences which have more than one possible parse is to concentrate on specific elements of the parser input and to not deal with such phenomena as attachment ambiguity . Ideally we expect a parser to successfully analyse a sentence on the basis of its grammar , but often there are problems caused by errors in the text or incompleteness of grammar and lexicon . Also the length of sentences and ambiguity of grammars often make it hard to successfully parse unrestricted text . An approach which addresses some of these issues is partial or shallow parsing . Abney ( 1997:125 ) succinctly describes partial parsing thus : . \" Partial parsing techniques aim to recover syntactic information efficiently and reliably from unrestricted text , by sacrificing completeness and depth of analysis . \" Partial parsers concentrate on recovering pieces of sentence structure which do not require large amounts of information ( such as lexical association information ) ; attachment remains unresolved for instance . We can see that in this way parsing efficiency is greatly improved . Another strategy for analysing language is part - of - speech tagging , in which we do not seek to find larger structures such as noun phrases but instead label each word in a sentence with its appropriate part of speech . Here is the original paragraph from Section 3 of this document : . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . The following table shows the tagger output , and we can see that most of the words have been correctly identified . additional . languages . NNS . language . required . VBN . require . . . SENT . . . As with partial parsing , we are not trying to find correct attachments and since it is a limited task the success rate is quite high . The information derived from tagging can itself have input into partial parsing or into improving the performance of traditional parsers . Some of the decision task as to what is the correct part of speech to assign to a word is based on the probability of two or three word sequences ( bigrams and trigrams ) occurring , even where words can be assigned more than one part of speech . For instance , in our example tagged text the sequence ' the transfer module ' occurs . Transfer is of course also a verb , but the likelihood of a determiner ( the ) being followed by a verb is lower than the likelihood of a determiner noun sequence . See als o the Visual Interactive Syntax Learning ( VISL ) website . An online parser and a variety of other tools concerned with English grammar , including games and quizzes , can be found here . 5.1.3 Parsing erroneous input . Of course , in CALL we are dealing with texts that have been produced by language learners at various levels of proficiency and accuracy . It is therefore reasonable to assume that the parser has to be prepared to deal with linguistic errors in the input . One thing we could do is to complement our grammar for correct sentences with a grammar of incorrect sentences - an error grammar , i.e. we capture individual and/or typical errors in a separate rule system . The advantage of this error grammar approach is that the feedback can be very specific and is normally fairly reliable because this feedback can be attached to a very specific rule . The big drawback , however , is that individual learner errors have to be anticipated in the sense that each error needs to be covered by an adequate rule . However , as already stated it is not only in texts that have been produced by language learners that we find erroneous structures . Machine translation is facing similar problems . Dina & Malnati review approaches \" concerning the design and the implementation of grammars able to deal with ' real input ' . \" ( Dina & Malnati 1993:75 ) . They list four approaches : . The rule - based approach which relies on two sets of rules : one for grammatical input and the other for ungrammatical input . Dina & Malnati point out quite rightly that normally well - formedness conditions should be sufficient and the second set of rules results in linguistic redundancy . The main problem with using this approach in a parser - based CALL system is the problem of having to anticipate the errors learners are likely to make . The metarule - based approach uses a set of well - formedness rules and if none of them can be applied calls an algorithm that relaxes some constraints and records the kind of violation . Dina & Malnati note the procedurality of the algorithm causes problems when confronted with multiple errors - something very likely in any text produced by a language learner . The preference - based approach comprises an overgenerating grammar and a set of preference rules . \" [ ... ] each time a formal condition is removed from a b - rule to make its applicability context wider , a preference rule must be added to the grammar . Such a p - rule must be able to state - in the present context of the b - rule - the condition that has been previously removed . \" ( Dina & Malnati 1993:78 ) Again a source for linguistic redundancy which might result in inconsistencies in the grammar . They claim that , due to the overgeneration of possible interpretations , \" the system would be completely unusable in an applied context . \" ( ibid.:79 ) . The constraint - based approach is based on the following assumptions : . each ( sub)tree is marked by an index of error ( initially set to 0 ) ; . the violation of a constraint in a rule does not block the application , but increases the error index of the generated ( sub)tree ; . at the end of parsing the object marked by the smallest index is chosen . Consequently , the \" most plausible interpretation of a [ ... ] sentence is the one which satisfies the largest number of constraints . \" ( Dina & Malnati 1993:80 ) . We have seen in Section 3 that Machine Translation ( MT ) and the political and scientific interest in machine translation played a significant role in the acceptance ( or non - acceptance ) as well as the general development of Human Language Technologies . By 1964 , however , the promise of operational MT systems still seemed distant and the sponsors set up a committee , which recommended in 1966 that funding for MT should be reduced . It brought to an end a decade of intensive MT research activity . ( Hutchins 1986:39 ) . It is then perhaps not surprising that the mid-1960s saw the birth of another discipline : Computer Assisted Language Learning ( CALL ) . The PLATO project , which was initiated at the University of Illinois in 1960 , is widely regarded as the beginning of CALL - although CALL was just part of this huge package of general Computer Assisted Learning ( CAL ) programs running on mainframe computers . PLATO IV ( 1972 ) was probably the version of this project that had the biggest impact on the development of CALL ( Hart 1995 ) . At the same time , another American university , Brigham Young University , received government funding for a CALL project , TICCIT ( Time - Shared Interactive , Computer Controlled Information Television ) ( Jones 1995 ) . Other well - known and still widely used programs were developed soon afterwards : . The CALIS / WinCALIS ( Computer Aided Language Instruction System ) authoring tools , Duke University ( Borchardt 1995 ) . The TUCO package for learners of German , developed by Heimy Taylor and Werner Haas , Ohio State University . See Module 3.2 , Section 5.9 . The CLEF package for learners of French , which was produced by a consortium of Canadian universities in the late 1970s and is still going strong today . See Module 3.2 , Section 5.9 . In the UK , John Higgins developed Storyboard in the early 1980s , a total Cloze text reconstruction program for the microcomputer ( Higgins & Johns 1984:57 ) . ( Levy 1997:24 - 25 ) describes how other programs extended this idea further . See Section 8.3 , Module 1.4 , headed Total text reconstruction : total Cloze , for further information on total Cloze programs . In recent years the development of CALL has been greatly influenced by the technology and by our knowledge of and our expertise in using it , so that not only the design of most CALL software , but also its classification has been technology - driven . For example , Wolff ( 1993:21 ) distinguished five groups of applications : . The late 1980s saw the beginning of attempts which are mostly subsumed under Intelligent CALL ( ICALL ) , a \" mix of AI [ Artificial Intelligence ] techniques and CALL \" ( Matthews 1992b : i ) . Early AI - based CALL was not without its critics , however : . And that , fundamentally , is why my initial enthusiasm has now turned so sour . ( Last 1989:153 ) . For a more up - to - date and positive point of view of Artifical Intelligence , see Dodigovic ( 2005 ) . Bowerman ( 1993:31 ) notes : \" Weischedel et al . ( 1978 ) produced the first ICALL [ Intelligent CALL ] system which dealt with comprehension exercises . It made use of syntactic and semantic knowledge to check students ' answers to comprehension questions . \" As far as could be ascertained , this was just the early swallow that did not create a summer . Krüger - Thielmann ( 1992:51ff . ) lists and summarises the following early projects in ICALL : ALICE , ATHENA , BOUWSTEEN & COGO , EPISTLE , ET , LINGER , VP2 , XTRA - TE , Zock . Matthews ( 1993:5 ) identifies Linguistic Theory and Second Language Acquisition Theory as the two main disciplines which inform Intelligent CALL and which are ( or will be ) informed by Intelligent CALL . He adds : \" the obvious AI research areas from which ICALL should be able to draw the most insights are Natural Language Processing ( NLP ) and Intelligent Tutoring Systems ( ITS ) \" ( Matthews1993:6 ) . Matthews shows that it is possible to \" conceive of an ICALL system in terms of the classical ITS architecture \" ( ibid . ) The system consists of three modules - expert , student and teacher module - and an interface . The expert module is the one that \" houses \" the language knowledge of the system . It is this part which can process any piece of text produced by a learner - in an ideal system . This is usually done with the help of a parser of some kind : . ( Holland et al . 1993:28 ) . This notion of parser - based CALL not only captures the nature of the field much better than the somewhat misleading term \" Intelligent CALL \" ( Is all other CALL un - intelligent ? ) , it also identifies the use of Human Language Technologies as one possible approach in CALL alongside others such as multimedia - based CALL and Web - based CALL and thus identifies parser - based CALL as one possible way forward for CALL . In some cases , the ( technology - defined ) borders between these sub - fields of CALL are not even clearly identifiable , as we will see in some of the projects mentioned in the following paragraphs . To exemplify recent advances in the use of sophisticated human language technology in CALL , let us have a look at some of the projects that were presented at two conferences in the late 1990s . The first one is the Language Teaching and Language Technology conference in Groningen in 1997 ( Jager et al . 1998 ) . Witt & Young ( 1998 ) , on the other hand , are concerned with assessing pronunciation . They implemented and tested a pronunciation scoring algorithm which is based on speech recognition ( see Section 4.2 ) and uses hidden Markov models . \" The results show that - at least for this setup with artificially generated pronunciation errors - the GOP [ goodness of pronunciation ] scoring method is a viable assessment tool . \" A third paper on pronunciation at this conference , by Skrelin & Volskaja ( 1998 ) outlined the use of speech synthesis ( see Section 4.1 ) in language learning and lists dictation , distinction of homographs , a sound dictionary and pronunciation drills as possible applications . \" The project vision foresees two main areas where GLOSSER applications can be used . First , in language learning and second , as a tool for users that have a bit of knowledge of a foreign language , but can not read it easily or reliably \" ( Dokter & Nerbonne 1998:88 ) . Dokter & Nerbonne report on the French - Dutch demonstrator running under UNIX . The demonstrator : . uses morphological analysis to provide additional grammatical information on individual words and to simplify dictionary look - up ; . relies on automatic word selection ; . offers the opportunity to insert glosses ( taken form the dictionary look - up ) into the text ; . relies on string - based word sense disambiguation ( \" Whenever a lexical context is found in the text that is also provided in the dictionary , the example in the dictionary is highlighted . \" ( op.cit.:93 ) . Roosma & Prószéky ( 1998 ) draw attention to the fact that GLOSSER works with the following language pairs : English - Estonian - Hungarian , English - Bulgarian , French - Dutch and describe a demonstrator version running under Windows . Dokter et al ( 1998 ) conclude in their user study \" that Glosser - RuG improves the ease with which language students can approach a foreign language text \" ( Dokter et al . 1998:175 ) . The latter project relies on a spellchecker , morphological analyser , syntactic parser and a lexical database for Basque , and the authors report on the development of an interlanguage model . At another conference ( UMIST , May 1998 ) , which brought together a group of researchers who are exploring the use of HLT in CALL software , Schulze et al . ( 1999 ) and Tschichold ( 1999 ) discussed strategies for improving the success rate of grammar checkers . Menzel & Schröder ( 1999 ) described error diagnosis in a multi - level representation . The demonstration system captures the relations of entities in a simple town scenery . The available syntactic , semantic and pragmatic information is checked simultaneously for constraint violations , i.e. errors made by the language learners . Visser ( 1999 ) introduced CALLex , a program for learning vocabulary based on lexical functions . Diaz de Ilarraza et al . ( 1999 ) described aspects of IDAZKIDE , a learning environment for Spanish learners of Basque . The program contains the following modules : wide - coverage linguistic tools ( lexical database with 65,000 entries ; spell checker ; a word form proposer and a morphological analyser ) , an adaptive user interface and a student modelling system . The model of the students ' language knowledge , i.e. their interlanguage , is based on a corpus analysis ( 300 texts produced by learners of Basque ) . Foucou & Kübler ( 1999 ) presented a Web - based environment for teaching technical English to students of computing . Ward et al . ( 1999 ) showed that Natural Language Processing techniques combined with a graphical interface can be used to produce meaningful language games . Davies & Poesio ( 1998 ) reported on tests of simple CALL prototypes that have been created using CSLUrp , a graphical authoring system for the creation of spoken dialogue systems . They argue that since it is evident that today 's dialogue systems are usable in CALL software , it is now possible and necessary to study the integration of corrective feedback in these systems . Mitkov ( 1998 ) outlined early plans for a new CALL project , The Language Learner 's Workbench . It is the aim of this project to incorporate a number of already available HLT tools and to package them for language learners . These examples of CALL applications that make use of Human Language Technologies are by no means exhaustive . They not only illustrate that research in HLT in CALL is vibrant , but also that HLT has an important contribution to make in the further development of CALL . Of course , both disciplines are still rather young and many projects in both areas , CALL and HLT , have not even reached the stage of the implementation of a fully functional prototype yet . A number of CALL packages that make use of speech recognition have reached the commercial market and are being used successfully by learners all over the world ( see Section 4.2 ) . Speech synthesis , certainly at word level , has achieved a clarity of pronunciation that makes it a viable tool for language learning ( see Section 4.1 ) . Many popular electronic dictionaries now incorporate speech synthesis systems . Part - of - speech taggers have reached a level of accuracy that makes them usable in the automatic pre - processing of learner texts . Morphological analysers for a number of languages automatically provide grammatical information on vocabulary items in context and make automatic dictionary look - ups of inflected or derived word forms possible . This progress in HLT and CALL has mainly been possible as the result of our better understanding of the structures of language - our understanding of linguistics . The lack of linguistic modelling and the insufficient deployment of Natural Language Processing techniques has sometimes been given as one reason for the lack of progress in some areas of CALL : see , for example , Levy ( 1997:3 ) , citing Kohn ( 1994 ) . [ ... ] Kohn suggests that current CALL is lacking because of poor linguistic modelling , insufficient deployment of natural language processing techniques , an emphasis on special - purpose rather than general - purpose technology , and a neglect of the ' human ' dimnesion of CALL ( Kohn 1994:32 ) . The examples in the previous section have shown that it is possible to apply certain linguistic theories ( e.g. phonology and morphology ) to Human Language Technologies and implement this technology in CALL software . This is , of course , true . However , it does not mean that interesting fragments or aspects of a given language can not be captured by a formal linguistic theory and hence implemented in a CALL application . In other words , if one can not capture the German language in its entirety in order to implement this linguistic knowledge in a computer program , this does not mean that one can not capture interesting linguistic phenomena of that language . This means even if we are only able to describe a fragment of a given language adequately we can still make very good use of this description in computer applications for language learning . What is the kind of knowledge we ought to have about language before we can attempt to produce an HLT tool that can be put to effective use in CALL ? Let us look at one particular aspect of language - grammar . In recent years , the usefulness of conscious learning of grammar has been discussed time and again , very often in direct opposition to what has been termed \" the communicative approach \" . ( ibid.:6 ) This assumption leads to the question of what role exactly the computer ( program ) has to play in a sensitive , rich and enjoyable grammar - learning process . The diversity of approaches outlined in this special issue of ReCALL on grammar illustrates that there are many different roads to successful grammar learning that will need to be explored . In this module , only the example of parser - based CALL will be discussed . Let us take a grammar checker for language learners as a specific example in point . This grammar checker could then be integrated into a CALL program , a word - processor , an email editor , a Web page editor etc . The design of such a grammar checker is mainly based on findings in theoretical linguistics and second language acquisition theory . Let us start with second language acquisition theory . Research in second language acquisition has proved that grammar learning can lead to more successful language acquisition . Here learners have the opportunity to correct grammatical errors and mistakes that they have made while concentrating on the subject matter and the communicative function of the text . It is at this stage that a grammar checker for language learners can provide useful and stimulating guidance . In order to ascertain the computational features of such a grammar checker , let us first consider what exactly we mean by \" grammar \" in a language learning context . Helbig discusses possible answers to this question from the point of view of the teaching and learning of foreign languages in general : . As a starting point for answering our question concerning the relevance ( and necessity ) of grammar in foreign language teaching we used a differentiation of what was and is understood by the term \" grammar \" : . Grammar A : the system of rules that is inherent to the object language itself and is independent of the fact whether it has been captured by Linguistics or not ; . Grammar B : the scientific - linguistic description of the language inherent system of rules , the modelling of Grammar A by Linguistics ; . Grammar C : the system of rules intern to the speaker and listener which is formed in the head of the learner during language acquisition and which forms the basis for him / her to produce and understand correct sentences and texts and to use them appropriately in communication . ( Helbig:1975 ) . Helbig identifies further a Grammar B1 and a Grammar B2 - the former being a linguistic grammar and the latter being a learner grammar . The description of grammar B1c is a literal translation of Helbig 's wording - in the terminology used now , the term \" interlanguage \" appears to be the most appropriate . The application of Helbig 's grammar classification to CALL produces the following results : . Grammar A remains as defined by Helbig . In other words it refers to the target grammar of the interlanguage continuum . Grammar B1a is the grammar which enables the parser to process grammatically well - formed sentences in the target language . Grammars B1b , B1c and B2 enable the grammar checking CALL tool to detect errors in the learner input and provide the linguistic information to generate feedback . Grammar C is the grammar system which the CALL tool should help to correct and expand . Additionally , the grammar checker will gather data for learner profiles which should allow useful insights into the development of Grammar C of learners you have used the program . Consequently , Grammar B in its entirety and Grammar C will have to be considered first and foremost when developing the grammar checker . The question then arises : If Grammar A provides the linguistic data for the parser developer , how can we \" feed \" these different grammars into a computer program ? The computer requires that any grammar which we intend to use in any program ( or programming language , for that matter ) be mathematically exact . Grammars which satisfy this condition are normally referred to as formal grammars . The mathematical description of these grammars uses set theory . Therefore , a language L is said to have a vocabulary V . If there were no restrictions on how to construct strings , the number of possible strings is infinite . This becomes clear when one considers that each vocabulary item of V could be repeated infinitely in order to construct a string . However , as language learners in particular know any language L adheres to a finite set of ( grammar ) rules . This explains why grammar teaching software that attempts to anticipate possible incorrect answers can only do this successfully if the answer domain is severely restricted and the anticipation process will therefore much simpler . Could a computer program perform this task - a task based on infinite possibilities ? Yes , it could - but not based on infinite possibilities . That is why it will be necessary to look for an approach which is based on a finite set of possibilities , which can then be pre - programmed . Let us therefore consider L the set of strings that can be constructed using the ( formal ) grammar G . A formal grammar can be defined as follows ( see e.g. Allen 1995 ): . G ( VN , VT , R , S ) . And here we are already dealing with sets which have a finite number of members . The number of grammatical rules is fairly limited . This is certainly the case when we only consider the basic grammar rules of a language that will have to be learned by the intermediate to early advanced learner . ( Note here what we said earlier about Grammar B2 - the learner grammar : It was only a subset of Grammar 1 - the linguistic grammar . ) Formal grammars have been used in a number of CALL projects . Matthews ( 1993 ) continues his discussion of grammar frameworks for CALL which he started in 1992 ( Matthews 1992a ) . He lists eight major grammar frameworks that have been used in CALL : . Of course , these are only some examples . More recently , Tschichold et al . ( 1994 ) reported on a prototype for correcting English texts produced by French learners . This system relies on a number of different finite state automata for pre - processing , filtering and detecting ( Tschichold et al.1994 ) . Brehony & Ryan ( 1994 ) report on \" Francophone Stylistic Grammar Checking ( FSGC ) using Link Grammars \" . They adapted the post - processing section of an existing parser so that it would detect stylistic errors in English input produced by French learners . His plea is for the use of the PPT ( Principles and Parameters Theory ( Chomsky 1986 ) as a grammar framework for CALL applications , basing his judgement on three criteria : computational effectiveness , linguistic perspicuity and acquisitional perspicuity ( Matthews 1993:9 ) . In later parts of his paper , Matthews compares rule- and principle - based frameworks using DCGs ( Definite Clause Grammars ) as the example for the latter . He concludes that principle - based frameworks ( and consequently principle - based parsing ) are the most suitable grammar frameworks for what he calls Intelligent CALL . Recently , other unification - based grammar frameworks not included in Matthews ' list have been used in CALL . Hagen , for instance , describes \" an object - oriented , unification - based parser called HANOI \" ( Hagen 1995 ) which uses formalisms developed in Head - Driven Phrase Structure Grammar ( HPSG ) . He quotes Zajac : . Combining object - oriented approaches to linguistic description with unification - based grammar formalisms [ ... ] is very attractive . On one hand , we gain the advantages of the object - oriented approach : abstraction and generalisation through the use of inheritance . On the other hand , we gain a fully declarative framework , with all the advantages of logical formalisms [ ... ] . Of course , not even this extended list is comprehensive - at best it could be described as indicative of the variety of linguistic approaches used in parser - based Computer Assisted Language Learning and , in particular , in the field of grammar checking . At the end of this short excursion into formal grammar(s ) it can be concluded that any CALL grammar checker component needs as its foundation a formal grammar describing as comprehensively as possible the knowledge we have about the target language grammar . This was the grammar that Helbig ( 1975 ) refers to as Grammar B1a . But what part do the other grammars play in a CALL environment ? Let us stay with the example of a parser - based grammar checker for language learners . Hence , the provision of adequate feedback on the morpho - syntactic structure of parts of the text produced by learners is the most important task for this parser - based grammar checker . Let us therefore consider the place of feedback provision within a parser grammar . In other words , as a good teacher would do - the grammar checker would offer advice on how to change an ungrammatical structure into a corresponding grammatically well - formed structure . As stated earlier this approach would be based on an infinite number of construction possibilities . Therefore , the provision of adequate feedback and help to the learner appears to be difficult if not impossible . However , it has been indicated above that feedback could be linked to the finite sets on which the formal grammar relies . How can this be done ? Each member of the three sets which will have to be considered here . The non - terminal symbols like NP and VP , the words and the set of morpho - syntactic rules carry certain features that determine their behaviour in a sentence and determine their relation to other signs within the sentence . These features which restrict what the text producer can do with a given ( terminal or non - terminal ) symbol in a sentence and under what conditions a particular grammatical rule has to be applied will be labelled constraints . Let us return to our provisional description of feedback , which can now be formulated more precisely . Feedback shows the relation . by explaining the underlying constraint of the anticipated construction in L based on Grammar B2 . to support production of construction in L . and by reasoning about the likely cause of the rule violation . to extend Grammar C - the learner - inherent grammar . And secondly , this above description of feedback given by a grammar checker which is based on a modified parser shows that it is possible to construct tools that support the focus on form by learners during the reflection stage of a text production process . Even if the grammar checker were only to detect a small number of morpho - syntactic errors , this would be beneficial for the learners as long as they were aware of the limitations of this CALL tool . On the other hand , the feedback description contains still a number of question marks in parentheses after some of the important keywords - whether the intended aims of grammar checking can be achieved can only be validated through the use and thorough testing of such a grammar checker . We should better not make any such assumption ( in the scientific sense - we do hope for these improvements , of course ) and better wait until such a parser - based grammar checker is actually tested in a series of proper learning experiments . Let us now leave the discussion of some of the underlying linguistics behind and discuss the role of parser - based applications in language learning . Natural language parsers take written language as their input and produce a formal representation of the syntactic and sometimes semantic structure of this input . The role they have to play in computer - assisted language learning has been under scrutiny in the last decade : ( Matthews 1992a ) ; ( Holland et al . 1993 ) ; ( Nagata 1996 ) . See also Heift ( 2001 ) . Holland et al . discussed the \" possibilities and limitations of parser - based language tutors \" ( Holland et al . 1993:28 ) . Comparing parser - based CALL to what they label as conventional CALL they come to the conclusion that : . [ ... ] in parser - based CALL the student has relatively free rein and can write a potentially huge variety of sentences . ICALL thus permits practice of production skills , which require recalling and constructing , not just recognising [ as in conventional CALL ] , words and structures . ( Holland et al.1993:31 ) . However , at the same time , parsing imposes certain limitations . Parsers tend to concentrate on the syntax of the textual input , thus \" ICALL may actually subvert a principal goal of language pedagogy , that of communicating meanings rather than producing the right forms \" ( Holland et al.1993:32 ) . This disadvantage can be avoided by a \" focus on form \" which is mainly achieved by putting the parser / grammar checker to use within a relevant , authentic communicative task and at a time chosen by and convenient to the learner / text producer . Juozulynas ( 1994 ) evaluated the potential usefulness of syntactic parsers in error diagnosis . He analysed errors in an approximately 400 page corpus of German essays by American college students in second - year language courses . His study shows that : . [ ... ] syntax is the most problematic area , followed by morphology . ( Juozulynas 1994:5 ) . Juozulynas adapted a taxonomic schema by Hendrickson which comprises four categories : syntax , morphology , orthography , lexicon . Juozulynas ' argument for splitting orthography into spelling and punctuation is easily justified in the context of syntactic parsing . Parts of punctuation can be described by using syntactic bracketing rules , and punctuation errors can consequently be dealt with by a syntactic parser . Lexical and spelling errors form , according to Juozulynas , a rather small part of the overall number of learner errors . Some of these errors will , of course , be identified during dictionary look - up , but if words that are in the dictionary are used in a nonsensical way , the parser will not recognise them unless specific error rules ( e.g. for false friends ) are built in . Consequently , a parser - based CALL application can play a useful role in detecting many of the morpho - syntactic errors which constitute a high percentage of learner errors in freely produced texts . Nevertheless , the fact remains : . A second limitation of ICALL is that parsers are not foolproof . Because no parser today can accurately analyse all the syntax of a language , false acceptance and false alarms are inevitable . ( Holland et al . 1993:33 ) . This is something not only developers of parser - based CALL , but also language learners using such software have to take into account . In other words , this limitation of parser - based CALL has to be taken into consideration during the design and implementation process and when integrating this kind of CALL software in the learning process . A final limitation of ICALL is the cost of developing NLP systems . By comparison with simple CALL , NLP development depends on computational linguists and advanced programmers as well as on extensive resources for building and testing grammars . Beyond this , instructional shells and lessons must be built around NLP , incurring the same expense as developing shells and lessons for CALL . ( Holland et al . 1993:33 ) . It is mainly this disadvantage of parser - based CALL that explains the lack of commercially available ( and commercially viable ) CALL applications which make good use of HLT . However , it is to be hoped that this hurdle can be overcome in the not too distant future because sufficient expertise in the area has accumulated over recent years . More and more computer programs make good use of this technology , and many of these have already \" entered \" the realm of computer - assisted language learning , as can be seen from the examples in the previous section . Holland et al . ( 1993 ) answer their title question \" What are parsers good for ? \" on the basis of their own experience with BRIDGE , a parser - based CALL program for American military personnel learning German , and on the basis of some initial testing with a small group of experienced learners of German . They present the following points : . ICALL appears to be good for form - focused instruction offering learners the chance to work on their own linguistic errors by this method , not only to improve their performance in the foreign language but also to improve their language awareness . ICALL appears to be good for selected kinds of students . The authors list the following characteristics which might influence the degree of usefulness of ICALL for certain students : . i. intermediate proficiency . ii . analytical orientation . iii . tolerance of ambiguity . iv . confidence as learners . ICALL is good for research because the parser automatically tracks whole sentence responses and detects , classifies , and records errors . It might facilitate the assessment of the students grammatical competency and thus help us discover patterns of acquisition . ICALL [ ... ] can play a role in communicative practice . The authors argue for the embedding of parser - based CALL in \" graphics microworlds \" which help to capture some basic semantics . Given that we would like to harness the advantages of parser - based CALL , how do we take the limitations into consideration in the process of designing and implementing a parser - based CALL system ? What implications does the use of parsing technology have for human - computer interaction ? [ I]n discourse analytic terms ( Grice 1975 ) , the nature of the contract between student and CALL tutor is straightforward , respecting the traditional assumption that the teacher is right , whereas the ICALL contract is less well defined . ( Holland et al . 1993:33f . ) . The rigidity of traditional CALL in which the program controls the linguistic input by the learner to a very large extent has often given rise to a criticism of CALL which accuses CALL developers and practitioners of relying on behaviourist programmed instruction . If one wants to give learners full control over their linguistic input , for example , by relying on parsing technology , what are then the terms according to which the communicative interaction of computer ( program ) and learner can be defined ? The differences between humans and machines have obviously to be taken into consideration in order to understand the interaction of learners with CALL programs : . Machines are compiled out of individual parts for a very specific purpose ( totum fix et partibus ) ; whereas humans are holistic entities whose parts can be differentiated ( partes fiunt ex toto ) . Humans process all sorts of experiences and repeatedly and interactively create their own environment - something machines can not do . They \" calculate \" a problem on the basis of pre - wired rules . The main features of human thoughts are the inherent contradictions and the ability to cope with them , something that will not be calculable due to its complexity , variety and degree of detail ( Schmitz 1992:209f . ) . These differences between humans and machines can for our purposes , i.e. the theoretical description of human - computer interaction , be legitimately reduced to the distinction between actions and operations as is done in Activity Theory . The main point of this theory for our consideration here is that communicative activities can be divided into actions which are intentional , i.e. goal - driven ; and these can be sub - divided into operations which are condition - triggered . These operations are normally learnt as actions . In the example referred to in the quotation above , the gear - switching is learnt as an action . The learner - driver is asked by the driving instructor to change gear and this becomes the goal of the learner . Once the learner - driver has performed this action a sufficient number of times , this action becomes more and more automated and in the process loses more and more of its intentionality . A proficient driver might have the goal to accelerate the car which will necessitate switching into higher gear , but this is now triggered by a condition ( the difference between engine speed and speed of the car ) . It can thus be argued that humans learn to perform complex actions by learning to perform certain operations in a certain order . Machines , on the other hand , are only made to perform certain ( sometimes rather complex ) operations . This has some bearing on our understanding of the human - computer interaction that takes place when a learner uses language - learning software . When , for instance , the spell checker is started in a word - processing package , the software certainly does not have ' proof - read ' the learner 's document . The computer just responds to the clicking of the spellchecker menu item and performs the operation of checking the strings in the document against the entries in a machine dictionary . For the computer user ( in our case a learner ) , it might look like the computer is proof - reading the document . Normally , one only realises that no \" proper \" document checking is going on if a correctly spelled word is not found in the dictionary or nonsensical alternatives are given for a simple spelling error . Person X interacts with Person Y in that he observes Person Y 's action , reasons about the likely intention for that action and reacts according to this assumed intention . This , for example , explains why many learners get just as frustrated when an answer they believe to be right is rejected by the computer as they would get if it were rejected by their tutor . Of course , an ideal computer - assisted language learning system would avoid such pitfalls and not reject a correct response or overlook an incorrect one . Since any existing system can only approximate to this ideal , researchers and developers in parser - based CALL can only attempt to build systems that can perform complex structured sequences of ( linguistic ) operations so that learners can interact meaningfully and successfully with the computer . Grammatica is able to identify parts of speech in English and French with a fair degree of accuracy and show , for example , how verbs are conjugated in different tenses and how plurals of nouns are formed . Abeillé A. ( 1992 ) \" A lexicalised tree adjoining grammar for French and its relevance to language teaching \" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Abney S. ( 1997 ) \" Part - of - speech tagging and partial parsing \" . In Young S. & Bloothooft G. ( eds . ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Allen J. ( 1995 ) Natural language understanding , New York : John Benjamins Publishing Company . Alwang G. ( 1999 ) \" Speech recognition \" , PC Magazine , 10 November 1999 . Antos G. ( 1982 ) Grundlagen einer Theorie des Formulierens . Textherstellung in geschriebener und gesprochener Sprache , Tübingen : Niemeyer . Arnold D. , Balkan . L , Meijer S. , Humphreys R. L. & Sadler L. ( 1994 ) Machine Translation : an introductory guide , Manchester : NEC Blackwell . Bellos D. ( 2011 ) Is that a fish in your ear ? Translation and the meaning of everything , Harlow : Penguin / Particular Books . Bennett P. ( 1997 ) Feature - based approaches to grammar , Manchester : UMIST , Unpublished Manuscript . Bennett P. & Paggio P. ( eds . ) ( 1993 ) Preference in EUROTRA , Luxembourg : European Commission . Bloothooft G. , Dommelen W. , van Espain C. , Green P. , Hazan V. & Wigforss E. ( eds . ) ( 1997 ) The landscape of future education in speech communication sciences : ( 1 ) analysis , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bloothooft G. , van Dommelen W. , Espain C. , Hazan V. , Huckvale M. & Wigforss E. ( eds . ) ( 1998 ) The landscape of future education in speech communication sciences : ( 2 ) proposals , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bolt P. & Yazdani M. ( 1998 ) \" The evolution of a grammar - checking program : LINGER to ISCA \" , CALL 11 , 1 : 55 - 112 . Borchardt F. ( 1995 ) \" Language and computing at Duke University : or , Virtue Triumphant , for the time being \" , CALICO Journal 12 , 4 : 57 - 83 . Bowerman C. ( 1993 ) Intelligent computer - aided language learning . LICE : a system to support undergraduates writing in German , Manchester : UMIST , Unpublished doctoral dissertation . Brehony T. & Ryan K. ( 1994 ) \" Francophone stylistic grammar checking ( FSGC ) : using link grammars \" , CALL 7 , 3 : 257 - 269 . Brocklebank P. ( 1998 ) An experiment in developing a prototype intelligent teaching system from a parser written in Prolog , Manchester , UMIST , Unpublished MPhil dissertation . Brown P.F. , Della Pietra S.A. , Della Pietra V.J. & Mercer R.L. ( 1993 ) \" The mathematics of statistical Machine Translation : parameter estimation \" , Computational Linguistics 19 , 2 : 263 - 311 . Buchmann B. ( 1987 ) \" Early history of Machine Translation \" . In King M. ( ed . ) Machine Translation today : the state of the art , Edinburgh : University Press . Bull S. ( 1994 ) \" Learning languages : implications for student modelling in ICALL \" , ReCALL 6 , 1 : 34 - 39 . Bureau Lingua / DELTA ( 1993 ) Foreign language learning and the use of new technologies , Brussels : European Commission . Cameron K. ( ed . ) ( 1989 ) Computer Assisted Language Learning , Oxford : Intellect . Carson - Berndsen J. ( 1998 ) \" Computational autosegmental phonology in pronunciation teaching \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Chanier D. , Pengelly M. , Twidale M. & Self J. ( 1992 ) \" Conceptual modelling in error analysis in computer - assisted language learning \" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Chen L. & Barry L. ( 1989 ) \" XTRA - TE : Using natural language processing software to develop an ITS for language learning \" . In Fourth International Conference on Artificial Intelligence and Education : 54 - 70 . Chomsky N. ( 1986 ) Knowledge of language : its nature , origin , and use , New York : Praeger . CLEF ( Computer - assisted Learning Exercises for French ) ( 1985 ) Developed by the CLEF Group , Canada , including authors at the University of Guelph , the University of Calgary and the University of Western Ontario . Also published by Cambridge University Press , 1998 : ISBN 0 - 521 - 59277 - 1 . Curzon L. B. ( 1985 ) Teaching in further education : an outline of principles and practice . London : Holt , Rinehart & Winston , ( 3rd edition ) . Davies G. ( 1988 ) \" CALL software development \" . In Jung Udo O.H .. ( ed . ) Computers in applied linguistics and language learning : a CALL handbook , Frankfurt : Peter Lang . Davies G. ( 1996 ) Total - text reconstruction programs : a brief history , Maidenhead : Camsoft . Davies S. & Poesio M. ( 1998 ) \" The provision of corrective feedback in a spoken dialogue system \" . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) ( 1991 ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Diaz de Ilarranza A. , Maritxalar M. & Oronoz M. ( 1998 ) \" Reusability of language technology in support of corpus studies in an ICALL environment \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Diaz de Ilarranza A. , Maritxalar A. , Maritxalar M. & Oronoz M. ( 1999 ) \" IDAZKIDE : An intelligent computer - assisted language learning environment for Second Language Acquisition \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 12 - 19 . Dodigovic M. ( 2005 ) Artificial intelligence in second language learning : raising error awarenes s , Clevedon : Multilingual Matters . Dokter D. & Nerbonne J. ( 1998 ) \" A session with Glosser - RuG \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Dokter D. , Nerbonne J. , Schurcks - Grozeva L. & Smit P. ( 1998 ) \" Glosser - RuG : a user study \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Ehsani F. & Knodt E. ( 1998 ) \" Speech technology in computer - aided language learning : strengths and limitations of a new CALL paradigm \" , Language Learning and Technology 2 , 1 : 45 - 60 . Ellis R. ( 1994 ) The study of Second Language Acquisition , Oxford : OUP . European Commission ( 1996 ) Language and technology : from the Tower of Babel to the Global Village , Luxembourg : European Commission . ISBN 92 - 827 - 6974 - 7 . Fechner J. ( ed . ) ( 1994 ) Neue Wege i m computergestützten Fremdsprachenunterricht , Berlin : Langenscheidt . Feuerman K. , Marshall C. , Newman D. & Rypa M. ( 1987 ) \" The CALLE project \" , CALICO Journal 4 : 25 - 34 . Foucou P - Y. & Kübler N. ( 1999 ) \" A Web - based language learning environment : general architecture \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 31 - 39 . Fum D. , Pani B. & Tasso C. ( 1992 ) \" Native vs. formal grammars : a case for integration in the design of a foreign language tutor \" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hagen L.K. ( 1995 ) \" Unification - based parsing applications for intelligent foreign language tutoring systems , CALICO Journal 12 , 2 - 3 : 5 - 31 . Hamilton S. ( 1998 ) \" A CALL user study \" . In Jager S. , Nerbonne J. & van Essen A.(eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Handke J. ( 1992 ) \" WIZDOM : a multiple - purpose language tutoring system based on AI techniques \" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hart R. ( 1995 ) \" The Illinois PLATO foreign languages project \" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 15 - 37 . Heift T. ( 2001 ) \" Error - specific and individualised feedback in a Web - based language tutoring system : Do they read it ? \" ReCALL 13 , 1 : 99 - 109 . Heift T. & Schulze M. ( eds . ) ( 2003 ) Error diagnosis and error correction in CALL , CALICO Journal Special Issue 20 , 3 . Heift T. & Schulze M. ( 2007 ) Errors and intelligence in CALL : parsers and pedagogues , London and New York : Routledge . Helbig G. ( 1975 ) \" Bemerkungen zum Problem von Grammatik und Fremdsprachenunterricht \" , Deutsch als Fremdsprache 6 , 12 : 325 - 332 . Higgins J. & Johns T. ( 1984 ) Computers in language learning , London : Collins . Holland M. , Maisano R. , Alderks C. & Martin J. ( 1993 ) \" Parsers in tutors : what are they good for ? \" CALICO Journal 11 , 1 : 28 - 46 . Holland M. & Fisher F.P. ( eds . ) ( 2007 ) T he path of speech technologies in Computer Assisted Language Learning : from research toward practice , London and New York : Routledge . Hutchins W.J. ( 1986 ) Machine Translation : past , present , future , Chichester : Ellis Horwood . Hutchins W.J. ( 1997 ) \" Fifty years of the computer and translation \" , Machine Translation Review 6 , October 1997 : 22 - 24 . Hutchins W.J. & Somers H.L. ( 1992 ) An introduction to Machine Translation , London : Academic Press . Jager S. ( 2001 ) \" From gap - filling to filling the gap : a re - assessment of Natural Language Processing in CALL \" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Jager S. , Nerbonne J. & van Essen A. ( eds . ) ( 1998 ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Jones R. ( 1995 ) \" TICCIT and CLIPS : The early years \" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 84 - 96 . Jung Udo O.H. & Vanderplank R. ( eds . ) ( 1994 ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Juozulynas V. ( 1994 ) \" Errors in the composition of second - year German students : an empirical study of parser - based ICALI \" , CALICO Journal 12 , 1 : 5 - 17 . King M. ( ed . ) ( 1987 ) Machine Translation today : the state of the art , Edinburgh : Edinburgh University Press . Klein W. & Dittmar N. ( 1979 ) Developing grammars : the acquisition of German syntax by foreign workers , Heidelberg : Springer . Klein W. & Perdue C. ( 1992 ) Utterance structure ( developing grammars again ) , Amsterdam : John Benjamins Publishing Company . Klein W. ( 1986 ) Second language acquisition , Cambridge : Cambridge University Press . Kohn K. ( 1994 ) \" Distributive language learning in a computer - based multilingual communication environment \" . In Jung Udo O.H. & Vanderplank R. ( eds . ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Krashen S. ( 1981 ) Second language acquisition and second language learning , Oxford : Pergamon . Krashen S. ( 1982 ) Principles and practice in second language acquisition , Oxford : Pergamon . Krüger - Thielmann K. ( 1992 ) Wissensbasierte Sprachlernsysteme . Neue Möglichkeiten für den computergestützten Sprachunterricht , Tübingen : Gunter Narr . Labrie G. & Singh L. ( 1991 ) \" Parsing , error diagnosis and instruction in a French tutor \" , CALICO Journal 9 : 9 - 25 . Last R. ( 1989 ) Artificial Intelligence techniques in language learning , Chichester : Ellis Horwood . Last R. ( 1992 ) \" Computers and language learning : past , present - and future ? \" In Butler C. ( ed . ) Computers and written texts , Oxford : Blackwell . Levin L. , Evans D. & Gates D. ( 1991 ) \" The Alice system : a workbench for learning and using language \" , CALICO Journal 9 : 27 - 55 . Levy M. ( 1997 ) Computer - assisted language learning : context and conceptualisation , Oxford : Oxford University Press . Lightbown P.M. & Spada N. ( 1993 ) How languages are learned , Oxford : Oxford University Press . Long M. ( 1991 ) \" Focus on form : a design feature in language teaching methodology \" . In de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Manning C. & Schütze H. ( 1999 ) Foundations of statistical Natural Language Processing , Cambridge MA , MIT Press . Matthews C. ( 1992a ) \" Going AI : foundations of ICALL \" , CALL 5 , 1 - 2 : 13 - 31 . Matthews C. ( 1992b ) Intelligent CALL ( ICALL ) bibliography , Hull : University of Hull , CTI Centre for Modern Languages . Matthews C. ( 1993 ) \" Grammar frameworks in Intelligent CALL \" , CALICO Journal 11 , 1 : 5 - 27 . Matthews C. ( 1994 ) \" Intelligent Computer Assisted Language Learning as cognitive science : the choice of syntactic frameworks for language tutoring \" , Journal of Artificial Intelligence in Education 5 , 4 : 533 - 56 . Menzel W. & Schröder I. ( 1999 ) \" Error diagnosis for language learning systems \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 20 - 30 . Michel G. ( ed . ) ( 1985 ) Grundfragen der Kommunikationsbefähigung , Leipzig : Bibliographisches Institut . Mitkov R. ( 1998 ) Language Learner 's Workbench . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . Mitkov R. & Nicolov N. ( eds . ) ( 1997 ) Recent advances in Natural Language Processing . Amsterdam : John Benjamins Publishing Company . Murphy M. , Krüger A. & Griesz A. , ( 1998 ) \" RECALL \" -towards a knowledge - based approach to CALL . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nagata N. ( 1996 ) \" Computer vs. workbook instruction in second language acquisition \" , CALICO Journal 14 , 1 : 53 - 75 . Nerbonne J. , Jager S. & van Essen A. ( 1998 ) Introduction . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nirenburg S. ( ed . ) ( 1986 ) Machine Translation : theoretical and methodological issues , Cambridge : Cambridge University Press . Pijls F. , Daelmans W. & Kempen G. ( 1987 ) \" Artificial intelligence tools for grammar and spelling instruction , Instructional Science 16 : 319 - 336 . Pollard C. & Sag I.A. ( 1987 ) Information - Based Syntax and Semantics , Chicago : University Press . Pollard C. & Sag I.A. ( 1994 ) Head - Driven Phrase Structure Grammar , Chicago : University Press . Ramsay A. & Schäler R. ( 1997 ) \" Case and word order in English and German \" . In Mitkov R. & Nicolov N. ( eds . ) Recent advances in Natural Language Processing , Amsterdam : John Benjamins Publshing Company : 15 - 34 . Ramsay A. & Schulze M. ( 1999 ) \" Die Struktur deutscher Lexeme \" , German Linguistic and Cultural Studies , Peter Lang , Submitted Manuscript . Reifler E. ( 1958 ) \" The Machine Translation project at the University of Washington , Seattle , Washington , USA \" . In Proceedings of the Eighth International Congress of Linguists , Oslo University Press : 514 - 518 . Roosmaa T. & Prószéky G. ( 1998 ) \" GLOSSER - using language technology tools for reading texts in a foreign language \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Salaberry R. ( 1996 ) \" A theoretical foundation for the development of pedagogical tasks in computer mediated communication \" , CALICO Journal 14 , 1 : 5 - 34 . Sanders R. ( 1991 ) \" Error analysis in purely syntactic parsing of free input : the example of German \" , CALICO Journal 9 , 1 : 72 - 89 . Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 . Schmitz U. ( 1992 ) Computerlinguistik , Opladen : Westdeutscher Verlag . Schulze M. ( 1997 ) \" Textana - text production in a hypertext environment \" , CALL 10 , 1 : 71 - 82 . Schulze M. ( 1998 ) \" Teaching grammar - learning grammar . Aspects of Second Language Acquisition in CALL \" , CALL 11 , 2 : 215 - 228 . Schulze M. ( 2001 ) \" Human Language Technologies in Computer Assisted Language Learning \" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Schulze M. , Hamel M - J. & Thompson J. ( eds . ) ( 1999 ) Language processing in CALL , ReCALL Special Issue . Schumann J.H. & Stenson N. ( eds . ) ( 1975 ) New frontiers in second language learning , Rowley : Newbury House . Schwind C. ( 1990 ) \" An intelligent language tutoring system \" , International Journal of Man - Machine Studies 33 : 557 - 579 . Selinker L. ( 1992 ) Rediscovering interlanguage , London : Longman . Skrelin P. & Volskaja N. ( 1998 ) \" The application of new technologies in the development of education programs \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Smith R. ( 1990 ) Dictionary of Artificial Intelligence , London : Collins . Späth P. ( 1994 ) \" Hypertext und Expertensysteme i m Sprachunterricht \" . In Fechner J. ( ed . ) Neue Wege i m computergestützten Fremdsprachenunterricht , Berlin : Langenscheidt . Stenzel B. ( ed . ) ( 1985 ) Computergestützter Fremdsprachenunterricht . Ein Handbuch , Berlin : Langenscheidt . Swartz M. & Yazdani M. ( eds . ) ( 1992 ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Taylor H. ( 1987 ) TUCO II . Published by Gessler Educational Software , New York . Based on earlier programs developed by Taylor H. & Haas W. at Ohio State University in the 1970s : DECU ( Deutscher Computerunterricht ) and TUCO ( Tutorial Computer ) . Taylor H. ( 1998 ) Computer assisted text production : feedback on grammatical errors made by learners of English as a Foreign Language , Manchester : UMIST , MSc Dissertation . Tschichold C. ( 1999 ) \" Intelligent grammar checking for CALL \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 5 - 11 . Tschichold C. , Bodme F. , Cornu E. , Grosjean F. , Grosjean L. , Kübler N. & Tschumi C. ( 1994 ) \" Detecting and correcting errors in second language texts \" , CALL 7 , 2 : 151 - 160 . Visser H. ( 1999 ) \" CALLex ( Computer - Aided Learning of Lexical functions ) - a CALL game to study lexical relationships based on a semantic database \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 50 - 56 . Ward R. , Foot R. & Rostron A.B. ( 1999 ) \" Language processing in computer - assisted language learning : language with a purpose \" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 40 - 49 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \" Translation \" . Reproduced in Locke W.N. & Booth A.D. ( eds . ) ( 1955 ) Machine translation of languages : fourteen essays , Cambridge , Mass : Technology Press of the Massachusetts Institute of Technology : 15 - 23 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \" Translation \" . See \" 50th anniversary of machine translation \" , MT News International , Issue 22 ( Vol . 8 , 1 ) , July 1999 : 5 - 6 . Weischedel R. , Voge W. & James M. ( 1978 ) \" An artificial intelligence approach to language instruction \" , Artificial Intelligence 10 : 225 - 240 . Wilks Y. & Farwell D. ( 1992 ) \" Building an intelligent second language tutoring system from whatever bits you happen to have lying around \" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag , . Whitelock P.J. & Kilby K. ( 1995 ) Linguistic and computational techniques in Machine Translation system design , London : University College Press . Witt S. & Young S. ( 1998 ) \" Computer - assisted pronunciation teaching based on automatic speech recognition \" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Wolff D. ( 1993 ) \" New technologies for foreign language teaching \" . In Foreign language learning and the use of new technologies , Bureau Lingua / DELTA , Brussels , European Commission . Young S. & Bloothooft G. ( eds . ) ( 1997 ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Zähner C. ( 1991 ) \" Word grammars in ICALL \" . In Savolainen H. & Telenius J. ( eds . ) EuroCALL 91 proceedings , Helsinki : Helsinki School of Economics . Zech J. ( 1985 ) \" Methodische Probleme einer tätigkeitsorientierten Ausbildung des sprachlich - kommunikativen Könnens \" . In Michel G. ( ed . ) Grundfragen der Kommunikationsbefähigung , Leipzig : Bibliographisches Institut . CALICO ( Computer Assisted Language Instruction Consortium ) : CALICO is a professional association devoted to promoting the use of technology enhanced language learning . CALICO 's sister association in Europe is EUROCALL . EUROCALL : EUROCALL is a professional association devoted to promoting the use of technology enhanced language learning , based at the University of Ulster , Northern Ireland . EUROCALL 's sister association in the USA is CALICO . ICALL is an interdisciplinary research field integrating insights from computational linguistics and artificial intelligence into computer - aided language learning . Such integration is needed for CALL systems to be able to analyze language automatically , to make them aware of language as such . This makes it possible to provide individualized feedback to learners working on exercises , to ( semi-)automatically prepare or enhance texts for learners , and to automatically create and use detailed learner models . See NLP SIG , the Special Interest Group within EUROCALL , with which ICALL closely collaborates . InSTIL : The name of a now defunct Special Interest Group dedicated to Integrating Speech Technology in Language Learning , which was set up within the EUROCALL and CALICO professional associations . A good deal of the work undertaken by InSTIL has now been taken over by ICALL and NLP SIG . NLP SIG : The name of the Special Interest Group for Natural Language Processing within the EUROCALL professional association . See ICALL , the Special Interest Group within CALICO , with which NLP SIG closely collaborates . Virtual Linguistics Campus : It includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Document last updated 29 April 2012 . This page is maintained by Graham Davies . Please cite this Web page as : Gupta P. & Schulze M. ( 2012 ) Human Language Technologies ( HLT ) . Module 3.5 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \n",
      "--- 0.06994056701660156 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "start_time = time.time()\n",
    "################################################################################\n",
    "# Filtering\n",
    "################################################################################\n",
    "pretty = lambda x : json.dumps(x, indent=2, sort_keys=True)\n",
    "solr_select = 'http://localhost:8983/solr/depcc-small/select?q='\n",
    "#train_path  = 'data/04-hyper/train.jsonl'\n",
    "train_path  = 'data/02-acl-arc/train.jsonl'\n",
    "#train_path  = 'data/07-imdb/train.jsonl'\n",
    "with open(train_path, 'r') as train_file:\n",
    "    json_lines = []\n",
    "    lines = train_file.readlines()\n",
    "    for line in lines:\n",
    "        j = json.loads(line)\n",
    "        json_lines.append(j)\n",
    "N = len(json_lines)\n",
    "print(N)\n",
    "\n",
    "# Query with training example\n",
    "j = json_lines[0]\n",
    "query = j['text'].replace(' ', '+')\n",
    "#query = \"\"\n",
    "#with open('data/02-acl-arc/lda_union.txt', 'r') as file:\n",
    "#    query = file.read()\n",
    "print(query)\n",
    "#print(len(query.split()))\n",
    "#sys.exit()\n",
    "rp_retrieval = requests.get(solr_select + query).json()\n",
    "cc_docs = (rp_retrieval['response']['docs'])\n",
    "print('Number of retrieved documents: %d' % len(cc_docs))\n",
    "cc_doc0 = json.loads(cc_docs[0]['_src_'])\n",
    "cc_doc100 = \"\"\n",
    "for i in range(10):\n",
    "    cc_doc100 += json.loads(cc_docs[i]['_src_'])['text']\n",
    "print(cc_doc100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation by sentences(Documents -> Passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(cc_doc100)\n",
    "cc_psgs = []\n",
    "psg = ''\n",
    "num_tokens = 0\n",
    "for sent in doc.sents:\n",
    "    if num_tokens < 100:\n",
    "        psg += sent.text\n",
    "        num_tokens += len(sent)\n",
    "    else:\n",
    "#        print(num_tokens)\n",
    "        cc_psgs.append({'doc_id' : '', 'doc_text'  : psg,  'title': ''  })\n",
    "        num_tokens = 0\n",
    "        psg = ''\n",
    "#print(len(cc_psgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def encode(X):\n",
    "    encoder = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_df=0.3)\n",
    "    encoder.fit(X)\n",
    "    print(\"Dimension: \", len(encoder.vocabulary_))\n",
    "    embedding = encoder.transform(X).toarray()\n",
    "    return embedding\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train psg length 1688\n",
      "cc psg length 54\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "train_psgs = []\n",
    "cc_psgs = []\n",
    "with open(\"emb/train_sample.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        train_psgs.append(row[1])\n",
    "\n",
    "\n",
    "with open(\"emb/cc_sample.tsv\") as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        cc_psgs.append(row[1])\n",
    "\n",
    "\n",
    "MAX_TR_PSGS = len(train_psgs)\n",
    "MAX_CC_PSGS = len(cc_psgs)\n",
    "print('train psg length %d' % MAX_TR_PSGS)\n",
    "print('cc psg length %d' % MAX_CC_PSGS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Sparse Encoding on train+CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension:  26613\n",
      "(1742, 26613)\n",
      "(1688, 26613)\n",
      "(54, 26613)\n"
     ]
    }
   ],
   "source": [
    "emb = encode(train_psgs + cc_psgs)\n",
    "print(emb.shape)\n",
    "train_embeddings = emb[:1688]\n",
    "cc_embeddings = emb[1688:]\n",
    "\n",
    "print(train_embeddings.shape)\n",
    "print(cc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 26613\n",
      "Number of CC passages: 54\n",
      "10 26613\n",
      "Number of train passages: 10\n",
      "trained? True\n",
      "Total number of indexed CC passages:  54\n",
      "\n",
      "Using an indentical CC set\n",
      "================================================================\n",
      "4 nearest neighbors\n",
      "[[ 0 12  4  6  2 40  5 10 23  3  9 35 24 43 31 18 49 36 14 30 22 28 38 13\n",
      "   7 32 53 46 34 21 15  1 41 19 26 48 52 16 17 42 39 33 11 27 20 25  8 37\n",
      "  45 50 47 51 29 44]\n",
      " [ 1  2 48  3 39 14  5 46  9 31 16 19 28 40  0 41 32 36 35 20  4  6 22 50\n",
      "  27 37 53  7 24 33 23 26 47 15  8 25 30 18 34 17 38 29 21 11 45 12 51 52\n",
      "  43 13 49 10 42 44]\n",
      " [ 2 41  1  0 48 39  3 53  5 49 46 12 22  4 33 28 14 32 23 18 45 36 19  9\n",
      "   6 24 10 35 21 15 17 11 31 27 34 38 20 26 13 16 52 44 40 42 51 43 37 47\n",
      "  50  7 25 29  8 30]\n",
      " [ 3 40 44 39 41 10  9  1  2 14 23  5 31 37  0 36  4 28 19 17 35 32 33 20\n",
      "  22 42 27 13 26  7 53 24 30 12 46 18 43 16 48 11 15 52 25 45 47 38  8 34\n",
      "  49 51 21  6 50 29]\n",
      " [ 4  0 40 23 11 10  3  9  2 41 13 32 36 15 12  5 19 16 24 43 26 35 31  1\n",
      "  28 18 20 17 49 34 27 38 14 33 47 39  8 46 48 53 37 52 45 21 51 50  6 29\n",
      "  25  7 42 22 44 30]]\n",
      "\n",
      "distances(sanity check)\n",
      "[[0.        1.7612034 1.7722296 1.8271785 1.8416016 1.8593804 1.8763949\n",
      "  1.8995278 1.9026402 1.9192234 1.9201925 1.921787  1.9220282 1.9361615\n",
      "  1.9363468 1.94004   1.9405514 1.941971  1.9454324 1.9510381 1.9510516\n",
      "  1.9533793 1.9536519 1.9540513 1.9543886 1.9562356 1.956845  1.9632593\n",
      "  1.9645172 1.9668217 1.9680306 1.9680853 1.9681711 1.968577  1.9696302\n",
      "  1.9697493 1.9698515 1.9699799 1.9723918 1.9728361 1.9793231 1.9802215\n",
      "  1.9849836 1.9869034 1.9880257 1.9890255 1.9893295 1.9999982 1.9999985\n",
      "  1.9999986 1.9999987 1.9999988 1.999999  2.0000005]\n",
      " [0.        1.78231   1.8770142 1.8894941 1.9263216 1.9278721 1.9324689\n",
      "  1.9472586 1.9475521 1.949965  1.9502794 1.9556278 1.9631017 1.9661862\n",
      "  1.9680853 1.9685816 1.9720004 1.972976  1.9741049 1.9746014 1.9758832\n",
      "  1.978057  1.9782355 1.979083  1.979647  1.9808544 1.9813389 1.9816488\n",
      "  1.9817605 1.9823169 1.982392  1.9825978 1.9835272 1.9851425 1.9851899\n",
      "  1.9859586 1.9877248 1.9879156 1.9879463 1.9928392 1.9931701 2.0000012\n",
      "  2.0000012 2.0000012 2.0000014 2.0000014 2.0000017 2.0000017 2.0000017\n",
      "  2.000002  2.0000024 2.0000026 2.0000029 2.0000038]\n",
      " [0.        1.7619644 1.78231   1.8416016 1.8487519 1.8925076 1.8992105\n",
      "  1.9102998 1.916398  1.9254684 1.927061  1.9290613 1.9307233 1.9352398\n",
      "  1.9395117 1.9424608 1.9436493 1.9446344 1.9464682 1.9465009 1.9475198\n",
      "  1.9503158 1.9516006 1.9528463 1.9536676 1.9555799 1.9577162 1.9632597\n",
      "  1.9640577 1.9680717 1.968149  1.9684076 1.9689993 1.969479  1.9701705\n",
      "  1.9706274 1.9708562 1.9732342 1.9783163 1.9795167 1.9857768 1.9870768\n",
      "  1.9881278 1.992387  1.994366  1.9999977 1.999998  1.9999982 1.9999983\n",
      "  1.9999985 1.9999987 1.9999989 1.9999996 2.0000007]\n",
      " [0.        1.7760216 1.7777882 1.8333673 1.8464022 1.8682737 1.8746319\n",
      "  1.8894941 1.8992105 1.9041667 1.9055979 1.9066037 1.9168615 1.9176469\n",
      "  1.9192234 1.9229918 1.9294647 1.9341466 1.9365844 1.9438535 1.9502137\n",
      "  1.9512055 1.9537724 1.9542696 1.9565874 1.957186  1.9572676 1.9629997\n",
      "  1.9634631 1.9645455 1.9676771 1.970748  1.9715557 1.972535  1.9738468\n",
      "  1.9762956 1.9795631 1.9803362 1.9812764 1.9838258 1.9845246 1.9857025\n",
      "  1.986227  1.9864259 1.9890003 1.9933002 1.9933233 1.9934951 1.994551\n",
      "  1.9953077 1.9999983 1.9999985 1.9999986 1.9999988]\n",
      " [0.        1.7722296 1.9073929 1.9132657 1.9221975 1.9272722 1.9294647\n",
      "  1.9351048 1.9352398 1.9352608 1.9397597 1.941309  1.9497803 1.9506382\n",
      "  1.9567909 1.9580467 1.963613  1.9663049 1.9664409 1.9685124 1.9705846\n",
      "  1.9717398 1.9755048 1.9758832 1.9759058 1.9763101 1.9768289 1.9783411\n",
      "  1.9829525 1.983026  1.9852827 1.9855464 1.98588   1.9864599 1.9874945\n",
      "  1.9878274 1.9883695 1.9887528 1.9891337 1.9912488 1.9999987 1.9999987\n",
      "  1.9999988 1.9999989 1.9999992 1.9999992 1.9999992 1.9999994 1.9999996\n",
      "  1.9999998 2.        2.0000007 2.000001  2.0000014]]\n",
      "\n",
      "===============================================================\n",
      "Using the query(train set)\n",
      "4 nearest neighbors\n",
      "[[30  9 45 14 22  3 43 28  0 16 44 25 34 13  7 39  5 52 46 17 50 49 40 33\n",
      "  24  1 12 29 32 20 53  4 51 15  2 35  6 47 31 27 38 19 37 23 26 48 42 41\n",
      "  36 18 21 11 10  8]\n",
      " [52 19  6 20 44 39 37 29 28 24 22 47 46 40 43 32 31 25 36 16 15 13  9 12\n",
      "   8  7 48 49 45 50 41 42 38 51 33 34 30 35 26 27 23  4 17 18 14  5  3 10\n",
      "  11 53 21  1  2  0]\n",
      " [ 3  8  2 31 41 38  7 44 39 37 29 47 46 40 43 32 30 25 36 28 24 22 16 15\n",
      "  13 12 11  9 48 49 45 50 42 51 33 34 35  5  4 26 27 23 52 17 18 14 19 10\n",
      "  20 53 21  6  1  0]\n",
      " [34 32  4 51  3 17 25 33  5 44 39 37 47 46 40 43 31 29 26 36 28 24 22 16\n",
      "  15 13 48 49 45 50 41 42 38  9 12  8  7 30 35 27 23 52  1 18 14 19 10 11\n",
      "  20 53 21  6  2  0]\n",
      " [28 30 29 25 14 38 23  7 11 19 20 33 49 34  1 45 27  0  2 35 17 40 24 37\n",
      "  26  3 32  8 16 39  9 36 47 48 50 46 44 51 43 42 41 52 31 22 18 15 13 12\n",
      "  10 53 21  6  5  4]]\n",
      "\n",
      "distances\n",
      "[[1.8950956 1.914635  1.9270768 ... 2.        2.        2.       ]\n",
      " [1.9274273 1.9594933 1.9626863 ... 2.        2.        2.       ]\n",
      " [1.9720054 1.9721605 1.9724406 ... 2.        2.        2.       ]\n",
      " ...\n",
      " [1.9394811 1.9448698 1.9465729 ... 2.        2.        2.       ]\n",
      " [1.966484  1.9674733 1.9698756 ... 2.        2.        2.       ]\n",
      " [1.9422138 1.9728205 1.9743501 ... 2.        2.        2.       ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Nearest Neighbor (FAISS)\n",
    "################################################################################\n",
    "#print(emb[0])\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Read CC embeddings (DATABASE)\n",
    "#cc_embeddings = np.load('emb/cc_sample_0.pkl', allow_pickle=True)\n",
    "nb = cc_embeddings.shape[0] # database size\n",
    "d =  cc_embeddings.shape[1]\n",
    "print(nb,d)\n",
    "xb = np.array(cc_embeddings, dtype='float32')\n",
    "print('Number of CC passages: %d' % nb)\n",
    "\n",
    "# Read train embeddings (QUERY)\n",
    "#train_embeddings = np.load('emb/train_sample_0.pkl', allow_pickle=True)\n",
    "nq = 10 # query size\n",
    "d = train_embeddings.shape[1]\n",
    "print(nq,d)\n",
    "xq = np.array(train_embeddings, dtype='float32')\n",
    "print('Number of train passages: %d' % nq)\n",
    "\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "\n",
    "print('trained? %r' % index.is_trained)\n",
    "index.add(xb)                  # add vectors to the index\n",
    "print('Total number of indexed CC passages: ', index.ntotal)\n",
    "print()\n",
    "print('Using an indentical CC set')\n",
    "k = nb                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity check\n",
    "print('================================================================')\n",
    "print('4 nearest neighbors')\n",
    "print(I)\n",
    "print()\n",
    "print('distances(sanity check)')\n",
    "print(D)\n",
    "print()\n",
    "\n",
    "\n",
    "print('===============================================================')\n",
    "print('Using the query(train set)')\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print('4 nearest neighbors')\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print('\\ndistances')\n",
    "print(D)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1688, 54)\n",
      "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .\n",
      "\n",
      "CLOSEST passages in CC:\n",
      "-------------------------------------------------------------\n",
      "Closest 0\n",
      "words themselves . [ 9 ] These are syntactic qualities since each of these\n",
      "arguments bears a direct syntactic relation to their head as much as they hold a\n",
      "semantic place within the underlying argument structure . In order to extract\n",
      "this kind of subcategorization and selectional information from unstructured\n",
      "text , we first need to impose syntactic order on it . A second , more practical\n",
      "option is to assign syntactic structure to a sentence using automatic methods .\n",
      "Automatic parsing generally requires the presence of a treebank - a large\n",
      "collection of manually annotated sentences - and a\n",
      "-------------------------------------------------------------\n",
      "Closest 1\n",
      "to include statistical information about a word 's actual use , these\n",
      "contemporary projects are exploiting advances in computational linguistics that\n",
      "have been made over the past thirty years . Before turning , however , to how we\n",
      "can adapt these technologies in the creation of a new and complementary\n",
      "reference work , we must first address the use of such lexica . Like the OED ,\n",
      "Classical lexica generally include a list of citations under each headword ,\n",
      "providing testimony by real authors for each sense . Of necessity , these\n",
      "citations are usually only exemplary selections , though\n",
      "-------------------------------------------------------------\n",
      "Closest 2\n",
      "A Maximum - Entropy - Inspired Parser \" , Proceedings of NAACL ( 2000 ) .\n",
      "Collins 1999 Collins , Michael . \" Head - Driven Statistical Models for Natural\n",
      "Language Parsing \" , Ph.D. thesis . Philadelphia : University of Pennsylvania ,\n",
      "1999 . Freund 1840 Freund , Wilhelm ( ed . ) Wörterbuch der lateinischen Sprache\n",
      ": nach historisch - genetischen Principien , mit steter Berücksichtigung der\n",
      "Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 .\n",
      "Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \"\n",
      "-------------------------------------------------------------\n",
      "Closest 3\n",
      "between two words based on their appearance in similar contexts . In creating a\n",
      "lexicon with these features , we are exploring two strengths of automated\n",
      "methods : they can analyze not only very large bodies of data but also provide\n",
      "customized analysis for particular texts or collections . We can thus not only\n",
      "identify patterns in one hundred and fifty million words of later Latin but also\n",
      "compare which senses of which words appear in the one hundred and fifty thousand\n",
      "words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry\n",
      "could look\n",
      "------------------------------------------------------------\n",
      "...\n",
      "-------------------------------------------------------------\n",
      "Farthest 50\n",
      "a given sentence ( as in Figure 3 ) , the translation probability is divided\n",
      "evenly between them . The weighted list of translation equivalents we identify\n",
      "using this technique can provide the foundation for our further lexical work .\n",
      "In the example above , we have induced from our collection of parallel texts\n",
      "that the headword oratio is primarily used with two senses : speech and prayer .\n",
      "Our approach , however , does have two clear advantages which complement those\n",
      "of traditional lexica : first , this method allows us to include statistics\n",
      "about actual word usage in\n",
      "-------------------------------------------------------------\n",
      "Farthest 51\n",
      "\" lemmatized searching . \" The search results are thus significantly diluted by\n",
      "a large number of false positives . The advantage of the Perseus and TLG\n",
      "lemmatized search is that it gives scholars the opportunity to find all the\n",
      "instances of a given word form or lemma in the textual collections they each\n",
      "contain . The TLL , however , is impeccable in precision , while the Perseus and\n",
      "TLG results are dirty . What we need is a resource to combine the best of both .\n",
      "Where do we want to be ? The OLD and TLL are\n",
      "-------------------------------------------------------------\n",
      "Farthest 52\n",
      "the TLL provides comprehensive listings by Classical authors for many of its\n",
      "lemmata . These citations essentially function as an index into the textual\n",
      "collection . If I am interested in the places in Classical literature where the\n",
      "verb libero means to acquit , I can consult the OLD and then turn to the source\n",
      "texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more\n",
      "comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This\n",
      "is what we might consider a manual form of\n",
      "-------------------------------------------------------------\n",
      "Farthest 53\n",
      "and idioms ( such as bear fruit ) can also be uncovered as well . This corpus -\n",
      "based approach has since been augmented in two dimensions . [ 4 ] At the same\n",
      "time , researchers are also subjecting their corpora to more complex automatic\n",
      "processes to extract more knowledge from them . While word frequency and\n",
      "collocation analysis is fundamentally a task of simple counting , projects such\n",
      "as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable\n",
      "lexicographers to induce information about a word 's grammatical behavior as\n",
      "well . In their ability\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "print(I.shape)\n",
    "print(train_psgs[0])\n",
    "print()\n",
    "print('CLOSEST passages in CC:')\n",
    "\n",
    "for i in range(4):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Closest %d' % i)\n",
    "    closest = I[0][i]\n",
    "    print(textwrap.fill(cc_psgs[closest],80))\n",
    "\n",
    "print('------------------------------------------------------------')\n",
    "print('...')\n",
    "\n",
    "for i in range(MAX_CC_PSGS-4, MAX_CC_PSGS):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Farthest %d' % i)\n",
    "    farthest = I[0][i]\n",
    "    print(textwrap.fill(cc_psgs[farthest],80))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}