{
  "responseHeader":{
    "status":0,
    "QTime":4,
    "params":{
      "q":"lexical language rules information natural analysis text e.g lin processing rule extraction general question found speech chen applications work computational \nlinguistic document retrieval system english information problem test instance description pairs simple morphological language evidence source cfxxx inference task prosodic \nsystems dialogue knowledge number current approaches semantic litman learning mitkov taskar speech domain recent recently interest resolution shown student proposed \noch approach automatic generation evaluation corpus text zhao model ney proposed error employed previously rate maximum training berger standard chen \nword model based models corpus collins e.g tree translation parse brown results words sentences sentence feature brill disambiguation approach sense \ntranslation structure representation number terms semantic proposed machine work annotation head problem context sato theory language galley represent kilgarriff process \nsystem coreference tasks method version semantic applied performance relations palmer xue relation recognition verb muc resolution conll chinese show recent \ngrammar database phrase tag treebank features grammars model set arabic hpsg habash syntactic databases structure penn moschitti section resources nomenclature \nprevious work algorithm similar parser features training set data experiments approach results feature grammar parsing method nivre juola obtained technique \nlearning methods statistical work charniak collins made parsers parsing words syntactic buchholz phrases documents data roth ratnaparkhi full significant research",
      "fl":"*,score"}},
  "response":{"numFound":2979189,"start":0,"maxScore":240.83232,"numFoundExact":true,"docs":[
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":240.83232},
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":215.41312},
      {
        "id":"32c9f64f-c981-4437-9424-056c60cde52d",
        "_src_":"{\"url\": \"http://ancienthebrewpoetry.typepad.com/ancient_hebrew_poetry/2009/01/why-i-love-virginia-woolf.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148758.73/warc/CC-MAIN-20160205193908-00166-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"It is the aim of this module to explore some of the aspects and challenges in Human Language Technologies ( HLT ) that are of relevance to Computer Assisted Language Learning ( CALL ) . Starting with a brief outline of some of the early attempts in HLT , using the example of Machine Translation ( MT ) , it will become apparent that experiences and results in this area had a direct bearing on some of the developments in CALL . CALL soon became a multi - disciplinary field of research , development and practice . Some researchers began to develop CALL applications that made use of Human Language Technologies , and a few such applications will be introduced in this module . The advantages and limitations of applying HLT to CALL will be discussed , using the example of parser - based CALL . This brief discussion will form the basis for first hypotheses about the nature of human - computer interaction ( HCI ) in parser - based CALL . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . Piklu Gupta : At this time of writing this module Piklu was a lecturer in German Linguistics at the University of Hull , UK . He is now working for Fraunhofer IPSI . Mathias Schulze : At this time of writing this module Mathias was a lecturer in German at UMIST , now merged with the University of Manchester , UK . He is now working at the University of Waterloo , Canada . His main research interest is in parser - based CALL and linguistics . He is an active member of the NLP SIG within the EUROCALL professional association and ICALL within the CALICO professional association . Graham Davies , ICT4LT Editor , Thames Valley University , UK . Graham has been interested in Machine Translation since 1976 . Human Language Technologies ( HLT ) is a relatively new term that embraces a wide range of areas of research and development in the sphere of what used to be called Language Technologies or Language Engineering . The aim of this module is to familiarise the student with key areas of HLT , including a range of Natural Language Processing ( NLP ) applications . NLP is a general term used to describe the use of computers to process information expressed in natural ( i.e. human ) languages . The term NLP is used in a number of different contexts in this document and is one of the most important branches of HLT . There is a Special Interest Group in Language Processing , NLP SIG , within the EUROCALL professional association , and a Special Interest Group in Intelligent Computer Assisted Language Instruction ( ICALL ) within the CALICO professional association . Both have similar aims , namely to further research in a number of areas that are mentioned in this module , such as : . Artificial Intelligence ( AI ) . Computational Linguistics . Corpus - Driven and Corpus Linguistics . Formal Linguistics . Machine Aided Translation ( MAT ) . Machine Translation ( MT ) . Natural Language Interfaces . Natural Language Processing ( NLP ) . Theoretical Linguistics . All of the above are areas of research that have produced results which have proven , are proving and will prove very useful in the field of Computer Assisted Language Learning . Of course , this module can not teach you everything there is to know about HLT . This is neither necessary nor possible . The two main authors of this module are living proof of that ; they both started off as language teachers and then got interested in HLT . A multilingual CD - ROM titled A world of understanding was produced in 1998 on behalf of the Information Society and Media Directorate General of the European Commission under its former name , DGXIII . The aim of the CD - ROM was to demonstrate the importance of HLT in helping to realise the benefits of the Multilingual Information Society , in particular forming a review and record of the Language Engineering Sector of the Fourth Framework Programme of the European Union ( 1994 - 98 ) . 1.1 Introduction to HLT . [ ... ] there is no doubt that the development of tools ( technology ) depends on language - it is difficult to imagine how any tool - from a chisel to a CAT scanner - could be built without communication , without language . What is less obvious is that the development and the evolution of language - its effectiveness in communicating faster , with more people , and with greater clarity - depends more and more on sophisticated tools . ( European Commission : Language and technology 1996:1 ) . Language and technology lists the following examples of language technology ( using an admittedly broad understanding of the term ) : . photocopier ( p. 10 ) . laser printer ( p. 11 ) . fax machine ( p. 12 ) . desktop publishing ( p. 13 ) . scanner , modem ( p. 15 ) . electronic mail ( p. 16 ) . machine translation ( p. 17 ) . translator 's workbench ( p. 18 ) . tape recorder , database search engines ( p. 19 ) . telephone ( p. 25 ) . Many of these are already being used in language learning and teaching . The field of human language technology covers a broad range of activities with the eventual goal of enabling people to communicate with machines using natural communication skills . Research and development activities include the coding , recognition , interpretation , translation , and generation of language . [ ... ] Advances in human language technology offer the promise of nearly universal access to online information and services . Since almost everyone speaks and understands a language , the development of spoken language systems will allow the average person to interact with computers without special skills or training , using common devices such as the telephone . These systems will combine spoken language understanding and generation to allow people to interact with computers using speech to obtain information on virtually any topic , to conduct business and to communicate with each other more effectively . [ Source : Foreword to ( Cole 1997 ) ] . Facilitating and supporting all aspects of human communication through machines has interested researchers for a number of centuries . The use of mechanical devices to overcome language barriers was proposed first in the seventeenth century . Then , suggestions for numerical codes to be used to mediate between languages were made by Leibnitz , Descartes and others ( v. Hutchins 1986:21 ) . The beginnings of what we describe today as Human Language Technologies are , of course , closely connected to the advent of computers . ( i ) Various games , e.g. chess , noughts and crosses , bridge , poker ( ii ) The learning of languages ( iii ) Translation of languages ( iv ) Cryptography ( v ) Mathematics . Of these ( i ) , ( iv ) , and to a lesser extent ( iii ) and ( v ) are good in that they require little contact with the outside world . For instance in order that the machine should be able to play games its only organs need be ' eyes ' capable of distinguishing the various positions on a specially made board , and means for announcing its own moves . Mathematics should preferably be resticted to branches where diagrams are not much used . Of the above possible fields the learning of languages would be the most impressive , since it is the most human of these activities . This field sees however to depend too much on sense organs and locomotion to be feasible . ( Turing 1948:9 ) . Later on , Machine Translation enjoyed a period of popularity with researchers and funding bodies in the United States and the Soviet Union : . From 1956 onwards , the dollars ( and roubles ) really started to flow . Between 1956 and 1959 , no less than twelve research groups became established at various US universities and private corporations and research centres . Although linguists , language teachers and computer users today may find these predictions ridiculous , it was the enthusiasm and the work during this time that form the basis of many developments in HLT today . Research and development in HLT is nowadays more rapidly transferred into commercial systems than was the case up until the 1980s . Indeed HLT is becoming increasingly pervasive in our everyday lives . Here are some examples : . Machine Translation ( Section 3 ): There are many online translation systems that can be accessed free of charge , causing headaches for teachers whose students thought that they could save themselves time and who were blissfully unaware of the unreliability of their output ( Section 3.2 ) . Speech synthesis ( Section 4.1 ): Satellite navigation ( satnav ) devices for motor vehicles use systems that read out road numbers , street names and directions for the driver , and their output is surprisingly good . Speech recognition ( Section 4.2 ): If you make a telephone call to a customer support service you may hear a telephone recording that asks you to say a word or short phrase so that you can be connected to the appropriate department . Other previously unexpected areas of use are emerging . It is now , for instance , common for mobile phones to have what is known as predictive text input to aid the writing of short text messages . Instead of having to press one of the nine keys a number of times to produce the correct letter in a word , software in the phone compares users ' key presses to a linguistic database to determine the correct ( or most likely ) word . Most Internet search engines also now incorporate some kind of linguistic technology to enable users to enter a query in natural language , for example \\\" What is meant by log - likelihood ratio ? \\\" is as acceptable a query as simply \\\" log - likelihood ratio \\\" . What are the possible benefits for language teaching and learning of using HLT ? Here are some examples : . Teachers might want to preprocess a text to highlight certain grammatical phenomena or patterns . This can easily be done with a word - processor . Teachers might use part - of - speech taggers ( see Section 5 ) which could save them the trouble of having to manually annotate a text . Parsers available either on the Web or for local use on PCs can generate a graphical representation of sentence structure that may be useful for grammatical analysis for more advanced learners . Machine Translation ( MT ) has been the dream of computer scientists since the 1940s . The student 's attention is drawn in particular to the following publications , which provide a very useful introduction to MT : . Hutchins ( 1999 ) \\\" The development and use of machine translation systems and computer - based translation tools \\\" . Paper given at the International Symposium on Machine Translation and Computer Language Information Processing , 26 - 28 June 1999 , Beijing , China . 3.1 Machine Translation : a brief history . Initial work on Machine Translation ( MT ) systems was typified by what we would now consider to be a naive approach to the \\\" problem \\\" of natural language translation . Successful decoding of encrypted messages by machines during World War II led some scientists , most notably Warren Weaver , to view the translation process as essentially analogous with decoding . The concept of Machine Translation in the modern age can be traced back to the 1940s . Warren Weaver , Director of the Natural Sciences Division of the Rockefeller Foundation , wrote to his friend Norbert Wiener on 4 March 1947 - short ly after the first computers and computer programs had been produced : . Recognising fully , even though necessarily vaguely , the semantic difficulties because of multiple meanings , etc . , I have wondered if it were unthinkable to design a computer which would translate . Even if it would translate only scientific material ( where the semantic difficulties are very notably less ) , and even if it did produce an inelegant ( but intelligible ) result , it would seem to me worth while . When I look at an article in Russian , I say \\\" This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode \\\" . Have you ever thought about this ? As a linguist and expert on computers , do you think it is worth thinking about ? Cited in Hutchins ( 1997 ) . Weaver was possibly chastened by Wiener 's pessimistic reply : . I frankly am afraid the boundaries of words in different languages are too vague and the emotional and international connotations are too extensive to make any quasi - mechanical translation scheme very hopeful . But Weaver remained undeterred and composed his famous 1949 Memorandum , titled simply \\\" Translation \\\" , which he sent to some 30 noteworthy minds of the time . It posited in more detail the need for and possibility of MT . Thus began the first era of MT research . A direct system would comprise a bilingual dictionary containing potential replacements or target language equivalents for each word in the source language . A restriction of such MT systems was therefore that they were unidirectional and could not accommodate many languages unlike the systems that followed . Rules for choosing correct replacements were incorporated but functioned on a basic level ; although there was some initial morphological analysis prior to dictionary lookup , subsequent local re - ordering and final generation of the target text , there was no scope for syntactic analysis let alone semantic analysis ! Inevitably this often led to poor quality output , which certainly contributed to the severe criticism of MT in the 1966 Automatic Language Processing Advisory Committee ( ALPAC ) report which stated that it saw little use for MT in the foreseeable future . The damning judgment of the ALPAC report effectively halted research funding for machine translation in the USA throughout the 1960s and 1970s . We can say that both technical constraints and the lack of a linguistic basis hampered MT systems . The system developed at Georgetown University , Washington DC , and first demonstrated at IBM in New York in 1954 had no clear separation of translation knowledge and processing algorithms , making modification of the system difficult . In the period following the ALPAC report the need was increasingly felt for an approach to MT system design which would avoid many of the pitfalls of 1 G systems . By this time opinion had shifted towards the view that linguistic developments should influence system design and development . Indeed it can be said that the second generation ( 2 G ) of \\\" indirect \\\" systems owed much to linguistic theories of the time . 2 G systems can be divided essentially into \\\" interlingual \\\" and \\\" transfer \\\" systems . We will look first of all at interlingual systems , or rather those claiming to adopt an interlingual approach . Although Warren Weaver had put forward the idea of an intermediary \\\" universal \\\" language as a possible route to machine translation in his 1947 letter to Norbert Wiener , linguistics was unable to offer any models to apply until the 1960s . By virtue of its introduction of the concept of \\\" deep structure \\\" , Noam Chomsky 's theory of transformational generative grammar appeared to offer a route towards \\\" universal \\\" semantic representations and thus appeared to provide a model for the structure of a so - called interlingua . An interlingua is not a natural language , rather it can be seen as a meaning representation which is independent of both the source and the target language of translation . An interlingua system maps from a language 's surface structure to the interlingua and vice versa . A truly interlingual approach to system design has obvious advantages , the most important of which is economy , since an interlingual representation can be applied for any language pair and facilitates addition of other language pairs without major additions to the system . The next section looks at \\\" transfer \\\" systems . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . ( For n languages the number of transfer modules required would be n ( n -1 ) or n ( n -1 ) /2 if the modules are reversible ) . An important advance in 2 G systems when compared to 1 G was the separation of algorithms ( software ) from linguistic data ( lingware ) . In a system such as the Georgetown model the program mixed language modelling , translation and the processing thereof in one program . This meant that the program was monolithic and it was easy to introduce errors when trying to rectify an existing shortcoming . The move towards separating software and lingware was hastened by parallel advances in both computational and linguistic techniques . The adoption of linguistic formalisms in the design of systems and the development of high level programming languages enabled MT workers to code in a more problem - oriented way . The development in programming languages meant that it was becoming ever easier to code rules for translation in a meaningful manner and arguably improved the quality of these rules . The declarative nature of linguistic description could now be far more explicitly reflected in the design of programs for MT . . Early MT systems were predominantly parser - based , one of the first steps in such a system being to parse and tag the source language : see Section 5 on Parsing and Tagging . More recent current approaches to MT rely less on formal linguistic descriptions than the transfer approach described above . Translation Memory ( TM ) systems are now in widespread commercial use : see below and Chapter 10 of Arnold et al . ( 1994 ) . Example - Based Machine Translation ( EBMT ) is a relatively new technology which aims to combine both traditional MT and more recent TM paradigms by reusing previous translations and applying various degrees of linguistic knowledge to convert fuzzy matches into exact ones : see the Wikipedia article on EBMT . However , some early definitions of EBMT refer to what is now known as TM and they often exclude the concept of fuzzy matches . Essentially , Google Translat e begins by examining and comparing massive corpora of texts on the Web that have already been translated by human beings . It looks for matches between source and target texts and uses complex statistical analysis routines to look for statistically significant patterns , i.e. it works out the rules of the interrelationships between source and target texts for itself . As more and more corpora are added to the Web this means that Google Translate will keep improving until it reaches a point where it will be very difficult to tell that a machine has done the translation . I remember early machine translation tools translating \\\" Wie geht es dir ? \\\" as \\\" How goes it you ? \\\" Now Google Translate gets it right : \\\" How are you ? \\\" Thus we have , in a sense , come full circle in that Weaver 's ideas of applying statistical techniques are seen as a fruitful basis for MT . . 3.2 Commercial MT packages . There are many automatic translation packages on the market - as well as free packages on the Web . While such packages may be useful for extracting the gist of a text they should not be seen as a serious replacement for the human translator . Some are not all that bad , producing translations that are half - intelligible , letting you know whether a text is worth having translated properly . See : . Professional human translators are making increasing use of Translation Memory ( TM ) packages . TM packages store texts that have previously been translated , together with their source texts , in a large database . Chunks of new texts to be translated are then matched against the translated texts in the database and suggested translations are offered to the human translator wherever a match is found . The human translator has to intervene regularly in this process of translation , making corrections and amendments as necessary . TM systems can save hours of time ( estimated at up to 80 % of a translator 's time ) , especially when translating texts that are repetitive or that use lots of standard phrases and sentence formulations . Producing updates of technical manuals is a typical application of TM systems . Examples of TM systems include : . An example of automatic translations can be found at the Newstran website . This site is extremely useful for locating newspapers in a wide range of languages . You can also locate selected newspapers that have been translated using a Machine Translation system . Another approach to translation is the stored phrase bank , for example LinguaWrite , which was aimed at the business user and contained a large database of equivalent phrases and sentences in different languages to facilitate the writing of business letters . LinguaWrite was programmed by Marco Bruzzone in the 1980s and marketed by Camsoft , but it is no longer available and has not been updated . David Sephton 's Tick - Tack ( Primrose Publishing ) adopted a similar approach , beginning as a package consisting of \\\" building blocks \\\" of language for business communication , but it now embraces other topics . 3.3 Just for fun . 3.3.1 Some apocryphal stuff . The following examples have often been cited as mistakes made by machine translation ( MT ) systems . Whether they are real examples or not can not be verified . Russian - English : In a technical text that had been translated from Russian into English the term water sheep kept appearing . When the Russian source text was checked it was found that it was actually referring to a hydraulic ram . Russian - English : Idioms are often a problem . Russian - English : Another example , similar to the one above , is where out of sight , out of mind ended up being translated as the equivalent of blind and stupid . MT systems do , however , often make mistakes . The Systran MT system , which has been used by the European Commission , translated the English phrase pregnant women and children into des femmes et enfants enceints , which implies that both the women and the children are pregnant . Although it is an interpretation of the original phrase that is theoretically possible , it is also clearly wrong . 3.3.2 Translations of nursery rhymes . Try using an online machine translator to translate a text from English into another language and then back again . The results are often amusing , especially if you are translating nursery rhymes ! ( i ) Bah , bah , black sheep translated into French and then back again into English , using Babel Fish . English source text : Bah , bah , black sheep , have you any wool ? Yes sir , yes sir , three bags full . One for the master , one for the dame , and one for the little boy who lives down the lane . French translation : Bah , bah , mouton noir , vous ont n'importe quelles laines ? Oui monsieur , oui monsieur , trois sacs compl\\u00e8tement . Un pour le ma\\u00eetre , un pour dame , et un pour le petit gar\\u00e7on qui vit en bas de la ruelle . And back into English again : Bah , bah , black sheep , have you n ' imports which wools ? Yes Sir , yes Sir , three bags completely . For the Master , for lady , and for the little boy who lives in bottom of the lane . ( ii ) Humpty Dumpty translated into Italian and then back again into English , using Babel Fish . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty si \\u00e8 seduto su una parete . Humpty Dumpty ha avuto una grande caduta . I cavalli di tutto il re e gli uomini di tutto il re non hanno potuto un Humpty ancora . And back into English again : Humpty Dumpty has been based on a wall . Humpty Dumpty has had a great fall . The horses of all the king and the men of all the king have not been able a Humpty still . ( iii ) Humpty Dumpty translated into Italian and then back again into English , using Google Translate . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty sedeva su un muro . Humpty Dumpty ha avuto un grande caduta . Tutti i cavalli del re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme . And back into English again : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . All the king 's horses and all the king 's men could not put Humpty together again . Now , the above is an interesting result ! Google Translate used to be a very unreliable MT tool . It drives language teachers mad , as their students often use it to do their homework , e.g. translating from a given text into a foreign language or drafting their own compositions and then translating them . Mistakes made by Google Translate used to be very easy to spot , but ( as indicated above in Section 3.1 ) Google changed its translation engine a few years ago and now uses a Statistical Machine Translation ( SMT ) approach . The Humpty Dumpty translation back into English from the Italian appears to indicate that Google Translate has matched the whole text and got it right . Clever ! 3.3.3 Translations of business and journalistic texts . ( i ) A business text , translated with Google Translate and Babel Fish . Google Translate was used to translate the following text from German into English : Die Handelskammer in Saarbr\\u00fccken hat uns Ihre Anschrift zur Verf\\u00fcgung gestellt . Wir sind ein mittelgro\\u00dfes Fachgesch\\u00e4ft in Stuttgart , und wir spezialisieren uns auf den Verkauf von Personalcomputern . This was rendered as : The Chamber of Commerce in Saarbr\\u00fccken has provided us your address is available . We are a medium sized shop in Stuttgart , and we specialize in sales of personal computers . Babel Fish produced a better version : The Chamber of Commerce in Saarbruecken put your address to us at the disposal . We are a medium sized specialist shop in Stuttgart , and we specialize in the sale of personal computers . ( ii ) A journalistic text , translated with Google Translate and Babel Fish . Die deutsche Exportwirtschaft k\\u00e4mpft mit der weltweiten Konjunkturflaute und muss deshalb von den Zeiten zweistelligen Wachstums Abschied nehmen . [ Ludolf Wartenberg vom Bundesverband der Deutschen Industrie ] . This was rendered by Google Translate as : The German export economy is struggling with the global downturn and must therefore take the times of double - digit growth goodbye . [ Ludolf Wartenberg from the Federation of German Industry ] . The German export trade and industry fights with the world - wide recession and must take therefore from the times of two digit growth parting . [ Ludolf waiting mountain of the Federal association of the German industry . ] Computers are normally associated with two standard input devices , the keyboard and the mouse , and two standard output devices , the display screen and the printer . All these restrict language input and output . However , computer programs and hardware devices that enable the computer to handle human speech are now commonplace . All modern computers allow the user to plug in a microphone and record his / her own voice . A variety of other sources of sound recordings can also be used . Storing these sound files is not a problem anymore as a result of the immensely increased capacity and reduced cost of storage media and improved compression techniques that enable the size of sound files to be substantially reduced . For further information on the applications of sound recording and playback technology to CALL see Module 2.2 , Introduction to multimedia CALL . A range of computer software is available for speech analysis . Spoken input can be analysed according to a wide variety of parameters and the analysis can be represented graphically or numerically . Of course , graphic output is not immediately useful for the uninitiated viewer , and hence we are not arguing that this kind of graphical representation will prove useful to the language learner . On the other hand , specialists are well capable of interpreting this speech analysis data . The information we get from speech analysis has proven very valuable indeed for speech synthesis and speech recognition , which are dealt with in the following two sections . 4.1 Speech synthesis . Speech synthesis describes the process of generating human - like speech by computer . Producing natural sounding speech is a complex process in which one has to consider a range of factors that go beyond just converting characters to sounds because very often there is no one - to - one relation between them . The intonation of particular sentence types and the rhythm of particular utterances also have to be considered . Currently speech synthesis is far more advanced and more robust than speech recognition ( see Section 4.2 below ) . The naturalness of artificially produced utterances is now very impressive compared to what used to be produced by earlier speech synthesis systems in which the intonation and timing were far from natural and resulted in the production of monotonous , robot - like speech . Many people are now unaware that so - called talking dictionaries use speech synthesis software rather than recordings of human voices . In - car satellite navigation ( satnav ) systems can produce a range of different types of human voices , both male and female in a number of different languages , and \\\" talk \\\" to the car driver guiding him / her to a chosen destination . So far , however , speech synthesis has not been as widely used in CALL as speech recognition . This is probably due to the fact that language teachers ' requirements regarding the presentation of spoken language are very demanding . Anything that sounds artificial is likely to be rejected . Some language teachers even reject speakers whose regional accent is too far from what is considered standard or received pronunciation . There is , however , a category of speech synthesis technology known as Text To Speech ( TTS ) technology that is widely used for practical purposes . TTS software falls into the category of assistive technology , which has a vital role in improving accessiblity for a wide range of computer users with special needs , which is now governed by legislation in the UK . The Special Educational Needs and Disability Act ( SENDA ) of 2001 covers educational websites and obliges their designers \\\" to make reasonable adjustments to ensure that people who are disabled are not put at a substantial disadvantage compared to people who are not disabled . \\\" See JISC 's website on Disability Legislation and ICT in Further and Higher Education - Essentials . See the Glossary for definitions of assistive technology and accessiblity . TTS is important in making computers accessible to blind or partially sighted people as it enables them to \\\" read \\\" from the screen . TTS technology can be linked to any written input in a variety of languages , e.g. automatic pronunciation of words from an online dictionary , reading aloud of a text , etc . These are examples of TTS software : . Festival Speech Synthesis System : From the Centre for Speech Technology Research at the University of Edinburgh . Festival offers a general framework for building speech synthesis systems as well as including examples of various modules . Just for fun I entered the phrase \\\" Pas d'elle yeux Rh\\u00f4ne que nous \\\" into a couple of French language synthesisers . It 's a nonsense sentence in French but it comes out sounding like a French person trying to pronounce a well - known expression in English . Try it ! There are also Web - based tools that enable you to create animated cartoons or movies incorporating TTS , for example : . Voki enables you to create and customise your own speaking cartoon character . You can choose the TTS option ( as in Graham Davies 's example on the right ) to give the character a voice , or you can record your own voice . ReadTheWords : A tool that works in much the same way as Voki , but without the option of recording one 's own voice . An excellent tool that helps people with hearing impairments to learn how to articulate is the CSLU Speech Toolkit . To what extent speech synthesis systems are suitable for CALL is a matter for further discussion . See the article by Handley & Hamel ( 2005 ) , who report on their progress towards the development of a benchmark for determining the adequacy of speech synthesis systems for use in CALL . The article mentions a Web - based package called FreeText , for advanced learners of French , the outcome of a project funded by the European Commission . 4.2 Speech recognition . Speech recognition describes the use of computers to recognise spoken words . Speech recognition has not reached such a high level of performance as speech synthesis ( see Section 4.1 above ) , but it has certainly become usable in CALL in recent years . EyeSpeak English is a typical example of the use of speech recognition software for helping students improve their English pronunciation . Speech recognition is a non - trivial task because the same spoken word does not produce entirely the same sound waves when uttered by different people or even when uttered by the same person on different occasions . The process is complex : the computer has to digitise the sound , transform it to discard unneeded information , and then try to match it with words stored in a dictionary . The most efficient speech recognition systems are speaker - dependent , i.e. they are trained to recognise a particular person 's speech and can then distinguish thousands of words uttered by that person . If one remembers that each of the parameters analysed could have been affected by some speaker - independent background noise or by some idiosyncratic pronunciation features of this particular speaker then it already becomes clear how difficult the interpretation of the analysis data is for a speech recognition program . The following information is taken from an article written by Norman Harris of DynEd , a publisher of CALL software incorporating ASR : . Speech recognition technology has finally come of age - at least for language training purposes for young adults and adults . The essence of real language is not in discrete single words - language students need to practice complete phrases and sentences in realistic contexts . Moreover , programs which were trained to accept a speaker 's individual pronunciation quirks were not ideally suited to helping students move toward more standard pronunciation . These technologies also failed if the speaker 's voice changed due to common colds , laryngitis and other throat ailments , rendering them useless until the speaker recovered or retrained the speech engine . The solution to these problems came with the development of continuous speech recognition engines that were speaker independent . These programs are able to deal with complete sentences spoken at a natural pace , not just isolated words . Such flexibility with regard to pronunciation paradigms means that today 's speaker - independent speech recognition programs are not ideal for direct pronunciation practice . Nonetheless , exercises which focus on fluency and word order , and with native speaker models which are heard immediately after a student 's utterance had been successfully recognized , have been shown to indirectly result in much improved pronunciation . Another trade off is that the greater flexibility and leniency which allows these programs to \\\" recognize \\\" sentences spoken by students with a wide variety of accents , also limits the accuracy of the programs , especially for similar sounding words and phrases . Some errors may be accepted as correct . Native speakers testing the \\\" understanding \\\" of programs \\\" tuned \\\" to the needs of non - native speakers may be bothered by this , but most teachers , after careful consideration of the different needs and psychologies of native speakers and learners , will accept the trade off . Students do not expect to be understood every time . If they are required occasionally to repeat a sentence which the program has not recognized or which the program has misinterpreted , there may be some small frustration , but language students are much more likely to take this in their stride than would native speakers . On the other hand , if the program does \\\" understand \\\" such students , however imperfect their pronunciation , they typically experience a huge sense of satisfaction , a feel good factor native speakers simply can not enjoy to anywhere near the same degree . The worst thing for a student is a program that is too demanding of perfection - such programs will quickly lead to student frustration or the kind of embarrassed , hesitant unwillingness to speak English typical of many classrooms . Even if we accept that accuracy needs to be responsive to proficiency in order to encourage students to speak , we must , as teachers , be concerned that errors do not become reinforced . A recent breakthrough is the implementation of apps such as Apple 's Siri on the iPhone 4S and Evi , which is available for the iPhone and the Android . These apps are quite impressive at recognising speech and providing answers to questions submitted by the user . Evi 's performance was tested by the author of this paragraph . \\\" She \\\" immediately provided correct answers to these questions submitted by voice input : . In which American state is Albuquerque ? In addition , Evi may link to relevant websites that provide further information . Text input is also accepted . In this section we outline the essentials of parsing , first of all by describing the components of a parsing system and then discussing different kinds of parser . We look at one linguistic phenomenon which causes problems for parsing and finally examine potential solutions to the difficulties raised by parsing . Put in simple terms , a parser is a program that maps strings of a language into its structures . The most basic components needed by a parser are a lexicon containing words that may be parsed and a grammar , consisting of rules which determine grammatical structures . The first parsers were developed for the analysis of programming languages ; obviously as artificial , regular languages they present fewer problems than a natural language . It is most useful to think of parsing as a search problem which has to be solved . It can be solved using an algorithm which can be defined as : . [ ... ] a formal procedure that always produces a correct or optimal result . An algorithm applies a step - by - step procedure that guarantees a specific outcome or solves a specific problem . The procedure of an algorithm performs a computation in a finite amount of time . Programmers specify the algorithm the program will follow when they develop a conventional program . ( Smith 1990 ) . Parsing algorithms define a procedure that looks for the optimum combination of grammatical rules that generate a tree structure for the input sentence . How might we define these grammatical rules in a concise way that is amenable to computer processing ? A useful construct for our purposes is a so - called context - free grammar ( CFG ) . A CFG consists of rules containing a single symbol on the left - hand side and one or more on the right - hand side . For example , the statement that a sentence can consist of : . a noun phrase and a verb phrase can be expressed by the following rewrite rule . S \\u00ae NP VP . This means that a sentence S can be ' rewritten ' as a noun phrase NP followed by a verb phrase VP which are in their turn defined in the grammar . A noun phrase , for example , can consist of a determiner DET and a noun N. These symbols are known as non - terminals and the words represented by these symbols are terminal symbols . Parsing algorithms can proceed top - down or bottom - up . In some cases , top - down and bottom - up algorithms can be combined . Below are simple descriptions of two parsing strategies . 5.1.1 Top Down ( depth first ) . Top down strategy works from non - terminal symbols : . S \\u00ae NP VP . and then breaks them down into constituents . The strategy assumes we have an S and tries to fit it in . If we choose to search depth first , then we proceed down one side of the tree at a time . The search will end successfully if it manages to break down the sentence into all its terminal symbols ( words ) . 5.1.2 Bottom up ( breadth first ) . A bottom up strategy looks at elements of an S and assigns categories to them to form larger constituents until we arrive at an S. If we choose to search breadth first , then we proceed consecutively through each layer and stop successfully once we have constructed a sentence . Let 's look now at one linguistic phenomenon which causes problems for parsers - that of so - called attachment ambiguity . Consider the following sentence : . The man saw the man in the park with a telescope . Parser output can be represented as a bracketed list or , more commonly , a tree structure . Here is the output of two possible parses for the sentence above . One way of dealing with the problem of sentences which have more than one possible parse is to concentrate on specific elements of the parser input and to not deal with such phenomena as attachment ambiguity . Ideally we expect a parser to successfully analyse a sentence on the basis of its grammar , but often there are problems caused by errors in the text or incompleteness of grammar and lexicon . Also the length of sentences and ambiguity of grammars often make it hard to successfully parse unrestricted text . An approach which addresses some of these issues is partial or shallow parsing . Abney ( 1997:125 ) succinctly describes partial parsing thus : . \\\" Partial parsing techniques aim to recover syntactic information efficiently and reliably from unrestricted text , by sacrificing completeness and depth of analysis . \\\" Partial parsers concentrate on recovering pieces of sentence structure which do not require large amounts of information ( such as lexical association information ) ; attachment remains unresolved for instance . We can see that in this way parsing efficiency is greatly improved . Another strategy for analysing language is part - of - speech tagging , in which we do not seek to find larger structures such as noun phrases but instead label each word in a sentence with its appropriate part of speech . Here is the original paragraph from Section 3 of this document : . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . The following table shows the tagger output , and we can see that most of the words have been correctly identified . additional . languages . NNS . language . required . VBN . require . . . SENT . . . As with partial parsing , we are not trying to find correct attachments and since it is a limited task the success rate is quite high . The information derived from tagging can itself have input into partial parsing or into improving the performance of traditional parsers . Some of the decision task as to what is the correct part of speech to assign to a word is based on the probability of two or three word sequences ( bigrams and trigrams ) occurring , even where words can be assigned more than one part of speech . For instance , in our example tagged text the sequence ' the transfer module ' occurs . Transfer is of course also a verb , but the likelihood of a determiner ( the ) being followed by a verb is lower than the likelihood of a determiner noun sequence . See als o the Visual Interactive Syntax Learning ( VISL ) website . An online parser and a variety of other tools concerned with English grammar , including games and quizzes , can be found here . 5.1.3 Parsing erroneous input . Of course , in CALL we are dealing with texts that have been produced by language learners at various levels of proficiency and accuracy . It is therefore reasonable to assume that the parser has to be prepared to deal with linguistic errors in the input . One thing we could do is to complement our grammar for correct sentences with a grammar of incorrect sentences - an error grammar , i.e. we capture individual and/or typical errors in a separate rule system . The advantage of this error grammar approach is that the feedback can be very specific and is normally fairly reliable because this feedback can be attached to a very specific rule . The big drawback , however , is that individual learner errors have to be anticipated in the sense that each error needs to be covered by an adequate rule . However , as already stated it is not only in texts that have been produced by language learners that we find erroneous structures . Machine translation is facing similar problems . Dina & Malnati review approaches \\\" concerning the design and the implementation of grammars able to deal with ' real input ' . \\\" ( Dina & Malnati 1993:75 ) . They list four approaches : . The rule - based approach which relies on two sets of rules : one for grammatical input and the other for ungrammatical input . Dina & Malnati point out quite rightly that normally well - formedness conditions should be sufficient and the second set of rules results in linguistic redundancy . The main problem with using this approach in a parser - based CALL system is the problem of having to anticipate the errors learners are likely to make . The metarule - based approach uses a set of well - formedness rules and if none of them can be applied calls an algorithm that relaxes some constraints and records the kind of violation . Dina & Malnati note the procedurality of the algorithm causes problems when confronted with multiple errors - something very likely in any text produced by a language learner . The preference - based approach comprises an overgenerating grammar and a set of preference rules . \\\" [ ... ] each time a formal condition is removed from a b - rule to make its applicability context wider , a preference rule must be added to the grammar . Such a p - rule must be able to state - in the present context of the b - rule - the condition that has been previously removed . \\\" ( Dina & Malnati 1993:78 ) Again a source for linguistic redundancy which might result in inconsistencies in the grammar . They claim that , due to the overgeneration of possible interpretations , \\\" the system would be completely unusable in an applied context . \\\" ( ibid.:79 ) . The constraint - based approach is based on the following assumptions : . each ( sub)tree is marked by an index of error ( initially set to 0 ) ; . the violation of a constraint in a rule does not block the application , but increases the error index of the generated ( sub)tree ; . at the end of parsing the object marked by the smallest index is chosen . Consequently , the \\\" most plausible interpretation of a [ ... ] sentence is the one which satisfies the largest number of constraints . \\\" ( Dina & Malnati 1993:80 ) . We have seen in Section 3 that Machine Translation ( MT ) and the political and scientific interest in machine translation played a significant role in the acceptance ( or non - acceptance ) as well as the general development of Human Language Technologies . By 1964 , however , the promise of operational MT systems still seemed distant and the sponsors set up a committee , which recommended in 1966 that funding for MT should be reduced . It brought to an end a decade of intensive MT research activity . ( Hutchins 1986:39 ) . It is then perhaps not surprising that the mid-1960s saw the birth of another discipline : Computer Assisted Language Learning ( CALL ) . The PLATO project , which was initiated at the University of Illinois in 1960 , is widely regarded as the beginning of CALL - although CALL was just part of this huge package of general Computer Assisted Learning ( CAL ) programs running on mainframe computers . PLATO IV ( 1972 ) was probably the version of this project that had the biggest impact on the development of CALL ( Hart 1995 ) . At the same time , another American university , Brigham Young University , received government funding for a CALL project , TICCIT ( Time - Shared Interactive , Computer Controlled Information Television ) ( Jones 1995 ) . Other well - known and still widely used programs were developed soon afterwards : . The CALIS / WinCALIS ( Computer Aided Language Instruction System ) authoring tools , Duke University ( Borchardt 1995 ) . The TUCO package for learners of German , developed by Heimy Taylor and Werner Haas , Ohio State University . See Module 3.2 , Section 5.9 . The CLEF package for learners of French , which was produced by a consortium of Canadian universities in the late 1970s and is still going strong today . See Module 3.2 , Section 5.9 . In the UK , John Higgins developed Storyboard in the early 1980s , a total Cloze text reconstruction program for the microcomputer ( Higgins & Johns 1984:57 ) . ( Levy 1997:24 - 25 ) describes how other programs extended this idea further . See Section 8.3 , Module 1.4 , headed Total text reconstruction : total Cloze , for further information on total Cloze programs . In recent years the development of CALL has been greatly influenced by the technology and by our knowledge of and our expertise in using it , so that not only the design of most CALL software , but also its classification has been technology - driven . For example , Wolff ( 1993:21 ) distinguished five groups of applications : . The late 1980s saw the beginning of attempts which are mostly subsumed under Intelligent CALL ( ICALL ) , a \\\" mix of AI [ Artificial Intelligence ] techniques and CALL \\\" ( Matthews 1992b : i ) . Early AI - based CALL was not without its critics , however : . And that , fundamentally , is why my initial enthusiasm has now turned so sour . ( Last 1989:153 ) . For a more up - to - date and positive point of view of Artifical Intelligence , see Dodigovic ( 2005 ) . Bowerman ( 1993:31 ) notes : \\\" Weischedel et al . ( 1978 ) produced the first ICALL [ Intelligent CALL ] system which dealt with comprehension exercises . It made use of syntactic and semantic knowledge to check students ' answers to comprehension questions . \\\" As far as could be ascertained , this was just the early swallow that did not create a summer . Kr\\u00fcger - Thielmann ( 1992:51ff . ) lists and summarises the following early projects in ICALL : ALICE , ATHENA , BOUWSTEEN & COGO , EPISTLE , ET , LINGER , VP2 , XTRA - TE , Zock . Matthews ( 1993:5 ) identifies Linguistic Theory and Second Language Acquisition Theory as the two main disciplines which inform Intelligent CALL and which are ( or will be ) informed by Intelligent CALL . He adds : \\\" the obvious AI research areas from which ICALL should be able to draw the most insights are Natural Language Processing ( NLP ) and Intelligent Tutoring Systems ( ITS ) \\\" ( Matthews1993:6 ) . Matthews shows that it is possible to \\\" conceive of an ICALL system in terms of the classical ITS architecture \\\" ( ibid . ) The system consists of three modules - expert , student and teacher module - and an interface . The expert module is the one that \\\" houses \\\" the language knowledge of the system . It is this part which can process any piece of text produced by a learner - in an ideal system . This is usually done with the help of a parser of some kind : . ( Holland et al . 1993:28 ) . This notion of parser - based CALL not only captures the nature of the field much better than the somewhat misleading term \\\" Intelligent CALL \\\" ( Is all other CALL un - intelligent ? ) , it also identifies the use of Human Language Technologies as one possible approach in CALL alongside others such as multimedia - based CALL and Web - based CALL and thus identifies parser - based CALL as one possible way forward for CALL . In some cases , the ( technology - defined ) borders between these sub - fields of CALL are not even clearly identifiable , as we will see in some of the projects mentioned in the following paragraphs . To exemplify recent advances in the use of sophisticated human language technology in CALL , let us have a look at some of the projects that were presented at two conferences in the late 1990s . The first one is the Language Teaching and Language Technology conference in Groningen in 1997 ( Jager et al . 1998 ) . Witt & Young ( 1998 ) , on the other hand , are concerned with assessing pronunciation . They implemented and tested a pronunciation scoring algorithm which is based on speech recognition ( see Section 4.2 ) and uses hidden Markov models . \\\" The results show that - at least for this setup with artificially generated pronunciation errors - the GOP [ goodness of pronunciation ] scoring method is a viable assessment tool . \\\" A third paper on pronunciation at this conference , by Skrelin & Volskaja ( 1998 ) outlined the use of speech synthesis ( see Section 4.1 ) in language learning and lists dictation , distinction of homographs , a sound dictionary and pronunciation drills as possible applications . \\\" The project vision foresees two main areas where GLOSSER applications can be used . First , in language learning and second , as a tool for users that have a bit of knowledge of a foreign language , but can not read it easily or reliably \\\" ( Dokter & Nerbonne 1998:88 ) . Dokter & Nerbonne report on the French - Dutch demonstrator running under UNIX . The demonstrator : . uses morphological analysis to provide additional grammatical information on individual words and to simplify dictionary look - up ; . relies on automatic word selection ; . offers the opportunity to insert glosses ( taken form the dictionary look - up ) into the text ; . relies on string - based word sense disambiguation ( \\\" Whenever a lexical context is found in the text that is also provided in the dictionary , the example in the dictionary is highlighted . \\\" ( op.cit.:93 ) . Roosma & Pr\\u00f3sz\\u00e9ky ( 1998 ) draw attention to the fact that GLOSSER works with the following language pairs : English - Estonian - Hungarian , English - Bulgarian , French - Dutch and describe a demonstrator version running under Windows . Dokter et al ( 1998 ) conclude in their user study \\\" that Glosser - RuG improves the ease with which language students can approach a foreign language text \\\" ( Dokter et al . 1998:175 ) . The latter project relies on a spellchecker , morphological analyser , syntactic parser and a lexical database for Basque , and the authors report on the development of an interlanguage model . At another conference ( UMIST , May 1998 ) , which brought together a group of researchers who are exploring the use of HLT in CALL software , Schulze et al . ( 1999 ) and Tschichold ( 1999 ) discussed strategies for improving the success rate of grammar checkers . Menzel & Schr\\u00f6der ( 1999 ) described error diagnosis in a multi - level representation . The demonstration system captures the relations of entities in a simple town scenery . The available syntactic , semantic and pragmatic information is checked simultaneously for constraint violations , i.e. errors made by the language learners . Visser ( 1999 ) introduced CALLex , a program for learning vocabulary based on lexical functions . Diaz de Ilarraza et al . ( 1999 ) described aspects of IDAZKIDE , a learning environment for Spanish learners of Basque . The program contains the following modules : wide - coverage linguistic tools ( lexical database with 65,000 entries ; spell checker ; a word form proposer and a morphological analyser ) , an adaptive user interface and a student modelling system . The model of the students ' language knowledge , i.e. their interlanguage , is based on a corpus analysis ( 300 texts produced by learners of Basque ) . Foucou & K\\u00fcbler ( 1999 ) presented a Web - based environment for teaching technical English to students of computing . Ward et al . ( 1999 ) showed that Natural Language Processing techniques combined with a graphical interface can be used to produce meaningful language games . Davies & Poesio ( 1998 ) reported on tests of simple CALL prototypes that have been created using CSLUrp , a graphical authoring system for the creation of spoken dialogue systems . They argue that since it is evident that today 's dialogue systems are usable in CALL software , it is now possible and necessary to study the integration of corrective feedback in these systems . Mitkov ( 1998 ) outlined early plans for a new CALL project , The Language Learner 's Workbench . It is the aim of this project to incorporate a number of already available HLT tools and to package them for language learners . These examples of CALL applications that make use of Human Language Technologies are by no means exhaustive . They not only illustrate that research in HLT in CALL is vibrant , but also that HLT has an important contribution to make in the further development of CALL . Of course , both disciplines are still rather young and many projects in both areas , CALL and HLT , have not even reached the stage of the implementation of a fully functional prototype yet . A number of CALL packages that make use of speech recognition have reached the commercial market and are being used successfully by learners all over the world ( see Section 4.2 ) . Speech synthesis , certainly at word level , has achieved a clarity of pronunciation that makes it a viable tool for language learning ( see Section 4.1 ) . Many popular electronic dictionaries now incorporate speech synthesis systems . Part - of - speech taggers have reached a level of accuracy that makes them usable in the automatic pre - processing of learner texts . Morphological analysers for a number of languages automatically provide grammatical information on vocabulary items in context and make automatic dictionary look - ups of inflected or derived word forms possible . This progress in HLT and CALL has mainly been possible as the result of our better understanding of the structures of language - our understanding of linguistics . The lack of linguistic modelling and the insufficient deployment of Natural Language Processing techniques has sometimes been given as one reason for the lack of progress in some areas of CALL : see , for example , Levy ( 1997:3 ) , citing Kohn ( 1994 ) . [ ... ] Kohn suggests that current CALL is lacking because of poor linguistic modelling , insufficient deployment of natural language processing techniques , an emphasis on special - purpose rather than general - purpose technology , and a neglect of the ' human ' dimnesion of CALL ( Kohn 1994:32 ) . The examples in the previous section have shown that it is possible to apply certain linguistic theories ( e.g. phonology and morphology ) to Human Language Technologies and implement this technology in CALL software . This is , of course , true . However , it does not mean that interesting fragments or aspects of a given language can not be captured by a formal linguistic theory and hence implemented in a CALL application . In other words , if one can not capture the German language in its entirety in order to implement this linguistic knowledge in a computer program , this does not mean that one can not capture interesting linguistic phenomena of that language . This means even if we are only able to describe a fragment of a given language adequately we can still make very good use of this description in computer applications for language learning . What is the kind of knowledge we ought to have about language before we can attempt to produce an HLT tool that can be put to effective use in CALL ? Let us look at one particular aspect of language - grammar . In recent years , the usefulness of conscious learning of grammar has been discussed time and again , very often in direct opposition to what has been termed \\\" the communicative approach \\\" . ( ibid.:6 ) This assumption leads to the question of what role exactly the computer ( program ) has to play in a sensitive , rich and enjoyable grammar - learning process . The diversity of approaches outlined in this special issue of ReCALL on grammar illustrates that there are many different roads to successful grammar learning that will need to be explored . In this module , only the example of parser - based CALL will be discussed . Let us take a grammar checker for language learners as a specific example in point . This grammar checker could then be integrated into a CALL program , a word - processor , an email editor , a Web page editor etc . The design of such a grammar checker is mainly based on findings in theoretical linguistics and second language acquisition theory . Let us start with second language acquisition theory . Research in second language acquisition has proved that grammar learning can lead to more successful language acquisition . Here learners have the opportunity to correct grammatical errors and mistakes that they have made while concentrating on the subject matter and the communicative function of the text . It is at this stage that a grammar checker for language learners can provide useful and stimulating guidance . In order to ascertain the computational features of such a grammar checker , let us first consider what exactly we mean by \\\" grammar \\\" in a language learning context . Helbig discusses possible answers to this question from the point of view of the teaching and learning of foreign languages in general : . As a starting point for answering our question concerning the relevance ( and necessity ) of grammar in foreign language teaching we used a differentiation of what was and is understood by the term \\\" grammar \\\" : . Grammar A : the system of rules that is inherent to the object language itself and is independent of the fact whether it has been captured by Linguistics or not ; . Grammar B : the scientific - linguistic description of the language inherent system of rules , the modelling of Grammar A by Linguistics ; . Grammar C : the system of rules intern to the speaker and listener which is formed in the head of the learner during language acquisition and which forms the basis for him / her to produce and understand correct sentences and texts and to use them appropriately in communication . ( Helbig:1975 ) . Helbig identifies further a Grammar B1 and a Grammar B2 - the former being a linguistic grammar and the latter being a learner grammar . The description of grammar B1c is a literal translation of Helbig 's wording - in the terminology used now , the term \\\" interlanguage \\\" appears to be the most appropriate . The application of Helbig 's grammar classification to CALL produces the following results : . Grammar A remains as defined by Helbig . In other words it refers to the target grammar of the interlanguage continuum . Grammar B1a is the grammar which enables the parser to process grammatically well - formed sentences in the target language . Grammars B1b , B1c and B2 enable the grammar checking CALL tool to detect errors in the learner input and provide the linguistic information to generate feedback . Grammar C is the grammar system which the CALL tool should help to correct and expand . Additionally , the grammar checker will gather data for learner profiles which should allow useful insights into the development of Grammar C of learners you have used the program . Consequently , Grammar B in its entirety and Grammar C will have to be considered first and foremost when developing the grammar checker . The question then arises : If Grammar A provides the linguistic data for the parser developer , how can we \\\" feed \\\" these different grammars into a computer program ? The computer requires that any grammar which we intend to use in any program ( or programming language , for that matter ) be mathematically exact . Grammars which satisfy this condition are normally referred to as formal grammars . The mathematical description of these grammars uses set theory . Therefore , a language L is said to have a vocabulary V . If there were no restrictions on how to construct strings , the number of possible strings is infinite . This becomes clear when one considers that each vocabulary item of V could be repeated infinitely in order to construct a string . However , as language learners in particular know any language L adheres to a finite set of ( grammar ) rules . This explains why grammar teaching software that attempts to anticipate possible incorrect answers can only do this successfully if the answer domain is severely restricted and the anticipation process will therefore much simpler . Could a computer program perform this task - a task based on infinite possibilities ? Yes , it could - but not based on infinite possibilities . That is why it will be necessary to look for an approach which is based on a finite set of possibilities , which can then be pre - programmed . Let us therefore consider L the set of strings that can be constructed using the ( formal ) grammar G . A formal grammar can be defined as follows ( see e.g. Allen 1995 ): . G ( VN , VT , R , S ) . And here we are already dealing with sets which have a finite number of members . The number of grammatical rules is fairly limited . This is certainly the case when we only consider the basic grammar rules of a language that will have to be learned by the intermediate to early advanced learner . ( Note here what we said earlier about Grammar B2 - the learner grammar : It was only a subset of Grammar 1 - the linguistic grammar . ) Formal grammars have been used in a number of CALL projects . Matthews ( 1993 ) continues his discussion of grammar frameworks for CALL which he started in 1992 ( Matthews 1992a ) . He lists eight major grammar frameworks that have been used in CALL : . Of course , these are only some examples . More recently , Tschichold et al . ( 1994 ) reported on a prototype for correcting English texts produced by French learners . This system relies on a number of different finite state automata for pre - processing , filtering and detecting ( Tschichold et al.1994 ) . Brehony & Ryan ( 1994 ) report on \\\" Francophone Stylistic Grammar Checking ( FSGC ) using Link Grammars \\\" . They adapted the post - processing section of an existing parser so that it would detect stylistic errors in English input produced by French learners . His plea is for the use of the PPT ( Principles and Parameters Theory ( Chomsky 1986 ) as a grammar framework for CALL applications , basing his judgement on three criteria : computational effectiveness , linguistic perspicuity and acquisitional perspicuity ( Matthews 1993:9 ) . In later parts of his paper , Matthews compares rule- and principle - based frameworks using DCGs ( Definite Clause Grammars ) as the example for the latter . He concludes that principle - based frameworks ( and consequently principle - based parsing ) are the most suitable grammar frameworks for what he calls Intelligent CALL . Recently , other unification - based grammar frameworks not included in Matthews ' list have been used in CALL . Hagen , for instance , describes \\\" an object - oriented , unification - based parser called HANOI \\\" ( Hagen 1995 ) which uses formalisms developed in Head - Driven Phrase Structure Grammar ( HPSG ) . He quotes Zajac : . Combining object - oriented approaches to linguistic description with unification - based grammar formalisms [ ... ] is very attractive . On one hand , we gain the advantages of the object - oriented approach : abstraction and generalisation through the use of inheritance . On the other hand , we gain a fully declarative framework , with all the advantages of logical formalisms [ ... ] . Of course , not even this extended list is comprehensive - at best it could be described as indicative of the variety of linguistic approaches used in parser - based Computer Assisted Language Learning and , in particular , in the field of grammar checking . At the end of this short excursion into formal grammar(s ) it can be concluded that any CALL grammar checker component needs as its foundation a formal grammar describing as comprehensively as possible the knowledge we have about the target language grammar . This was the grammar that Helbig ( 1975 ) refers to as Grammar B1a . But what part do the other grammars play in a CALL environment ? Let us stay with the example of a parser - based grammar checker for language learners . Hence , the provision of adequate feedback on the morpho - syntactic structure of parts of the text produced by learners is the most important task for this parser - based grammar checker . Let us therefore consider the place of feedback provision within a parser grammar . In other words , as a good teacher would do - the grammar checker would offer advice on how to change an ungrammatical structure into a corresponding grammatically well - formed structure . As stated earlier this approach would be based on an infinite number of construction possibilities . Therefore , the provision of adequate feedback and help to the learner appears to be difficult if not impossible . However , it has been indicated above that feedback could be linked to the finite sets on which the formal grammar relies . How can this be done ? Each member of the three sets which will have to be considered here . The non - terminal symbols like NP and VP , the words and the set of morpho - syntactic rules carry certain features that determine their behaviour in a sentence and determine their relation to other signs within the sentence . These features which restrict what the text producer can do with a given ( terminal or non - terminal ) symbol in a sentence and under what conditions a particular grammatical rule has to be applied will be labelled constraints . Let us return to our provisional description of feedback , which can now be formulated more precisely . Feedback shows the relation . by explaining the underlying constraint of the anticipated construction in L based on Grammar B2 . to support production of construction in L . and by reasoning about the likely cause of the rule violation . to extend Grammar C - the learner - inherent grammar . And secondly , this above description of feedback given by a grammar checker which is based on a modified parser shows that it is possible to construct tools that support the focus on form by learners during the reflection stage of a text production process . Even if the grammar checker were only to detect a small number of morpho - syntactic errors , this would be beneficial for the learners as long as they were aware of the limitations of this CALL tool . On the other hand , the feedback description contains still a number of question marks in parentheses after some of the important keywords - whether the intended aims of grammar checking can be achieved can only be validated through the use and thorough testing of such a grammar checker . We should better not make any such assumption ( in the scientific sense - we do hope for these improvements , of course ) and better wait until such a parser - based grammar checker is actually tested in a series of proper learning experiments . Let us now leave the discussion of some of the underlying linguistics behind and discuss the role of parser - based applications in language learning . Natural language parsers take written language as their input and produce a formal representation of the syntactic and sometimes semantic structure of this input . The role they have to play in computer - assisted language learning has been under scrutiny in the last decade : ( Matthews 1992a ) ; ( Holland et al . 1993 ) ; ( Nagata 1996 ) . See also Heift ( 2001 ) . Holland et al . discussed the \\\" possibilities and limitations of parser - based language tutors \\\" ( Holland et al . 1993:28 ) . Comparing parser - based CALL to what they label as conventional CALL they come to the conclusion that : . [ ... ] in parser - based CALL the student has relatively free rein and can write a potentially huge variety of sentences . ICALL thus permits practice of production skills , which require recalling and constructing , not just recognising [ as in conventional CALL ] , words and structures . ( Holland et al.1993:31 ) . However , at the same time , parsing imposes certain limitations . Parsers tend to concentrate on the syntax of the textual input , thus \\\" ICALL may actually subvert a principal goal of language pedagogy , that of communicating meanings rather than producing the right forms \\\" ( Holland et al.1993:32 ) . This disadvantage can be avoided by a \\\" focus on form \\\" which is mainly achieved by putting the parser / grammar checker to use within a relevant , authentic communicative task and at a time chosen by and convenient to the learner / text producer . Juozulynas ( 1994 ) evaluated the potential usefulness of syntactic parsers in error diagnosis . He analysed errors in an approximately 400 page corpus of German essays by American college students in second - year language courses . His study shows that : . [ ... ] syntax is the most problematic area , followed by morphology . ( Juozulynas 1994:5 ) . Juozulynas adapted a taxonomic schema by Hendrickson which comprises four categories : syntax , morphology , orthography , lexicon . Juozulynas ' argument for splitting orthography into spelling and punctuation is easily justified in the context of syntactic parsing . Parts of punctuation can be described by using syntactic bracketing rules , and punctuation errors can consequently be dealt with by a syntactic parser . Lexical and spelling errors form , according to Juozulynas , a rather small part of the overall number of learner errors . Some of these errors will , of course , be identified during dictionary look - up , but if words that are in the dictionary are used in a nonsensical way , the parser will not recognise them unless specific error rules ( e.g. for false friends ) are built in . Consequently , a parser - based CALL application can play a useful role in detecting many of the morpho - syntactic errors which constitute a high percentage of learner errors in freely produced texts . Nevertheless , the fact remains : . A second limitation of ICALL is that parsers are not foolproof . Because no parser today can accurately analyse all the syntax of a language , false acceptance and false alarms are inevitable . ( Holland et al . 1993:33 ) . This is something not only developers of parser - based CALL , but also language learners using such software have to take into account . In other words , this limitation of parser - based CALL has to be taken into consideration during the design and implementation process and when integrating this kind of CALL software in the learning process . A final limitation of ICALL is the cost of developing NLP systems . By comparison with simple CALL , NLP development depends on computational linguists and advanced programmers as well as on extensive resources for building and testing grammars . Beyond this , instructional shells and lessons must be built around NLP , incurring the same expense as developing shells and lessons for CALL . ( Holland et al . 1993:33 ) . It is mainly this disadvantage of parser - based CALL that explains the lack of commercially available ( and commercially viable ) CALL applications which make good use of HLT . However , it is to be hoped that this hurdle can be overcome in the not too distant future because sufficient expertise in the area has accumulated over recent years . More and more computer programs make good use of this technology , and many of these have already \\\" entered \\\" the realm of computer - assisted language learning , as can be seen from the examples in the previous section . Holland et al . ( 1993 ) answer their title question \\\" What are parsers good for ? \\\" on the basis of their own experience with BRIDGE , a parser - based CALL program for American military personnel learning German , and on the basis of some initial testing with a small group of experienced learners of German . They present the following points : . ICALL appears to be good for form - focused instruction offering learners the chance to work on their own linguistic errors by this method , not only to improve their performance in the foreign language but also to improve their language awareness . ICALL appears to be good for selected kinds of students . The authors list the following characteristics which might influence the degree of usefulness of ICALL for certain students : . i. intermediate proficiency . ii . analytical orientation . iii . tolerance of ambiguity . iv . confidence as learners . ICALL is good for research because the parser automatically tracks whole sentence responses and detects , classifies , and records errors . It might facilitate the assessment of the students grammatical competency and thus help us discover patterns of acquisition . ICALL [ ... ] can play a role in communicative practice . The authors argue for the embedding of parser - based CALL in \\\" graphics microworlds \\\" which help to capture some basic semantics . Given that we would like to harness the advantages of parser - based CALL , how do we take the limitations into consideration in the process of designing and implementing a parser - based CALL system ? What implications does the use of parsing technology have for human - computer interaction ? [ I]n discourse analytic terms ( Grice 1975 ) , the nature of the contract between student and CALL tutor is straightforward , respecting the traditional assumption that the teacher is right , whereas the ICALL contract is less well defined . ( Holland et al . 1993:33f . ) . The rigidity of traditional CALL in which the program controls the linguistic input by the learner to a very large extent has often given rise to a criticism of CALL which accuses CALL developers and practitioners of relying on behaviourist programmed instruction . If one wants to give learners full control over their linguistic input , for example , by relying on parsing technology , what are then the terms according to which the communicative interaction of computer ( program ) and learner can be defined ? The differences between humans and machines have obviously to be taken into consideration in order to understand the interaction of learners with CALL programs : . Machines are compiled out of individual parts for a very specific purpose ( totum fix et partibus ) ; whereas humans are holistic entities whose parts can be differentiated ( partes fiunt ex toto ) . Humans process all sorts of experiences and repeatedly and interactively create their own environment - something machines can not do . They \\\" calculate \\\" a problem on the basis of pre - wired rules . The main features of human thoughts are the inherent contradictions and the ability to cope with them , something that will not be calculable due to its complexity , variety and degree of detail ( Schmitz 1992:209f . ) . These differences between humans and machines can for our purposes , i.e. the theoretical description of human - computer interaction , be legitimately reduced to the distinction between actions and operations as is done in Activity Theory . The main point of this theory for our consideration here is that communicative activities can be divided into actions which are intentional , i.e. goal - driven ; and these can be sub - divided into operations which are condition - triggered . These operations are normally learnt as actions . In the example referred to in the quotation above , the gear - switching is learnt as an action . The learner - driver is asked by the driving instructor to change gear and this becomes the goal of the learner . Once the learner - driver has performed this action a sufficient number of times , this action becomes more and more automated and in the process loses more and more of its intentionality . A proficient driver might have the goal to accelerate the car which will necessitate switching into higher gear , but this is now triggered by a condition ( the difference between engine speed and speed of the car ) . It can thus be argued that humans learn to perform complex actions by learning to perform certain operations in a certain order . Machines , on the other hand , are only made to perform certain ( sometimes rather complex ) operations . This has some bearing on our understanding of the human - computer interaction that takes place when a learner uses language - learning software . When , for instance , the spell checker is started in a word - processing package , the software certainly does not have ' proof - read ' the learner 's document . The computer just responds to the clicking of the spellchecker menu item and performs the operation of checking the strings in the document against the entries in a machine dictionary . For the computer user ( in our case a learner ) , it might look like the computer is proof - reading the document . Normally , one only realises that no \\\" proper \\\" document checking is going on if a correctly spelled word is not found in the dictionary or nonsensical alternatives are given for a simple spelling error . Person X interacts with Person Y in that he observes Person Y 's action , reasons about the likely intention for that action and reacts according to this assumed intention . This , for example , explains why many learners get just as frustrated when an answer they believe to be right is rejected by the computer as they would get if it were rejected by their tutor . Of course , an ideal computer - assisted language learning system would avoid such pitfalls and not reject a correct response or overlook an incorrect one . Since any existing system can only approximate to this ideal , researchers and developers in parser - based CALL can only attempt to build systems that can perform complex structured sequences of ( linguistic ) operations so that learners can interact meaningfully and successfully with the computer . Grammatica is able to identify parts of speech in English and French with a fair degree of accuracy and show , for example , how verbs are conjugated in different tenses and how plurals of nouns are formed . Abeill\\u00e9 A. ( 1992 ) \\\" A lexicalised tree adjoining grammar for French and its relevance to language teaching \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Abney S. ( 1997 ) \\\" Part - of - speech tagging and partial parsing \\\" . In Young S. & Bloothooft G. ( eds . ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Allen J. ( 1995 ) Natural language understanding , New York : John Benjamins Publishing Company . Alwang G. ( 1999 ) \\\" Speech recognition \\\" , PC Magazine , 10 November 1999 . Antos G. ( 1982 ) Grundlagen einer Theorie des Formulierens . Textherstellung in geschriebener und gesprochener Sprache , T\\u00fcbingen : Niemeyer . Arnold D. , Balkan . L , Meijer S. , Humphreys R. L. & Sadler L. ( 1994 ) Machine Translation : an introductory guide , Manchester : NEC Blackwell . Bellos D. ( 2011 ) Is that a fish in your ear ? Translation and the meaning of everything , Harlow : Penguin / Particular Books . Bennett P. ( 1997 ) Feature - based approaches to grammar , Manchester : UMIST , Unpublished Manuscript . Bennett P. & Paggio P. ( eds . ) ( 1993 ) Preference in EUROTRA , Luxembourg : European Commission . Bloothooft G. , Dommelen W. , van Espain C. , Green P. , Hazan V. & Wigforss E. ( eds . ) ( 1997 ) The landscape of future education in speech communication sciences : ( 1 ) analysis , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bloothooft G. , van Dommelen W. , Espain C. , Hazan V. , Huckvale M. & Wigforss E. ( eds . ) ( 1998 ) The landscape of future education in speech communication sciences : ( 2 ) proposals , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bolt P. & Yazdani M. ( 1998 ) \\\" The evolution of a grammar - checking program : LINGER to ISCA \\\" , CALL 11 , 1 : 55 - 112 . Borchardt F. ( 1995 ) \\\" Language and computing at Duke University : or , Virtue Triumphant , for the time being \\\" , CALICO Journal 12 , 4 : 57 - 83 . Bowerman C. ( 1993 ) Intelligent computer - aided language learning . LICE : a system to support undergraduates writing in German , Manchester : UMIST , Unpublished doctoral dissertation . Brehony T. & Ryan K. ( 1994 ) \\\" Francophone stylistic grammar checking ( FSGC ) : using link grammars \\\" , CALL 7 , 3 : 257 - 269 . Brocklebank P. ( 1998 ) An experiment in developing a prototype intelligent teaching system from a parser written in Prolog , Manchester , UMIST , Unpublished MPhil dissertation . Brown P.F. , Della Pietra S.A. , Della Pietra V.J. & Mercer R.L. ( 1993 ) \\\" The mathematics of statistical Machine Translation : parameter estimation \\\" , Computational Linguistics 19 , 2 : 263 - 311 . Buchmann B. ( 1987 ) \\\" Early history of Machine Translation \\\" . In King M. ( ed . ) Machine Translation today : the state of the art , Edinburgh : University Press . Bull S. ( 1994 ) \\\" Learning languages : implications for student modelling in ICALL \\\" , ReCALL 6 , 1 : 34 - 39 . Bureau Lingua / DELTA ( 1993 ) Foreign language learning and the use of new technologies , Brussels : European Commission . Cameron K. ( ed . ) ( 1989 ) Computer Assisted Language Learning , Oxford : Intellect . Carson - Berndsen J. ( 1998 ) \\\" Computational autosegmental phonology in pronunciation teaching \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Chanier D. , Pengelly M. , Twidale M. & Self J. ( 1992 ) \\\" Conceptual modelling in error analysis in computer - assisted language learning \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Chen L. & Barry L. ( 1989 ) \\\" XTRA - TE : Using natural language processing software to develop an ITS for language learning \\\" . In Fourth International Conference on Artificial Intelligence and Education : 54 - 70 . Chomsky N. ( 1986 ) Knowledge of language : its nature , origin , and use , New York : Praeger . CLEF ( Computer - assisted Learning Exercises for French ) ( 1985 ) Developed by the CLEF Group , Canada , including authors at the University of Guelph , the University of Calgary and the University of Western Ontario . Also published by Cambridge University Press , 1998 : ISBN 0 - 521 - 59277 - 1 . Curzon L. B. ( 1985 ) Teaching in further education : an outline of principles and practice . London : Holt , Rinehart & Winston , ( 3rd edition ) . Davies G. ( 1988 ) \\\" CALL software development \\\" . In Jung Udo O.H .. ( ed . ) Computers in applied linguistics and language learning : a CALL handbook , Frankfurt : Peter Lang . Davies G. ( 1996 ) Total - text reconstruction programs : a brief history , Maidenhead : Camsoft . Davies S. & Poesio M. ( 1998 ) \\\" The provision of corrective feedback in a spoken dialogue system \\\" . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) ( 1991 ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Diaz de Ilarranza A. , Maritxalar M. & Oronoz M. ( 1998 ) \\\" Reusability of language technology in support of corpus studies in an ICALL environment \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Diaz de Ilarranza A. , Maritxalar A. , Maritxalar M. & Oronoz M. ( 1999 ) \\\" IDAZKIDE : An intelligent computer - assisted language learning environment for Second Language Acquisition \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 12 - 19 . Dodigovic M. ( 2005 ) Artificial intelligence in second language learning : raising error awarenes s , Clevedon : Multilingual Matters . Dokter D. & Nerbonne J. ( 1998 ) \\\" A session with Glosser - RuG \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Dokter D. , Nerbonne J. , Schurcks - Grozeva L. & Smit P. ( 1998 ) \\\" Glosser - RuG : a user study \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Ehsani F. & Knodt E. ( 1998 ) \\\" Speech technology in computer - aided language learning : strengths and limitations of a new CALL paradigm \\\" , Language Learning and Technology 2 , 1 : 45 - 60 . Ellis R. ( 1994 ) The study of Second Language Acquisition , Oxford : OUP . European Commission ( 1996 ) Language and technology : from the Tower of Babel to the Global Village , Luxembourg : European Commission . ISBN 92 - 827 - 6974 - 7 . Fechner J. ( ed . ) ( 1994 ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Feuerman K. , Marshall C. , Newman D. & Rypa M. ( 1987 ) \\\" The CALLE project \\\" , CALICO Journal 4 : 25 - 34 . Foucou P - Y. & K\\u00fcbler N. ( 1999 ) \\\" A Web - based language learning environment : general architecture \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 31 - 39 . Fum D. , Pani B. & Tasso C. ( 1992 ) \\\" Native vs. formal grammars : a case for integration in the design of a foreign language tutor \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hagen L.K. ( 1995 ) \\\" Unification - based parsing applications for intelligent foreign language tutoring systems , CALICO Journal 12 , 2 - 3 : 5 - 31 . Hamilton S. ( 1998 ) \\\" A CALL user study \\\" . In Jager S. , Nerbonne J. & van Essen A.(eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Handke J. ( 1992 ) \\\" WIZDOM : a multiple - purpose language tutoring system based on AI techniques \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hart R. ( 1995 ) \\\" The Illinois PLATO foreign languages project \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 15 - 37 . Heift T. ( 2001 ) \\\" Error - specific and individualised feedback in a Web - based language tutoring system : Do they read it ? \\\" ReCALL 13 , 1 : 99 - 109 . Heift T. & Schulze M. ( eds . ) ( 2003 ) Error diagnosis and error correction in CALL , CALICO Journal Special Issue 20 , 3 . Heift T. & Schulze M. ( 2007 ) Errors and intelligence in CALL : parsers and pedagogues , London and New York : Routledge . Helbig G. ( 1975 ) \\\" Bemerkungen zum Problem von Grammatik und Fremdsprachenunterricht \\\" , Deutsch als Fremdsprache 6 , 12 : 325 - 332 . Higgins J. & Johns T. ( 1984 ) Computers in language learning , London : Collins . Holland M. , Maisano R. , Alderks C. & Martin J. ( 1993 ) \\\" Parsers in tutors : what are they good for ? \\\" CALICO Journal 11 , 1 : 28 - 46 . Holland M. & Fisher F.P. ( eds . ) ( 2007 ) T he path of speech technologies in Computer Assisted Language Learning : from research toward practice , London and New York : Routledge . Hutchins W.J. ( 1986 ) Machine Translation : past , present , future , Chichester : Ellis Horwood . Hutchins W.J. ( 1997 ) \\\" Fifty years of the computer and translation \\\" , Machine Translation Review 6 , October 1997 : 22 - 24 . Hutchins W.J. & Somers H.L. ( 1992 ) An introduction to Machine Translation , London : Academic Press . Jager S. ( 2001 ) \\\" From gap - filling to filling the gap : a re - assessment of Natural Language Processing in CALL \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Jager S. , Nerbonne J. & van Essen A. ( eds . ) ( 1998 ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Jones R. ( 1995 ) \\\" TICCIT and CLIPS : The early years \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 84 - 96 . Jung Udo O.H. & Vanderplank R. ( eds . ) ( 1994 ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Juozulynas V. ( 1994 ) \\\" Errors in the composition of second - year German students : an empirical study of parser - based ICALI \\\" , CALICO Journal 12 , 1 : 5 - 17 . King M. ( ed . ) ( 1987 ) Machine Translation today : the state of the art , Edinburgh : Edinburgh University Press . Klein W. & Dittmar N. ( 1979 ) Developing grammars : the acquisition of German syntax by foreign workers , Heidelberg : Springer . Klein W. & Perdue C. ( 1992 ) Utterance structure ( developing grammars again ) , Amsterdam : John Benjamins Publishing Company . Klein W. ( 1986 ) Second language acquisition , Cambridge : Cambridge University Press . Kohn K. ( 1994 ) \\\" Distributive language learning in a computer - based multilingual communication environment \\\" . In Jung Udo O.H. & Vanderplank R. ( eds . ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Krashen S. ( 1981 ) Second language acquisition and second language learning , Oxford : Pergamon . Krashen S. ( 1982 ) Principles and practice in second language acquisition , Oxford : Pergamon . Kr\\u00fcger - Thielmann K. ( 1992 ) Wissensbasierte Sprachlernsysteme . Neue M\\u00f6glichkeiten f\\u00fcr den computergest\\u00fctzten Sprachunterricht , T\\u00fcbingen : Gunter Narr . Labrie G. & Singh L. ( 1991 ) \\\" Parsing , error diagnosis and instruction in a French tutor \\\" , CALICO Journal 9 : 9 - 25 . Last R. ( 1989 ) Artificial Intelligence techniques in language learning , Chichester : Ellis Horwood . Last R. ( 1992 ) \\\" Computers and language learning : past , present - and future ? \\\" In Butler C. ( ed . ) Computers and written texts , Oxford : Blackwell . Levin L. , Evans D. & Gates D. ( 1991 ) \\\" The Alice system : a workbench for learning and using language \\\" , CALICO Journal 9 : 27 - 55 . Levy M. ( 1997 ) Computer - assisted language learning : context and conceptualisation , Oxford : Oxford University Press . Lightbown P.M. & Spada N. ( 1993 ) How languages are learned , Oxford : Oxford University Press . Long M. ( 1991 ) \\\" Focus on form : a design feature in language teaching methodology \\\" . In de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Manning C. & Sch\\u00fctze H. ( 1999 ) Foundations of statistical Natural Language Processing , Cambridge MA , MIT Press . Matthews C. ( 1992a ) \\\" Going AI : foundations of ICALL \\\" , CALL 5 , 1 - 2 : 13 - 31 . Matthews C. ( 1992b ) Intelligent CALL ( ICALL ) bibliography , Hull : University of Hull , CTI Centre for Modern Languages . Matthews C. ( 1993 ) \\\" Grammar frameworks in Intelligent CALL \\\" , CALICO Journal 11 , 1 : 5 - 27 . Matthews C. ( 1994 ) \\\" Intelligent Computer Assisted Language Learning as cognitive science : the choice of syntactic frameworks for language tutoring \\\" , Journal of Artificial Intelligence in Education 5 , 4 : 533 - 56 . Menzel W. & Schr\\u00f6der I. ( 1999 ) \\\" Error diagnosis for language learning systems \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 20 - 30 . Michel G. ( ed . ) ( 1985 ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . Mitkov R. ( 1998 ) Language Learner 's Workbench . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . Mitkov R. & Nicolov N. ( eds . ) ( 1997 ) Recent advances in Natural Language Processing . Amsterdam : John Benjamins Publishing Company . Murphy M. , Kr\\u00fcger A. & Griesz A. , ( 1998 ) \\\" RECALL \\\" -towards a knowledge - based approach to CALL . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nagata N. ( 1996 ) \\\" Computer vs. workbook instruction in second language acquisition \\\" , CALICO Journal 14 , 1 : 53 - 75 . Nerbonne J. , Jager S. & van Essen A. ( 1998 ) Introduction . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nirenburg S. ( ed . ) ( 1986 ) Machine Translation : theoretical and methodological issues , Cambridge : Cambridge University Press . Pijls F. , Daelmans W. & Kempen G. ( 1987 ) \\\" Artificial intelligence tools for grammar and spelling instruction , Instructional Science 16 : 319 - 336 . Pollard C. & Sag I.A. ( 1987 ) Information - Based Syntax and Semantics , Chicago : University Press . Pollard C. & Sag I.A. ( 1994 ) Head - Driven Phrase Structure Grammar , Chicago : University Press . Ramsay A. & Sch\\u00e4ler R. ( 1997 ) \\\" Case and word order in English and German \\\" . In Mitkov R. & Nicolov N. ( eds . ) Recent advances in Natural Language Processing , Amsterdam : John Benjamins Publshing Company : 15 - 34 . Ramsay A. & Schulze M. ( 1999 ) \\\" Die Struktur deutscher Lexeme \\\" , German Linguistic and Cultural Studies , Peter Lang , Submitted Manuscript . Reifler E. ( 1958 ) \\\" The Machine Translation project at the University of Washington , Seattle , Washington , USA \\\" . In Proceedings of the Eighth International Congress of Linguists , Oslo University Press : 514 - 518 . Roosmaa T. & Pr\\u00f3sz\\u00e9ky G. ( 1998 ) \\\" GLOSSER - using language technology tools for reading texts in a foreign language \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Salaberry R. ( 1996 ) \\\" A theoretical foundation for the development of pedagogical tasks in computer mediated communication \\\" , CALICO Journal 14 , 1 : 5 - 34 . Sanders R. ( 1991 ) \\\" Error analysis in purely syntactic parsing of free input : the example of German \\\" , CALICO Journal 9 , 1 : 72 - 89 . Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 . Schmitz U. ( 1992 ) Computerlinguistik , Opladen : Westdeutscher Verlag . Schulze M. ( 1997 ) \\\" Textana - text production in a hypertext environment \\\" , CALL 10 , 1 : 71 - 82 . Schulze M. ( 1998 ) \\\" Teaching grammar - learning grammar . Aspects of Second Language Acquisition in CALL \\\" , CALL 11 , 2 : 215 - 228 . Schulze M. ( 2001 ) \\\" Human Language Technologies in Computer Assisted Language Learning \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Schulze M. , Hamel M - J. & Thompson J. ( eds . ) ( 1999 ) Language processing in CALL , ReCALL Special Issue . Schumann J.H. & Stenson N. ( eds . ) ( 1975 ) New frontiers in second language learning , Rowley : Newbury House . Schwind C. ( 1990 ) \\\" An intelligent language tutoring system \\\" , International Journal of Man - Machine Studies 33 : 557 - 579 . Selinker L. ( 1992 ) Rediscovering interlanguage , London : Longman . Skrelin P. & Volskaja N. ( 1998 ) \\\" The application of new technologies in the development of education programs \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Smith R. ( 1990 ) Dictionary of Artificial Intelligence , London : Collins . Sp\\u00e4th P. ( 1994 ) \\\" Hypertext und Expertensysteme i m Sprachunterricht \\\" . In Fechner J. ( ed . ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Stenzel B. ( ed . ) ( 1985 ) Computergest\\u00fctzter Fremdsprachenunterricht . Ein Handbuch , Berlin : Langenscheidt . Swartz M. & Yazdani M. ( eds . ) ( 1992 ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Taylor H. ( 1987 ) TUCO II . Published by Gessler Educational Software , New York . Based on earlier programs developed by Taylor H. & Haas W. at Ohio State University in the 1970s : DECU ( Deutscher Computerunterricht ) and TUCO ( Tutorial Computer ) . Taylor H. ( 1998 ) Computer assisted text production : feedback on grammatical errors made by learners of English as a Foreign Language , Manchester : UMIST , MSc Dissertation . Tschichold C. ( 1999 ) \\\" Intelligent grammar checking for CALL \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 5 - 11 . Tschichold C. , Bodme F. , Cornu E. , Grosjean F. , Grosjean L. , K\\u00fcbler N. & Tschumi C. ( 1994 ) \\\" Detecting and correcting errors in second language texts \\\" , CALL 7 , 2 : 151 - 160 . Visser H. ( 1999 ) \\\" CALLex ( Computer - Aided Learning of Lexical functions ) - a CALL game to study lexical relationships based on a semantic database \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 50 - 56 . Ward R. , Foot R. & Rostron A.B. ( 1999 ) \\\" Language processing in computer - assisted language learning : language with a purpose \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 40 - 49 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . Reproduced in Locke W.N. & Booth A.D. ( eds . ) ( 1955 ) Machine translation of languages : fourteen essays , Cambridge , Mass : Technology Press of the Massachusetts Institute of Technology : 15 - 23 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . See \\\" 50th anniversary of machine translation \\\" , MT News International , Issue 22 ( Vol . 8 , 1 ) , July 1999 : 5 - 6 . Weischedel R. , Voge W. & James M. ( 1978 ) \\\" An artificial intelligence approach to language instruction \\\" , Artificial Intelligence 10 : 225 - 240 . Wilks Y. & Farwell D. ( 1992 ) \\\" Building an intelligent second language tutoring system from whatever bits you happen to have lying around \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag , . Whitelock P.J. & Kilby K. ( 1995 ) Linguistic and computational techniques in Machine Translation system design , London : University College Press . Witt S. & Young S. ( 1998 ) \\\" Computer - assisted pronunciation teaching based on automatic speech recognition \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Wolff D. ( 1993 ) \\\" New technologies for foreign language teaching \\\" . In Foreign language learning and the use of new technologies , Bureau Lingua / DELTA , Brussels , European Commission . Young S. & Bloothooft G. ( eds . ) ( 1997 ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Z\\u00e4hner C. ( 1991 ) \\\" Word grammars in ICALL \\\" . In Savolainen H. & Telenius J. ( eds . ) EuroCALL 91 proceedings , Helsinki : Helsinki School of Economics . Zech J. ( 1985 ) \\\" Methodische Probleme einer t\\u00e4tigkeitsorientierten Ausbildung des sprachlich - kommunikativen K\\u00f6nnens \\\" . In Michel G. ( ed . ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . CALICO ( Computer Assisted Language Instruction Consortium ) : CALICO is a professional association devoted to promoting the use of technology enhanced language learning . CALICO 's sister association in Europe is EUROCALL . EUROCALL : EUROCALL is a professional association devoted to promoting the use of technology enhanced language learning , based at the University of Ulster , Northern Ireland . EUROCALL 's sister association in the USA is CALICO . ICALL is an interdisciplinary research field integrating insights from computational linguistics and artificial intelligence into computer - aided language learning . Such integration is needed for CALL systems to be able to analyze language automatically , to make them aware of language as such . This makes it possible to provide individualized feedback to learners working on exercises , to ( semi-)automatically prepare or enhance texts for learners , and to automatically create and use detailed learner models . See NLP SIG , the Special Interest Group within EUROCALL , with which ICALL closely collaborates . InSTIL : The name of a now defunct Special Interest Group dedicated to Integrating Speech Technology in Language Learning , which was set up within the EUROCALL and CALICO professional associations . A good deal of the work undertaken by InSTIL has now been taken over by ICALL and NLP SIG . NLP SIG : The name of the Special Interest Group for Natural Language Processing within the EUROCALL professional association . See ICALL , the Special Interest Group within CALICO , with which NLP SIG closely collaborates . Virtual Linguistics Campus : It includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Document last updated 29 April 2012 . This page is maintained by Graham Davies . Please cite this Web page as : Gupta P. & Schulze M. ( 2012 ) Human Language Technologies ( HLT ) . Module 3.5 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \"}",
        "_version_":1692580922897989632,
        "score":195.07433},
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":188.0312},
      {
        "id":"196d7ee8-6c21-4795-accc-22c761580d50",
        "_src_":"{\"url\": \"https://interchangeableparts.wordpress.com/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701154221.36/warc/CC-MAIN-20160205193914-00035-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Software documentation is like sex : when it is good , it is very , very good ; and when it is bad , it is better than nothing . ( Anonymous . ) There are two ways of constructing a software design : one way is to make it so simple that there are obviously no deficiencies ; the other way is to make it so complicated that there are no obvious deficiencies . ( C.A.R. Hoare ) . A computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology . Thus , programs must be written for people to read , and only incidentally for machines to execute . ( The Structure and Interpretation of Computer Programs , H. Abelson , G. Sussman and J. Sussman , 1985 . ) If you try to make something beautiful , it is often ugly . If you try to make something useful , it is often beautiful . ( Oscar Wilde ) 1 . GATE 2 is an infrastructure for developing and deploying software components that process human language . It is nearly 15 years old and is in active use for all types of computational task involving human language . GATE excels at text analysis of all shapes and sizes . From large corporations to small startups , from \\u20ac multi - million research consortia to undergraduate projects , our user community is the largest and most diverse of any system of this type , and is spread across all but one of the continents 3 . GATE is open source free software ; users can obtain free support from the user and developer community via GATE.ac.uk or on a commercial basis from our industrial partners . We are the biggest open source language processing project with a development team more than double the size of the largest comparable projects ( many of which are integrated with GATE 4 ) . More than \\u20ac 5 million has been invested in GATE development 5 ; our objective is to make sure that this continues to be money well spent for all GATE 's users . GATE has grown over the years to include a desktop client for developers , a workflow - based web application , a Java library , an architecture and a process . GATE is : . One of our original motivations was to remove the necessity for solving common engineering problems before doing useful research , or re - engineering before deploying research results into applications . Core functions of GATE take care of the lion 's share of the engineering : . modelling and persistence of specialised data structures . measurement , evaluation , benchmarking ( never believe a computing researcher who has n't measured their results in a repeatable and open setting ! ) visualisation and editing of annotations , ontologies , parse trees , etc . . a finite state transduction language for rapid prototyping and efficient implementation of shallow analysis methods ( JAPE ) . extraction of training instances for machine learning . pluggable machine learning implementations ( Weka , SVM Light , ... ) . On top of the core functions GATE includes components for diverse language processing tasks , e.g. parsers , morphology , tagging , Information Retrieval tools , Information Extraction components for various languages , and many others . GATE Developer and Embedded are supplied with an Information Extraction system ( ANNIE ) which has been adapted and evaluated very widely ( numerous industrial systems , research systems evaluated in MUC , TREC , ACE , DUC , Pascal , NTCIR , etc . ) . ANNIE is often used to create RDF or OWL ( metadata ) for unstructured content ( semantic annotation ) . GATE version 1 was written in the mid-1990s ; at the turn of the new millenium we completely rewrote the system in Java ; version 5 was released in June 2009 . We invite you to give it a try , to get involved with the GATE community , and to contribute to human language science , engineering and development . This book describes how to use GATE to develop language processing components , test their performance and deploy them as parts of other applications . In the rest of this chapter : . ( Often the process of getting a new component is as simple as typing the URL into GATE Developer ; the system will do the rest . ) The material presented in this book ranges from the conceptual ( e.g. ' what is software architecture ? ' ) to practical instructions for programmers ( e.g. how to deal with GATE exceptions ) and linguists ( e.g. how to write a pattern grammar ) . Furthermore , GATE 's highly extensible nature means that new functionality is constantly being added in the form of new plugins . Important functionality is as likely to be located in a plugin as it is to be integrated into the GATE core . This presents something of an organisational challenge . Our ( no doubt imperfect ) solution is to divide this book into three parts . Part I covers installation , using the GATE Developer GUI and using ANNIE , as well as providing some background and theory . We recommend the new user to begin with Part I . Part II covers the more advanced of the core GATE functionality ; the GATE Embedded API and JAPE pattern language among other things . Part III provides a reference for the numerous plugins that have been created for GATE . Although ANNIE provides a good starting point , the user will soon wish to explore other resources , and so will need to consult this part of the text . We recommend that Part III be used as a reference , to be dipped into as necessary . In Part III , plugins are grouped into broad areas of functionality . Software Architecture ' is used rather loosely here to mean computer infrastructure for software development , including development environments and frameworks , as well as the more usual use of the term to denote a macro - level organisational structure for software systems [ Shaw & Garlan 96 ] . Language Engineering ( LE ) may be defined as : . ... the discipline or act of engineering software systems that perform tasks involving processing human language . Both the construction process and its outputs are measurable and predictable . The literature of the field relates to both application of relevant scientific results and a body of practice . [ Cunningham 99a ] . The relevant scientific results in this case are the outputs of Computational Linguistics , Natural Language Processing and Artificial Intelligence in general . Unlike these other disciplines , LE , as an engineering discipline , entails predictability , both of the process of constructing LE - based software and of the performance of that software after its completion and deployment in applications . Some working definitions : . Computational Linguistics ( CL ) : science of language that uses computation as an investigative tool . Natural Language Processing ( NLP ) : science of computation whose subject matter is data structures and algorithms for computer processing of human language . Language Engineering ( LE ) : building NLP systems whose cost and outputs are measurable and predictable . Software Architecture : macro - level organisational principles for families of systems . In this context is also used as infrastructure . Software Architecture for Language Engineering ( SALE ) : software infrastructure , architecture and development tools for applied CL , NLP and LE . ( Of course the practice of these fields is broader and more complex than these definitions . ) In the scientific endeavours of NLP and CL , GATE 's role is to support experimentation . In this context GATE 's significant features include support for automated measurement ( see Chapter 10 ) , providing a ' level playing field ' where results can easily be repeated across different sites and environments , and reducing research overheads in various ways . GATE as an architecture suggests that the elements of software systems that process natural language can usefully be broken down into various types of component , known as resources 8 . Components are reusable software chunks with well - defined interfaces , and are a popular architectural form , used in Sun 's Java Beans and Microsoft 's . Net , for example . GATE components are specialised types of Java Bean , and come in three flavours : . LanguageResources ( LRs ) represent entities such as lexicons , corpora or ontologies ; . ProcessingResources ( PRs ) represent entities that are primarily algorithmic , such as parsers , generators or ngram modellers ; . VisualResources ( VRs ) represent visualisation and editing components that participate in GUIs . These definitions can be blurred in practice as necessary . Collectively , the set of resources integrated with GATE is known as CREOLE : a Collection of REusable Objects for Language Engineering . All the resources are packaged as Java Archive ( or ' JAR ' ) files , plus some XML configuration data . The JAR and XML files are made available to GATE by putting them on a web server , or simply placing them in the local file space . Section 1.3.2 introduces GATE 's built - in resource set . When using GATE to develop language processing functionality for an application , the developer uses GATE Developer and GATE Embedded to construct resources of the three types . This may involve programming , or the development of Language Resources such as grammars that are used by existing Processing Resources , or a mixture of both . GATE Developer is used for visualisation of the data structures produced and consumed during processing , and for debugging , performance measurement and so on . For example , figure 1.1 is a screenshot of one of the visualisation tools . GATE Developer is analogous to systems like Mathematica for Mathematicians , or JBuilder for Java programmers : it provides a convenient graphical environment for research and development of language processing software . When an appropriate set of resources have been developed , they can then be embedded in the target client application using GATE Embedded . GATE Embedded is supplied as a series of JAR files . 9 To embed GATE - based language processing facilities in an application , these JAR files are all that is needed , along with JAR files and XML configuration files for the various resources that make up the new facilities . GATE includes resources for common LE data structures and algorithms , including documents , corpora and various annotation types , a set of language analysis components for Information Extraction and a range of data visualisation and editing components . GATE supports documents in a variety of formats including XML , RTF , email , HTML , SGML and plain text . In all cases the format is analysed and converted into a single unified model of annotation . The annotation format is a modified form the TIPSTER format [ Grishman 97 ] which has been made largely compatible with the Atlas format [ Bird & Liberman 99 ] , and uses the now standard mechanism of ' stand - off markup ' . GATE documents , corpora and annotations are stored in databases of various sorts , visualised via the development environment , and accessed at code level via the framework . See Chapter 5 for more details of corpora etc . . A family of Processing Resources for language analysis is included in the shape of ANNIE , A Nearly - New Information Extraction system . These components use finite state techniques to implement various tasks from tokenisation to semantic tagging or verb phrase chunking . All ANNIE components communicate exclusively via GATE 's document and annotation resources . See Chapter 6 for more details . Other CREOLE resources are described in Part III . JAPE , a Java Annotation Patterns Engine , provides regular - expression based pattern / action rules over annotations - see Chapter 8 . The ' annotation diff ' tool in the development environment implements performance metrics such as precision and recall for comparing annotations . Typically a language analysis component developer will mark up some documents by hand and then use these along with the diff tool to automatically measure the performance of the components . See Chapter 10 . GUK , the GATE Unicode Kit , fills in some of the gaps in the JDK 's 10 support for Unicode , e.g. by adding input methods for various languages from Urdu to Chinese . See Section 3.10.2 for more details . This section gives a very brief example of a typical use of GATE to develop and deploy language processing capabilities in an application , and to generate quantitative results for scientific publication . Let 's imagine that a developer called Fatima is building an email client 11 for Cyberdyne Systems ' large corporate Intranet . In this application she would like to have a language processing system that automatically spots the names of people in the corporation and transforms them into mailto hyperlinks . A little investigation shows that GATE 's existing components can be tailored to this purpose . Fatima starts up GATE Developer , and creates a new document containing some example emails . She then loads some processing resources that will do named - entity recognition ( a tokeniser , gazetteer and semantic tagger ) , and creates an application to run these components on the document in sequence . Having processed the emails , she can see the results in one of several viewers for annotations . The GATE components are a decent start , but they need to be altered to deal specially with people from Cyberdyne 's personnel database . Therefore Fatima creates new ' cyber- ' versions of the gazetteer and semantic tagger resources , using the ' bootstrap ' tool . This tool creates a directory structure on disk that has some Java stub code , a Makefile and an XML configuration file . After several hours struggling with badly written documentation , Fatima manages to compile the stubs and create a JAR file containing the new resources . She tells GATE Developer the URL of these files 12 , and the system then allows her to load them in the same way that she loaded the built - in resources earlier on . Fatima then creates a second copy of the email document , and uses the annotation editing facilities to mark up the results that she would like to see her system producing . She saves this and the version that she ran GATE on into her serial datastore . From now on she can follow this routine : . Run her application on the email test corpus . Check the performance of the system by running the ' annotation diff ' tool to compare her manual results with the system 's results . This gives her both percentage accuracy figures and a graphical display of the differences between the machine and human outputs . Make edits to the code , pattern grammars or gazetteer lists in her resources , and recompile where necessary . To make the alterations that she requires , Fatima re - implements the ANNIE gazetteer so that it regenerates itself from the local personnel data . She then alters the pattern grammar in the semantic tagger to prioritise recognition of names from that source . This latter job involves learning the JAPE language ( see Chapter 8 ) , but as this is based on regular expressions it is n't too difficult . Eventually the system is running nicely , and her accuracy is 93 % ( there are still some problem cases , e.g. when people use nicknames , but the performance is good enough for production use ) . Now Fatima stops using GATE Developer and works instead on embedding the new components in her email application using GATE Embeddded . She takes the accuracy measures that she has attained for her system and writes a paper for the Journal of Nasturtium Logarithm Encitement describing the approach used and the results obtained . Because she used GATE for development , she can cite the repeatability of her experiments and offer access to example binary versions of her software by putting them on an external web server . This section contains an incomplete list of publications describing systems that used GATE in competitive quantitative evaluation programmes . These programmes have had a significant impact on the language processing field and the widespread presence of GATE is some measure of the maturity of the system and of our understanding of its likely performance on diverse text processing tasks . describes the performance of an SVM - based learning system in the NTCIR-6 Patent Retrieval Task . The system achieved the best result on two of three measures used in the task evaluation , namely the R - Precision and F - measure . The system obtained close to the best result on the remaining measure ( A - Precision ) . describes a cross - source coreference resolution system based on semantic clustering . It uses GATE for information extraction and the SUMMA system to create summaries and semantic representations of documents . One system configuration ranked 4th in the Web People Search 2007 evaluation . describes a cross - lingual summarization system which uses SUMMA components and the Arabic plugin available in GATE to produce summaries in English from a mixture of English and Arabic documents . Open - Domain Question Answering : . The University of Sheffield has a long history of research into open - domain question answering . GATE has formed the basis of much of this research resulting in systems which have ranked highly during independent evaluations since 1999 . The first successful question answering system developed at the University of Sheffield was evaluated as part of TREC 8 and used the LaSIE information extraction system ( the forerunner of ANNIE ) which was distributed with GATE [ Humphreys et al . 99 ] . Further research was reported in [ Scott & Gaizauskas . 00 ] , [ Greenwood et al . 02 ] , [ Gaizauskas et al . 03 ] , [ Gaizauskas et al . 04 ] and [ Gaizauskas et al . 05 ] . In 2004 the system was ranked 9th out of 28 participating groups . describes techniques for answering definition questions . The system uses definition patterns manually implemented in GATE as well as learned JAPE patterns induced from a corpus . In 2004 , the system was ranked 4th in the TREC / QA evaluations . describes a multidocument summarization system implemented using summarization components compatible with GATE ( the SUMMA system ) . The system was ranked 2nd in the Document Understanding Evaluation programmes . describe participation in the TIDES surprise language program . ANNIE was adapted to Cebuano with four person days of effort , and achieved an F - measure of 77.5 % . Unfortunately , ours was the only system participating ! describe results obtained on systems designed for the ACE task ( Automatic Content Extraction ) . Although a comparison to other participating systems can not be revealed due to the stipulations of ACE , results show 82%-86 % precision and recall . To get HTML reports from profiled processing resources , there is a new menu item in the ' Tools ' menu called ' Profiling reports ' , see chapter 11 . To deal with quality assurance of annotations , one component has been updated and two new components have been added . The annotation diff tool has a new mode to copy annotations to a consensus set , see section 10.2.1 . An annotation stack view has been added in the document editor and it allows to copy annotations to a consensus set , see section 3.4.3 . A corpus view has been added for all corpus to get statistics like precision , recall and F - measure , see section 10.3 . An annotation stack view has been added in the document editor to make easier to see overlapping annotations , see section 3.4.3 . Added an isInitialised ( ) method to gate . Gate ( ) . The ontology API ( package gate.creole.ontology has been changed , the existing ontology implementation based on Sesame1 and OWLIM2 ( package gate.creole.ontology.owlim ) has been moved into the plugin Ontology_OWLIM2 . An upgraded implementation based on Sesame2 and OWLIM3 that also provides a number of new features has been added as plugin Ontology . See Section 14.12 for a detailed description of all changes . The new Imports : statement at the beginning of a JAPE grammar file can now be used to make additional Java import statements available to the Java RHS code , see 8.6.5 . The User Guide has been amalgamated with the Programmer 's Guide ; all material can now be found in the User Guide . The ' How - To ' chapter has been converted into separate chapters for installation , GATE Developer and GATE Embedded . Other material has been relocated to the appropriate specialist chapter . Plugin names have been rationalised . Mappings exist so that existing applications will continue to work , but the new names should be used in the future . Plugin name mappings are given in Appendix B . The Montreal Transducer has been made obsolete . The UIMA integration layer ( Chapter 18 ) has been upgraded to work with Apache UIMA 2.2.2 . The JAPE debugger has been removed . Debugging of JAPE has been made easier as stack traces now refer to the JAPE source file and line numbers instead of the generated Java source code . Oracle and PostGreSQL are no longer supported . The MIAKT Natural Language Generation plugin has been removed . The Minorthird plugin has been removed . Minorthird has changed significantly since this plugin was written . We will consider writing an up - to - date Minorthird plugin in the future . A new gazetteer , Large KB Gazetteer ( in the plugin ' Gazetteer_LKB ' ) has been added , see Section 13.9 for details . gate.creole.tokeniser.chinesetokeniser.ChineseTokeniser and related resources under the plugins / ANNIE / tokeniser / chinesetokeniser folder have been removed . Please refer to the Lang_Chinese plugin for resources related to the Chinese language in GATE . A number of improvements to the benchmarking support in GATE . JAPE transducers now log the time spent in individual phases of a multi - phase grammar and by individual rules within each phase . Other PRs that use JAPE grammars internally ( the pronominal coreferencer , English tokeniser ) log the time taken by their internal transducers . A reporting tool , called ' Profiling reports ' under the ' Tools ' menu makes summary information easily available . For more details , see chapter 11 . We have added a new PR called ' Segment Processing PR ' . As the name suggests this PR allows processing individual segments of a document independently of one other . For more details , please look at the section 16.2.8 . The gate . Controller implementations provided with the main GATE distribution now also implement the gate . ProcessingResource interface . This means that an application can now contain another application as one of its components . LingPipe is a suite of Java libraries for the linguistic analysis of human language . We have provided a plugin called ' LingPipe ' with wrappers for some of the resources available in the LingPipe library . For more details , see the section 19.15 . OpenNLP provides tools for sentence detection , tokenization , pos - tagging , chunking and parsing , named - entity detection , and coreference . The tools use Maximum Entropy modelling . We have provided a plugin called ' OpenNLP ' with wrappers for some of the resources available in the OpenNLP Tools library . For more details , see section 19.16 . A new plugin has been added to provide an easy route to integrate taggers with GATE . The Tagger_Framework plugin provides examples of incorporating a number of external taggers which should serve as a starting point for using other taggers . See Section 17.4 for more details . reviews the current state of the art in email processing and communication research , focusing on the roles played by email in information management , and commercial and research efforts to integrate a semantic - based approach to email . investigates two techniques for making SVMs more suitable for language learning tasks . Firstly , an SVM with uneven margins ( SVMUM ) is proposed to deal with the problem of imbalanced training data . Secondly , SVM active learning is employed in order to alleviate the difficulty in obtaining labelled training data . The algorithms are presented and evaluated on several Information Extraction ( IE ) tasks . presents a semantic - based prototype that is made for an open - source software engineering project with the goal of exploring methods for assisting open - source developers and software users to learn and maintain the system without major effort . discusses methods of measuring the performance of ontology - based information extraction systems , focusing particularly on the Balanced Distance Metric ( BDM ) , a new metric we have proposed which aims to take into account the more flexible nature of ontologically - based applications . describes the development of a system for content mining using domain ontologies , which enables the extraction of relevant information to be fed into models for analysis of financial and operational risk and other business intelligence applications such as company intelligence , by means of the XBRL standard . describes experiments for the cross - document coreference task in SemEval 2007 . Our cross - document coreference system uses an in - house agglomerative clustering implementation to group documents referring to the same entity . describes the application of ontology - based extraction and merging in the context of a practical e - business application for the EU MUSING Project where the goal is to gather international company intelligence and country / region information . studies Japanese - English cross - language patent retrieval using Kernel Canonical Correlation Analysis ( KCCA ) , a method of correlating linear relationships between two variables in kernel defined feature spaces . ( Proceedings of the 5th International Semantic Web Conference ( ISWC2006 ) ) In this paper the problem of disambiguating author instances in ontology is addressed . We describe a web - based approach that uses various features such as publication titles , abstract , initials and co - authorship information . describes work in progress concerning the application of Controlled Language Information Extraction - CLIE to a Personal Semantic Wiki - Semper- Wiki , the goal being to permit users who have no specialist knowledge in ontology tools or languages to semi - automatically annotate their respective personal Wiki pages . discusses existing evaluation metrics , and proposes a new method for evaluating the ontology population task , which is general enough to be used in a variety of situation , yet more precise than many current metrics . describes an approach that allows users to create and edit ontologies simply by using a restricted version of the English language . The controlled language described is based on an open vocabulary and a restricted set of grammatical constructs . ( Proceedings of Fifth International Conference on Recent Advances in Natural Language Processing ( RANLP2005 ) ) It is a full - featured annotation indexing and search engine , developed as a part of the GATE . It is powered with Apache Lucene technology and indexes a variety of documents supported by the GATE . ( Proceedings of Ninth Conference on Computational Natural Language Learning ( CoNLL-2005 ) ) uses the uneven margins versions of two popular learning algorithms SVM and Perceptron for IE to deal with the imbalanced classification problems derived from IE . ( Proceedings of the 2nd European Workshop on the Integration of Knowledge , Semantic and Digital Media Technologies ( EWIMT 2005))Digital Media Preservation and Access through Semantically Enhanced Web - Annotation . describes a sentence extraction system that produces two sorts of multi - document summaries ; a general - purpose summary of a cluster of related documents and an entity - based summary of documents related to a particular person . 8 The terms ' resource ' and ' component ' are synonymous in this context . ' Resource ' is used instead of just ' component ' because it is a common term in the literature of the field : cf . the Language Resources and Evaluation conference series [ LREC-1 98 , LREC-2 00 ] . 9 The main JAR file ( gate.jar ) supplies the framework . Built - in resources and various 3rd - party libraries are supplied as separate JARs ; for example ( guk.jar , the GATE Unicode Kit . ) contains Unicode support ( e.g. additional input methods for languages not currently supported by the JDK ) . They are separate because the latter has to be a Java extension with a privileged security profile . 10 JDK : Java Development Kit , Sun Microsystem 's Java implementation . Unicode support is being actively improved by Sun , but at the time of writing many languages are still unsupported . In fact , Unicode itself does n't support all languages , e.g. Sylheti ; hopefully this will change in time . 11 Perhaps because Outlook Express trashed her mail folder again , or because she got tired of Microsoft - specific viruses and had n't heard of Netscape or Emacs . 12 While developing , she uses a file:/ ... URL ; for deployment she can put them on a web server . 13 Languages other than Java require an additional interface layer , such as JNI , the Java Native Interface , which is in C. \"}",
        "_version_":1692670548559003648,
        "score":184.2426},
      {
        "id":"9e40f1f0-4308-4ccb-9b33-f7513a6a6abe",
        "_src_":"{\"url\": \"http://www.shareable.net/blog/differentiating-the-sharing-economy-from-the-anarchy-economy\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168065.93/warc/CC-MAIN-20160205193928-00322-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Affiliated with . Abstract . Background . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system . It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources , integrating diverse text processing applications , and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Evaluations were performed on each system , resulting in F - scores ranging from .26 - .72 ( precision .39 - .85 , recall .16 - .85 ) . Additionally , each of these systems was run over all abstracts in MEDLINE , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . Conclusion . OpenDMAP advances the performance standards for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . The output of the information extraction system is always constructed from elements of an ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction . Electronic supplementary material . The online version of this article ( doi : 10 . 1186/\\u200b1471 - 2105 - 9 - 78 ) contains supplementary material , which is available to authorized users . Background . Conceptual analysis is the process of mapping from natural language texts to a formal representation of the objects and predicates ( together , the concepts ) meant by the text . The history of attempts to build programs to do conceptual analysis dates back to at least 1967 [ 1 ] . Recent advances in the availability of high quality ontologies , in the ability to accurately recognize named entities in texts , and in language processing methods generally have made possible a significant advance in concept analysis , arguably the most difficult and general natural language processing task . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system that significantly advances the state of the art . We also discuss its application to three important information extraction tasks in molecular biology . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . In a recent PLoS Biology essay Rebholz - Schuhmann [ 2 ] argued , \\\" It is only a matter of time and effort before we are able to extract facts [ from articles in the primary literature ] automatically . The consequences are likely to be profound . \\\" Existing examples include extraction of information about gene - gene interactions [ 3 ] , alternative splicing [ 4 ] , functional analysis of mutations [ 5 ] , phosphorylation sites [ 6 ] , and regulatory sites [ 7 ] . The primary significance of OpenDMAP to these efforts is that it leverages the large - scale efforts being made in biomedical ontology development , such as the Open Biomedical Ontologies Foundry ( OBO Foundry ) [ 8 ] . Logical representations of reality , such as those built on the OBO Foundry , use a set of predicates that formally describe properties of , or relationships among , objects . Predicates are defined with a specific number and type of admissible arguments . For example , the predicate expresses might be specified to take two arguments , a gene and a cell type , meaning that the specified gene is expressed in all normal cells of the specified type . Such predicates can also be related to each other through abstraction ( \\\" is a \\\" ) and packaging ( \\\" part of \\\" ) hierarchies , as done in the OBO Foundry . The semantics defined by the predicates and hierarchies in such ontologies provide a powerful tool for natural language processing . Independently constructed ontologies have played at best a modest role in prior natural language processing systems . Other language processing systems have used either small , ad hoc conceptual representations developed specifically for the application , or structured linguistic resources , such as WordNet [ 11 ] , which do not meet the logical requirements for an ontology . While the implementation reported below exploits only a small portion of the OBO Foundry , and the crucial Relationship Ontology component of the Foundry is still in an early stage of development , the organizing principles of OpenDMAP generalize straightforwardly . These systems and their extensions have been used to extract semantic relationships relevant to pharmacogenomics [ 16 ] and to compare alternative sources of information [ 17 ] , among other applications . OpenDMAP is like MetaMap and its descendents in that it can only produce output drawn from a predefined semantic representation . The main difference is that MetaMap , SemRep and SemGen are structured as traditional NLP systems , with a lexicon that enumerates possible concepts that might be associated with a word or phrase . Multiple possible mappings are returned , with rankings . OpenDMAP provides an alternative method of organizing knowledge about language , so that each concept has associated with it a set of patterns that describe how that concept can be realized in language ; there is no explicit lexicon . To appreciate the differences between OpenDMAP and previous work in biomedical text mining , it is also useful to contrast its handling of syntactic structure and of semantic content with other systems . At one end of the spectrum are systems that employ essentially asyntactic representations . Early in the modern period of genomic natural language processing , some such systems were able to achieve significant ( and in some cases ground - breaking ) results using techniques based on text literals only . These include [ 18 - 20 ] . One line of subsequent work has attempted to increase the coverage of these early systems , which utilized manually - built patterns , by automatically acquiring considerably larger sets of patterns - see , for example , Huang et al . 2004 [ 21 ] . Another line of subsequent work has focused on adding a modest , but still useful , level of linguistic abstraction by explicitly including either lexical categories ( parts of speech ) , word stems , or both [ 22 , 23 ] . These systems were essentially agrammatical ; in contrast , OpenDMAP utilizes a classic form of \\\" semantic grammar , \\\" freely mixing text literals , semantically typed basal syntactic constituents , and semantically defined classes of entities . Although OpenDMAP is capable of utilizing full syntactic parses , the patterns for the three separate tasks discussed in this paper utilize primarily shallow syntactic parses ( the development phase of the transport project reports results using syntactic dependency information ) . It remains to be seen what depth of syntactic parsing is useful in biomedical text mining . All of the systems discussed thus far have in common the fact that they employ some notion of explicit patterns , be they agrammatical , syntactic , or semantic . In a separate line of work , patterns are entirely implicit - that is , they exist only to the extent that they are captured by orthogonal features . This work approaches relation extraction as a classification problem ; a classic example is the work of Craven and Kumlein 1999 [ 32 ] . Bunescu et al . 2005 [ 33 ] presents a detailed analysis of a number of classification - based approaches ; the state of the art is characterized by the participants in the recent BioCreative protein - protein interaction shared task [ 34 ] . OpenDMAP has been applied in three domains : protein transport , protein - protein interaction and the expression of a gene in a particular cell type . The three application domains are independently significant . Protein transport , the directed movement of proteins from one cellular compartment to another , is a broadly important biological phenomenon . Although protein subcellular localization information is centralized ( e.g. through ontological annotations at NCBI and in various model organism databases ) , information about transport is not . Protein transport information is published throughout the scientific literature , but no previous method was able to capture it systematically . Protein - protein interaction extraction has been the subject of dozens of systems ( see , e.g. a review in [ 35 ] ) . Widely used web resources such as IHOP [ 3 ] and Chilibot [ 36 ] are based entirely on automated extraction of protein - protein interactions from text . This task was used in the BioCreative community evaluation , described below . The protein transport task is illustrative of another distinguishing aspect of the OpenDMAP approach : it provides mechanisms for handling relationships involving more than two entities . Note that the protein transport predicate has at least three arguments : what protein is transported , from where , and to where ( our model also includes a fourth argument : the transporting protein ) . Although some linguistic expressions of the concept may elide an argument , the predicate itself inherently describes a greater than binary relationship . Wattarujeekrit et al . [ 38 ] and Cohen and Hunter [ 39 ] present evidence that many important predicates in biomedicine require more than two arguments . However , most previous efforts at extracting relationships from biomedical text have addressed exclusively binary relationships . Geneways [ 40 ] and RLMPS - P [ 41 ] are the only other biomedical IE systems of which we are aware that extracted greater than binary relationships , and neither is ontology - driven . Assessing the accuracy of an information extraction system is a very labor - intensive activity . In order to identify information that could have been extracted , but was not ( a \\\" false negative \\\" ) , a person must go through a large volume of text to determine all of the relevant assertions . To estimate the reliability of these manually derived assertions , at least two people must complete that task to assess inter - rater reliability . Once such data is used for one evaluation and system developers have seen it , further use of the data will generate upwardly biased accuracy estimates as system developers fit their systems to it . For these reasons , large - scale community evaluations of information extraction systems are particularly important . The second Critical Assessment of Information Extraction in Biology , ( BioCreative ) [ 34 , 42 ] , community evaluation included a test of systems designed to extract human protein - protein interaction information from the full texts of hundreds of journal articles , called the IPS task . Human curators from the IntAct database [ 43 ] manually extracted interaction assertions from these articles using the same curatorial standards as for the database . The results produced by human experts were compared to the results submitted from 45 systems developed by laboratories around the world , providing the best current assessment of the accuracy of protein interaction information extraction systems . The performance of OpenDMAP on the protein interaction task was evaluated as part of this shared task . More limited evaluations of the accuracy in the other applications are also reported in the results section . The accuracy of an information extraction system depends on the genre of texts on which it operates [ 44 ] . This report demonstrates the application of OpenDMAP to full texts of scientific journal articles , to Medline abstracts , and to GeneRIFs ( single sentences or sentence fragments that are selected by human curators for relevance to the function of a particular gene product ) . GeneRIFs are particularly attractive targets for information extraction , due to their roughly sentential length ( identified by [ 44 ] as the optimum ) , breadth of coverage , manual preselection for relevance , and association with at least one normalized gene reference . Despite these attractive features , this is the first report of an information extraction system targeting them . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Each of these systems was run over all abstracts in Medline as of June 18 , 2007 , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . These results are provided in RDF format in the Additional Files 1 , 2 , 3 , 4 . One particularly striking result is the diversity of journals from which these assertions were mined . The transport relationships were extracted from 2,340 different journals ; the interaction relationships from 4,103 different journals ; and the expression relationships from 2,984 different journals . A total of 4,434 unique journals contributed to these results , nearly 40 % of the journals indexed in Medline each year ( see Figure 1 ) . OpenDMAP coverage of MEDLINE . The gray bars indicate the number of journals indexed by MEDLINE each year . The red bars indicate the number of journal abstracts from which OpenDMAP extracted at least one assertion regarding transport , interaction or expression . In recent years , more than 40 % of biomedical journals contain such information . 2007 is partial data ( through July 1 ) . For the BioCreative evaluation , the interaction system was run on the full texts of all of the 359 articles in the test set , producing 385 interaction assertions . Performance was averaged per article , since a few articles had a very large number of interactions and would have dominated a per assertion calculation . OpenDMAP 's average F - measure of 0.29 was 10 % higher than the next best scoring system , and more than three standard deviations above the mean performance . OpenDMAP 's recall was similar to the other high scoring systems ; its advantage arose from being substantially more precise ( fewer false positives ) , achieving an average precision of 0.39 , more than 20 % better than the next best system . Due to IntAct 's curation criteria , which require clear experimental evidence for an interaction in the text , these results are quite conservative . Many \\\" false positives \\\" were in fact assertions of interactions , but fell short of the evidential requirements for IntAct curation . A manual evaluation of the performance of the protein transport recognition system was based on all 570 GeneRIFs containing a form of the word \\\" translocate \\\" ( 382 of which were about protein transport , and 188 were about the transport of something else ) . Since transport is a greater than binary relationship , the extraction was only counted as correct if all of the components extracted matched the human annotation . For that strict criterion , OpenDMAP achieved precision of 0.75 and a recall of 0.49 ( F - score of 0.59 ) . If incomplete extractions are counted as correct , precision is unchanged at 0.75 and recall rises to 0.67 ( F - score of 0.71 ) . A substantial proportion of the errors were due to imperfect recognition of proteins ; if OpenDMAP is given correct protein identifications as inputs , precision is 0.77 , strict recall is 0.67 ( F - score of 0.72 ) and incomplete recall is 0.85 ( F - score of 0.81 ) . A manual evaluation of the performance of the expression recognition system was based on 324 GeneRIFs containing a form of the word \\\" express , \\\" ( these sentences contained 469 assertions about expression , 205 of which were about gene expression in 178 different cell types ) . Open DMAP had a precision of 0.64 , but missed many statements that annotators identified as expression assertions , achieving a recall of only 0.16 ( F - score of 0.26 ) . A substantial portion of these errors were due to imperfect recognition of gene names ; if OpenDMAP is given correct gene identifications as input , precision is 0.85 and recall is 0.36 ( F - score of 0.51 ) . Many other failures to identify expression assertions were related to coordination ; the test set had an average of more than two expression assertions per sentence , but the IE system extracted only about 1.3 assertions per sentence . Discussion . As demonstrated by its performance in the community evaluation , OpenDMAP advances the state of the art for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . There are several reasons why OpenDMAP exhibits better performance than any other biomedical information extraction system to date . OpenDMAP is an extension of the Direct Memory Access Parsing ( DMAP ) paradigm described in [ 45 ] and [ 46 ] . Three innovations distinguish the present work from those prior efforts . First , the ontology component of OpenDMAP is independent of the rest of the system . The knowledge representation component is the well - established , open source Prot\\u00e9g\\u00e9 ontology development system [ 47 , 48 ] , and OpenDMAP concept analyzers can be associated with any ontology compatible with Prot\\u00e9g\\u00e9 , for example , the OBO Foundry . Second , OpenDMAP is fully integrated with the open source Unstructured Information Management Architecture , ( UIMA ) [ 49 - 51 ] , which allows the results of any text processing application interfaced to UIMA to be exploited by the OpenDMAP system . As demonstrated below , this mechanism facilitates the use of many external language processing systems , including tokenizers , sentence boundary detectors , entity recognition systems , and syntactic parsers . Since the inputs and outputs of each system are mapped by UIMA to a common annotation structure accessed by OpenDMAP , the use , comparison and combination of various approaches to language processing can all be fully integrated into OpenDMAP patterns . The third innovation in the OpenDMAP system is an expanded pattern language for specifying how concepts can be expressed in text . The intimate connection between the ontology and the natural language processing system provides two significant advantages over prior information extraction systems generally . First , the output of the information extraction system is always constructed from elements of the ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . In contrast , the outputs of most natural language processing systems are grounded only in substrings of text , not normalized to any model at all . Progress in normalizing biological entities recognized in text to specific database identifiers [ 52 - 54 ] has made the output of text processing systems much more valuable . Mapping the properties and relationships extracted to a community ontology similarly provides a significant increment in the value of the output from text processing systems . The second advantage of the OpenDMAP approach is that all of the knowledge used by the system to recognize concepts is structured by the ontology . In contrast , the nearly universal alternative approach is to embody knowledge of language into a lexicon , which associates individual lexical items with their possible semantic interpretations . In the OpenDMAP approach , information about which concepts are potentially relevant to the analysis of a particular text passage straightforwardly places limits on the linguistic knowledge relevant to analyzing that passage . This approach finesses many difficult ambiguity resolution problems faced by lexicon - driven systems , since these limits on the knowledge applied to conceptual analysis prevent many multiple interpretation problems from arising at all . For example , the string \\\" hunk \\\" refers to a cell type ( human natural killer cells ) , a gene ( hormonally upregulated Neu - associated kinase ) , and the general English word meaning a large piece of something without definite shape . A traditional , lexicon - driven system would have an explicit method for assigning the correct word sense to any occurrence of the string \\\" hunk . \\\" The fact that there might be possible alternative interpretations of the matching string has no consequence , and no explicit ambiguity resolution step is necessary . Ambiguity is a leading cause of errors in text processing systems , and this approach is one of the contributing factors to OpenDMAP 's superior performance . Our top - down approach to restricting possible interpretations does not address all problems due to ambiguity in language ; for example , errors in preprocessing systems ( e.g. syntactic parsing , see below ) are not effected . The use of UIMA greatly facilitates the incorporation of various applications as input to OpenDMAP . The outputs of NLP tools integrated into the system are described by the extensible UIMA type system . In the case that a new type of information is produced by a preprocessor , OpenDMAP patterns would have to be modified to take advantage of the new type of information available . For example , the first time an external cell type tagging system is added , the UIMA type of the result of that processor must be linked to a cell - type concept in an OpenDMAP ontology in order for it to be used in patterns . However , if a new NLP tool produces a UIMA output type that has been used by OpenDMAP previously , then no changes in the ontology or patterns are needed . We believe that the outputs of information extraction systems are not likely to useful until the F - score ( or at least the precision ) is greater than about 0.85 [ 34 ] , so the various sources of error in these systems must be addressed . A significant cause of errors in the OpenDMAP system as evaluated is incorrect identification of gene and protein names . The UIMA architecture makes it trivial to adopt and exploit better gene / protein recognition systems as they are developed . Use of such a system should improve the performance of OpenDMAP . Error analysis of the false positives in the transport data set indicates that more than 80 % are due to errors in the syntactic analysis . For example , in the sentence \\\" Rho protein regulates the tyrosine phosphorylation of FAK through translocation from the nucleus to the membrane , \\\" the subject of the translocation was incorrectly identified as FAK ( rather than Rho ) by the Stanford parser . That parser was developed for general English rather than biomedical text , so using specialized syntactic analysis systems may improve the precision of OpenDMAP . Remaining problems in false positives are due to problematic tokenization , failures to properly resolve anaphoric reference , and , rarely , negation . False negatives are due to gaps in concept recognition patterns , more than half of which arise from a failure to properly handle coordinated clauses and conjunctions . Addressing these issues remains an open area of research . Another issue was that the Stanford parser was too slow to use in the application of the transport system to all of Medline , so it was n't run . OpenDMAP ignores aspects of patterns that require inputs that are n't present , so the patterns that contained syntactic dependencies did not have to be altered . These syntactic constraints are important for accuracy , however . Tested on the gold standard set for the system without the parser precision drops to 0.62 , while strict recall remains largely unchanged , rising to 0.51 . Conclusion . Despite OpenDMAP elevating the state of the art for biomedical information extraction significantly beyond previous levels , error rates remain high . In the most challenging BioCreative task , finding curatable assertions in full text documents , only about 29 % of the relevant assertions were found , and only about 39 % of the extracted assertions were completely correct . Such error rates mean that automatically generated databases can not replace manual curation efforts . However , the evidence is quite clear that manual curation can not keep up with the rate of data generation [ 56 ] . The surprisingly large number of journals that contained information relevant to these three IE tasks suggests that the temporal approach taken in [ 56 ] may actually underestimate the severity of the problem . Although the outputs produced by large - scale IE systems are not yet suitable for producing factual databases for direct use by biomedical researchers , the current level of performance provides two important facilities to the research community . First , the results of these efforts can be used to significantly increase the efficiency of manual curation efforts . Each extracted assertion is tied to a specific text , which can be used to direct the attention of manual curators both to relevant documents and to specific relevant passages within a document . Effective integration of IE results into curatorial workflows will require the development of new tools . OpenDMAP developers are working with curators at IntAct to address these issues . The open source availability of OpenDMAP will facilitate the work of others addressing this issue as well . The second important use of the sorts of results that IE systems are currently able to generate is in statistical integration with multiple sources of noisy data , such as those described in [ 57 ] and [ 58 ] . As demonstrated in the latter , the proper addition of even noisy data from the literature substantially improves the quality and coverage of protein - protein interaction networks for several species . Methods . OpenDMAP uses Prot\\u00e9g\\u00e9 [ 47 ] to provide an object model for the possible concepts ( predicates and objects ) that might be found in a text . Prot\\u00e9g\\u00e9 models concepts ( including actions ) as classes that participate in abstraction and packaging hierarchies , and relationships as class - specific slots . For example , protein transport is modeled as a class ( called PROTEIN - TRANSPORT ) and the relationship between a transport event and the protein transported in that event is represented as a slot in that class ( called [ TRANSPORTED - ENTITY ] ) . Slots can take on values , which can be constrained to be instances of other classes . For example , the [ TRANSPORTED - ENTITY ] slot of the PROTEIN - TRANSPORT class is constrained to be an instance of either of the classes PROTEIN or MOLECULAR - COMPLEX . Figure 2 shows a portion of the model used for the transport , which includes biological entities , such as molecular complexes and cellular components , and biological processes , particularly protein transport . This model is drawn almost entirely from the Gene Ontology ( GO ) [ 53 ] although the relationships that define the four slots shown in Figure 2 are from a provisional submission to the OBO Foundry Relationship Ontology and are not official . Preprocessing tools ( ABNER [ 55 ] and LingPipe [ 59 ] ) were applied to tag instances of proteins , genes , and cell types . Screenshot of the Prot\\u00e9g\\u00e9 ontology for the protein transport task . The slots of the protein transport class are shown in the lower right panel of this screen shot . Note that the subclasses of Cellular Component and Protein Transport are not shown . For the transport task , patterns were produced for 30 ontology concepts ; eight directly related to transport and 22 others for cellular components that are the sources and destinations of transport . A large number of other concepts ( e.g. genes , proteins and cell types ) do not have explicit patterns associated with them , but are instead tagged as such by UIMA tools during preprocessing . The protein - protein interaction task involved producing patterns for nine concepts , and the cell expression task required patterns for six additional concepts . The UIMA architecture [ 49 ] manages the processing of document sets . Various combinations of these tools were used in the different applications . For example , the GeneRIFs used in the transport application did not require sentence segmentation , and the applications to all medline abstracts did not use syntactic elements in patterns because the Stanford Parser was too slow to run over all of Medline . The results of this preprocessing are stored in UIMA 's common annotation structure . In order to be able to recognize a concept in text , OpenDMAP associates one or more patterns with each concept . A pattern describes the words , phrases , parts of speech , syntactic structures or concepts that should cause an instance of the associated concept to be recognized . A simple pattern , such as the one shown in equation 1 , enumerates a disjunction of words that should trigger recognition of a concept . The patterns for all of the CELLULAR - COMPONENT concepts were derived from the GO term names and synonyms , supplemented with derivational variants , such as the adjectival \\\" nuclear \\\" in equation 1 . Twenty - two GO cellular component terms were used , along with 19 synonyms associated with the GO terms , and 78 additional derivational variants generated by inspection of the training corpus . More complex patterns can include references to non - terminals , particularly other concepts . Equation 2 is one of the patterns for recognizing instances of the PROTEIN - TRANSPORT concept . When a pattern that includes a slot name is matched , the instance created has its slots filled with the concepts that matched the slot names in the pattern . For example , the above pattern matches the GeneRIF that contains \\\" ... Bax translocation to mitochondia ... \\\" ( from Entrez GeneID 27113 ) . Since the entire pattern matches , an instance of PROTEIN - TRANSPORT is created , with the Bax protein concept in its [ TRANSPORTED - ENTITY ] slot and an instance of the mitochondria concept ( from GO 's cellular component hierarchy ) in its [ TRANSPORT - DESTINATION ] slot . OpenDMAP patterns can express variability in word and phrase order . Note , for example , that equation 2 would fail to match the phrase \\\" Bax translocation to mitochondria from the cytosol . \\\" The special pattern marker @ is used to identify a set of subpatterns that are both optional and can occur before or after a required phrase ; multiple @ marked phrases can occur in any order . For example , equation 2 can be modified with this marker to recognize the above text : . Many sentences in the literature express multiple concepts , making extraction of even simple assertions problematic . Consider the following GeneRIF from GeneID:29560 : \\\" ... HIF-1alpha which is present in glomus cells translocates to the nucleus .... \\\" The intervening phrase \\\" which is present in glomus cells \\\" prevents the pattern in equation 3 from matching that sentence . OpenDMAP does have a wildcard character ( underscore ) that could be added to the pattern in equation 3 , between the [ TRANSPORTED - ENTITY ] concept and the word \\\" translocation , \\\" allowing this sentence to be matched . However , using such a wild card would make any protein mentioned before the word \\\" translocation \\\" match the pattern , which is too promiscuous . To address this problem , OpenDMAP allows patterns to specify syntactic constraints on potential matches . Furthermore , the reliance on the exact word \\\" translocation \\\" can be relaxed to be any reference to a transport action word , including both verbal and nominal forms of multiple terms ( e.g. , transported , translocation ) . The PROTEIN - TRANSPORT class is extended to have an [ action ] slot that specifies the type of transportation action , to keep track of the term that was used . Equation 4 demonstrates the pattern language for specifying syntactic constraints : . The use of the variable \\\" x \\\" in the specification identifies a specific syntactic unit , linking the dependency to the head of a phrase . Multiple variables can be used to specify constraints on different syntactic units within a sentence . OpenDMAP patterns are very powerful . Only five such patterns , shown in equations 5 - 9 were required for the transport extraction system performance noted above . These patterns were devised manually , based on expert knowledge of the domain and on a small training set of sample GeneRIFs . The test data used in the transport and expression evaluations were marked up by domain experts trained in conceptual annotation , using the Knowtator annotation tool [ 62 ] . Availability of data and software . The results of the information extraction effort are available as RDF format files in the Additional Files 1 , 2 , 3 , 4 . Declarations . Acknowledgements . This work was funded by NIH grants R01NLM008111 and R01NLM009254 to LH . ZL was also supported in part by the Intramural Research Program of the NIH , NLM . Electronic supplementary material . 1471 - 2105 - 9 - 78-S1.GZ Additional file 1 : Transport instances from MEDLINE . This file contains the RDF formatted instances of transport , mined from MEDLINE with OpenDMAP . ( GZ 2785 kb ) . 1471 - 2105 - 9 - 78-S2.GZ Additional file 2 : Interaction instances from MEDLINE , part 1 . The interaction data set is very large . This file contains the first half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 6035 kb ) . 1471 - 2105 - 9 - 78-S3.GZ Additional file 3 : Interaction instances from MEDLINE , part 2 . The interaction data set is very large . This file contains the second half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 5994 kb ) . 1471 - 2105 - 9 - 78-S4.GZ Additional file 4 : Expression instances from MEDLINE . This file contains the RDF formatted instances of expression , mined from MEDLINE with OpenDMAP . ( GZ 7154 kb ) . Authors ' contributions . LH conceived of the project , supervised the design and implementation of the system and wrote the manuscript . ZL was responsible for the transport project , including writing the patterns and analyzing the results ; he also contributed suggestions for the interaction task . JRF implemented the OpenDMAP pattern recognition engine and its UIMA wrapper . WAB wrote all other infrastructure software , including the other UIMA wrappers , managed the data , applied OpenDMAP to all of MEDLINE , and designed and built other software . HLJ was responsible for the interaction and expression projects , including writing the patterns , analyzing the results , and doing the associated error analyses . PVO managed the creation of the gold standard data for transport and contributed to the design of the pattern language syntax . KBC managed the team , selected the preprocessing tools , coordinated and supervised the interaction and expression task efforts , and provided linguistic and software design contributions during all phases of the project . All authors have read and approved this manuscript . Authors ' Affiliations . Center for Computational Pharmacology , University of Colorado School of Medicine . National Center for Biotechnology Information , National Library of Medicine . PowerSet , Inc. . Department of Computer Science , University of Colorado . References . Sparck Jones K : Natural language processing : A historical review . Current Issues in Computational Linguistics : in Honour of Don Walker ( Ed Zampolli , Calzolari and Palmer ) , Amsterdam : Kluwer 1994 . Rebholz - Schuhmann D , Kirsch H , Couto F : Facts from text -- is text mining ready to deliver ? PLoS Biol 2005 , 3 ( 2 ) : e65 . View Article PubMed . Hoffmann R , Valencia A : A gene network for navigating the literature . Nat Genet 2004/07/01 Edition 2004 , 36 ( 7 ) : 664 . View Article PubMed . Shah PK , Jensen LJ , Bou\\u00e9 S , Bork P : Extraction of transcript diversity from scientific literature . PLoS Comput Biol 2005 , 1 ( 1 ) : e10 . View Article PubMed . Horn F , Lau AL , Cohen FE : Automated extraction of mutation data from the literature : application of MuteXt to G protein - coupled receptors and nuclear hormone receptors . Bioinformatics 2004 , 20 ( 4 ) : 557 - 568 . View Article PubMed . Hu ZZ , Narayanaswamy M , Ravikumar KE , Vijay - Shanker K , Wu CH : Literature mining and database annotation of protein phosphorylation using a rule - based system . Bioinformatics 2005/04/09 Edition 2005 , 21 ( 11 ) : 2759 - 2765 . View Article PubMed . Saric J , Jensen LJ , Ouzounova R , Rojas I , Bork P : Extraction of regulatory gene / protein networks from Medline . Bioinformatics 2005/07/28 Edition 2006 , 22 ( 6 ) : 645 - 650 . View Article PubMed . Guarino N : Formal ontology in information systems . Trento , Italy , IOS Press 1998 , 3 - 15 . Hersh W , Bhupatiraju R , Ross L , Johnson P , Cohen A , Kraemer D : TREC 2004 Genomics track overview . National Institute of Standards and Technology 2004 . Fellbaum C : WordNet : An Electronic Lexical Database ( Language , Speech , and Communication ) . MIT Press 1998 . Aronson A : Effective Mapping of Biomedical Text to the UMLS Metathesaurus : The MetaMap Program . AMIA Annu Symp Proc 2001 , 17 - 21 . Rindflesch TC , Fiszman M : The interaction of domain knowledge and linguistic structure in natural language processing : interpreting hypernymic propositions in biomedical text . J Biomed Inform 2003 , 36 ( 6 ) : 462 - 477 . View Article PubMed . Rindflesch TC , Libbus B , Hristovski D , Aronson AR , Kilicoglu H : Semantic relations asserting the etiology of genetic diseases . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 554 - 558 . Masseroli M , Kilicoglu H , Lang FM , Rindflesch TC : Argument - predicate distance as a filter for enhancing precision in extracting predications on the genetic etiology of disease . BMC Bioinformatics 2006/06/10 Edition 2006 , 7 : 291 . View Article PubMed . Ahlers CB , Fiszman M , Demner - Fushman D , Lang FM , Rindflesch TC : Extracting semantic predications from Medline citations for pharmacogenomics . Pac Symp Biocomput 2007/11/10 Edition 2007 , 209 - 220 . Libbus B , Kilicoglu H , Rindflesch TC , Mork JG , Aronson AR , Hirschman L , Pustejovsky J : Using Natural Language Processing , LocusLink and the Gene Ontology to Compare OMIM to MEDLINE . HLT - NAACL 2004 Workshop : BioLINK 2004 , Linking Biological Literature , Ontologies and Databases 2004 , 69 - 76 . Blaschke C , Andrade MA , Ouzounis C , Valencia A : Automatic extraction of biological information from scientific text : protein - protein interactions . Proc Int Conf Intell Syst Mol Biol 2000/04/29 Edition 1999 , 60 - 67 . Blaschke C , Oliveros JC , Valencia A : Mining functional information associated with expression arrays . Funct Integr Genomics 2002/01/17 Edition 2001 , 1 ( 4 ) : 256 - 268 . View Article PubMed . Blaschke C , Valencia A : Can bibliographic pointers for known biological data be found automatically ? Protein interactions as a case study . Comparative and Functional Genomics 2001 , 2 ( 4 ) : 196 - 206 . View Article PubMed . Huang M , Zhu X , Hao Y , Payan DG , Qu K , Li M : Discovering patterns to extract protein - protein interactions from biomedical full texts . Proc JNLPBA , COLING 2004 , 22 - 28 . Temkin JM , Gilder MR : Extraction of protein interaction information from unstructured text using a context - free grammar . Bioinformatics Oxford Univ Press 2003 , 19 ( 16 ) : 2046 - 2053 . Corney DP , Buxton BF , Langdon WB , Jones DT : BioRAT : extracting biological information from full - length papers . Bioinformatics 2004/07/03 Edition 2004 , 20 ( 17 ) : 3206 - 3213 . View Article PubMed . Park JC , Kim HS , Kim JJ : Bidirectional incremental parsing for automatic pathway identification with combinatory categorial grammar . Pac Symp Biocomput 2001 , 396 - 407 . Yakushiji A , Tateisi Y , Miyao Y , Tsujii J : Event extraction from biomedical papers using a full parser . Pac Symp Biocomput 2001 , 408 - 419 . Gaizauskas R , Demetriou G , Artymiuk PJ , Willett P : Protein structures and information extraction from biological texts : the PASTA system . Bioinformatics 2002/12/25 Edition 2003 , 19 ( 1 ) : 135 - 143 . View Article PubMed . Leroy G , Chen H , Martinez JD : A shallow parser based on closed - class words to capture relations in biomedical text . J Biomed Inform 2003 , 36 ( 3 ) : 145 - 158 . View Article PubMed . Koike A , Niwa Y , Takagi T : Automatic extraction of gene / protein biological functions from biomedical text . Bioinformatics 2004/10/29 Edition 2005 , 21 ( 7 ) : 1227 - 1236 . View Article PubMed . Rinaldi F , Schneider G , Kaljurand K , Hess M , Romacker M : An environment for relation mining over richly annotated corpora : the case of GENIA . BMC Bioinformatics 2006 , 7 Suppl 3 : S3 . View Article PubMed . McInnes BT , Pedersen T , Pakhomov SV : Determining the Syntactic Structure of Medical Terms in Clinical Notes . Proc Assoc Comp Ling 2007 . Pyysalo S , Ginter F , Haverinen K , Heimonen J , Salakoski T , Laippala V : On the unification of syntactic annotations under the Stanford dependency scheme : A case study on BioInfer and GENIA . Association for Computational Linguistics 2007 . Craven M , Kumlien J : Constructing biological knowledge bases by extracting information from text sources . Proc Int Conf Intell Syst Mol Biol 1999 , 77 - 86 . Bunescu R , Ge R , Kate RJ , Marcotte EM , Mooney RJ , Ramani AK , Wong YW : Comparative experiments on learning information extractors for proteins and their interactions . Artif Intell Med 2005/04/07 Edition 2005 , 33 ( 2 ) : 139 - 155 . View Article PubMed . Krallinger M , Leitner F , Valencia A : Assessment of the second BioCreative PPI task : automatic extraction of protein - protein interactions . Hunter L , Cohen KB : Biomedical Language Processing : What 's Beyond PubMed ? Molecular Cell Cell 2006 , 21 : 589 - 594 . Chen H , Sharp BM : Content - rich biological network constructed by mining PubMed abstracts . BMC Bioinformatics 2004 , 5 : 147 . View Article PubMed . Wattarujeekrit T , Shah PK , Collier N : PASBio : predicate - argument structures for event extraction in molecular biology . BMC Bioinformatics 2004 , 5 : 155 - 175 . View Article PubMed . Cohen KB , Hunter L : A critical review of PASBio 's argument structures for biomedical verbs . BMC Bioinformatics 2006 , 7 ( Suppl . 3 ) : S5 . View Article PubMed . J Biomed Inform 2004/03/16 Edition 2004 , 37 ( 1 ) : 43 - 53 . View Article PubMed . Narayanaswamy M , Ravikumar KE , Vijay - Shanker K : Beyond the clause : extraction of phosphorylation information from medline abstracts . Bioinformatics 2005 . , 21 Suppl 1 : . Nucleic Acids Res 2003/12/19 Edition 2004 , 32 ( Database issue ) : D452 - 5 . View Article PubMed . Ding J , Berleant D , Nettleton D , Wurtele E : Mining MEDLINE : Abstracts , Sentences , or Phrases ? Pac Symp Biocomput 2002 , 7 : 326 - 337 . Martin C : Direct Memory Access Parsing . Yale University 1992 . Fitzgerald W : Building Embedded Conceptual Parsers . Northwestern University 1994 . Noy NF , Crubezy M , Fergerson RW , Knublauch H , Tu SW , Vendetti J , Musen MA : Protege-2000 : an open - source ontology - development and knowledge - acquisition environment . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 953 . Hirschman L , Colosimo M , Morgan A , Yeh A : Overview of BioCreAtIvE task 1B : normalized gene lists . BMC Bioinformatics 2005/06/18 Edition 2005 , 6 Suppl 1 : S11 . View Article PubMed . The Gene Ontology Consortium . Nat Genet 2000/05/10 Edition 2000 , 25 ( 1 ) : 25 - 29 . View Article PubMed . Genome Biology 2008 . Settles B : ABNER : an open source tool for automatically tagging genes , proteins and other entity names in text . Bioinformatics 2005 , 21 ( 14 ) : 3191 - 3192 . View Article PubMed . Baumgartner WA Jr. , Cohen KB , Fox LM , Acquaah - Mensah G , Hunter L : Manual curation is not sufficient for annotation of genomic databases . Bioinformatics 2007 , 23 ( 14 ) : e. . Leach SM , Gabow AP , Hunter L , Goldberg D : Assessing and combining reliability of protein interaction sources . Pac Symp Biocomp 2007 , 12 : 433 - -444 . View Article . Gabow AP , Leach SM , Baumgartner WA Jr. , Hunter L , Goldberg D : Improving Protein Function Prediction Methods with Integrated Literature Data . Carpenter B : Phrasal queries with LingPipe and Lucene : ad hoc genomics text retrieval . 13th Annual Text Retrieval Conference 2004 . Klein D , Manning CD : Fast Exact Inference with a Factored Model for Natural Language Parsing . Advances in Neural Information Processing Systems MIT Press 2003 . , 15 : . Kehler A , Appelt D , Taylor L , Simma A : The ( non ) utility of predicate - argument frequencies for pronoun interpretation . Proc of HLT - NAACL 2004 , 4 : 289 - 296 . Ogren P : Knowtator : a Protege plugin for annotated copus construction . HLT - NAACL 2006 , 273 . Copyright . \\u00a9 Hunter et al . 2008 . This article is published under license to BioMed Central Ltd. \"}",
        "_version_":1692580791959158784,
        "score":181.76979},
      {
        "id":"1dc09097-354c-45fd-8983-d8a1d471f3e9",
        "_src_":"{\"url\": \"https://channel9.msdn.com/Forums/Coffeehouse/Time-Lapse-Video-Control-Center\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158481.37/warc/CC-MAIN-20160205193918-00090-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Extracting Information from Text . For any given question , it 's likely that someone has written the answer down somewhere . The amount of natural language text that is available in electronic form is truly staggering , and is increasing every day . However , the complexity of natural language can make it very difficult to access the information in that text . The state of the art in NLP is still a long way from being able to build general - purpose representations of meaning from unrestricted text . If we instead focus our efforts on a limited set of questions or \\\" entity relations , \\\" such as \\\" where are different facilities located , \\\" or \\\" who is employed by what company , \\\" we can make significant progress . The goal of this chapter is to answer the following questions : . How can we build a system that extracts structured data , such as tables , from unstructured text ? What are some robust methods for identifying the entities and relationships described in a text ? Which corpora are appropriate for this work , and how do we use them for training and evaluating our models ? Along the way , we 'll apply techniques from the last two chapters to the problems of chunking and named - entity recognition . 1 Information Extraction . Information comes in many shapes and sizes . One important form is structured data , where there is a regular and predictable organization of entities and relationships . For example , we might be interested in the relation between companies and locations . Given a particular company , we would like to be able to identify the locations where it does business ; conversely , given a location , we would like to discover which companies do business in that location . If our data is in tabular form , such as the example in 1.1 , then answering these queries is straightforward . If this location data was stored in Python as a list of tuples ( entity , relation , entity ) , then the question \\\" Which organizations operate in Atlanta ? \\\" could be translated as follows : . The fourth Wells account moving to another agency is the packaged paper - products division of Georgia - Pacific Corp. , which arrived at Wells only last fall . Like Hertz and the History Channel , it is also leaving for an Omnicom - owned agency , the BBDO South unit of BBDO Worldwide . BBDO South in Atlanta , which handles corporate advertising for Georgia - Pacific , will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels , said Ken Haldin , a spokesman for Georgia - Pacific in Atlanta . If you read through ( 1 ) , you will glean the information required to answer the example question . But how do we get a machine to understand enough about ( 1 ) to return the answers in 1.2 ? This is obviously a much harder task . Unlike 1.1 , ( 1 ) contains no structure that links organization names with location names . One approach to this problem involves building a very general representation of meaning ( 10 . ) In this chapter we take a different approach , deciding in advance that we will only look for very specific kinds of information in text , such as the relation between organizations and locations . Rather than trying to use text like ( 1 ) to answer the question directly , we first convert the unstructured data of natural language sentences into the structured data of 1.1 . Then we reap the benefits of powerful query tools such as SQL . This method of getting meaning from text is called Information Extraction . Information Extraction has many applications , including business intelligence , resume harvesting , media analysis , sentiment detection , patent search , and email scanning . A particularly important area of current research involves the attempt to extract structured data out of electronically - available scientific literature , especially in the domain of biology and medicine . 1.1 Information Extraction Architecture . 1.1 shows the architecture for a simple information extraction system . It begins by processing a document using several of the procedures discussed in 3 and 5 . : first , the raw text of the document is split into sentences using a sentence segmenter , and each sentence is further subdivided into words using a tokenizer . Next , each sentence is tagged with part - of - speech tags , which will prove very helpful in the next step , named entity detection . In this step , we search for mentions of potentially interesting entities in each sentence . Finally , we use relation detection to search for likely relations between different entities in the text . Figure 1.1 : Simple Pipeline Architecture for an Information Extraction System . This system takes the raw text of a document as its input , and generates a list of ( entity , relation , entity ) tuples as its output . For example , given a document that indicates that the company Georgia - Pacific is located in Atlanta , it might generate the tuple ( [ ORG : ' Georgia - Pacific ' ] ' in ' [ LOC : ' Atlanta ' ] ) . To perform the first three tasks , we can define a simple function that simply connects together NLTK 's default sentence segmenter , word tokenizer , and part - of - speech tagger : . Note . Remember that our program samples assume you begin your interactive session or your program with : import nltk , re , pprint . Next , in named entity detection , we segment and label the entities that might participate in interesting relations with one another . Typically , these will be definite noun phrases such as the knights who say \\\" ni \\\" , or proper names such as Monty Python . In some tasks it is useful to also consider indefinite nouns or noun chunks , such as every student or cats , and these do not necessarily refer to entities in the same way as definite NP s and proper names . Finally , in relation extraction , we search for specific patterns between pairs of entities that occur near one another in the text , and use those patterns to build tuples recording the relationships between the entities . 2 Chunking . The basic technique we will use for entity detection is chunking , which segments and labels multi - token sequences as illustrated in 2.1 . The smaller boxes show the word - level tokenization and part - of - speech tagging , while the large boxes show higher - level chunking . Each of these larger boxes is called a chunk . Like tokenization , which omits whitespace , chunking usually selects a subset of the tokens . Also like tokenization , the pieces produced by a chunker do not overlap in the source text . Figure 2.1 : Segmentation and Labeling at both the Token and Chunk Levels . In this section , we will explore chunking in some depth , beginning with the definition and representation of chunks . We will see regular expression and n - gram approaches to chunking , and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus . We will then return in ( 5 ) and 6 to the tasks of named entity recognition and relation extraction . 2.1 Noun Phrase Chunking . We will begin by considering the task of noun phrase chunking , or NP - chunking , where we search for chunks corresponding to individual noun phrases . For example , here is some Wall Street Journal text with NP -chunks marked using brackets : . [ The / DT market / NN ] for / IN [ system - management / NN software / NN ] for / IN [ Digital / NNP ] [ ' s / POS hardware / NN ] is / VBZ fragmented / JJ enough / RB that / IN [ a / DT giant / NN ] such / JJ as / IN [ Computer / NNP Associates / NNPS ] should / MD do / VB well / RB there / RB . As we can see , NP -chunks are often smaller pieces than complete noun phrases . For example , the market for system - management software for Digital 's hardware is a single noun phrase ( containing two nested noun phrases ) , but it is captured in NP -chunks by the simpler chunk the market . One of the motivations for this difference is that NP -chunks are defined so as not to contain other NP -chunks . Consequently , any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP -chunk , since they almost certainly contain further noun phrases . One of the most useful sources of information for NP -chunking is part - of - speech tags . This is one of the motivations for performing part - of - speech tagging in our information extraction system . We demonstrate this approach using an example sentence that has been part - of - speech tagged in 2.2 . In order to create an NP -chunker , we will first define a chunk grammar , consisting of rules that indicate how sentences should be chunked . In this case , we will define a simple grammar with a single regular - expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ ) and then a noun ( NN ) . Using this grammar , we create a chunk parser , and test it on our example sentence . The result is a tree , which we can either print , or display graphically . 2.2 Tag Patterns . The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words . Tag patterns are similar to regular expression patterns ( 3.4 ) . Now , consider the following noun phrases from the Wall Street Journal : . another / DT sharp / JJ dive / NN trade / NN figures / NNS any / DT new / JJ policy / NN measures / NNS earlier / JJR stages / NNS Panamanian / JJ dictator / NN Manuel / NNP Noriega / NNP . This will chunk any sequence of tokens beginning with an optional determiner , followed by zero or more adjectives of any type ( including relative adjectives like earlier / JJR ) , followed by one or more nouns of any type . However , it is easy to find many more complicated examples which this rule will not cover : . his / PRP$ Mansion / NNP House / NNP speech / NN the / DT price / NN cutting / VBG 3/CD % /NN to / TO 4/CD % /NN more / JJR than / IN 10/CD % /NN the / DT fastest / JJS developing / VBG trends / NNS ' s / POS skill / NN . Note . Your Turn : Try to come up with tag patterns to cover these cases . Test them using the graphical interface nltk.app.chunkparser ( ) . Continue to refine your tag patterns with the help of the feedback given by this tool . 2.3 Chunking with Regular Expressions . To find the chunk structure for a given sentence , the RegexpParser chunker begins with a flat structure in which no tokens are chunked . The chunking rules are applied in turn , successively updating the chunk structure . Once all of the rules have been invoked , the resulting chunk structure is returned . 2.3 shows a simple chunk grammar consisting of two rules . The first rule matches an optional determiner or possessive pronoun , zero or more adjectives , then a noun . The second rule matches one or more proper nouns . We also define an example sentence to be chunked , and run the chunker on this input . ( \\\" her \\\" , \\\" PP$ \\\" ) , ( \\\" long \\\" , \\\" JJ \\\" ) , ( \\\" golden \\\" , \\\" JJ \\\" ) , ( \\\" hair \\\" , \\\" NN \\\" ) ] . The $ symbol is a special character in regular expressions , and must be backslash escaped in order to match the tag PP$ . If a tag pattern matches at overlapping locations , the leftmost match takes precedence . For example , if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns , then only the first two nouns will be chunked : . Once we have created the chunk for money market , we have removed the context that would have permitted fund to be included in a chunk . Note . We have added a comment to each of our chunk rules . These are optional ; when they are present , the chunker prints these comments as part of its tracing output . 2.4 Exploring Text Corpora . In 2 we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence of part - of - speech tags . We can do the same work more easily with a chunker , as follows : . Note . 2.5 Chinking . Sometimes it is easier to define what we want to exclude from a chunk . We can define a chink to be a sequence of tokens that is not included in a chunk . In the following example , barked / VBD at / IN is a chink : . [ the / DT little / JJ yellow / JJ dog / NN ] barked / VBD at / IN [ the / DT cat / NN ] . Chinking is the process of removing a sequence of tokens from a chunk . If the matching sequence of tokens spans an entire chunk , then the whole chunk is removed ; if the sequence of tokens appears in the middle of the chunk , these tokens are removed , leaving two chunks where there was only one before . If the sequence is at the periphery of the chunk , these tokens are removed , and a smaller chunk remains . These three possibilities are illustrated in 2.1 . 2.6 Representing Chunks : Tags vs Trees . As befits their intermediate status between tagging and parsing ( 8 . ) , chunk structures can be represented using either tags or trees . The most widespread file representation uses IOB tags . In this scheme , each token is tagged with one of three special chunk tags , I ( inside ) , O ( outside ) , or B ( begin ) . A token is tagged as B if it marks the beginning of a chunk . Subsequent tokens within the chunk are tagged I . All other tokens are tagged O . The B and I tags are suffixed with the chunk type , e.g. B - NP , I - NP . Of course , it is not necessary to specify a chunk type for tokens that appear outside a chunk , so these are just labeled O . An example of this scheme is shown in 2.5 . IOB tags have become the standard way to represent chunk structures in files , and we will also be using this format . Here is how the information in 2.5 would appear in a file : . We PRP B - NP saw VBD O the DT B - NP yellow JJ I - NP dog NN I - NP . In this representation there is one token per line , each with its part - of - speech tag and chunk tag . This format permits us to represent more than one chunk type , so long as the chunks do not overlap . As we saw earlier , chunk structures can also be represented using trees . These have the benefit that each chunk is a constituent that can be manipulated directly . An example is shown in 2.6 . NLTK uses trees for its internal representation of chunks , but provides methods for reading and writing such trees to the IOB format . 3 Developing and Evaluating Chunkers . Now you have a taste of what chunking does , but we have n't explained how to evaluate chunkers . As usual , this requires a suitably annotated corpus . We begin by looking at the mechanics of converting IOB format into an NLTK tree , then at how this is done on a larger scale using a chunked corpus . We will see how to score the accuracy of a chunker relative to a corpus , then look at some more data - driven ways to search for NP chunks . Our focus throughout will be on expanding the coverage of a chunker . 3.1 Reading IOB Format and the CoNLL 2000 Corpus . Using the corpus module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation . The chunk categories provided in this corpus are NP , VP and PP . As we have seen , each sentence is represented using multiple lines , as shown below : . he PRP B - NP accepted VBD B - VP the DT B - NP position NN I - NP ... . A conversion function chunk.conllstr2tree ( ) builds a tree representation from one of these multi - line strings . Moreover , it permits us to choose any subset of the three chunk types to use , here just for NP chunks : . draw ( ) . We can use the NLTK corpus module to access a larger amount of chunked text . The CoNLL 2000 corpus contains 270k words of Wall Street Journal text , divided into \\\" train \\\" and \\\" test \\\" portions , annotated with part - of - speech tags and chunk tags in the IOB format . We can access the data using nltk.corpus.conll2000 . Here is an example that reads the 100th sentence of the \\\" train \\\" portion of the corpus : . As you can see , the CoNLL 2000 corpus contains three chunk types : NP chunks , which we have already seen ; VP chunks such as has already delivered ; and PP chunks such as because of . Since we are only interested in the NP chunks right now , we can use the chunk_types argument to select them : . 3.2 Simple Evaluation and Baselines . Now that we can access a chunked corpus , we can evaluate chunkers . We start off by establishing a baseline for the trivial chunk parser cp that creates no chunks : . The IOB tag accuracy indicates that more than a third of the words are tagged with O , i.e. not in an NP chunk . However , since our tagger did not find any chunks , its precision , recall , and f - measure are all zero . Now let 's try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags ( e.g. CD , DT , and JJ ) . As you can see , this approach achieves decent results . However , we can improve on it by adopting a more data - driven approach , where we use the training corpus to find the chunk tag ( I , O , or B ) that is most likely for each part - of - speech tag . In other words , we can build a chunker using a unigram tagger ( 4 ) . But rather than trying to determine the correct part - of - speech tag for each word , we are trying to determine the correct chunk tag , given each word 's part - of - speech tag . In 3.1 , we define the UnigramChunker class , which uses a unigram tagger to label sentences with chunk tags . Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK 's ChunkParserI interface , and the IOB representation used by the embedded tagger . The class defines two methods : a constructor which is called when we build a new UnigramChunker ; and the parse method which is used to chunk new sentences . class UnigramChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . return nltk.chunk.conlltags2tree(conlltags ) . The constructor expects a list of training sentences , which will be in the form of chunk trees . It first converts training data to a form that is suitable for training the tagger , using tree2conlltags to map each chunk tree to a list of word , tag , chunk triples . It then uses that converted training data to train a unigram tagger , and stores it in self.tagger for later use . The parse method takes a tagged sentence as its input , and begins by extracting the part - of - speech tags from that sentence . It then tags the part - of - speech tags with IOB chunk tags , using the tagger self.tagger that was trained in the constructor . Next , it extracts the chunk tags , and combines them with the original sentence , to yield conlltags . Finally , it uses conlltags2tree to convert the result back into a chunk tree . Now that we have UnigramChunker , we can train it using the CoNLL 2000 corpus , and test its resulting performance : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 92.9 % Precision : 79.9 % Recall : 86.8 % F - Measure : 83.2 % . This chunker does reasonably well , achieving an overall f - measure score of 83 % . Let 's take a look at what it 's learned , by using its unigram tagger to assign a tag to each of the part - of - speech tags that appear in the corpus : . It has discovered that most punctuation marks occur outside of NP chunks , with the exception of # and $ , both of which are used as currency markers . It has also found that determiners ( DT ) and possessives ( PRP$ and WP$ ) occur at the beginnings of NP chunks , while noun types ( NN , NNP , NNPS , NNS ) mostly occur inside of NP chunks . Having built a unigram chunker , it is quite easy to build a bigram chunker : we simply change the class name to BigramChunker , and modify line in 3.1 to construct a BigramTagger rather than a UnigramTagger . The resulting chunker has slightly higher performance than the unigram chunker : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 93.3 % Precision : 82.3 % Recall : 86.8 % F - Measure : 84.5 % . 3.3 Training Classifier - Based Chunkers . Both the regular - expression based chunkers and the n - gram chunkers decide what chunks to create entirely based on part - of - speech tags . However , sometimes part - of - speech tags are insufficient to determine how a sentence should be chunked . For example , consider the following two statements : . These two sentences have the same part - of - speech tags , yet they are chunked differently . In the first sentence , the farmer and rice are separate chunks , while the corresponding material in the second sentence , the computer monitor , is a single chunk . Clearly , we need to make use of information about the content of the words , in addition to just their part - of - speech tags , if we wish to maximize chunking performance . One way that we can incorporate information about the content of words is to use a classifier - based tagger to chunk the sentence . Like the n - gram chunker considered in the previous section , this classifier - based chunker will work by assigning IOB tags to the words in a sentence , and then converting those tags to chunks . For the classifier - based tagger itself , we will use the same approach that we used in 1 to build a part - of - speech tagger . The basic code for the classifier - based NP chunker is shown in 3.2 . It consists of two classes . The first class is almost identical to the ConsecutivePosTagger class from 1.5 . The only two differences are that it calls a different feature extractor and that it uses a MaxentClassifier rather than a NaiveBayesClassifier . The second class is basically a wrapper around the tagger class that turns it into a chunker . During training , this second class maps the chunk trees in the training corpus into tag sequences ; in the parse ( ) method , it converts the tag sequence provided by the tagger back into a chunk tree . class ConsecutiveNPChunkTagger ( nltk . TaggerI ) : def _ _ init _ _ ( self , train_sents ) : . train_set . append ( ( featureset , tag ) ) . history.append(tag ) . history.append(tag ) . return zip(sentence , history ) class ConsecutiveNPChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . nltk.chunk.tree2conlltags(sent ) ] for sent in train_sents ] . return nltk.chunk.conlltags2tree(conlltags ) . The only piece left to fill in is the feature extractor . We begin by defining a simple feature extractor which just provides the part - of - speech tag of the current token . Using this feature extractor , our classifier - based chunker is very similar to the unigram chunker , as is reflected in its performance : . We can also add a feature for the previous part - of - speech tag . Adding this feature allows the classifier to model interactions between adjacent tags , and results in a chunker that is closely related to the bigram chunker . Next , we 'll try adding a feature for the current word , since we hypothesized that word content should be useful for chunking . We find that this feature does indeed improve the chunker 's performance , by about 1.5 percentage points ( which corresponds to about a 10 % reduction in the error rate ) . Finally , we can try extending the feature extractor with a variety of additional features , such as lookahead features , paired features , and complex contextual features . join(sorted(tags ) ) . Note . Your Turn : Try adding different features to the feature extractor function npchunk_features , and see if you can further improve the performance of the NP chunker . 4 Recursion in Linguistic Structure . 4.1 Building Nested Structure with Cascaded Chunkers . So far , our chunk structures have been relatively flat . Trees consist of tagged tokens , optionally grouped under a chunk node such as NP . However , it is possible to build chunk structures of arbitrary depth , simply by creating a multi - stage chunk grammar containing recursive rules . 4.1 has patterns for noun phrases , prepositional phrases , verb phrases , and sentences . This is a four - stage chunk grammar , and can be used to create structures having a depth of at most four . ( \\\" sit \\\" , \\\" VB \\\" ) , ( \\\" on \\\" , \\\" IN \\\" ) , ( \\\" the \\\" , \\\" DT \\\" ) , ( \\\" mat \\\" , \\\" NN \\\" ) ] . Unfortunately this result misses the VP headed by saw . It has other shortcomings too . Let 's see what happens when we apply this chunker to a sentence having deeper nesting . Notice that it fails to identify the VP chunk starting at . The solution to these problems is to get the chunker to loop over its patterns : after trying all of them , it repeats the process . We add an optional second argument loop to specify the number of times the set of patterns should be run : . Note . This cascading process enables us to create deep structures . However , creating and debugging a cascade is difficult , and there comes a point where it is more effective to do full parsing ( see 8 . ) Also , the cascading process can only produce trees of fixed depth ( no deeper than the number of stages in the cascade ) , and this is insufficient for complete syntactic analysis . 4.2 Trees . A tree is a set of connected labeled nodes , each reachable by a unique path from a distinguished root node . Here 's an example of a tree ( note that they are standardly drawn upside - down ) : . We use a ' family ' metaphor to talk about the relationships of nodes in a tree : for example , S is the parent of VP ; conversely VP is a child of S . Also , since NP and VP are both children of S , they are also siblings . For convenience , there is also a text format for specifying trees : . ( S ( NP Alice ) ( VP ( V chased ) ( NP ( Det the ) ( N rabbit ) ) ) ) . Although we will focus on syntactic trees , trees can be used to encode any homogeneous hierarchical structure that spans a sequence of linguistic forms ( e.g. morphological structure , discourse structure ) . In the general case , leaves and node values do not have to be strings . In NLTK , we create a tree by giving a node label and a list of children : . We can incorporate these into successively larger trees as follows : . Here are some of the methods available for tree objects : . The bracketed representation for complex trees can be difficult to read . In these cases , the draw method can be very useful . It opens a new window , containing a graphical representation of the tree . The tree display window allows you to zoom in and out , to collapse and expand subtrees , and to print the graphical representation to a postscript file ( for inclusion in a document ) . 4.3 Tree Traversal . def traverse ( t ) : . try : . ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) . 5 Named Entity Recognition . At the start of this chapter , we briefly introduced named entities ( NEs ) . Named entities are definite noun phrases that refer to specific types of individuals , such as organizations , persons , dates , and so on . 5.1 lists some of the more commonly used types of NEs . These should be self - explanatory , except for \\\" Facility \\\" : human - made artifacts in the domains of architecture and civil engineering ; and \\\" GPE \\\" : geo - political entities such as city , state / province , and country . The goal of a named entity recognition ( NER ) system is to identify all textual mentions of the named entities . This can be broken down into two sub - tasks : identifying the boundaries of the NE , and identifying its type . While named entity recognition is frequently a prelude to identifying relations in Information Extraction , it can also contribute to other tasks . For example , in Question Answering ( QA ) , we try to improve the precision of Information Retrieval by recovering not whole pages , but just those parts which contain an answer to the user 's question . Most QA systems take the documents returned by standard Information Retrieval , and then attempt to isolate the minimal text snippet in the document containing the answer . Now suppose the question was Who was the first President of the US ? , and one of the documents that was retrieved contained the following passage : . The Washington Monument is the most prominent structure in Washington , D.C. and one of the city 's early attractions . It was built in honor of George Washington , who led the country to independence and then became its first President . Analysis of the question leads us to expect that an answer should be of the form X was the first President of the US , where X is not only a noun phrase , but also refers to a named entity of type PERSON . This should allow us to ignore the first sentence in the passage . While it contains two occurrences of Washington , named entity recognition should tell us that neither of them has the correct type . How do we go about identifying named entities ? One option would be to look up each word in an appropriate list of names . For example , in the case of locations , we could use a gazetteer , or geographical dictionary , such as the Alexandria Gazetteer or the Getty Gazetteer . However , doing this blindly runs into problems , as shown in 5.1 . Figure 5.1 : Location Detection by Simple Lookup for a News Story : Looking up every word in a gazetteer is error - prone ; case distinctions may help , but these are not always present . Observe that the gazetteer has good coverage of locations in many countries , and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam . Of course we could omit such locations from the gazetteer , but then we wo n't be able to identify them when they do appear in a document . It gets even harder in the case of names for people or organizations . Any list of such names will probably have poor coverage . New organizations come into existence every day , so if we are trying to deal with contemporary newswire or blog entries , it is unlikely that we will be able to recognize many of the entities using gazetteer lookup . Another major source of difficulty is caused by the fact that many named entity terms are ambiguous . Thus May and North are likely to be parts of named entities for DATE and LOCATION , respectively , but could both be part of a PERSON ; conversely Christian Dior looks like a PERSON but is more likely to be of type ORGANIZATION . A term like Yankee will be ordinary modifier in some contexts , but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders . Further challenges are posed by multi - word names like Stanford University , and by names that contain other names such as Cecil H. Green Library and Escondido Village Conference Service Center . In named entity recognition , therefore , we need to be able to identify the beginning and end of multi - token sequences . Named entity recognition is a task that is well - suited to the type of classifier - based approach that we saw for noun phrase chunking . In particular , we can build a tagger that labels each word in a sentence using the IOB format , where chunks are labeled by their appropriate type . Here is part of the CONLL 2002 ( conll2002 ) Dutch training data : . Eddy N B - PER Bonte N I - PER is V O woordvoerder N O van Prep O diezelfde Pron O Hogeschool N B - ORG . Punc O . In this representation , there is one token per line , each with its part - of - speech tag and its named entity tag . Based on this training corpus , we can construct a tagger that can be used to label new sentences ; and use the nltk.chunk.conlltags2tree ( ) function to convert the tag sequences into a chunk tree . NLTK provides a classifier that has already been trained to recognize named entities , accessed with the function nltk.ne_chunk ( ) . 6 Relation Extraction . Once named entities have been identified in a text , we then want to extract the relations that exist between them . As indicated earlier , we will typically be looking for relations between specified types of named entity . One way of approaching this task is to initially look for all triples of the form ( X , \\u03b1 , Y ) , where X and Y are named entities of the required types , and \\u03b1 is the string of words that intervenes between X and Y . We can then use regular expressions to pull out just those instances of \\u03b1 that express the relation that we are looking for . The following example searches for strings that contain the word in . The special regular expression ( ? ! \\\\b.+ing\\\\b ) is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of , where in is followed by a gerund . As shown above , the conll2002 Dutch corpus contains not just named entity annotation but also part - of - speech tags . This allows us to devise patterns that are sensitive to these tags , as shown in the next example . The method clause ( ) prints out the relations in a clausal form , where the binary relation symbol is specified as the value of parameter relsym . Note . This will show you the actual words that intervene between the two NEs and also their left and right context , within a default 10-word window . With the help of a Dutch dictionary , you might be able to figure out why the result VAN ( ' annie_lennox ' , ' eurythmics ' ) is a false hit . 7 Summary . Information extraction systems search large bodies of unrestricted text for specific types of entities and relations , and use them to populate well - organized databases . These databases can then be used to find answers for specific questions . The typical architecture for an information extraction system begins by segmenting , tokenizing , and part - of - speech tagging the text . The resulting data is then searched for specific types of entity . Finally , the information extraction system looks at entities that are mentioned near one another in the text , and tries to determine whether specific relationships hold between those entities . Entity recognition is often performed using chunkers , which segment multi - token sequences , and label them with the appropriate entity type . Common entity types include ORGANIZATION , PERSON , LOCATION , DATE , TIME , MONEY , and GPE ( geo - political entity ) . Chunkers can be constructed using rule - based systems , such as the RegexpParser class provided by NLTK ; or using machine learning techniques , such as the ConsecutiveNPChunker presented in this chapter . In either case , part - of - speech tags are often a very important feature when searching for chunks . Although chunkers are specialized to create relatively flat data structures , where no two chunks are allowed to overlap , they can be cascaded together to build nested structures . Relation extraction can be performed using either rule - based systems which typically look for specific patterns in the text that connect entities and the intervening words ; or using machine - learning systems which typically attempt to learn such patterns automatically from a training corpus . 8 Further Reading . The popularity of chunking is due in great part to pioneering work by Abney e.g. , ( Church , Young , & Bloothooft , 1996 ) . The IOB format ( or sometimes BIO Format ) was developed for NP chunking by ( Ramshaw & Marcus , 1995 ) , and was used for the shared NP bracketing task run by the Conference on Natural Language Learning ( CoNLL ) in 1999 . The same format was adopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of a shared task on NP chunking . 9 Exercises . Why are three tags necessary ? What problem would be caused if we used I and O tags exclusively ? Try to do this by generalizing the tag pattern that handled singular noun phrases . Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk . Develop a simple chunker using the regular expression chunker nltk . RegexpParser . Discuss any tag sequences that are difficult to chunk reliably . Develop a chunker that starts by putting the whole sentence in a single chunk , and then does the rest of its work solely by chinking . Determine which tags ( or tag sequences ) are most likely to make up chinks with the help of your own utility program . Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules . Add these patterns to the grammar , one per line . Test your work using some tagged sentences of your own devising . ( Note that most chunking corpora contain some internal inconsistencies , such that any reasonable rule - based approach will produce errors . ) Evaluate your chunker on 100 sentences from a chunked corpus , and report the precision , recall and F - measure . Use the chunkscore.missed ( ) and chunkscore.incorrect ( ) methods to identify the errors made by your chunker . Discuss . Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter . Use any combination of rules for chunking , chinking , merging or splitting . Instead of requiring manual correction of tagger output , good chunkers are able to work with the erroneous output of taggers . Look for other examples of correctly chunked noun phrases with incorrect tags . Study its errors and try to work out why it does n't get 100 % accuracy . Experiment with trigram chunking . Are you able to improve the performance any more ? Instead of assigning POS tags to words , here we will assign IOB tags to the POS tags . E.g. if the tag DT ( determiner ) often occurs at the start of a chunk , it will be tagged B ( begin ) . Evaluate the performance of these chunking methods relative to the regular expression chunking methods covered in this chapter . that it is possible to establish an upper limit to tagging performance by looking for ambiguous n - grams , n - grams that are tagged in more than one possible way in the training data . Apply the same method to determine an upper bound on the performance of an n - gram chunker . Write functions to do the following tasks for your chosen type : . List all the tag sequences that occur with each instance of this chunk type . Count the frequency of each tag sequence , and produce a ranked list in order of decreasing frequency ; each line should consist of an integer ( the frequency ) and the tag sequence . Inspect the high - frequency tag sequences . Use these as the basis for developing a better chunker . For example , the phrase : [ every / DT time / NN ] [ she / PRP ] sees / VBZ [ a / DT newspaper / NN ] contains two consecutive chunks , and our baseline chunker will incorrectly combine the first two : [ every / DT time / NN she / PRP ] . Write a program that finds which of these chunk - internal tags typically occur at the start of a chunk , then devise one or more rules that will split up these chunks . Combine these with the existing baseline chunker and re - evaluate it , to see if you have discovered an improved baseline . The format uses square brackets , and we have encountered it several times during this chapter . The Treebank corpus can be accessed using : for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid ) . These are flat trees , just as we got using nltk.corpus.conll2000.chunked_sents ( ) . The functions nltk.tree.pprint ( ) and nltk.chunk.tree2conllstr ( ) can be used to create Treebank and IOB strings from a tree . Write functions chunk2brackets ( ) and chunk2iob ( ) that take a single chunk tree as their sole argument , and return the required multi - line string representation . Write command - line conversion utilities bracket2iob.py and iob2bracket.py that take a file in Treebank or CoNLL format ( resp ) and convert it to the other format . ( Obtain some raw Treebank or CoNLL data from the NLTK Corpora , save it to a file , and then use for line in open(filename ) to access it from Python . ) Investigate other models of the context , such as the n-1 previous part - of - speech tags , or some combination of previous chunk tags along with previous and following part - of - speech tags . Now observe how a chunker may re - use this sequence information . For example , both tasks will make use of the information that nouns tend to follow adjectives ( in English ) . It would appear that the same information is being maintained in two places . Is this likely to become a problem as the size of the rule sets grows ? If so , speculate about any ways that this problem might be addressed . \"}",
        "_version_":1692668359994245120,
        "score":173.4266},
      {
        "id":"bfdc7bfe-8f1f-4d1c-8639-3037543b201e",
        "_src_":"{\"url\": \"https://www.behance.net/gallery/YWFT-Mimic/6892595\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153585.76/warc/CC-MAIN-20160205193913-00244-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"We present a method for recognizing semantic roles for Spanish sentences . If a complete parse can not be produced , a partial structure is built with some ( if not all ) dependency relations identified . Evaluation shows that in spite of its simplicity , the parser 's accuracy is superior to the available existing parsers for Spanish . A particularly interesting ambiguity which we have decided to analyze deeper , is the Prepositional Phrase Attachment Disambiguation . The system uses an ordered set of simple heuristic rules for determining iteratively the relationships between words to which a governor has not been yet assigned . For resolving certain cases of ambiguity we use cooccurrence statistics of words collected previously in an unsupervised manner , whether it be from big corpora , or from the Web ( through a search engine such as Google ) . Collecting these statistics is done by using Selectional Preferences . In order to evaluate our system , we developed a Method for Converting a Gold Standard from a constituent format to a dependency format . Additionally , each one of the modules of the system ( Selectional Preferences Acquisition and Prepositional Phrase Attachment Disambiguation ) , is evaluated in a separate and independent way to verify that they work properly . Finally we present some Applications of our system : Word Sense Disambiguation and Linguistic Steganography . Keywords : dependency parsing , pp attachment disambiguation , constituent to dependency conversion , heuristic rules , hybrid parser , selectional preferences . Resumen . Se presenta un m\\u00e9todo para reconocer los roles sem\\u00e1nticos de las oraciones en espa\\u00f1ol , es decir , identificar el papel que tiene cada uno de los elementos de la oraci\\u00f3n . Si no se puede producir un an\\u00e1lisis completo , se construye una estructura parcial con algunas ( si no todas ) relaciones de dependencia identificadas . La evaluaci\\u00f3n muestra que a pesar de su simplicidad , la precisi\\u00f3n del analizador es superior a aquella de los analizadores existentes actuales para el espa\\u00f1ol . A pesar de que ciertas reglas gramaticales y los recursos l\\u00e9xicos usados son espec\\u00edficos para el espa\\u00f1ol , el enfoque sugerido es independiente del lenguaje . Una ambig\\u00fcedad interesante que hemos decidido analizar a mayor profundidad , es la desambiguaci\\u00f3n de sintagma preposicional . El sistema usa un conjunto ordenado de reglas heur\\u00edsticas simples para determinar iterativamente las relaciones entre palabras para las cuales no se les ha asignado a\\u00fan un gobernante . Estas estad\\u00edsticas han sido obtenidas previamente de una manera no supervisada , ya sea a partir de grandes corpus de texto , o a trav\\u00e9s de Internet ( a trav\\u00e9s de un motor de b\\u00fasqueda como Google ) . Para evaluar este sistema , desarrollamos un m\\u00e9todo para convertir un est\\u00e1ndar existente , de un formato de constituyentes a un formato de dependencias . Adicionalmente , cada uno de los m\\u00f3dulos del sistema ( Adquisici\\u00f3n de Preferencias de Selecci\\u00f3n , Desambiguaci\\u00f3n de Sintagma Preposicional ) se eval\\u00faa de una forma separada e independiente para verificar su correcto funcionamiento . Finalmente , presentamos algunas aplicaciones de nuestro sistema : Desambiguaci\\u00f3n de sentidos de palabras y Estaganograf\\u00eda ling\\u00fc\\u00edstica . Palabras clave : an\\u00e1lisis de dependencias , desambiguaci\\u00f3n de frase preposicional , conversi\\u00f3n de constituyentes a dependencias , reglas heur\\u00edsticas , analizador sint\\u00e1ctico h\\u00edbrido , preferencias de selecci\\u00f3n . Agirre E. , D. Mart\\u00ednez . Unsupervised WSD based on automatically retrieved examples : The importance of bias . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , EMNLP , Barcelona , Spain , 2004 . [ Links ] . Agirre , E. D. Martinez . [ Links ] . Agirre , E. , D. Martinez . Integrating selectional preferences in WordNet . [ Links ] . Apresyan , Yuri D. , Igor Boguslavski , Leonid Iomdin , Alexandr Lazurski , Nikolaj Pertsov , Vladimir Sannikov , Leonid Tsinman . Moscow , Nauka , 1989 . [ Links ] . Bolshakov , Igor A. [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . A Very Large Database of Collocations and Semantic Links . Proc . Conf . [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . On Detection of Malapropisms by Multistage Collocation Testing . Conf . on Application of Natural Language to Information Systems . [ Links ] . In Proceedings of the 6 th Applied Natural Language Processing Conference , Seattle , Washington , USA , 2000 . [ Links ] . Brants , Thorsten . In : Proc . [ Links ] . Brill , Eric , Philip Resnik . [ Links ] . Briscoe , Ted . John Carroll , Jonathan Graham and Ann Copestake . Relational evaluation schemes . In : Procs . [ Links ] . Calvo , Hiram , Alexander Gelbukh . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Improving Prepositional Phrase Attachment Disambiguation Using the Web as Corpus , In A. Sanfeliu and J. Shulcloper ( Eds . ) Calvo , Hiram , Alexander Gelbukh . Natural Language Interface Framework for Spatial Object Composition Systems . Procesamiento de Lenguaje Natural 31 , 2003 . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Acquiring Selectional Preferences from Untagged Text for Prepositional Phrase Attachment Disambiguation . In : Proc . [ Links ] . Calvo , Hiram . Alexander Gelbukh , Adam Kilgarriff . Distributional Thesaurus versus WordNet : A Comparison of Backoff Techniques for Unsupervised PP Attachment . [ Links ] . Carreras , Xavier , Isaac Chao , Lluis Padr\\u00f3 , Muntsa Padr\\u00f3 . Proc . 4 th Intern . Conf . [ Links ] . Carroll , J. , D. McCarthy . Word sense disambiguation using automatically acquired verbal preferences . [ Links ] . Chomsky , Noam . Syntactic Structures . The Hague : Mouton & Co , 1957 . [ Links ] . Civit , Montserrat , and Maria Ant\\u00f2nia Mart\\u00ed . Est\\u00e1ndares de anotaci\\u00f3n morfosint\\u00e1ctica para el espa\\u00f1ol . Workshop of tools and resources for Spanish and Portuguese . IBERAMIA 04 , Mexico , 2004 . [ Links ] . Copestake , Ann , Dan Flickinger , Ivan A. Sag . Minimal Recursion Semantics . An introduction . CSLI , Stanford University , 1997 . [ Links ] . In : Recent Advances in Dependency Grammar . Proc . D\\u00edaz , Isabel , Lidia Moreno , Inmaculada Fuentes , Oscar Pastor . In : Alexander Gelbukh ( ed . ) [ Links ] . Dik , Simon C. , The Theory of Functional Grammar . Part I : The structure of the clause . Dordrecht , Foris , 1989 . [ Links ] . Dirk , L\\u00fcdtke , Satoshi Sato . [ Links ] . Gelbukh , A. , G. Sidorov , L. Chanona . Corpus virtual , virtual : Un diccionario grande de contextos de palabras espa\\u00f1olas compilado a trav\\u00e9s de Internet . In : Julio Gonzalo , Anselmo Pe\\u00f1as , Antonio Ferr\\u00e1ndez , eds . : Proc . [ Links ] . Gelbukh , A. , S. Torres , H. Calvo . Transforming a Constituency Treebank into a Dependency Treebank . Submitted to Procesamiento del Lenguaje Natural No . 34 , Spain , 2005 . [ Links ] . Gelbukh , Alexander , Grigori Sidorov , Francisco Vel\\u00e1squez . An\\u00e1lisis morfol\\u00f3gico autom\\u00e1tico del espa\\u00f1ol a trav\\u00e9s de generaci\\u00f3n . [ Links ] . Gladki , A. V. Syntax Structures of Natural Language in Automated Dialogue Systems ( in Russian ) . Moscow , Nauka , 1985 . [ Links ] . Kudo , T. , Y. Matsumoto . Use of Support Vector Learning for Chunk Identification . [ Links ] . Lara , Luis Fernando . Diccionario del espa\\u00f1ol usual en M\\u00e9xico . Digital edition . Colegio de M\\u00e9xico , Center of Linguistic and Literary Studies , 1996 . [ Links ] . [ Links ] . Mel'cuk , Igor A. Dependency Syntax : Theory and Practice . State U. Press of NY , 1988 . [ Links ] . Mel'cuk , Igor A. Lexical Functions : A Tool for the Description of Lexical Relations in the Lexicon . In : L. Wanner ( ed . ) [ Links ] . [ Links ] . Monedero , J. , Gonz\\u00e1lez , J. Go\\u00f1i , C. Iglesias , A. Nieto . Obtenci\\u00f3n autom\\u00e1tica de marcos de subcategorizaci\\u00f3n verbal a partir de texto etiquetado : el sistema SOAMAS . [ Links ] . Text Mining at Detail Level Using Conceptual Graphs . In : Uta Priss et al . ( Eds . ) : Conceptual Structures : Integration and Interfaces , 10 th Intern . Conf . [ Links ] . Information Retrieval with Conceptual Graph Matching . Proc . Conf . [ Links ] . Evaluation of TnT Tagger for Spanish . In Proc . [ Links ] . Pollard , Carl , and Ivan Sag . University of Chicago Press , Chicago , IL and London , UK , 1994 . [ Links ] . Prescher , Detlef , Stefan Riezler , and Mats Rooth . In Proceedings of the 18th International Conference on Computational Linguistics , Saarland University , Saarbr\\u00fccken , Germany , 2000 . [ Links ] . Ratnaparkhi , Adwait , Jeff Reynar , and Salim Roukos . A Maximum Entropy Model for Prepositional Phrase Attachment . [ Links ] . [ Links ] . Resnik , P. Selectional preference and sense disambiguation , ACL SIGLEX Workshop on Tagging Text with Lexical Semantics : Why , What , and How ? [ Links ] . Ph.D. Thesis , University of Pennsylvania , December , 1993 . [ Links ] . Sag , Ivan , Tom Wasow , and Emily M. Bender . Syntactic Theory . A Formal Introduction ( 2nd Edition ) . CSLI Publications , Stanford , CA , 2003 [ Links ] . Sebasti\\u00e1n , N. , M. A. Mart\\u00ed , M. F. Carreiras , and F. Cuestos . LEXESP , l\\u00e9xico informatizado del espa\\u00f1ol , Edicions de la Universitat de Barcelona , 2000 . [ Links ] . Sowa , John F. 1984 . Conceptual Structures : Information Processing in Mind and Machine . [ Links ] . Steele , James ( ed . ) Linguistics , Lexicography , and Implications . Ottawa : Univ . of Ottawa Press , 1990 . [ Links ] . Su\\u00e1rez , A. , M. Palomar . [ Links ] . Tapanainen , Pasi . Academic Dissertation . University of Helsinki , Language Technology , Department of General Linguistics , Faculty of Arts , 1999 . [ Links ] . Tesni\\u00e8re , Lucien . El\\u00e9ments de syntaxe structurale . Paris : Librairie Klincksieck , 1959 . [ Links ] . Volk , Martin . Exploiting the WWW as a corpus to resolve PP attachment ambiguities . In Proceeding of Corpus Linguistics 2001 . Lancaster , 2001 . [ Links ] . Weinreich , Uriel . Explorations in Semantic Theory , Mouton , The Hague , 1972 . [ Links ] . Yarowsky , D. , Hierarchical decision lists for word sense disambiguation . [ Links ] . Yarowsky , D. , S. Cucerzan , R. Florian , C. Schafer , R. Wicentowski . [ Links ] . Yuret , Deniz . Discovery of Linguistic Relations Using Lexical Attraction , PhD thesis , MIT , 1998 . [ Links ] \"}",
        "_version_":1692671237020450816,
        "score":165.82045},
      {
        "id":"8841b732-73d5-4b93-9c89-b71737b7f380",
        "_src_":"{\"url\": \"http://www.locatetv.com/tv/2-broke-girls/season-1/7328131\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161946.96/warc/CC-MAIN-20160205193921-00023-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Latest News . January 2012 . Our paper on Action Science Explorer was accepted by JASIST , the Journal of the American Society for Information Science and Technology . See the Publications section below for more details . July 2010 . The iOpener Workbench has been renamed Action Science Explorer ( ASE ) . Description . The goal of the iOpener project is to generate readily - consumable surveys of different scientific domains and topics , targeted to different audiences and levels . Part of this is the Action Science Explorer ( ASE ) , a new tool which presents the academic literature for a field using many different modalities : lists of articles , their full texts , automatic text summaries , and visualizations of the structure of the citation network . SocialAction provides us with powerful network analysis capabilities including force - directed citation network visualization , ranking and filtering papers by statistical measures , scatterplots of paper attributes and statistics , categorical and numerical range coloring , and automatic cluster detection . Using visualizations of the citation network we can easily find unexpected trends , clusters , gaps and outliers . Additionally , visualizations can immediately identify invalid data that is easily missed in tabular views . It integrates with Microsoft Word , OpenOffice.org , and LaTeX / BibTeX , which allows quick adding of citations to discovered articles when writing survey papers . These tools are linked together to form multiple coordinated views of the data . Clicking on a node in the citation network selects it and its corresponding paper in the reference manager , displaying its abstract , review , and other data associated with it . Moreover , when clusters of nodes are selected their papers are floated to the top of the reference manager for easy perusal . The inverse is true as well , with any paper , group , or search term selected in the reference manager highlighting the corresponding nodes in the network . There are other coordinated views that provide the user with other aspects of the field . When any node or cluster is selected , the In - Cite Text window displays the text of all incoming citations to the paper(s ) , i.e. the whole sentences from the citing papers that include the citation to the selected paper(s ) . These are displayed in a hyperlinked list that allows the user to select any one of them to show their surrounding context in the Out - Cite Text window . This window shows the full text of the paper citing one of the selected papers , with highlighting showing the selected citation sentence as well as any other sentences that include hyperlinked citations to other papers . The last view is the summary window , which can contain various multi - document summaries of a selected cluster . Using automatic summarization techniques , we can summarize all of the incoming citations to papers within that cluster , hopefully providing key insights into that research community . Action Science Explorer integrates these many components in order to provide a tool that supports rapid understanding of scientific literature . Users can analyze the network of citations between papers , identify key papers and research clusters , automatically summarize them , dig into the full text of articles to extract context , make annotations , write reviews , and finally export their findings in many of document authoring formats . We hope this infrastructure will enable users to generate readily - consumable surveys of scientific fields . Data & Summarization . As part of the iOpener project we developed the ACL Anthology Network ( AAN ) reference dataset , which includes more than 16,000 papers , each distinguished with a unique ACL ID , together with their full - texts , abstracts , and citation information . It also includes other valuable meta - data such as author affiliations , citation and collaboration networks , and various centrality measures . Moreover , we automatically extracted all sentences in the collection that cite a particular target paper in order to create a citation context for it . We used this dataset to evaluate the effectiveness of ASE through our user studies , as well as to quantitatively test our text summarization approaches . We developed several techniques to summarize scientific literature , including C - LexRank , a novel graph - based method for the automatic creation of citation - based summaries of the target papers . C - LexRank is built on top of the Clairlib library and uses network community detection to identify the main contributions of the target papers and then produces a summary that highlights these contributions . Using numerical analysis and evaluations , we showed that the surveys generated using the citation network model of scientific literature have higher quality than summaries that only use abstracts and those generated using other state of the art summarization systems . Software . ASE currently requires substantial data processing for many of the views , as much of the data required is not available from publisher databases . ASE is available only to our collaborators . Those who wish to explore publication and other databases should review the currently available commercial , open source , and research tools listed below . Publications . Books and Other One - Time Publications . Abstract : Keeping up with rapidly growing research fields , especially when there are multiple interdisciplinary sources , requires substantial effort for researchers , program managers , or venture capital investors . Current theories and tools are directed at finding a paper or website , not gaining an understanding of the key papers , authors , controversies , and hypotheses . This report presents an effort to integrate statistics , text analysis , and visualization in a multiple coordinated window environment that supports exploration . Our prototype system , Action Science Explorer ( ASE ) , provides an environment for demonstrating principles of coordination and conducting iterative usability tests of them with interested and knowledgeable users . We developed an understanding of the value of reference management , statistics , citation context extraction , natural language summarization for single and multiple documents , filters to interactively select key papers , and network visualization to see citation patterns and identify clusters . The three - phase usability study guided our revisions to ASE and led us to improve the testing methods . BibTeX : . Gove , R. , Dunne , C. , Shneiderman , B. , Klavans , J. & Dorr , B. ( 2011 ) , \\\" Evaluating visual and statistical exploration of scientific literature networks \\\" , In VL / HCC ' 11 : Proc . 2011 IEEE Symposium on Visual Languages and Human - Centric Computing . Abstract : Action Science Explorer ( ASE ) is a tool designed to support users in rapidly generating readily consumable summaries of academic literature . It uses citation network visualization , ranking and filtering papers by network statistics , and automatic clustering and summarization techniques . We describe how early formative evaluations of ASE led to a mature system evaluation , consisting of an in - depth empirical evaluation with four domain experts . The evaluation tasks were of two types : predefined tasks to test system performance in common scenarios , and user - defined tasks to test the system 's usefulness for custom exploration goals . We contribute a taxonomy of features for literature search and exploration tools and describe exploration goals identified by our participants . BibTeX : . Gove , R. ( 2011 ) , \\\" Understanding scientific literature networks : case study evaluations of integrating vizualizations and statistics \\\" . School : University of Maryland , Department of Computer Science . Abstract : Investigators frequently need to quickly learn new research domains in order to advance their research . This thesis presents five contributions to understanding how software tools help researchers explore scientific literature networks . First , this thesis summarizes capabilities in existing bibliography tools , which reveals patterns of capabilities by system type . Next , six participants in two user studies evaluate Action Science Explorer ( ASE ) , which is designed to create surveys of scientific literature and integrates visualizations and statistics . Users found document - level statistics and attribute rankings to be convenient when beginning literature exploration . The user studies also identify users ? questions when exploring academic literature , which include examining the evolution of a field , identifying author relationships , and searching for review papers . The evaluations reveal some shortcomings of ASE , and this thesis outlines improvements to ASE and lists user requirements for bibliographic exploration . Finally , I recommend strategies for evaluating bibliographic exploration tools based on experiences evaluating ASE . BibTeX : . Dunne , C. , Shneiderman , B. , Dorr , B. & Klavans , J. ( 2010 ) , \\\" iOpener Workbench : tools for rapid understanding of scientific literature \\\" , In Proc . 27th Annual Human - Computer Interaction Lab Symposium . College Park , MD . May 2010 . Posters . Dunne , C. ( 2011 ) , \\\" Interactive data visualization for rapid understanding of scientific literature \\\" , Poster at VAC ' 11 : Visual Analytics Consortium Meeting . May , 2011 . Abstract : We developed Action Science Explorer ( ASE ) , a tool designed to support users in rapidly generating easily consumable summaries of academic literature . ASE uses bibliometric lexical link mining to create a citation network for a field and context for each citation , automatic clustering and multi - document summarization techniques to extract key points , and potent network analysis and visualization tools to aid in the exploration task . These techniques provide several coordinated views of the underlying data . BibTeX : . Gove , R. ( 2011 ) , \\\" Action Science Explorer \\\" , Poster at 28th Annual Human - Computer Interaction Lab Symposium . May , 2011 . Presentations . Dunne , C. ( 2011 ) , \\\" Visual analytic tools for monitoring and understanding the emergence and evolution of innovations in science & technology \\\" , Talk at OECD - KNOWINNO workshop on measuring the use and impact of knowledge exchange mechanisms . November , 2011 . Abstract : The internet and other ICTs have had an important role in promoting the use of datamining tools for assembling , interlinking and analysing information from diverse sources . In this session we will explore how advanced data analytics tools can be used for identifying and measuring knowledge flows between different parties and to what extent they can complement more traditional data sources such as patents , publications and surveys . BibTeX : . Dunne , C. ( 2011 ) , \\\" What researchers want \\\" , Talk at STM 3rd Master Class on Developing Leadership and Innovation . November , 2011 . Dunne , C. ( 2011 ) , \\\" Action Science Explorer : interactive data visualization for rapid understanding of scientific literature \\\" , Talk at STM Annual Spring Conference . April , 2011 . Abstract : We developed Action Science Explorer ( ASE ) , a tool designed to support users in rapidly generating easily consumable summaries of academic literature . ASE uses bibliometric lexical link mining to create a citation network for a field and context for each citation , automatic clustering and multi - document summarization techniques to extract key points , and potent network analysis and visualization tools to aid in the exploration task . These techniques provide several coordinated views of the underlying data . BibTeX : . Shneiderman , B. ( 2011 ) , \\\" Information visualization : A transformative technology for ACS \\\" , Talk at American Chemical Society . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Social discovery in an information abundant world \\\" , Talk at National Federation for Advanced Information Systems : Miles Conrad Award Lecture . February , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at University of North Carolina -- Charlotte , Dept of Computer Science . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Success stories in visual analytics \\\" , Panel at VAC ' 11 : Visual Analytics Consortium Meeting . May , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at Yale University , Dept of Computer Science . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at Georgia Tech Graphics , Visualization & Usability Brown Bag Lunch Seminar . March , 2011 . Shneiderman , B. ( 2010 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at IBM Research Center . October , 2010 . Shneiderman , B. ( 2010 ) , \\\" Distinguished lecture in computational science : Information visualization for knowledge discovery \\\" , Talk at Harvard University , School of Engineering and Applied Sciences . October , 2010 . Abstract : The task of paraphrasing is inherently familiar to speakers of all languages . Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language?words , phrases , and sentences?is an important part of natural language processing ( NLP ) and is being increasingly employed to improve the performance of several NLP applications . In this article , we attempt to conduct a comprehensive and application - independent survey of data - driven phrasal and sentential paraphrase generation methods , while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research . Recent work done in manual and automatic construction of paraphrase corpora is also examined . We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation . BibTeX : . Aris , A. , Shneiderman , B. , Qazvinian , V. & Radev , D. ( 2009 ) , \\\" Visual overviews for discovering key papers and influences across research fronts \\\" , JASIST : Journal of the American Society for Information Science and Technology . Vol . 60(11 ) , pp . 2219 - 2228 . Abstract : Gaining a rapid overview of an emerging scientific topic , sometimes called research fronts , is an increasingly common task due to the growing amount of interdisciplinary collaboration . Visual overviews that show temporal patterns of paper publication and citation links among papers can help researchers and analysts to see the rate of growth of topics , identify key papers , and understand influences across subdisciplines . This article applies a novel network - visualization tool based on meaningful layouts of nodes to present research fronts and show citation links that indicate influences across research fronts . To demonstrate the value of two - dimensional layouts with multiple regions and user control of link visibility , we conducted a design - oriented , preliminary case study with 6 domain experts over a 4-month period . The main benefits were being able ( a ) to easily identify key papers and see the increasing number of papers within a research front , and ( b ) to quickly see the strength and direction of influence across related research fronts . BibTeX : . Radev , D.R. , Joseph , M.T. , Gibson , B. & Muthukrishnan , P. ( 2009 ) , \\\" A bibliometric and network analysis of the field of computational linguistics \\\" , JASIST : Journal of the American Society for Information Science and Technology . John Wiley & Sons . Abstract : The ACL Anthology is a large collection of research papers in computational linguistics . Citation data was obtained using text extraction from a collection of PDF files with significant manual post - processing performed to clean up the results . Manual annotation of the references was then performed to complete the citation network . We analyzed the networks of paper citations , author citations , and author collaborations in an attempt to identify the most central papers and authors . Also , we propose an improved method for comparing different measures of impact based on correlation . The analysis includes general network statistics , PageRank , metrics across publication years and venues , impact factor and h - index , as well as other measures . BibTeX : . Elkiss , A. , Shen , S. , Fader , A. , Erkan , G. , States , D. & Radev , D.R. ( 2008 ) , \\\" Blind men and elephants : What do citation summaries tell us about a research article ? \\\" , JASIST : Journal of the American Society for Information Science and Technology . Vol . 59(1 ) , pp . 51 - 62 . Wiley Subscription Services , Inc. , A Wiley Company . Abstract : The old Asian legend about the blind men and the elephant comes to mind when looking at how different authors of scientific papers describe a piece of related prior work . It turns out that different citations to the same paper often focus on different aspects of that paper and that neither provides a full description of its full set of contributions . In this article , we will describe our investigation of this phenomenon . We studied citation summaries in the context of research papers in the biomedical domain . A citation summary is the set of citing sentences for a given article and can be used as a surrogate for the actual article in a variety of scenarios . It contains information that was deemed by peers to be important . Our study shows that citation summaries overlap to some extent with the abstracts of the papers and that they also differ from them in that they focus on different aspects of these papers than do the abstracts . In addition to this , co - cited articles ( which are pairs of articles cited by another article ) tend to be similar . We show results based on a lexical similarity metric called cohesion to justify our claims . BibTeX : . Books and Other One - Time Publications . Abu - Jbara , A. & Radev , D. ( 2011 ) , \\\" Coherent citation - based summarization of scientific papers \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 500 - 509 . Association for Computational Linguistics . Abstract : In citation - based summarization , text written by several researchers is leveraged to identify the important aspects of a target paper . Previous work on this problem focused almost exclusively on its extraction aspect ( i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper ) . Meanwhile , the fluency of the produced summaries has been mostly ignored . For example , diversity , readability , cohesion , and ordering of the sentences included in the summary have not been thoroughly considered . This resulted in noisy and confusing summaries . In this work , we present an approach for producing readable and cohesive citation - based summaries . Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency . BibTeX : . Hu , Y. , Boyd - Graber , J. & Satinoff , B. ( 2011 ) , \\\" Interactive topic modeling \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 248 - 257 . Association for Computational Linguistics . Abstract : Topic models have been used extensively as a tool for corpus exploration , and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data . However , creating such extensions requires expertise in machine learning unavailable to potential end - users of topic modeling software . In this work , we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation ( LDA ) by adding constraints that enforce that sets of words must appear together in the same topic . We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference ; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions . BibTeX : . Muthukrishnan , P. , Radev , D. & Mei , Q. ( 2011 ) , \\\" Simultaneous similarity learning and feature - weight learning for document clustering \\\" , In HLT - TextGraphs ' 11 : Proc . TextGraphs-6 workshop on Graph - based Methods for Natural Language Processing . Portland , OR . June 2011 . , pp . 42 - 50 . Association for Computational Linguistics . Abstract : A key problem in document classification and clustering is learning the similarity between documents . Traditional approaches include estimating similarity between feature vectors of documents where the vectors are computed using TF - IDF in the bag - of - words model . However , these approaches do not work well when either similar documents do not use the same vocabulary or the feature vectors are not estimated correctly . In this paper , we represent documents and keywords using multiple layers of connected graphs . We pose the problem of simultaneously learning similarity between documents and keyword weights as an edge - weight regularization problem over the different layers of graphs . Unlike most feature weight learning algorithms , we propose an unsupervised algorithm in the proposed framework to simultaneously optimize similarity and the keyword weights . We extrinsically evaluate the performance of the proposed similarity measure on two different tasks , clustering and classification . The proposed similarity measure outperforms the similarity measure proposed by ( Muthukrishnan et al . , 2010 ) , a state - of - the - art classification algorithm ( Zhou and Burges , 2007 ) and three different baselines on a variety of standard , large data sets . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2011 ) , \\\" Exploiting phase transition in similarity networks for clustering \\\" , In AAAI ' 11 : Proc . 25th Conference on Artificial Intelligence . August 2011 . Abstract : In this paper , we model the pair - wise similarities of a set of documents as a weighted network with a single cutoff parameter . Such a network can be thought of an ensemble of unweighted graphs , each consisting of edges with weights greater than the cutoff value . We look at this network ensemble as a complex system with a temperature parameter , and refer to it as a Latent Network . Our experiments on a number of datasets from two different domains show that certain properties of latent networks like clustering coef?cient , average shortest path , and connected components exhibit patterns that are signi?cantly divergent from randomized networks . We explain that these patterns re?ect the network phase transition as well as the existence of a community structure in document collections . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2011 ) , \\\" Learning from human collective behavior to introduce diversity in summary generation \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 1098 - 1108 . Association for Computational Linguistics . Abstract : We analyze collective discourse , a collective human behavior in content generation , and show that it exhibits diversity , a property of general collective systems . Using extensive analysis , we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization . We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries . We show how different summaries use various phrasal information units ( i.e. , nuggets ) to express the same atomic semantic units , called factoids . Finally , we present a ranker that employs distributional similarities to build a network of words , and captures the diversity of perspectives by detecting communities in this network . Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity . BibTeX : . Satinoff , B. & Boyd - Graber , J. ( 2011 ) , \\\" Trivial classification : What features do humans use for classification ? \\\" , In Workshop on Crowdsourcing Technologies for Language and Cognition Studies . Whidby , M. , Zajic , D. & Dorr , B.J. ( 2011 ) , \\\" Citation handling for improved summarization of scientific documents \\\" . University of Maryland , Technical Report LAMP - TR-157 , 2011 . Abstract : In this paper we present the first steps toward improving summarization of scientific documents through citation analysis and parsing . Prior work ( Mohammad et al . , 2009 ) argues that citation texts ( sentences that cite other papers ) play a crucial role in automatic summarization of a topical area , but did not take into account the noise introduced by the citations themselves . We demonstrate that it is possible to improve summarization output through careful handling of these citations . We base our experiments on the application of an improved trimming approach to summarization of citation texts extracted from Question - Answering and Dependency - Parsing documents . We demonstrate that confidence scores from the Stanford NLP Parser ( Klein and Manning , 2003 ) are significantly improved , and that Trimmer ( Zajic et al . , 2007 ) , a sentence - compression tool , is able to generate higher - quality candidates . Our summarization output is currently used as part of a larger system , Action Science Explorer ( ASE ) ( Gove , 2011 ) . BibTeX : . Lin , J. , Madnani , N. & Dorr , B. ( 2010 ) , \\\" Putting the user in the loop : Interactive maximal marginal relevance for query - focused summarization \\\" , In HLT ' 10 : Proc . Human Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics . June 2010 . , pp . 305 - 308 . Association for Computational Linguistics . Abstract : This work represents an initial attempt to move beyond \\\" single - shot \\\" summarization to interactive summarization . We present an extension to the classic Maximal Marginal Relevance ( MMR ) algorithm that places a user \\\" in the loop \\\" to assist in candidate selection . Experiments in the complex interactive Question Answering ( ciQA ) task at TREC 2007 show that interactively - constructed responses are significantly higher in quality than automatically - generated ones . This novel algorithm provides a starting point for future work on interactive summarization . BibTeX : . Qazvinian , V. , Radev , D.R. & \\u00d6zg\\u00fcr , A. ( 2010 ) , \\\" Citation summarization through keyphrase extraction \\\" , In COLING ' 10 : Proc . 23rd International Conference on Computational LInguistics . August 2010 . , pp . 895 - 903 . Abstract : This paper presents an approach to summarize single scientific papers , by extracting its contributions from the set of citation sentences written in other papers . Our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary . Comparisons show how this methodology excels at the task of single paper summarization , and how it out - performs other multi - document summarization methods . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2010 ) , \\\" Identifying non - explicit citing sentences for citation - based summarization \\\" , In ACL ' 10 : Proc . 48th Annual Meeting of the Association for Computational Linguistics . July 2010 . , pp . 555 - 564 . Abstract : Identifying background ( context ) information in scientific articles can help scholars understand major contributions in their research area more easily . In this paper , we propose a general framework based on probabilistic inference to extract such context information from scientific papers . We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create , and employ a Belief Propagation mechanism to detect likely context sentences . We also address the problem of generating surveys of scientific papers . Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone . BibTeX : . Mohammad , S. , Dunne , C. & Dorr , B. ( 2009 ) , \\\" Generating high - coverage semantic orientation lexicons from overtly marked words and a thesaurus \\\" , In EMNLP ' 09 : Proc . 2009 conference on Empirical Methods in Natural Language Processing . Morristown , NJ , USA . August 2009 . , pp . 599 - 608 . Association for Computational Linguistics . Abstract : Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words . A number of approaches have been proposed for creating such lexicons , but they tend to be computationally expensive , and usually rely on significant manual annotation and large corpora . Most of these methods use WordNet . In contrast , we propose a simple approach to generate a high - coverage semantic orientation lexicon , which includes both individual words and multi - word expressions , using only a Roget - like thesaurus and a handful of affixes . Further , the lexicon has properties that support the Polyanna Hypothesis . Using the General Inquirer as gold standard , we show that our lexicon has 14 percentage points more correct entries than the leading WordNet - based high - coverage lexicon ( SentiWordNet ) . In an extrinsic evaluation , we obtain significantly higher performance in determining phrase polarity using our thesaurus - based lexicon than with any other . Additionally , we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above . BibTeX : . Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics . Stroudsburg , PA , USA . , pp . 584 - 592 . Association for Computational Linguistics . Abstract : The number of research publications in various disciplines is growing exponentially . Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material . In this paper we present the first steps in producing an automatically generated , readily consumable , technical survey . Specifically we explore the combination of citation information and summarization techniques . Even though prior work ( Teufel et al . , 2006 ) argues that citation text is unsuitable for summarization , we show that in the framework of multi - document survey creation , citation texts can play a crucial role . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2009 ) , \\\" The evolution of scientific title networks \\\" , In ICWSM ' 09 : Proc . 2009 International AAAI Conference on Weblogs and Social Media poster session . Abstract : In spite of enormous previous efforts to model the growth of various networks , there have only been a few works that successfully describe the evolution of latent networks . In a latent network edges do not represent interactions between nodes , but show some proximity values . In this paper we analyze the structure and evolution of a specific type of latent networks over time by looking at a wide range of document similarity networks , in which scientific titles are nodes and their similarities are weighted edges . We use scientific papers as the corpora in order to determine the behavior of authors in choosing words for article titles . The aim of our work is to see whether term selection for titles depends on earlier published titles . BibTeX : . Radev , D.R. , Muthukrishnan , P. & Qazvinian , V. ( 2009 ) , \\\" The ACL Anthology Network corpus \\\" , In NLPIR4DL ' 09 : Proc . ACL - IJCNLP 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries . Stroudsburg , PA , USA . , pp . 54 - 61 . Association for Computational Linguistics . Abstract : We introduce the ACL Anthology Network ( AAN ) , a manually curated networked database of citations , collaborations , and summaries in the field of Computational Linguistics . We also present a number of statistics about the network including the most cited authors , the most central collaborators , as well as network statistics about the paper citation , author citation , and author collaboration networks . BibTeX : . Aris , A. ( 2008 ) , \\\" Visualizing and exploring networks using Semantic Substrates \\\" . School : University of Maryland , Department of Computer Science . Abstract : Visualizing and exploring network data has been a challenging problem for HCI ( Human - Computer Interaction ) Information Visualization researchers due to the complexity of representing networks ( graphs ) . Research in this area has concentrated on improving the visual organization of nodes and links according to graph drawing aesthetics criteria , such as minimizing link crossings and the longest link length . Semantic substrates offer a different approach by which node locations represent node attributes . Users define semantic substrates for a given dataset according to the dataset characteristics and the questions , needs , and tasks of users . The substrates are typically 2 - 5 non - overlapping rectangular regions that meaningfully lay out the nodes of the network , based on the node attributes . Link visibility filters are provided to enable users to limit link visibility to those within or across regions . The reduced clutter and visibility of only selected links are designed to help users find meaningful relationships . Applications include legal precedent ( with court cases citing one another ) , food - web ( predator - prey relationships ) data , scholarly paper citations , and U. S. Senate voting patterns . These case studies , which had networks of up to 4,296 nodes and 16,385 links , helped refine NVSS and the semantic substrate approach , as well as understand its limitations . The case study approach enabled users to gain insights and form hypotheses about their data , while providing guidance for NVSS revisions . The proposed guidelines for semantic substrate definitions are potentially applicable to other datasets such as social networks , business networks , and email communication . NVSS appears to be an effective tool because it offers a user - controlled and understandable method of exploring networks . The main contributions of this dissertation include the extensive exploration of semantic substrates , implementation of software to define substrates , guidelines to design good substrates , and case studies to illustrate the applicability of the approach to various domains and its benefits . BibTeX : . Sixth International Language Resources and Evaluation . May 2008 . , pp . 1755 - 1759 . European Language Resources Association ( ELRA ) . Dorr , B. , Mohammad , S. & Onyshkevych , B. ( 2008 ) , \\\" From linguistic annotations to knowledge objects \\\" , In SKDOU ' 08 : Proc . Symposium on Semantic Knowledge Discovery , Organizaiton and Use . November 2008 . Klavans , Judith , Shneiderman , Ben & others ( 2008 ) , \\\" Motivating interactive summarizations : User guided exploration of new domains \\\" . Mohammad , S. , Dorr , B.J. & Hirst , G. ( 2008 ) , \\\" Computing word - pair antonymy \\\" , In EMNLP ' 08 : Proc . 2008 conference on Empirical Methods in Natural Language Processing . October 2008 . , pp . 982 - 991 . Association for Computational Linguistics . Abstract : Knowing the degree of antonymy between words has widespread applications in natural language processing . Manually - created lexicons have limited coverage and do not include most semantically contrasting word pairs . We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus . The approach is evaluated on a set of closest - opposite questions , obtaining a precision of over 80 % . Along the way , we discuss what humans consider antonymous and how antonymy manifests itself in utterances . BibTeX : . November 2008 . At the heart of our summarization system is Trimmer , which generates multiple alternative compressed versions of the source sentences that act as candidate sentences for inclusion in the summary . For the ? rst time , we investigated the use of automatically generated antonym pairs for both text summarization and recognizing textual entailment . The UMD summaries for the opinion task were especially effective in providing non - redundant information ( rank 3 out of a total 19 submissions ) . More coherent summaries resulted when using the antonymy feature as compared to when not using it . On the RTE task , even when using only automatically generated antonyms the system performed as well as when using a manually compiled list of antonyms . BibTeX : . Mohammad , S. , Dorr , B. & Hirst , G. ( 2008 ) , \\\" Towards antonymy - aware natural language applications \\\" , In SKDOU ' 08 : Proc . Symposium on Semantic Knowledge Discovery , Organizaiton and Use . November 2008 . Muthukrishnan , P. , Gerrish , J. & Radev , D.R. ( 2008 ) , \\\" Detecting multiple facets of an event using graph - based unsupervised methods \\\" , In COLING ' 08 : Proc . 22nd International Conference on Computational Linguistics . August 2008 . , pp . 609 - 616 . Abstract : We propose a new unsupervised method for topic detection that automatically identifies the different facets of an event . We use pointwise Kullback - Leibler divergence along with the Jaccard coefficient to build a topic graph which represents the community structure of the different facets . The problem is formulated as a weighted set cover problem with dynamically varying weights . The algorithm is domain - independent and generates a representative set of informative and discriminative phrases that cover the entire event . We evaluate this algorithm on a large collection of blog postings about different news events and report promising results . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2008 ) , \\\" Scientific paper summarization using citation summary networks \\\" , In COLING ' 08 : Proc . 22nd International Conference on Computational Linguistics . Stroudsburg , PA , USA . , pp . 689 - 696 . Association for Computational Linguistics . Abstract : Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study . One possible way to overcome this problem is to summarize a scientific topic . In this paper , we propose a model of summarizing a single article , which can be further used to summarize an entire topic . Our model is based on analyzing others ' viewpoint of the target article 's contributions and the study of its citation summary network using a clustering approach . BibTeX : . Shneiderman , B. ( 2008 ) , \\\" Research agenda : Visual overviews for exploratory search \\\" , In National Science Foundation Workshop on Information Seeking Support Systems . June 2008 . Abstract : Exploratory search is necessary when users knowledge of the domain is incomplete or when initial user goals do not match available data or metadata that is the basis for search indexing attributes . Such mismatches mean that users need to learn more in order to develop a better understanding of the domain or to revise their search goals . Exploratory search processes may take weeks or months , so interfaces that support prolonged exploration are necessary . The attraction of exploratory search is that users can take on more ambitious goals that require substantial learning and creative leaps to bridge the gaps between what they know and that they seek . BibTeX : . Shneiderman , Ben , Aris , Aleks & others ( 2008 ) , \\\" Visual summarization of topic evaluation and topic dependencies \\\" . Joseph , M.T. & Radev , D.R. ( 2007 ) , \\\" Citation analysis , centrality , and the ACL Anthology \\\" . University of Michigan . Department of Electrical Engineering and Computer Science , Technical Report CSE - TR-535 - 07 , 2007 . Abstract : We analyze the ACL Anthology citation network in an attempt to identify the most ? central ? papers and authors using graph - based methods . Citation data was obtained using text extraction from the library of PDF files with some post - processing performed to clean up the results . Manual annotation of the references was then performed to complete the citation network . The analysis compares metrics across publication years and venues , such as citations in and out . The most cited paper , central papers , and papers with the highest impact factor are also established . BibTeX : . Radev , D.R. , Hodges , M. , Fader , A. , Joseph , M. , Gerrish , J. , Schaller , M. , dePeri , J. & Gibson , B. ( 2007 ) , \\\" CLAIRLIB documentation v1.03 \\\" . University of Michigan . Department of Electrical Engineering and Computer Science , Technical Report CSE - TR-536 - 07 , 2007 . \"}",
        "_version_":1692580798769659904,
        "score":163.2028},
      {
        "id":"e326311c-beec-42a9-b2a8-0360d587a93c",
        "_src_":"{\"url\": \"http://www.walkoffwalk.com/2010/07/angry-old-hall-of-famer-now-ha.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166141.55/warc/CC-MAIN-20160205193926-00223-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Natural Language Processing in Textual Information Retrieval and Related Topics . Mari Vallez ; Rafael Pedraza - Jimenez . Citaci\\u00f3n recomendada : Mari Vallez ; Rafael Pedraza - Jimenez . Natural Language Processing in Textual Information Retrieval and Related Topics [ en linea]. \\\" Hipertext.net \\\" , num . \\\" Natural Language Processing \\\" ( NLP ) as a discipline has been developing for many years . It was formed in 1960 as a sub - field of Artificial Intelligence and Linguistics , with the aim of studying problems in the automatic generation and understanding of natural language . At first its methods were widely accepted and successful . However , when applied in controlled environments and with a generic vocabulary , many problems arose . Among those problems were polysemy and synonymy . In recent years contributions to this field have improved substantially , allowing for the processing of huge amounts of textual information with an acceptable level of efficacy . An example of this is the application of these techniques as an essential component in web search engines , in automated translation tools or in summary generators [ Baeza - Yates , 2004]. Problems with natural language processing : linguistic variation and ambiguity . Natural language , understood as a tool that people use to express themselves , has specific properties that reduce the efficacy of textual information retrieval systems . These properties are linguistic variation and ambiguity . By linguistic variation we mean the possibility of using different words or expressions to communicate the same idea . Linguistic ambiguity is when a word or phrase allows for more than one interpretation . Both phenomenons affect the information retrieval process , even though in different ways . Linguistic variation provokes document silence , that is , the omission of relevant documents that fulfil information needs , because the same terms were not used as those found in the document . Ambiguity , on the other hand , implies document noise , or the inclusion of non - meaningful documents , since documents were retrieved that used the same term but with a different meaning . These characteristics make automated language processing considerably difficult . The following is a set of examples that show the repercussions of these phenomena in information retrieval : . At a morphological level , the same word may play different morph - syntactic roles relative to the context in which they appear , causing ambiguity problems ( example 1 ) . Example 1 . A notebook was the present that his wife gave him when all of us were present at the party . In this case , the word \\\" present \\\" acts both as an adjective and noun , and with different meanings . At a syntactic level , focusing on the study of established relations between words to form larger linguistic units , phrases and sentences , ambiguities are produced as a consequence of the possibility of associating a sentence with more than one syntactic structure . On the other hand , this variation supposes the possibility of expressing the same idea , but changing the order of the sentence 's syntactic structure . ( example 2 ) . Example 2 . He ate the chocolates on the plane . This example could mean that \\\" He ate the chocolates that were in the plane \\\" or that \\\" He ate the chocolates when he was flying in the plane . \\\" At a semantic level we study the meaning of a word and sentence by studying the meaning of each of the words in it . Ambiguity is produced because a word can have one or various meanings , which is known as polysemy ( example 3 ) . Example 3 . Paul was reading a newspaper in the bank . The term \\\" bank \\\" could refer to a financial institution or a mound . And we must also keep in mind lexical variation which refers to the possibility of using different terms for the same meaning , that is , a synonymy ( example 4 ) : . Example 4 : Car / Auto / Automobile . At a pragmatic level , based on a language 's relationship to its context , we often can not use a literal and automated interpretation of the terms used . In specific circumstances , the sense of the words in the sentence must be interpreted at a level that includes the context in which the sentence is found . ( example 5 ) . Example 5 . Give me a break . Here we are asking for rest from work , or we could be asking the perceiver to leave us alone . Another important topic is ambiguity provoked by an anaphora , for example , the presence of pronouns and adverbs that refer to something that was previously mentioned ( example 6 ) . Example 6 . It was terrible for him she had not to manipulate it . Who is he ? And she ? What was not manipulated ? It is impossible to understand this sentence out of context . All of these examples demonstrate the complexity of language , and that any automated processing is not easy or obvious . Natural Language processing in textual information retrieval . As the reader has probably already deduced , the complexity associated with natural language is especially key when retrieving textual information [ Baeza - Yates , 1999 ] to satisfy a user 's information needs . In other words , a textual information retrieval system carries out the following tasks in response to a user 's query ( image 1 ) : . Indexing the collection of documents : in this phase , NLP techniques are applied to generate an index containing document descriptions . Normally each document is described through a set of terms that , in theory , best represents its content . When a user formulates a query , the system analyses it , and if necessary , transforms it with the hope of representing the user 's information needs in the same way as the document content is represented . The system compares the description of each document with that of the query , and presents the user with those documents whose descriptions are closest to the query description . The results are usually listed in order of relevancy , that is , by the level of similarity between the document and query descriptions . As of now there are no NLP techniques that allow us to extract a document 's or query 's meaning without any mistakes . In fact , the scientific community is divided on the procedure to follow in reaching this goal . In the following section we will explain the functions and peculiarities of the two key approaches to natural language processing : a statistical approach and a linguistic focus . Both proposals differ considerably , even though in practice natural language processing systems use a mixed approach , combining techniques from both focuses . Statistical processing of natural language . Statistical processing of natural language [ Manning , 1999 ] represents the classical model of information retrieval systems , and is characterised from each document 's set of key words , known as the terms index . This is a very simple focus based on the \\\" bag of words . \\\" In this approach , all words in a document are treated as its index terms . Moreover , each term is assigned a weight in function of its importance , usually determined by its appearance frequency within the document . This way the word 's order , structure , meaning , etc , are not taken into consideration . These models are then limited to pairing the documents ' words with that of the query 's . Its simplicity and efficacy has become the most commonly used contemporary models in textual information retrieval systems . This document processing model involves the following stages : . a)Document pre - processing : fundamentally consisting in preparing the documents for its parameterisation , eliminating any elements considered as superfluous . b)Parameterisation : a stage of minimal complexity once the relevant terms have been identified . This consists in quantifying the document 's characteristics ( that is , the terms ) . Below we will illustrate their function using this paper 's first paragraph as an example , assuming that it is XML tagged . So the document on which we would apply the pre - processed and parameterisation techniques would be the following : . ( Example 6 ) . Stemming terms is a linguistic process that attempts to determine the base ( lemma ) of each word in a text . Its aim is to reduce a word to its root , so that the key words in a query or document are represented by their roots instead of the original words . The lemma of word is its basic form along with its inflected forms . For example , \\\" inform \\\" could be the lemma of \\\" information \\\" or \\\" inform . \\\" However , these algorithms have the -inconvenience of sometimes not grouping words that should be grouped , and vice versa : erroneously presenting words as equals . Parametrising documents consists in assigning a weight to each one of the relevant terms associated to a document . A term 's weight is usually calculated as a function of its appearance frequency in the document , indicating the importance of these terms as the document 's content description ( example 8) . Example 8 . Fragment of a parametrised document ( see how the frequencies of each term changes as the quantification of the remaining terms in the document continues ) . One of the most often used methods to estimate the importance of a term is the TF.IDF system ( Term Frequency , Inverse Document Frequency ) . It is designed to calculate the importance of a term relative to its appearance frequency in a document , but as a function of the total appearance frequency for all of the corpus ' documents . That is , the fact that a term appears often in one document is indicative that that term is representative of the content , but only when that term does not appear frequently in all documents . If it appeared frequently in all documents , it would not have any discriminatory value ( for example , it would be absurd to represent the content of a document in a recipe database by the frequency of the word food , even though it appears often ) . Finally , and as we have already mentioned , we must describe two commonly used techniques in the statistical processing of natural language : . a ) Detecting N - Grams : this consists in identifying words that are usually together ( compound words , proper nouns , etc . ) to be able to process them as a single conceptual unit . This is usually done by estimating the probability of two words that are often together make up a single term ( compound ) . These techniques attempt to identify compound terms such as \\\" accommodation service \\\" or \\\" European Union . \\\" b ) Stopwords lists : a list of empty words in a terms list ( prepositions , determiners , pronouns , etc . ) considered to have little semantic value , and are eliminated when found in document , leaving them out of the terms index to be analysed . Deleting all of these terms avoids document noise problems and saves on resources , since in documents few elements are repeated frequently . Linguistic processing of natural language . This approach is based on the application of different techniques and rules that explicitly encode linguistic knowledge [ Sanderson , 2000]. The documents are analysed through different linguistic levels ( as previously mentioned ) by linguistic tools that incorporate each level 's own annotations to the text . Below we show the different steps to take in a linguistic analysis of documents , even though not all systems use them . The morphological analysis is performed by taggers that assign each word to a grammatical category according to the morphological characteristics found . After having identified and analysed the words in a text , the next step is to see how they are related and used together in making larger grammatical units , phrases and sentences . Therefore a syntax analysis of the text is performed . This is when parsers are applied : descriptive formalism that demonstrate the text 's syntax structure . The techniques used to apply and create parsers vary and depend on the aim of the syntax analysis . For information retrieval it is often used for a superficial analysis aiming to only identify the most meaningful structures : nominal sentences , verbal and prepositional sentence , values , etc . This level of analysis is usually used to optimise resources and not slow down the system 's response . From the text 's syntax structure , the next aim is to obtain the meaning of the sentences within it . The aim is to obtain the sentence 's semantic representation from the elements that make it up . One of the most often used tools in semantic processing is the lexicographic database WordNet . This is an annotated semantic lexicon in different languages made up of synonym groups called synsets which provide short definitions along with the different semantic relationships between synonym groups . There are different fields of research relative to information retrieval and natural language processing that focus on the problem from other perspectives , but whose final aim is to facilitate information access . Information extraction consists in extracting entities , events and existing relationships between elements in a text or group of texts . This is one way of efficiently accessing large documents since it extracts parts of the document shown in its content . The information generated can be used as knowledge and ontology databases . Summary generators compress a text 's most relevant information . The techniques most often used vary according to the rate of compression , the summary 's aim , the text 's genre and language ( or languages ) of the original text , among other factors . Question answering aims to give a specific response to the formulated query . The information needs must be well - defined : dates , places , etc . Here the processing of natural language attempts to identify the type of response to provide ( by disambiguating the question , analysing the set restrictions , and the use of information extraction techniques . These systems are considered to be the potential successors to the current information retrieval systems . START natural language system is an example of one of these systems . Retrieving multi - language information involves the possibility of retrieving information even though the question and/or documents are in different languages . Automatic translators are used on the documents and/or questions , or the use of interlingua mechanisms to interpret documents . These systems are still a great challenge to researches since they combine two key aspects of the Web 's current context : retrieving information and processing multilingual information . Finally , we must cite the automatic text classification techniques , which automatically assign a set of documents into categories within predefined classifications . The correct description of the document 's characteristics ( usually through the use of statistical techniques -- pre - processed and parametrisation ) strongly influences the quality of the grouping / categorization by these techniques . Conclusions . With the aim of understanding the current Natural Language Process , we have concisely defined the key concepts and techniques associated with this field , along with some simple examples to help the reader better understand . Moreover , we have shown how , despite its years of experience , NLP is a very live and developing field of linguistics , with its many challenges still to overcome due to natural language 's ambiguity . We have paid special attention to the differences between statistical and linguistic methods in natural language processing . Even the scientific communities that support each approach are usually at odds , and NLP is often applied by using a combination of techniques from both approaches . Our experience in this field has made us conclude that it is not possible to claim one approach is better than the other ; this even includes the use of a mixed approach . Relative to information retrieval , the statistical processing techniques are more often used in commercial applications . However , in our opinion , the behaviour and efficacy of the different NLP techniques vary depending on the nature of the task at hand , the type of documents to analyse and the computational cost to assume . Overall , we can deduce the need to continue working on this with the hope of creating new techniques or focuses that will help us overcome these existing shortcomings . This is the only way we can finally reach what seems like the impossible dream of automatic comprehension of natural language . Finally , and as an Annexe ( Annexe 1 ) , we have described some of the unique aspects of processing Spanish , including the mention of some of the key initiatives developed to process this language . Acknowledgments . This project has been partially financed by the Ministerio de Educaci\\u00f3n y Ciencia ( Spain ) as part of the HUM2004 - 03162/FILO project . References . [ Allan , 1995 ] J. Allan [ et al.]. Recent experiments with INQUERY , in : HARMAN , D.K. from : The Fourth Text Retrieval Conference , NIST SP 500 - 236 , Gaithersburg , Maryland . [ Baeza - Yates , 1999 ] Baeza - Yates , R. and Ribeiro - Neto , Berthier . Modern information retrieval . Addison - Wesley Longman . [ Baeza - Yates , 2004 ] Baeza - Yates , R. ( 2004 ) . Challenges in the Interaction of Information Retrieval and Natural Language Processing . in Proc . 5 th International Conference on Computational Linguistics and Intelligent Text Processing ( CICLing 2004 ) , Seoul , Corea . Lecture Notes in Computer Science vol . 2945 , pages 445 - 456 , Springer . [ Carmona , 1998 ] J. Carmona [ et al.]. An environment for morphosyntactic processing of unrestricted spanish text . In : LREC 98 : Procedings of the First International Conference on Language Resources and Evaluation , Granada , Espa\\u00f1a . [ Figuerola , 2000 ] C. G. Figuerola . La investigaci\\u00f3n sobre recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol . In : C.Gonzalo Garc\\u00eda and V. Garc\\u00eda Yedra , editors , Documentaci\\u00f3n , Terminolog\\u00eda y Traducci\\u00f3n , pages 73 - 82 . S\\u00edntesis , Madrid . [ Figuerola , 2004 ] C. G. Figuerola [ et al.]. La recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol y la normalizaci\\u00f3n de t\\u00e9rminos , in : Revista Iberoamericana de Inteligencia Artificial , vol VIII , n\\u00ba 22 , pp . 135 - 145 . [ Manning , 1999 ] Manning , C. D. and Sch\\u00fctze , H. ( 1999 ) . Foundations of statistical natural language processing . MIT Press . Cambridge , MA : May , p. 680 . [ Rodr\\u00edguez , 1996 ] S. Rodr\\u00edguez y J. Carretero . A formal approach to Spanish morphology : the COES tools . En : XII Congreso de la SEPLN , Sevilla , pp . 118 - 126 . [ Sanderson , 2000 ] Sanderson , M. ( 2000 ) . Retrieving with good sense , In : Information Retrieval , 2 , 49 - 69 . [ Santana , 1997 ] O. Santana [ et al.]. Flexionador y lematizador autom\\u00e1tico de formas verbales , In : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XIX(2 ) , pp . 229 - 282 . [ Santana , 1999 ] O. Santana [ et al.]. Flexionador y lematizador de formas nominales , en : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XXI(2 ) , pp . 253 - 297 . [ Strzalkowski , 1999 ] Strzalkowski , T. ( 1999 ) . Natural Language Information Retrieval . Netherlands : Kluwer Academic Publishers . As an annexe we will go on to show some of the most important characteristics of natural language processing in Spanish : . Empty words lists [ Figuerola , 2000 ] : creating these tools for Spanish is quite a challenge , mainly due to the lack of collections and statistical studies in Spanish that would advise for or against its use . Furthermore , creating these lists varies in function of whether or not they are used in processing general or specific information . If our corpus is not field specific , then the list of empty words should mainly include : determiners , pronouns , adverbs , prepositions and conjunctions . But if the information to be analysed is field specific , this list should be modified and/or extended by an expert of the field in question . We must also mention that many researches have noted the advantage of using fixed expressions as elements in empty words lists . Specifically [ Allan , 1995 ] recommends using a short list of \\\" empty phrases \\\" : indication of , which are , how are , information on . Stemming techniques : the majority of information retrieval techniques use frequency counts of terms found in the documents and queries . This implies the need to standardise these terms in order for the count to be carried out properly , taking into consideration those terms with the same lemma or root . There are various lemma and morphological analysers for Spanish . Finally , it is worth noting that experiments of this kind of algorithms in Spanish has shown that standardising terms through stemming techniques provides improved results . The S - stemmer algorithm comes up with surprising results . This algorithm is very simple and basically it simply reduces plural words to their singular form . In its original version ( for English ) , this algorithm only eliminates the last \\\" s \\\" of each word . For Spanish , this algorithm could be reinforced by including plural forms of nouns and adjectives with consonants , thus ending in \\\" es . \\\" Eliminating the \\\" es \\\" suffixes could produce inconsistencies with words ending in \\\" e \\\" in their singular form , which would require the elimination of \\\" e \\\" endings . We have also shown that eliminating gender specific \\\" a \\\" or \\\" o \\\" endings improves results . The greatest advantage of this algorithm is its simplicity . However , the inconvenience is that the S - stemmer is incapable of distinguishing nouns and adjectives from other grammatical categories , thus applying it to all words ; it also does not distinguish between irregular plural forms . But on the other hand , by treating all words in the same form , it does not introduce additional noise . University Pompeu Fabra . Department of Communication . DigiDoc Research Group Campus de la Comunicaci\\u00f3 . Roc Boronat , 138 , office 53804 . Barcelona 08018 Tel : 93 542 13 11 . E - mail : cristofol.rovira@upf.edu Legal deposit B-49106 - 2002 - ISSN 1695 - 5498 \"}",
        "_version_":1692669059566403584,
        "score":161.05217}]
  }}
