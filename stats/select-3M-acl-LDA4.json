{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"word model based models corpus collins e.g tree translation parse brown results words sentences sentence feature brill disambiguation approach sense",
      "fl":"*,score"}},
  "response":{"numFound":1332337,"start":0,"maxScore":35.89041,"numFoundExact":true,"docs":[
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":35.89041},
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":31.496538},
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":28.695162},
      {
        "id":"18e4e021-57ca-4059-acea-c98ca6ae1dee",
        "_src_":"{\"url\": \"http://www.pandasthumb.org/archives/2007/02/the_haeckelwell.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153998.27/warc/CC-MAIN-20160205193913-00200-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Categorizing and Tagging Words . Back in elementary school you learnt the difference between nouns , verbs , adjectives , and adverbs . These \\\" word classes \\\" are not just the idle invention of grammarians , but are useful categories for many language processing tasks . As we will see , they arise from simple analysis of the distribution of words in text . The goal of this chapter is to answer the following questions : . What are lexical categories and how are they used in natural language processing ? What is a good Python data structure for storing words and their categories ? How can we automatically tag each word of a text with its word class ? Along the way , we 'll cover some fundamental techniques in NLP , including sequence labeling , n - gram models , backoff , and evaluation . These techniques are useful in many areas , and tagging gives us a simple context in which to present them . We will also see how tagging is the second step in the typical NLP pipeline , following tokenization . The process of classifying words into their parts of speech and labeling them accordingly is known as part - of - speech tagging , POS - tagging , or simply tagging . Parts of speech are also known as word classes or lexical categories . The collection of tags used for a particular task is known as a tagset . Our emphasis in this chapter is on exploiting tags , and tagging text automatically . 1 Using a Tagger . A part - of - speech tagger , or POS - tagger , processes a sequence of words , and attaches a part of speech tag to each word ( do n't forget to import nltk ): . Here we see that and is CC , a coordinating conjunction ; now and completely are RB , or adverbs ; for is IN , a preposition ; something is NN , a noun ; and different is JJ , an adjective . Note . NLTK provides documentation for each tag , which can be queried using the tag , e.g. nltk.help.upenn_tagset ( ' RB ' ) , or a regular expression , e.g. nltk.help.upenn_tagset ( ' NN . Some corpora have README files with tagset documentation , see nltk.corpus . readme ( ) , substituting in the name of the corpus . Let 's look at another example , this time including some homonyms : . Notice that refuse and permit both appear as a present tense verb ( VBP ) and a noun ( NN ) . E.g. refUSE is a verb meaning \\\" deny , \\\" while REFuse is a noun meaning \\\" trash \\\" ( i.e. they are not homophones ) . Thus , we need to know which word is being used in order to pronounce the text correctly . ( For this reason , text - to - speech systems usually perform POS - tagging . ) Note . Your Turn : Many words , like ski and race , can be used as nouns or verbs with no difference in pronunciation . Can you think of others ? Hint : think of a commonplace object and try to put the word to before it to see if it can also be a verb , or think of an action and try to put the before it to see if it can also be a noun . Now make up a sentence with both uses of this word , and run the POS - tagger on this sentence . Lexical categories like \\\" noun \\\" and part - of - speech tags like NN seem to have their uses , but the details will be obscure to many readers . You might wonder what justification there is for introducing this extra level of information . Many of these categories arise from superficial analysis the distribution of words in text . Consider the following analysis involving woman ( a noun ) , bought ( a verb ) , over ( a preposition ) , and the ( a determiner ) . The text.similar ( ) method takes a word w , finds all contexts w 1 w w 2 , then finds all words w ' that appear in the same context , i.e. w 1 w ' w 2 . Observe that searching for woman finds nouns ; searching for bought mostly finds verbs ; searching for over generally finds prepositions ; searching for the finds several determiners . A tagger can correctly identify the tags on these words in the context of a sentence , e.g. The woman bought over $ 150,000 worth of clothes . A tagger can also model our knowledge of unknown words , e.g. we can guess that scrobbling is probably a verb , with the root scrobble , and likely to occur in contexts like he was scrobbling . 2 Tagged Corpora . 2.1 Representing Tagged Tokens . By convention in NLTK , a tagged token is represented using a tuple consisting of the token and the tag . We can create one of these special tuples from the standard string representation of a tagged token , using the function str2tuple ( ) : . We can construct a list of tagged tokens directly from a string . The first step is to tokenize the string to access the individual word / tag strings , and then to convert each of these into a tuple ( using str2tuple ( ) ) . , ' . ' ) ] 2.2 Reading Tagged Corpora . Several of the corpora included with NLTK have been tagged for their part - of - speech . Here 's an example of what you might see if you opened a file from the Brown Corpus with a text editor : . The / at Fulton / np - tl County / nn - tl Grand / jj - tl Jury / nn - tl said / vbd Friday / nr an / at investigation / nn of / in Atlanta's / np$ recent / jj primary / nn election / nn produced / vbd / no / at evidence / nn ' ' / ' ' that / cs any / dti irregularities / nns took / vbd place / nn . Other corpora use a variety of formats for storing part - of - speech tags . NLTK 's corpus readers provide a uniform interface so that you do n't have to be concerned with the different file formats . In contrast with the file fragment shown above , the corpus reader for the Brown Corpus represents the data as shown below . Note that part - of - speech tags have been converted to uppercase , since this has become standard practice since the Brown Corpus was published . Whenever a corpus contains tagged text , the NLTK corpus interface will have a tagged_words ( ) method . Here are some more examples , again using the output format illustrated for the Brown Corpus : . Not all corpora employ the same set of tags ; see the tagset help functionality and the readme ( ) methods mentioned above for documentation . Initially we want to avoid the complications of these tagsets , so we use a built - in mapping to the \\\" Universal Tagset \\\" : . , ... ] . Tagged corpora for several other languages are distributed with NLTK , including Chinese , Hindi , Portuguese , Spanish , Dutch and Catalan . These usually contain non - ASCII text , and Python always displays this in hexadecimal when printing a larger structure such as a list . If your environment is set up correctly , with appropriate editors and fonts , you should be able to display individual strings in a human - readable way . For example , 2.1 shows data accessed using nltk.corpus.indian . Figure 2.1 : POS - Tagged Data from Four Indian Languages : Bangla , Hindi , Marathi , and Telugu . If the corpus is also segmented into sentences , it will have a tagged_sents ( ) method that divides up the tagged words into sentences rather than presenting them as one big list . This will be useful when we come to developing automatic taggers , as they are trained and tested on lists of sentences , not words . 2.3 A Universal Part - of - Speech Tagset . Tagged corpora use many different conventions for tagging words . To help us get started , we will be looking at a simplified tagset ( shown in 2.1 ) . Let 's see which of these tags are the most common in the news category of the Brown corpus : . most_common ( ) [ ( ' NOUN ' , 30640 ) , ( ' VERB ' , 14399 ) , ( ' ADP ' , 12355 ) , ( ' . ' Note . Your Turn : Plot the above frequency distribution using tag_fd . What percentage of words are tagged using the first five tags of the above list ? We can use these tags to do powerful searches using a graphical POS - concordance tool nltk.app.concordance ( ) . Use it to search for any combination of words and POS tags , e.g. N N N N , hit / VD , hit / VN , or the ADJ man . 2.4 Nouns . Nouns generally refer to people , places , things , or concepts , e.g. : woman , Scotland , book , intelligence . Nouns can appear after determiners and adjectives , and can be the subject or object of the verb , as shown in 2.2 . The simplified noun tags are N for common nouns like book , and NP for proper nouns like Scotland . Let 's inspect some tagged text to see what parts of speech occur before a noun , with the most frequent ones first . Then we construct a FreqDist from the tag parts of the bigrams . , ' VERB ' , ' CONJ ' , ' NUM ' , ' ADV ' , ' PRT ' , ' PRON ' , ' X ' ] . This confirms our assertion that nouns occur after determiners and adjectives , including numeral adjectives ( tagged as NUM ) . 2.5 Verbs . Verbs are words that describe events and actions , e.g. fall , eat in 2.3 . In the context of a sentence , verbs typically express a relation involving the referents of one or more noun phrases . What are the most common verbs in news text ? Let 's sort all the verbs by frequency : . Note that the items being counted in the frequency distribution are word - tag pairs . Since words and tags are paired , we can treat the word as a condition and the tag as an event , and initialize a conditional frequency distribution with a list of condition - event pairs . This lets us see a frequency - ordered list of tags given a word : . most_common ( ) [ ( ' VERB ' , 25 ) , ( ' NOUN ' , 3 ) ] . We can reverse the order of the pairs , so that the tags are the conditions , and the words are the events . Now we can see likely words for a given tag . We will do this for the WSJ tagset rather than the universal tagset : . To clarify the distinction between VBD ( past tense ) and VBN ( past participle ) , let 's find words which can be both VBD and VBN , and see some surrounding text : . In this case , we see that the past participle of kicked is preceded by a form of the auxiliary verb have . Is this generally true ? Note . Your Turn : Given the list of past participles produced by list(cfd2 [ ' VN ' ] ) , try to collect a list of all the word - tag pairs that immediately precede items in that list . 2.6 Adjectives and Adverbs . Two other important word classes are adjectives and adverbs . Adjectives describe nouns , and can be used as modifiers ( e.g. large in the large pizza ) , or in predicates ( e.g. the pizza is large ) . English adjectives can have internal structure ( e.g. fall+ing in the falling stocks ) . Adverbs modify verbs to specify the time , manner , place or direction of the event described by the verb ( e.g. quickly in the stocks fell quickly ) . Adverbs may also modify adjectives ( e.g. really in Mary 's teacher was really nice ) . English has several categories of closed class words in addition to prepositions , such as articles ( also often called determiners ) ( e.g. , the , a ) , modals ( e.g. , should , may ) , and personal pronouns ( e.g. , she , they ) . Each dictionary and grammar classifies these words differently . Note . Your Turn : If you are uncertain about some of these parts of speech , study them using nltk.app.concordance ( ) , or watch some of the Schoolhouse Rock ! grammar videos available at YouTube , or consult the Further Reading section at the end of this chapter . 2.7 Unsimplified Tags . Let 's find the most frequent nouns of each noun part - of - speech type . The program in 2.2 finds all tags starting with NN , and provides a few example words for each one . You will see that there are many variants of NN ; the most important contain $ for possessive nouns , S for plural nouns ( since plural nouns typically end in s ) and P for proper nouns . In addition , most of the tags have suffix modifiers : -NC for citations , -HL for words in headlines and -TL for titles ( a feature of Brown tabs ) . def findtags ( tag_prefix , tagged_text ) : . return dict((tag , cfd[tag]. NN [ ( ' year ' , 137 ) , ( ' time ' , 97 ) , ( ' state ' , 88 ) , ( ' week ' , 85 ) , ( ' man ' , 72 ) ] . NN$ [ ( \\\" year 's \\\" , 13 ) , ( \\\" world 's \\\" , 8) , ( \\\" state 's \\\" , 7 ) , ( \\\" nation 's \\\" , 6 ) , ( \\\" company 's \\\" , 6 ) ] . NN$-HL [ ( \\\" Golf 's \\\" , 1 ) , ( \\\" Navy 's \\\" , 1 ) ] . NN$-TL [ ( \\\" President 's \\\" , 11 ) , ( \\\" Army 's \\\" , 3 ) , ( \\\" Gallery 's \\\" , 3 ) , ( \\\" University 's \\\" , 3 ) , ( \\\" League 's \\\" , 3 ) ] . NN - HL [ ( ' sp . ' , 2 ) , ( ' problem ' , 2 ) , ( ' Question ' , 2 ) , ( ' business ' , 2 ) , ( ' Salary ' , 2 ) ] . NN - NC [ ( ' eva ' , 1 ) , ( ' aya ' , 1 ) , ( ' ova ' , 1 ) ] . NN - TL [ ( ' President ' , 88 ) , ( ' House ' , 68 ) , ( ' State ' , 59 ) , ( ' University ' , 42 ) , ( ' City ' , 41 ) ] . NN - TL - HL [ ( ' Fort ' , 2 ) , ( ' Dr. ' , 1 ) , ( ' Oak ' , 1 ) , ( ' Street ' , 1 ) , ( ' Basin ' , 1 ) ] . NNS [ ( ' years ' , 101 ) , ( ' members ' , 69 ) , ( ' people ' , 52 ) , ( ' sales ' , 51 ) , ( ' men ' , 46 ) ] . NNS$ [ ( \\\" children 's \\\" , 7 ) , ( \\\" women 's \\\" , 5 ) , ( \\\" janitors ' \\\" , 3 ) , ( \\\" men 's \\\" , 3 ) , ( \\\" taxpayers ' \\\" , 2 ) ] . NNS$-HL [ ( \\\" Dealers ' \\\" , 1 ) , ( \\\" Idols ' \\\" , 1 ) ] . NNS$-TL [ ( \\\" Women 's \\\" , 4 ) , ( \\\" States ' \\\" , 3 ) , ( \\\" Giants ' \\\" , 2 ) , ( \\\" Bros. ' \\\" , 1 ) , ( \\\" Writers ' \\\" , 1 ) ] . NNS - HL [ ( ' comments ' , 1 ) , ( ' Offenses ' , 1 ) , ( ' Sacrifices ' , 1 ) , ( ' funds ' , 1 ) , ( ' Results ' , 1 ) ] . NNS - TL [ ( ' States ' , 38 ) , ( ' Nations ' , 11 ) , ( ' Masters ' , 10 ) , ( ' Rules ' , 9 ) , ( ' Communists ' , 9 ) ] . NNS - TL - HL [ ( ' Nations ' , 1 ) ] . When we come to constructing part - of - speech taggers later in this chapter , we will use the unsimplified tags . 2.8 Exploring Tagged Corpora . Let 's briefly return to the kinds of exploration of corpora we saw in previous chapters , this time exploiting POS tags . Suppose we 're studying the word often and want to see how it is used in text . We could ask to see the words that follow often . However , it 's probably more instructive use the tagged_words ( ) method to look at the part - of - speech tag of the following words : . VERB ADJ 2 8 7 4 37 6 . Notice that the most high - frequency parts of speech following often are verbs . Nouns never appear in this position ( in this particular corpus ) . In code - three - word - phrase we consider each three - word window in the sentence , and check if they meet our criterion . If the tags match , we print the corresponding words . combined to achieve . continue to place . serve to protect . wanted to wait . allowed to place . expected to become ... . Finally , let 's look for words that are highly ambiguous as to their part of speech tag . Understanding why such words are tagged as they are in each context can help us clarify the distinctions between the tags . most_common ( ) ] ... print ( word , ' ' . Note . Your Turn : Open the POS concordance tool nltk.app.concordance ( ) and load the complete Brown Corpus ( simplified tagset ) . Now pick some of the above words and see how the tag of the word correlates with the context of the word . E.g. search for near to see all forms mixed together , near / ADJ to see it used as an adjective , near N to see just those cases where a noun follows , and so forth . For a larger set of examples , modify the supplied code so that it lists words having three distinct tags . 3 Mapping Words to Properties Using Python Dictionaries . As we have seen , a tagged word of the form ( word , tag ) is an association between a word and a part - of - speech tag . Once we start doing part - of - speech tagging , we will be creating programs that assign a tag to a word , the tag which is most likely in a given context . We can think of this process as mapping from words to tags . The most natural way to store mappings in Python uses the so - called dictionary data type ( also known as an associative array or hash array in other programming languages ) . In this section we look at dictionaries and see how they can represent a variety of language information , including parts of speech . 3.1 Indexing Lists vs Dictionaries . A text , as we have seen , is treated in Python as a list of words . An important property of lists is that we can \\\" look up \\\" a particular item by giving its index , e.g. text1[100 ] . Notice how we specify a number , and get back a word . We can think of a list as a simple kind of table , as shown in 3.1 . Figure 3.1 : List Look - up : we access the contents of a Python list with the help of an integer index . Contrast this situation with frequency distributions ( 3 ) , where we specify a word , and get back a number , e.g. fdist [ ' monstrous ' ] , which tells us the number of times a given word has occurred in a text . Look - up using words is familiar to anyone who has used a dictionary . Some more examples are shown in 3.2 . Figure 3.2 : Dictionary Look - up : we access the entry of a dictionary using a key such as someone 's name , a web domain , or an English word ; other names for dictionary are map , hashmap , hash , and associative array . In the case of a phonebook , we look up an entry using a name , and get back a number . When we type a domain name in a web browser , the computer looks this up to get back an IP address . A word frequency table allows us to look up a word and find its frequency in a text collection . In all these cases , we are mapping from names to numbers , rather than the other way around as with a list . In general , we would like to be able to map between arbitrary types of information . 3.1 lists a variety of linguistic objects , along with what they map . Most often , we are mapping from a \\\" word \\\" to some structured object . For example , a document index maps from a word ( which we can represent as a string ) , to a list of pages ( represented as a list of integers ) . In this section , we will see how to represent such mappings in Python . 3.2 Dictionaries in Python . Python provides a dictionary data type that can be used for mapping between arbitrary types . It is like a conventional dictionary , in that it gives you an efficient way to look things up . However , as we see from 3.1 , it has a much wider range of uses . To illustrate , we define pos to be an empty dictionary and then add four entries to it , specifying the part - of - speech of some words . We add entries to a dictionary using the familiar square bracket notation : . So , for example , says that the part - of - speech of colorless is adjective , or more specifically , that the key ' colorless ' is assigned the value ' ADJ ' in dictionary pos . When we inspect the value of pos we see a set of key - value pairs . Once we have populated the dictionary in this way , we can employ the keys to retrieve values : . Of course , we might accidentally use a key that has n't been assigned a value . KeyError : ' green ' . This raises an important question . Unlike lists and strings , where we can use len ( ) to work out which integers will be legal indexes , how do we work out the legal keys for a dictionary ? If the dictionary is not too big , we can simply inspect its contents by evaluating the variable pos . As we saw above ( line ) , this gives us the key - value pairs . Notice that they are not in the same order they were originally entered ; this is because dictionaries are not sequences but mappings ( cf . 3.2 ) , and the keys are not inherently ordered . Alternatively , to just find the keys , we can convert the dictionary to a list - or use the dictionary in a context where a list is expected , as the parameter of sorted ( ) , or in a for loop . Note . When you type list(pos ) you might see a different order to the one shown above . If you want to see the keys in order , just sort them . As well as iterating over all keys in the dictionary with a for loop , we can use the for loop as we did for printing lists : . Finally , the dictionary methods keys ( ) , values ( ) and items ( ) allow us to access the keys , values , and key - value pairs as separate lists . We can even sort tuples , which orders them according to their first element ( and if the first elements are the same , it uses their second elements ) . We want to be sure that when we look something up in a dictionary , we only get one value for each key . Now suppose we try to use a dictionary to store the fact that the word sleep can be used as both a verb and a noun : . Initially , pos [ ' sleep ' ] is given the value ' V ' . But this is immediately overwritten with the new value ' N ' . In other words , there can only be one entry in the dictionary for ' sleep ' . In fact , this is what we saw in 4 for the CMU Pronouncing Dictionary , which stores multiple pronunciations for a single word . 3.3 Defining Dictionaries . We can use the same key - value pair format to create a dictionary . There 's a couple of ways to do this , and we will normally use the first : . Note that dictionary keys must be immutable types , such as strings and tuples . If we try to define a dictionary using a mutable key , we get a TypeError : . 3.4 Default Dictionaries . If we try to access a key that is not in a dictionary , we get an error . However , its often useful if a dictionary can automatically create an entry for this new key and give it a default value , such as zero or the empty list . For this reason , a special kind of dictionary called a defaultdict is available . In order to use it , we have to supply a parameter which can be used to create the default value , e.g. int , float , str , list , dict , tuple . Note . These default values are actually functions that convert other objects to the specified type ( e.g. int ( \\\" 2 \\\" ) , list ( \\\" 2 \\\" ) ) . When they are called with no parameter - int ( ) , list ( ) - they return 0 and [ ] respectively . The above examples specified the default value of a dictionary entry to be the default value of a particular data type . However , we can specify any default value we like , simply by providing the name of a function that can be called with no arguments to create the required value . Let 's return to our part - of - speech example , and create a dictionary whose default value for any entry is ' N ' . When we access a non - existent entry , it is automatically added to the dictionary . Note . The above example used a lambda expression , introduced in 4.4 . This lambda expression specifies no parameters , so we call it using parentheses with no arguments . Thus , the definitions of f and g below are equivalent : . Let 's see how default dictionaries could be used in a more substantial language processing task . Many language processing tasks - including tagging - struggle to correctly process the hapaxes of a text . They can perform better with a fixed vocabulary and a guarantee that no new words will appear . We can preprocess a text to replace low - frequency words with a special \\\" out of vocabulary \\\" token UNK , with the help of a default dictionary . ( Can you work out how to do this without reading on ? ) We need to create a default dictionary that maps each word to its replacement . The most frequent n words will be mapped to themselves . Everything else will be mapped to UNK . 3.5 Incrementally Updating a Dictionary . We can employ dictionaries to count occurrences , emulating the method for tallying words shown in fig - tally . We begin by initializing an empty defaultdict , then process each part - of - speech tag in the text . If the tag has n't been seen before , it will have a zero count by default . [ ' ADJ ' , ' PRT ' , ' ADV ' , ' X ' , ' CONJ ' , ' PRON ' , ' VERB ' , ' . ' [ ( ' NOUN ' , 30640 ) , ( ' VERB ' , 14399 ) , ( ' ADP ' , 12355 ) , ( ' . ' [ ' NOUN ' , ' VERB ' , ' ADP ' , ' . ' , ' DET ' , ' ADJ ' , ' ADV ' , ' CONJ ' , ' PRON ' , ' PRT ' , ' NUM ' , ' X ' ] . The listing in 3.3 illustrates an important idiom for sorting a dictionary by its values , to show words in decreasing order of frequency . The first parameter of sorted ( ) is the items to sort , a list of tuples consisting of a POS tag and a frequency . The second parameter specifies the sort key using a function itemgetter ( ) . In general , itemgetter(n ) returns a function that can be called on some other sequence object to obtain the n th element , e.g. : . The last parameter of sorted ( ) specifies that the items should be returned in reverse order , i.e. decreasing values of frequency . There 's a second useful programming idiom at the beginning of 3.3 , where we initialize a defaultdict and then use a for loop to update its values . Here 's a schematic version : . ... my_dictionary [ item_key ] is updated with information about item . Here 's another instance of this pattern , where we index words according to their last two letters : . The following example uses the same pattern to create an anagram dictionary . ( You might experiment with the third line to get an idea of why this program works . ) join(sorted(word ) ) ... anagrams[key]. Since accumulating words like this is such a common task , NLTK provides a more convenient way of creating a defaultdict(list ) , in the form of nltk . Index ( ) . Note . nltk . Index is a defaultdict(list ) with extra support for initialization . Similarly , nltk . FreqDist is essentially a defaultdict(int ) with extra support for initialization ( along with sorting and plotting methods ) . 3.6 Complex Keys and Values . We can use default dictionaries with complex keys and values . Let 's study the range of possible tags for a word , given the word itself , and the tag of the previous word . We will see how this information can be used by a POS tagger . This example uses a dictionary whose default value for an entry is a dictionary ( whose default value is int ( ) , i.e. zero ) . Notice how we iterated over the bigrams of the tagged corpus , processing a pair of word - tag pairs for each iteration . Each time through the loop we updated our pos dictionary 's entry for ( t1 , w2 ) , a tag and its following word . When we look up an item in pos we must specify a compound key , and we get back a dictionary object . A POS tagger could use such information to decide that the word right , when preceded by a determiner , should be tagged as ADJ . 3.7 Inverting a Dictionary . Dictionaries support efficient lookup , so long as you want to get the value for any key . If d is a dictionary and k is a key , we type d[k ] and immediately obtain the value . Finding a key given a value is slower and more cumbersome : . If we expect to do this kind of \\\" reverse lookup \\\" often , it helps to construct a dictionary that maps values to keys . In the case that no two keys have the same value , this is an easy thing to do . We just get all the key - value pairs in the dictionary , and create a new dictionary of value - key pairs . The next example also illustrates another way of initializing a dictionary pos with key - value pairs . Let 's first make our part - of - speech dictionary a bit more realistic and add some more words to pos using the dictionary update ( ) method , to create the situation where multiple keys have the same value . Then the technique just shown for reverse lookup will no longer work ( why not ? ) Instead , we have to use append ( ) to accumulate the words for each part - of - speech , as follows : . Now we have inverted the pos dictionary , and can look up any part - of - speech and find all words having that part - of - speech . We can do the same thing even more simply using NLTK 's support for indexing as follows : . 4 Automatic Tagging . In the rest of this chapter we will explore various ways to automatically add part - of - speech tags to text . We will see that the tag of a word depends on the word and its context within a sentence . For this reason , we will be working with data at the level of ( tagged ) sentences rather than words . We 'll begin by loading the data we will be using . 4.1 The Default Tagger . The simplest possible tagger assigns the same tag to each token . This may seem to be a rather banal step , but it establishes an important baseline for tagger performance . In order to get the best result , we tag each word with the most likely tag . Let 's find out which tag is most likely ( now using the unsimplified tagset ) : . max ( ) ' NN ' . Now we can create a tagger that tags everything as NN . , ' NN ' ) ] . Unsurprisingly , this method performs rather poorly . On a typical corpus , it will tag only about an eighth of the tokens correctly , as we see below : . evaluate(brown_tagged_sents ) 0.13089484257215028 . Default taggers assign their tag to every single word , even words that have never been encountered before . As it happens , once we have processed several thousand words of English text , most new words will be nouns . As we will see , this means that default taggers can help to improve the robustness of a language processing system . We will return to them shortly . 4.2 The Regular Expression Tagger . The regular expression tagger assigns tags to tokens on the basis of matching patterns . For instance , we might guess that any word ending in ed is the past participle of a verb , and any word ending with 's is a possessive noun . We can express these as a list of regular expressions : . [ 0 - 9]+ ( . [ 0 - 9]+ ) ? $ ' , ' CD ' ) , # cardinal numbers ... ( r ' . Note that these are processed in order , and the first one that matches is applied . Now we can set up a tagger and use it to tag a sentence . Now its right about a fifth of the time . evaluate(brown_tagged_sents ) 0.20326391789486245 . The final regular expression \\\" . This is equivalent to the default tagger ( only much less efficient ) . Instead of re - specifying this as part of the regular expression tagger , is there a way to combine this tagger with the default tagger ? We will see how to do this shortly . Note . Your Turn : See if you can come up with patterns to improve the performance of the above regular expression tagger . ( Note that 1 describes a way partially automate such work . ) 4.3 The Lookup Tagger . A lot of high - frequency words do not have the NN tag . Let 's find the hundred most frequent words and store their most likely tag . We can then use this information as the model for a \\\" lookup tagger \\\" ( an NLTK UnigramTagger ): . evaluate(brown_tagged_sents ) 0.45578495136941344 . It should come as no surprise by now that simply knowing the tags for the 100 most frequent words enables us to tag a large fraction of tokens correctly ( nearly half in fact ) . Let 's see what it does on some untagged input text : . , ' . ' ) ] Many words have been assigned a tag of None , because they were not among the 100 most frequent words . In these cases we would like to assign the default tag of NN . In other words , we want to use the lookup table first , and if it is unable to assign a tag , then use the default tagger , a process known as backoff ( 5 ) . We do this by specifying one tagger as a parameter to the other , as shown below . Now the lookup tagger will only store word - tag pairs for words other than nouns , and whenever it can not assign a tag to a word it will invoke the default tagger . Let 's put all this together and write a program to create and evaluate lookup taggers having a range of sizes , in 4.1 . def performance ( cfd , wordlist ) : . max ( ) ) for word in wordlist ) . return baseline_tagger . most_common ( ) . pylab.plot(sizes , perfs , ' -bo ' ) . pylab.title ( ' Lookup Tagger Performance with Varying Model Size ' ) . pylab.xlabel ( ' Model Size ' ) . pylab.ylabel ( ' Performance ' ) . pylab.show ( ) . Observe that performance initially increases rapidly as the model size grows , eventually reaching a plateau , when large increases in model size yield little improvement in performance . ( This example used the pylab plotting package , discussed in 4.8 . ) 4.4 Evaluation . In the above examples , you will have noticed an emphasis on accuracy scores . In fact , evaluating the performance of such tools is a central theme in NLP . Recall the processing pipeline in fig - sds ; any errors in the output of one module are greatly multiplied in the downstream modules . We evaluate the performance of a tagger relative to the tags a human expert would assign . Since we do n't usually have access to an expert and impartial human judge , we make do instead with gold standard test data . This is a corpus which has been manually annotated and which is accepted as a standard against which the guesses of an automatic system are assessed . The tagger is regarded as being correct if the tag it guesses for a given word is the same as the gold standard tag . Of course , the humans who designed and carried out the original gold standard annotation were only human . Further analysis might show mistakes in the gold standard , or may eventually lead to a revised tagset and more elaborate guidelines . Nevertheless , the gold standard is by definition \\\" correct \\\" as far as the evaluation of an automatic tagger is concerned . Note . Developing an annotated corpus is a major undertaking . Apart from the data , it generates sophisticated tools , documentation , and practices for ensuring high quality annotation . The tagsets and other coding schemes inevitably depend on some theoretical position that is not shared by all , however corpus creators often go to great lengths to make their work as theory - neutral as possible in order to maximize the usefulness of their work . We will discuss the challenges of creating a corpus in 11 . . . 5 N - Gram Tagging . 5.1 Unigram Tagging . Unigram taggers are based on a simple statistical algorithm : for each token , assign the tag that is most likely for that particular token . For example , it will assign the tag JJ to any occurrence of the word frequent , since frequent is used as an adjective ( e.g. a frequent word ) more often than it is used as a verb ( e.g. I frequent this cafe ) . A unigram tagger behaves just like a lookup tagger ( 4 ) , except there is a more convenient technique for setting it up , called training . In the following code sample , we train a unigram tagger , use it to tag a sentence , then evaluate : . , ' . ' ) ] evaluate(brown_tagged_sents ) 0.9349006503968017 . We train a UnigramTagger by specifying tagged sentence data as a parameter when we initialize the tagger . The training process involves inspecting the tag of each word and storing the most likely tag for any word in a dictionary , stored inside the tagger . 5.2 Separating the Training and Testing Data . Now that we are training a tagger on some data , we must be careful not to test it on the same data , as we did in the above example . A tagger that simply memorized its training data and made no attempt to construct a general model would get a perfect score , but would also be useless for tagging new text . Instead , we should split the data , training on 90 % and testing on the remaining 10 % : . evaluate(test_sents ) 0.811721 ... . Although the score is worse , we now have a better picture of the usefulness of this tagger , i.e. its performance on previously unseen text . 5.3 General N - Gram Tagging . When we perform a language processing task based on unigrams , we are using one item of context . In the case of tagging , we only consider the current token , in isolation from any larger context . Given such a model , the best we can do is tag each word with its a priori most likely tag . This means we would tag a word such as wind with the same tag , regardless of whether it appears in the context the wind or to wind . An n - gram tagger is a generalization of a unigram tagger whose context is the current word together with the part - of - speech tags of the n -1 preceding tokens , as shown in 5.1 . The tag to be chosen , t n , is circled , and the context is shaded in grey . An n - gram tagger picks the tag that is most likely in the given context . A 1-gram tagger is another term for a unigram tagger : i.e. , the context used to tag a token is just the text of the token itself . 2-gram taggers are also called bigram taggers , and 3-gram taggers are called trigram taggers . The NgramTagger class uses a tagged training corpus to determine which part - of - speech tag is most likely for each context . Here we see a special case of an n - gram tagger , namely a bigram tagger . First we train it , then use it to tag untagged sentences : . , ' . ' ) ] , None ) ] . Notice that the bigram tagger manages to tag every word in a sentence it saw during training , but does badly on an unseen sentence . As soon as it encounters a new word ( i.e. , 13.5 ) , it is unable to assign a tag . It can not tag the following word ( i.e. , million ) even if it was seen during training , simply because it never saw it during training with a None tag on the previous word . Consequently , the tagger fails to tag the rest of the sentence . Its overall accuracy score is very low : . evaluate(test_sents ) 0.102063 ... . As n gets larger , the specificity of the contexts increases , as does the chance that the data we wish to tag contains contexts that were not present in the training data . This is known as the sparse data problem , and is quite pervasive in NLP . As a consequence , there is a trade - off between the accuracy and the coverage of our results ( and this is related to the precision / recall trade - off in information retrieval ) . Caution ! n - gram taggers should not consider context that crosses a sentence boundary . Accordingly , NLTK taggers are designed to work with lists of sentences , where each sentence is a list of words . At the start of a sentence , t n-1 and preceding tags are set to None . 5.4 Combining Taggers . One way to address the trade - off between accuracy and coverage is to use the more accurate algorithms when we can , but to fall back on algorithms with wider coverage when necessary . For example , we could combine the results of a bigram tagger , a unigram tagger , and a default tagger , as follows : . Try tagging the token with the bigram tagger . If the bigram tagger is unable to find a tag for the token , try the unigram tagger . If the unigram tagger is also unable to find a tag , use a default tagger . Most NLTK taggers permit a backoff - tagger to be specified . The backoff - tagger may itself have a backoff tagger : . Note . Your Turn : Extend the above example by defining a TrigramTagger called t3 , which backs off to t2 . Note that we specify the backoff tagger when the tagger is initialized so that training can take advantage of the backoff tagger . Thus , if the bigram tagger would assign the same tag as its unigram backoff tagger in a certain context , the bigram tagger discards the training instance . This keeps the bigram tagger model as small as possible . 5.5 Tagging Unknown Words . Our approach to tagging unknown words still uses backoff to a regular - expression tagger or a default tagger . These are unable to make use of context . Thus , if our tagger encountered the word blog , not seen during training , it would assign it the same tag , regardless of whether this word appeared in the context the blog or to blog . How can we do better with these unknown words , or out - of - vocabulary items ? A useful method to tag unknown words based on context is to limit the vocabulary of a tagger to the most frequent n words , and to replace every other word with a special word UNK using the method shown in 3 . During training , a unigram tagger will probably learn that UNK is usually a noun . However , the n - gram taggers will detect contexts in which it has some other tag . For example , if the preceding word is to ( tagged TO ) , then UNK will probably be tagged as a verb . 5.6 Storing Taggers . Training a tagger on a large corpus may take a significant time . Instead of training a tagger every time we need one , it is convenient to save a trained tagger in a file for later re - use . Let 's save our tagger t2 to a file t2.pkl . Now , in a separate Python process , we can load our saved tagger . Now let 's check that it can be used for tagging . , ' . ' ) ] 5.7 Performance Limitations . What is the upper limit to the performance of an n - gram tagger ? Consider the case of a trigram tagger . How many cases of part - of - speech ambiguity does it encounter ? We can determine the answer to this question empirically : . N ( ) for c in ambiguous_contexts ) / cfd . N ( ) 0.049297702068029296 . Thus , one out of twenty trigrams is ambiguous [ EXAMPLES]. Given the current word and the previous two tags , in 5 % of cases there is more than one tag that could be legitimately assigned to the current word according to the training data . Assuming we always pick the most likely tag in such ambiguous contexts , we can derive a lower bound on the performance of a trigram tagger . Another way to investigate the performance of a tagger is to study its mistakes . Some tags may be harder than others to assign , and it might be possible to treat them specially by pre- or post - processing the data . A convenient way to look at tagging errors is the confusion matrix . It charts expected tags ( the gold standard ) against actual tags generated by a tagger : . Based on such analysis we may decide to modify the tagset . Perhaps a distinction between tags that is difficult to make can be dropped , since it is not important in the context of some larger processing task . Another way to analyze the performance bound on a tagger comes from the less than 100 % agreement between human annotators . [ MORE ] . In general , observe that the tagging process collapses distinctions : e.g. lexical identity is usually lost when all personal pronouns are tagged PRP . At the same time , the tagging process introduces new distinctions and removes ambiguities : e.g. deal tagged as VB or NN . This characteristic of collapsing certain distinctions and introducing new distinctions is an important feature of tagging which facilitates classification and prediction . When we introduce finer distinctions in a tagset , an n - gram tagger gets more detailed information about the left - context when it is deciding what tag to assign to a particular word . However , the tagger simultaneously has to do more work to classify the current token , simply because there are more tags to choose from . Conversely , with fewer distinctions ( as with the simplified tagset ) , the tagger has less information about context , and it has a smaller range of choices in classifying the current token . We have seen that ambiguity in the training data leads to an upper limit in tagger performance . Sometimes more context will resolve the ambiguity . In other cases however , as noted by ( Church , Young , & Bloothooft , 1996 ) , the ambiguity can only be resolved with reference to syntax , or to world knowledge . Despite these imperfections , part - of - speech tagging has played a central role in the rise of statistical approaches to natural language processing . In the early 1990s , the surprising accuracy of statistical taggers was a striking demonstration that it was possible to solve one small part of the language understanding problem , namely part - of - speech disambiguation , without reference to deeper sources of linguistic knowledge . Can this idea be pushed further ? In 7 . , we shall see that it can . 6 Transformation - Based Tagging . A potential issue with n - gram taggers is the size of their n - gram table ( or language model ) . If tagging is to be employed in a variety of language technologies deployed on mobile computing devices , it is important to strike a balance between model size and tagger performance . An n - gram tagger with backoff may store trigram and bigram tables , large sparse arrays which may have hundreds of millions of entries . A second issue concerns context . The only information an n - gram tagger considers from prior context is tags , even though words themselves might be a useful source of information . It is simply impractical for n - gram models to be conditioned on the identities of words in the context . In this section we examine Brill tagging , an inductive tagging method which performs very well using models that are only a tiny fraction of the size of n - gram taggers . Brill tagging is a kind of transformation - based learning , named after its inventor . The general idea is very simple : guess the tag of each word , then go back and fix the mistakes . In this way , a Brill tagger successively transforms a bad tagging of a text into a better one . As with n - gram tagging , this is a supervised learning method , since we need annotated training data to figure out whether the tagger 's guess is a mistake or not . However , unlike n - gram tagging , it does not count observations but compiles a list of transformational correction rules . The process of Brill tagging is usually explained by analogy with painting . Suppose we were painting a tree , with all its details of boughs , branches , twigs and leaves , against a uniform sky - blue background . Instead of painting the tree first then trying to paint blue in the gaps , it is simpler to paint the whole canvas blue , then \\\" correct \\\" the tree section by over - painting the blue background . In the same fashion we might paint the trunk a uniform brown before going back to over - paint further details with even finer brushes . Brill tagging uses the same idea : begin with broad brush strokes then fix up the details , with successively finer changes . Let 's look at an example involving the following sentence : . We will examine the operation of two rules : ( a ) Replace NN with VB when the previous word is TO ; ( b ) Replace TO with IN when the next tag is NNS . 6.1 illustrates this process , first tagging with the unigram tagger , then applying the rules to fix the errors . In this table we see two rules . All such rules are generated from a template of the following form : \\\" replace T 1 with T 2 in the context C \\\" . Typical contexts are the identity or the tag of the preceding or following word , or the appearance of a specific tag within 2 - 3 words of the current word . During its training phase , the tagger guesses values for T 1 , T 2 and C , to create thousands of candidate rules . Each rule is scored according to its net benefit : the number of incorrect tags that it corrects , less the number of correct tags it incorrectly modifies . Brill taggers have another interesting property : the rules are linguistically interpretable . Compare this with the n - gram taggers , which employ a potentially massive table of n - grams . We can not learn much from direct inspection of such a table , in comparison to the rules learned by the Brill tagger . 6.1 demonstrates NLTK 's Brill tagger . Training Brill tagger on 80 sentences ... . Finding initial useful rules ... . Found 6555 useful rules . read ( ) ) . Example 6.1 ( code_brill_demo . 7 How to Determine the Category of a Word . Now that we have examined word classes in detail , we turn to a more basic question : how do we decide what category a word belongs to in the first place ? In general , linguists use morphological , syntactic , and semantic clues to determine the category of a word . 7.1 Morphological Clues . The internal structure of a word may give useful clues as to the word 's category . So if we encounter a word that ends in -ness , this is very likely to be a noun . English verbs can also be morphologically complex . For instance , the present participle of a verb ends in -ing , and expresses the idea of ongoing , incomplete action ( e.g. falling , eating ) . The -ing suffix also appears on nouns derived from verbs , e.g. the falling of the leaves ( this is known as the gerund ) . 7.2 Syntactic Clues . Another source of information is the typical contexts in which a word can occur . For example , assume that we have already determined the category of nouns . Then we might say that a syntactic criterion for an adjective in English is that it can occur immediately before a noun , or immediately following the words be or very . According to these tests , near should be categorized as an adjective : . 7.3 Semantic Clues . Finally , the meaning of a word is a useful clue as to its lexical category . For example , the best - known definition of a noun is semantic : \\\" the name of a person , place or thing \\\" . Within modern linguistics , semantic criteria for word classes are treated with suspicion , mainly because they are hard to formalize . Nevertheless , semantic criteria underpin many of our intuitions about word classes , and enable us to make a good guess about the categorization of words in languages that we are unfamiliar with . For example , if all we know about the Dutch word verjaardag is that it means the same as the English word birthday , then we can guess that verjaardag is a noun in Dutch . However , some care is needed : although we might translate zij is vandaag jarig as it 's her birthday today , the word jarig is in fact an adjective in Dutch , and has no exact equivalent in English . 7.4 New Words . All languages acquire new lexical items . A list of words recently added to the Oxford Dictionary of English includes cyberslacker , fatoush , blamestorm , SARS , cantopop , bupkis , noughties , muggle , and robata . Notice that all these new words are nouns , and this is reflected in calling nouns an open class . By contrast , prepositions are regarded as a closed class . 7.5 Morphology in Part of Speech Tagsets . Common tagsets often capture some morpho - syntactic information ; that is , information about the kind of morphological markings that words receive by virtue of their syntactic role . Consider , for example , the selection of distinct grammatical forms of the word go illustrated in the following sentences : . Each of these forms - go , goes , gone , and went - is morphologically distinct from the others . Consider the form , goes . This occurs in a restricted set of grammatical contexts , and requires a third person singular subject . Thus , the following sentences are ungrammatical . We can easily imagine a tagset in which the four distinct grammatical forms just discussed were all tagged as VB . Although this would be adequate for some purposes , a more fine - grained tagset provides useful information about these forms that can help other processors that try to detect patterns in tag sequences . The Brown tagset captures these distinctions , as summarized in 7.1 . In addition to this set of verb tags , the various forms of the verb to be have special tags : be / BE , being / BEG , am / BEM , are / BER , is /BEZ , been / BEN , were / BED and was / BEDZ ( plus extra tags for negative forms of the verb ) . All told , this fine - grained tagging of verbs means that an automatic tagger that uses this tagset is effectively carrying out a limited amount of morphological analysis . Most part - of - speech tagsets make use of the same basic categories , such as noun , verb , adjective , and preposition . However , tagsets differ both in how finely they divide words into categories , and in how they define their categories . For example , is might be tagged simply as a verb in one tagset ; but as a distinct form of the lexeme be in another tagset ( as in the Brown Corpus ) . This variation in tagsets is unavoidable , since part - of - speech tags are used in different ways for different tasks . In other words , there is no one ' right way ' to assign tags , only more or less useful ways depending on one 's goals . 8 Summary . Words can be grouped into classes , such as nouns , verbs , adjectives , and adverbs . These classes are known as lexical categories or parts of speech . Parts of speech are assigned short labels , or tags , such as NN , VB , . The process of automatically assigning parts of speech to words in text is called part - of - speech tagging , POS tagging , or just tagging . Automatic tagging is an important step in the NLP pipeline , and is useful in a variety of situations including : predicting the behavior of previously unseen words , analyzing word usage in corpora , and text - to - speech systems . Some linguistic corpora , such as the Brown Corpus , have been POS tagged . A variety of tagging methods are possible , e.g. default tagger , regular expression tagger , unigram tagger and n - gram taggers . These can be combined using a technique known as backoff . Taggers can be trained and evaluated using tagged corpora . Backoff is a method for combining models : when a more specialized model ( such as a bigram tagger ) can not assign a tag in a given context , we backoff to a more general model ( such as a unigram tagger ) . Part - of - speech tagging is an important , early example of a sequence classification task in NLP : a classification decision at any one point in the sequence makes use of words and tags in the local context . N - gram taggers can be defined for large values of n , but once n is larger than 3 we usually encounter the sparse data problem ; even with a large quantity of training data we only see a tiny fraction of possible contexts . Transformation - based tagging involves learning a series of repair rules of the form \\\" change tag s to tag t in context c \\\" , where each rule fixes mistakes and possibly introduces a ( smaller ) number of errors . 9 Further Reading . Chapters 4 and 5 of ( Jurafsky & Martin , 2008 ) contain more advanced material on n - grams and part - of - speech tagging . The \\\" Universal Tagset \\\" is described by ( Petrov , Das , & McDonald , 2012 ) . Other approaches to tagging involve machine learning methods ( chap - data - intensive ) . In 7 . we will see a generalization of tagging called chunking in which a contiguous sequence of words is assigned a single tag . For tagset documentation , see nltk.help.upenn_tagset ( ) and nltk.help.brown_tagset ( ) . Lexical categories are introduced in linguistics textbooks , including those listed in 1 . . . There are many other kinds of tagging . Words can be tagged with directives to a speech synthesizer , indicating which words should be emphasized . Words can be tagged with sense numbers , indicating which sense of the word was used . Words can also be tagged with morphological features . Examples of each of these kinds of tags are shown below . For space reasons , we only show the tag for a single word . Note also that the first two examples use XML - style tags , where elements in angle brackets enclose the word that is tagged . ( Wordnet form / nn sense 4 : \\\" shape , form , configuration , contour , conformation \\\" ) . Morphological tagging , from the Turin University Italian Treebank : E ' italiano , come progetto e realizzazione , il primo ( PRIMO ADJ ORDIN M SING ) porto turistico dell ' Albania . Note that tagging is also performed at higher levels . Here is an example of dialogue act tagging , from the NPS Chat Corpus ( Forsyth & Martell , 2007 ) included with NLTK . Each turn of the dialogue is categorized as to its communicative function : . Statement User117 Dude ... , I wanted some of that ynQuestion User120 m I missing something ? Bye User117 I 'm gon na go fix food , I 'll be back later . System User122 JOIN System User2 slaps User122 around a bit with a large trout . Statement User121 18/m pm me if u tryin to chat . 10 Exercises . Manually tag these headlines to see if knowledge of the part - of - speech tags removes the ambiguity . What different pronunciations and parts of speech are involved ? Discuss any other examples of mappings you can think of . What type of information do they map from and to ? Create a dictionary d , and add some entries . What happens if you try to access a non - existent entry , e.g. d [ ' xyz ' ] ? Check that the item was deleted . Now issue the command d1.update(d2 ) . What did this do ? What might it be useful for ? Define keys like headword , part - of - speech , sense , and example , and assign them suitable values . Observe that some words are not assigned a tag . Why not ? Train an affix tagger and run it on some new text . Experiment with different settings for the affix length and the minimum word length . Discuss your findings . Next , run it on some new data . What happens to the performance of the tagger ? Why ? Which nouns are more common in their plural form , rather than their singular form ? ( Only consider regular plurals , formed with the -s suffix . ) Which word has the greatest number of distinct tags . What are they , and what do they represent ? List tags in order of decreasing frequency . What do the 20 most frequent tags represent ? Which tags are nouns most commonly found after ? What do these tags represent ? What happens to the tagger performance for the various model sizes when a backoff tagger is omitted ? Consider the curve in 4.2 ; suggest a good size for a lookup tagger that balances memory and performance . Can you come up with scenarios where it would be preferable to minimize memory usage , or to maximize performance with no regard for memory usage ? ( Hint : write a program to work out what percentage of tokens of a word are assigned the most likely tag for that word , on average . ) What proportion of word types are always assigned the same part - of - speech tag ? How many words are ambiguous , in the sense that they appear with at least two tags ? What percentage of word tokens in the Brown Corpus involve these ambiguous words ? Let 's try to figure out how the evaluation method works : . A tagger t takes a list of words as input , and produces a list of tagged words as output . However , t.evaluate ( ) is given correctly tagged text as its only parameter . What must it do with this input before performing the tagging ? Once the tagger has created newly tagged text , how might the evaluate ( ) method go about comparing it with the original tagged text and computing the accuracy score ? Now examine the source code to see how the method is implemented . Inspect nltk.tag.api . _ _ file _ _ to discover the location of the source code , and open this file using an editor ( be sure to use the api.py file and not the compiled api.pyc binary file ) . Produce an alphabetically sorted list of the distinct words tagged as MD . Identify words that can be plural nouns or third person singular verbs ( e.g. deals , flies ) . Identify three - word prepositional phrases of the form IN + DET + NN ( eg . in the lab ) . What is the ratio of masculine to feminine pronouns ? Investigate the full range of adverbs that appear before these four verbs . This tagger only checks for cardinal numbers . By testing for particular prefix or suffix strings , it should be possible to guess other tags . For example , we could tag any word that ends with -s as a plural noun . Define a regular expression tagger ( using RegexpTagger ( ) ) that tests for at least five other patterns in the spelling of words . ( Use inline documentation to explain the rules . ) Evaluate the tagger using its accuracy ( ) method , and try to come up with ways to improve its performance . Discuss your findings . How does objective evaluation help in the development process ? Investigate the performance of n - gram taggers as n increases from 1 to 6 . Tabulate the accuracy score . Estimate the training data required for these taggers , assuming a vocabulary size of 10 5 and a tagset size of 10 2 . If the language is morphologically complex , or if there are any orthographic clues ( e.g. capitalization ) to word classes , consider developing a regular expression tagger for it ( ordered after the unigram tagger , and before the default tagger ) . How does the accuracy of your tagger(s ) compare with the same taggers run on English data ? Discuss any issues you encounter in applying these methods to the language . Plot the performance curve for a unigram tagger , as the amount of training data is varied . Define a dictionary to do the mapping , and evaluate the tagger on the simplified data . Such a tagger has fewer distinctions to make , but much less information on which to base its work . Discuss your findings . It is possible for a bigram tagger to fail part way through a sentence even if it contains no unseen words ( even if the sentence was used during training ) . In what circumstance can this happen ? Can you write a program to find some examples of this ? Now train and evaluate a bigram tagger on this data . How much does this help ? What is the contribution of the unigram tagger and default tagger now ? What do you notice about the shape of the resulting plot ? Does the gradient tell you anything ? Experiment with the tagger by setting different values for the parameters . Is there any trade - off between training time ( corpus size ) and performance ? Print a table with the integers 1 . .10 in one column , and the number of distinct words in the corpus having 1 . .10 distinct tags in the other column . For the word with the greatest number of distinct tags , print out sentences from the corpus containing the word , one for each possible tag . Can this be used to discriminate between the epistemic and deontic uses of must ? Create three different combinations of the taggers . Test the accuracy of each combined tagger . Which combination works best ? Try varying the size of the training corpus . How does it affect your results ? These methods will not do well for texts having new words that are not nouns . Consider the sentence I like to blog on Kim 's blog . If blog is a new word , then looking at the previous tag ( TO versus NP$ ) would probably be helpful . I.e. we need a default tagger that is sensitive to the preceding tag . Create a new kind of unigram tagger that looks at the tag of the previous word , and ignores the current word . ( The best way to do this is to modify the source code for UnigramTagger ( ) , which presumes knowledge of object - oriented programming in Python . ) Add this tagger to the sequence of backoff taggers ( including ordinary trigram and bigram taggers that look at words ) , right before the usual default tagger . Evaluate the contribution of this new unigram tagger . Review Abney 's discussion concerning the impossibility of exact tagging ( Church , Young , & Bloothooft , 1996 ) . Explain why correct tagging of these examples requires access to other kinds of information than just words and tags . How might you estimate the scale of this problem ? Delete some of the rule templates , based on what you learned from inspecting rules.out . Add some new rule templates which employ contexts that might help to correct the errors you saw in errors.out . Compare their relative performance and discuss which method is the most legitimate . ( You might use n - fold cross validation , discussed in 3 , to improve the accuracy of the evaluations . ) Make sure that the unigram and default backoff taggers have access to the full vocabulary . \"}",
        "_version_":1692669029431377920,
        "score":28.213144},
      {
        "id":"54d25c03-e458-4702-9871-2935f55db0fc",
        "_src_":"{\"url\": \"https://kb.osu.edu/dspace/handle/1811/346\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166570.91/warc/CC-MAIN-20160205193926-00022-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Learning to Classify Text . Detecting patterns is a central part of Natural Language Processing . Words ending in -ed tend to be past tense verbs ( 5 . ) Frequent use of will is indicative of news text ( 3 ) . These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic . But how did we know where to start looking , which aspects of form to associate with which aspects of meaning ? The goal of this chapter is to answer the following questions : . How can we identify particular features of language data that are salient for classifying it ? How can we construct models of language that can be used to perform language processing tasks automatically ? What can we learn about language from these models ? Along the way we will study some important machine learning techniques , including decision trees , naive Bayes ' classifiers , and maximum entropy classifiers . We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) . Before looking at these methods , we first need to appreciate the broad scope of this topic . 1 Supervised Classification . Classification is the task of choosing the correct class label for a given input . In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance . Some examples of classification tasks are : . Deciding whether an email is spam or not . Deciding what the topic of a news article is , from a fixed list of topic areas such as \\\" sports , \\\" \\\" technology , \\\" and \\\" politics . \\\" Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution . The basic classification task has a number of interesting variants . For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified . A classifier is called supervised if it is built based on training corpora containing the correct label for each input . The framework used by supervised classification is shown in 1.1 . Figure 1.1 : Supervised Classification . ( a ) During training , a feature extractor is used to convert each input value to a feature set . These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section . Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model . ( b ) During prediction , the same feature extractor is used to convert unseen inputs to feature sets . These feature sets are then fed into the model , which generates predicted labels . In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks . Our discussion is not intended to be comprehensive , but to give a representative sample of tasks that can be performed with the help of text classifiers . 1.1 Gender Identification . In 4 we saw that male and female names have some distinctive characteristics . Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male . Let 's build a classifier to model these differences more precisely . The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features . For this example , we 'll start by just looking at the final letter of a given name . The following feature extractor function builds a dictionary containing relevant information about a given name : . The returned dictionary , known as a feature set , maps from feature names to their values . Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' . Feature values are values with simple types , such as booleans , numbers , and strings . Note . Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings . But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute . Indeed , it is even possible to use very complex and informative values , such as the output of a second supervised classifier , as features . Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels . Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set . The training set is used to train a new \\\" naive Bayes \\\" classifier . We will learn more about the naive Bayes classifier later in the chapter . For now , let 's just test it out on some names that did not appear in its training data : . Observe that these character names from The Matrix are correctly classified . Although this science fiction movie is set in 2199 , it still conforms with our expectations about names and genders . We can systematically evaluate the classifier on a much larger quantity of unseen data : . Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : . This listing shows that the names in the training set that end in \\\" a \\\" are female 33 times more often than they are male , but names that end in \\\" k \\\" are male 32 times more often than they are female . These ratios are known as likelihood ratios , and can be useful for comparing different feature - outcome relationships . Note . Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative . Retrain the classifier with these new features , and test its accuracy . When working with large corpora , constructing a single list that contains the features of every instance can use up a large amount of memory . In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2 Choosing The Right Features . Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model . Much of the interesting work in building a classifier is deciding what features might be relevant , and how we can represent them . Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand . Typically , feature extractors are built through a process of trial - and - error , guided by intuitions about what information is relevant to the problem . It 's common to start with a \\\" kitchen sink \\\" approach , including all the features that you can think of , and then checking to see which features actually are helpful . We take this approach for name gender features in 1.2 . def gender_features2 ( name ) : . lower ( ) . lower ( ) for letter in ' abcdefghijklmnopqrstuvwxyz ' : . count(letter ) . return features . Example 1.2 ( code_gender_features_overfitting . py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features . The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus . This problem is known as overfitting , and can be especially problematic when working with small training sets . Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis . First , we select a development set , containing the corpus data for creating the model . This development set is then subdivided into the training set and the dev - test set . The training set is used to train the model , and the dev - test set is used to perform error analysis . The test set serves in our final evaluation of the system . For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set . The division of the corpus data into different subsets is shown in 1.3 . Figure 1.3 : Organization of corpus data for training supervised classifiers . The corpus data is divided into two sets : the development set , and the test set . The development set is often further subdivided into a training set and a dev - test set . Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set . Using the dev - test set , we can generate a list of the errors that the classifier makes when predicting name genders : . We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) . The feature set can then be adjusted accordingly . The names classifier that we have built generates about 100 errors on the dev - test corpus : . Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders . For example , names ending in yn appear to be predominantly female , despite the fact that names ending in n tend to be male ; and names ending in ch are usually male , even though names that end in h tend to be female . We therefore adjust our feature extractor to include features for two - letter suffixes : . Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : . This error analysis procedure can then be repeated , checking for patterns in the errors that are made by the newly improved classifier . Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set . But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data . It is therefore important to keep the test set separate , and unused , until our model development is complete . At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification . In 1 , we saw several examples of corpora where documents have been labeled with categories . Using these corpora , we can build classifiers that will automatically tag new documents with appropriate category labels . First , we construct a list of documents , labeled with the appropriate categories . For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative . words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews . Next , we define a feature extractor for documents , so the classifier will know which aspects of the data it should pay attention to ( 1.4 ) . For document topic identification , we can define a feature for each word , indicating whether the document contains that word . To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus . We can then define a feature extractor that simply checks whether each of these words is present in a given document . words ( ) ) . return features . words ( ' pos / cv957_8737 . The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) . Now that we 've defined our feature extractor , we can use it to train a classifier to label new movie reviews ( 1.5 ) . To check how reliable the resulting classifier is , we compute its accuracy on the test set . And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging . In 5 . we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word . However , this regular expression tagger had to be hand - crafted . Instead , we can train a classifier to work out which suffixes are most informative . Let 's begin by finding out what the most common suffixes are : . Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features . Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties . The classifier will rely exclusively on these highlighted properties when determining how to label inputs . In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has . Now that we 've defined our feature extractor , we can use it to train a new \\\" decision tree \\\" classifier ( to be discussed in 4 ): . One nice feature of decision tree models is that they are often fairly easy to interpret - we can even instruct NLTK to print them out as pseudocode : . if endswith ( . ) Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \\\" , \\\" . Next , the classifier checks if the word ends in \\\" the \\\" , in which case it 's almost certainly a determiner . This \\\" suffix \\\" gets used early by the decision tree because the word \\\" the \\\" is so common . Continuing on , the classifier checks if the word ends in \\\" s \\\" . 1.5 Exploiting Context . By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix . However , as long as the feature extractor just looks at the target word , we have no way to add features that depend on the context that the word appears in . But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \\\" fly , \\\" knowing that the previous word is \\\" a \\\" will allow us to determine that it is functioning as a noun , not a verb . In order to accommodate features that depend on a word 's context , we must revise the pattern that we used to define our feature extractor . Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word . This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features . Example 1.6 ( code_suffix_pos_tag . py ) : Figure 1.6 : A part - of - speech classifier whose feature detector examines the context in which a word appears in order to determine which part of speech tag should be assigned . In particular , the identity of the previous word is included as a feature . It is clear that exploiting contextual features improves the performance of our part - of - speech tagger . For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \\\" large \\\" or the word \\\" gubernatorial \\\" . However , it is unable to learn the generalization that a word is probably a noun if it follows an adjective , because it does n't have access to the previous word 's part - of - speech tag . In general , simple classifiers always treat each input as independent from all other inputs . In many contexts , this makes perfect sense . For example , decisions about whether names tend to be male or female can be made on a case - by - case basis . However , there are often cases , such as part - of - speech tagging , where we are interested in solving classification problems that are closely related to one another . 1.6 Sequence Classification . In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs . In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence . One sequence classification strategy , known as consecutive classification or greedy sequence classification , is to find the most likely class label for the first input , then to use that answer to help find the best label for the next input . The process can then be repeated until all of the inputs have been labeled . This strategy is demonstrated in 1.7 . First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far . Each tag in history corresponds with a word in sentence . But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word . Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) . Having defined a feature extractor , we can proceed to build our sequence classifier . During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk . TaggerI ) : def _ _ init _ _ ( self , train_sents ) : . train_set . append ( ( featureset , tag ) ) . history.append(tag ) . history.append(tag ) . return zip(sentence , history ) . 1.7 Other Methods for Sequence Classification . One shortcoming of this approach is that we commit to every decision that we make . For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake . One solution to this problem is to adopt a transformational strategy instead . Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs . The Brill tagger , described in ( 1 ) , is a good example of this strategy . Another solution is to assign scores to all of the possible sequences of part - of - speech tags , and to choose the sequence whose overall score is highest . This is the approach taken by Hidden Markov Models . Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags . However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags . These probabilities are then combined to calculate probability scores for tag sequences , and the tag sequence with the highest probability is chosen . Unfortunately , the number of possible tag sequences is quite large . Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence . In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) . Given that restriction , it is possible to use dynamic programming ( 4.7 ) to efficiently find the most likely tag sequence . In particular , for each consecutive word index i , a score is computed for each possible current and previous tag . This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences . 2 Further Examples of Supervised Classification . 2.1 Sentence Segmentation . Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence . The first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features : . Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens . Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' : tokens[i-1]. Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : . To use this classifier to perform sentence segmentation , we simply check each punctuation mark to see whether it 's labeled as a boundary ; and divide the list of words at the boundary marks . The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) . return sents . 2.2 Identifying Dialogue Act Types . When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker . This interpretation is most straightforward for performative statements such as \\\" I forgive you \\\" or \\\" I bet you ca n't climb that hill . \\\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions . Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation . The NPS Chat Corpus , which was demonstrated in 1 , consists of over 10,000 posts from instant messaging sessions . These posts have all been labeled with one of 15 dialogue act types , such as \\\" Statement , \\\" \\\" Emotion , \\\" \\\" ynQuestion \\\" , and \\\" Continuer . \\\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts . The first step is to extract the basic messaging data . We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : . Next , we 'll define a simple feature extractor that checks what words the post contains : . Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment . Recognizing textual entailment ( RTE ) is the task of determining whether a given piece of text T entails another text called the \\\" hypothesis \\\" ( as already discussed in 5 ) . To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams . Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset . The label True indicates that the entailment holds , and False , that it fails to hold . Challenge 3 , Pair 34 ( True ) . T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism . H : China is a member of SCO . Challenge 3 , Pair 81 ( False ) . T : According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart . H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory . It should be emphasized that the relationship between text and hypothesis is not intended to be logical entailment , but rather whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true . We can treat RTE as a classification task , in which we try to predict the True / False label for each pair . Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level . In the ideal case , we would expect that if there is an entailment , then all the information expressed by the hypothesis should also be present in the text . Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment . Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) . In addition , some high frequency function words are filtered out as \\\" stopwords \\\" . def rte_features ( rtepair ) : . return features . Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \\\" Recognizing Text Entailment \\\" Feature Extractor . The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference . To illustrate the content of these features , we examine some attributes of the text / hypothesis Pair 34 shown earlier : . These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True . The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these . Although this figure is not very impressive , it requires significant effort , and more linguistic processing , to achieve much better results . 2.4 Scaling Up to Large Datasets . Python provides an excellent environment for performing basic text processing and feature extraction . If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages . Once these packages have been installed , NLTK can transparently invoke them ( via system calls ) to train classifier models significantly faster than the pure - Python classifier implementations . See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK . 3 Evaluation . In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model . The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it . Evaluation can also be an effective tool for guiding us in making future improvements to the model . 3.1 The Test Set . Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs . This test set typically has the same format as the training set . When building the test set , there is often a trade - off between the amount of data available for testing and the amount available for training . For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances . But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times . Additionally , if the test set contains many closely related instances - such as instances drawn from a single document - then the size of the test set should be increased to ensure that this lack of diversity does not skew the evaluation results . When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation . Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set . The more similar these two datasets are , the less confident we can be that evaluation results will generalize to other datasets . For example , consider the part - of - speech tagging task . At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : . In this case , our test set will be very similar to our training set . The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres . What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training . If there is any consistent pattern within a document - say , if a given word appears with a particular part - of - speech tag especially frequently - then that difference will be reflected in both the development set and the test set . A somewhat better approach is to ensure that the training set and test set are taken from different documents : . If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : . If we build a classifier that performs well on this test set , then we can be confident that it has the power to generalize well beyond the data that it was trained on . 3.2 Accuracy . The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled . The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 . When interpreting the accuracy score of a classifier , it is important to take into consideration the frequencies of the individual class labels in the test set . For example , consider a classifier that determines the correct word sense for each occurrence of the word bank . If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 . In that case , an accuracy of 95 % would hardly be impressive , since we could achieve that accuracy with a model that always returns the financial - institution sense . However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result . ( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall . Another instance where accuracy scores can be misleading is in \\\" search \\\" tasks , such as information retrieval , where we are attempting to find documents that are relevant to a particular task . Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % . It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : . True positives are relevant items that we correctly identified as relevant . True negatives are irrelevant items that we correctly identified as irrelevant . False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant . False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant . Given these four numbers , we can define the following metrics : . Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) . Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) . The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \\u00d7 Precision \\u00d7 Recall ) / ( Precision + Recall ) . 3.4 Confusion Matrices . When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made . A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i . In the following example , we generate a confusion matrix for the bigram tagger developed in 4 : . The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) . Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets . XXX explain use of \\\" reference \\\" in the legend above . 3.5 Cross - Validation . In order to evaluate our models , we must reserve a portion of the annotated data for the test set . As we already mentioned , if the test set is too small , then our evaluation may not be accurate . However , making the test set larger usually means making the training set smaller , which can have a significant impact on performance if a limited amount of annotated data is available . One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation . In particular , we subdivide the original corpus into N subsets called folds . For each of these folds , we train a model using all of the data except the data in that fold , and then test that model on the fold . Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable . A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets . If we get very similar scores for all N training sets , then we can be fairly confident that the score is accurate . On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score . 4 Decision Trees . In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers . As we 've seen , it 's possible to treat these learning methods as black boxes , simply training models and using them for prediction without understanding how they work . But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set . An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded . And an understanding of the generated models can allow us to extract information about which features are most informative , and how those features relate to one another . A decision tree is a simple flowchart that selects labels for input values . This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels . To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node . This node contains a condition that checks one of the input value 's features , and selects a branch based on that feature 's value . Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features . We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value . 4.1 shows an example decision tree model for the name gender task . Once we have a decision tree , it is straightforward to use it to assign labels to new input values . What 's less straightforward is how we can build a decision tree that models a given training set . But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \\\" decision stump \\\" for a corpus . A decision stump is a decision tree with a single node that decides how to classify inputs based on a single feature . It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value . In order to build a decision stump , we must first decide which feature should be used . The simplest method is to just build a decision stump for each possible feature , and see which one achieves the highest accuracy on the training data , although there are other alternatives that we will discuss below . Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) . Given the algorithm for choosing decision stumps , the algorithm for growing larger decision trees is straightforward . We begin by selecting the overall best decision stump for the classification task . We then check the accuracy of each of the leaves on the training set . Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf . 4.1 Entropy and Information Gain . As was mentioned before , there are several methods for identifying the most informative feature for a decision stump . One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature . To measure how disorganized the original set of input values are , we calculate entropy of their labels , which will be high if the input values have highly varied labels , and low if many input values all have the same label . In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : . Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male . For example , 4.2 shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names . Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low . In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2 P(l ) is small ) . On the other hand , if the input values have a wide variety of labels , then there are many labels with a \\\" medium \\\" frequency , where neither P(l ) nor log 2 P(l ) is small , so the entropy is high . 4.3 demonstrates how to calculate the entropy of a list of labels . import math def entropy ( labels ) : . Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump . To do so , we calculate the entropy for each of the decision stump 's leaves , and take the average of those leaf entropy values ( weighted by the number of samples in each leaf ) . The information gain is then equal to the original entropy minus this new , reduced entropy . The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain . Another consideration for decision trees is efficiency . The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree . A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples . Decision trees have a number of useful qualities . To begin with , they 're simple to understand , and easy to interpret . This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features . Decision trees are especially well suited to cases where many hierarchical categorical distinctions can be made . For example , decision trees can be very effective at capturing phylogeny trees . However , decision trees also have a few disadvantages . One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small . As a result , these lower decision nodes may . overfit the training set , learning patterns that reflect idiosyncrasies of the training set rather than linguistically significant patterns in the underlying problem . One solution to this problem is to stop dividing nodes once the amount of training data becomes too small . Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test . A second problem with decision trees is that they force features to be checked in a specific order , even when features may act relatively independently of one another . For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are . Since there is limited space near the top of the decision tree , most of these features will need to be repeated on many different branches in the tree . And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large . A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label . Since these features make relatively small incremental improvements , they tend to occur very low in the decision tree . But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have . If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label . The fact that decision trees require that features be checked in a specific order limits their ability to exploit features that are relatively independent of one another . The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \\\" in parallel . \\\" 5 Naive Bayes Classifiers . In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value . To choose a label for an input value , the naive Bayes classifier begins by calculating the prior probability of each label , which is determined by checking frequency of each label in the training set . The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label . The label whose likelihood estimate is the highest is then assigned to the input value . 5.1 illustrates this process . Figure 5.1 : An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document . In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \\\" automotive \\\" label . But it then considers the effect of each feature . In this example , the input document contains the word \\\" dark , \\\" which is a weak indicator for murder mysteries , but it also contains the word \\\" football , \\\" which is a strong indicator for sports documents . After every feature has made its contribution , the classifier checks which label it is closest to , and assigns that label to the input . Individual features make their contribution to the overall decision by \\\" voting against \\\" labels that do n't occur with that feature very often . In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature . The overall effect will be to reduce the score of the murder mystery label slightly more than the score of the sports label , and to significantly reduce the automotive label with respect to the other two labels . This process is illustrated in 5.2 and 5.3 . Figure 5.2 : Calculating label likelihoods with naive Bayes . Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data . Every feature then contributes to the likelihood estimate for each label , by multiplying it by the probability that input values with that label will have that feature . The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent . 5.1 Underlying Probabilistic Model . Of course , this assumption is unrealistic ; features are often highly dependent on one another . We 'll return to some of the consequences of this assumption at the end of this section . This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another . Figure 5.3 : A Bayesian Network Graph illustrating the generative process that is assumed by the naive Bayes classifier . To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label . Every feature is assumed to be entirely independent of every other feature , given the label . Note . If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing . However , this simple approach can become problematic when a feature never occurs with a given label in the training set . Thus , the input will never be assigned this label , regardless of how well the other features fit the label . In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur . For example , we may not have seen any murder mystery documents that contained the word \\\" football , \\\" but we would n't want to conclude that it 's completely impossible for such documents to exist . For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities . The nltk.probability module provides support for a wide variety of smoothing techniques . 5.3 Non - Binary Features . We have assumed here that each feature is binary , i.e. that each input either has a feature or does not . Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \\\" color - is - red \\\" . Another alternative is to use regression methods to model the probabilities of numeric features . 5.4 The Naivete of Independence . The reason that naive Bayes classifiers are called \\\" naive \\\" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) . In particular , almost all real - world problems contain features with varying degrees of dependence on one another . If we had to avoid any features that were dependent on one another , it would be very difficult to construct good feature sets that provide the required information to the machine learning algorithm . So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ? One problem that arises is that the classifier can end up \\\" double - counting \\\" the effect of highly correlated features , pushing the classifier closer to a given label than is justified . To see how this can occur , consider a name gender classifier that contains two identical features , f 1 and f 2 . In other words , f 2 is an exact copy of f 1 , and contains no new information . When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose . Thus , the information content of these two features will be given more weight than it deserves . Of course , we do n't usually build naive Bayes classifiers that contain two identical features . However , we do build classifiers that contain features which are dependent on one another . For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature . For features like these , the duplicated information may be given more weight than is justified by the training set . 5.5 The Cause of Double - Counting . The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined . One solution , therefore , is to consider the possible interactions between feature contributions during training . We could then use those interactions to adjust the contributions that individual features make . To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : . Here , w[label ] is the \\\" starting score \\\" for a given label , and w[f , label ] is the contribution made by a given feature towards a label 's likelihood . We call these values w[label ] and w[f , label ] the parameters or weights for the model . Using the naive Bayes algorithm , we set each of these parameters independently : . However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values . 6 Maximum Entropy Classifiers . The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier . But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier . In particular , it looks for the set of parameters that maximizes the total likelihood of the training corpus , which is defined as : . Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set . Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution . These iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values , but do not necessarily provide a means of determining when those optimal values have been reached . Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn . This is especially true when the size of the training set , the number of features , and the number of labels are all large . Note . Some iterative optimization techniques are much faster than others . When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods . 6.1 The Maximum Entropy Model . The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier . Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label . The naive Bayes classifier model defines a parameter for each label , specifying its prior probability , and a parameter for each ( feature , label ) pair , specifying the contribution of individual features towards a label 's likelihood . In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters . In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label . This will sometimes allow the model to \\\" generalize \\\" over some of the differences between related labels or features . Each combination of labels and features that receives its own parameter is called a joint - feature . Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values . Note . In literature that describes and discusses Maximum Entropy models , the term \\\" features \\\" often refers to joint - features ; the term \\\" contexts \\\" refers to what we have been calling ( simple ) features . Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model . In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] . Given the joint - features for a Maximum Entropy model , the score assigned to a label for a given input is simply the product of the parameters associated with the joint - features that apply to that input and label : . 6.2 Maximizing Entropy . The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions . An example will help to illustrate this principle . Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) . At first , we are not told anything more about the word or the senses . There are many probability distributions that we could choose for the ten senses , such as : . Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other . On the other hand , distributions ( ii ) and ( iii ) reflect assumptions that are not supported by what we know . One way to capture this intuition that distribution ( i ) is more \\\" fair \\\" than the other two is to invoke the concept of entropy . In the discussion of decision trees , we described entropy as a measure of how \\\" disorganized \\\" a set of labels was . In particular , if a single label dominates then entropy is low , but if the labels are more evenly distributed then entropy is high . In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high . In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest . Next , suppose that we are told that sense A appears 55 % of the time . Once again , there are many distributions that are consistent with this new piece of information , such as : . But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) . Finally , suppose that we are told that the word \\\" up \\\" appears in the nearby context 10 % of the time , and that when it does appear in the context there 's an 80 % chance that sense A or C will be used . In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : . Furthermore , the remaining probabilities appear to be \\\" evenly distributed . \\\" Throughout this example , we have restricted ourselves to distributions that are consistent with what we know ; among these , we chose the distribution with the highest entropy . This is exactly what the Maximum Entropy classifier does as well . In particular , for each joint - feature , the Maximum Entropy model calculates the \\\" empirical frequency \\\" of that feature - i.e. , the frequency with which it occurs in the training set . It then searches for the distribution which maximizes entropy , while still predicting the correct frequency for each joint - feature . 6.3 Generative vs Conditional Classifiers . An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer . The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair . As a result , generative models can be used to answer the following questions : . How likely is a given input value with a given label ? What is the most likely label for an input that might have one of two values ( but we do n't know which ) ? The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier . Thus , conditional models can still be used to answer questions 1 and 2 . However , conditional models can not be used to answer the remaining questions 3 - 6 . However , this additional power comes at a price . Because the model is more powerful , it has more \\\" free parameters \\\" which need to be learned . However , the size of the training set is fixed . Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values . As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions . However , if we do need answers to questions like 3 - 6 , then we have no choice but to use a generative model . The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline . Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline . 7 Modeling Linguistic Patterns . Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns . Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models . Either way , these explicit models serve two important purposes : they help us to understand linguistic patterns , and they can be used to make predictions about new language data . The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used . Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another . Other models , such as multi - level neural networks , are much more opaque . Although it can be possible to gain insight by studying them , it typically takes a lot more work . But all explicit models can make predictions about new \\\" unseen \\\" language data that was not included in the corpus used to build the model . These predictions can be evaluated to assess the accuracy of the model . Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data . These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering . 7.1 What do models tell us ? It 's important to understand what we can learn about language from an automatically constructed model . One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models . Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns . For example , as we saw in 3.1 , the synonyms absolutely and definitely are not interchangeable : we say absolutely adore not definitely adore , and definitely prefer not absolutely prefer . In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns . For example , we might introduce the abstract concept of \\\" polar verb \\\" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar . Our explanatory model would contain the constraint that absolutely can only combine with polar verbs , and definitely can only combine with non - polar verbs . In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships . Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another . If our goal is to understand the linguistic patterns , then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns . 8 Summary . Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data . Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input . Supervised classifiers can perform a wide variety of NLP tasks , including document classification , part - of - speech tagging , sentence segmentation , dialogue act type identification , and determining entailment relations , and many other tasks . When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance . When evaluating a supervised classifier , it is important that you use fresh data , that was not included in the training or dev - test set . Otherwise , your evaluation results may be unrealistically optimistic . Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features . Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label . In naive Bayes classifiers , each feature independently contributes to the decision of which label should be used . This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another . Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set . Most of the models that are automatically constructed from a corpus are descriptive - they let us know which features are relevant to a given patterns or construction , but they do n't give any information about causal relationships between those features and patterns . 9 Further Reading . Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python . For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) . Examples of these challenge competitions include CoNLL Shared Tasks , the ACE competitions , the Recognizing Textual Entailment competitions , and the AQUAINT competitions . 10 Exercises . Find out what type and quantity of annotated data is required for developing such systems . Why do you think a large amount of data is required ? Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set . Then , starting with the example name gender classifier , make incremental improvements . Use the dev - test set to check your progress . Once you are satisfied with your classifier , check its final performance on the test set . How does the performance on the test set compare to the performance on the dev - test set ? Is this what you 'd expect ? It contains data for four words : hard , interest , line , and serve . Choose one of these four words , and load the corresponding data : . Using this dataset , build a classifier that predicts the correct sense tag for a given instance . Can you explain why these particular features are informative ? Do you find any of them surprising ? Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier . Compare the performance of the three classifiers on your selected task . How do you think that your results might be different if you used a different feature extractor ? What features are relevant in this distinction ? Build a classifier that predicts when each word should be used . However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others . For example , a ynQuestion dialog act is much more likely to be answered by a yanswer than by a greeting . Make use of this fact to build a consecutive classifier for labeling dialog acts . Be sure to consider what features might be useful . See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas . However , many words occur very infrequently , and some of the most informative words in a document may never have occurred in our training data . One solution is to make use of a lexicon , which describes how different words relate to one another . Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data . Each instance in the corpus is encoded as a PPAttachment object : . Select only the instances where inst.attachment is N : . Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns . For example , given the pair of nouns \\\" team \\\" and \\\" researchers , \\\" the classifier should predict the preposition \\\" of \\\" . Explore this issue by looking at corpus data ; writing programs as needed . \"}",
        "_version_":1692668188415754240,
        "score":28.146362},
      {
        "id":"1dc09097-354c-45fd-8983-d8a1d471f3e9",
        "_src_":"{\"url\": \"https://channel9.msdn.com/Forums/Coffeehouse/Time-Lapse-Video-Control-Center\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158481.37/warc/CC-MAIN-20160205193918-00090-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Extracting Information from Text . For any given question , it 's likely that someone has written the answer down somewhere . The amount of natural language text that is available in electronic form is truly staggering , and is increasing every day . However , the complexity of natural language can make it very difficult to access the information in that text . The state of the art in NLP is still a long way from being able to build general - purpose representations of meaning from unrestricted text . If we instead focus our efforts on a limited set of questions or \\\" entity relations , \\\" such as \\\" where are different facilities located , \\\" or \\\" who is employed by what company , \\\" we can make significant progress . The goal of this chapter is to answer the following questions : . How can we build a system that extracts structured data , such as tables , from unstructured text ? What are some robust methods for identifying the entities and relationships described in a text ? Which corpora are appropriate for this work , and how do we use them for training and evaluating our models ? Along the way , we 'll apply techniques from the last two chapters to the problems of chunking and named - entity recognition . 1 Information Extraction . Information comes in many shapes and sizes . One important form is structured data , where there is a regular and predictable organization of entities and relationships . For example , we might be interested in the relation between companies and locations . Given a particular company , we would like to be able to identify the locations where it does business ; conversely , given a location , we would like to discover which companies do business in that location . If our data is in tabular form , such as the example in 1.1 , then answering these queries is straightforward . If this location data was stored in Python as a list of tuples ( entity , relation , entity ) , then the question \\\" Which organizations operate in Atlanta ? \\\" could be translated as follows : . The fourth Wells account moving to another agency is the packaged paper - products division of Georgia - Pacific Corp. , which arrived at Wells only last fall . Like Hertz and the History Channel , it is also leaving for an Omnicom - owned agency , the BBDO South unit of BBDO Worldwide . BBDO South in Atlanta , which handles corporate advertising for Georgia - Pacific , will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels , said Ken Haldin , a spokesman for Georgia - Pacific in Atlanta . If you read through ( 1 ) , you will glean the information required to answer the example question . But how do we get a machine to understand enough about ( 1 ) to return the answers in 1.2 ? This is obviously a much harder task . Unlike 1.1 , ( 1 ) contains no structure that links organization names with location names . One approach to this problem involves building a very general representation of meaning ( 10 . ) In this chapter we take a different approach , deciding in advance that we will only look for very specific kinds of information in text , such as the relation between organizations and locations . Rather than trying to use text like ( 1 ) to answer the question directly , we first convert the unstructured data of natural language sentences into the structured data of 1.1 . Then we reap the benefits of powerful query tools such as SQL . This method of getting meaning from text is called Information Extraction . Information Extraction has many applications , including business intelligence , resume harvesting , media analysis , sentiment detection , patent search , and email scanning . A particularly important area of current research involves the attempt to extract structured data out of electronically - available scientific literature , especially in the domain of biology and medicine . 1.1 Information Extraction Architecture . 1.1 shows the architecture for a simple information extraction system . It begins by processing a document using several of the procedures discussed in 3 and 5 . : first , the raw text of the document is split into sentences using a sentence segmenter , and each sentence is further subdivided into words using a tokenizer . Next , each sentence is tagged with part - of - speech tags , which will prove very helpful in the next step , named entity detection . In this step , we search for mentions of potentially interesting entities in each sentence . Finally , we use relation detection to search for likely relations between different entities in the text . Figure 1.1 : Simple Pipeline Architecture for an Information Extraction System . This system takes the raw text of a document as its input , and generates a list of ( entity , relation , entity ) tuples as its output . For example , given a document that indicates that the company Georgia - Pacific is located in Atlanta , it might generate the tuple ( [ ORG : ' Georgia - Pacific ' ] ' in ' [ LOC : ' Atlanta ' ] ) . To perform the first three tasks , we can define a simple function that simply connects together NLTK 's default sentence segmenter , word tokenizer , and part - of - speech tagger : . Note . Remember that our program samples assume you begin your interactive session or your program with : import nltk , re , pprint . Next , in named entity detection , we segment and label the entities that might participate in interesting relations with one another . Typically , these will be definite noun phrases such as the knights who say \\\" ni \\\" , or proper names such as Monty Python . In some tasks it is useful to also consider indefinite nouns or noun chunks , such as every student or cats , and these do not necessarily refer to entities in the same way as definite NP s and proper names . Finally , in relation extraction , we search for specific patterns between pairs of entities that occur near one another in the text , and use those patterns to build tuples recording the relationships between the entities . 2 Chunking . The basic technique we will use for entity detection is chunking , which segments and labels multi - token sequences as illustrated in 2.1 . The smaller boxes show the word - level tokenization and part - of - speech tagging , while the large boxes show higher - level chunking . Each of these larger boxes is called a chunk . Like tokenization , which omits whitespace , chunking usually selects a subset of the tokens . Also like tokenization , the pieces produced by a chunker do not overlap in the source text . Figure 2.1 : Segmentation and Labeling at both the Token and Chunk Levels . In this section , we will explore chunking in some depth , beginning with the definition and representation of chunks . We will see regular expression and n - gram approaches to chunking , and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus . We will then return in ( 5 ) and 6 to the tasks of named entity recognition and relation extraction . 2.1 Noun Phrase Chunking . We will begin by considering the task of noun phrase chunking , or NP - chunking , where we search for chunks corresponding to individual noun phrases . For example , here is some Wall Street Journal text with NP -chunks marked using brackets : . [ The / DT market / NN ] for / IN [ system - management / NN software / NN ] for / IN [ Digital / NNP ] [ ' s / POS hardware / NN ] is / VBZ fragmented / JJ enough / RB that / IN [ a / DT giant / NN ] such / JJ as / IN [ Computer / NNP Associates / NNPS ] should / MD do / VB well / RB there / RB . As we can see , NP -chunks are often smaller pieces than complete noun phrases . For example , the market for system - management software for Digital 's hardware is a single noun phrase ( containing two nested noun phrases ) , but it is captured in NP -chunks by the simpler chunk the market . One of the motivations for this difference is that NP -chunks are defined so as not to contain other NP -chunks . Consequently , any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP -chunk , since they almost certainly contain further noun phrases . One of the most useful sources of information for NP -chunking is part - of - speech tags . This is one of the motivations for performing part - of - speech tagging in our information extraction system . We demonstrate this approach using an example sentence that has been part - of - speech tagged in 2.2 . In order to create an NP -chunker , we will first define a chunk grammar , consisting of rules that indicate how sentences should be chunked . In this case , we will define a simple grammar with a single regular - expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ ) and then a noun ( NN ) . Using this grammar , we create a chunk parser , and test it on our example sentence . The result is a tree , which we can either print , or display graphically . 2.2 Tag Patterns . The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words . Tag patterns are similar to regular expression patterns ( 3.4 ) . Now , consider the following noun phrases from the Wall Street Journal : . another / DT sharp / JJ dive / NN trade / NN figures / NNS any / DT new / JJ policy / NN measures / NNS earlier / JJR stages / NNS Panamanian / JJ dictator / NN Manuel / NNP Noriega / NNP . This will chunk any sequence of tokens beginning with an optional determiner , followed by zero or more adjectives of any type ( including relative adjectives like earlier / JJR ) , followed by one or more nouns of any type . However , it is easy to find many more complicated examples which this rule will not cover : . his / PRP$ Mansion / NNP House / NNP speech / NN the / DT price / NN cutting / VBG 3/CD % /NN to / TO 4/CD % /NN more / JJR than / IN 10/CD % /NN the / DT fastest / JJS developing / VBG trends / NNS ' s / POS skill / NN . Note . Your Turn : Try to come up with tag patterns to cover these cases . Test them using the graphical interface nltk.app.chunkparser ( ) . Continue to refine your tag patterns with the help of the feedback given by this tool . 2.3 Chunking with Regular Expressions . To find the chunk structure for a given sentence , the RegexpParser chunker begins with a flat structure in which no tokens are chunked . The chunking rules are applied in turn , successively updating the chunk structure . Once all of the rules have been invoked , the resulting chunk structure is returned . 2.3 shows a simple chunk grammar consisting of two rules . The first rule matches an optional determiner or possessive pronoun , zero or more adjectives , then a noun . The second rule matches one or more proper nouns . We also define an example sentence to be chunked , and run the chunker on this input . ( \\\" her \\\" , \\\" PP$ \\\" ) , ( \\\" long \\\" , \\\" JJ \\\" ) , ( \\\" golden \\\" , \\\" JJ \\\" ) , ( \\\" hair \\\" , \\\" NN \\\" ) ] . The $ symbol is a special character in regular expressions , and must be backslash escaped in order to match the tag PP$ . If a tag pattern matches at overlapping locations , the leftmost match takes precedence . For example , if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns , then only the first two nouns will be chunked : . Once we have created the chunk for money market , we have removed the context that would have permitted fund to be included in a chunk . Note . We have added a comment to each of our chunk rules . These are optional ; when they are present , the chunker prints these comments as part of its tracing output . 2.4 Exploring Text Corpora . In 2 we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence of part - of - speech tags . We can do the same work more easily with a chunker , as follows : . Note . 2.5 Chinking . Sometimes it is easier to define what we want to exclude from a chunk . We can define a chink to be a sequence of tokens that is not included in a chunk . In the following example , barked / VBD at / IN is a chink : . [ the / DT little / JJ yellow / JJ dog / NN ] barked / VBD at / IN [ the / DT cat / NN ] . Chinking is the process of removing a sequence of tokens from a chunk . If the matching sequence of tokens spans an entire chunk , then the whole chunk is removed ; if the sequence of tokens appears in the middle of the chunk , these tokens are removed , leaving two chunks where there was only one before . If the sequence is at the periphery of the chunk , these tokens are removed , and a smaller chunk remains . These three possibilities are illustrated in 2.1 . 2.6 Representing Chunks : Tags vs Trees . As befits their intermediate status between tagging and parsing ( 8 . ) , chunk structures can be represented using either tags or trees . The most widespread file representation uses IOB tags . In this scheme , each token is tagged with one of three special chunk tags , I ( inside ) , O ( outside ) , or B ( begin ) . A token is tagged as B if it marks the beginning of a chunk . Subsequent tokens within the chunk are tagged I . All other tokens are tagged O . The B and I tags are suffixed with the chunk type , e.g. B - NP , I - NP . Of course , it is not necessary to specify a chunk type for tokens that appear outside a chunk , so these are just labeled O . An example of this scheme is shown in 2.5 . IOB tags have become the standard way to represent chunk structures in files , and we will also be using this format . Here is how the information in 2.5 would appear in a file : . We PRP B - NP saw VBD O the DT B - NP yellow JJ I - NP dog NN I - NP . In this representation there is one token per line , each with its part - of - speech tag and chunk tag . This format permits us to represent more than one chunk type , so long as the chunks do not overlap . As we saw earlier , chunk structures can also be represented using trees . These have the benefit that each chunk is a constituent that can be manipulated directly . An example is shown in 2.6 . NLTK uses trees for its internal representation of chunks , but provides methods for reading and writing such trees to the IOB format . 3 Developing and Evaluating Chunkers . Now you have a taste of what chunking does , but we have n't explained how to evaluate chunkers . As usual , this requires a suitably annotated corpus . We begin by looking at the mechanics of converting IOB format into an NLTK tree , then at how this is done on a larger scale using a chunked corpus . We will see how to score the accuracy of a chunker relative to a corpus , then look at some more data - driven ways to search for NP chunks . Our focus throughout will be on expanding the coverage of a chunker . 3.1 Reading IOB Format and the CoNLL 2000 Corpus . Using the corpus module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation . The chunk categories provided in this corpus are NP , VP and PP . As we have seen , each sentence is represented using multiple lines , as shown below : . he PRP B - NP accepted VBD B - VP the DT B - NP position NN I - NP ... . A conversion function chunk.conllstr2tree ( ) builds a tree representation from one of these multi - line strings . Moreover , it permits us to choose any subset of the three chunk types to use , here just for NP chunks : . draw ( ) . We can use the NLTK corpus module to access a larger amount of chunked text . The CoNLL 2000 corpus contains 270k words of Wall Street Journal text , divided into \\\" train \\\" and \\\" test \\\" portions , annotated with part - of - speech tags and chunk tags in the IOB format . We can access the data using nltk.corpus.conll2000 . Here is an example that reads the 100th sentence of the \\\" train \\\" portion of the corpus : . As you can see , the CoNLL 2000 corpus contains three chunk types : NP chunks , which we have already seen ; VP chunks such as has already delivered ; and PP chunks such as because of . Since we are only interested in the NP chunks right now , we can use the chunk_types argument to select them : . 3.2 Simple Evaluation and Baselines . Now that we can access a chunked corpus , we can evaluate chunkers . We start off by establishing a baseline for the trivial chunk parser cp that creates no chunks : . The IOB tag accuracy indicates that more than a third of the words are tagged with O , i.e. not in an NP chunk . However , since our tagger did not find any chunks , its precision , recall , and f - measure are all zero . Now let 's try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags ( e.g. CD , DT , and JJ ) . As you can see , this approach achieves decent results . However , we can improve on it by adopting a more data - driven approach , where we use the training corpus to find the chunk tag ( I , O , or B ) that is most likely for each part - of - speech tag . In other words , we can build a chunker using a unigram tagger ( 4 ) . But rather than trying to determine the correct part - of - speech tag for each word , we are trying to determine the correct chunk tag , given each word 's part - of - speech tag . In 3.1 , we define the UnigramChunker class , which uses a unigram tagger to label sentences with chunk tags . Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK 's ChunkParserI interface , and the IOB representation used by the embedded tagger . The class defines two methods : a constructor which is called when we build a new UnigramChunker ; and the parse method which is used to chunk new sentences . class UnigramChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . return nltk.chunk.conlltags2tree(conlltags ) . The constructor expects a list of training sentences , which will be in the form of chunk trees . It first converts training data to a form that is suitable for training the tagger , using tree2conlltags to map each chunk tree to a list of word , tag , chunk triples . It then uses that converted training data to train a unigram tagger , and stores it in self.tagger for later use . The parse method takes a tagged sentence as its input , and begins by extracting the part - of - speech tags from that sentence . It then tags the part - of - speech tags with IOB chunk tags , using the tagger self.tagger that was trained in the constructor . Next , it extracts the chunk tags , and combines them with the original sentence , to yield conlltags . Finally , it uses conlltags2tree to convert the result back into a chunk tree . Now that we have UnigramChunker , we can train it using the CoNLL 2000 corpus , and test its resulting performance : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 92.9 % Precision : 79.9 % Recall : 86.8 % F - Measure : 83.2 % . This chunker does reasonably well , achieving an overall f - measure score of 83 % . Let 's take a look at what it 's learned , by using its unigram tagger to assign a tag to each of the part - of - speech tags that appear in the corpus : . It has discovered that most punctuation marks occur outside of NP chunks , with the exception of # and $ , both of which are used as currency markers . It has also found that determiners ( DT ) and possessives ( PRP$ and WP$ ) occur at the beginnings of NP chunks , while noun types ( NN , NNP , NNPS , NNS ) mostly occur inside of NP chunks . Having built a unigram chunker , it is quite easy to build a bigram chunker : we simply change the class name to BigramChunker , and modify line in 3.1 to construct a BigramTagger rather than a UnigramTagger . The resulting chunker has slightly higher performance than the unigram chunker : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 93.3 % Precision : 82.3 % Recall : 86.8 % F - Measure : 84.5 % . 3.3 Training Classifier - Based Chunkers . Both the regular - expression based chunkers and the n - gram chunkers decide what chunks to create entirely based on part - of - speech tags . However , sometimes part - of - speech tags are insufficient to determine how a sentence should be chunked . For example , consider the following two statements : . These two sentences have the same part - of - speech tags , yet they are chunked differently . In the first sentence , the farmer and rice are separate chunks , while the corresponding material in the second sentence , the computer monitor , is a single chunk . Clearly , we need to make use of information about the content of the words , in addition to just their part - of - speech tags , if we wish to maximize chunking performance . One way that we can incorporate information about the content of words is to use a classifier - based tagger to chunk the sentence . Like the n - gram chunker considered in the previous section , this classifier - based chunker will work by assigning IOB tags to the words in a sentence , and then converting those tags to chunks . For the classifier - based tagger itself , we will use the same approach that we used in 1 to build a part - of - speech tagger . The basic code for the classifier - based NP chunker is shown in 3.2 . It consists of two classes . The first class is almost identical to the ConsecutivePosTagger class from 1.5 . The only two differences are that it calls a different feature extractor and that it uses a MaxentClassifier rather than a NaiveBayesClassifier . The second class is basically a wrapper around the tagger class that turns it into a chunker . During training , this second class maps the chunk trees in the training corpus into tag sequences ; in the parse ( ) method , it converts the tag sequence provided by the tagger back into a chunk tree . class ConsecutiveNPChunkTagger ( nltk . TaggerI ) : def _ _ init _ _ ( self , train_sents ) : . train_set . append ( ( featureset , tag ) ) . history.append(tag ) . history.append(tag ) . return zip(sentence , history ) class ConsecutiveNPChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . nltk.chunk.tree2conlltags(sent ) ] for sent in train_sents ] . return nltk.chunk.conlltags2tree(conlltags ) . The only piece left to fill in is the feature extractor . We begin by defining a simple feature extractor which just provides the part - of - speech tag of the current token . Using this feature extractor , our classifier - based chunker is very similar to the unigram chunker , as is reflected in its performance : . We can also add a feature for the previous part - of - speech tag . Adding this feature allows the classifier to model interactions between adjacent tags , and results in a chunker that is closely related to the bigram chunker . Next , we 'll try adding a feature for the current word , since we hypothesized that word content should be useful for chunking . We find that this feature does indeed improve the chunker 's performance , by about 1.5 percentage points ( which corresponds to about a 10 % reduction in the error rate ) . Finally , we can try extending the feature extractor with a variety of additional features , such as lookahead features , paired features , and complex contextual features . join(sorted(tags ) ) . Note . Your Turn : Try adding different features to the feature extractor function npchunk_features , and see if you can further improve the performance of the NP chunker . 4 Recursion in Linguistic Structure . 4.1 Building Nested Structure with Cascaded Chunkers . So far , our chunk structures have been relatively flat . Trees consist of tagged tokens , optionally grouped under a chunk node such as NP . However , it is possible to build chunk structures of arbitrary depth , simply by creating a multi - stage chunk grammar containing recursive rules . 4.1 has patterns for noun phrases , prepositional phrases , verb phrases , and sentences . This is a four - stage chunk grammar , and can be used to create structures having a depth of at most four . ( \\\" sit \\\" , \\\" VB \\\" ) , ( \\\" on \\\" , \\\" IN \\\" ) , ( \\\" the \\\" , \\\" DT \\\" ) , ( \\\" mat \\\" , \\\" NN \\\" ) ] . Unfortunately this result misses the VP headed by saw . It has other shortcomings too . Let 's see what happens when we apply this chunker to a sentence having deeper nesting . Notice that it fails to identify the VP chunk starting at . The solution to these problems is to get the chunker to loop over its patterns : after trying all of them , it repeats the process . We add an optional second argument loop to specify the number of times the set of patterns should be run : . Note . This cascading process enables us to create deep structures . However , creating and debugging a cascade is difficult , and there comes a point where it is more effective to do full parsing ( see 8 . ) Also , the cascading process can only produce trees of fixed depth ( no deeper than the number of stages in the cascade ) , and this is insufficient for complete syntactic analysis . 4.2 Trees . A tree is a set of connected labeled nodes , each reachable by a unique path from a distinguished root node . Here 's an example of a tree ( note that they are standardly drawn upside - down ) : . We use a ' family ' metaphor to talk about the relationships of nodes in a tree : for example , S is the parent of VP ; conversely VP is a child of S . Also , since NP and VP are both children of S , they are also siblings . For convenience , there is also a text format for specifying trees : . ( S ( NP Alice ) ( VP ( V chased ) ( NP ( Det the ) ( N rabbit ) ) ) ) . Although we will focus on syntactic trees , trees can be used to encode any homogeneous hierarchical structure that spans a sequence of linguistic forms ( e.g. morphological structure , discourse structure ) . In the general case , leaves and node values do not have to be strings . In NLTK , we create a tree by giving a node label and a list of children : . We can incorporate these into successively larger trees as follows : . Here are some of the methods available for tree objects : . The bracketed representation for complex trees can be difficult to read . In these cases , the draw method can be very useful . It opens a new window , containing a graphical representation of the tree . The tree display window allows you to zoom in and out , to collapse and expand subtrees , and to print the graphical representation to a postscript file ( for inclusion in a document ) . 4.3 Tree Traversal . def traverse ( t ) : . try : . ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) . 5 Named Entity Recognition . At the start of this chapter , we briefly introduced named entities ( NEs ) . Named entities are definite noun phrases that refer to specific types of individuals , such as organizations , persons , dates , and so on . 5.1 lists some of the more commonly used types of NEs . These should be self - explanatory , except for \\\" Facility \\\" : human - made artifacts in the domains of architecture and civil engineering ; and \\\" GPE \\\" : geo - political entities such as city , state / province , and country . The goal of a named entity recognition ( NER ) system is to identify all textual mentions of the named entities . This can be broken down into two sub - tasks : identifying the boundaries of the NE , and identifying its type . While named entity recognition is frequently a prelude to identifying relations in Information Extraction , it can also contribute to other tasks . For example , in Question Answering ( QA ) , we try to improve the precision of Information Retrieval by recovering not whole pages , but just those parts which contain an answer to the user 's question . Most QA systems take the documents returned by standard Information Retrieval , and then attempt to isolate the minimal text snippet in the document containing the answer . Now suppose the question was Who was the first President of the US ? , and one of the documents that was retrieved contained the following passage : . The Washington Monument is the most prominent structure in Washington , D.C. and one of the city 's early attractions . It was built in honor of George Washington , who led the country to independence and then became its first President . Analysis of the question leads us to expect that an answer should be of the form X was the first President of the US , where X is not only a noun phrase , but also refers to a named entity of type PERSON . This should allow us to ignore the first sentence in the passage . While it contains two occurrences of Washington , named entity recognition should tell us that neither of them has the correct type . How do we go about identifying named entities ? One option would be to look up each word in an appropriate list of names . For example , in the case of locations , we could use a gazetteer , or geographical dictionary , such as the Alexandria Gazetteer or the Getty Gazetteer . However , doing this blindly runs into problems , as shown in 5.1 . Figure 5.1 : Location Detection by Simple Lookup for a News Story : Looking up every word in a gazetteer is error - prone ; case distinctions may help , but these are not always present . Observe that the gazetteer has good coverage of locations in many countries , and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam . Of course we could omit such locations from the gazetteer , but then we wo n't be able to identify them when they do appear in a document . It gets even harder in the case of names for people or organizations . Any list of such names will probably have poor coverage . New organizations come into existence every day , so if we are trying to deal with contemporary newswire or blog entries , it is unlikely that we will be able to recognize many of the entities using gazetteer lookup . Another major source of difficulty is caused by the fact that many named entity terms are ambiguous . Thus May and North are likely to be parts of named entities for DATE and LOCATION , respectively , but could both be part of a PERSON ; conversely Christian Dior looks like a PERSON but is more likely to be of type ORGANIZATION . A term like Yankee will be ordinary modifier in some contexts , but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders . Further challenges are posed by multi - word names like Stanford University , and by names that contain other names such as Cecil H. Green Library and Escondido Village Conference Service Center . In named entity recognition , therefore , we need to be able to identify the beginning and end of multi - token sequences . Named entity recognition is a task that is well - suited to the type of classifier - based approach that we saw for noun phrase chunking . In particular , we can build a tagger that labels each word in a sentence using the IOB format , where chunks are labeled by their appropriate type . Here is part of the CONLL 2002 ( conll2002 ) Dutch training data : . Eddy N B - PER Bonte N I - PER is V O woordvoerder N O van Prep O diezelfde Pron O Hogeschool N B - ORG . Punc O . In this representation , there is one token per line , each with its part - of - speech tag and its named entity tag . Based on this training corpus , we can construct a tagger that can be used to label new sentences ; and use the nltk.chunk.conlltags2tree ( ) function to convert the tag sequences into a chunk tree . NLTK provides a classifier that has already been trained to recognize named entities , accessed with the function nltk.ne_chunk ( ) . 6 Relation Extraction . Once named entities have been identified in a text , we then want to extract the relations that exist between them . As indicated earlier , we will typically be looking for relations between specified types of named entity . One way of approaching this task is to initially look for all triples of the form ( X , \\u03b1 , Y ) , where X and Y are named entities of the required types , and \\u03b1 is the string of words that intervenes between X and Y . We can then use regular expressions to pull out just those instances of \\u03b1 that express the relation that we are looking for . The following example searches for strings that contain the word in . The special regular expression ( ? ! \\\\b.+ing\\\\b ) is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of , where in is followed by a gerund . As shown above , the conll2002 Dutch corpus contains not just named entity annotation but also part - of - speech tags . This allows us to devise patterns that are sensitive to these tags , as shown in the next example . The method clause ( ) prints out the relations in a clausal form , where the binary relation symbol is specified as the value of parameter relsym . Note . This will show you the actual words that intervene between the two NEs and also their left and right context , within a default 10-word window . With the help of a Dutch dictionary , you might be able to figure out why the result VAN ( ' annie_lennox ' , ' eurythmics ' ) is a false hit . 7 Summary . Information extraction systems search large bodies of unrestricted text for specific types of entities and relations , and use them to populate well - organized databases . These databases can then be used to find answers for specific questions . The typical architecture for an information extraction system begins by segmenting , tokenizing , and part - of - speech tagging the text . The resulting data is then searched for specific types of entity . Finally , the information extraction system looks at entities that are mentioned near one another in the text , and tries to determine whether specific relationships hold between those entities . Entity recognition is often performed using chunkers , which segment multi - token sequences , and label them with the appropriate entity type . Common entity types include ORGANIZATION , PERSON , LOCATION , DATE , TIME , MONEY , and GPE ( geo - political entity ) . Chunkers can be constructed using rule - based systems , such as the RegexpParser class provided by NLTK ; or using machine learning techniques , such as the ConsecutiveNPChunker presented in this chapter . In either case , part - of - speech tags are often a very important feature when searching for chunks . Although chunkers are specialized to create relatively flat data structures , where no two chunks are allowed to overlap , they can be cascaded together to build nested structures . Relation extraction can be performed using either rule - based systems which typically look for specific patterns in the text that connect entities and the intervening words ; or using machine - learning systems which typically attempt to learn such patterns automatically from a training corpus . 8 Further Reading . The popularity of chunking is due in great part to pioneering work by Abney e.g. , ( Church , Young , & Bloothooft , 1996 ) . The IOB format ( or sometimes BIO Format ) was developed for NP chunking by ( Ramshaw & Marcus , 1995 ) , and was used for the shared NP bracketing task run by the Conference on Natural Language Learning ( CoNLL ) in 1999 . The same format was adopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of a shared task on NP chunking . 9 Exercises . Why are three tags necessary ? What problem would be caused if we used I and O tags exclusively ? Try to do this by generalizing the tag pattern that handled singular noun phrases . Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk . Develop a simple chunker using the regular expression chunker nltk . RegexpParser . Discuss any tag sequences that are difficult to chunk reliably . Develop a chunker that starts by putting the whole sentence in a single chunk , and then does the rest of its work solely by chinking . Determine which tags ( or tag sequences ) are most likely to make up chinks with the help of your own utility program . Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules . Add these patterns to the grammar , one per line . Test your work using some tagged sentences of your own devising . ( Note that most chunking corpora contain some internal inconsistencies , such that any reasonable rule - based approach will produce errors . ) Evaluate your chunker on 100 sentences from a chunked corpus , and report the precision , recall and F - measure . Use the chunkscore.missed ( ) and chunkscore.incorrect ( ) methods to identify the errors made by your chunker . Discuss . Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter . Use any combination of rules for chunking , chinking , merging or splitting . Instead of requiring manual correction of tagger output , good chunkers are able to work with the erroneous output of taggers . Look for other examples of correctly chunked noun phrases with incorrect tags . Study its errors and try to work out why it does n't get 100 % accuracy . Experiment with trigram chunking . Are you able to improve the performance any more ? Instead of assigning POS tags to words , here we will assign IOB tags to the POS tags . E.g. if the tag DT ( determiner ) often occurs at the start of a chunk , it will be tagged B ( begin ) . Evaluate the performance of these chunking methods relative to the regular expression chunking methods covered in this chapter . that it is possible to establish an upper limit to tagging performance by looking for ambiguous n - grams , n - grams that are tagged in more than one possible way in the training data . Apply the same method to determine an upper bound on the performance of an n - gram chunker . Write functions to do the following tasks for your chosen type : . List all the tag sequences that occur with each instance of this chunk type . Count the frequency of each tag sequence , and produce a ranked list in order of decreasing frequency ; each line should consist of an integer ( the frequency ) and the tag sequence . Inspect the high - frequency tag sequences . Use these as the basis for developing a better chunker . For example , the phrase : [ every / DT time / NN ] [ she / PRP ] sees / VBZ [ a / DT newspaper / NN ] contains two consecutive chunks , and our baseline chunker will incorrectly combine the first two : [ every / DT time / NN she / PRP ] . Write a program that finds which of these chunk - internal tags typically occur at the start of a chunk , then devise one or more rules that will split up these chunks . Combine these with the existing baseline chunker and re - evaluate it , to see if you have discovered an improved baseline . The format uses square brackets , and we have encountered it several times during this chapter . The Treebank corpus can be accessed using : for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid ) . These are flat trees , just as we got using nltk.corpus.conll2000.chunked_sents ( ) . The functions nltk.tree.pprint ( ) and nltk.chunk.tree2conllstr ( ) can be used to create Treebank and IOB strings from a tree . Write functions chunk2brackets ( ) and chunk2iob ( ) that take a single chunk tree as their sole argument , and return the required multi - line string representation . Write command - line conversion utilities bracket2iob.py and iob2bracket.py that take a file in Treebank or CoNLL format ( resp ) and convert it to the other format . ( Obtain some raw Treebank or CoNLL data from the NLTK Corpora , save it to a file , and then use for line in open(filename ) to access it from Python . ) Investigate other models of the context , such as the n-1 previous part - of - speech tags , or some combination of previous chunk tags along with previous and following part - of - speech tags . Now observe how a chunker may re - use this sequence information . For example , both tasks will make use of the information that nouns tend to follow adjectives ( in English ) . It would appear that the same information is being maintained in two places . Is this likely to become a problem as the size of the rule sets grows ? If so , speculate about any ways that this problem might be addressed . \"}",
        "_version_":1692668359994245120,
        "score":26.37608},
      {
        "id":"70fb47ba-960d-44e3-bdd6-c28eb76753de",
        "_src_":"{\"url\": \"http://www.thebrewsite.com/next-session-thanks-to-the-big-boys/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156448.92/warc/CC-MAIN-20160205193916-00286-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"class nltk.corpus.reader.aligned . class nltk.corpus.reader.api . CategorizedCorpusReader ( kwargs ) [ source ] \\u00b6 . Bases : object . A mixin class used to aid in the implementation of corpus readers for categorized corpora . This class defines the method categories ( ) , which returns a list of the categories for the corpus or for a specified set of fileids ; and overrides fileids ( ) to take a categories argument , restricting the set of fileids to be returned . Call _ _ init _ _ ( ) to set up the mapping . Override all view methods to accept a categories parameter , which can be used instead of the fileids parameter , to select which fileids should be included in the returned view . Return a list of file identifiers for the files that make up this corpus , or that make up the given category(s ) if specified . class nltk.corpus.reader.api . Bases : object . A base class for \\\" corpus reader \\\" classes , each of which can be used to read a specific corpus format . Each individual corpus reader instance is used to read a specific corpus , consisting of one or more files under a common root directory . Each file is identified by its file identifier , which is the relative path to the file from the root directory . A separate subclass is be defined for each corpus format . These subclasses define one or more methods that provide ' views ' on the corpus contents , such as words ( ) ( for a list of words ) and parsed_sents ( ) ( for a list of parsed sentences ) . Called with no arguments , these methods will return the contents of the entire corpus . For most corpora , these methods define one or more selection arguments , such as fileids or categories , which can be used to select which portion of the corpus should be returned . fileids ( None or str or list ) - Specifies the set of fileids for which paths should be returned . Can be None , for all fileids ; a list of file identifiers , for a specified set of fileids ; or a single file identifier , for a single file . Note that the return value is always a list of paths , even if fileids is a single file identifier . include_encoding - If true , then return a list of ( path_pointer , encoding ) tuples . Load this corpus ( if it has not already been loaded ) . This is used by LazyCorpusLoader as a simple method that can be used to make sure a corpus is loaded - e.g. , in case a user wants to do help(some_corpus ) . Reader for the Alpino Dutch Treebank . This corpus has a lexical breakdown structure embedded , as read by _ parse Unfortunately this puts punctuation and some other words out of the sentence order in the xml element tree . This is no good for tag _ and word _ _ tag and _ word will be overridden to use a non - default new parameter ' ordered ' to the overridden _ normalize function . The _ parse function can then remain untouched . class nltk.corpus.reader.bracket_parse . Bo Pang and Lillian Lee . \\\" Seeing stars : Exploiting class relationships for . sentiment categorization with respect to rating scales \\\" . Proceedings of the ACL , 2005 . class nltk.corpus.reader.categorized_sents . A reader for corpora in which each row represents a single instance , mainly a sentence . Istances are divided into categories based on their file identifiers ( see CategorizedCorpusReader ) . Since many corpora allow rows that contain more than one sentence , it is possible to specify a sentence tokenizer to retrieve all sentences instead than all rows . Examples using the Subjectivity Dataset : . Examples using the Sentence Polarity Dataset : . sents ( ) [ [ ' simplistic ' , ' , ' , ' silly ' , ' and ' , ' tedious ' , ' . ' ] categories ( ) [ ' neg ' , ' pos ' ] . Corpus reader for the XML version of the CHILDES corpus . Copy the needed parts of the CHILDES XML corpus into the NLTK data directory ( nltk_data / corpora / CHILDES/ ) . For access to the file text use the usual nltk functions , words ( ) , sents ( ) , tagged_words ( ) and tagged_sents ( ) . the given file(s ) as a list of sentences or utterances , each encoded as a list of word strings . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of sentences , each encoded as a list of ( word , tag ) tuples . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of tagged words and punctuation symbols , encoded as tuples ( word , tag ) . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . Map a corpus file to its web version on the CHILDES website , and open it in a web browser . The complete URL to be used is : . If no urlbase is passed , we try to calculate it . This requires that the childes corpus was set up to mirror the folder hierarchy under childes.psy.cmu.edu/data-xml/ , e.g. : nltk_data / corpora / childes / Eng - USA / Cornell/ ? ? ? or nltk_data / corpora / childes / Romance / Spanish / Aguirre/ ? ? ? The function first looks ( as a special case ) if \\\" Eng - USA \\\" is on the path consisting of + fileid ; then if \\\" childes \\\" , possibly followed by \\\" data - xml \\\" , appears . If neither one is found , we use the unmodified fileid and hope for the best . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . A reader for corpora that contain chunked ( and optionally tagged ) documents . class nltk.corpus.reader.chunked . Reader for chunked ( and optionally tagged ) corpora . Paragraphs are split using a block reader . They are then tokenized into sentences using a sentence tokenizer . Finally , these sentences are parsed into chunk trees using a string - to - chunktree conversion function . Each of these steps can be performed using a default function or a custom function . By default , paragraphs are split on blank lines ; sentences are listed one per line ; and sentences are parsed into chunk trees using nltk.chunk.tagstr2tree . the given file(s ) as a list of paragraphs , each encoded as a list of sentences , which are in turn encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of sentences , each encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of tagged words and chunks . Words are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . Chunks are encoded as depth - one trees over ( word , tag ) tuples or word strings . class nltk.corpus.reader.chunked . File Format : Each line consists of an uppercased word , a counter ( for alternative pronunciations ) , and a transcription . E.g. : NATURAL 1 N AE1 CH ER0 AH0 L . The dictionary contains 127069 entries . Of these , 119400 words are assigned a unique pronunciation , 6830 words have two pronunciations , and 839 words have three or more pronunciations . Many of these are fast - speech variants . Phonemes : There are 39 phonemes , as shown below : . class nltk.corpus.reader.cmudict . Nitin Jindal and Bing Liu . \\\" Identifying Comparative Sentences in Text Documents \\\" . Proceedings of the ACM SIGIR International Conference on Information Retrieval ( SIGIR-06 ) , 2006 . Nitin Jindal and Bing Liu . \\\" Mining Comprative Sentences and Relations \\\" . Proceedings of Twenty First National Conference on Artificial Intelligence ( AAAI-2006 ) , 2006 . Murthy Ganapathibhotla and Bing Liu . \\\" Mining Opinions in Comparative Sentences \\\" . Proceedings of the 22nd International Conference on Computational Linguistics ( Coling-2008 ) , Manchester , 18 - 22 August , 2008 . feature , comparison . comparisons ( ) ) 853 . A ConllCorpusReader whose data file contains three columns : words , pos , and chunk . class nltk.corpus.reader.conll . A corpus reader for CoNLL - style files . These files consist of a series of sentences , separated by blank lines . Each sentence is encoded using a table ( or \\\" grid \\\" ) of values , where each line corresponds to a single word , and each column corresponds to an annotation type . The set of columns used by CoNLL - style files can vary from corpus to corpus ; the ConllCorpusReader constructor therefore takes an argument , columntypes , which is used to specify the columns that are used by a given corpus . @todo : Possibly add caching of the grid corpus view ? This would . allow the same grid view to be used by different data access methods ( eg words ( ) and parsed_sents ( ) could both share the same grid corpus view object ) . @todo : Better support for -DOCSTART- . Currently , we just ignore . it , but it could be used to define methods that retrieve a document at a time ( eg parsed_documents ( ) ) . A list of ( argspan , argid ) tuples , specifying the location and type for each of the arguments identified by this instance . argspan is a tuple start , end , indicating that the argument consists of the words[start : end ] . class nltk.corpus.reader.dependency . lu ( 3238 ) . frame . lexUnit [ ' glint.v ' ] is fn . frame_by_name ( ' Replacing ' ) is fn . lus ( ' replace.v ' ) [ 0 ] . lus ( ' prejudice.n ' ) [ 0 ] . frame . frame_relations ( ' Partiality ' ) True . Details for a specific annotated document can be obtained using this class 's annotated_document ( ) function and pass it the value of the ' ID ' field . corpname for x in fn . name ( str ) - A regular expression pattern used to search the file name of each annotated document . The document 's file name contains the name of the corpus that the document is from , followed by two underscores \\\" _ _ \\\" followed by the document name . So , for example , the file name \\\" LUCorpus - v0.3__20000410_nyt - NEW . xml \\\" is from the corpus named \\\" LUCorpus - v0.3 \\\" and the document name is \\\" 20000410_nyt - NEW . xml \\\" . Lists frame element objects . If ' name ' is provided , this is treated as a case - insensitive regular expression to filter by frame name . ( Case - insensitivity is because casing of frame element names is not always consistent across frames . ) frame . name , fe . name ) for fe in fn . name for fe in fn . fes ( ' ^sound$ ' ) ) 2 . Get the details for the specified Frame using the frame 's name or i d number . Usage examples : . frame ( ' Imposing_obligation ' ) frame ( 1494 ) : Imposing_obligation ... . The dict that is returned from this function will contain the following information about the Frame : . ' name ' : the name of the Frame ( e.g. ' Birth ' , ' Apply_heat ' , etc . ) . ' definition ' : textual definition of the Frame . ' ID ' : the internal ID number of the Frame . ' semTypes ' : a list of semantic types for this frame . Each item in the list is a dict containing the following keys : . ' name ' : can be used with the semtype ( ) function . ' ID ' : can be used with the semtype ( ) function . ' lexUnit ' : a dict containing all of the LUs for this frame . The keys in this dict are the names of the LUs and the value for each key is itself a dict containing info about the LU ( see the lu ( ) function for more info . ) FE ' : a dict containing the Frame Elements that are part of this frame . The keys in this dict are the names of the FEs ( e.g. ' Body_system ' ) and the values are dicts containing the following keys . definition \\\" This frame includes words that name ... \\\" . Also see the frame ( ) function for details about what is contained in the dict that is returned . Get the details for the specified Frame using the frame 's name . definition \\\" This frame includes words that name ... \\\" . frame - ( optional ) frame object , name , or ID ; only relations involving . frame_relations ( fn . frames ( r ' ( ? A brief intro to Frames ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A Frame is a script - like conceptual structure that describes a particular type of situation , object , or event along with the participants and props that are needed for that Frame . For example , the \\\" Apply_heat \\\" frame describes a common situation involving a Cook , some Food , and a Heating_Instrument , and is evoked by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . . We call the roles of a Frame \\\" frame elements \\\" ( FEs ) and the frame - evoking words are called \\\" lexical units \\\" ( LUs ) . FrameNet includes relations between Frames . Several types of relations are defined , of which the most important are : . Inheritance : An IS - A relation . The child frame is a subtype of the parent frame , and each FE in the parent is bound to a corresponding FE in the child . An example is the \\\" Revenge \\\" frame which inherits from the \\\" Rewards_and_punishments \\\" frame . Using : The child frame presupposes the parent frame as background , e.g the \\\" Speed \\\" frame \\\" uses \\\" ( or presupposes ) the \\\" Motion \\\" frame ; however , not all parent FEs need to be bound to child FEs . Subframe : The child frame is a subevent of a complex event represented by the parent , e.g. the \\\" Criminal_process \\\" frame has subframes of \\\" Arrest \\\" , \\\" Arraignment \\\" , \\\" Trial \\\" , and \\\" Sentencing \\\" . Perspective_on : The child frame provides a particular perspective on an un - perspectivized parent frame . A pair of examples consists of the \\\" Hiring \\\" and \\\" Get_a_job \\\" frames , which perspectivize the \\\" Employment_start \\\" frame from the Employer 's and the Employee 's point of view , respectively . Returns a list of all frames that contain LUs in which the name attribute of the LU matchs the given regular expression pat . Note that LU names are composed of \\\" lemma . POS \\\" , where the \\\" lemma \\\" part can be made up of either a single lexeme ( e.g. ' run ' ) or multiple lexemes ( e.g. ' a little ' ) . frames_by_lemma ( r ' ( ? Get information about a specific Lexical Unit using the i d number fn_luid . This function reads the LU information from the xml file on disk each time it is called . You may want to cache this info if you plan to call this function with the same i d number multiple times . Usage examples : . lu ( 256 ) . lu ( 256 ) . definition ' COD : be aware of beforehand ; predict . ' lu ( 256 ) . frame . lu ( 256 ) . The dict that is returned from this function will contain most of the following information about the LU . Note that some LUs do not contain all of these pieces of information - particularly ' totalAnnotated ' and ' incorporatedFE ' may be missing in some LUs : . ' _ type ' : ' lu ' . ' status ' : e.g. ' Created ' . ' frame ' : Frame that this LU belongs to . ' POS ' : the part of speech of this LU ( e.g. ' N ' ) . ' totalAnnotated ' : total number of examples annotated with this LU . ' incorporatedFE ' : FE that incorporates this LU ( e.g. ' Ailment ' ) . ' sentenceCount ' : a dict with the following two keys : . ' annotated ' : number of sentences annotated with this LU . ' total ' : total number of sentences with this LU . ' lexemes ' : a list of dicts describing the lemma of this LU . Each dict in the list contains these keys : - ' POS ' : part of speech e.g. ' N ' - ' name ' : either single - lexeme e.g. ' merger ' or . Consider : \\\" take over.v \\\" as in : . Germany took over the Netherlands in 2 days . Germany took the Netherlands over in 2 days . In this case , ' breakBefore ' would be \\\" true \\\" for the lexeme \\\" over \\\" . Contrast this with \\\" take after.v \\\" as in : . Under the hood , this implementation looks up the lexical unit information in the frame definition file . That file does not contain corpus annotations , so the LU files will be accessed on demand if those are needed . In principle , valence patterns could be loaded here too , though these are not currently supported . Returns basic information about the LU whose i d is fn_luid . This is basically just a wrapper around the lu ( ) function with \\\" subCorpus \\\" info excluded . lus ( r ' ( ? A brief intro to Lexical Units ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A lexical unit ( LU ) is a pairing of a word with a meaning . For example , the \\\" Apply_heat \\\" Frame describes a common situation involving a Cook , some Food , and a Heating Instrument , and is _ evoked _ by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . These frame - evoking words are the LUs in the Apply_heat frame . Each sense of a polysemous word is a different LU . We have used the word \\\" word \\\" in talking about LUs . The reality is actually rather complex . When we say that the word \\\" bake \\\" is polysemous , we mean that the lemma \\\" bake.v \\\" ( which has the word - forms \\\" bake \\\" , \\\" bakes \\\" , \\\" baked \\\" , and \\\" baking \\\" ) is linked to three different frames : . Apply_heat : \\\" Michelle baked the potatoes for 45 minutes . \\\" Cooking_creation : \\\" Michelle baked her mother a cake for her birthday . \\\" Absorb_heat : \\\" The potatoes have to bake for more than 30 minutes . \\\" These constitute three different LUs , with different definitions . Multiword expressions such as \\\" given name \\\" and hyphenated words like \\\" shut - eye \\\" can also be LUs . Idiomatic phrases such as \\\" middle of nowhere \\\" and \\\" give the slip ( to ) \\\" are also defined as LUs in the appropriate frames ( \\\" Isolated_places \\\" and \\\" Evading \\\" , respectively ) , and their internal structure is not analyzed . Framenet provides multiple annotated examples of each sense of a word ( i.e. each LU ) . Moreover , the set of examples ( approximately 20 per LU ) illustrates all of the combinatorial possibilities of the lexical unit . Each LU is linked to a Frame , and hence to the other words which evoke that Frame . This makes the FrameNet database similar to a thesaurus , grouping together semantically similar words . In the simplest case , frame - evoking words are verbs such as \\\" fried \\\" in : . \\\" Matilde fried the catfish in a heavy iron skillet . \\\" Sometimes event nouns may evoke a Frame . For example , \\\" reduction \\\" evokes \\\" Cause_change_of_scalar_position \\\" in : . \\\" ... the reduction of debt levels to $ 665 million from $ 2.6 billion . \\\" Adjectives may also evoke a Frame . For example , \\\" asleep \\\" may evoke the \\\" Sleep \\\" frame as in : . \\\" They were asleep for hours . \\\" Many common nouns , such as artifacts like \\\" hat \\\" or \\\" tower \\\" , typically serve as dependents rather than clearly evoking their own frames . A regular expression pattern used to search the LU names . Note that LU names take the form of a dotted string ( e.g. \\\" run.v \\\" or \\\" a little.adv \\\" ) in which a lemma preceeds the \\\" . \\\" and a POS follows the dot . The lemma may be composed of a single lexeme ( e.g. \\\" run \\\" ) or of multiple lexemes ( e.g. \\\" a little \\\" ) . If ' name ' is not given , then all LUs will be returned . The valid POSes are : . v - verb n - noun a - adjective adv - adverb prep - preposition num - numbers intj - interjection art - article c - conjunction scon - subordinating conjunction . Return type : . list of LU objects ( dicts ) . See the lu ( ) function for info about the specifics of LU objects . Apply inference rules to distribute semtypes over relations between FEs . For FrameNet 1.5 , this results in 1011 semtypes being propagated . ( Not done by default because it requires loading all frame files , which takes several seconds . If this needed to be fast , it could be rewritten to traverse the neighboring relations on demand for each FE semtype . ) frames ( ) for fe in f . FE . values ( ) if fe . frames ( ) for fe in f . FE . values ( ) if fe . semType ) 5252 . keys ( ) ) [ ' ID ' , ' _ type ' , ' abbrev ' , ' definition ' , ' name ' , ' rootType ' , ' subTypes ' , ' superType ' ] . This corpus contains the NEWSWIRE development test data for the NIST 1999 IE - ER Evaluation . ref.nwt and filenames were shortened . The corpus contains the following files : APW_19980314 , APW_19980424 , APW_19980429 , NYT_19980315 , NYT_19980403 , and NYT_19980407 . class nltk.corpus.reader.ieer . nltk.corpus.reader.ieer . A list of all documents in this corpus . nltk.corpus.reader.ieer . A dictionary whose keys are the names of documents in this corpus ; and whose values are descriptions of those documents ' contents . Corpus reader designed to work with corpus created by IPI PAN . The corpus includes information about text domain , channel and categories . You can access possible values using domains ( ) , channels ( ) and categories ( ) . The reader supports methods : words , sents , paras and their tagged versions . The IPIPAN Corpus contains tags indicating if there is a space between two tokens . As a result in place where there should be no space between two tokens new pair ( '' , ' no - space ' ) will be inserted ( for tagged data ) and just '' for methods without tags . The corpus reader can also try to append spaces between words . As a result either ' ' or ( ' ' , ' space ' ) will be inserted between tokens . By default , xml entities like \\\" and & are replaced by corresponding characters . Reader for corpora following the TEI - p5 xml scheme , such as MULTEXT - East . MULTEXT - East contains part - of - speech - tagged words with a quite precise tagging scheme . These tags can be converted to the Universal tagset . class nltk.corpus.reader.nkjp . XML_Tool ( root , filename ) [ source ] \\u00b6 . Bases : object . Helper class creating xml file to one without references to nkjp : namespace . That 's needed because the XMLCorpusView assumes that one can find short substrings of XML that are valid XML , which is not true if a namespace is declared at top level . Corpus reader for the nombank corpus , which augments the Penn Treebank with information about the predicate argument structure of every noun instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - noun basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . A list of tuples ( argloc , argid ) , specifying the location and identifier for each of the predicate 's argument in the containing sentence . Argument identifiers are strings such as ' ARG0 ' or ' ARGM - TMP ' . This list does not contain the predicate . Reader for Liu and Hu opinion lexicon . Blank lines and readme are ignored . words ( ) [ ' 2-faced ' , ' 2-faces ' , ' abnormal ' , ' abolish ' , ... ] . The OpinionLexiconCorpusReader provides shortcuts to retrieve positive / negative words : . negative ( ) [ ' 2-faced ' , ' 2-faces ' , ' abnormal ' , ' abolish ' , ... ] . Note that words from words ( ) method are sorted by file i d , not alphabetically : . In the pl196x corpus each category is stored in single file and thus both methods provide identical functionality . In order to accommodate finer granularity , a non - standard textids ( ) method was implemented . All the main functions can be supplied with a list of required chunks - giving much more control to the user . class nltk.corpus.reader.plaintext . Reader for Europarl corpora that consist of plaintext documents . Documents are divided into chapters instead of paragraphs as for regular plaintext documents . Chapters are separated using blank lines . Everything is inherited from PlaintextCorpusReader except that : . Since the corpus is pre - processed and pre - tokenized , the word tokenizer should just split the line at whitespaces . For the same reason , the sentence tokenizer should just split the paragraph at line breaks . There is a new ' chapters ( ) ' method that returns chapters instead instead of paragraphs . The ' paras ( ) ' method inherited from PlaintextCorpusReader is made non - functional to remove any confusion between chapters and paragraphs for Europarl . class nltk.corpus.reader.plaintext . Reader for corpora that consist of plaintext documents . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specificed as parameters to the constructor . This corpus reader can be customized ( e.g. , to skip preface sections of specific document formats ) by creating a subclass and overriding the CorpusView class variable . 42960 gives authority to administration V 46742 gives inventors of microchip N . The PP attachment is to the verb phrase ( V ) or noun phrase ( N ) , i.e. : . ( VP gives ( NP authority ) ( PP to administration ) ) ( VP gives ( NP inventors ( PP of microchip ) ) ) . The corpus contains the following files : . training : training set devset : development test set , used for algorithm development . test : test set , used to report results bitstrings : word classes derived from Mutual Information Clustering for the Wall Street Journal . Corpus reader for the propbank corpus , which augments the Penn Treebank with information about the predicate argument structure of every verb instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - verb basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . A list of tuples ( argloc , argid ) , specifying the location and identifier for each of the predicate 's argument in the containing sentence . Argument identifiers are strings such as ' ARG0 ' or ' ARGM - TMP ' . This list does not contain the predicate . Murthy Ganapathibhotla and Bing Liu . \\\" Mining Opinions in Comparative Sentences \\\" . Proceedings of the 22nd International Conference on Computational Linguistics ( Coling-2008 ) , Manchester , 18 - 22 August , 2008 . Bing Liu , Minqing Hu and Junsheng Cheng . \\\" Opinion Observer : Analyzing and Comparing . Opinions on the Web \\\" . Proceedings of the 14th international World Wide Web conference ( WWW-2005 ) , May 10 - 14 , 2005 , in Chiba , Japan . class nltk.corpus.reader.pros_cons . , ' On ' , ' - ' , ' off ' , ' switch ' , ' too ' , ' easy ' , ' to ' , ' maneuver ' , ' . ' ] words ( ' IntegratedPros.txt ' ) [ ' Easy ' , ' to ' , ' use ' , ' , ' , ' economical ' , ' ! ' , ... ] . Related papers : . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ( KDD-04 ) , 2004 . Minqing Hu and Bing Liu . \\\" Mining Opinion Features in Customer Reviews \\\" . Proceedings of Nineteeth National Conference on Artificial Intelligence ( AAAI-2004 ) , 2004 . Xiaowen Ding , Bing Liu and Philip S. Yu . \\\" A Holistic Lexicon - Based Appraoch to . Opinion Mining . \\\" Proceedings of First ACM International Conference on Web Search and Data Mining ( WSDM-2008 ) , Feb 11 - 12 , 2008 , Stanford University , Stanford , California , USA . Symbols used in the annotated reviews : . [ t ] : the title of the review : Each [ t ] tag starts a review . [ + n ] : Positive opinion , n is the opinion strength : 3 strongest , and 1 weakest . Note that the strength is quite subjective . You may want ignore it , but only considering + and - . [ -n ] : Negative opinion # # : start of each sentence . Each line is a sentence . [ u ] : feature not appeared in the sentence . [ p ] : feature not appeared in the sentence . Pronoun resolution is needed . [ s ] : suggestion or recommendation . [ cc ] : comparison with a competing product from a different brand . [ cs ] : comparison with a competing product from the same brand . Note : Some of the files ( e.g. \\\" ipod.txt \\\" , \\\" Canon PowerShot SD500.txt \\\" ) do not . provide separation between different reviews . This is due to the fact that the dataset was specifically designed for aspect / feature - based sentiment analysis , for which sentence - level annotation is sufficient . For document- level classification and analysis , this peculiarity should be taken into consideration . class nltk.corpus.reader.reviews . class nltk.corpus.reader.reviews . Reader for the Customer Review Data dataset by Hu , Liu ( 2004 ) . Note : we are not applying any sentence tokenization at the moment , just word tokenization . reviews ( ' Canon_G3 . We can also reach the same information directly from the stream : . features ( ' Canon_G3 . txt ' ) [ ( ' canon powershot g3 ' , ' +3 ' ) , ( ' use ' , ' +2 ' ) , ... ] . We can compute stats for specific product features : . features ( ' Canon_G3 . features ( ' Canon_G3 . Corpus reader for the Recognizing Textual Entailment ( RTE ) Challenge Corpora . The files were taken from the RTE1 , RTE2 and RTE3 datasets and the files were regularized . The latter are the gold standard annotated files . Each entailment corpus is a list of ' text'/'hypothesis ' pairs . The following example is taken from RTE3 : . The sale was made to pay Yukos ' US$ 27.5 billion tax bill , Yuganskneftegaz was originally sold for US$ 9.4 billion to a little known company Baikalfinansgroup which was later bought by the Russian state - owned oil company Rosneft . Baikalfinansgroup was sold to Rosneft . In order to provide globally unique IDs for each pair , a new attribute challenge has been added to the root element entailment - corpus of each file , taking values 1 , 2 or 3 . The GID is formatted ' m - n ' , where ' m ' is the challenge number and ' n ' is the pair ID . class nltk.corpus.reader.rte . Corpus reader for the SemCor Corpus . For access to the complete XML data structure , use the xml ( ) method . For access to simple word lists and tagged word lists , use words ( ) , sents ( ) , tagged_words ( ) , and tagged_sents ( ) . the given file(s ) as a list of tagged chunks , represented in tree form . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) the given file(s ) as a list of sentences . Each sentence is represented as a list of tagged chunks ( in tree form ) . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) The NLTK version of the Senseval 2 files uses well - formed XML . Each instance of the ambiguous words \\\" hard \\\" , \\\" interest \\\" , \\\" line \\\" , and \\\" serve \\\" is tagged with a sense identifier , and supplied with context . class nltk.corpus.reader.senseval . obj_score ( ) 0.125 . class nltk.corpus.reader.sentiwordnet . SentiSynset ( pos_score , neg_score , synset ) [ source ] \\u00b6 . Feng - Yi Chen , Pi - Fang Tsai , Keh - Jiann Chen , and Chu - Ren Huang ( 1999 ) The Construction of Sinica Treebank . Computational Linguistics and Chinese Language Processing , 4 , pp 87 - 104 . Huang Chu - Ren , Keh - Jiann Chen , Feng - Yi Chen , Keh - Jiann Chen , Zhao - Ming Gao , and Kuang - Yu Chen . Sinica Treebank : Design Criteria , Annotation Guidelines , and On - line Interface . Proceedings of 2nd Chinese Language Processing Workshop , Association for Computational Linguistics . Chen Keh - Jiann and Yu - Ming Hsieh ( 2004 ) Chinese Treebanks and Grammar Extraction , Proceedings of IJCNLP-04 , pp560 - 565 . class nltk.corpus.reader.sinica_treebank . class nltk.corpus.reader.switchboard . SwitchboardTurn ( words , speaker , i d ) [ source ] \\u00b6 . Bases : list . A specialized list object used to encode switchboard utterances . The elements of the list are the words in the utterance ; and two attributes , speaker and i d , are provided to retrieve the spearker identifier and utterance i d . Note that utterance ids are only unique within a given discourse . A corpus reader for the MAC_MORPHO corpus . Each line contains a single tagged word , using ' _ ' as a separator . Sentence boundaries are based on the end - sentence tag ( ' _ . ' ) Paragraph information is not included in the corpus , so each paragraph returned by self.paras ( ) and self.tagged_paras ( ) contains a single sentence . class nltk.corpus.reader.tagged . Reader for simple part - of - speech tagged corpora . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specified as parameters to the constructor . Words are parsed using nltk.tag.str2tuple . By default , ' / ' is used as the separator . I.e. , words should have the form : . word1/tag1 word2/tag2 word3/tag3 ... . But custom separators may be specified as parameters to the constructor . Part of speech tags are case - normalized to upper case . class nltk.corpus.reader.tagged . A specialized corpus view for tagged documents . It can be customized via flags to divide the tagged corpus documents up by sentence or paragraph , and to include or omit part of speech tags . TaggedCorpusView objects are typically created by TaggedCorpusReader ( not directly by nltk users ) . List of utterances in the corpus . There are total 160 utterances , each of which corresponds to a unique utterance of a speaker . Here 's an example of an utterance identifier in the list : . speakers . List of speaker IDs . An example of speaker ID : . dr1 - fvmh0 . Note that if you split an item ID with colon and take the first element of the result , you will get a speaker ID . The second element of the result is a sentence ID . dictionary ( ) . Phonetic dictionary of words contained in this corpus . This is a Python dictionary from words to phoneme lists . spkrinfo ( ) . Speaker information table . It 's a Python dictionary from speaker IDs to records of 10 fields . Speaker IDs the same as the ones in timie.speakers . Each record is a dictionary from field names to values , and the fields are as follows : . : unknown ) comments comments by the recorder . The 4 functions are as follows . Given a list of items , returns an iterator of a list of word lists , each of which corresponds to an item ( sentence ) . If offset is set to True , each element of the word list is a tuple of word(string ) , start offset and end offset , where offset is represented as a number of 16kHz samples . Given a list of items , returns an iterator of a list of phoneme lists , each of which corresponds to an item ( sentence ) . If offset is set to True , each element of the phoneme list is a tuple of word(string ) , start offset and end offset , where offset is represented as a number of 16kHz samples . Given an item , returns a chunk of audio samples formatted into a string . When the fuction is called , if start and end are omitted , the entire samples of the recording will be returned . If only end is omitted , samples from the start offset to the end of the recording will be returned . play(data ) . Play the given audio samples . The audio samples can be obtained from the timit.audiodata function . class nltk.corpus.reader.timit . Times . Lat0117 ' , ' Chinese_Mandarin - HZ ' , ' Marathi - UTF8 ' , ' Azeri_Azerbaijani_Cyrillic - Az . Times . Cyr . Normal0117 ' , ' Hungarian_Magyar - Unicode ' , ' Japanese_Nihongo - JIS ' , ' Vietnamese - VIQR ' , ' Amharic - Afenegus6 . A stream backed corpus view for corpus files that consist of sequences of serialized Python objects ( serialized using pickle.dump ) . One use case for this class is to store the result of running feature detection on a corpus to disk . This can be useful when performing feature detection is expensive ( so we do n't want to repeat it ) ; but the corpus is too large to store in memory . The following example illustrates this technique : . A ' view ' of a corpus file , which acts like a sequence of tokens : it can be accessed by index , iterated over , etc . However , the tokens are only constructed as - needed - the entire corpus is never stored in memory at once . The constructor to StreamBackedCorpusView takes two arguments : a corpus fileid ( specified as a string or as a PathPointer ) ; and a block reader . A \\\" block reader \\\" is a function that reads zero or more tokens from a stream , and returns them as a list . A very simple example of a block reader is : . readline ( ) . split ( ) . This simple block reader reads a single line at a time , and returns a single token ( consisting of a string ) for each whitespace - separated substring on the line . When deciding how to define the block reader for a given corpus , careful consideration should be given to the size of blocks handled by the block reader . Smaller block sizes will increase the memory requirements of the corpus view 's internal data structures ( by 2 integers per block ) . On the other hand , larger block sizes may decrease performance for random access to the corpus . ( But note that larger block sizes will not decrease performance for iteration . ) Internally , CorpusView maintains a partial mapping from token index to file position , with one entry per block . When a token with a given index i is requested , the CorpusView constructs it as follows : . First , it searches the toknum / filepos mapping for the token index closest to ( but less than or equal to ) i . Then , starting at the file position corresponding to that index , it reads one block at a time using the block reader until it reaches the requested token . The toknum / filepos mapping is created lazily : it is initially empty , but every time a new block is read , the block 's initial token is added to the mapping . ( Thus , the toknum / filepos map has one entry per block . ) In order to increase efficiency for random access patterns that have high degrees of locality , the corpus view may cache one or more blocks . Each CorpusView object internally maintains an open file object for its underlying corpus file . This file should be automatically closed when the CorpusView is garbage collected , but if you wish to close it manually , use the close ( ) method . If you access a CorpusView 's items after it has been closed , the file object will be automatically re - opened . Warning : . If the contents of the file are modified during the lifetime of the CorpusView , then the CorpusView 's behavior is undefined . Warning : . Variables : . _ block_reader - The function used to read a single block from the underlying file stream . _ toknum - A list containing the token index of each block that has been processed . In particular , _ toknum[i ] is the token index of the first token in block i . Together with _ filepos , this forms a partial mapping between token indices and file positions . _ filepos - A list containing the file position of each block that has been processed . In particular , _ toknum[i ] is the file position of the first character in block i . Together with _ toknum , this forms a partial mapping between token indices and file positions . _ stream - The stream used to access the underlying corpus file . _ len - The total number of tokens in the corpus , if known ; or None , if the number of tokens is not yet known . _ eofpos - The character position of the last character in the file . This is calculated when the corpus view is initialized , and is used to decide when the end of file has been reached . _ cache - A cache of the most recently read block . Close the file stream associated with this corpus view . This can be useful if you are worried about running out of file handles ( although the stream should automatically be closed upon garbage collection of the corpus view ) . If the corpus view is accessed after it is closed , it will be automatically re - opened . Concatenate together the contents of multiple documents from a single corpus , using an appropriate concatenation function . This utility function is used by corpus readers when the user requests more than one document at a time . nltk.corpus.reader.util . find_corpus_fileids ( root , regexp ) [ source ] \\u00b6 . nltk.corpus.reader.util . Read a sequence of tokens from a stream , where tokens begin with lines that match start_re . If end_re is specified , then tokens end with lines that match end_re ; otherwise , tokens end whenever the next line matching start_re or EOF is found . nltk.corpus.reader.util . Read a sequence of s - expressions from the stream , and leave the stream 's file position at the end the last complete s - expression read . This function will always return at least one s - expression , unless there are no more s - expressions in the file . If the file ends in in the middle of an s - expression , then that incomplete s - expression is returned when the end of the file is reached . From the VerbNet site : \\\" VerbNet ( VN ) ( Kipper - Schuler 2006 ) is the largest on - line verb lexicon currently available for English . It is a hierarchical domain - independent , broad - coverage verb lexicon with mappings to other lexical resources such as WordNet ( Miller , 1990 ; Fellbaum , 1998 ) , Xtag ( XTAG Research Group , 2001 ) , and FrameNet ( Baker et al . , 1998 ) . Return a list of the verbnet class identifiers . If a file identifier is specified , then return only the verbnet class identifiers for classes ( and subclasses ) defined by that file . If a lemma is specified , then return only verbnet class identifiers for classes that contain that lemma as a member . If a wordnetid is specified , then return only identifiers for classes that contain that wordnetid as a member . If a classid is specified , then return only identifiers for subclasses of the specified verbnet class . fileid_or_classid - An identifier specifying which class should be returned . Can be a file identifier ( such as ' put-9.1 . xml ' ) , or a verbnet class identifier ( such as ' put-9.1 ' ) or a short verbnet class identifier ( such as ' 9.1 ' ) . class nltk.corpus.reader.wordnet . Lemma ( wordnet_corpus_reader , synset , name , lexname_index , lex_id , syntactic_marker ) [ source ] \\u00b6 . Bases : nltk.corpus.reader.wordnet . _ WordNetObject . The lexical entry for a single morphological form of a sense - disambiguated word . Create a Lemma from a \\\" ... \\\" string where : is the morphological stem identifying the synset is one of the module attributes ADJ , ADJ_SAT , ADV , NOUN or VERB is the sense number , counting from 0 . is the morphological form of interest . Note that and can be different , e.g. the Synset ' salt.n.03 ' has the Lemmas ' salt.n.03 . salt ' , ' salt.n.03 . saltiness ' and ' salt.n.03 . salinity ' . Lemma attributes , accessible via methods with the same name : . - name : The canonical name of this lemma . - synset : The synset that this lemma belongs to . - syntactic_marker : For adjectives , the WordNet string identifying the . class nltk.corpus.reader.wordnet . Synset ( wordnet_corpus_reader ) [ source ] \\u00b6 . Bases : nltk.corpus.reader.wordnet . _ WordNetObject . Create a Synset from a \\\" . \\\" string where : is the word 's morphological stem is one of the module attributes ADJ , ADJ_SAT , ADV , NOUN or VERB is the sense number , counting from 0 . Synset attributes , accessible via methods with the same name : . name : The canonical name of this synset , formed using the first lemma of this synset . Note that this may be different from the name passed to the constructor if that string used a different lemma to identify the synset . pos : The synset 's part of speech , matching one of the module level attributes ADJ , ADJ_SAT , ADV , NOUN or VERB . Return the transitive closure of source under the rel relationship , breadth - first . closure ( hyp ) ) [ Synset('canine.n.02 ' ) , Synset('domestic_animal . n.01 ' ) , Synset('carnivore.n.01 ' ) , Synset('animal.n.01 ' ) , Synset('placental.n.01 ' ) , Synset('organism.n.01 ' ) , Synset('mammal.n.01 ' ) , Synset('living_thing . n.01 ' ) , Synset('vertebrate.n.01 ' ) , Synset('whole.n.02 ' ) , Synset('chordate.n.01 ' ) , Synset('object.n.01 ' ) , Synset('physical_entity . n.01 ' ) , Synset('entity.n.01 ' ) ] . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . A float score denoting the similarity of the two Synset objects , in the range 0 to 1 . Get a list of lowest synset(s ) that both synsets have as a hypernym . By setting the use_min_depth flag to True , the behavior of NLTK2 can be preserved . This was changed in NLTK3 to give more accurate results in a small set of cases , generally with synsets concerning people . ( eg : ' chef.n.01 ' , ' fireman.n.01 ' , etc . ) . This method is an implementation of Ted Pedersen 's \\\" Lowest Common Subsumer \\\" method from the Perl Wordnet module . It can return either \\\" self \\\" or \\\" other \\\" if they are a hypernym of the other . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( False by default ) creates a fake root that connects all the taxonomies . Set it to True to enable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will need to be added for nouns as well . use_min_depth ( bool ) - This setting mimics older ( v2 ) behavior of NLTK wordnet If True , will use the min_depth function to calculate the lowest common hypernyms . This is known to give strange results for some synset pairs ( eg : ' chef.n.01 ' , ' fireman.n.01 ' ) but is retained for backwards compatibility . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Returns the distance of the shortest path linking the two synsets ( if one exists ) . For each synset , all the ancestor nodes and their distances are recorded and compared . The ancestor node common to both synsets that can be reached with the minimum number of traversals is used . If no ancestor nodes are common , None is returned . If a node is compared with itself 0 is returned . n.01 ' ) , [ Synset('whole.n.02 ' ) , [ Synset('object.n.01 ' ) , [ Synset('physical_entity . n.01 ' ) , [ Synset('entity.n.01 ' ) ] ] ] ] ] ] ] ] ] ] ] ] ] , [ Synset('domestic_animal . n.01 ' ) , [ Synset('animal.n.01 ' ) , [ Synset('organism.n.01 ' ) , [ Synset('living_thing . n.01 ' ) , [ Synset('whole.n.02 ' ) , [ Synset('object.n.01 ' ) , [ Synset('physical_entity . n.01 ' ) , [ Synset('entity.n.01 ' ) ] ] ] ] ] ] ] ] ] . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . nltk.corpus.reader.wordnet . A table of strings that are used to express verb frames . class nltk.corpus.reader.wordnet . WordNetCorpusReader ( root , omw_reader ) [ source ] \\u00b6 . corpus ( CorpusReader ) - The corpus from which we create an information . content dictionary . : type weight_senses_equally : bool : param weight_senses_equally : If this is True , gives all possible senses equal weight rather than dividing by the number of possible senses . ( If a word has 3 synses , each sense gets 0.3333 per appearance when this is False , 1.0 when it is true . ) : param smoothing : How much do we smooth synset counts ( default is 1.0 ) : type smoothing : float : return : An information content dictionary . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Find a possible base form for the given form , with the given part of speech , by checking WordNet 's list of exceptional forms , and by recursively stripping affixes for this part of speech until a form in WordNet is found . morphy ( ' hardrock ' , wn . morphy ( ' book ' , wn . morphy ( ' book ' , wn . ADJ ) . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Load all synsets with a given lemma and part of speech tag . If no pos is specified , all synsets for all parts of speech will be loaded . If lang is specified , all the synsets associated with the lemma name of that language will be returned . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . Load an information content file from the wordnet_ic corpus and return a dictionary . This dictionary has just two keys , NOUN and VERB , whose values are dictionaries that map from synsets to information content values . nltk.corpus.reader.wordnet . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . nltk.corpus.reader.wordnet . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . nltk.corpus.reader.wordnet . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . A float score denoting the similarity of the two Synset objects , in the range 0 to 1 . nltk.corpus.reader.wordnet . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . nltk.corpus.reader.wordnet . Resnik Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) . nltk.corpus.reader.wordnet . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . A corpus view that selects out specified elements from an XML file , and provides a flat list - like interface for accessing them . ( Note : XMLCorpusView is not used by XMLCorpusReader itself , but may be used by subclasses of XMLCorpusReader . ) Every XML corpus view has a \\\" tag specification \\\" , indicating what XML elements should be included in the view ; and each ( non - nested ) element that matches this specification corresponds to one item in the view . Tag specifications are regular expressions over tag paths , where a tag path is a list of element tag names , separated by ' / ' , indicating the ancestry of the element . Some examples : . ' foo ' : A top - level element whose tag is foo . foo / bar ' : An element whose tag is bar and whose parent is a top - level element whose tag is foo . The view items are generated from the selected XML elements via the method handle_elt ( ) . By default , this method returns the element as - is ( i.e. , as an ElementTree object ) ; but it can be overridden , either via subclassing or via the elt_handler constructor parameter . context ( str ) - A string composed of element tags separated by forward slashes , indicating the XML context of the given element . For example , the string ' foo / bar / baz ' indicates that the element is a baz element whose parent is a bar element and whose grandparent is a top - level foo element . Corpus reader for the York - Toronto - Helsinki Parsed Corpus of Old English Prose ( YCOE ) , a 1.5 million word syntactically - annotated corpus of Old English prose texts . nltk.corpus.reader.ycoe . NLTK corpus readers . The modules in this package provide functions that can be used to read corpus fileids in a variety of formats . These functions can be used to read both the corpus fileids that are distributed in the NLTK corpus package , and corpus fileids that are part of external corpora . Each corpus module defines one or more \\\" corpus reader functions \\\" , which can be used to read documents from that corpus . These functions take an argument , item , which is used to indicate which document should be read from the corpus : . If item is one of the unique identifiers listed in the corpus module 's items variable , then the corresponding document will be loaded from the NLTK corpus package . If item is a fileid , then that file will be read . Additionally , corpus reader functions can be given lists of item names ; in which case , they will return a concatenation of the corresponding documents . Corpus reader functions are named based on the type of information they return . Some common examples , and their return types , are : . sents ( ) : list of ( list of str ) . paras ( ) : list of ( list of ( list of str ) ) . tagged_words ( ) : list of ( str , str ) tuple . tagged_sents ( ) : list of ( list of ( str , str ) ) . tagged_paras ( ) : list of ( list of ( list of ( str , str ) ) ) . chunked_sents ( ) : list of ( Tree w/ ( str , str ) leaves ) . parsed_sents ( ) : list of ( Tree with str leaves ) . parsed_paras ( ) : list of ( list of ( Tree with str leaves ) ) . xml ( ) : A single xml ElementTree . raw ( ) : unprocessed corpus contents . For example , to read a list of the words in the Brown Corpus , use nltk.corpus.brown.words ( ) : . join ( brown . words ( ) ) ) The , Fulton , County , Grand , Jury , said , ... . class nltk.corpus.reader . Bases : object . A base class for \\\" corpus reader \\\" classes , each of which can be used to read a specific corpus format . Each individual corpus reader instance is used to read a specific corpus , consisting of one or more files under a common root directory . Each file is identified by its file identifier , which is the relative path to the file from the root directory . A separate subclass is be defined for each corpus format . These subclasses define one or more methods that provide ' views ' on the corpus contents , such as words ( ) ( for a list of words ) and parsed_sents ( ) ( for a list of parsed sentences ) . Called with no arguments , these methods will return the contents of the entire corpus . For most corpora , these methods define one or more selection arguments , such as fileids or categories , which can be used to select which portion of the corpus should be returned . fileids ( None or str or list ) - Specifies the set of fileids for which paths should be returned . Can be None , for all fileids ; a list of file identifiers , for a specified set of fileids ; or a single file identifier , for a single file . Note that the return value is always a list of paths , even if fileids is a single file identifier . include_encoding - If true , then return a list of ( path_pointer , encoding ) tuples . Load this corpus ( if it has not already been loaded ) . This is used by LazyCorpusLoader as a simple method that can be used to make sure a corpus is loaded - e.g. , in case a user wants to do help(some_corpus ) . class nltk.corpus.reader . CategorizedCorpusReader ( kwargs ) [ source ] \\u00b6 . Bases : object . A mixin class used to aid in the implementation of corpus readers for categorized corpora . This class defines the method categories ( ) , which returns a list of the categories for the corpus or for a specified set of fileids ; and overrides fileids ( ) to take a categories argument , restricting the set of fileids to be returned . Call _ _ init _ _ ( ) to set up the mapping . Override all view methods to accept a categories parameter , which can be used instead of the fileids parameter , to select which fileids should be included in the returned view . Return a list of file identifiers for the files that make up this corpus , or that make up the given category(s ) if specified . class nltk.corpus.reader . Reader for corpora that consist of plaintext documents . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specificed as parameters to the constructor . This corpus reader can be customized ( e.g. , to skip preface sections of specific document formats ) by creating a subclass and overriding the CorpusView class variable . class nltk.corpus.reader . Reader for simple part - of - speech tagged corpora . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specified as parameters to the constructor . Words are parsed using nltk.tag.str2tuple . By default , ' / ' is used as the separator . I.e. , words should have the form : . word1/tag1 word2/tag2 word3/tag3 ... . But custom separators may be specified as parameters to the constructor . Part of speech tags are case - normalized to upper case . class nltk.corpus.reader . Reader for chunked ( and optionally tagged ) corpora . Paragraphs are split using a block reader . They are then tokenized into sentences using a sentence tokenizer . Finally , these sentences are parsed into chunk trees using a string - to - chunktree conversion function . Each of these steps can be performed using a default function or a custom function . By default , paragraphs are split on blank lines ; sentences are listed one per line ; and sentences are parsed into chunk trees using nltk.chunk.tagstr2tree . the given file(s ) as a list of paragraphs , each encoded as a list of sentences , which are in turn encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of sentences , each encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of tagged words and chunks . Words are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . Chunks are encoded as depth - one trees over ( word , tag ) tuples or word strings . A corpus reader for the MAC_MORPHO corpus . Each line contains a single tagged word , using ' _ ' as a separator . Sentence boundaries are based on the end - sentence tag ( ' _ . ' ) Paragraph information is not included in the corpus , so each paragraph returned by self.paras ( ) and self.tagged_paras ( ) contains a single sentence . class nltk.corpus.reader . Reader for the Alpino Dutch Treebank . This corpus has a lexical breakdown structure embedded , as read by _ parse Unfortunately this puts punctuation and some other words out of the sentence order in the xml element tree . This is no good for tag _ and word _ _ tag and _ word will be overridden to use a non - default new parameter ' ordered ' to the overridden _ normalize function . The _ parse function can then remain untouched . class nltk.corpus.reader . class nltk.corpus.reader . Reader for Europarl corpora that consist of plaintext documents . Documents are divided into chapters instead of paragraphs as for regular plaintext documents . Chapters are separated using blank lines . Everything is inherited from PlaintextCorpusReader except that : . Since the corpus is pre - processed and pre - tokenized , the word tokenizer should just split the line at whitespaces . For the same reason , the sentence tokenizer should just split the paragraph at line breaks . There is a new ' chapters ( ) ' method that returns chapters instead instead of paragraphs . The ' paras ( ) ' method inherited from PlaintextCorpusReader is made non - functional to remove any confusion between chapters and paragraphs for Europarl . Corpus reader for the propbank corpus , which augments the Penn Treebank with information about the predicate argument structure of every verb instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - verb basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . From the VerbNet site : \\\" VerbNet ( VN ) ( Kipper - Schuler 2006 ) is the largest on - line verb lexicon currently available for English . It is a hierarchical domain - independent , broad - coverage verb lexicon with mappings to other lexical resources such as WordNet ( Miller , 1990 ; Fellbaum , 1998 ) , Xtag ( XTAG Research Group , 2001 ) , and FrameNet ( Baker et al . , 1998 ) . Return a list of the verbnet class identifiers . If a file identifier is specified , then return only the verbnet class identifiers for classes ( and subclasses ) defined by that file . If a lemma is specified , then return only verbnet class identifiers for classes that contain that lemma as a member . If a wordnetid is specified , then return only identifiers for classes that contain that wordnetid as a member . If a classid is specified , then return only identifiers for subclasses of the specified verbnet class . fileid_or_classid - An identifier specifying which class should be returned . Can be a file identifier ( such as ' put-9.1 . xml ' ) , or a verbnet class identifier ( such as ' put-9.1 ' ) or a short verbnet class identifier ( such as ' 9.1 ' ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . stem - If true , then use word stems instead of word strings . class nltk.corpus.reader . A corpus reader for CoNLL - style files . These files consist of a series of sentences , separated by blank lines . Each sentence is encoded using a table ( or \\\" grid \\\" ) of values , where each line corresponds to a single word , and each column corresponds to an annotation type . The set of columns used by CoNLL - style files can vary from corpus to corpus ; the ConllCorpusReader constructor therefore takes an argument , columntypes , which is used to specify the columns that are used by a given corpus . @todo : Possibly add caching of the grid corpus view ? This would . allow the same grid view to be used by different data access methods ( eg words ( ) and parsed_sents ( ) could both share the same grid corpus view object ) . @todo : Better support for -DOCSTART- . Currently , we just ignore . it , but it could be used to define methods that retrieve a document at a time ( eg parsed_documents ( ) ) . corpus ( CorpusReader ) - The corpus from which we create an information . content dictionary . : type weight_senses_equally : bool : param weight_senses_equally : If this is True , gives all possible senses equal weight rather than dividing by the number of possible senses . ( If a word has 3 synses , each sense gets 0.3333 per appearance when this is False , 1.0 when it is true . ) : param smoothing : How much do we smooth synset counts ( default is 1.0 ) : type smoothing : float : return : An information content dictionary . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Find a possible base form for the given form , with the given part of speech , by checking WordNet 's list of exceptional forms , and by recursively stripping affixes for this part of speech until a form in WordNet is found . morphy ( ' hardrock ' , wn . morphy ( ' book ' , wn . morphy ( ' book ' , wn . ADJ ) . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Load all synsets with a given lemma and part of speech tag . If no pos is specified , all synsets for all parts of speech will be loaded . If lang is specified , all the synsets associated with the lemma name of that language will be returned . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . class nltk.corpus.reader . WordNetICCorpusReader ( root , fileids ) [ source ] \\u00b6 . Load an information content file from the wordnet_ic corpus and return a dictionary . This dictionary has just two keys , NOUN and VERB , whose values are dictionaries that map from synsets to information content values . class nltk.corpus.reader . Corpus reader for the nombank corpus , which augments the Penn Treebank with information about the predicate argument structure of every noun instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - noun basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . Corpus reader designed to work with corpus created by IPI PAN . The corpus includes information about text domain , channel and categories . You can access possible values using domains ( ) , channels ( ) and categories ( ) . The reader supports methods : words , sents , paras and their tagged versions . The IPIPAN Corpus contains tags indicating if there is a space between two tokens . As a result in place where there should be no space between two tokens new pair ( '' , ' no - space ' ) will be inserted ( for tagged data ) and just '' for methods without tags . The corpus reader can also try to append spaces between words . As a result either ' ' or ( ' ' , ' space ' ) will be inserted between tokens . By default , xml entities like \\\" and & are replaced by corresponding characters . In the pl196x corpus each category is stored in single file and thus both methods provide identical functionality . In order to accommodate finer granularity , a non - standard textids ( ) method was implemented . All the main functions can be supplied with a list of required chunks - giving much more control to the user . Corpus reader for the XML version of the CHILDES corpus . Copy the needed parts of the CHILDES XML corpus into the NLTK data directory ( nltk_data / corpora / CHILDES/ ) . For access to the file text use the usual nltk functions , words ( ) , sents ( ) , tagged_words ( ) and tagged_sents ( ) . the given file(s ) as a list of sentences or utterances , each encoded as a list of word strings . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of sentences , each encoded as a list of ( word , tag ) tuples . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of tagged words and punctuation symbols , encoded as tuples ( word , tag ) . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . Map a corpus file to its web version on the CHILDES website , and open it in a web browser . The complete URL to be used is : . If no urlbase is passed , we try to calculate it . This requires that the childes corpus was set up to mirror the folder hierarchy under childes.psy.cmu.edu/data-xml/ , e.g. : nltk_data / corpora / childes / Eng - USA / Cornell/ ? ? ? or nltk_data / corpora / childes / Romance / Spanish / Aguirre/ ? ? ? The function first looks ( as a special case ) if \\\" Eng - USA \\\" is on the path consisting of + fileid ; then if \\\" childes \\\" , possibly followed by \\\" data - xml \\\" , appears . If neither one is found , we use the unmodified fileid and hope for the best . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . class nltk.corpus.reader . Corpus reader for the SemCor Corpus . For access to the complete XML data structure , use the xml ( ) method . For access to simple word lists and tagged word lists , use words ( ) , sents ( ) , tagged_words ( ) , and tagged_sents ( ) . the given file(s ) as a list of tagged chunks , represented in tree form . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) the given file(s ) as a list of sentences . Each sentence is represented as a list of tagged chunks ( in tree form ) . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) lu ( 3238 ) . frame . lexUnit [ ' glint.v ' ] is fn . frame_by_name ( ' Replacing ' ) is fn . lus ( ' replace.v ' ) [ 0 ] . lus ( ' prejudice.n ' ) [ 0 ] . frame . frame_relations ( ' Partiality ' ) True . Details for a specific annotated document can be obtained using this class 's annotated_document ( ) function and pass it the value of the ' ID ' field . corpname for x in fn . name ( str ) - A regular expression pattern used to search the file name of each annotated document . The document 's file name contains the name of the corpus that the document is from , followed by two underscores \\\" _ _ \\\" followed by the document name . So , for example , the file name \\\" LUCorpus - v0.3__20000410_nyt - NEW . xml \\\" is from the corpus named \\\" LUCorpus - v0.3 \\\" and the document name is \\\" 20000410_nyt - NEW . xml \\\" . Lists frame element objects . If ' name ' is provided , this is treated as a case - insensitive regular expression to filter by frame name . ( Case - insensitivity is because casing of frame element names is not always consistent across frames . ) frame . name , fe . name ) for fe in fn . name for fe in fn . fes ( ' ^sound$ ' ) ) 2 . Get the details for the specified Frame using the frame 's name or i d number . Usage examples : . frame ( ' Imposing_obligation ' ) frame ( 1494 ) : Imposing_obligation ... . The dict that is returned from this function will contain the following information about the Frame : . ' name ' : the name of the Frame ( e.g. ' Birth ' , ' Apply_heat ' , etc . ) . ' definition ' : textual definition of the Frame . ' ID ' : the internal ID number of the Frame . ' semTypes ' : a list of semantic types for this frame . Each item in the list is a dict containing the following keys : . ' name ' : can be used with the semtype ( ) function . ' ID ' : can be used with the semtype ( ) function . ' lexUnit ' : a dict containing all of the LUs for this frame . The keys in this dict are the names of the LUs and the value for each key is itself a dict containing info about the LU ( see the lu ( ) function for more info . ) FE ' : a dict containing the Frame Elements that are part of this frame . The keys in this dict are the names of the FEs ( e.g. ' Body_system ' ) and the values are dicts containing the following keys . definition \\\" This frame includes words that name ... \\\" . Also see the frame ( ) function for details about what is contained in the dict that is returned . Get the details for the specified Frame using the frame 's name . definition \\\" This frame includes words that name ... \\\" . frame - ( optional ) frame object , name , or ID ; only relations involving . frame_relations ( fn . frames ( r ' ( ? A brief intro to Frames ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A Frame is a script - like conceptual structure that describes a particular type of situation , object , or event along with the participants and props that are needed for that Frame . For example , the \\\" Apply_heat \\\" frame describes a common situation involving a Cook , some Food , and a Heating_Instrument , and is evoked by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . . We call the roles of a Frame \\\" frame elements \\\" ( FEs ) and the frame - evoking words are called \\\" lexical units \\\" ( LUs ) . FrameNet includes relations between Frames . Several types of relations are defined , of which the most important are : . Inheritance : An IS - A relation . The child frame is a subtype of the parent frame , and each FE in the parent is bound to a corresponding FE in the child . An example is the \\\" Revenge \\\" frame which inherits from the \\\" Rewards_and_punishments \\\" frame . \"}",
        "_version_":1692670479883567104,
        "score":25.96673},
      {
        "id":"ac717a6a-2e7b-4a9f-ba89-425864ab9974",
        "_src_":"{\"url\": \"http://www.peoplesworld.org/toussaint-released-contract-battle-continues/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146600.56/warc/CC-MAIN-20160205193906-00255-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"HOW - TO GUIDE : Installing and running the Joshua Decoder . Note : these instructions are several years out of date . This document gives instructions on how to install and use the Joshua decoder . Joshua is an open - source decoder for parsing - based machine translation . Joshua uses the synchronous context free grammar ( SCFG ) formalism in its approach to statistical machine translation , and the software implements the algorithms that underly the approach . The Berkeley Aligner - this software is used to align words across sentence pairs in a bilingual parallel corpus . Word alignment takes place before extracting an SCFG . After you have downloaded the srilm tar file , type the following commands to install it : . mkdir srilm mv srilm.tgz srilm/ cd srilm/ tar xfz srilm.tgz make . If the build fails , please follow the instructions in SRILM 's INSTALL file . After you successfully compile SRILM , Joshua will need to know what directory it is in . You can type pwd to get the absolute path to the sirlm/ directory that you created . Once you 've figured out the path , set an SRILM environment variable by typing : . Where \\\" /path / to / srilm \\\" is replaced with your path . You 'll also need to set a JAVA_HOME environment variable . For Mac OS X this usually is done by typing : . These variables will need to be set every time you use Joshua , so it 's useful to add them to your . bashrc , . bash_profile or . profile file . Download and Install Joshua . Running ant will compile the Java classes and link in srilm . If everything works properly , you should see the message BUILD SUCCESSFUL . If you get a BUILD FAILED message , it may be because you have not properly set the paths to SRILM and JAVA_HOME , or because srilm was not compiled properly , as described above . For the examples in this document , you will need to set a JOSHUA environment variable : . Run the example model . To test to make sure that the decoder is installed properly , we 'll translate 5 sentences using a small translation model that loads quickly . The sentences that we will translate are contained in example / example . test.in . The small translation grammar contains 15,939 rules -- you can get the count of the number of rules by running gunzip -c example / example . The first part of the rule is the left - hand side non - terminal . The second and third parts are the right - hand side . The three numbers listed after each translation rules are negative log probabilities that signify , in order : . You can use the grammar to translate the test set by running . config.srilm \\\\ example / example . test.in \\\\ example / example . nbest.srilm.out . For those of you who are n't very familiar with Java , the arguments are the following : . -Xmx1 g -- this tells Java to use 1 GB of memory . -cp $ JOSHUA / bin -- this specifies the directory that contains the Java class files . joshua.decoder.JoshuaDecoder -- This is the class that is run . If you want to look at the the source code for this class , you can find it in src / joshua / decoder / JoshuaDecoder.java . example / example . config.srilm -- This is the configuration file used by Joshua . example / example . test.in -- This is the input file containing the sentences to translate . example / example . nbest.srilm.out -- This is the output file that the n - best translations will be written to . You can inspect the output file by typing head example / example . nbest.srilm.out . This file contains the n - best translations , under the model . The first 10 lines that you see above are 10 best translations of the first sentence . Each line contains 4 fields . To get the 1-best translations for each sentence in the test set without all of the extra information , you can run the following command : . nbest.srilm.out \\\\ example / example . nbest.srilm.out.1best . You cat then look at the 1-best output file by typing cat example / example . nbest.srilm.out.1best : . the goal of gene scientists is to provide diagnostic tools to found of the flawed genes , are still provide a to stop these genes treatments . If your translations are identical to the ones above then Joshua is installed correctly . With this small model , there are many untranslated words , and the quality of the translations is very low . In the next steps , we 'll show you how to train a model for a new language pair , using a larger training corpus that will result in higher quality translations . Once you 've gathered your data , you will need to do several preprocess steps : sentence alignment , tokenization , normalization , and subsampling . Sentence alignment . In this exercise , we 'll start with an existing sentence - aligned parallel corpus . Download this tarball , which contains a Spanish - Engish parallel corpus , along with a dev and a test set : data.tar.gz . The data tarball contains two training directories training/ , which includes a subset of the corpus , and full - training , which includes the full corpus . I strongly recommend staring with the smaller set , and building an end - to - end system with it , since many steps take a very long time on the full data set . You should debug on the smaller set to avoid wasting time . Tokenization . Joshua uses whitespace to delineate words . For many languages , tokenization can be as simple as separating punctation off as its own token . For languages like Chinese , which do n't put spaces around words , tokenization can be more tricky . For this example we 'll use the simple tokenizer that is released as part of the WMT . It 's located in the tarball under the scripts directory . To use it type the following commands : . en.tok . Normalization . After tokenization , we recommend that you normalize your data by lowercasing it . The system treats words with variant capitalization as distinct , which can lead to worse probability estimates for their translation , since the counts are fragmented . For other languages you might want to normalize the text in other ways . You can lowercase your tokenized data with the following script : . cat es - en / full - training / training . en.tok.lc cat es - en / full - training / training . es.tok.lc . Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . Although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . After tokenization and lowercasing , the file looks like this ( head -3 es - en / full - training / training . en.tok.lc ): . resumption of the session i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . You must preprocess your dev and test sets in the same way you preprocess your training data . Run the following commands on the data that you downloaded : . cat es - en / dev / news - dev2009 . es.tok.lc cat es - en / dev / news - dev2009 . en.tok.lc cat es - en / test / newstest2009 . es.tok.lc cat es - en / test / newstest2009 . en.tok.lc . Subsampling ( optional ) . Sometimes the amount of training data is so large that it makes creating word alignments extremely time - consuming and memory - intesive . We therefore provide a facility for subsampling the training corpus to select sentences that are relevant for a test set . es.tok.lc es - en / test / newstest2009 . You can see how much the subsampling step reduces the training data , by yping wc -lw es - en / full - training / training . tok.lc es - en / full - training / subsampled / subsample . tok.lc : . 1411589 39411018 training / training . en.tok.lc 1411589 41042110 training / training . es.tok.lc 671429 16721564 training / subsampled / subsample . en.tok.lc 671429 17670846 training / subsampled / subsample . es.tok.lc . Step 3 : Create word alignments . Before extracting a translation grammar , we first need to create word alignments for our parallel corpus . In this example , we show you how to use the Berkeley aligner . You may also use Giza++ to create the alignments , although that program is a little unwieldy to install . To run the Berkeley aligner you first need to set up a configuration file , which defines the models that are used to align the data , how the program runs , and which files are to be aligned . Here is an example configuration file ( you should create your own version of this file and save it as training / word - align . conf ): . # # word - align . conf # # ---------------------- # # This is an example training script for the Berkeley # # word aligner . In this configuration it uses two HMM # # alignment models trained jointly and then decoded # # using the competitive thresholding heuristic . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Training : Defines the training regimen # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # forwardModels MODEL1 HMM reverseModels MODEL1 HMM mode JOINT JOINT iters 5 5 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Execution : Controls output and program flow # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # execDir alignments create saveParams true numThreads 1 msPerLine 10000 alignTraining # # # # # # # # # # # # # # # # # # Language / Data # # # # # # # # # # # # # # # # # foreignSuffixes.tok.lc englishSuffixen.tok.lc . # Choose the training sources , which can either be directories or files that list files / directories trainSourcessubsampled/ . sentencesMAX . # 1-best output . competitiveThresholding . To run the Berkeley aligner , first set an environment variable saying where the aligner 's jar file is located ( this environment variable is just used for convenience in this document , and is not necessary for running the aligner in general : . You 'll need to create an empty directory called example / test . This is because the Berkeley aligner generally expects to test against a set of manually word - aligned data : . cd es - en / full - training/ mkdir -p example / test . After you 've created the word - align . config file , you can run the aligner with this command : . nohup java -d64 -Xmx10 g -jar $ BERKELEYALIGNER / berkeleyaligner . jar + + word - align . conf & . If the program finishes right away , then it probably terminated with an error . You can read the nohup.out file to see what went wrong . Common problems include a missing example / test directory , or a file not found exception . When you re - run the program , you will need to manually remove the alignments/ directory . When you are aligning tens of millions of words worth of data , the word alignment process will take several hours to complete . While it is running , you can skip ahead and complete step 4 , but not step 5 . Step 4 : Train a language model . Most translation models also make use of an n - gram language model as a way of assigning higher probability to hypothesis translations that look like fluent examples of the target language . Joshua provides support for n - gram language models , either through a built in data structure , or through external calls to the SRI language modeling toolkit ( srilm ) . To use large language models , we recommend srilm . If you successfully installed srilm in Step 1 , then you should be able to train a language model with the following command : . mkdir -p model / lm $ SRILM / bin / macosx64/ngram - count \\\\ -order 3 \\\\ -unk \\\\ -kndiscount1 -kndiscount2 -kndiscount3 \\\\ -text training / training . en.tok.lc \\\\ -lm model / lm / europarl . en.trigram.lm . ( Note : the above assumes that you are on a 64-bit machine running Mac OS X. If that 's not the case , your path to ngram - count will be slightly different . ) This will train a trigram language model on the English side of the parallel corpus . We use the . tok.lc file because it is important to have the input to the LM training be tokenized and normalized in the same way as the input data for word alignment and translation grammar extraction . The -order 3 tells srilm to produce a trigram language model . You can set this to a higher value , and srilm will happily output 4-gram , 5-gram or even higher order language models . The -kndiscount tells SRILM to use modified Kneser - Ney discounting as its smoothing scheme . Other smoothing schemes that are implemented in SRILM include Good - Turing and Witten - Bell . Given that the English side of the parallel corpus is a relatively small amount of data in terms of language modeling , it only takes a few minutes a few minutes to output the LM . The uncompressed LM is 144 megabytes large ( du -h europarl.en.trigram.lm ) . Step 5 : Extract a translation grammar . We 'll use the word alignments to create a translation grammar similar to the Chinese one shown in Step 1 . The translation grammar is created by looking for where the foreign language phrases from the test set occur in the training set , and then using the word alignments to figure out which foreign phrases are aligned . Create a suffix array index . To find the foreign phrases in the test set , we first create an easily searchable index , called a suffix array , for the training data . java -Xmx500 m -cp $ JOSHUA / bin/ \\\\ joshua.corpus.suffix_array.Compile \\\\ training / subsampled / subsample . es.tok.lc \\\\ training / subsampled / subsample . en.tok.lc \\\\ training / subsampled / training . en.tok.lc-es . tok.lc.align \\\\ model . This compiles the index that Joshua will use for its rule extraction , and puts it into a directory named model . Extract grammar rules for the dev set . The following command will extract a translation grammar from the suffix array index of your word - aligned parallel corpus , where the grammar rules apply to the foreign phrases in the dev set dev / news - dev2009 . es.tok.lc : . /model \\\\ mert / news - dev2009 . es.tok.lc.grammar.raw \\\\ dev / news - dev2009 . es.tok.lc & . Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u mert / news - dev2009 . es.tok.lc.grammar.raw \\\\ -o mert / news - dev2009 . es.tok.lc.grammar . You will also need to create a small \\\" glue grammar \\\" , in a file called model / hiero . glue that contains these rules that allow hiero - style grammars to reach the goal state : . Step 6 : Run minimum error rate training . After we 've extracted the grammar for the dev set we can run minimum error rate training ( MERT ) . MERT is a method for setting the weights of the different feature functions the translation model to maximize the translation quality on the dev set . Translation quality is calculated according to an automatic metric , such as Bleu . Our implementation of MERT allows you to easily implement some other metric , and optimize your paramters to that . There 's even a YouTube tutorial to show you how . A MERT configuration file . A separate file with the list of the feature functions used in your model , along with their possible ranges . Create a MERT configuration file . In this example we name the file mert / mert . config . Its contents are : . # # # MERT parameters # target sentences file name ( in this case , file name prefix ) -r dev / news - dev2009 . en.tok.lc -rps 1 # references per sentence -p mert / params . txt # parameter file -m BLEU 4 closest # evaluation metric and its options -maxIt 10 # maximum MERT iterations -ipi 20 # number of intermediate initial points per iteration -cmd mert / decoder_command # file containing commands to run decoder -decOut mert / news - dev2009 . output.nbest # file prodcued by decoder -dcfg mert / joshua . You can see a list of the other parameters available in our MERT implementation by running this command : . java -cp $ JOSHUA / bin joshua.zmert.ZMERT -h . Next , create a file called mert / params . txt that specifies what feature functions you are using in your mode . In our baseline model , this file should contain the following information : . Next , create a file called mert / decoder_command that contains the following command : . config \\\\ dev / news - dev2009 . es.tok.lc \\\\ mert / news - dev2009 . output.nbest . Next , create a configuration file for joshua at mert / joshua . config that contains the following : . es.tok.lc.grammar . glue . # lm config . # tm config . # pruning config . # nbest config . # remote lm server config , we should first prepare remote_symbol_tbl before starting any jobs . /voc.remote.sym . /remote.lm.server.list . # parallel deocoder : it can not be used together with remote lm . # # # # # # model weights . # lm order weight . lm 1.0 . # phrasemodel owner column(0-indexed ) weight . phrasemodel pt 0 1.4037585111897322 . phrasemodel pt 1 0.38379188013385945 . phrasemodel pt 2 0.47752204361625605 . # arityphrasepenalty owner start_arity end_arity weight . # arityphrasepenalty pt 0 0 1.0 . # arityphrasepenalty pt 1 2 -1.0 . # phrasemodel mono 0 0.5 . # wordpenalty weight . wordpenalty -2.721711092619053 . Finally , run the command to start MERT : . nohup java -cp $ JOSHUA / bin \\\\ joshua.zmert.ZMERT \\\\ -maxMem 1500 mert / mert . config & . While MERT is running , you can skip ahead to the first part of the next step and extract the grammar for the test set . Step 7 : Decode a test set . When MERT finishes , it will output a file mert / joshua . config . ZMERT.final that contains the news weights for the different feature functions . You can copy this config file and use it to decode the test set . Extract grammar rules for the test set . Before decoding the test set , you 'll need to extract a translation grammar for the foreign phrases in the test set test / newstest2009 . es.tok.lc : . /model \\\\ test / newstest2009 . es.tok.lc.grammar.raw \\\\ test / newstest2009 . es.tok.lc & . Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u test / newstest2009 . es.tok.lc.grammar.raw \\\\ -o test / newstest2009 . es.tok.lc.grammar . Once the grammar extraction has completed , you can edit the joshua.config file for the test set . cp mert / joshua . config . ZMERT.final test / joshua . config . es.tok.lc.grammar . After you have done that , you can decode the test set with the following command : . config \\\\ test / newstest2009 . es.tok.lc \\\\ test / newstest2009 . output.nbest . After the decoder has finished , you can extract the 1-best translations from the n - best list using the following command : . output.nbest \\\\ test / newstest2009 . output.1best . Step 8 : Recase and detokenize . You 'll notice that your output is all lowercased and has the punctuation split off . In order to make the output more readable to human beings ( remember us ? ) , it 'd be good to fix these problems and use proper punctuation and spacing . These are called recasing and detokenization , respectively . We can do recasing using SRILM , and can do detokenization with a perl script . To build a recasing model first train a language model on true cased English text : . $ SRILM / bin / macosx64/ngram - count \\\\ -unk \\\\ -order 5 \\\\ -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -kndiscount5 \\\\ -text training / training . en.tok \\\\ -lm model / lm / training . TrueCase.5gram.lm . Next , you 'll need to create a list of all of the alternative ways that each word can be capitalized . This will be stored in a map file that lists a lowercased word as the key and associates it with all of the variant capitalization of that word . Here 's an example perl script to create the map : . /usr / bin / perl # # truecase - map . cat training / training . map . Finally , recase the lowercased 1-best translation by running the SRILM disambig program , which takes the map of alternative capitalizations , creates a confusion network , and uses truecased LM to find the best path through it : . $ SRILM / bin / macosx / disambig \\\\ -lm model / lm / training . TrueCase.5gram.lm \\\\ -keep - unk \\\\ -order 5 \\\\ -map model / lm / true - case . map \\\\ -text test / mt09 . output.1best.recased . Where strip - sent - tags . perl is : . Step 9 : Score the translations . The quality of machine translation is commonly measured using the BLEU metric , which automatically compares a system 's output against reference human translations . You can score your output using the JoshuaEval class , Joshua 's built - in scorer : . en.output \\\\ -ref dev / dev2006 . en.small \\\\ -m BLEU 4 closest \"}",
        "_version_":1692668574655578112,
        "score":25.019676},
      {
        "id":"fe955458-f0ee-4906-8213-8c3e4de97bc5",
        "_src_":"{\"url\": \"http://eol.org/data_objects/22903368\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168076.20/warc/CC-MAIN-20160205193928-00173-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Implementation of the DMV+CCM Parser . Please let me know if you download this software . Write an e - mail to this address . Introduction . This package includes implementations of the CCM , DMV and DMV+CCM parsers from Klein and Manning ( 2004 ) , and code for testing them with the WSJ , Negra and Cast3LB corpora ( English , German and Spanish respectively ) . A detailed description of the parsers can be found in Klein ( 2005 ) . This work was done as part of the PhD in Computer Science I am doing at FaMAF , Universidad Nacional de Cordoba , Argentina , under the supervision of Gabriel Infante - Lopez , with a research fellowship from CONICET . About Version 0.2.0 . This version is aimed at reproducing some of the results from Klein and Manning ( 2004 ) . The implemented DMV and DMV+CCM models are the versions with the one - side - first constraint . We could n't reproduce yet the results for these models without the one - side - first constraint . The following table shows the performance of the models over the WSJ10 corpus : . DMV reaches the given values at the 20th iteration . CCM converges to the given values since the 40th iteration . The results for DMV+CCM reported in the table were arbitrarily picked from the 10th iteration . Dependencies . Installation and Configuration . Extract lq-nlp-commons.tar.gz and lq-dmvccm.tar.gz into a folder and add the new folders to the Python PATH . For instance : . Usage . Quickstart . To train ( with 10 iterations ) and test the CCM parser with the WSJ10 do the following ( replace ' my_wsj_path ' with the path to the WSJ corpus ) : . To parse a sentence with the resulting parser do something like this : . Parser Instantiation , Training and Testing . The WSJ10 corpus is used by default to train the parsers . The WSJ10 corpus is automatically extracted from the WSJ corpus . You must place the WSJ corpus in a folder named wsj_comb ( or edit lq - nlp - commons / wsj . py , or read below ) . For instance , to train ( with 10 iterations ) and test the CCM parser with the WSJ10 do the following : . To give the treebanks explicitly : . The Negra corpus must be in a file negra - corpus / negra - corpus2 . penn ( in Penn format ) , and the Cast3LB corpus in a folder named 3lb - cast ( also in Penn format ) . To use alternative locations for the treebanks use the parameter basedir when creating the object . For instance : . ( similarly for Negra and Cast3LB ) . When loaded for the first time , the extracted corpora are saved into files to avoid having to process the entire treebanks again . The files are saved in the NLTK data path ( nltk.data.path[0 ] ) , usually $ HOME / nltk_data , to be loaded in future instantiations of the treebanks . The DMV and DMV+CCM parsers are in the classes dmv . DMV and dmvccm . DMVCCM . The current implementation of DMV+CCM has the one - side - first constraint . They are used in the same way CCM is . For example : . Take into account that training DMV and DMV+CCM is much slower than training CCM . A single training step can take more than 20 minutes . Parser Usage . Once you have a trained instance of CCM , DMV or DMV+CCM , you can parse sentences with the parse ( ) method . You may give a list of words or an instance of the sentence . Sentence class from lq - nlp - commons . The parse ( ) method returns a pair ( b , p ) , where b is an instance of bracketing . Bracketing and p is the probability of the bracketing . You can get the set of brackets from b.brackets or convert the bracketing to a tree with b.treefy ( ) . For instance : . In the case of DMV and DMV+CCM you can also use the method dep_parse ( ) to get the dependency structure parsed by these models ( Klein , 2005 ) . For instance : . References . Franco M. Luque . Una Implementaci\\u00f3n del Modelo DMV+CCM para Parsing No Supervisado ( 2011 ) . 2do Workshop Argentino en Procesamiento de Lenguaje Natural , C\\u00f3rdoba , Argentina . Klein , D. and Manning , C. D. ( 2004 ) . Corpus - based induction of syntactic structure : Models of dependency and constituency . In ACL , pages 478 - 485 . Klein , D. ( 2005 ) . The Unsupervised Learning of Natural Language Structure . PhD thesis , Stanford University . \"}",
        "_version_":1692669796068360192,
        "score":24.720118},
      {
        "id":"bfdc7bfe-8f1f-4d1c-8639-3037543b201e",
        "_src_":"{\"url\": \"https://www.behance.net/gallery/YWFT-Mimic/6892595\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153585.76/warc/CC-MAIN-20160205193913-00244-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"We present a method for recognizing semantic roles for Spanish sentences . If a complete parse can not be produced , a partial structure is built with some ( if not all ) dependency relations identified . Evaluation shows that in spite of its simplicity , the parser 's accuracy is superior to the available existing parsers for Spanish . A particularly interesting ambiguity which we have decided to analyze deeper , is the Prepositional Phrase Attachment Disambiguation . The system uses an ordered set of simple heuristic rules for determining iteratively the relationships between words to which a governor has not been yet assigned . For resolving certain cases of ambiguity we use cooccurrence statistics of words collected previously in an unsupervised manner , whether it be from big corpora , or from the Web ( through a search engine such as Google ) . Collecting these statistics is done by using Selectional Preferences . In order to evaluate our system , we developed a Method for Converting a Gold Standard from a constituent format to a dependency format . Additionally , each one of the modules of the system ( Selectional Preferences Acquisition and Prepositional Phrase Attachment Disambiguation ) , is evaluated in a separate and independent way to verify that they work properly . Finally we present some Applications of our system : Word Sense Disambiguation and Linguistic Steganography . Keywords : dependency parsing , pp attachment disambiguation , constituent to dependency conversion , heuristic rules , hybrid parser , selectional preferences . Resumen . Se presenta un m\\u00e9todo para reconocer los roles sem\\u00e1nticos de las oraciones en espa\\u00f1ol , es decir , identificar el papel que tiene cada uno de los elementos de la oraci\\u00f3n . Si no se puede producir un an\\u00e1lisis completo , se construye una estructura parcial con algunas ( si no todas ) relaciones de dependencia identificadas . La evaluaci\\u00f3n muestra que a pesar de su simplicidad , la precisi\\u00f3n del analizador es superior a aquella de los analizadores existentes actuales para el espa\\u00f1ol . A pesar de que ciertas reglas gramaticales y los recursos l\\u00e9xicos usados son espec\\u00edficos para el espa\\u00f1ol , el enfoque sugerido es independiente del lenguaje . Una ambig\\u00fcedad interesante que hemos decidido analizar a mayor profundidad , es la desambiguaci\\u00f3n de sintagma preposicional . El sistema usa un conjunto ordenado de reglas heur\\u00edsticas simples para determinar iterativamente las relaciones entre palabras para las cuales no se les ha asignado a\\u00fan un gobernante . Estas estad\\u00edsticas han sido obtenidas previamente de una manera no supervisada , ya sea a partir de grandes corpus de texto , o a trav\\u00e9s de Internet ( a trav\\u00e9s de un motor de b\\u00fasqueda como Google ) . Para evaluar este sistema , desarrollamos un m\\u00e9todo para convertir un est\\u00e1ndar existente , de un formato de constituyentes a un formato de dependencias . Adicionalmente , cada uno de los m\\u00f3dulos del sistema ( Adquisici\\u00f3n de Preferencias de Selecci\\u00f3n , Desambiguaci\\u00f3n de Sintagma Preposicional ) se eval\\u00faa de una forma separada e independiente para verificar su correcto funcionamiento . Finalmente , presentamos algunas aplicaciones de nuestro sistema : Desambiguaci\\u00f3n de sentidos de palabras y Estaganograf\\u00eda ling\\u00fc\\u00edstica . Palabras clave : an\\u00e1lisis de dependencias , desambiguaci\\u00f3n de frase preposicional , conversi\\u00f3n de constituyentes a dependencias , reglas heur\\u00edsticas , analizador sint\\u00e1ctico h\\u00edbrido , preferencias de selecci\\u00f3n . Agirre E. , D. Mart\\u00ednez . Unsupervised WSD based on automatically retrieved examples : The importance of bias . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , EMNLP , Barcelona , Spain , 2004 . [ Links ] . Agirre , E. D. Martinez . [ Links ] . Agirre , E. , D. Martinez . Integrating selectional preferences in WordNet . [ Links ] . Apresyan , Yuri D. , Igor Boguslavski , Leonid Iomdin , Alexandr Lazurski , Nikolaj Pertsov , Vladimir Sannikov , Leonid Tsinman . Moscow , Nauka , 1989 . [ Links ] . Bolshakov , Igor A. [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . A Very Large Database of Collocations and Semantic Links . Proc . Conf . [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . On Detection of Malapropisms by Multistage Collocation Testing . Conf . on Application of Natural Language to Information Systems . [ Links ] . In Proceedings of the 6 th Applied Natural Language Processing Conference , Seattle , Washington , USA , 2000 . [ Links ] . Brants , Thorsten . In : Proc . [ Links ] . Brill , Eric , Philip Resnik . [ Links ] . Briscoe , Ted . John Carroll , Jonathan Graham and Ann Copestake . Relational evaluation schemes . In : Procs . [ Links ] . Calvo , Hiram , Alexander Gelbukh . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Improving Prepositional Phrase Attachment Disambiguation Using the Web as Corpus , In A. Sanfeliu and J. Shulcloper ( Eds . ) Calvo , Hiram , Alexander Gelbukh . Natural Language Interface Framework for Spatial Object Composition Systems . Procesamiento de Lenguaje Natural 31 , 2003 . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Acquiring Selectional Preferences from Untagged Text for Prepositional Phrase Attachment Disambiguation . In : Proc . [ Links ] . Calvo , Hiram . Alexander Gelbukh , Adam Kilgarriff . Distributional Thesaurus versus WordNet : A Comparison of Backoff Techniques for Unsupervised PP Attachment . [ Links ] . Carreras , Xavier , Isaac Chao , Lluis Padr\\u00f3 , Muntsa Padr\\u00f3 . Proc . 4 th Intern . Conf . [ Links ] . Carroll , J. , D. McCarthy . Word sense disambiguation using automatically acquired verbal preferences . [ Links ] . Chomsky , Noam . Syntactic Structures . The Hague : Mouton & Co , 1957 . [ Links ] . Civit , Montserrat , and Maria Ant\\u00f2nia Mart\\u00ed . Est\\u00e1ndares de anotaci\\u00f3n morfosint\\u00e1ctica para el espa\\u00f1ol . Workshop of tools and resources for Spanish and Portuguese . IBERAMIA 04 , Mexico , 2004 . [ Links ] . Copestake , Ann , Dan Flickinger , Ivan A. Sag . Minimal Recursion Semantics . An introduction . CSLI , Stanford University , 1997 . [ Links ] . In : Recent Advances in Dependency Grammar . Proc . D\\u00edaz , Isabel , Lidia Moreno , Inmaculada Fuentes , Oscar Pastor . In : Alexander Gelbukh ( ed . ) [ Links ] . Dik , Simon C. , The Theory of Functional Grammar . Part I : The structure of the clause . Dordrecht , Foris , 1989 . [ Links ] . Dirk , L\\u00fcdtke , Satoshi Sato . [ Links ] . Gelbukh , A. , G. Sidorov , L. Chanona . Corpus virtual , virtual : Un diccionario grande de contextos de palabras espa\\u00f1olas compilado a trav\\u00e9s de Internet . In : Julio Gonzalo , Anselmo Pe\\u00f1as , Antonio Ferr\\u00e1ndez , eds . : Proc . [ Links ] . Gelbukh , A. , S. Torres , H. Calvo . Transforming a Constituency Treebank into a Dependency Treebank . Submitted to Procesamiento del Lenguaje Natural No . 34 , Spain , 2005 . [ Links ] . Gelbukh , Alexander , Grigori Sidorov , Francisco Vel\\u00e1squez . An\\u00e1lisis morfol\\u00f3gico autom\\u00e1tico del espa\\u00f1ol a trav\\u00e9s de generaci\\u00f3n . [ Links ] . Gladki , A. V. Syntax Structures of Natural Language in Automated Dialogue Systems ( in Russian ) . Moscow , Nauka , 1985 . [ Links ] . Kudo , T. , Y. Matsumoto . Use of Support Vector Learning for Chunk Identification . [ Links ] . Lara , Luis Fernando . Diccionario del espa\\u00f1ol usual en M\\u00e9xico . Digital edition . Colegio de M\\u00e9xico , Center of Linguistic and Literary Studies , 1996 . [ Links ] . [ Links ] . Mel'cuk , Igor A. Dependency Syntax : Theory and Practice . State U. Press of NY , 1988 . [ Links ] . Mel'cuk , Igor A. Lexical Functions : A Tool for the Description of Lexical Relations in the Lexicon . In : L. Wanner ( ed . ) [ Links ] . [ Links ] . Monedero , J. , Gonz\\u00e1lez , J. Go\\u00f1i , C. Iglesias , A. Nieto . Obtenci\\u00f3n autom\\u00e1tica de marcos de subcategorizaci\\u00f3n verbal a partir de texto etiquetado : el sistema SOAMAS . [ Links ] . Text Mining at Detail Level Using Conceptual Graphs . In : Uta Priss et al . ( Eds . ) : Conceptual Structures : Integration and Interfaces , 10 th Intern . Conf . [ Links ] . Information Retrieval with Conceptual Graph Matching . Proc . Conf . [ Links ] . Evaluation of TnT Tagger for Spanish . In Proc . [ Links ] . Pollard , Carl , and Ivan Sag . University of Chicago Press , Chicago , IL and London , UK , 1994 . [ Links ] . Prescher , Detlef , Stefan Riezler , and Mats Rooth . In Proceedings of the 18th International Conference on Computational Linguistics , Saarland University , Saarbr\\u00fccken , Germany , 2000 . [ Links ] . Ratnaparkhi , Adwait , Jeff Reynar , and Salim Roukos . A Maximum Entropy Model for Prepositional Phrase Attachment . [ Links ] . [ Links ] . Resnik , P. Selectional preference and sense disambiguation , ACL SIGLEX Workshop on Tagging Text with Lexical Semantics : Why , What , and How ? [ Links ] . Ph.D. Thesis , University of Pennsylvania , December , 1993 . [ Links ] . Sag , Ivan , Tom Wasow , and Emily M. Bender . Syntactic Theory . A Formal Introduction ( 2nd Edition ) . CSLI Publications , Stanford , CA , 2003 [ Links ] . Sebasti\\u00e1n , N. , M. A. Mart\\u00ed , M. F. Carreiras , and F. Cuestos . LEXESP , l\\u00e9xico informatizado del espa\\u00f1ol , Edicions de la Universitat de Barcelona , 2000 . [ Links ] . Sowa , John F. 1984 . Conceptual Structures : Information Processing in Mind and Machine . [ Links ] . Steele , James ( ed . ) Linguistics , Lexicography , and Implications . Ottawa : Univ . of Ottawa Press , 1990 . [ Links ] . Su\\u00e1rez , A. , M. Palomar . [ Links ] . Tapanainen , Pasi . Academic Dissertation . University of Helsinki , Language Technology , Department of General Linguistics , Faculty of Arts , 1999 . [ Links ] . Tesni\\u00e8re , Lucien . El\\u00e9ments de syntaxe structurale . Paris : Librairie Klincksieck , 1959 . [ Links ] . Volk , Martin . Exploiting the WWW as a corpus to resolve PP attachment ambiguities . In Proceeding of Corpus Linguistics 2001 . Lancaster , 2001 . [ Links ] . Weinreich , Uriel . Explorations in Semantic Theory , Mouton , The Hague , 1972 . [ Links ] . Yarowsky , D. , Hierarchical decision lists for word sense disambiguation . [ Links ] . Yarowsky , D. , S. Cucerzan , R. Florian , C. Schafer , R. Wicentowski . [ Links ] . Yuret , Deniz . Discovery of Linguistic Relations Using Lexical Attraction , PhD thesis , MIT , 1998 . [ Links ] \"}",
        "_version_":1692671237020450816,
        "score":24.232267}]
  }}
