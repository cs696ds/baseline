{
  "responseHeader":{
    "status":0,
    "QTime":3,
    "params":{
      "q":"systems dialogue knowledge number current approaches semantic litman learning mitkov taskar speech domain recent recently interest resolution shown student proposed",
      "fl":"*,score"}},
  "response":{"numFound":1441973,"start":0,"maxScore":20.209232,"numFoundExact":true,"docs":[
      {
        "id":"750ce92b-ca0b-4ffb-9a19-352c3f7aa53e",
        "_src_":"{\"url\": \"http://newint.org/columns/exposure/2009/01/01/cardeal/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156448.92/warc/CC-MAIN-20160205193916-00042-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Buy Antivert Without Prescription \\\" Approved Online Pharmacy . Conversation Panel 1 will be held on Monday , July 14 , 15:30 to 17:30 , in the Reception Room . The topic is on globalization and localization . The trigger question to start the conversation will be ... . What can we expect in social systems and economies as the world simultaneously seems to be becoming global ( with free trade , information and communication technologies ) and becoming local ( as supplies of energy , soil and water have become stressed ) ? Conversation Panel 2 will be held on Tuesday , July 15 , 15:30 to 17:30 , in the Reception Room . The topic is on information revolution / services revolution in business . The trigger question to start the conversation will be ... . How much have learned about the \\\" neweconomy \\\" associated with the \\\" information revolution \\\" or \\\" services revolution \\\" , and what do n't we yet know ? The first paper session of the Special Integration Group on Systems Applications in Business and Industry will discuss two papers on the theme of dialogue . Gary Metcalf , \\\" Dialogue and Ecological Engineering in Social Systems Design \\\" . A number of systems theorists and practitioners have described ways in which human systems of thought and interaction might be consciously designed . Banathy ( 1996 ) specifically proposed approaches to the design of human social systems through conversation and dialogue . More recently , Allen , et al , ( 2003 ) have proposed distinctions between environmental engineering and ecological engineering , which offer valuable insights into some of the difficulties inherent in the design of human systems . This paper will explore ways in which dialogue in the development of social systems might be related to ecological engineering in biological eco - systems , and how the design of organizational and other social systems might be better understood . David Ing , \\\" Business Models and Evolving Economic Paradigms : A Systems Science Approach \\\" . For professionals at the beginning of the 21st century , much of the conventional wisdom on business management and engineering is founded in the 20th century industrial / manufacturing paradigm . In developed economies , however , the service sector now dominates the manufacturing sector , just as manufacturing prevailed over the agricultural sector after the industrial revolution . Simultaneously , as end products have transitioned from material outputs to information in digital form , traditional business models are under siege . The economic sociology in this new world challenges the integrity of models , methods and interventions successful in an earlier paradigm . Since 2005 , IBM has encouraged universities to develop a new field of Services Science , Management and Engineering ( SSME ) . Researchers are responding with development of a new science of service systems , but mature foundations will require years of collaboration . In the absence of a well - established science from which educational curricula can be deduced , teachers can develop educational programs for joint learning , guided inductively by relevance and pragmatism . A new course on business models - ways in which business organizations operate and evolve - is proposed . Complementing traditional management and/or engineering curricula , this course challenges students to reconsider contexts , surface assumptions and explore alternative approaches to business . With a domain that includes both human and technological parts , systems science serves as a skeleton on which content can be structured . The second paper session of the of the Special Integration Group on Systems Applications in Business and Industry has three papers . Unfortunately , Marianne Kosits will not be present to discuss her article on \\\" Meaningful Measurement in the Contemporary Enterprise \\\" , as she was called away for a business meeting in China . I had previously discussed a dialogue with Doug McDavid , and he has agreed to take advantage of this open slot . Doug McDavid will speak on \\\" A Viewpoint on Business Architecture \\\" . He had posted an article on his blog , beginning with ... . We are hearing more and more about the concept of business architecture these days , as a specialized professional area of expertise . What follows is a personal viewpoint , developed over a number of years of study and experience . In the context of growing interest , it is fair to ask , \\\" What is the specialization that is represented by business architecture ? \\\" In answer to that question , I like to turn the phrase around and ask , \\\" What do we mean by the architecture of a business ? \\\" The second and third papers in this session have themes related to measurement and organizations : . Kambiz E. Maani and Annie Fan , \\\" Systems Thinking for Team and Organisational learning : Case of Performance Measure Conflicts in a Multinational Supply Chain \\\" . Performance measurement and management have received great deal of attention in the literature in recent years . However to date , there is scant attention to dynamics and trade - offs amongst performance indicators in theory and in practice ( Santos , Belton et al . 2002 ) . Thus , performance management systems ( PMS ) have remained static , fragmented , and backward looking ( Bourne et al . 2000 ) leading to adverse outcomes , often unknown to managers and organisations . A systems view of performance , on the other hand , calls for a holistic approach to performance measurement integrating multiple dimensions , functions and time horizons across the enterprise . A systemic performance measurement would take into account the interdependencies of functions and their dynamic influence on the performance of the organisation as a whole . This paper addresses this challenge using the four level thinking ( Senge , 1991 ) and causal loop models to highlight the inter - relationships between the KPIs and their trade - offs within and across different functions . The study reports on an action research within a multinational company where through real case scenarios we demonstrate how KPIs influence , contribute or impede one another in a manufacturing / supply chain setting . The paper reveals how the use of systems thinking concepts and causal loop models by novice users facilitated an open environment for cross - functional communication and collaborations , leading to team and organisational learning and enhanced performance . Junya Minegishi , Andreas Gehrmann , Yoshimitsu Nagai , Syohei Ishizu , \\\" Improving the usability of Ontology based Audit Support System \\\" . Auditing against Generic Management System requirements , like requirements of ISO 9001 , is an established means for evaluating organizational capabilities . In ISO 9001 , auditors check individual management systems based on generic management system standards . Auditors faced with semantic problems because they must interpret the meaning of individual complex management system from the stand point of generic management system standards . To solve this semantic problem , audit support system has been developed using ontology editor . However the audit support system is not widespread , because the ontology editor is so complex . In ontology editor Prot\\u00e9g\\u00e9 , too many functions for the ontology operations are provided . The main objective of this paper is to develop a new audit support plug - in system , which supports auditors who do n't know about ontology concepts will be able to solve the semantic problems . In this paper , first we analyze complexity of conventional audit support system . Next , we construct plug - in system that is customized in audit by the use of prot\\u00e9g\\u00e9 plug - in function . In addition , we demonstrate the use of audit support system following the typical audit activities . The third paper session of the of the Special Integration Group on Systems Applications in Business and Industry has three papers . on the theme of learning and design . John Pourdehnad , \\\" Idealized Design : An \\\" Open Innovation \\\" Process for Successful Business Model Creation \\\" . In industry after industry , companies with superior performance are displaying innovation in the totality of the way they are doing business . This explains why a recent IBM survey of over 765 CEOs shows : Business Model Innovation is on the top of their list . Traditional models of innovation , which relied solely on \\\" creative types , \\\" usually within R&D functions or strategic planning function , are being replaced with \\\" open innovation \\\" . One of the most potent open innovation processes , is idealized design . Originally conceived as an internal process to facilitate corporate planning , idealized \\\" design thinking \\\" is now being used for opportunity recognition . In this paper , the operating principles of idealized design as an open innovation process together with the Enterprise 2.0 , a system wide enabling technology that facilitates participation , is discussed . Takafumi Nakamura and Kyoichi Kijima , \\\" Failure of foresight : Learning from system failures through dynamic model \\\" . A dynamic model for holistically examining system failures is proposed , for the purpose of preventing further occurrence of these failures . An understanding system failure correctly is crucial to preventing further occurrence of system failures . Quick fixes can even damage organizational performance to a level worse than the original state . There is well known side effect of \\\" normalized deviance \\\" which leads NASA 's Challenger and Columbia space shuttle disasters . And there is so called \\\" incubation period \\\" which leads to catastrophic system failures in the end . However this indicates there is a good chance to avoid catastrophic system failures if we can sense the incubation period correctly and respond the normalized deviance effect properly . If we do n't understand system failure correctly , we ca n't solve it effectively . Therefore we first define three failure classes to treat dynamic aspects of system failures . They are Class 1 ( Failure of deviance ) , Class 2 ( Failure of interface ) and Class 3 ( Failure of foresight ) respectively . Then we propose a dynamic model to understand system failure dynamically through turning hindsight to foresight to prevent further occurrence . An application example in IT engineering demonstrates that the proposed model proactively promotes double loop learning from previous system failures . Shankar Sankaran , \\\" Incorporating Systems Thinking in Organizational Change Projects using Action Research by Practitioners Conducting Academic Research \\\" . This paper will first explore the use systems thinking in action research projects . It will then describe three ' real ' action research projects , where systems thinking processes were used by managers who conducted action research , to introduce change in their own organizations . It will elaborate how applying systems thinking principles supported the application of action research . All three managers have successfully completed their doctorates in programs conducted by an Australian University . The paper will then discuss the merits and problems in applying systems thinking in action research projects and conclude with how systems thinking approaches could be effectively applied by management researchers planning to conduct academic research . The principal author of this paper was involved in the supervision of the doctoral research of the projects discussed in this paper . The three managers are being contacted for participating in writing this paper . The fourth paper session of the of the Special Integration Group on Systems Applications in Business and Industry has three papers on systems science bringing new perspectives to knotty issues . Allenna Leonard , \\\" Symbiosis as a Metaphor for Sustainability Practice in Human Affairs \\\" . This concept paper is an exploration of various symbiotic relationships and their potential relevance for the organization and conduct of human affairs . Many types of symbiosis exist : between plants , between plant and animal life and between different animals . They contribute to protection and defense , cleaning , reproduction , nutrition , transportation and illumination . Some symbiots are so tightly coupled that they are not able to exist , or exist in the same form , separately . Others can exist separately but they are less viable alone than together . Still others benefit from but do not depend upon the relationship . All seem to provide complementary features and strengths that either enhance the success and well being of both or impose a bearable burden on the non - advantaged partner . We are seeking , and none too soon , new ways to make a difference in the achievement of sustainable relationships in human society and organizations and between human activity and the natural environment . A broader and deeper appreciation of symbiosis in the general public and among researchers in different disciplines may make a contribution to both innovation and a more effective application of existing knowledge and tools . Jerome Galbrun , Kyoichi Kijima , \\\" Growth Strategy and Hierarchy Theory : Emergence of Super - players in the Healthcare Computed Tomography Oligopoly \\\" . This paper examines how firms discover effective strategic positions in a business technology - driven oligopoly context ( limited players , no possible entrant and rapid technological change ) . In such settings , neither rational deduction nor local search is likely to lead a firm to a successful growth : firms escalate by launching new products faster , developing new services or acquiring new capabilities . Demonstrating the complexity of the business oligopoly , however , allows us to define the emergence of a new type of players , \\\" super - player \\\" , able to write a new set of rules and to substantially influence the industry for a given period of time . With respect to the Hierarchy Theory , we find the attributes of context changing , filtering information and simplifying multilevel business systems for this \\\" super - player \\\" . Abraham Briones - Juarez , Ricardo Tejeida - Padilla , Oswaldo Morales - Matamoros , \\\" A Soft Systems Methodology approach to Design a Restaurant Management Model for a Great Tourism Hotel \\\" . This paper is about the design of a systemic model used in restaurants ' management inside the hotels of Great Tourism category in Mexico City , applied to the Restaurant the Gifts of the Hotel Sheraton Historical Center . The designed model was conceptually defined with the restructuring of the information flows , the reorganization of the restaurant 's organizational structure and the view of the elements that affect the system in its intern and external environments . \"}",
        "_version_":1692668839197671424,
        "score":20.209232},
      {
        "id":"e6653f80-e7ea-43ca-bcd0-5f8096ba87ac",
        "_src_":"{\"url\": \"http://sachachua.com/blog/category/life/health/page/2/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166739.77/warc/CC-MAIN-20160205193926-00098-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Interoperability of Adaptive Learning Components As interoperability is one of the crucial problems in this area , we analyzed it in D1.2 ( published also as journal paper ) . Personalized adaptive learning requires semantic - based and context - aware systems to manage the Web knowledge efficiently as well as to achieve semantic interoperability between heterogeneous information resources and services . The technological and conceptual differences can be bridged either by means of standards or via approaches based on the Semantic Web . This documents d Author(s ) : Kravcik Milos , Aroyo Lora , Dolog Peter , Houben Geert- . Investigation of Q - Learning in the Context of a Virtual Learning Environment We investigate the possibility to apply a known machine learning algorithm of Q - learning in the domain of a Virtual Learning Environment ( VLE ) . It is important in this problem domain to have algorithms that learn their optimal values in a rather short time expressed in terms of the iteration number . The problem domain is a VLE in which an agent plays a role of the teacher . With time it moves to different states and makes decisions which regarding action to choose for moving from current state to Author(s ) : Baziukaite Dalia . The IPO ( Input - Process - Output ) relationships are proposed to address this issue . The formulation of IPO relationship are described in detail and the application of IPO relationship is shown through the case study of specifying the requirements of an a Author(s ) : Salim Siti Salwah , Marzuki Normala , Kasirun Zarinah . Intelligent Computer Teacher in E - Learning Systems Intelligent computer teacher and appropriate simulated learning environment were elaborated ensuring high quality individualization of learning process . Individualization of learning process is realized in two ways . Learners can use node points structure for selection of their own learning path ensuring individualization of learning process . Intelligent computer teacher ( by the use of artificial intelligence means ) can adapt to learners knowledge and abilities ensuring also individualization of Author(s ) : Chec Jolanta . In this article , we present the solutions implemented in the LMS eCampus that are theoretically grounded and have been proved in praxis . Adding certain functions could considerably extend the range of tasks for which they could provide support . Putting the needed information at the distance of a mouse click would allow for active reading . This would require tight coupling of the dictionary with a text editor : all the information in the dictionary should be accessible via a mouseclick . Dictionaries combined with a flashcard system and an exercise generator Author(s ) : Zock Michael . Approximate Reasoning Techniques for Intelligent Diagnostic Instruction Intelligent instruction of fault diagnosis , or troubleshooting , tasks requires the capability to automatically infer the significance of particular test outcomes observed by the learner in a practice environment . This single process is central to virtually every aspect of intelligent instruction in this domain , ranging from evaluating learner proficiency to recommending an effective testing strategy . In highly complex target systems this task becomes intractable , for the number of possible fault Author(s ) : Towne Douglas M. . Toward a Web based environment for Evaluation and Design of Pedagogical Hypermedia In order to guide and help the educational Web sites authors in the conception / evaluation process of their prototypes , it is especially interesting to propose them interactive guides . Among the existent interactive guides , only few offer a good content structure or an appropriate navigation in their system . We propose a web based system used to help teachers to design multimedia documents and to evaluate their prototypes . Our current research objectives are to create a methodology to sustain the Author(s ) : Trigano Philippe , Pacurar Giacomini Ecaterina . Scripting argumentative knowledge construction in computer - supported learning environments Argumentative knowledge construction in computer - supported collaborative learning environments is often weak . This experimental study analyzes two collaboration scripts , which should facilitate argumentative knowledge construction . One script aims to support the construction of argumentation sequences and the other script aims to support the argument construction . These scripts should facilitate the argumentative knowledge construction on both the micro- and macrostructure of argumentation durin Author(s ) : Stegmann Karsten , Weinberger Armin , Fischer Frank , Ma . Investigating multimodal interactions for the design of learning environments : a case study in scien This thesis focuses on multimodal interactions for the design of a learning environment . The process of designing such systems involves studying the benefits of multimodal interactions in learning . Therefore , it analyses the structure of the interactive space between the learner and the content to be learnt , and introduces and tests a framework to structure it . It proposes that multimodal interactions can encourage rhythmic cycles of engagement and reflection that enhance learners ' meaning con Author(s ) : Anastopoulou Stamatina . Streaming Media in the Classroom - An overview of the current use of streaming technologies and the Video and sound can support education in Europe in a significant way as they appeal to learning modalities other than the purely textual linguistic ones usually found in traditional learning environments . Their use can also lead to increased visual literacy . However despite the fact that video and sound have been used for educational purposes since television was invented most practitioners agree they have never had the impact that was expected . This is largely due to the many challenges faced i Author(s ) : Vanbuel Mathy , Bijnens Helena , Bijnens Mathijs . Organis\\u00e9e dans le cadre des Journ\\u00e9es Victor Segalen 2010 par l'Unit\\u00e9 Mixte de Formation Continue en Sant\\u00e9 de l'Universit\\u00e9 Bordeaux 2 , cette formation s'adresse avant tout aux m\\u00e9decins g\\u00e9n\\u00e9ralistes . Y sont abord\\u00e9es diff\\u00e9rentes th\\u00e9matiques sur lesquelles les conf\\u00e9renciers font part des donn\\u00e9es les plus r\\u00e9centes sur les tr Author(s ) : No creator set . Ancien professeur \\u00e0 L'Universit\\u00e9 Rennes 2 , Philippe Hamon enseigne \\u00e0 Paris , et a publi\\u00e9 en 2000 chez Corti un ouvrage critique absolument remarquable sur les usages de l'image au XIXe et ses r\\u00e9percussions sur l'\\u00e9criture litt\\u00e9raire . L'entretien porte sur les relations entre les faits esth\\u00e9tiques , l'histoire sociale et celle des techniques , les repr\\u00e9sentations collectives et l'\\u00e9criture . Cette industrie , rurale pour l'essentiel , explique une prosp\\u00e9rit\\u00e9 do nt nous percevons les traces gr\\u00e2ce notamment aux c\\u00e9l\\u00e8bres enclos paroissiaux . Le commerce de la toile a \\u00e9g Author(s ) : No creator set . Collection \\\" Les Bretons et leur Histoire \\\" L'expression des marins indique bien l'esprit de ce film : confronter le mythe et la r\\u00e9alit\\u00e9 de la p\\u00eache \\u00e0 la morue . Le mythe , c'est celui qu'engendre l'incroyable succ\\u00e8s de \\\" P\\u00eacheur d'Islande \\\" de Pierre Loti en 1886 , et celui de \\\" la Paimpolaise \\\" de Th\\u00e9odore Botrel en 1895 . La r\\u00e9alit\\u00e9 , c'est celle d'une activit\\u00e9 \\u00e9conomique ancienne : la p\\u00eache \\u00e0 Terre - Neuve commence vers 1500 et concerne des Author(s ) : No creator set . Danielle Ofri , MD Neilly Series Part 2 10/27/2010 Journey with one doctor from the nation 's oldest and most legendary public hospital as she navigates the eye - opening cultural permutations of today 's America . Ofri is the author of Medicine in Translation : Journey with my Patients Introduction by Clayton Baker , MD Assistant Professor of Medicine Author(s ) : No creator set . Curtis White Neilly Series Part 4 11/11/2010 Curtis White presents \\\" Inevitably , a Romantic . \\\" A discussion of Romanticism and its relation to American culture since the ' 60s . Social critic , essayist , and novelist , White has authored five novels , several works of nonfiction and edited works , and numerous articles and essays Introduction by Patrick Daubert Class of 2011 Author(s ) : No creator set . Jezero Titicaca Lokacija posnetka : Isla del Sol , Bolivija . Najvi\\u0161je le\\u017ee\\u010de jezero na svetu.,Location of a photo : Isla del Sol , Bolivia . The highest ( commercially navigable ) lake in the world . \"}",
        "_version_":1692670025299656709,
        "score":20.02331},
      {
        "id":"541e143a-b260-4a7c-a0fa-439e1c088d63",
        "_src_":"{\"url\": \"http://zinewiki.com/zinewiki/index.php?title=BCSFAzine&diff=76795&oldid=61693\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701147841.50/warc/CC-MAIN-20160205193907-00073-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Personalized Learning Path Delivery . Hend Madhour 1 and Maia Wentland Forte 1 . Introduction . Getting the \\\" right \\\" set of pedagogical documents or proper information is a challenge that a learner can hardly overcome in an open environment . Not only is the form and content of online material very heterogeneous but it almost impossible for a user , and even more so for a novice , to discriminate between the plethora of available documents the best suited ones . Unless some kind of personalization is performed to better adapt the proposed material to the user 's needs . In this chapter , we will first examine different manners of performing such personalization in the framework of the today 's richest open source information environment , the Web . Then , we will concentrate on adaptive educational systems and dig more into the reference models these systems are built on . After showing the limitations of such models in an educational context , we will present a new reference model that we claim brings an elegant solution to these limitations , namely the Lausanne model . State of the Art . In this section , we present approaches , techniques and tools used to provide personalization in the today 's richest open source information environment , the Web . Personalization approaches . Personalization consists in adapting the behaviour of the system according to some specific information related to an individual user . We have classified these approaches into three types , depending on how information is collected : . Individual Vs Collaborative . Personalization advocates the user 's individuality . To adapt the system behaviour to the users ' needs , the system collects specific information about the user herself ( interest , preferences , age , etc . ) and/or about her interactions with the system ( interaction history ) . Two approaches , illustrating each of these , can be highlighted : . ( i ) Individual : Building a user profile , or individual model , that contains information about what the user likes / dislikes and making use of this profile to predict / adapt future interactions ( Ken , 1995 ) . ( ii ) Collaborative : Using the active user profile and the ones of others that share some common interests . This approach is called collaborative filtering and is used by ( Resnick et al . , 1994 ) . Reactive Vs Proactive . Some personalization systems need explicit interactions that takes the form of a special request or a feedback . Those systems are called reactive systems . We mention Entree ( Burke et al . , 1996 ) , Detorecs ( Fesenmaier et al . , 2003 ) , ExpertClerk ( Shimazu , 2002 ) as examples of those systems . Other personalization systems learn about user preferences and give recommendations using this information if given by the user . In fact , giving such information is not compulsory to run those systems . Besides , users choose to follow or ignore generated recommendations . Those systems are called proactive systems . User Vs Item oriented . Sources of the information needed to perform personalization are of two types : . Information about the user : past feedback , behaviour , demographic information ( age , sex , origin , education ) . Information about the item : content description , domain / product ontology . Most user oriented systems ( Ken , 1995 ) ; ( Ghani & Fano , 2002 ) are based on user 's behaviour ( past bought or rated items ) . Few ones ( Krulwich , 1997 ) exploit demographic information because such information is difficult to collect . Personalization techniques . Content filtering . User profile based on item 's content description : it is used to predict unseen items rate . Rated items analysis . Systems using this technique tend to recommend items that have big similarities with previously seen items . Social techniques . Collaborative filtering . This technique is an alternative to content filtering . In fact , it enhances the users collaboration by helping each others to filter items . The users rate consumed items . Rates given by users sharing the same interests are used to recommend suitable items . Based on a matrix item / user , the system recommends by prediction adequate items . Genetic algorithms . In the social context , artificial intelligence can take advantage of biological observations of certain species to imitate their experience . This trend , called evolutionary computation , uses genetic algorithms on one hand and swarm intelligence techniques on the other hand . The ACO algorithm belongs to this trend . In a simplified reality , the ants start moving randomly . Then , when they find food , they come back to their colony , secreting in their way chemical substances called pheromones . If other ants find this path , there is a high probability that they follow the path marked with pheromones reinforcing it when coming back . Consequently , the more a path is visited , the more it will be reinforced . Conversely , because the pheromone evaporates , the less reinforced paths slowly vanish leading all ants to follow the shortest one . Applied to distance learning , such algorithms require to model learner 's behaviour , task that for the time being , has not been very efficiently accomplished . For example , ( Semet , 2003 ) models the learner 's memory evaporation by a mathematical function depending on a parameter assessed within seconds . This leaves us a bit perplexed because we do not have the means to demonstrate the validity of this assertion . Personalization tools . User Modelling . Overlay model . Popular user models are overlay user models ; these share the same representation as domain models and are used to represent a user knowledge / interest in a resource space . These models are vectors of attributes ( measures of interest or knowledge ) , one for each concept in the domain model . These are updated from user navigation in the domain model . However , the definition of the domain models and of the users ' models is done manually . User profile . It represents cognitive styles , intentions , learning styles and preferences . It is entirely user oriented because information is provided by the user itself . For this reason , result suitability depends on the information provided by him . Stereotype model . It takes the form of a couple ( stereotype , value ) or a Boolean value that indicates either or not user belongs to a specific stereotype . This model is simpler but less powerful that the overlay model . Updating user model . Acquiring the user model can be done explicitly via interviews and questionnaires or implicitly via information inference based on available user information ( by learning or direct observation ) . Implicit acquisition needs to automate the manner to fill user model attributes . This automation petrifies user model reliability . Besides , specific attributes such as preferences and proficiencies can not be deduced by the system . That is why it is important to let the user participate in acquiring his user model . This way is called cooperative modelling . Sharing user model . In an open environment , mobility between different learning environment is a need . It is important to have a unique user model that is updated during the curriculum . A first initiative consists in defining specifications and standards used by all learning environment . The current specification of PAPI ( Papi , 2008 ) splits the learner information into 6 areas : personal information and preference information , performance , relations , portfolio and security . IMS / LIP ( Lip , 2008 ) is a specification that describes the learner 's characteristics to personalize the content . Conversely , LIP divides the learner information into 9 areas : interest , affiliation , QCL ( Qualifications , Certifications and licenses ) , activity , goal , identification , competency , relationship , security key , transcript and accessibility . This specification is much more detailed than PAPI and provides almost a complete users ' profile . But none of them differentiate the access rights allowing the user to modify all areas . Another initiative is to adopt a web service such as UMOWS ( User Model Web Service ) ( Bielikova & Kuruc , 2005 ) that update a centralized user model after each learning environment call . Personalized education systems . The use of IT in education covers a wide range of very different activities : authoring , course management , web sites conception , communication , simulations , learning environments , and much more . Its contribution to the process goes all the way from being a simple on - line help or support ( current traditional or blended learning ) to that of sophisticated main device ( distance learning ) . Because the one - size - fits - all paradigm can not be applied to individualized learning , adaptability is becoming a must . Hence , courseware is meant to be tailored according to the learner 's needs . Two main families of computerized applications aspire to offer this adaptability : Intelligent Tutoring Systems ( ITS ) ( Brusilovsky , 1992 ) and Adaptive Hypermedia Systems ( AHS ) ( Brusilovsky , 1996 ) . Intelligent tutoring systems . Intelligent Tutoring systems ( Brusilovsky , 1992 ) rely on curriculum sequencing mechanisms to provide the student with a path through the learning material . An adaptability algorithm computes this so - called personalized path corresponding to the course construction , the curriculum sequencing ( Shang et al . , 2001 ) . Construct dynamically page contents based on the tutor decision for what the learner should study next . ITS usually provide an evaluation of the learner 's level of mastery of the domain concepts through an answer analysis and error feedback process that eventually allows the system to update the user 's model . This process is called Intelligent solution analysis ( Serengul & Smith - Atakan , 1998 ) . Finally , the learner may need some help during the solving process from displaying a hint to executing the next step for him . This intelligent help is provided by an interactive problem solving support ( Serengul & Smith - Atakan , 1998 ) . Adaptive hypermedia systems . Adaptive Hypermedia ( AH ) ( Brusilovsky , 1996 ) was born as a trial to combine intelligent tutoring systems and educational hypermedia . As in ITS , adaptive education hypermedia focus on the learner , while at the same time it has been greatly influenced by adaptive navigation support in educational hypermedia ( Brusilovsky , 1996 ) . In fact , adaptability implies the integration of a student model in the system in the framework of a curriculum which sequence depends on pedagogical objectives , user 's needs and motivation . The Adaptive presentation is a functionality that helps the hypermedia take advantage of information included in the student 's model of a connected ITS . The first one scaffolds upon previously acquired knowledge , while the second organizes the domain knowledge by topics and levels of mastery . The Adaptive navigation support aims at helping users to find their paths in hyperspace by adapting the navigation and displayed functionalities to the goals , knowledge , and other explicited characteristics of an individual user . \\u00a7 In adaptable education systems , the tutor is involved as an expert . Some research ( Hernandez & Noguez , 2005 ) tries to model the tutor 's expertise in order to automate the learning process as much as possible . Modelling this expertise is a restrictive process because we postulate that it is impossible to formalize all of the tutor 's know - how . Moreover , when modelling the learner , the tutor intervention in the learning process should be taken into account . Besides , other actors ' interventions in the system implementation must be clarified such as those of the author and the instructor . Reference models . A reference model allows to collect the specifications and best practices to provide documentation and guidelines for a community of practice . It must also be precise enough to show the know how and broad enough to be malleable . In fact , standardize consists in establishing definitions and specifications of syntactic and semantic rules , and descriptions of environments . A standard must be neither prescriptive nor exclusive . The concept of reference model was discussed at the level of hypermedia well ahead of educational objects , while a major part of hypermedia are for educational purposes . For this reason , we propose to present existing reference models for adaptive hypermedia systems . Dexter . The Dexter model ( Halasz & Schwartz , 1990 ) describes the structures needed to define the links between information items . It was designed to clarify the concepts in existing hypertext systems . Two levels are presented in this model : the atomic component and the component compound . The components are composed of collections of other components ( atomic or composite ) and links that connect the components to their children . These components are treated as a single component . In addition , components ( atomic or composite ) can be connected by means of anchors . An anchor is part of the component , it can be a fragment of text , graphics , etc . . A component ( atomic or composite ) contains three main parts : content , semantic attributes , and presentation specifications . Finally , two types of relationships exist in this model : the links via anchors and the relationships between components and their children . Amsterdam . The Amsterdam model ( Hardman et al . , 1994 ) is an extension of the Dexter model and the CMIF ( CWI Multimedia Interchange Format ) model ( Hardman et al . , 1993 ) . The latter takes into account the synchronization of blocks of information . It essentially supports multimedia while the model only supports Dexter hypertext because of the lack of synchronization information . Amsterdam addresses the need to have a model for hypermedia . The Amsterdam model adds to Dexter model the notion of channels that are in fact predefined presentation specifications . The channels are abstract output devices that are used to define overall characteristics of a certain type of media such as the volume for an audio channel . For an atomic component , specifications presentation is enriched by the name and length of the channel . Composite component was extended to include sync arcs , anchors referencing anchors descendants and a start time for each son . Dortmund . The Dortmund Family of Hypermedia Models ( DFHM ) ( Tochtermann & Dittrich , 1996 ) is a family of interrelated models rather than a single model with a specification of a data type . The concepts introduced in these models are : . Link structures are a set of links that connect parts of the hyper document . Several links can be assigned to the same document . This can be very useful in the expression of different contexts for different users . Views are defined like the databases views to the extent that the information that is not interesting or relevant to a user is hidden . File is a container of nodes , links and other files . The records support the links between records , documents and documents beyond those folders . AHAM . The model AHAM ( Adaptive Hypermedia Application Model ) ( De Bra et al . , 1999 ) is a reference model for adaptive hypermedia . In order to adapt , AHAM offers three models : the domain model , the user model and the adaptation model . The domain model represents author view concerning the application domain . It describes the structure of an adaptive hypertext system as a finite set of component concepts ( concepts and relations between concepts ) . The concept is an abstract representation of a node of information in the application domain . Two types of concept are defined : atomic and composite . An atomic concept is a piece of information when a composite concept is a sequence composed of concepts that can be composite ( an abstract concept ) or atomic ( page ) . The relationship between concepts must be transformed into relations between atomic concepts because only the pages can be displayed . This transformation is done by adjusting the engine . Several types of relationships are considered in the model AHAM : hyperlink , prerequisites , inhibiting . The user model contains information that the system records about the user . It is based on user knowledge on the concepts . For each user , the system maintains a table where it stores the attributes describing the knowledge for each concept in the domain . Two attributes are present at least : ( i ) the value of knowledge that indicates the extent of knowledge of the user on a concept and ( ii ) the attribute read that indicates whether the user read something about the concept . This attribute can be either boolean or a list of access time . The adaptation model contains the rules to be applied to teaching in the previous models for the purpose of adaptation . AHAM uses a rule language adaptation that looks like SQL ( Simple Query Language ) except that it does not contain the FROM part because it will always refer to the user model and the application domain . For example , to update the knowledge of a concept whose page is P , the following rule is followed : . The condition C states that access to a page can only be done if the attribute ready is true . If you have accessed to P , the A states that the attribute knowledge is given to well known . The adaptation engine performs a number of tasks when a user accesses the system : It retrieves the corresponding user model . All attributes for all concepts are retrieved . The other attributes used in teaching rules but not included in the user model are initialized to a default value . For example if the attribute ready to know is not in the user model , it is set to true for all concepts . The engine determines the corresponding concept C based on the user model and teaching rules . It generates an HTML page by following the presentation specifications and updates the attributes of the user model . Munich . The Munich model ( Koch & Wirsing , 2002 ) is based on the Dexter model and extends it for an adaptation purpose . Indeed , added to the navigational relationships ( links ) , there are also conceptual relationships ( part of , prerequisite of , in the same page as a variant of ) . Like AHAM , a user model and a model of adaptation are required . The Munich model does not take into account aspects such as typical multimedia synchronization . It aims to construct specific views to the user , as it is the case in the DFHM model . It differs from other reference models previously described by its object - oriented approach and therefore the use of UML ( Unified Modelling Language ) modelling technique . Limitations . Although the existing adaptive hypermedia systems have a potential in providing suitable learning resources , they remain relatively closed environments . In addition , the existing reference models tend to be generic and consequently do not address issues related to specific systems such as learning systems . More specifically , existing reference models suffer from several limitations . One of its fundamental limitations is the fact that it considers only two actors in the learning process : the author and the user . All of these are called the author in the AHAM model . The tutor who supervises and facilitates the learning process . The learner who is meant to benefit from the adaptive system . Besides , the domain is composed of concepts connected by relationships following a restrictive vocabulary . On the one hand , there is no typology of concepts on which the domain model is based . On the other hand , there is no way to describe elements other than the concepts . Moreover , in the context of an open environment , the used restrictive vocabulary is a problem because it forces various sources to use the same vocabulary . This seems difficult even between two different designers . Finally , the concepts do not have a description formalism facilitating their discovery . Finally , the user model takes into account only the information about the resource . It is often an overlay model . The current standards such as IEEE / PAPI and IMS / LIP are not used . The adaptation model is generic and allows to the author to define the pedagogical rules . It remains dependent on the author , who in addition to learning resources must take time to design rules that take into account the specificities of each user . This requires a big effort . In addition , the relevance of the generated course depends strongly on these rules . Because of all these limitations we propose a new reference model , called the Lausanne Model , extending AHAM , in order to meet our needs . Lausanne Model . Domain model . In the context of a learning environment , the Domain Model focuses on pedagogical material content and aims at describing it by representing its entities and their relationships in a standardized manner . After discussing the granularity issue , we go in some details into the indexation and annotation problems and then proceed to illustrate our MLR solution . Entities . The Learning community has come up with a number of ways to depict hypermedia each proposing different manners of tackling issues such as the level of granularity or the description scheme . Asset : is a document lowest level of granularity . As such , assets can be pictures , illustrations , diagrams , audio and video , animations , as well as text fragments . The Learning information is a group of assets expressing the same meaning . For example , a figure associated with its comment is learning information . The Learning Object represents the semantic structure ( or network ) in which learning information is grouped . It is associated to a context and is described with a specific formalism . We call unit a coherent learning entity of adequate level of granularity . Description . Two methods are available to facilitate the search and retrieval of educational resources , i.e. indexation and annotation . Indexation . It consists in describing a document by giving values to a number of predefined fields ( often specifications and standards ) . This information , called metadata , is then stored and linked to the educational resource it describes . To - date the two major standards are LOM ( Learning Object Meta - data ) ( LOM , 2002 ) and DC ( Dublin Core , ) . The DC is intended to describe any document while the LOM specializes in educational documents . Several other initiatives exist , some of which are adaptations ( profiles ) of LOM and were designed to reduce its complexity . We can mention : LOMFR ( LOMFR , 2006 ) and ManuEl ( De la Passadi\\u00e8re & Jarrot , 2004 ) . Recently ( MLR , 2005 ) , an initiative under the name of MLR ( Metadata for Learning Resources ) has been launched to develop a standard metadata addressing LOM drawbacks and offering new features . Annotation . The annotation is \\\" a commentary on an object as the commentator said it was noticeably distinguishable from the object itself and the reader interprets as noticeably distinguishable from the object itself \\\" ( Baldonado et al . , 2000 ) . ( Huart et al . , 1996 ) found a correlation between the different methods of annotation and their semantics ( or objective ) . Although annotation can provide useful semantic information , as long as this information is not properly stored , it remains unsharable . Conversely , segmentation , looked upon as an annotation instance , could contribute to remedy to this shortcoming . Each presentation chain is considered to have the smallest contextually pertinent level of granularity in the framework of the implicated hypermedia . Although relations between presentation chains are preserved after segmentation has taken place they are lost when indexed individually . We here below put forward an example of how we have solved this issue . Let us consider the context of segmentation in which it can be claimed that the highest level of granularity of a document is the document itself , while the smallest level of granularity is any of its identified presentation chain seen as a learning object per se . We have adapted MLR to describe the relationships between learning objects capitalizing on the semantic information obtained when segmenting the document . We distinguish four types of metadata ( Figure 1 ): . Navigational metadata describes the relationships between presentation chains ( MLR : Description : Relation ) . Navigation is done through a graph of resource anchors called Learning Object Network ( Madhour et al . , 2006 ) . Conceptual metadata that comprises all the attributes of a concept other than those belonging to navigational metadata . It is stored , when it exists , in the Description category ( MLR : Description : Description ) . It is associated directly with the resources in one and same file which is uploaded in a Learning Object Repository ( LOR ) . Adaptation metadata that aims at personalizing the navigation in a given domain taking into account the environmental constraints ( MLR : Contextualization ) , the security issues ( MLR : Access ) . Relationship . Pursuing with our previous example , this field is deduced automatically as relational information has been defined during segmentation . Since segmentation is , by far , not the only way to produce learning objects , we have examined the literature and concluded that the other existing relationships are not compatible with one another . This has prompted us to try and define generic relationships that could support any kind of useful semantic links . Ultimately , the Lausanne Domain Model is composed of a set of indexed entities called learning objects related to one another by any pertinent link . The indexation scheme describes : . Bearing in mind that we aim at being able to deliver a suitable personalized path to a learner , we now need to proceed to explain XUM ( eXtended User Model ) which is the Lausanne User Model . After considering two main standards in user profile - IEEE / PAPI and IMS / LIP , we propose a mapping from XUM to PAPI and LIP and vice - versa thus favouring the user mobility from one environment to another . We then show how XUM can be mapped to some MLR attributes in order to easily retrieve suitable learning objects and finalize this presentation by considering the responsibility of each actor in the learning process . Figure 1 . MLR . User Model . To achieve our goal , we need to gather as much information as possible about the learner to derive and determine a number of specific useful characteristics : a User Model . We have studied two standards of a user model : IEEE / PAPI and IMS / LIP . The current specification of PAPI splits the learner information into 6 areas : . individual information and preference information , performance , relations , portfolio and security . IMS / LIP describes the learner 's characteristics to personalize the content . LIP divides the learner information into 9 areas : interest , affiliation , QCL ( Qualifications , Certifications and licenses ) , activity , goal , identification , competency , relationship , security key , transcript and accessibility . Much more detailed than PAPI , LIP nevertheless entitles the learner to modify the attributes of his user model ( like PAPI ) . Because we believe that the responsibility of the learning process should not be entirely delegated to the learner , we think that this possibility should be shared and restricted . We propose therefore to split the learner 's attributes into four categories depending on the modification rights : machine driven , learner driven , system driven and tutor driven . For any specific learner to be able to easily retrieve adequate learning objects , and therefore for the system to provide a personalized learning path , we propose to map XUM with some MLR attributes ( as a metadata example ) . In the following , we describe each category of user model attributes and its mapping with IMS / LIP and IEEE / PAPI on one hand and with MLR fields ( as an example of metadata ) when necessary on the other hand . Machine Driven modification category . The Machine driven modification category contains the system properties attributes ( such as memory size and processor speed ) as well as the learning constraints ( such as the delivery mode and accessibility ) . We think that these attributes are necessary to prevent the system to include in a personalized path elements that can not be supported by the learner 's machine . Security is a learner driven field that will be dealt with later on . The System properties descriptor indicates the memory size , the power of the processor , the network characteristics ( bit rate , etc . ) . It can be mapped to the technical requirements field ( MLR : Access : Technical Requirements ) . The Delivery mode defines the document format , i.e. video , image or text as well as the font and font size ( for partially - sighted persons for example ) . It can be mapped to adaptability field ( MLR : Access : Adaptability ) because it defines the delivery mode ( has auditory , has visual ) . Accessibility refers to language , disabilities , and preferences . It can be entirely mapped to the accessibility field of IMS / LIP but in so far the Preferences field of IEEE / PAPI is concerned it can be mapped only to XUM : Accessibility : Preferences . Learner Driven modification category . The Learner driven modification category includes all information that we think a learner can provide such as security , demographic data , interest , affiliation general goal and stereotype . Security refers here to the learner 's security credentials , such as passwords , challenge / responses , private keys and public keys . It can be mapped entirely to the security field of IEEE / PAPI and IMS / LIP . Affiliation relates to membership of professional organisations . Only the correspondence with the affiliation field of IMS / LIP is identified . Goal : The learner can only define his general goal . All sub - goals are determined by the instructor . In fact , in the case of traditional learning , the student can choose within the curriculum , the courses he wishes to attend but he can never go down to select chapters and sections . The course organisation must remain the instructor 's responsibility . Only the correspondence with the goal field of IMS / LIP is identified . This field is of paramount importance as it is the basis of the adaptation process and our aim is to deliver a well - suited personalized learning path ( a set of suitable interconnected units ) . Demographic data corresponds to all personal information relevant to learning such as age , gender , name , address , role , etc . The identification field in the case of IMS / LIP consists of both biographic and demographic data . This biographic data will be mapped to the stereotype field which will be described later . The Personal field in the case of IEEE / PAPI can be mapped entirely to this field . It can be mapped to the audience field ( MLR : Contextualization : audience ) . Interest contains information about hobbies and recreational activities . Only the . correspondence with the goal field of IMS / LIP is identified . It can be mapped . to the subject field ( MLR : Description : Description : Subject ) . System Driven modification category . The System driven modification category includes the interaction history , portfolio and proficiencies . Interaction history : For each user and item there is an annotation that indicates the state of a learning item : read , unread , knowledge ( or proficiency ) , waiting ( for learning a special prerequisite ) . Relationship ( IMS / LIP ) and relations fields can be mapped to this field . The only difference is that those relationships have not specific terminology . Portfolio It is a collection of a learner 's accomplishments and works that is intended to illustrate and confirm abilities and achievements . It can be mapped entirely to the transcript field of IMS / LIP and the portfolio field of IEEE / PAPI . Adaptation model . Based on an active user model ( individual adaptation ) and on other users ' profiles ( social adaptation ) , the Adaptation model describes how the adaptation is performed . The individual adaptation process filters items based on the user 's needs as they are mainly recorded in the fields of the machine driven modification category . It is used when calculating the next unit to be visited . The Social adaptation process aims at providing a personalized learning path based on the experience of other learners provided they share a similar knowledge level ( proficiencies ) and the same interests . Our algorithm includes both processes and aims at building a suitable learning path for a specific learner . In our case , we try to use XUM as a way to model the learner behaviour . Our algorithm takes as input an instance of XUM denoted X and the current goal of X. It works based on a set of interconnected units where each unit has a routing table associated a fitness function that guides learners to select the best step to do next . Learner classification . Although students have heterogeneous knowledge level , it is possible to create temporary groups taking into account only the prerequisite associated with the unit or the goal . Group ( x ) function returns as result the group to which x belongs . The routing table . Associated to each unit , the routing table gives an idea about the possible neighbours and probabilities that the learner might move toward each of these neighbours . These probabilities depend on the group the learner belongs to as well as on the pheromones spread . The pheromones spread is deferred because it depends on the result generated by the learner following his goal evaluation once his path comes to an end . This result is given by Evaluate(Goal ) and takes the form of a number . \\u03b8 with . The pheromone . \\u03c4 is in our case the average of . \\u03b8 s evaluated for each member of the defined group . I is the individualization factor , it is a function of the environment and the learner characteristics ( his preference for example ) . i Is the pheromone filled earlier by learners of the same group . \\u03b2 and . \\u03b3 are weights to be determined according to the needed results . They are parameterized by a human being . To further clarify our train of thoughts , we demonstrate here below how I is calculated with U being an indexed unit of the network and X being an instance of XUM for a learner X ( Figure 2 ) . According to mapping rules defined previously , we can deduct that U suits x. Figure 2 . Example of mapping between XUM and MLR . Path Delivery . Learning path delivery algorithm . The basic assumption is that each student has a set of information describing him . He has a specific goal that he shares or not with other students . Learning is asynchronous because everyone has a different earning pace and different time constraints . The learner moves from unit to unit in order to achieve his goal . Because the goal is not a unit in itself , but knowledge to be mastered , the goal unit is not defined before hand . It must be determined dynamically according to the learner 's progress . The SubGoal function determines the unit to be reached in order to achieve the goal . The fact remains that all learners belonging to the same group have the same starting point called Source that is determined by the function Source ( group , Goal ) . The purpose of our algorithm is to build the suitable learning path for a learner x. The proposed algorithm follows the meta heuristic pseudo code of ACO in the case of deferred spread of pheromones . We believe that the Ant colony algorithm may provide an optimal solution to the problem of learning path delivery if we define properly the fitness function . Conclusion . In this article , we have described the Lausanne Reference Model , designed for learning object systems . The Domain Model is described as a set of indexed learning objects . We have used the Phoenix tool ( Fernandes et al . 2005 ) that allows segmenting on the fly any hypermedia document and dynamically build a pedagogical network of presentation chains that we have in turn indexed with MLR . The User Model , baptized XUM , is based on the user profile two main standards , the IEEE / PAPI and IMS / LIP . In order to validate the usability of XUM , we have shown that it can be mapped with some MLR attributes ( as a metadata example ) facilitating the retrieval of suitable learning objects based on XUM and MLR . The Adaptation Model is based on the ACO algorithm which has the advantage of benefiting from the social dimension and provides an optimal learning path . The Lausanne Model differs from other existing models like AHAM , Munich , Amsterdam and Dexter mainly in that : . It considers learning issues such as granularity level , description formalism , quality , intellectual property . To be noted that the two latter are raised in the context of an open environment which is not in the scope of this article . Learning objects are organized in a network where links are pertinent but not limited to a particular ontology . It enhances user mobility from one environment to another . It considers both individual and social adaptation . Future work will consist in simulating the implementation of the algorithm in different situations to determine the best weights of the fitness function . Besides , the integration of the Lausanne Model based system in a learning environment remains to be done . This system , applied in a lifelong learning process , could bring a real added - value if coupled to a knowledge portfolio . 3 - P. Brusilovsky , D. Cooper , 1999 Adapts : Adaptive hypermedia for a web - based performance support system , Proceedings of the 2nd Workshop on Adaptive Systems and User Modeling on the WWW , 41 47 , Canada . 7 - M. Bielikova , J. Kuruc , 2005 Sharing user models for adaptive hypermedia applications . In ISDA'05 : Proceedings of the 5th International Conference on Intelligent Systems Design and Applications , 506 513 , Washington , DC , USA . 9 - P. Brusilovsky , 1992 A framework for intelligent knowledge sequencing and task sequencing . In ITS'92 : Proceedings of the Second International Conference on Intelligent Tutoring Systems , 499 506 , London , UK , 1992 . Springer - Verlag . 12 - P. De Bra , G. Houben , H. Wu , 1999 Aham : a dexter - based reference model for adaptive hypermedia . In HYPERTEXT'99 : Proceedings of the tenth ACM Conference on Hypertext and hypermedia : returningto our diverse roots , 147 156 , New York , NY , USA . 14 - E. Duval , E. Forte , K. Cardinaels , B. Verhoeven , R. Van Durm , K. Hendrikx , Forte . M. Wentland , M. Macowicz , K. Warkentyne , F. Haenni , 2001 The ariadne knowledge pool system . Communications of the ACM , 44(5 ) . 16 - B. De La Passardi\\u00e8re , P. Jarraud , 2004 Manuel , un profil d'application de lom pour c@mpusciences . STICEF- Sciences et Technologies de l'Information et de la Communication pour l'Education et la Formation . 21 - G. Fischer , T. Mastaglio , B. Reeves , J. Rieman , 1990 Minimalist explanations in knowledge - based systems , Proc . 23 rd Annual Hawaii International Conference on System Sciences , Kailua - Kona , HI , 309 317 . 25 - M. Gonschorek , C. Herzog , 1995 Using hypertext for an adaptive helpsystem in an intelligent tutoring system , Proc . 7 th World Conference on Artificial Intelligence in education , Washington , DC , 274 281 . 27 - L. D. Hardman , C. A. Bulterman , G. Van Rossum , 1993 Authoring multimedia in the cmif environment . In CHI'93 : INTERACT'93 and CHI'93 conference companion on Human factors in computing systems , 101 102 , New York , NY , USA . 31 - J. Hernandez , J. Noguez , 2005 Affective Behavior in Intelligent Tutoring Systems for Virtual Laboratories , Proc . 7th World Conference on Artificial Intelligence in Education , Amsterdam , Netherlands , 51 56 . 37 - N. Koch , M. Wirsing , 2002 The munich reference model for adaptive hypermedia applications . In AH'02 : Proceedings of the Second International Conference on Adaptive Hypermedia and Adaptive Web - Based Systems , 213 222 , London , UK , Springer - Verlag . 41 - E. Melis , E. Andr\\u00e8s , J. B\\u00fcdenbender , A. Frishauf , G. Goguadse , P. Libbrecht , M. Pollet , C. Ullrich , 2001 Activemath : A web - based learning environment . International Journal of Artificial Intelligence in Education , 12(4 ) , 385 407 . 42 - B. N. Miller , I. Albert , K. L. Shyong , J. A. Konstan , J. Riedl , 2003 Movielens unplugged : Experiences with an occasionally connected recommender system . In Proceedings of ACM 2003 Conference on Intelligent User Interfaces ( IUI'03 ) ( Accepted Poster ) , Chapel Hill , North Carolina . 43 - H. Madhour , E. Fernandes , M. Wentland Forte , 2006 Learning object network : Towards a semantic navigation support . In Ed - Media world conference on educational multimedia , hypermedia and telecommunications , Orlando , FL , USA . 48 - P. Resnick , N. Iacovou , M. Suchak , P. Bergstorm , J. Riedl , 1994 GroupLens : An Open Architecture for Collaborative Filtering of Netnews . In Proceedings of ACM 1994 Conference on Computer Supported Cooperative Work , 175 186 , Chapel Hill , North Carolina . 49 - Y. Semet , 2003 Application de l'optimisation par colonies de fourmis \\u00e0 la structuration automatique de parcours p\\u00e9dagogiques . M\\u00e9moire de fin d'\\u00e9tudes d'ing\\u00e9nieur . Universit\\u00e9 de Technologie de Campi\\u00e8gne . 51 - U. Shardanand , P. Maes , 1995 Social information filtering : algorithms for automating word of mouth . In CHI'95 : Proceedings of the SIGCHI conference on Human factors in computing systems , 210 217 , New York , NY , USA . Publishing Co. . 55 - U. Timm , M. Rosewitz , 1998 Benutzermodellierung in der elektronischen produktberatung- konzept und prototypische realisierung in einer on - line umgebung . In Proceedings of ABIS-98 : workshop on adaptivity and user modeling in interactive software systems . Notes . [ 1 ] - A Concept is a semantic element explicitly defined in the text . Its definition is composed of either already identified concepts or of prerequisites defined elsewhere . It is characterized by a presentation order , a label , a gender , a type , a complexity degree and content . \"}",
        "_version_":1692670898668044288,
        "score":19.93476},
      {
        "id":"c571723b-ac5b-4d77-b774-7fa8b0bd58e5",
        "_src_":"{\"url\": \"http://www.aadl.org/node/41213\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162648.4/warc/CC-MAIN-20160205193922-00039-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Collaborative Authoring of Adaptive Educational Hypermedia by Enriching a Semantic Wiki 's Output . Nurjanah , Dade ( 2009 ) Collaborative Authoring of Adaptive Educational Hypermedia by Enriching a Semantic Wiki 's Output . At International Conference on User Modeling and Personalization ( UMAP ) 2009 , Trento , Italy , 22 - 24 Jun 2009 . Download . Description / Abstract . This research is concerned with harnessing collaborative approaches for the authoring of Adaptive Educational Hypermedia ( AEH ) systems . It involves the enhancement of Semantic Wikis with pedagogy aware features to this end . There are many challenges in understanding how communities of interest can efficiently collaborate for learning content authoring , in introducing pedagogy to the developed knowledge models and in specifying user models for efficient delivery of AEH systems . The contribution of this work will be the development of a model of collaborative authoring which includes domain specification , content elicitation , and definition of pedagogic approach . The proposed model will be implemented in a prototype AEH authoring system that will be tested and evaluated in a formal education context . Keywords : . Keywords : Adaptive Educational Hypermedia , collaborative authoring , Semantic Wikis , communities of interest . ASCII Citation Atom BibTeX Dublin Core Dublin Core EP3 XML EndNote HTML Citation HTML Citation JSON METS MODS MPEG-21 DIDL OpenURL ContextObject OpenURL ContextObject in Span RDF+N - Triples RDF+N3 RDF+XML Refer Reference Manager Simple Metadata XML ( eprints 2.3 style ) \"}",
        "_version_":1692671189030273025,
        "score":19.88557},
      {
        "id":"32c9f64f-c981-4437-9424-056c60cde52d",
        "_src_":"{\"url\": \"http://ancienthebrewpoetry.typepad.com/ancient_hebrew_poetry/2009/01/why-i-love-virginia-woolf.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148758.73/warc/CC-MAIN-20160205193908-00166-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"It is the aim of this module to explore some of the aspects and challenges in Human Language Technologies ( HLT ) that are of relevance to Computer Assisted Language Learning ( CALL ) . Starting with a brief outline of some of the early attempts in HLT , using the example of Machine Translation ( MT ) , it will become apparent that experiences and results in this area had a direct bearing on some of the developments in CALL . CALL soon became a multi - disciplinary field of research , development and practice . Some researchers began to develop CALL applications that made use of Human Language Technologies , and a few such applications will be introduced in this module . The advantages and limitations of applying HLT to CALL will be discussed , using the example of parser - based CALL . This brief discussion will form the basis for first hypotheses about the nature of human - computer interaction ( HCI ) in parser - based CALL . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . Piklu Gupta : At this time of writing this module Piklu was a lecturer in German Linguistics at the University of Hull , UK . He is now working for Fraunhofer IPSI . Mathias Schulze : At this time of writing this module Mathias was a lecturer in German at UMIST , now merged with the University of Manchester , UK . He is now working at the University of Waterloo , Canada . His main research interest is in parser - based CALL and linguistics . He is an active member of the NLP SIG within the EUROCALL professional association and ICALL within the CALICO professional association . Graham Davies , ICT4LT Editor , Thames Valley University , UK . Graham has been interested in Machine Translation since 1976 . Human Language Technologies ( HLT ) is a relatively new term that embraces a wide range of areas of research and development in the sphere of what used to be called Language Technologies or Language Engineering . The aim of this module is to familiarise the student with key areas of HLT , including a range of Natural Language Processing ( NLP ) applications . NLP is a general term used to describe the use of computers to process information expressed in natural ( i.e. human ) languages . The term NLP is used in a number of different contexts in this document and is one of the most important branches of HLT . There is a Special Interest Group in Language Processing , NLP SIG , within the EUROCALL professional association , and a Special Interest Group in Intelligent Computer Assisted Language Instruction ( ICALL ) within the CALICO professional association . Both have similar aims , namely to further research in a number of areas that are mentioned in this module , such as : . Artificial Intelligence ( AI ) . Computational Linguistics . Corpus - Driven and Corpus Linguistics . Formal Linguistics . Machine Aided Translation ( MAT ) . Machine Translation ( MT ) . Natural Language Interfaces . Natural Language Processing ( NLP ) . Theoretical Linguistics . All of the above are areas of research that have produced results which have proven , are proving and will prove very useful in the field of Computer Assisted Language Learning . Of course , this module can not teach you everything there is to know about HLT . This is neither necessary nor possible . The two main authors of this module are living proof of that ; they both started off as language teachers and then got interested in HLT . A multilingual CD - ROM titled A world of understanding was produced in 1998 on behalf of the Information Society and Media Directorate General of the European Commission under its former name , DGXIII . The aim of the CD - ROM was to demonstrate the importance of HLT in helping to realise the benefits of the Multilingual Information Society , in particular forming a review and record of the Language Engineering Sector of the Fourth Framework Programme of the European Union ( 1994 - 98 ) . 1.1 Introduction to HLT . [ ... ] there is no doubt that the development of tools ( technology ) depends on language - it is difficult to imagine how any tool - from a chisel to a CAT scanner - could be built without communication , without language . What is less obvious is that the development and the evolution of language - its effectiveness in communicating faster , with more people , and with greater clarity - depends more and more on sophisticated tools . ( European Commission : Language and technology 1996:1 ) . Language and technology lists the following examples of language technology ( using an admittedly broad understanding of the term ) : . photocopier ( p. 10 ) . laser printer ( p. 11 ) . fax machine ( p. 12 ) . desktop publishing ( p. 13 ) . scanner , modem ( p. 15 ) . electronic mail ( p. 16 ) . machine translation ( p. 17 ) . translator 's workbench ( p. 18 ) . tape recorder , database search engines ( p. 19 ) . telephone ( p. 25 ) . Many of these are already being used in language learning and teaching . The field of human language technology covers a broad range of activities with the eventual goal of enabling people to communicate with machines using natural communication skills . Research and development activities include the coding , recognition , interpretation , translation , and generation of language . [ ... ] Advances in human language technology offer the promise of nearly universal access to online information and services . Since almost everyone speaks and understands a language , the development of spoken language systems will allow the average person to interact with computers without special skills or training , using common devices such as the telephone . These systems will combine spoken language understanding and generation to allow people to interact with computers using speech to obtain information on virtually any topic , to conduct business and to communicate with each other more effectively . [ Source : Foreword to ( Cole 1997 ) ] . Facilitating and supporting all aspects of human communication through machines has interested researchers for a number of centuries . The use of mechanical devices to overcome language barriers was proposed first in the seventeenth century . Then , suggestions for numerical codes to be used to mediate between languages were made by Leibnitz , Descartes and others ( v. Hutchins 1986:21 ) . The beginnings of what we describe today as Human Language Technologies are , of course , closely connected to the advent of computers . ( i ) Various games , e.g. chess , noughts and crosses , bridge , poker ( ii ) The learning of languages ( iii ) Translation of languages ( iv ) Cryptography ( v ) Mathematics . Of these ( i ) , ( iv ) , and to a lesser extent ( iii ) and ( v ) are good in that they require little contact with the outside world . For instance in order that the machine should be able to play games its only organs need be ' eyes ' capable of distinguishing the various positions on a specially made board , and means for announcing its own moves . Mathematics should preferably be resticted to branches where diagrams are not much used . Of the above possible fields the learning of languages would be the most impressive , since it is the most human of these activities . This field sees however to depend too much on sense organs and locomotion to be feasible . ( Turing 1948:9 ) . Later on , Machine Translation enjoyed a period of popularity with researchers and funding bodies in the United States and the Soviet Union : . From 1956 onwards , the dollars ( and roubles ) really started to flow . Between 1956 and 1959 , no less than twelve research groups became established at various US universities and private corporations and research centres . Although linguists , language teachers and computer users today may find these predictions ridiculous , it was the enthusiasm and the work during this time that form the basis of many developments in HLT today . Research and development in HLT is nowadays more rapidly transferred into commercial systems than was the case up until the 1980s . Indeed HLT is becoming increasingly pervasive in our everyday lives . Here are some examples : . Machine Translation ( Section 3 ): There are many online translation systems that can be accessed free of charge , causing headaches for teachers whose students thought that they could save themselves time and who were blissfully unaware of the unreliability of their output ( Section 3.2 ) . Speech synthesis ( Section 4.1 ): Satellite navigation ( satnav ) devices for motor vehicles use systems that read out road numbers , street names and directions for the driver , and their output is surprisingly good . Speech recognition ( Section 4.2 ): If you make a telephone call to a customer support service you may hear a telephone recording that asks you to say a word or short phrase so that you can be connected to the appropriate department . Other previously unexpected areas of use are emerging . It is now , for instance , common for mobile phones to have what is known as predictive text input to aid the writing of short text messages . Instead of having to press one of the nine keys a number of times to produce the correct letter in a word , software in the phone compares users ' key presses to a linguistic database to determine the correct ( or most likely ) word . Most Internet search engines also now incorporate some kind of linguistic technology to enable users to enter a query in natural language , for example \\\" What is meant by log - likelihood ratio ? \\\" is as acceptable a query as simply \\\" log - likelihood ratio \\\" . What are the possible benefits for language teaching and learning of using HLT ? Here are some examples : . Teachers might want to preprocess a text to highlight certain grammatical phenomena or patterns . This can easily be done with a word - processor . Teachers might use part - of - speech taggers ( see Section 5 ) which could save them the trouble of having to manually annotate a text . Parsers available either on the Web or for local use on PCs can generate a graphical representation of sentence structure that may be useful for grammatical analysis for more advanced learners . Machine Translation ( MT ) has been the dream of computer scientists since the 1940s . The student 's attention is drawn in particular to the following publications , which provide a very useful introduction to MT : . Hutchins ( 1999 ) \\\" The development and use of machine translation systems and computer - based translation tools \\\" . Paper given at the International Symposium on Machine Translation and Computer Language Information Processing , 26 - 28 June 1999 , Beijing , China . 3.1 Machine Translation : a brief history . Initial work on Machine Translation ( MT ) systems was typified by what we would now consider to be a naive approach to the \\\" problem \\\" of natural language translation . Successful decoding of encrypted messages by machines during World War II led some scientists , most notably Warren Weaver , to view the translation process as essentially analogous with decoding . The concept of Machine Translation in the modern age can be traced back to the 1940s . Warren Weaver , Director of the Natural Sciences Division of the Rockefeller Foundation , wrote to his friend Norbert Wiener on 4 March 1947 - short ly after the first computers and computer programs had been produced : . Recognising fully , even though necessarily vaguely , the semantic difficulties because of multiple meanings , etc . , I have wondered if it were unthinkable to design a computer which would translate . Even if it would translate only scientific material ( where the semantic difficulties are very notably less ) , and even if it did produce an inelegant ( but intelligible ) result , it would seem to me worth while . When I look at an article in Russian , I say \\\" This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode \\\" . Have you ever thought about this ? As a linguist and expert on computers , do you think it is worth thinking about ? Cited in Hutchins ( 1997 ) . Weaver was possibly chastened by Wiener 's pessimistic reply : . I frankly am afraid the boundaries of words in different languages are too vague and the emotional and international connotations are too extensive to make any quasi - mechanical translation scheme very hopeful . But Weaver remained undeterred and composed his famous 1949 Memorandum , titled simply \\\" Translation \\\" , which he sent to some 30 noteworthy minds of the time . It posited in more detail the need for and possibility of MT . Thus began the first era of MT research . A direct system would comprise a bilingual dictionary containing potential replacements or target language equivalents for each word in the source language . A restriction of such MT systems was therefore that they were unidirectional and could not accommodate many languages unlike the systems that followed . Rules for choosing correct replacements were incorporated but functioned on a basic level ; although there was some initial morphological analysis prior to dictionary lookup , subsequent local re - ordering and final generation of the target text , there was no scope for syntactic analysis let alone semantic analysis ! Inevitably this often led to poor quality output , which certainly contributed to the severe criticism of MT in the 1966 Automatic Language Processing Advisory Committee ( ALPAC ) report which stated that it saw little use for MT in the foreseeable future . The damning judgment of the ALPAC report effectively halted research funding for machine translation in the USA throughout the 1960s and 1970s . We can say that both technical constraints and the lack of a linguistic basis hampered MT systems . The system developed at Georgetown University , Washington DC , and first demonstrated at IBM in New York in 1954 had no clear separation of translation knowledge and processing algorithms , making modification of the system difficult . In the period following the ALPAC report the need was increasingly felt for an approach to MT system design which would avoid many of the pitfalls of 1 G systems . By this time opinion had shifted towards the view that linguistic developments should influence system design and development . Indeed it can be said that the second generation ( 2 G ) of \\\" indirect \\\" systems owed much to linguistic theories of the time . 2 G systems can be divided essentially into \\\" interlingual \\\" and \\\" transfer \\\" systems . We will look first of all at interlingual systems , or rather those claiming to adopt an interlingual approach . Although Warren Weaver had put forward the idea of an intermediary \\\" universal \\\" language as a possible route to machine translation in his 1947 letter to Norbert Wiener , linguistics was unable to offer any models to apply until the 1960s . By virtue of its introduction of the concept of \\\" deep structure \\\" , Noam Chomsky 's theory of transformational generative grammar appeared to offer a route towards \\\" universal \\\" semantic representations and thus appeared to provide a model for the structure of a so - called interlingua . An interlingua is not a natural language , rather it can be seen as a meaning representation which is independent of both the source and the target language of translation . An interlingua system maps from a language 's surface structure to the interlingua and vice versa . A truly interlingual approach to system design has obvious advantages , the most important of which is economy , since an interlingual representation can be applied for any language pair and facilitates addition of other language pairs without major additions to the system . The next section looks at \\\" transfer \\\" systems . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . ( For n languages the number of transfer modules required would be n ( n -1 ) or n ( n -1 ) /2 if the modules are reversible ) . An important advance in 2 G systems when compared to 1 G was the separation of algorithms ( software ) from linguistic data ( lingware ) . In a system such as the Georgetown model the program mixed language modelling , translation and the processing thereof in one program . This meant that the program was monolithic and it was easy to introduce errors when trying to rectify an existing shortcoming . The move towards separating software and lingware was hastened by parallel advances in both computational and linguistic techniques . The adoption of linguistic formalisms in the design of systems and the development of high level programming languages enabled MT workers to code in a more problem - oriented way . The development in programming languages meant that it was becoming ever easier to code rules for translation in a meaningful manner and arguably improved the quality of these rules . The declarative nature of linguistic description could now be far more explicitly reflected in the design of programs for MT . . Early MT systems were predominantly parser - based , one of the first steps in such a system being to parse and tag the source language : see Section 5 on Parsing and Tagging . More recent current approaches to MT rely less on formal linguistic descriptions than the transfer approach described above . Translation Memory ( TM ) systems are now in widespread commercial use : see below and Chapter 10 of Arnold et al . ( 1994 ) . Example - Based Machine Translation ( EBMT ) is a relatively new technology which aims to combine both traditional MT and more recent TM paradigms by reusing previous translations and applying various degrees of linguistic knowledge to convert fuzzy matches into exact ones : see the Wikipedia article on EBMT . However , some early definitions of EBMT refer to what is now known as TM and they often exclude the concept of fuzzy matches . Essentially , Google Translat e begins by examining and comparing massive corpora of texts on the Web that have already been translated by human beings . It looks for matches between source and target texts and uses complex statistical analysis routines to look for statistically significant patterns , i.e. it works out the rules of the interrelationships between source and target texts for itself . As more and more corpora are added to the Web this means that Google Translate will keep improving until it reaches a point where it will be very difficult to tell that a machine has done the translation . I remember early machine translation tools translating \\\" Wie geht es dir ? \\\" as \\\" How goes it you ? \\\" Now Google Translate gets it right : \\\" How are you ? \\\" Thus we have , in a sense , come full circle in that Weaver 's ideas of applying statistical techniques are seen as a fruitful basis for MT . . 3.2 Commercial MT packages . There are many automatic translation packages on the market - as well as free packages on the Web . While such packages may be useful for extracting the gist of a text they should not be seen as a serious replacement for the human translator . Some are not all that bad , producing translations that are half - intelligible , letting you know whether a text is worth having translated properly . See : . Professional human translators are making increasing use of Translation Memory ( TM ) packages . TM packages store texts that have previously been translated , together with their source texts , in a large database . Chunks of new texts to be translated are then matched against the translated texts in the database and suggested translations are offered to the human translator wherever a match is found . The human translator has to intervene regularly in this process of translation , making corrections and amendments as necessary . TM systems can save hours of time ( estimated at up to 80 % of a translator 's time ) , especially when translating texts that are repetitive or that use lots of standard phrases and sentence formulations . Producing updates of technical manuals is a typical application of TM systems . Examples of TM systems include : . An example of automatic translations can be found at the Newstran website . This site is extremely useful for locating newspapers in a wide range of languages . You can also locate selected newspapers that have been translated using a Machine Translation system . Another approach to translation is the stored phrase bank , for example LinguaWrite , which was aimed at the business user and contained a large database of equivalent phrases and sentences in different languages to facilitate the writing of business letters . LinguaWrite was programmed by Marco Bruzzone in the 1980s and marketed by Camsoft , but it is no longer available and has not been updated . David Sephton 's Tick - Tack ( Primrose Publishing ) adopted a similar approach , beginning as a package consisting of \\\" building blocks \\\" of language for business communication , but it now embraces other topics . 3.3 Just for fun . 3.3.1 Some apocryphal stuff . The following examples have often been cited as mistakes made by machine translation ( MT ) systems . Whether they are real examples or not can not be verified . Russian - English : In a technical text that had been translated from Russian into English the term water sheep kept appearing . When the Russian source text was checked it was found that it was actually referring to a hydraulic ram . Russian - English : Idioms are often a problem . Russian - English : Another example , similar to the one above , is where out of sight , out of mind ended up being translated as the equivalent of blind and stupid . MT systems do , however , often make mistakes . The Systran MT system , which has been used by the European Commission , translated the English phrase pregnant women and children into des femmes et enfants enceints , which implies that both the women and the children are pregnant . Although it is an interpretation of the original phrase that is theoretically possible , it is also clearly wrong . 3.3.2 Translations of nursery rhymes . Try using an online machine translator to translate a text from English into another language and then back again . The results are often amusing , especially if you are translating nursery rhymes ! ( i ) Bah , bah , black sheep translated into French and then back again into English , using Babel Fish . English source text : Bah , bah , black sheep , have you any wool ? Yes sir , yes sir , three bags full . One for the master , one for the dame , and one for the little boy who lives down the lane . French translation : Bah , bah , mouton noir , vous ont n'importe quelles laines ? Oui monsieur , oui monsieur , trois sacs compl\\u00e8tement . Un pour le ma\\u00eetre , un pour dame , et un pour le petit gar\\u00e7on qui vit en bas de la ruelle . And back into English again : Bah , bah , black sheep , have you n ' imports which wools ? Yes Sir , yes Sir , three bags completely . For the Master , for lady , and for the little boy who lives in bottom of the lane . ( ii ) Humpty Dumpty translated into Italian and then back again into English , using Babel Fish . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty si \\u00e8 seduto su una parete . Humpty Dumpty ha avuto una grande caduta . I cavalli di tutto il re e gli uomini di tutto il re non hanno potuto un Humpty ancora . And back into English again : Humpty Dumpty has been based on a wall . Humpty Dumpty has had a great fall . The horses of all the king and the men of all the king have not been able a Humpty still . ( iii ) Humpty Dumpty translated into Italian and then back again into English , using Google Translate . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty sedeva su un muro . Humpty Dumpty ha avuto un grande caduta . Tutti i cavalli del re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme . And back into English again : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . All the king 's horses and all the king 's men could not put Humpty together again . Now , the above is an interesting result ! Google Translate used to be a very unreliable MT tool . It drives language teachers mad , as their students often use it to do their homework , e.g. translating from a given text into a foreign language or drafting their own compositions and then translating them . Mistakes made by Google Translate used to be very easy to spot , but ( as indicated above in Section 3.1 ) Google changed its translation engine a few years ago and now uses a Statistical Machine Translation ( SMT ) approach . The Humpty Dumpty translation back into English from the Italian appears to indicate that Google Translate has matched the whole text and got it right . Clever ! 3.3.3 Translations of business and journalistic texts . ( i ) A business text , translated with Google Translate and Babel Fish . Google Translate was used to translate the following text from German into English : Die Handelskammer in Saarbr\\u00fccken hat uns Ihre Anschrift zur Verf\\u00fcgung gestellt . Wir sind ein mittelgro\\u00dfes Fachgesch\\u00e4ft in Stuttgart , und wir spezialisieren uns auf den Verkauf von Personalcomputern . This was rendered as : The Chamber of Commerce in Saarbr\\u00fccken has provided us your address is available . We are a medium sized shop in Stuttgart , and we specialize in sales of personal computers . Babel Fish produced a better version : The Chamber of Commerce in Saarbruecken put your address to us at the disposal . We are a medium sized specialist shop in Stuttgart , and we specialize in the sale of personal computers . ( ii ) A journalistic text , translated with Google Translate and Babel Fish . Die deutsche Exportwirtschaft k\\u00e4mpft mit der weltweiten Konjunkturflaute und muss deshalb von den Zeiten zweistelligen Wachstums Abschied nehmen . [ Ludolf Wartenberg vom Bundesverband der Deutschen Industrie ] . This was rendered by Google Translate as : The German export economy is struggling with the global downturn and must therefore take the times of double - digit growth goodbye . [ Ludolf Wartenberg from the Federation of German Industry ] . The German export trade and industry fights with the world - wide recession and must take therefore from the times of two digit growth parting . [ Ludolf waiting mountain of the Federal association of the German industry . ] Computers are normally associated with two standard input devices , the keyboard and the mouse , and two standard output devices , the display screen and the printer . All these restrict language input and output . However , computer programs and hardware devices that enable the computer to handle human speech are now commonplace . All modern computers allow the user to plug in a microphone and record his / her own voice . A variety of other sources of sound recordings can also be used . Storing these sound files is not a problem anymore as a result of the immensely increased capacity and reduced cost of storage media and improved compression techniques that enable the size of sound files to be substantially reduced . For further information on the applications of sound recording and playback technology to CALL see Module 2.2 , Introduction to multimedia CALL . A range of computer software is available for speech analysis . Spoken input can be analysed according to a wide variety of parameters and the analysis can be represented graphically or numerically . Of course , graphic output is not immediately useful for the uninitiated viewer , and hence we are not arguing that this kind of graphical representation will prove useful to the language learner . On the other hand , specialists are well capable of interpreting this speech analysis data . The information we get from speech analysis has proven very valuable indeed for speech synthesis and speech recognition , which are dealt with in the following two sections . 4.1 Speech synthesis . Speech synthesis describes the process of generating human - like speech by computer . Producing natural sounding speech is a complex process in which one has to consider a range of factors that go beyond just converting characters to sounds because very often there is no one - to - one relation between them . The intonation of particular sentence types and the rhythm of particular utterances also have to be considered . Currently speech synthesis is far more advanced and more robust than speech recognition ( see Section 4.2 below ) . The naturalness of artificially produced utterances is now very impressive compared to what used to be produced by earlier speech synthesis systems in which the intonation and timing were far from natural and resulted in the production of monotonous , robot - like speech . Many people are now unaware that so - called talking dictionaries use speech synthesis software rather than recordings of human voices . In - car satellite navigation ( satnav ) systems can produce a range of different types of human voices , both male and female in a number of different languages , and \\\" talk \\\" to the car driver guiding him / her to a chosen destination . So far , however , speech synthesis has not been as widely used in CALL as speech recognition . This is probably due to the fact that language teachers ' requirements regarding the presentation of spoken language are very demanding . Anything that sounds artificial is likely to be rejected . Some language teachers even reject speakers whose regional accent is too far from what is considered standard or received pronunciation . There is , however , a category of speech synthesis technology known as Text To Speech ( TTS ) technology that is widely used for practical purposes . TTS software falls into the category of assistive technology , which has a vital role in improving accessiblity for a wide range of computer users with special needs , which is now governed by legislation in the UK . The Special Educational Needs and Disability Act ( SENDA ) of 2001 covers educational websites and obliges their designers \\\" to make reasonable adjustments to ensure that people who are disabled are not put at a substantial disadvantage compared to people who are not disabled . \\\" See JISC 's website on Disability Legislation and ICT in Further and Higher Education - Essentials . See the Glossary for definitions of assistive technology and accessiblity . TTS is important in making computers accessible to blind or partially sighted people as it enables them to \\\" read \\\" from the screen . TTS technology can be linked to any written input in a variety of languages , e.g. automatic pronunciation of words from an online dictionary , reading aloud of a text , etc . These are examples of TTS software : . Festival Speech Synthesis System : From the Centre for Speech Technology Research at the University of Edinburgh . Festival offers a general framework for building speech synthesis systems as well as including examples of various modules . Just for fun I entered the phrase \\\" Pas d'elle yeux Rh\\u00f4ne que nous \\\" into a couple of French language synthesisers . It 's a nonsense sentence in French but it comes out sounding like a French person trying to pronounce a well - known expression in English . Try it ! There are also Web - based tools that enable you to create animated cartoons or movies incorporating TTS , for example : . Voki enables you to create and customise your own speaking cartoon character . You can choose the TTS option ( as in Graham Davies 's example on the right ) to give the character a voice , or you can record your own voice . ReadTheWords : A tool that works in much the same way as Voki , but without the option of recording one 's own voice . An excellent tool that helps people with hearing impairments to learn how to articulate is the CSLU Speech Toolkit . To what extent speech synthesis systems are suitable for CALL is a matter for further discussion . See the article by Handley & Hamel ( 2005 ) , who report on their progress towards the development of a benchmark for determining the adequacy of speech synthesis systems for use in CALL . The article mentions a Web - based package called FreeText , for advanced learners of French , the outcome of a project funded by the European Commission . 4.2 Speech recognition . Speech recognition describes the use of computers to recognise spoken words . Speech recognition has not reached such a high level of performance as speech synthesis ( see Section 4.1 above ) , but it has certainly become usable in CALL in recent years . EyeSpeak English is a typical example of the use of speech recognition software for helping students improve their English pronunciation . Speech recognition is a non - trivial task because the same spoken word does not produce entirely the same sound waves when uttered by different people or even when uttered by the same person on different occasions . The process is complex : the computer has to digitise the sound , transform it to discard unneeded information , and then try to match it with words stored in a dictionary . The most efficient speech recognition systems are speaker - dependent , i.e. they are trained to recognise a particular person 's speech and can then distinguish thousands of words uttered by that person . If one remembers that each of the parameters analysed could have been affected by some speaker - independent background noise or by some idiosyncratic pronunciation features of this particular speaker then it already becomes clear how difficult the interpretation of the analysis data is for a speech recognition program . The following information is taken from an article written by Norman Harris of DynEd , a publisher of CALL software incorporating ASR : . Speech recognition technology has finally come of age - at least for language training purposes for young adults and adults . The essence of real language is not in discrete single words - language students need to practice complete phrases and sentences in realistic contexts . Moreover , programs which were trained to accept a speaker 's individual pronunciation quirks were not ideally suited to helping students move toward more standard pronunciation . These technologies also failed if the speaker 's voice changed due to common colds , laryngitis and other throat ailments , rendering them useless until the speaker recovered or retrained the speech engine . The solution to these problems came with the development of continuous speech recognition engines that were speaker independent . These programs are able to deal with complete sentences spoken at a natural pace , not just isolated words . Such flexibility with regard to pronunciation paradigms means that today 's speaker - independent speech recognition programs are not ideal for direct pronunciation practice . Nonetheless , exercises which focus on fluency and word order , and with native speaker models which are heard immediately after a student 's utterance had been successfully recognized , have been shown to indirectly result in much improved pronunciation . Another trade off is that the greater flexibility and leniency which allows these programs to \\\" recognize \\\" sentences spoken by students with a wide variety of accents , also limits the accuracy of the programs , especially for similar sounding words and phrases . Some errors may be accepted as correct . Native speakers testing the \\\" understanding \\\" of programs \\\" tuned \\\" to the needs of non - native speakers may be bothered by this , but most teachers , after careful consideration of the different needs and psychologies of native speakers and learners , will accept the trade off . Students do not expect to be understood every time . If they are required occasionally to repeat a sentence which the program has not recognized or which the program has misinterpreted , there may be some small frustration , but language students are much more likely to take this in their stride than would native speakers . On the other hand , if the program does \\\" understand \\\" such students , however imperfect their pronunciation , they typically experience a huge sense of satisfaction , a feel good factor native speakers simply can not enjoy to anywhere near the same degree . The worst thing for a student is a program that is too demanding of perfection - such programs will quickly lead to student frustration or the kind of embarrassed , hesitant unwillingness to speak English typical of many classrooms . Even if we accept that accuracy needs to be responsive to proficiency in order to encourage students to speak , we must , as teachers , be concerned that errors do not become reinforced . A recent breakthrough is the implementation of apps such as Apple 's Siri on the iPhone 4S and Evi , which is available for the iPhone and the Android . These apps are quite impressive at recognising speech and providing answers to questions submitted by the user . Evi 's performance was tested by the author of this paragraph . \\\" She \\\" immediately provided correct answers to these questions submitted by voice input : . In which American state is Albuquerque ? In addition , Evi may link to relevant websites that provide further information . Text input is also accepted . In this section we outline the essentials of parsing , first of all by describing the components of a parsing system and then discussing different kinds of parser . We look at one linguistic phenomenon which causes problems for parsing and finally examine potential solutions to the difficulties raised by parsing . Put in simple terms , a parser is a program that maps strings of a language into its structures . The most basic components needed by a parser are a lexicon containing words that may be parsed and a grammar , consisting of rules which determine grammatical structures . The first parsers were developed for the analysis of programming languages ; obviously as artificial , regular languages they present fewer problems than a natural language . It is most useful to think of parsing as a search problem which has to be solved . It can be solved using an algorithm which can be defined as : . [ ... ] a formal procedure that always produces a correct or optimal result . An algorithm applies a step - by - step procedure that guarantees a specific outcome or solves a specific problem . The procedure of an algorithm performs a computation in a finite amount of time . Programmers specify the algorithm the program will follow when they develop a conventional program . ( Smith 1990 ) . Parsing algorithms define a procedure that looks for the optimum combination of grammatical rules that generate a tree structure for the input sentence . How might we define these grammatical rules in a concise way that is amenable to computer processing ? A useful construct for our purposes is a so - called context - free grammar ( CFG ) . A CFG consists of rules containing a single symbol on the left - hand side and one or more on the right - hand side . For example , the statement that a sentence can consist of : . a noun phrase and a verb phrase can be expressed by the following rewrite rule . S \\u00ae NP VP . This means that a sentence S can be ' rewritten ' as a noun phrase NP followed by a verb phrase VP which are in their turn defined in the grammar . A noun phrase , for example , can consist of a determiner DET and a noun N. These symbols are known as non - terminals and the words represented by these symbols are terminal symbols . Parsing algorithms can proceed top - down or bottom - up . In some cases , top - down and bottom - up algorithms can be combined . Below are simple descriptions of two parsing strategies . 5.1.1 Top Down ( depth first ) . Top down strategy works from non - terminal symbols : . S \\u00ae NP VP . and then breaks them down into constituents . The strategy assumes we have an S and tries to fit it in . If we choose to search depth first , then we proceed down one side of the tree at a time . The search will end successfully if it manages to break down the sentence into all its terminal symbols ( words ) . 5.1.2 Bottom up ( breadth first ) . A bottom up strategy looks at elements of an S and assigns categories to them to form larger constituents until we arrive at an S. If we choose to search breadth first , then we proceed consecutively through each layer and stop successfully once we have constructed a sentence . Let 's look now at one linguistic phenomenon which causes problems for parsers - that of so - called attachment ambiguity . Consider the following sentence : . The man saw the man in the park with a telescope . Parser output can be represented as a bracketed list or , more commonly , a tree structure . Here is the output of two possible parses for the sentence above . One way of dealing with the problem of sentences which have more than one possible parse is to concentrate on specific elements of the parser input and to not deal with such phenomena as attachment ambiguity . Ideally we expect a parser to successfully analyse a sentence on the basis of its grammar , but often there are problems caused by errors in the text or incompleteness of grammar and lexicon . Also the length of sentences and ambiguity of grammars often make it hard to successfully parse unrestricted text . An approach which addresses some of these issues is partial or shallow parsing . Abney ( 1997:125 ) succinctly describes partial parsing thus : . \\\" Partial parsing techniques aim to recover syntactic information efficiently and reliably from unrestricted text , by sacrificing completeness and depth of analysis . \\\" Partial parsers concentrate on recovering pieces of sentence structure which do not require large amounts of information ( such as lexical association information ) ; attachment remains unresolved for instance . We can see that in this way parsing efficiency is greatly improved . Another strategy for analysing language is part - of - speech tagging , in which we do not seek to find larger structures such as noun phrases but instead label each word in a sentence with its appropriate part of speech . Here is the original paragraph from Section 3 of this document : . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . The following table shows the tagger output , and we can see that most of the words have been correctly identified . additional . languages . NNS . language . required . VBN . require . . . SENT . . . As with partial parsing , we are not trying to find correct attachments and since it is a limited task the success rate is quite high . The information derived from tagging can itself have input into partial parsing or into improving the performance of traditional parsers . Some of the decision task as to what is the correct part of speech to assign to a word is based on the probability of two or three word sequences ( bigrams and trigrams ) occurring , even where words can be assigned more than one part of speech . For instance , in our example tagged text the sequence ' the transfer module ' occurs . Transfer is of course also a verb , but the likelihood of a determiner ( the ) being followed by a verb is lower than the likelihood of a determiner noun sequence . See als o the Visual Interactive Syntax Learning ( VISL ) website . An online parser and a variety of other tools concerned with English grammar , including games and quizzes , can be found here . 5.1.3 Parsing erroneous input . Of course , in CALL we are dealing with texts that have been produced by language learners at various levels of proficiency and accuracy . It is therefore reasonable to assume that the parser has to be prepared to deal with linguistic errors in the input . One thing we could do is to complement our grammar for correct sentences with a grammar of incorrect sentences - an error grammar , i.e. we capture individual and/or typical errors in a separate rule system . The advantage of this error grammar approach is that the feedback can be very specific and is normally fairly reliable because this feedback can be attached to a very specific rule . The big drawback , however , is that individual learner errors have to be anticipated in the sense that each error needs to be covered by an adequate rule . However , as already stated it is not only in texts that have been produced by language learners that we find erroneous structures . Machine translation is facing similar problems . Dina & Malnati review approaches \\\" concerning the design and the implementation of grammars able to deal with ' real input ' . \\\" ( Dina & Malnati 1993:75 ) . They list four approaches : . The rule - based approach which relies on two sets of rules : one for grammatical input and the other for ungrammatical input . Dina & Malnati point out quite rightly that normally well - formedness conditions should be sufficient and the second set of rules results in linguistic redundancy . The main problem with using this approach in a parser - based CALL system is the problem of having to anticipate the errors learners are likely to make . The metarule - based approach uses a set of well - formedness rules and if none of them can be applied calls an algorithm that relaxes some constraints and records the kind of violation . Dina & Malnati note the procedurality of the algorithm causes problems when confronted with multiple errors - something very likely in any text produced by a language learner . The preference - based approach comprises an overgenerating grammar and a set of preference rules . \\\" [ ... ] each time a formal condition is removed from a b - rule to make its applicability context wider , a preference rule must be added to the grammar . Such a p - rule must be able to state - in the present context of the b - rule - the condition that has been previously removed . \\\" ( Dina & Malnati 1993:78 ) Again a source for linguistic redundancy which might result in inconsistencies in the grammar . They claim that , due to the overgeneration of possible interpretations , \\\" the system would be completely unusable in an applied context . \\\" ( ibid.:79 ) . The constraint - based approach is based on the following assumptions : . each ( sub)tree is marked by an index of error ( initially set to 0 ) ; . the violation of a constraint in a rule does not block the application , but increases the error index of the generated ( sub)tree ; . at the end of parsing the object marked by the smallest index is chosen . Consequently , the \\\" most plausible interpretation of a [ ... ] sentence is the one which satisfies the largest number of constraints . \\\" ( Dina & Malnati 1993:80 ) . We have seen in Section 3 that Machine Translation ( MT ) and the political and scientific interest in machine translation played a significant role in the acceptance ( or non - acceptance ) as well as the general development of Human Language Technologies . By 1964 , however , the promise of operational MT systems still seemed distant and the sponsors set up a committee , which recommended in 1966 that funding for MT should be reduced . It brought to an end a decade of intensive MT research activity . ( Hutchins 1986:39 ) . It is then perhaps not surprising that the mid-1960s saw the birth of another discipline : Computer Assisted Language Learning ( CALL ) . The PLATO project , which was initiated at the University of Illinois in 1960 , is widely regarded as the beginning of CALL - although CALL was just part of this huge package of general Computer Assisted Learning ( CAL ) programs running on mainframe computers . PLATO IV ( 1972 ) was probably the version of this project that had the biggest impact on the development of CALL ( Hart 1995 ) . At the same time , another American university , Brigham Young University , received government funding for a CALL project , TICCIT ( Time - Shared Interactive , Computer Controlled Information Television ) ( Jones 1995 ) . Other well - known and still widely used programs were developed soon afterwards : . The CALIS / WinCALIS ( Computer Aided Language Instruction System ) authoring tools , Duke University ( Borchardt 1995 ) . The TUCO package for learners of German , developed by Heimy Taylor and Werner Haas , Ohio State University . See Module 3.2 , Section 5.9 . The CLEF package for learners of French , which was produced by a consortium of Canadian universities in the late 1970s and is still going strong today . See Module 3.2 , Section 5.9 . In the UK , John Higgins developed Storyboard in the early 1980s , a total Cloze text reconstruction program for the microcomputer ( Higgins & Johns 1984:57 ) . ( Levy 1997:24 - 25 ) describes how other programs extended this idea further . See Section 8.3 , Module 1.4 , headed Total text reconstruction : total Cloze , for further information on total Cloze programs . In recent years the development of CALL has been greatly influenced by the technology and by our knowledge of and our expertise in using it , so that not only the design of most CALL software , but also its classification has been technology - driven . For example , Wolff ( 1993:21 ) distinguished five groups of applications : . The late 1980s saw the beginning of attempts which are mostly subsumed under Intelligent CALL ( ICALL ) , a \\\" mix of AI [ Artificial Intelligence ] techniques and CALL \\\" ( Matthews 1992b : i ) . Early AI - based CALL was not without its critics , however : . And that , fundamentally , is why my initial enthusiasm has now turned so sour . ( Last 1989:153 ) . For a more up - to - date and positive point of view of Artifical Intelligence , see Dodigovic ( 2005 ) . Bowerman ( 1993:31 ) notes : \\\" Weischedel et al . ( 1978 ) produced the first ICALL [ Intelligent CALL ] system which dealt with comprehension exercises . It made use of syntactic and semantic knowledge to check students ' answers to comprehension questions . \\\" As far as could be ascertained , this was just the early swallow that did not create a summer . Kr\\u00fcger - Thielmann ( 1992:51ff . ) lists and summarises the following early projects in ICALL : ALICE , ATHENA , BOUWSTEEN & COGO , EPISTLE , ET , LINGER , VP2 , XTRA - TE , Zock . Matthews ( 1993:5 ) identifies Linguistic Theory and Second Language Acquisition Theory as the two main disciplines which inform Intelligent CALL and which are ( or will be ) informed by Intelligent CALL . He adds : \\\" the obvious AI research areas from which ICALL should be able to draw the most insights are Natural Language Processing ( NLP ) and Intelligent Tutoring Systems ( ITS ) \\\" ( Matthews1993:6 ) . Matthews shows that it is possible to \\\" conceive of an ICALL system in terms of the classical ITS architecture \\\" ( ibid . ) The system consists of three modules - expert , student and teacher module - and an interface . The expert module is the one that \\\" houses \\\" the language knowledge of the system . It is this part which can process any piece of text produced by a learner - in an ideal system . This is usually done with the help of a parser of some kind : . ( Holland et al . 1993:28 ) . This notion of parser - based CALL not only captures the nature of the field much better than the somewhat misleading term \\\" Intelligent CALL \\\" ( Is all other CALL un - intelligent ? ) , it also identifies the use of Human Language Technologies as one possible approach in CALL alongside others such as multimedia - based CALL and Web - based CALL and thus identifies parser - based CALL as one possible way forward for CALL . In some cases , the ( technology - defined ) borders between these sub - fields of CALL are not even clearly identifiable , as we will see in some of the projects mentioned in the following paragraphs . To exemplify recent advances in the use of sophisticated human language technology in CALL , let us have a look at some of the projects that were presented at two conferences in the late 1990s . The first one is the Language Teaching and Language Technology conference in Groningen in 1997 ( Jager et al . 1998 ) . Witt & Young ( 1998 ) , on the other hand , are concerned with assessing pronunciation . They implemented and tested a pronunciation scoring algorithm which is based on speech recognition ( see Section 4.2 ) and uses hidden Markov models . \\\" The results show that - at least for this setup with artificially generated pronunciation errors - the GOP [ goodness of pronunciation ] scoring method is a viable assessment tool . \\\" A third paper on pronunciation at this conference , by Skrelin & Volskaja ( 1998 ) outlined the use of speech synthesis ( see Section 4.1 ) in language learning and lists dictation , distinction of homographs , a sound dictionary and pronunciation drills as possible applications . \\\" The project vision foresees two main areas where GLOSSER applications can be used . First , in language learning and second , as a tool for users that have a bit of knowledge of a foreign language , but can not read it easily or reliably \\\" ( Dokter & Nerbonne 1998:88 ) . Dokter & Nerbonne report on the French - Dutch demonstrator running under UNIX . The demonstrator : . uses morphological analysis to provide additional grammatical information on individual words and to simplify dictionary look - up ; . relies on automatic word selection ; . offers the opportunity to insert glosses ( taken form the dictionary look - up ) into the text ; . relies on string - based word sense disambiguation ( \\\" Whenever a lexical context is found in the text that is also provided in the dictionary , the example in the dictionary is highlighted . \\\" ( op.cit.:93 ) . Roosma & Pr\\u00f3sz\\u00e9ky ( 1998 ) draw attention to the fact that GLOSSER works with the following language pairs : English - Estonian - Hungarian , English - Bulgarian , French - Dutch and describe a demonstrator version running under Windows . Dokter et al ( 1998 ) conclude in their user study \\\" that Glosser - RuG improves the ease with which language students can approach a foreign language text \\\" ( Dokter et al . 1998:175 ) . The latter project relies on a spellchecker , morphological analyser , syntactic parser and a lexical database for Basque , and the authors report on the development of an interlanguage model . At another conference ( UMIST , May 1998 ) , which brought together a group of researchers who are exploring the use of HLT in CALL software , Schulze et al . ( 1999 ) and Tschichold ( 1999 ) discussed strategies for improving the success rate of grammar checkers . Menzel & Schr\\u00f6der ( 1999 ) described error diagnosis in a multi - level representation . The demonstration system captures the relations of entities in a simple town scenery . The available syntactic , semantic and pragmatic information is checked simultaneously for constraint violations , i.e. errors made by the language learners . Visser ( 1999 ) introduced CALLex , a program for learning vocabulary based on lexical functions . Diaz de Ilarraza et al . ( 1999 ) described aspects of IDAZKIDE , a learning environment for Spanish learners of Basque . The program contains the following modules : wide - coverage linguistic tools ( lexical database with 65,000 entries ; spell checker ; a word form proposer and a morphological analyser ) , an adaptive user interface and a student modelling system . The model of the students ' language knowledge , i.e. their interlanguage , is based on a corpus analysis ( 300 texts produced by learners of Basque ) . Foucou & K\\u00fcbler ( 1999 ) presented a Web - based environment for teaching technical English to students of computing . Ward et al . ( 1999 ) showed that Natural Language Processing techniques combined with a graphical interface can be used to produce meaningful language games . Davies & Poesio ( 1998 ) reported on tests of simple CALL prototypes that have been created using CSLUrp , a graphical authoring system for the creation of spoken dialogue systems . They argue that since it is evident that today 's dialogue systems are usable in CALL software , it is now possible and necessary to study the integration of corrective feedback in these systems . Mitkov ( 1998 ) outlined early plans for a new CALL project , The Language Learner 's Workbench . It is the aim of this project to incorporate a number of already available HLT tools and to package them for language learners . These examples of CALL applications that make use of Human Language Technologies are by no means exhaustive . They not only illustrate that research in HLT in CALL is vibrant , but also that HLT has an important contribution to make in the further development of CALL . Of course , both disciplines are still rather young and many projects in both areas , CALL and HLT , have not even reached the stage of the implementation of a fully functional prototype yet . A number of CALL packages that make use of speech recognition have reached the commercial market and are being used successfully by learners all over the world ( see Section 4.2 ) . Speech synthesis , certainly at word level , has achieved a clarity of pronunciation that makes it a viable tool for language learning ( see Section 4.1 ) . Many popular electronic dictionaries now incorporate speech synthesis systems . Part - of - speech taggers have reached a level of accuracy that makes them usable in the automatic pre - processing of learner texts . Morphological analysers for a number of languages automatically provide grammatical information on vocabulary items in context and make automatic dictionary look - ups of inflected or derived word forms possible . This progress in HLT and CALL has mainly been possible as the result of our better understanding of the structures of language - our understanding of linguistics . The lack of linguistic modelling and the insufficient deployment of Natural Language Processing techniques has sometimes been given as one reason for the lack of progress in some areas of CALL : see , for example , Levy ( 1997:3 ) , citing Kohn ( 1994 ) . [ ... ] Kohn suggests that current CALL is lacking because of poor linguistic modelling , insufficient deployment of natural language processing techniques , an emphasis on special - purpose rather than general - purpose technology , and a neglect of the ' human ' dimnesion of CALL ( Kohn 1994:32 ) . The examples in the previous section have shown that it is possible to apply certain linguistic theories ( e.g. phonology and morphology ) to Human Language Technologies and implement this technology in CALL software . This is , of course , true . However , it does not mean that interesting fragments or aspects of a given language can not be captured by a formal linguistic theory and hence implemented in a CALL application . In other words , if one can not capture the German language in its entirety in order to implement this linguistic knowledge in a computer program , this does not mean that one can not capture interesting linguistic phenomena of that language . This means even if we are only able to describe a fragment of a given language adequately we can still make very good use of this description in computer applications for language learning . What is the kind of knowledge we ought to have about language before we can attempt to produce an HLT tool that can be put to effective use in CALL ? Let us look at one particular aspect of language - grammar . In recent years , the usefulness of conscious learning of grammar has been discussed time and again , very often in direct opposition to what has been termed \\\" the communicative approach \\\" . ( ibid.:6 ) This assumption leads to the question of what role exactly the computer ( program ) has to play in a sensitive , rich and enjoyable grammar - learning process . The diversity of approaches outlined in this special issue of ReCALL on grammar illustrates that there are many different roads to successful grammar learning that will need to be explored . In this module , only the example of parser - based CALL will be discussed . Let us take a grammar checker for language learners as a specific example in point . This grammar checker could then be integrated into a CALL program , a word - processor , an email editor , a Web page editor etc . The design of such a grammar checker is mainly based on findings in theoretical linguistics and second language acquisition theory . Let us start with second language acquisition theory . Research in second language acquisition has proved that grammar learning can lead to more successful language acquisition . Here learners have the opportunity to correct grammatical errors and mistakes that they have made while concentrating on the subject matter and the communicative function of the text . It is at this stage that a grammar checker for language learners can provide useful and stimulating guidance . In order to ascertain the computational features of such a grammar checker , let us first consider what exactly we mean by \\\" grammar \\\" in a language learning context . Helbig discusses possible answers to this question from the point of view of the teaching and learning of foreign languages in general : . As a starting point for answering our question concerning the relevance ( and necessity ) of grammar in foreign language teaching we used a differentiation of what was and is understood by the term \\\" grammar \\\" : . Grammar A : the system of rules that is inherent to the object language itself and is independent of the fact whether it has been captured by Linguistics or not ; . Grammar B : the scientific - linguistic description of the language inherent system of rules , the modelling of Grammar A by Linguistics ; . Grammar C : the system of rules intern to the speaker and listener which is formed in the head of the learner during language acquisition and which forms the basis for him / her to produce and understand correct sentences and texts and to use them appropriately in communication . ( Helbig:1975 ) . Helbig identifies further a Grammar B1 and a Grammar B2 - the former being a linguistic grammar and the latter being a learner grammar . The description of grammar B1c is a literal translation of Helbig 's wording - in the terminology used now , the term \\\" interlanguage \\\" appears to be the most appropriate . The application of Helbig 's grammar classification to CALL produces the following results : . Grammar A remains as defined by Helbig . In other words it refers to the target grammar of the interlanguage continuum . Grammar B1a is the grammar which enables the parser to process grammatically well - formed sentences in the target language . Grammars B1b , B1c and B2 enable the grammar checking CALL tool to detect errors in the learner input and provide the linguistic information to generate feedback . Grammar C is the grammar system which the CALL tool should help to correct and expand . Additionally , the grammar checker will gather data for learner profiles which should allow useful insights into the development of Grammar C of learners you have used the program . Consequently , Grammar B in its entirety and Grammar C will have to be considered first and foremost when developing the grammar checker . The question then arises : If Grammar A provides the linguistic data for the parser developer , how can we \\\" feed \\\" these different grammars into a computer program ? The computer requires that any grammar which we intend to use in any program ( or programming language , for that matter ) be mathematically exact . Grammars which satisfy this condition are normally referred to as formal grammars . The mathematical description of these grammars uses set theory . Therefore , a language L is said to have a vocabulary V . If there were no restrictions on how to construct strings , the number of possible strings is infinite . This becomes clear when one considers that each vocabulary item of V could be repeated infinitely in order to construct a string . However , as language learners in particular know any language L adheres to a finite set of ( grammar ) rules . This explains why grammar teaching software that attempts to anticipate possible incorrect answers can only do this successfully if the answer domain is severely restricted and the anticipation process will therefore much simpler . Could a computer program perform this task - a task based on infinite possibilities ? Yes , it could - but not based on infinite possibilities . That is why it will be necessary to look for an approach which is based on a finite set of possibilities , which can then be pre - programmed . Let us therefore consider L the set of strings that can be constructed using the ( formal ) grammar G . A formal grammar can be defined as follows ( see e.g. Allen 1995 ): . G ( VN , VT , R , S ) . And here we are already dealing with sets which have a finite number of members . The number of grammatical rules is fairly limited . This is certainly the case when we only consider the basic grammar rules of a language that will have to be learned by the intermediate to early advanced learner . ( Note here what we said earlier about Grammar B2 - the learner grammar : It was only a subset of Grammar 1 - the linguistic grammar . ) Formal grammars have been used in a number of CALL projects . Matthews ( 1993 ) continues his discussion of grammar frameworks for CALL which he started in 1992 ( Matthews 1992a ) . He lists eight major grammar frameworks that have been used in CALL : . Of course , these are only some examples . More recently , Tschichold et al . ( 1994 ) reported on a prototype for correcting English texts produced by French learners . This system relies on a number of different finite state automata for pre - processing , filtering and detecting ( Tschichold et al.1994 ) . Brehony & Ryan ( 1994 ) report on \\\" Francophone Stylistic Grammar Checking ( FSGC ) using Link Grammars \\\" . They adapted the post - processing section of an existing parser so that it would detect stylistic errors in English input produced by French learners . His plea is for the use of the PPT ( Principles and Parameters Theory ( Chomsky 1986 ) as a grammar framework for CALL applications , basing his judgement on three criteria : computational effectiveness , linguistic perspicuity and acquisitional perspicuity ( Matthews 1993:9 ) . In later parts of his paper , Matthews compares rule- and principle - based frameworks using DCGs ( Definite Clause Grammars ) as the example for the latter . He concludes that principle - based frameworks ( and consequently principle - based parsing ) are the most suitable grammar frameworks for what he calls Intelligent CALL . Recently , other unification - based grammar frameworks not included in Matthews ' list have been used in CALL . Hagen , for instance , describes \\\" an object - oriented , unification - based parser called HANOI \\\" ( Hagen 1995 ) which uses formalisms developed in Head - Driven Phrase Structure Grammar ( HPSG ) . He quotes Zajac : . Combining object - oriented approaches to linguistic description with unification - based grammar formalisms [ ... ] is very attractive . On one hand , we gain the advantages of the object - oriented approach : abstraction and generalisation through the use of inheritance . On the other hand , we gain a fully declarative framework , with all the advantages of logical formalisms [ ... ] . Of course , not even this extended list is comprehensive - at best it could be described as indicative of the variety of linguistic approaches used in parser - based Computer Assisted Language Learning and , in particular , in the field of grammar checking . At the end of this short excursion into formal grammar(s ) it can be concluded that any CALL grammar checker component needs as its foundation a formal grammar describing as comprehensively as possible the knowledge we have about the target language grammar . This was the grammar that Helbig ( 1975 ) refers to as Grammar B1a . But what part do the other grammars play in a CALL environment ? Let us stay with the example of a parser - based grammar checker for language learners . Hence , the provision of adequate feedback on the morpho - syntactic structure of parts of the text produced by learners is the most important task for this parser - based grammar checker . Let us therefore consider the place of feedback provision within a parser grammar . In other words , as a good teacher would do - the grammar checker would offer advice on how to change an ungrammatical structure into a corresponding grammatically well - formed structure . As stated earlier this approach would be based on an infinite number of construction possibilities . Therefore , the provision of adequate feedback and help to the learner appears to be difficult if not impossible . However , it has been indicated above that feedback could be linked to the finite sets on which the formal grammar relies . How can this be done ? Each member of the three sets which will have to be considered here . The non - terminal symbols like NP and VP , the words and the set of morpho - syntactic rules carry certain features that determine their behaviour in a sentence and determine their relation to other signs within the sentence . These features which restrict what the text producer can do with a given ( terminal or non - terminal ) symbol in a sentence and under what conditions a particular grammatical rule has to be applied will be labelled constraints . Let us return to our provisional description of feedback , which can now be formulated more precisely . Feedback shows the relation . by explaining the underlying constraint of the anticipated construction in L based on Grammar B2 . to support production of construction in L . and by reasoning about the likely cause of the rule violation . to extend Grammar C - the learner - inherent grammar . And secondly , this above description of feedback given by a grammar checker which is based on a modified parser shows that it is possible to construct tools that support the focus on form by learners during the reflection stage of a text production process . Even if the grammar checker were only to detect a small number of morpho - syntactic errors , this would be beneficial for the learners as long as they were aware of the limitations of this CALL tool . On the other hand , the feedback description contains still a number of question marks in parentheses after some of the important keywords - whether the intended aims of grammar checking can be achieved can only be validated through the use and thorough testing of such a grammar checker . We should better not make any such assumption ( in the scientific sense - we do hope for these improvements , of course ) and better wait until such a parser - based grammar checker is actually tested in a series of proper learning experiments . Let us now leave the discussion of some of the underlying linguistics behind and discuss the role of parser - based applications in language learning . Natural language parsers take written language as their input and produce a formal representation of the syntactic and sometimes semantic structure of this input . The role they have to play in computer - assisted language learning has been under scrutiny in the last decade : ( Matthews 1992a ) ; ( Holland et al . 1993 ) ; ( Nagata 1996 ) . See also Heift ( 2001 ) . Holland et al . discussed the \\\" possibilities and limitations of parser - based language tutors \\\" ( Holland et al . 1993:28 ) . Comparing parser - based CALL to what they label as conventional CALL they come to the conclusion that : . [ ... ] in parser - based CALL the student has relatively free rein and can write a potentially huge variety of sentences . ICALL thus permits practice of production skills , which require recalling and constructing , not just recognising [ as in conventional CALL ] , words and structures . ( Holland et al.1993:31 ) . However , at the same time , parsing imposes certain limitations . Parsers tend to concentrate on the syntax of the textual input , thus \\\" ICALL may actually subvert a principal goal of language pedagogy , that of communicating meanings rather than producing the right forms \\\" ( Holland et al.1993:32 ) . This disadvantage can be avoided by a \\\" focus on form \\\" which is mainly achieved by putting the parser / grammar checker to use within a relevant , authentic communicative task and at a time chosen by and convenient to the learner / text producer . Juozulynas ( 1994 ) evaluated the potential usefulness of syntactic parsers in error diagnosis . He analysed errors in an approximately 400 page corpus of German essays by American college students in second - year language courses . His study shows that : . [ ... ] syntax is the most problematic area , followed by morphology . ( Juozulynas 1994:5 ) . Juozulynas adapted a taxonomic schema by Hendrickson which comprises four categories : syntax , morphology , orthography , lexicon . Juozulynas ' argument for splitting orthography into spelling and punctuation is easily justified in the context of syntactic parsing . Parts of punctuation can be described by using syntactic bracketing rules , and punctuation errors can consequently be dealt with by a syntactic parser . Lexical and spelling errors form , according to Juozulynas , a rather small part of the overall number of learner errors . Some of these errors will , of course , be identified during dictionary look - up , but if words that are in the dictionary are used in a nonsensical way , the parser will not recognise them unless specific error rules ( e.g. for false friends ) are built in . Consequently , a parser - based CALL application can play a useful role in detecting many of the morpho - syntactic errors which constitute a high percentage of learner errors in freely produced texts . Nevertheless , the fact remains : . A second limitation of ICALL is that parsers are not foolproof . Because no parser today can accurately analyse all the syntax of a language , false acceptance and false alarms are inevitable . ( Holland et al . 1993:33 ) . This is something not only developers of parser - based CALL , but also language learners using such software have to take into account . In other words , this limitation of parser - based CALL has to be taken into consideration during the design and implementation process and when integrating this kind of CALL software in the learning process . A final limitation of ICALL is the cost of developing NLP systems . By comparison with simple CALL , NLP development depends on computational linguists and advanced programmers as well as on extensive resources for building and testing grammars . Beyond this , instructional shells and lessons must be built around NLP , incurring the same expense as developing shells and lessons for CALL . ( Holland et al . 1993:33 ) . It is mainly this disadvantage of parser - based CALL that explains the lack of commercially available ( and commercially viable ) CALL applications which make good use of HLT . However , it is to be hoped that this hurdle can be overcome in the not too distant future because sufficient expertise in the area has accumulated over recent years . More and more computer programs make good use of this technology , and many of these have already \\\" entered \\\" the realm of computer - assisted language learning , as can be seen from the examples in the previous section . Holland et al . ( 1993 ) answer their title question \\\" What are parsers good for ? \\\" on the basis of their own experience with BRIDGE , a parser - based CALL program for American military personnel learning German , and on the basis of some initial testing with a small group of experienced learners of German . They present the following points : . ICALL appears to be good for form - focused instruction offering learners the chance to work on their own linguistic errors by this method , not only to improve their performance in the foreign language but also to improve their language awareness . ICALL appears to be good for selected kinds of students . The authors list the following characteristics which might influence the degree of usefulness of ICALL for certain students : . i. intermediate proficiency . ii . analytical orientation . iii . tolerance of ambiguity . iv . confidence as learners . ICALL is good for research because the parser automatically tracks whole sentence responses and detects , classifies , and records errors . It might facilitate the assessment of the students grammatical competency and thus help us discover patterns of acquisition . ICALL [ ... ] can play a role in communicative practice . The authors argue for the embedding of parser - based CALL in \\\" graphics microworlds \\\" which help to capture some basic semantics . Given that we would like to harness the advantages of parser - based CALL , how do we take the limitations into consideration in the process of designing and implementing a parser - based CALL system ? What implications does the use of parsing technology have for human - computer interaction ? [ I]n discourse analytic terms ( Grice 1975 ) , the nature of the contract between student and CALL tutor is straightforward , respecting the traditional assumption that the teacher is right , whereas the ICALL contract is less well defined . ( Holland et al . 1993:33f . ) . The rigidity of traditional CALL in which the program controls the linguistic input by the learner to a very large extent has often given rise to a criticism of CALL which accuses CALL developers and practitioners of relying on behaviourist programmed instruction . If one wants to give learners full control over their linguistic input , for example , by relying on parsing technology , what are then the terms according to which the communicative interaction of computer ( program ) and learner can be defined ? The differences between humans and machines have obviously to be taken into consideration in order to understand the interaction of learners with CALL programs : . Machines are compiled out of individual parts for a very specific purpose ( totum fix et partibus ) ; whereas humans are holistic entities whose parts can be differentiated ( partes fiunt ex toto ) . Humans process all sorts of experiences and repeatedly and interactively create their own environment - something machines can not do . They \\\" calculate \\\" a problem on the basis of pre - wired rules . The main features of human thoughts are the inherent contradictions and the ability to cope with them , something that will not be calculable due to its complexity , variety and degree of detail ( Schmitz 1992:209f . ) . These differences between humans and machines can for our purposes , i.e. the theoretical description of human - computer interaction , be legitimately reduced to the distinction between actions and operations as is done in Activity Theory . The main point of this theory for our consideration here is that communicative activities can be divided into actions which are intentional , i.e. goal - driven ; and these can be sub - divided into operations which are condition - triggered . These operations are normally learnt as actions . In the example referred to in the quotation above , the gear - switching is learnt as an action . The learner - driver is asked by the driving instructor to change gear and this becomes the goal of the learner . Once the learner - driver has performed this action a sufficient number of times , this action becomes more and more automated and in the process loses more and more of its intentionality . A proficient driver might have the goal to accelerate the car which will necessitate switching into higher gear , but this is now triggered by a condition ( the difference between engine speed and speed of the car ) . It can thus be argued that humans learn to perform complex actions by learning to perform certain operations in a certain order . Machines , on the other hand , are only made to perform certain ( sometimes rather complex ) operations . This has some bearing on our understanding of the human - computer interaction that takes place when a learner uses language - learning software . When , for instance , the spell checker is started in a word - processing package , the software certainly does not have ' proof - read ' the learner 's document . The computer just responds to the clicking of the spellchecker menu item and performs the operation of checking the strings in the document against the entries in a machine dictionary . For the computer user ( in our case a learner ) , it might look like the computer is proof - reading the document . Normally , one only realises that no \\\" proper \\\" document checking is going on if a correctly spelled word is not found in the dictionary or nonsensical alternatives are given for a simple spelling error . Person X interacts with Person Y in that he observes Person Y 's action , reasons about the likely intention for that action and reacts according to this assumed intention . This , for example , explains why many learners get just as frustrated when an answer they believe to be right is rejected by the computer as they would get if it were rejected by their tutor . Of course , an ideal computer - assisted language learning system would avoid such pitfalls and not reject a correct response or overlook an incorrect one . Since any existing system can only approximate to this ideal , researchers and developers in parser - based CALL can only attempt to build systems that can perform complex structured sequences of ( linguistic ) operations so that learners can interact meaningfully and successfully with the computer . Grammatica is able to identify parts of speech in English and French with a fair degree of accuracy and show , for example , how verbs are conjugated in different tenses and how plurals of nouns are formed . Abeill\\u00e9 A. ( 1992 ) \\\" A lexicalised tree adjoining grammar for French and its relevance to language teaching \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Abney S. ( 1997 ) \\\" Part - of - speech tagging and partial parsing \\\" . In Young S. & Bloothooft G. ( eds . ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Allen J. ( 1995 ) Natural language understanding , New York : John Benjamins Publishing Company . Alwang G. ( 1999 ) \\\" Speech recognition \\\" , PC Magazine , 10 November 1999 . Antos G. ( 1982 ) Grundlagen einer Theorie des Formulierens . Textherstellung in geschriebener und gesprochener Sprache , T\\u00fcbingen : Niemeyer . Arnold D. , Balkan . L , Meijer S. , Humphreys R. L. & Sadler L. ( 1994 ) Machine Translation : an introductory guide , Manchester : NEC Blackwell . Bellos D. ( 2011 ) Is that a fish in your ear ? Translation and the meaning of everything , Harlow : Penguin / Particular Books . Bennett P. ( 1997 ) Feature - based approaches to grammar , Manchester : UMIST , Unpublished Manuscript . Bennett P. & Paggio P. ( eds . ) ( 1993 ) Preference in EUROTRA , Luxembourg : European Commission . Bloothooft G. , Dommelen W. , van Espain C. , Green P. , Hazan V. & Wigforss E. ( eds . ) ( 1997 ) The landscape of future education in speech communication sciences : ( 1 ) analysis , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bloothooft G. , van Dommelen W. , Espain C. , Hazan V. , Huckvale M. & Wigforss E. ( eds . ) ( 1998 ) The landscape of future education in speech communication sciences : ( 2 ) proposals , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bolt P. & Yazdani M. ( 1998 ) \\\" The evolution of a grammar - checking program : LINGER to ISCA \\\" , CALL 11 , 1 : 55 - 112 . Borchardt F. ( 1995 ) \\\" Language and computing at Duke University : or , Virtue Triumphant , for the time being \\\" , CALICO Journal 12 , 4 : 57 - 83 . Bowerman C. ( 1993 ) Intelligent computer - aided language learning . LICE : a system to support undergraduates writing in German , Manchester : UMIST , Unpublished doctoral dissertation . Brehony T. & Ryan K. ( 1994 ) \\\" Francophone stylistic grammar checking ( FSGC ) : using link grammars \\\" , CALL 7 , 3 : 257 - 269 . Brocklebank P. ( 1998 ) An experiment in developing a prototype intelligent teaching system from a parser written in Prolog , Manchester , UMIST , Unpublished MPhil dissertation . Brown P.F. , Della Pietra S.A. , Della Pietra V.J. & Mercer R.L. ( 1993 ) \\\" The mathematics of statistical Machine Translation : parameter estimation \\\" , Computational Linguistics 19 , 2 : 263 - 311 . Buchmann B. ( 1987 ) \\\" Early history of Machine Translation \\\" . In King M. ( ed . ) Machine Translation today : the state of the art , Edinburgh : University Press . Bull S. ( 1994 ) \\\" Learning languages : implications for student modelling in ICALL \\\" , ReCALL 6 , 1 : 34 - 39 . Bureau Lingua / DELTA ( 1993 ) Foreign language learning and the use of new technologies , Brussels : European Commission . Cameron K. ( ed . ) ( 1989 ) Computer Assisted Language Learning , Oxford : Intellect . Carson - Berndsen J. ( 1998 ) \\\" Computational autosegmental phonology in pronunciation teaching \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Chanier D. , Pengelly M. , Twidale M. & Self J. ( 1992 ) \\\" Conceptual modelling in error analysis in computer - assisted language learning \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Chen L. & Barry L. ( 1989 ) \\\" XTRA - TE : Using natural language processing software to develop an ITS for language learning \\\" . In Fourth International Conference on Artificial Intelligence and Education : 54 - 70 . Chomsky N. ( 1986 ) Knowledge of language : its nature , origin , and use , New York : Praeger . CLEF ( Computer - assisted Learning Exercises for French ) ( 1985 ) Developed by the CLEF Group , Canada , including authors at the University of Guelph , the University of Calgary and the University of Western Ontario . Also published by Cambridge University Press , 1998 : ISBN 0 - 521 - 59277 - 1 . Curzon L. B. ( 1985 ) Teaching in further education : an outline of principles and practice . London : Holt , Rinehart & Winston , ( 3rd edition ) . Davies G. ( 1988 ) \\\" CALL software development \\\" . In Jung Udo O.H .. ( ed . ) Computers in applied linguistics and language learning : a CALL handbook , Frankfurt : Peter Lang . Davies G. ( 1996 ) Total - text reconstruction programs : a brief history , Maidenhead : Camsoft . Davies S. & Poesio M. ( 1998 ) \\\" The provision of corrective feedback in a spoken dialogue system \\\" . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) ( 1991 ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Diaz de Ilarranza A. , Maritxalar M. & Oronoz M. ( 1998 ) \\\" Reusability of language technology in support of corpus studies in an ICALL environment \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Diaz de Ilarranza A. , Maritxalar A. , Maritxalar M. & Oronoz M. ( 1999 ) \\\" IDAZKIDE : An intelligent computer - assisted language learning environment for Second Language Acquisition \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 12 - 19 . Dodigovic M. ( 2005 ) Artificial intelligence in second language learning : raising error awarenes s , Clevedon : Multilingual Matters . Dokter D. & Nerbonne J. ( 1998 ) \\\" A session with Glosser - RuG \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Dokter D. , Nerbonne J. , Schurcks - Grozeva L. & Smit P. ( 1998 ) \\\" Glosser - RuG : a user study \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Ehsani F. & Knodt E. ( 1998 ) \\\" Speech technology in computer - aided language learning : strengths and limitations of a new CALL paradigm \\\" , Language Learning and Technology 2 , 1 : 45 - 60 . Ellis R. ( 1994 ) The study of Second Language Acquisition , Oxford : OUP . European Commission ( 1996 ) Language and technology : from the Tower of Babel to the Global Village , Luxembourg : European Commission . ISBN 92 - 827 - 6974 - 7 . Fechner J. ( ed . ) ( 1994 ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Feuerman K. , Marshall C. , Newman D. & Rypa M. ( 1987 ) \\\" The CALLE project \\\" , CALICO Journal 4 : 25 - 34 . Foucou P - Y. & K\\u00fcbler N. ( 1999 ) \\\" A Web - based language learning environment : general architecture \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 31 - 39 . Fum D. , Pani B. & Tasso C. ( 1992 ) \\\" Native vs. formal grammars : a case for integration in the design of a foreign language tutor \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hagen L.K. ( 1995 ) \\\" Unification - based parsing applications for intelligent foreign language tutoring systems , CALICO Journal 12 , 2 - 3 : 5 - 31 . Hamilton S. ( 1998 ) \\\" A CALL user study \\\" . In Jager S. , Nerbonne J. & van Essen A.(eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Handke J. ( 1992 ) \\\" WIZDOM : a multiple - purpose language tutoring system based on AI techniques \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hart R. ( 1995 ) \\\" The Illinois PLATO foreign languages project \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 15 - 37 . Heift T. ( 2001 ) \\\" Error - specific and individualised feedback in a Web - based language tutoring system : Do they read it ? \\\" ReCALL 13 , 1 : 99 - 109 . Heift T. & Schulze M. ( eds . ) ( 2003 ) Error diagnosis and error correction in CALL , CALICO Journal Special Issue 20 , 3 . Heift T. & Schulze M. ( 2007 ) Errors and intelligence in CALL : parsers and pedagogues , London and New York : Routledge . Helbig G. ( 1975 ) \\\" Bemerkungen zum Problem von Grammatik und Fremdsprachenunterricht \\\" , Deutsch als Fremdsprache 6 , 12 : 325 - 332 . Higgins J. & Johns T. ( 1984 ) Computers in language learning , London : Collins . Holland M. , Maisano R. , Alderks C. & Martin J. ( 1993 ) \\\" Parsers in tutors : what are they good for ? \\\" CALICO Journal 11 , 1 : 28 - 46 . Holland M. & Fisher F.P. ( eds . ) ( 2007 ) T he path of speech technologies in Computer Assisted Language Learning : from research toward practice , London and New York : Routledge . Hutchins W.J. ( 1986 ) Machine Translation : past , present , future , Chichester : Ellis Horwood . Hutchins W.J. ( 1997 ) \\\" Fifty years of the computer and translation \\\" , Machine Translation Review 6 , October 1997 : 22 - 24 . Hutchins W.J. & Somers H.L. ( 1992 ) An introduction to Machine Translation , London : Academic Press . Jager S. ( 2001 ) \\\" From gap - filling to filling the gap : a re - assessment of Natural Language Processing in CALL \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Jager S. , Nerbonne J. & van Essen A. ( eds . ) ( 1998 ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Jones R. ( 1995 ) \\\" TICCIT and CLIPS : The early years \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 84 - 96 . Jung Udo O.H. & Vanderplank R. ( eds . ) ( 1994 ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Juozulynas V. ( 1994 ) \\\" Errors in the composition of second - year German students : an empirical study of parser - based ICALI \\\" , CALICO Journal 12 , 1 : 5 - 17 . King M. ( ed . ) ( 1987 ) Machine Translation today : the state of the art , Edinburgh : Edinburgh University Press . Klein W. & Dittmar N. ( 1979 ) Developing grammars : the acquisition of German syntax by foreign workers , Heidelberg : Springer . Klein W. & Perdue C. ( 1992 ) Utterance structure ( developing grammars again ) , Amsterdam : John Benjamins Publishing Company . Klein W. ( 1986 ) Second language acquisition , Cambridge : Cambridge University Press . Kohn K. ( 1994 ) \\\" Distributive language learning in a computer - based multilingual communication environment \\\" . In Jung Udo O.H. & Vanderplank R. ( eds . ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Krashen S. ( 1981 ) Second language acquisition and second language learning , Oxford : Pergamon . Krashen S. ( 1982 ) Principles and practice in second language acquisition , Oxford : Pergamon . Kr\\u00fcger - Thielmann K. ( 1992 ) Wissensbasierte Sprachlernsysteme . Neue M\\u00f6glichkeiten f\\u00fcr den computergest\\u00fctzten Sprachunterricht , T\\u00fcbingen : Gunter Narr . Labrie G. & Singh L. ( 1991 ) \\\" Parsing , error diagnosis and instruction in a French tutor \\\" , CALICO Journal 9 : 9 - 25 . Last R. ( 1989 ) Artificial Intelligence techniques in language learning , Chichester : Ellis Horwood . Last R. ( 1992 ) \\\" Computers and language learning : past , present - and future ? \\\" In Butler C. ( ed . ) Computers and written texts , Oxford : Blackwell . Levin L. , Evans D. & Gates D. ( 1991 ) \\\" The Alice system : a workbench for learning and using language \\\" , CALICO Journal 9 : 27 - 55 . Levy M. ( 1997 ) Computer - assisted language learning : context and conceptualisation , Oxford : Oxford University Press . Lightbown P.M. & Spada N. ( 1993 ) How languages are learned , Oxford : Oxford University Press . Long M. ( 1991 ) \\\" Focus on form : a design feature in language teaching methodology \\\" . In de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Manning C. & Sch\\u00fctze H. ( 1999 ) Foundations of statistical Natural Language Processing , Cambridge MA , MIT Press . Matthews C. ( 1992a ) \\\" Going AI : foundations of ICALL \\\" , CALL 5 , 1 - 2 : 13 - 31 . Matthews C. ( 1992b ) Intelligent CALL ( ICALL ) bibliography , Hull : University of Hull , CTI Centre for Modern Languages . Matthews C. ( 1993 ) \\\" Grammar frameworks in Intelligent CALL \\\" , CALICO Journal 11 , 1 : 5 - 27 . Matthews C. ( 1994 ) \\\" Intelligent Computer Assisted Language Learning as cognitive science : the choice of syntactic frameworks for language tutoring \\\" , Journal of Artificial Intelligence in Education 5 , 4 : 533 - 56 . Menzel W. & Schr\\u00f6der I. ( 1999 ) \\\" Error diagnosis for language learning systems \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 20 - 30 . Michel G. ( ed . ) ( 1985 ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . Mitkov R. ( 1998 ) Language Learner 's Workbench . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . Mitkov R. & Nicolov N. ( eds . ) ( 1997 ) Recent advances in Natural Language Processing . Amsterdam : John Benjamins Publishing Company . Murphy M. , Kr\\u00fcger A. & Griesz A. , ( 1998 ) \\\" RECALL \\\" -towards a knowledge - based approach to CALL . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nagata N. ( 1996 ) \\\" Computer vs. workbook instruction in second language acquisition \\\" , CALICO Journal 14 , 1 : 53 - 75 . Nerbonne J. , Jager S. & van Essen A. ( 1998 ) Introduction . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nirenburg S. ( ed . ) ( 1986 ) Machine Translation : theoretical and methodological issues , Cambridge : Cambridge University Press . Pijls F. , Daelmans W. & Kempen G. ( 1987 ) \\\" Artificial intelligence tools for grammar and spelling instruction , Instructional Science 16 : 319 - 336 . Pollard C. & Sag I.A. ( 1987 ) Information - Based Syntax and Semantics , Chicago : University Press . Pollard C. & Sag I.A. ( 1994 ) Head - Driven Phrase Structure Grammar , Chicago : University Press . Ramsay A. & Sch\\u00e4ler R. ( 1997 ) \\\" Case and word order in English and German \\\" . In Mitkov R. & Nicolov N. ( eds . ) Recent advances in Natural Language Processing , Amsterdam : John Benjamins Publshing Company : 15 - 34 . Ramsay A. & Schulze M. ( 1999 ) \\\" Die Struktur deutscher Lexeme \\\" , German Linguistic and Cultural Studies , Peter Lang , Submitted Manuscript . Reifler E. ( 1958 ) \\\" The Machine Translation project at the University of Washington , Seattle , Washington , USA \\\" . In Proceedings of the Eighth International Congress of Linguists , Oslo University Press : 514 - 518 . Roosmaa T. & Pr\\u00f3sz\\u00e9ky G. ( 1998 ) \\\" GLOSSER - using language technology tools for reading texts in a foreign language \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Salaberry R. ( 1996 ) \\\" A theoretical foundation for the development of pedagogical tasks in computer mediated communication \\\" , CALICO Journal 14 , 1 : 5 - 34 . Sanders R. ( 1991 ) \\\" Error analysis in purely syntactic parsing of free input : the example of German \\\" , CALICO Journal 9 , 1 : 72 - 89 . Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 . Schmitz U. ( 1992 ) Computerlinguistik , Opladen : Westdeutscher Verlag . Schulze M. ( 1997 ) \\\" Textana - text production in a hypertext environment \\\" , CALL 10 , 1 : 71 - 82 . Schulze M. ( 1998 ) \\\" Teaching grammar - learning grammar . Aspects of Second Language Acquisition in CALL \\\" , CALL 11 , 2 : 215 - 228 . Schulze M. ( 2001 ) \\\" Human Language Technologies in Computer Assisted Language Learning \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Schulze M. , Hamel M - J. & Thompson J. ( eds . ) ( 1999 ) Language processing in CALL , ReCALL Special Issue . Schumann J.H. & Stenson N. ( eds . ) ( 1975 ) New frontiers in second language learning , Rowley : Newbury House . Schwind C. ( 1990 ) \\\" An intelligent language tutoring system \\\" , International Journal of Man - Machine Studies 33 : 557 - 579 . Selinker L. ( 1992 ) Rediscovering interlanguage , London : Longman . Skrelin P. & Volskaja N. ( 1998 ) \\\" The application of new technologies in the development of education programs \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Smith R. ( 1990 ) Dictionary of Artificial Intelligence , London : Collins . Sp\\u00e4th P. ( 1994 ) \\\" Hypertext und Expertensysteme i m Sprachunterricht \\\" . In Fechner J. ( ed . ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Stenzel B. ( ed . ) ( 1985 ) Computergest\\u00fctzter Fremdsprachenunterricht . Ein Handbuch , Berlin : Langenscheidt . Swartz M. & Yazdani M. ( eds . ) ( 1992 ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Taylor H. ( 1987 ) TUCO II . Published by Gessler Educational Software , New York . Based on earlier programs developed by Taylor H. & Haas W. at Ohio State University in the 1970s : DECU ( Deutscher Computerunterricht ) and TUCO ( Tutorial Computer ) . Taylor H. ( 1998 ) Computer assisted text production : feedback on grammatical errors made by learners of English as a Foreign Language , Manchester : UMIST , MSc Dissertation . Tschichold C. ( 1999 ) \\\" Intelligent grammar checking for CALL \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 5 - 11 . Tschichold C. , Bodme F. , Cornu E. , Grosjean F. , Grosjean L. , K\\u00fcbler N. & Tschumi C. ( 1994 ) \\\" Detecting and correcting errors in second language texts \\\" , CALL 7 , 2 : 151 - 160 . Visser H. ( 1999 ) \\\" CALLex ( Computer - Aided Learning of Lexical functions ) - a CALL game to study lexical relationships based on a semantic database \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 50 - 56 . Ward R. , Foot R. & Rostron A.B. ( 1999 ) \\\" Language processing in computer - assisted language learning : language with a purpose \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 40 - 49 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . Reproduced in Locke W.N. & Booth A.D. ( eds . ) ( 1955 ) Machine translation of languages : fourteen essays , Cambridge , Mass : Technology Press of the Massachusetts Institute of Technology : 15 - 23 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . See \\\" 50th anniversary of machine translation \\\" , MT News International , Issue 22 ( Vol . 8 , 1 ) , July 1999 : 5 - 6 . Weischedel R. , Voge W. & James M. ( 1978 ) \\\" An artificial intelligence approach to language instruction \\\" , Artificial Intelligence 10 : 225 - 240 . Wilks Y. & Farwell D. ( 1992 ) \\\" Building an intelligent second language tutoring system from whatever bits you happen to have lying around \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag , . Whitelock P.J. & Kilby K. ( 1995 ) Linguistic and computational techniques in Machine Translation system design , London : University College Press . Witt S. & Young S. ( 1998 ) \\\" Computer - assisted pronunciation teaching based on automatic speech recognition \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Wolff D. ( 1993 ) \\\" New technologies for foreign language teaching \\\" . In Foreign language learning and the use of new technologies , Bureau Lingua / DELTA , Brussels , European Commission . Young S. & Bloothooft G. ( eds . ) ( 1997 ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Z\\u00e4hner C. ( 1991 ) \\\" Word grammars in ICALL \\\" . In Savolainen H. & Telenius J. ( eds . ) EuroCALL 91 proceedings , Helsinki : Helsinki School of Economics . Zech J. ( 1985 ) \\\" Methodische Probleme einer t\\u00e4tigkeitsorientierten Ausbildung des sprachlich - kommunikativen K\\u00f6nnens \\\" . In Michel G. ( ed . ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . CALICO ( Computer Assisted Language Instruction Consortium ) : CALICO is a professional association devoted to promoting the use of technology enhanced language learning . CALICO 's sister association in Europe is EUROCALL . EUROCALL : EUROCALL is a professional association devoted to promoting the use of technology enhanced language learning , based at the University of Ulster , Northern Ireland . EUROCALL 's sister association in the USA is CALICO . ICALL is an interdisciplinary research field integrating insights from computational linguistics and artificial intelligence into computer - aided language learning . Such integration is needed for CALL systems to be able to analyze language automatically , to make them aware of language as such . This makes it possible to provide individualized feedback to learners working on exercises , to ( semi-)automatically prepare or enhance texts for learners , and to automatically create and use detailed learner models . See NLP SIG , the Special Interest Group within EUROCALL , with which ICALL closely collaborates . InSTIL : The name of a now defunct Special Interest Group dedicated to Integrating Speech Technology in Language Learning , which was set up within the EUROCALL and CALICO professional associations . A good deal of the work undertaken by InSTIL has now been taken over by ICALL and NLP SIG . NLP SIG : The name of the Special Interest Group for Natural Language Processing within the EUROCALL professional association . See ICALL , the Special Interest Group within CALICO , with which NLP SIG closely collaborates . Virtual Linguistics Campus : It includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Document last updated 29 April 2012 . This page is maintained by Graham Davies . Please cite this Web page as : Gupta P. & Schulze M. ( 2012 ) Human Language Technologies ( HLT ) . Module 3.5 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \"}",
        "_version_":1692580922897989632,
        "score":19.399174},
      {
        "id":"6204f773-cb8f-428b-bf1c-c38feb4c97e3",
        "_src_":"{\"url\": \"https://docs.google.com/document/pub?id=1obxpxgZGvFu4jtTW9ciqB9twltLQCVdt_VvB1XYzFC4\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161942.67/warc/CC-MAIN-20160205193921-00277-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Workshop on Crowdsourcing for Search Evaluation . Organisers . Webpage . Abstract . The SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation ( CSE2010 ) solicits submissions on topics including but are not limited to the following areas : . Novel applications of crowdsourcing for evaluating search systems ( see examples below ) . Novel theoretical , experimental , and/or methodological developments advancing state - of - the - art knowledge of crowdsourcing for search evaluation . Tutorials on how the different forms of crowdsourcing might be best suited to or best executed in evaluating different search tasks . New software packages which simplify or otherwise improve general support for crowdsourcing , or particular support for crowdsourced search evaluation . Reflective or forward - looking vision on use of crowdsourcing in search evaluation as informed by prior and/or ongoing studies . How crowdsourcing technology or process can be adapted to encourage and facilitate more participation from outside the USA . The workshop especially calls for innovative solutions in the area of search evaluation involving significant use of a crowdsourcing platform such as Amazon 's Mechanical Turk , Crowdflower , LiveWork , etc . Novel applications of crowdsourcing are of particular interest . This includes but is not restricted to the following tasks : . cross - vertical search ( video , image , blog , etc . ) evaluation , . local search evaluation . mobile search evaluation . realtime / news search evaluation . entity search evaluation . discovering representative groups of rare queries , documents , and events in the long - tail of search . detecting / evaluating query alterations . For example , does the inherent geographic dispersal of crowdsourcing enable better assessment of a query 's local intent , its local - specific facets , or diversity of returned results ? Could crowd - sourcing be employed in near real - time to better assess query intent for breaking news and relevant information ? Most Innovative Awards Sponsored by Microsoft Bing . As further incentive to participation , authors of the most novel and innovative crowdsourcing - based search evaluation techniques ( e.g. using Amazon 's Mechanical Turk , Livework , Crowdflower , etc . ) will be recognized with \\\" Most Innovative Awards \\\" as judged by the workshop organizers . Selection will be based on the creativity , originality , and potential impact of the described proposal , and we expect the winners to describe risky , ground - breaking , and unexpected ideas . The provision of awards is thanks to generous support from Microsoft Bing , and the number and nature of the awards will depend on the quality of the submissions and overall availability of funds . All valid submissions to the workshop will be considered for the awards . Submission Instructions . Submissions should report new ( unpublished ) research results or ongoing research . Long paper submissions ( up to 8 pages ) will be primarily target oral presentations . Short papers submissions can be up to 4 pages long , and will primarily target poster presentations . Papers must be submitted as PDF files . Submissions should not be anonymized . Web page . Abstract . Current search systems are not adequate for individuals with specific needs : children , older adults , people with visual or motor impairments , and people with intellectual disabilities or low literacy . Search services are typically created for average users ( young or middle - aged adults without physical or mental disabilities ) and information retrieval methods are based on their perception of relevance as well . The workshop will be the first to raise the discussion on how to make search engines accessible for different types of users , including those with problems in reading , writing or comprehension of complex content . Search accessibility means that people whose abilities are considerably different from those that average users have will be able to successfully use search systems . The objective of the workshop is to provide a forum and initiate collaborations between academics and industrial practitioners interested in making search more usable for users in general and for users with specific needs in particular . We encourage presentation and participation from researchers working at the intersection of information retrieval , natural language processing , human - computer interaction , ambient intelligence and related areas . The workshop will be a mix of oral presentations for long papers ( maximum of 8 pages ) , a session for posters ( maximum of 2 pages ) and a panel discussion . All submissions will be reviewed by at least two PC members . Workshop proceedings will be available at the workshop . Webpage . Abstract . Desktop search refers to the process of searching within one 's personal space of information . Despite recent research interest , desktop search is under - explored compared to other search domains such as the web , semi - structured data , or flat text . Problems with existing desktop search tools include performance issues , an over - reliance on good query formulation , and a failure to fit within the user 's work flow or the user 's mental model . Evaluation of desktop search tools is difficult . There are no established or standardized baselines or evaluation metrics , and no commonly available test collections . Privacy concerns , the challenges of working with personal collections , and the individual differences in behaviour between users all must be addressed to advance research in this domain . This workshop will bring together academics and industrial practitioners interested in desktop search with the goal of fostering collaborations and addressing the challenges faced in this area . The workshop will be structured to encourage group discussion and active collaboration among attendees . We encourage participation from people in the fields of information retrieval , personal information management , natural language processing , human - computer interaction , and related areas . Workshop on Simulation of Interaction : Automated Evaluation of Interactive IR . Organisers . Webpage . Abstract . This workshop aims to explore the use of Simulation of Interactions to enable automated evaluation of Interactive Information Retrieval Systems and Applications . Standard test collections only enable a very limited type of interaction to be evaluated ( i.e. query - response ) . This is largely due to the high costs involved in going beyond this limited interaction and problems associated with replicability and repeatability of experiments . Arguably , Simulation of Interaction provides a cost - effective way to construct and repeat evaluations of interactive systems and applications . Sign up to this workshop shop if you are interested in Interactive IR retrieval and the modeling of users , systems , interactions and behaviors and how they can be simulated ( or not ) within automated evaluation methodologies for IR . The workshop is going to be lively and very interactive ( both online and offline ) compromising of discussions and debates all aimed at producing valuable community resources and references on simulation in IR . Workshop on Query Representation and Understanding . Organisers . Webpage . Abstract . Understanding the user 's intent or information need that underlies a query has long been recognized as a crucial part of effective information retrieval . Despite this , retrieval models , in general , have not focused on explicitly representing intent , and query processing has been limited to simple transformations such as stemming or spelling correction . With the recent availability of large amounts of data about user behavior and queries in web search logs , there has been an upsurge in interest in new approaches to query understanding and representing intent . This workshop has the goal of bringing together the different strands of research on query understanding , increasing the dialogue between researchers working in this relatively new area , and developing some common themes and directions , including definitions of tasks and evaluation methodology . Webpage . Abstract . This workshop aims to bring together both experienced and young researchers from distributed IR , including work on P2P search and efficiency of distributed systems for information processing . This edition of the workshop will favor novel , perhaps even outrageous ideas as opposed to finished research work , thus strongly encouraging the submission of position papers in addition to research papers . Position papers are important to foster discussion upon controversial and intriguing ideas on new ways of building distributed infrastructures for information processing . Workshop on Next - Generation Test Collections . Organisers . Webpage . Abstract . The standard evaluation methodology - the Cranfield paradigm of calculating evaluation measures using test collections - has struggled to keep up , as research shows that even test collections for terabyte - sized corpora suffer from unforeseen judgment bias and reusability challenges . This workshop invites cutting - edge research on tackling the problem of building test collections at the multi - terabyte scale that are realistic , fair , and reusable . The goal of the workshop is to map out the critical research questions that need to be asked and the types of collections we need to consider building in order to answer them . Webpage . Abstract . Modern information retrieval systems facilitate information access at unprecedented scale and level of sophistication . However , in many cases the underlying representation of text remains quite simple , often limited to using a weighted bag of words . Over the years , several approaches to automatic feature generation have been proposed ( such as Latent Semantic Indexing , Explicit Semantic Analysis , Hashing , and Latent Dirichlet Allocation ) , yet their application in large scale systems still remains the exception rather than the rule . On the other hand , numerous studies in NLP and IR resort to manually crafting features , which is a laborious and expensive process . Such studies often focus on one specific problem , and consequently many features they define are task- or domain - dependent . Consequently , little knowledge transfer is possible to other problem domains . This limits our understanding of how to reliably construct informative features for new tasks . An area of machine learning concerned with feature generation ( or constructive induction ) studies methods that endow computers with the ability to modify or enhance the representation language . Feature generation techniques search for new features that describe the target concepts better than the attributes supplied with the training instances . Complementary to feature generation , the issue of feature selection arises . It aims to retain only the most informative features , e.g. , in order to reduce noise and to avoid overfitting , and is essential when numerous features are automatically constructed . We believe that much can be done in the quest for automatic feature generation for text processing , for example , using large - scale knowledge bases as well as the sheer amounts of textual data easily accessible today . The purpose of this workshop is to bring together researchers from many related areas ( including information retrieval , machine learning , statistics , and natural language processing ) to address these issues and seek cross - pollination among the different fields . Webpage . Abstract . The aim of the workshop is to bring together a group of leaders in information retrieval and language modeling to discuss the challenges in information retrieval and how language modeling approaches may help address some of these challenges . Often discussed in the research community is the lack of large scale dataset and benchmarks to run experiments . In this workshop , we encourage researchers to use the Microsoft Web N - gram service to explore novel applications of language models ( e.g. long tail effects ) and use of these data for enhancing the search experience ( e.g. use of anchor text as a proxy to queries ) . We will also consider other applications such as machine translation , speech . We also encourage research and experiments using or comparing different n - grams data sets to ultimately help create at the workshop a useful n - gram baseline for the research community , in terms of n - gram attributes such as size , access , content and model types needed for researchers . Workshop Planned Activities : . Experiment results presented via talks , poster and demo session . Panel on providing access to data : academia needs , challenges and opportunities for industries to provide such data \"}",
        "_version_":1692670368438812672,
        "score":19.227625},
      {
        "id":"c3416997-ba4c-4c4a-8218-befee262833f",
        "_src_":"{\"url\": \"http://free.naplesplus.us/categories/view.php/1553/orlando\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146196.88/warc/CC-MAIN-20160205193906-00285-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Corresponding Author : . ABSTRACT . Background : Curriculum mapping , which is aimed at the systematic realignment of the planned , taught , and learned curriculum , is considered a challenging and ongoing effort in medical education . Second - generation curriculum managing systems foster knowledge management processes including curriculum mapping in order to give comprehensive support to learners , teachers , and administrators . The large quantity of custom - built software in this field indicates a shortcoming of available IT tools and standards . Objective : The project reported here aims at the systematic adoption of techniques and standards of the Social Semantic Web to implement collaborative curriculum mapping for a complete medical model curriculum . Methods : A semantic MediaWiki ( SMW)-based Web application has been introduced as a platform for the elicitation and revision process of the Aachen Catalogue of Learning Objectives ( ACLO ) . The semantic wiki uses a domain model of the curricular context and offers structured ( form - based ) data entry , multiple views , structured querying , semantic indexing , and commenting for learning objectives ( \\\" LOs \\\" ) . Semantic indexing of learning objectives relies on both a controlled vocabulary of international medical classifications ( ICD , MeSH ) and a folksonomy maintained by the users . An additional module supporting the global checking of consistency complements the semantic wiki . Statements of the Object Constraint Language define the consistency criteria . We evaluated the application by a scenario - based formative usability study , where the participants solved tasks in the ( fictional ) context of 7 typical situations and answered a questionnaire containing Likert - scaled items and free - text questions . Results : At present , ACLO contains roughly 5350 operational ( ie , specific and measurable ) objectives acquired during the last 25 months . The wiki - based user interface uses 13 online forms for data entry and 4 online forms for flexible searches of LOs , and all the forms are accessible by standard Web browsers . The formative usability study yielded positive results ( median rating of 2 ( \\\" good \\\" ) in all 7 general usability items ) and produced valuable qualitative feedback , especially concerning navigation and comprehensibility . Conclusions : The SMW - based approach enabled an agile implementation of computer - supported knowledge management . The approach , based on standard Social Semantic Web formats and technology , represents a feasible and effectively applicable compromise between answering to the individual requirements of curriculum management at a particular medical school and using proprietary systems . J Med Internet Res 2013;15(8):e169 . doi:10.2196/jmir.2623 . Citation . KEYWORDS . Introduction . Background . Curriculum mapping supports teachers , learners , and curriculum administrators by providing a comprehensive overview of a curriculum and its elements and their interrelations . It answers questions like \\\" Where do we teach what ? \\\" [ 1 ] . Curriculum mapping is considered to be a demanding , data - intensive , and essentially collaborative effort [ 2 ] . The number of custom - built software platforms for curriculum mapping indicates a lack of available tools and standards [ 3 ] . Social Semantic Web ( SSW ) approaches combine Web content , which can be partly \\\" understood \\\" ( ie , processed in a semantically sound way ) by computer programs with social software , enabling the collaborative creation and maintenance of content to take place . Thus , SSW approaches provide promising solutions for implementing and maintaining curriculum mapping in medicine as a knowledge management process . The revision of the national German medical licensing regulations enacted in 2002 enabled markedly changed medical curricula to be introduced at German medical schools , and this included the creation of a number of reform model curricula [ 4 ] . This change process increased the need for computer - supported curriculum mapping . Curriculum Mapping . In the 1970s , Hausman coined the term curriculum mapping in the context of curriculum planning [ 5 ] . Gjerde stressed the potentially positive effect of curriculum mapping on the congruence of learning objectives and tests used for evaluation [ 7 ] . In his comprehensive review of curriculum mapping , Harden recommended the approach as a pivotal factor in fostering coordination and communication of medical curricula [ 8 ] . In 2008 , a survey on the status of curriculum mapping in Canada and the United Kingdom found that 55 % of the responding medical schools were in the process of establishing a curriculum map [ 3 ] . Notably , Willett also found that curriculum mapping was considered to be an ongoing process requiring \\\" continual upgrading and maintenance \\\" . Following their analysis , the large quantity of custom - built software indicates a shortcoming of available IT tools that meet the requirements of curriculum management . Computer - Supported Curriculum Management . The Faculty of Medicine at McGill University implemented an electronic curriculum map at an early stage ; this allows a curriculum inventory to be performed [ 9 ] . Tufts Health Science Database ( HSDB ) was an early attempt to integrate content delivery and curriculum management . Lee et al describe the positive effects of HSDB on faculty development , curricular reform , and interdisciplinarity [ 10 ] . CurrMIT serves as a means to capture , manage , and compare the curricula of North American medical schools [ 11 ] . Curriculum mapping tools have also been developed in other health - related areas , for example , in the context of nursing education [ 12 ] . In contrast to the awareness of the role of electronic curriculum maps for innovative curriculum management and the early attempts to integrate learning content , many approaches - especially in German - speaking countries - focused on creating online learning objectives catalogs or databases . The Swiss Catalogue of Learning Objectives ( SCLO ) [ 13 ] was one of the most influential European projects for defining and managing structured , outcome - oriented learning objectives . SCLO provides an open - access Web portal that allows the objectives to be filtered by keyword , type , discipline , topic , and competence levels . In the meantime , LO catalogs similar to the SCLO have been implemented at various German medical schools [ 14 - 16 ] . The projects grant Web - based access to their users . All Web platforms are custom - built , which corresponds to the general trend found by Willett at UK and Canadian medical schools [ 3 ] . The online tool used for the Heidelberg Catalogue of Learning Objectives ( HCLO ) focuses on enabling interactive maintenance and improvement of the catalog . The Charit\\u00e9 University Hospital , Berlin , uses its platform to support systematic curriculum mapping , and claims to be one of the first German faculties to do so . Hege et al similarly advocated the use of computer - based LO catalogs in the process of managing and mapping a curriculum [ 15 , 16 ] . In addition to catalogs of learning objectives , which address the complete curricula of medical schools , some approaches concentrate on a given medical specialty or field [ 17 ] . Second - Generation Curriculum Management Systems . eMed , developed at the University of New South Wales , is reported to be an integrated second - generation curriculum management system , combining various tools and services . eMed supports very different aspects of curriculum management , ranging from curriculum mapping to student portfolios , and bridging the gap between organizational , curricular , and economic needs [ 18 ] . Bell et al reported on a curriculum - mapping project that concentrates especially on curricular quality management by systematic curriculum review ; this fosters integration , transparency , and communication [ 19 ] . The authors found the curriculum mapping process to be resource - intensive , and underlined the fact that none of the existing software tools designed for supporting curriculum mapping fully met the requirements of their project . Just recently , the University of Toronto developed CMap , a special computer - based curriculum - mapping tool that was able to detect an uneven distribution of teaching time with respect to the major topics and skills of a given planned curriculum [ 20 ] . Semantic Indexing . An important prerequisite for enabling curriculum mapping and comprehensive curriculum management , based on databases organizing curricular data and learning objective catalogs , is semantic indexing . Learning objectives need to be retrievable by medical topic , similar learning content should be associated , and the coverage of learning objectives by learning events or examinations needs to become visible . Denny et al investigated the ability of a text mining and classification tool to give an automatic estimate of the coverage of medical topics by lectures , based on texts documenting these lectures [ 21 ] . The approach proved successful in supporting semantic indexing of curricular events and outperformed , in that curricular context , a similar tool ( MetaMap ) that had primarily been designed for indexing scientific publications [ 21 , 22 ] . The taxonomy TIME ( Topics for Indexing Medical Education ) served as a means to index the elements of a curriculum map uniformly [ 23 ] . Thus , TIME enabled topic specific views , which show the contribution of curricular elements to specific outcomes . Dexter et al reported on using the US Medical Licensing Examination Step 1 Content Outline ( USMLE Step1 CO ) as a basis for indexing LOs in order to assess the completeness of topic coverage [ 24 ] . Instead of using preexisting taxonomies , semantic indexing can also be achieved by collaboratively created taxonomies , which evolve during the use of a given information system , so - called folksonomies . Gasevic et al reported an exemplary application of folksonomies to the maintenance of learning environments [ 25 ] . Aachen Catalogue of Learning Objectives . In 2003 , the Medical Faculty of the RWTH Aachen University started the Aachen Medical Model Curriculum ( AMMC ) . The curriculum implements a spiral - shaped education process ( \\\" spiral curriculum \\\" [ 26 ] ) and aims at an early integration of preclinical and clinical education . In the second and third years , 11 multidisciplinary modules , each focusing on a particular organ system , form the backbone of the curriculum . The AMMC follows an explicit mission statement and defines learning goals for all modules . Furthermore , a faculty - spanning consensus process , which took place before the start of the model curriculum , defined the content of all modules . Nonetheless , the documentation of learning objectives merely relied on item lists , which proved insufficient for maintaining and further developing the curriculum . Thus , in 2011 , the faculty formally decided to implement a comprehensive , Web - based catalog of competency - based learning objectives ( Aachen Catalogue of Learning Objectives or ACLO ) . The spiral curriculum and multidisciplinary modules create challenging requirements concerning the elicitation , revision , and communication process of the LOs . Aims . The Web - based Aachen Catalogue of Learning Objectives ( ACLO - Web ) aims to support the implementation , maintenance , and use of the ACLO by establishing a Web - based knowledge management system based on Social Semantic Web technology and standards . Methods . Elicitation and Revision Process of the ACLO . The faculty formed a special task force ( \\\" Learning Objectives Working Group \\\" ) in order to organize the implementation of the ACLO . The dean asked all contributing units ( clinics , institutes , and external departments ) to name representatives responsible for the detailed specification of LOs . Each representative needed to participate in a training program : mainly a 1-day workshop by trainers with certified competency in medical education . After the training , the representatives started the elicitation and specification of LOs based on predefined forms , while consulting everyone in their unit ( clinic or institute ) who was involved in teaching . This team continuously checked and improved the assignment of LOs to medical topics , curricular modules , and a responsible faculty . Furthermore , they supervised quality standards concerning the formulation of competency - based learning objectives . During the ACLO revision , the responsible representative , who could of course involve his or her colleagues , checked and eventually improved the LO specification . When the Learning Objectives Working Group finally approved the LOs , the catalog was made accessible to students . Faculty members and students now continuously check the LO catalog for inconsistencies and monitor whether the LOs are adequately addressed by the courses and lectures . Their feedback should lead to the future improvement of the catalog . Requirements . An information system supporting the process described below has to meet the following requirements : . Collaborative : The system needs to support a collaborative effort . It needs to provide decentralized access to a central LO repository , allow the storage and tracking of the whole history of changes made to the LOs in the collection ( ie , support a versioning mechanism ) , and manage user accounts and roles . Flexible : The system needs to be flexible with respect to changes in the representation , retrieval , and presentation of LOs . Ideally , the system should support the implementation process of the LO catalog from the very beginning . As a consequence of the ongoing implementation process , it should be possible to adapt the system to changing requirements . Social Semantic Web - Based Curriculum Management . The online catalog of learning objectives is based on Social Semantic Web technology , in order to support collaborative knowledge management flexibly . The project reported here uses MediaWiki ( the software platform of Wikipedia ) enhanced by additional software modules ( extensions ) . We adopted the Semantic MediaWiki ( SMW ) and Semantic Forms extensions . A MediaWiki application enables the collaborative maintenance of Web pages ; the pages can easily be edited and linked by wiki users using only a Web browser . Wiki users can revisit or even undo all changes to a page logged by the system in the page history . Similar content can be presented uniformly by using wiki templates combining predefined text fragments with new information entered via variables . Comments can be added to existing wiki pages . MediaWiki provides different approaches for managing user accounts and granting access . At present , read access to ACLO requires a general password communicated to all students and teachers . Readers can also enter comments and annotations ( reader role ) . ACLO grants write access ( ie , the right to change the core content ) , to registered users only ( author role ) . The creation of user accounts is restricted to the administrators of ACLO . Based on such categories , MediaWiki automatically generates index pages . The SMW extension [ 27 ] adds semantic relations ( SMW properties ) to the MediaWiki , allowing pages to be annotated by structured property - value pairs and associated by meaningful relations . SMW properties form subject - predicate - object triples ( referring page , property , referenced page , or value ) as introduced by the Semantic Web approach [ 28 ] . The enhanced representational means of SMW directly correspond to a Web Ontology Language ( OWL ) based knowledge representation : each SMW page represents an ( abstract ) individual , SMW properties correspond to OWL properties , and wiki categories to OWL classes , respectively . In general , SMW annotations can be translated - and actually exported - to OWL DL statements ( in OWL / RDF encoding ) , where OWL DL is an OWL sublanguage formally based on complete and decidable description logics [ 29 , 30 ] . Only some built - in SMW properties lack direct equivalents in OWL and are thus treated as annotation properties [ 27 ] . According to the above correspondence , most SMW annotations represent assertional knowledge , that is , they state facts concerning attributes of domain objects and their mutual relationships ( A - Box statements of an ontology - based knowledge representation ) . Some SMW applications exploited this further by importing exiting ontologies . SMW also defines an elaborated query syntax operating on its semantic annotations . SMW pages may include inline queries enabling the dynamic and consistent updating of facts throughout the wiki application . Furthermore , it is possible to not only query , but also to manage an SMW application based on a Resource Description Framework database ( RDF - Triple Store ) in combination with the SPARQL Protocol and RDF Query Language ( SPARQL ) [ 27 , 31 ] . SMW properties may also refer to primitive data types , such as strings , numbers , or dates , allowing pages to organize structured data instead of containing unstructured text . The Semantic Forms extension then supports structured data entry by defining forms . Thus , forms can foster structured and uniform data entry for all pages of a given category . When authors enter content via semantic forms , the created wiki pages are categorized automatically . Semantic forms offer drop - down lists , check boxes , and a sophisticated auto - completion mechanism , and allow data entry to be restricted to semantically sound values . As shown in Figure 1 , the approach relies mainly on semantic forms when imposing schema constraints . The semantic forms are the means available for imposing the respective constraints . Figure 1 shows the SMW mechanisms imposing semantic constraints and enabling structured data management ; each SMW category can use a semantic template for uniformly composing a page out of structured parameters and each template can make use of semantic forms allowing controlled data entry by online forms . Most notably , a template can include query syntax that dynamically and individually refers to the content of each page composed by that template . Figure 1 . The SMW - based approach to the representation and acquisition of curricular information , illustrated by an example taken from ACLO - Web ; the diagram includes abbreviated SMW syntax in order to illustrate why the references between the different building blocks are established . Core Data Model . Figure 2 shows the core data model of the LO catalog . The diagram focuses on the most relevant aspects : each LO needs to be assigned to teaching modules of the curriculum ( ie , courses , lectures , practicals , etc ) . Additionally , an LO needs to be characterized by the medical topic addressed by the LO and the medical specialty involved . The left - hand area of the diagram represents the curricular context ( given by \\\" Term \\\" , \\\" Segment \\\" , \\\" Curriculum \\\" ) . The regulations of the course of study and the examinations define a planned curriculum . As official regulations are revised from time to time , teaching modules may be redefined or moved to different semesters . Thus , the catalog can reuse LOs in different modules following different regulations by assigning them to modules belonging to different versions of the curriculum . Assigning a medical specialty to a teaching module implies that the respective clinic or institute is responsible for teaching ( is \\\" involved_in \\\" ) the multidisciplinary module . The direct relation \\\" defines \\\" indicates ( institutional ) authorship of medical objectives , which are learning objectives specifically addressing medical topics . The vast majority of LOs of ACLO belong to this type and are authored by teachers affiliated with a clinic or institute . The remaining LOs ( some more general learning goals ) were defined by conferences . These LOs are not associated with specific institutions . The model represents medical topics ( eg , diseases , therapeutic approaches , or professional skills ) as independent entities . Therefore , LOs can be assigned to a set of medical topics , which nonetheless may be revised or extended during the implementation process of the catalog . At present , all authors of learning objectives can enter new topics , while an auto - completion function reduces the risk of entering syntactic variants and fosters the user 's awareness of existing topics . Users can also form hierarchic or associative links between topics . Thus , the set of available topics and its structure evolves due to collaborative effort and , therefore , forms a folksonomy . The assignment of topic to LOs is carried out manually . It complements the automatic categorization of wiki content by the semantic forms . The use of a controlled set of topics promotes consistent semantic indexing and retrieval of LOs . The model also indicates references to external information . By referencing a MeSH - ID , an ICD - Code , an OPS - Code , or an IMPP - ID , the topics are linked to established medical classifications . The system therefore allows it to be determined which requirements of the final examinations are covered by the learning objectives of a given module , term , and the whole curriculum , respectively . Additionally , the indexing of learning objectives by topics allows similar learning objectives of a given topic to be retrieved , which is achieved at present by using SMW subqueries within the system . Both systems will be further integrated in the near future . Specification of Learning Objectives . The assignment of LOs to teaching modules , topics , and responsible specialties ( treated as independent model entities ) is complemented by a detailed specification of each LO . Roughly , this specification includes ( 1 ) the essential description of the LO , ( 2 ) further indexing , and ( 3 ) supplementary didactic information . The essential LO description formulates the observable behavior of the students , showing the level of competence required by the LO . Each LO description begins with a text introduction , which is chosen from a given set of templates ( eg , \\\" The students are able to ... \\\" , \\\" At least the top 10 % of the participants can ... \\\" ) . Semantic indexing is enabled by assigning predefined topics as described above . Supplementary didactic information allows an LO to be associated with other LOs that are named as prerequisites for achieving the LO in question ( predecessor LOs ) . It is also possible to indicate recommended learning and teaching formats for the LO ( eg , \\\" Lecture \\\" , \\\" Problem - based learning \\\" ) and recommended assessment formats ( eg , \\\" Multiple choice test \\\" , \\\" Objective structured practical examination \\\" ) . Global Consistency Checking . SMW technology enables structured data entry , semantic queries , and the dynamic consistent update of a large hypertext . Nonetheless , due to the relatively weak schema constraints imposed by the definition of categories and the semantic properties , complex or global semantic consistency criteria can not be enforced algorithmically . As an example , the SMW platform alone provides no means for checking if an LO that is declared to be a prerequisite of a given LO is associated with prior learning events . In addition , complex cardinality constraints can not be imposed on the SMW properties ( eg , \\\" There should be exactly one study section in each curriculum that has no predecessor \\\" ; in what follows , this is referred to as C1 ) . Therefore , ACLO - Web complements the SMW platform by adding a component enabling the definition and algorithmic check of enhanced consistency criteria ( the ACLO consistency module or ACLO - CM ) . ACLO - CM is designed as a separate Web application operating on the SMW Triple Store and an additional constraint repository ( Figure 3 ) . Consistency criteria are represented by expressions of the Object Constraint Language ( OCL ) . Criterion C1 , introduced above , is shown in Textbox 1 . ACLO - CM enables global consistency checking to be carried out at defined milestones . The component is not designed to prompt the users directly during data entry ( as this would not be feasible for global constraints ) . Instead , it produces a detailed checklist , to be used for revision , of the problems found . Formative Usability Study . A formative usability study assists the implementation of an SMW - based online catalog . Formative usability studies can be successfully carried out with only a few participants who can nonetheless indicate the most relevant usability problems . Nielsen recommended 5 - 8 testers [ 28 ] . We chose a scenario - based approach . The participants used the online catalog , sequentially solving tasks given in the context of a typical scenario that they were asked to imagine . We defined 7 imaginary scenarios involving both students and medical teachers : . ( S1 ) A student preparing for an examination and retrieving the learning objectives of courses recently visited . ( S2 ) A student interested in a topic because a relative suffers from a particular disease . ( S3 ) A student trying to gain an overview of the following term . ( S4 ) A student planning to specialize in a given medical field in the future , who is interested in where his or her chosen field is present in the curriculum . ( S5 ) A medical teacher preparing for a multidisciplinary course . ( S6 ) A medical teacher planning an examination . ( S7 ) A medical teacher trying to find out about the prior knowledge of the students enrolled in his or her course . Data was acquired using an online questionnaire containing the textual description ( vignette ) of the scenarios and instructing the participants on the specific tasks for each scenario . Items 2 and 3 were accompanied by a free - text field where redundant and missing information could be further specified . Finally , the participants could enter free - text feedback on positive aspects , negative aspects , and recommendations for improvements . Methods of Analysis . Due to the small number of participants , the scaled items of the usability test were analyzed by descriptive statistics only . The median , first quartile , minimal , and maximal values were derived from the data and visualized by box plot diagrams . The free - text feedback was analyzed by bottom - up qualitative text coding . The statements were then rearranged by producing a synopsis of statements assigned to the same keywords and interpreted . Software . The online questionnaire for the usability study was based on LimeSurvey ( v 1.85 ) [ 32 ] . Descriptive statistics and box plots were produced by R ( v 2.14.0 ) [ 33 ] . Qualitative text analysis used Microsoft OneNote 2010 . The online catalog was implemented on a virtualized Debian Linux - based XAMP server ( v 1.8.0 ) [ 34 ] using MediaWiki ( v 1.18.5 with SemanticBundle extension r20120327 ) [ 35 ] . Figure 2 . Results . State of ACLO - Web . The main activity concerning the data entry of LOs started in September 2011 . By February 2013 , 5350 LOs of the Aachen Medical Model Curriculum had been collected . The LOs were assigned to 69 modules , 61 medical specialties , and 243 medical topics that are defined in the system . Figure 4 shows the growth of the number of acquired LOs during this time interval . Lessons Learned During Implementation . The use of the free , open source SMW platform enabled a fast and low - budget implementation of ACLO - Web . A first version was available - and immediately used by the LO team - only a few weeks after starting the project . Therefore , the system ( categories , attributes , and online forms ) evolved during the implementation process of the catalog , a process that included several small modifications and one drastic change concerning the conceptual model . The latter resulted from improving the representation of different versions of the curriculum , due to changes in the local examination regulations ( ER ) . We had to learn that , in different ER versions , existing LOs are included in different modules and whole modules may belong to different terms . Thus , some of the relations given in the core data model ( Figure 2 ) needed to become dependent on the ER versions . Technically , this was achieved by introducing so - called semantic internal objects . The flexible way of imposing semantic constraints by semantic forms and templates ( Figure 1 ) greatly supported schema versioning and content migration . Additionally , for organizational reasons , a migration of ACLO - Web to different server hardware was necessary twice . In spite of an ongoing acquisition process , the system evolution went remarkably smoothly . Views and Queries Based on the Core Data Model . The online LO catalog provides different ways of viewing and retrieving LOs . In general , there are 2 methods of access : ( 1 ) browsing predefined overviews , and ( 2 ) searching by criteria individually defined by the user . The model introduced above grants 3 independent views : users can select and view LOs according to their position in the curricular context , their thematic focus , and the faculty and unit responsible for them , respectively . Figure 5 shows the main page of the catalog . In the left - hand area , the page offers access via overviews , and in the right - hand area , the user can access search forms . All overviews are implemented as predefined searches ( ie , overviews are not created manually , but instead are generated by the system using inline queries ) . Thus , all overviews are based on the actual state of the LO repository , and immediately show changes concerning added , modified , or deleted LO information . In order to use a predefined overview , students and faculty members first choose a curricular module , a teaching unit , a medical topic , or a MeSH term . The system then shows an overview of the LOs associated with this entity in a tabular layout . Detailed information on a particular LO is accessible by following the Web link to the wiki page describing the LO ( provided by the overview ) . All tables can be dynamically rearranged by clicking on the heading of one or more columns , indicating the sorting criterion . The default order presented first depends on the type of overview ( Table 1 ) . The overviews based on medical topic and medical specialty provide a longitudinal projection , that is , the users can see at which point in the course of study the topic is addressed or the specialty is involved in the curriculum , respectively . Collaborative Specification of Learning Objectives . As a key feature of the Social Semantic Web - based approach , the online catalog allows a distributed , collaborative , and structured data entry , requiring the client to have nothing more than a Web browser ( and of course an Internet connection ) . The wiki - based user interface uses 13 online forms for data entry and 4 online forms for flexible LO search . Figure 6 shows a screenshot of the form for describing LOs . Due to the high number of input widgets and the German labels of the system , the screenshot comes with additional English labels . The attributes used for specifying an LO have been introduced in the methods section above . In order to support offline data collection , we equipped the representatives responsible for the LO specification with predefined Excel forms that they could alternatively use for specifying LOs . The LO specifications were afterwards transferred to the system by the LO catalog team , and then checked online by the representatives . Following the elicitation process described above , primary data entry was centralized and carried out by the ACLO team in order to foster high and uniform data quality , and consistent semantic indexing . The faculty was then encouraged to revise the catalog . Following the wiki approach , ACLO - Web avoids the fine , granular access restrictions that are present in many content management systems . Thus , versioning support and page comments offered by the platform play a pivotal role in supporting an open , but nonetheless socially controlled , revision process . Global Consistency Checking : ACLO - CM . Figure 7 shows a screenshot of the component for checking global consistency criteria ( ACLO - CM ) . At present , ACLO - CM defines and checks 12 OCL constraints . Additionally , ACLO provides a convenient overview of modified SMW entries , which is structured by the categories of the model ( see Figure 1 ) . The report on existing consistency problems generated by ACLO - CM is structured by the categories of the affected SMW entries and the criteria checked . Figure 7 shows a typical ACLO - CM report . Formative Usability Study . The formative usability study was carried out in August 2012 . Five medical students ( in their second and third years of study ) volunteered to participate in the study . Figure 8 shows the results of the overall rating of usability items after completing all 7 scenarios . The qualitative analysis of the full - text answers yielded 30 separate statements assigned to 17 codes by the analysis . Table 2 gives an overview of the code categories , codes , and numbers of statements assigned . These last statements said that the LO presentation should avoid the repeated presentation of the standard introductory text and should integrate the competence levels ( verbs ) into the textual description of the behavior . With respect to the actual state of the LO repository , missing LOs were reported ; some participants named missing aspects in detail ( 5 statements ) . The statements contained 5 detailed recommendations , including the one proposing the attribute for multidisciplinary teaching formats . The extended search forms were recommended ( 2 statements ) ; one participant suggested a graphical timeline for visualizing the longitudinal distribution of LOs . The participants explicitly encouraged the alternative ways of accessing the LOs by overviews ( 1 statement ) , the structured , table - based presentation of the results ( 2 statements ) , and the longitudinal projection of a topic in the curriculum . Discussion . Principal Findings . The focus of this paper is to report on the application of Social Semantic Web technology to support the knowledge management process and to enable the implementation and collaborative maintenance of an online catalog of learning objectives of a complete medical curriculum . Thus , we address neither the issue of the effect of competence - based learning objectives on the learning outcome nor a summative evaluation of the quality of ACLO . Nonetheless , we report the results of the formative usability study because they were an essential part of the implementation process . Although it is known that usability studies can successfully rely on a relatively small number of participants [ 36 ] , in our case the number is at the low end even for a formative study . The selection of the participants may be further biased , because we asked the official medical student representatives to name students who would possibly volunteer to participate in the study . Thus , the participating students can be assumed to take more interest than average in the further development and improvement of the existing model curriculum . The study did not involve the faculty . Although a medical expert was part of the LO team , his feedback was continuously incorporated into the implementation process . Nonetheless , the usability study will have to be extended to faculty members in the future . Implications of the Usability Study . Taking into account the limitations mentioned above , the usability study nonetheless produced extremely valuable feedback that led to the improvement of the online catalog . The participants gave detailed hints on problems concerning orientation within the catalog , comprehensibility , and performance of the system . The hints on navigation problems and the interpretation of the short titles have already led to improvements in the present version of the system . The main criticism concerned not the application but the state of the catalog , which was indeed poor when the formative evaluation took place and has markedly improved since . The box plots yielded a low rating for the completeness of information . In the context of the same scenario , the qualitative feedback stated that information on multidisciplinary modules was missing , or proposed an additional wiki attribute for capturing the relevant information . These require further coordination and reconciliation . The authors did not know about these findings during the preparation of the scenarios . Enabling Nonproprietary Solutions . As noted in the introduction , custom - built software dominates the field of curriculum management systems . The SMW - based approach fills the gap between individual and flexible requirements and existing software platforms for supporting curriculum management . Furthermore , the data model and the information content of ACLO are accessible via standardized interfaces ( eg , SPARQL queries ) and can be exported to the W3C - standardized OWL format . Both aspects foster semantic interoperability and cross - platform migration . From a technical perspective , the availability of the MediaWiki Web application programing interface ( Web - API)-well known from Wikipedia - seems even more important . The Web - API requires nothing but a Web connection in order to enable other systems to interact with ACLO . The available formats and interfaces of the SMW - based approach not only improve standardized information access and system interoperability , but also allow importing existing classifications or ontologies to the system . This feature enabled , for example , the import of the ICD into ACLO . Furthermore , the system 's information content can be easily enriched by seamlessly including interwiki links to other wiki - based information sources including Wikipedia . Overall , the SMW - based approach shows potential to provide a versatile , but partly standardized platform for curriculum management , especially supporting collaborative aspects - far from being \\\" yet another IT tool \\\" . Comparison With Prior Work . As stated in the introduction , ontologies serve as a means for semantic indexing [ 21 - 24 ] . Semantic indexing in ACLO uses not only medical standard classification , but also a folksonomy - following the collaborative approach . In contrast , none of these systems profits from the semantic standards and collaborative approach of the Social Semantic Web . The Semantic Web has long been discussed as a means for supporting curriculum development by representing the learning design and content of a curriculum [ 37 ] . Recently , Coccoli et al outlined the potential of semantic wikis for collaborative curriculum development [ 40 ] . None of these system concepts or demonstrators made its way to productive use and none was intended to support curriculum management in the field of medical education . To the best of the authors ' knowledge , ACLO is pioneering as a medical curriculum management system rigorously based on a Social Semantic Web approach and platform . Agile Knowledge Management by SMW . The growth of the number of LOs during the past year shows that the system successfully supports the elicitation and systematic collection of LOs . As shown in Figure 4 , the environment successfully supports an active process of the elicitation of learning objectives . The rapid increase of the number of LOs during the past months can be explained by the preparation and announcement of a faculty - wide evaluation process ; this has obviously operated as an incentive . We received positive feedback concerning the applicability and usability of the system , not only from participants in the usability study but also from the LO team and faculty involved . This may be an effect of the familiarity of nearly all users with Wikipedia ; the use of the MediaWiki platform seems to have led to a low threshold for using the LO catalog . At present , ACLO uses the few user roles originally provided by the MediaWiki platform . Thus , all members of the faculty can edit the LOs contained in ACLO during the revision process . This fact is causing controversy - part of the faculty demands the introduction of fine - grained , role - based access control , while others advocate the existing approach , relying on versioning support , the transparency of the page history , and shared social media etiquette . Kiessling et al report on a systematic consensus process that improved the definition of ( outcome - based ) learning objectives , which the authors consider an intrinsically collaborative process [ 41 ] . Their statement is in line with the finding that curriculum management is resource - intensive and requires systematic change - management [ 9 ] . Additionally , Wong and Roberts argued that the procedural nature of curriculum mapping requires ongoing IT - enabled feedback [ 2 ] . Wikis proved to enable complex consensus processes and collaborative planning in the medical domain [ 42 ] . We consider that the SSW - based ( Web 3.0 ) approach is an ideal platform for enabling and efficiently supporting curriculum mapping . The SMW platform , its versioning support , and standardized import / export formats effectively enabled successful evolution of the system . The weak schema constraints imposed by the approach ( Figure 2 ) allowed a very flexible evolution of the model , while at the same time enabling structured information management and access . Thus , the SMW - based approach proved to enable an agile implementation of computer - supported knowledge management . The approach , based on standard Social Semantic Web formats and technology , represents a feasible and effectively applicable compromise between answering to the individual requirements of curriculum management at a particular medical school and using proprietary systems . Given the overall feasibility of a Web 3.0-based curriculum management system , these special aspects of a flexible and agile knowledge management can indeed foster collaborative curriculum mapping as a feedback - driven process . Future Directions . As mentioned before , further formative usability testing and the scheduled revision phase will involve all members of the faculty . ACLO will be open for all students at our medical school in approximately April 2013 . We are now preparing a summative evaluation of the system , including a log file analysis of the users ' behavior . At present , 2 questionnaires ( addressing students and faculty , respectively ) will undergo pilot testing and a revision process . The questionnaire for faculty contains scaled items concerning qualitative aspects of the learning objectives ' specifications ( SMART - criteria : Specific , Measurable , Accepted , Realistic , Timely ) , the completeness , and the perceived effectiveness of the catalog . The students ' questionnaire further addresses the students ' appraisal of the benefit from ACLO . A second curriculum ( dentistry ) is going to be included in 2014 . Maloney et al showed that medical students appreciate the benefit of online repositories of learning resources [ 43 ] . They also showed that milestones of the curriculum ( eg , examinations ) often trigger access to the repositories . Consequently , ACLO will be integrated with an already existing system used for the consistent indexing of eLearning media , by a semantic network based on the Medical Subject Headings and ICD codes , which were linked by associations taken from the SNOMED Clinical Terms terminology . The integration will link the topics and external vocabularies associated with ACLO learning objectives to concepts of the semantic network and will therefore extend the systems capability of finding similar LOs . Last but not least , the ongoing project of a German national catalog of competence - based medical learning objectives will heavily influence the future development of ACLO - Web and result in challenging tasks concerning system integration and LO identification . This will eventually be based on ontology mapping approaches . Conflicts of Interest . References . Denny JC , Smithers JD , Armstrong B , Spickard A. \\\" Where do we teach what ? \\\" Finding broad concepts in the medical school curriculum . J Gen Intern Med 2005 Oct;20(10):943 - 946 [ FREE Full text ] [ CrossRef ] [ Medline ] . Willett TG . Current status of curriculum mapping in Canada and the UK . Med Educ 2008 Aug;42(8):786 - 793 . [ CrossRef ] [ Medline ] . Nikendei C , Weyrich P , J\\u00fcnger J , Schrauth M. Medical education in Germany . Med Teach 2009 Jul;31(7):591 - 600 . [ Medline ] . Hausman JJ . Mapping as an approach to curriculum planning . Curriculum Theory Network 1974;4(2/3):192 - 198 . English FW . Curriculum Mapping . Educational Leadership 1980;37(7):558 - 559 . Gjerde CL . ' Curriculum mapping ' : objectives , instruction , and evaluation . J Med Educ 1981 Apr;56(4):316 - 323 . [ Medline ] . Harden RM . AMEE Guide No . 21 : Curriculum mapping : a tool for transparent and authentic teaching and learning . Med Teach 2001 Mar;23(2):123 - 137 . [ CrossRef ] [ Medline ] . Fleiszer DM , Posel NH . Development of an undergraduate medical curriculum : the McGill experience . Acad Med 2003 Mar;78(3):265 - 269 . [ Medline ] . Lee MY , Albright SA , Alkasab T , Damassa DA , Wang PJ , Eaton EK . Tufts Health Sciences Database : lessons , issues , and opportunities . Acad Med 2003 Mar;78(3):254 - 264 . [ Medline ] . Salas AA , Anderson MB , LaCourse L , Allen R , Candler CS , Cameron T , et al . CurrMIT : a tool for managing medical school curricula . Acad Med 2003 Mar;78(3):275 - 279 . [ Medline ] . Landry LG , Alameida MD , Orsolini - Hain L , Renwanz Boyle A , Priv\\u00e9 A , Chien A , et al . Responding to demands to change nursing education : use of curriculum mapping to assess curricular content . J Nurs Educ 2011 Oct;50(10):587 - 590 . [ CrossRef ] [ Medline ] . Bloch R , B\\u00fcrgi H. The Swiss catalogue of learning objectives . Med Teach 2002 Mar;24(2):144 - 150 . [ CrossRef ] [ Medline ] . Bosse HM , Dambe R , Juenger J , Kadmon M. An interdisciplinary and interactive online tool to manage the continuous development of learning objectives in a curriculum . Z Evid Fortbild Qual Gesundhwes 2011;105(2):116 - 123 . [ CrossRef ] [ Medline ] . Hege I , Siebeck M , Fischer MR . An online learning objectives database to map a curriculum . Med Educ 2007 Nov;41(11):1095 - 1096 . [ CrossRef ] [ Medline ] . Hege I , Nowak D , Kolb S , Fischer MR , Radon K. Developing and analysing a curriculum map in Occupational- and Environmental Medicine . BMC Med Educ 2010;10:60 [ FREE Full text ] [ CrossRef ] [ Medline ] . Dreinh\\u00f6fer KE , Walcher F , Obertacke U , Waydhas C , Josten C , R\\u00fcsseler M , et al . [ Development of a catalogue of undergraduate learning objectives for orthopaedics and traumatology]. Z Orthop Unfall 2008;146(4):520 - 533 . [ CrossRef ] [ Medline ] . Watson EG , Moloney PJ , Toohey SM , Hughes CS , Mobbs SL , Leeper JB , et al . Development of eMed : a comprehensive , modular curriculum - management system . Acad Med 2007 Apr;82(4):351 - 360 . [ CrossRef ] [ Medline ] . Bell CE , Ellaway RH , Rhind SM . Getting started with curriculum mapping in a veterinary degree program . J Vet Med Educ 2009;36(1):100 - 106 . [ CrossRef ] [ Medline ] . Oyewumi M , Isaac K , Schreiber M , Campisi P. Undergraduate otolaryngology education at the University of Toronto : a review using a curriculum mapping system . J Otolaryngol Head Neck Surg 2012 Feb;41(1):71 - 75 . [ Medline ] . Denny JC , Smithers JD , Miller RA , Spickard A. \\\" Understanding \\\" medical school curriculum content using KnowledgeMap . J Am Med Inform Assoc 2003;10(4):351 - 362 [ FREE Full text ] [ CrossRef ] [ Medline ] . Aronson AR , Lang FM . An overview of MetaMap : historical perspective and recent advances . J Am Med Inform Assoc 2010 Jun;17(3):229 - 236 . [ CrossRef ] [ Medline ] . Willett TG , Marshall KC , Broudo M , Clarke M. TIME as a generic index for outcome - based medical education . Med Teach 2007 Sep;29(7):655 - 659 . [ CrossRef ] [ Medline ] . Dexter J , Koshland G , Waer A , Anderson D. Mapping a curriculum database to the USMLE Step 1 content outline . Med Teach 2012;34(10):e666-e675 . [ CrossRef ] [ Medline ] . Gasevic D , Zouaq A , Torniai C , Jovanovic J , Hatala M. An Approach to Folksonomy - Based Ontology Maintenance for Learning Environments . IEEE Trans Learning Technol 2011 Oct;4(4):301 - 314 . [ CrossRef ] . Bruner J. The process of education . Cambridge : Harvard University Press ; 1977 . Kr\\u00f6tzsch M , Vrandecic D , V\\u00f6lkel M. Semantic MediaWiki . In : Cruz I , Decker S , Allemang D , Preist C , Schwabe D , Mika P , editors . The Semantic Web - ISWC 2006 . Heidelberg : Springer ; 2006:935 - 942 . Berners - Lee T , Hendler J. Publishing on the semantic web . Nature 2001 Apr 26;410(6832):1023 - 1024 . [ CrossRef ] [ Medline ] . Horrocks I , Patel - Schneider PF , Van Harmelen F. From SHIQ and RDF to OWL : The making of a web ontology language . Web semantics : science , services and agents on the World Wide Web 2003;1(1):7 - 26 . McGuinness DL , Van Harmelen F. OWL web ontology language overview . W3C recommendation 2004;10(2004 - 03):1 [ FREE Full text ] [ WebCite Cache ] . Nielsen J , Landauer TK . A mathematical model of the finding of usability problems . In : Ashlund S , Mullet K , Henderson A , Hollnagel E , White T , editors . Proceedings of the INTERACT'93 and CHI'93 conference on Human factors in computing systems . New York : ACM Press ; 1993:206 - 213 . Koper R. Use of the Semantic Web to Solve Some Basic Problems in Education . Journal of Interactive Media in Education 2004;6:1 - 23 [ FREE Full text ] [ WebCite Cache ] . Tang A , Rahman AA . Proof - of - Concept Design of an Ontology - Based Computing Curricula Management System . In : Cherifi H , Zain JM , El - Qawasmeh E , editors . Digital Information and Communication Technology and Its Applications . Heidelberg : Springer ; 2011:280 - 292 . Segedinac M , Konjovic Z , Surla D , Savic G. An OWL representation of the MLO model . In : Proceedings of the IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics ( SISY ) . 2012 Presented at : 2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics ( SISY ) ; Sept. 20 - 22 , 2012 ; Serbia p. 465 - 470 . Coccoli M , Vercelli G , Vivanet G. Semantic Wiki : a collaborative tool for instructional content design . Journal of e - Learning and Knowledge Society 2012;8(2):113 - 122 . Kiessling C , Dieterich A , Fabry G , H\\u00f6lzer H , Langewitz W , M\\u00fchlinghaus I , Committee CommunicationSocial Competencies of the Association for Medical Education Gesellschaft f\\u00fcr Medizinische Ausbildung , Basel Workshop Participants . Communication and social competencies in medical education in German - speaking countries : the Basel consensus statement . Results of a Delphi survey . Patient Educ Couns 2010 Nov;81(2):259 - 266 . [ CrossRef ] [ Medline ] . Gupta S , Wan FT , Newton D , Bhattacharyya OK , Chignell MH , Straus SE . WikiBuild : a new online collaboration process for multistakeholder tool development and consensus building . J Med Internet Res 2011;13(4):e108 [ FREE Full text ] [ CrossRef ] [ Medline ] . Maloney S , Chamberlain M , Morrison S , Kotsanas G , Keating JL , Ilic D. Health professional learner attitudes and use of digital learning resources . J Med Internet Res 2013;15(1):e7 [ FREE Full text ] [ CrossRef ] [ Medline ] . Edited by G Eysenbach ; submitted 17.03.13 ; peer - reviewed by M Broom , T Mabotuwana , B Good ; comments to author 23.04.13 ; revised version received 31.05.13 ; accepted 14.06.13 ; published 15.08.13 . Copyright . \\u00a9 Cord Spreckelsen , Sonja Finsterer , Jan Cremer , Hennig Schenkat . \"}",
        "_version_":1692580798803214336,
        "score":19.038872},
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":18.997463},
      {
        "id":"2e9d1444-9032-4b61-912b-e241c275f8c5",
        "_src_":"{\"url\": \"http://journeyspast.blogspot.com/2012/02/this-little-tablet-went-to.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701159031.19/warc/CC-MAIN-20160205193919-00083-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Abstract . Personalization and recommendation systems are a solution to the problem of content overload , especially in large information systems . In this thesis , a personalized recommendation system enhanced with semantic knowledge has been developed in order to overcome the most common limitations of traditional approaches : the cold - start and the sparsity problems . The recommender consists of the following two main components . A user - profile learning algorithm combines user 's feedback from different channels and employs domain inferences to construct accurate user profiles . A recommendation algorithm , using content - based filtering , exploits the semantic structure of the domain to obtain accurate predictions and generate the corresponding recommendations . The system 's design proposed is flexible enough to be potentially applied to applications of any domain that can be properly described using ontologies . In addition to the development of the recommendation system , an existing Web - application in the tourism domain has been extended and adapted in order to be able to integrate the recommender into it . The overall recommendation system has been evaluated and the results obtained indicate that it satisfies the requirements established . \"}",
        "_version_":1692668729375064064,
        "score":18.885761},
      {
        "id":"3a05a13d-0dd2-4d25-9de4-dc04160909f2",
        "_src_":"{\"url\": \"http://www.singleservecoffee.com/archives/004770.php\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153998.27/warc/CC-MAIN-20160205193913-00067-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"A video sponsored by the society discusses Searle 's Chinese Room Argument ( CRA ) and the heated debates surrounding it . In this video , which is accessible to the general public and those with interest in AI , Olly 's Philosophy Tube ... . On Friday 4th September , philosopher and AISB member Dr Yasemin J Erden , participated in an AI roundtable at Second Home , hosted by Index Ventures and SwiftKey . \\u00c2 Joining her on the panel were colleagues from academia and indu ... . The AISB Convention is an annual conference covering the range of AI and Cognitive Science , organised by the Society for the Study of Artificial Intelligence and Simulation of Behaviour . The 2016 Convention will be held at the Uni ... . Stephen Hawking thinks computers may surpass human intelligence and take over the world . This view is based on the ideology that all aspects of human mentality will eventually be realised by a program running on a suitable compu ... . All individual members of The Society for the Study of Artificial Intelligence and Simulation of Behaviour have a personal subscription to the Taylor Francis journal Connection Science as part of their membership . How to Acce ... . AISB Committee member and Research Fellow at Goldsmiths , University of London , Dr Mohammad Majid al - Rifaie was\\u00c2 interviewed\\u00c2 by the BBC ( in Farsi ) along with his colleague Mohammad Ali Javaheri Javid on the 6 November 2014 . He was a .. The channel currently holds a number of videos from the AISB 2010 Convention . Videos include the AISB round t .. Notice . AISB event Bulletin Item . Call for tutorials : European Semantic Web Conference 2007 ( ESWC 2007 ) . The explicit representation of the semantics of data , enriched with domain theories ( Ontologies ) , will enable a web that provides a qualitatively new level of service . It will weave together a large network of human knowledge and makes this knowledge machine - processable . Various automated services will help the users to achieve their goals by accessing and processing information in machine - understandable form . This network of knowledge systems will ultimately lead to truly intelligent systems , which will be employed for various specialized reasoning subsystems to accomplish complex tasks . Many technologies and methodologies are being developed within Artificial Intelligence , Natural Language Processing , Machine Learning , Databases , Multimedia Systems , Distributed Systems , Software Engineering and Information Systems that can contribute towards the realization of this vision . The 4th Annual European Semantic Web Conference ( ESWC 2007 ) will present the latest results in research and application of Semantic Web technologies , including knowledge mark - up languages , Semantic Web services , and ontology management . More details can be found on the ESWC 2007 homepage . In addition to the regular research and workshop programme , ESWC 2007 invites tutorials on relevant topics of interest . A tutorial should present the state of the art of a semantic web area enabling attendees to fully appreciate the current issues , main schools of thought and possible application areas . Proposals for tutorials are welcome for the ESWC 2007 topics of interest ( see bottom ) ; tutorial proposals are requested to follow the submission process defined below . TUTORIAL PROPOSALS ESWC 2007 tutorials may be either for a full day or for a half day . Unless there is a clear rationale we will give preference to half day tutorials over full day tutorials . Tutorials proposed for the ESWC 2007 should cover one topic in appropriate depth ( see ESWC 2007 topics below ) , and present this in a appropriate manner which enables attendees to fully comprehend and apply emerging Semantic Web technologies . Although tutorials may focus entirely on theoretical aspects , we strongly encourage hands - on sessions where appropriate . For each accepted tutorial , we offer one discount of 180 EUR on the registration fee for ESWC 2007 ( 50 % of the Early - Bird registration fee ) . \"}",
        "_version_":1692668269627965440,
        "score":18.760265}]
  }}
