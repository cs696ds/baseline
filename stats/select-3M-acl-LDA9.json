{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"learning methods statistical work charniak collins made parsers parsing words syntactic buchholz phrases documents data roth ratnaparkhi full significant research",
      "fl":"*,score"}},
  "response":{"numFound":2979189,"start":0,"maxScore":29.29097,"numFoundExact":true,"docs":[
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":29.29097},
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":26.102386},
      {
        "id":"8cb67a18-8861-4dc6-9ff7-698e71b6dc2a",
        "_src_":"{\"url\": \"http://bulbapedia.bulbagarden.net/w/index.php?title=Barrage_(move)&oldid=1959218\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162035.80/warc/CC-MAIN-20160205193922-00120-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Course Features . Course Description . This course is a graduate introduction to natural language processing - the study of human language from a computational perspective . It covers syntactic , semantic and discourse processing models , emphasizing machine learning or corpus - based methods and algorithms . It also covers applications of these methods and models in syntactic parsing , information extraction , statistical machine translation , dialogue systems , and summarization . The subject qualifies as an Artificial Intelligence and Applications concentration subject . Course Collections . Find Courses by Topic . Michael Collins , and Regina Barzilay . 6.864 Advanced Natural Language Processing , Fall 2005 . License : Creative Commons BY - NC - SA . Tools . Our Corporate Supporters . About MIT OpenCourseWare . MIT OpenCourseWare makes the materials used in the teaching of almost all of MIT 's subjects available on the Web , free of charge . With more than 2,200 courses available , OCW is delivering on the promise of open sharing of knowledge . Learn more \\\" \"}",
        "_version_":1692670526734991360,
        "score":21.93117},
      {
        "id":"32c9f64f-c981-4437-9424-056c60cde52d",
        "_src_":"{\"url\": \"http://ancienthebrewpoetry.typepad.com/ancient_hebrew_poetry/2009/01/why-i-love-virginia-woolf.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148758.73/warc/CC-MAIN-20160205193908-00166-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"It is the aim of this module to explore some of the aspects and challenges in Human Language Technologies ( HLT ) that are of relevance to Computer Assisted Language Learning ( CALL ) . Starting with a brief outline of some of the early attempts in HLT , using the example of Machine Translation ( MT ) , it will become apparent that experiences and results in this area had a direct bearing on some of the developments in CALL . CALL soon became a multi - disciplinary field of research , development and practice . Some researchers began to develop CALL applications that made use of Human Language Technologies , and a few such applications will be introduced in this module . The advantages and limitations of applying HLT to CALL will be discussed , using the example of parser - based CALL . This brief discussion will form the basis for first hypotheses about the nature of human - computer interaction ( HCI ) in parser - based CALL . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . Piklu Gupta : At this time of writing this module Piklu was a lecturer in German Linguistics at the University of Hull , UK . He is now working for Fraunhofer IPSI . Mathias Schulze : At this time of writing this module Mathias was a lecturer in German at UMIST , now merged with the University of Manchester , UK . He is now working at the University of Waterloo , Canada . His main research interest is in parser - based CALL and linguistics . He is an active member of the NLP SIG within the EUROCALL professional association and ICALL within the CALICO professional association . Graham Davies , ICT4LT Editor , Thames Valley University , UK . Graham has been interested in Machine Translation since 1976 . Human Language Technologies ( HLT ) is a relatively new term that embraces a wide range of areas of research and development in the sphere of what used to be called Language Technologies or Language Engineering . The aim of this module is to familiarise the student with key areas of HLT , including a range of Natural Language Processing ( NLP ) applications . NLP is a general term used to describe the use of computers to process information expressed in natural ( i.e. human ) languages . The term NLP is used in a number of different contexts in this document and is one of the most important branches of HLT . There is a Special Interest Group in Language Processing , NLP SIG , within the EUROCALL professional association , and a Special Interest Group in Intelligent Computer Assisted Language Instruction ( ICALL ) within the CALICO professional association . Both have similar aims , namely to further research in a number of areas that are mentioned in this module , such as : . Artificial Intelligence ( AI ) . Computational Linguistics . Corpus - Driven and Corpus Linguistics . Formal Linguistics . Machine Aided Translation ( MAT ) . Machine Translation ( MT ) . Natural Language Interfaces . Natural Language Processing ( NLP ) . Theoretical Linguistics . All of the above are areas of research that have produced results which have proven , are proving and will prove very useful in the field of Computer Assisted Language Learning . Of course , this module can not teach you everything there is to know about HLT . This is neither necessary nor possible . The two main authors of this module are living proof of that ; they both started off as language teachers and then got interested in HLT . A multilingual CD - ROM titled A world of understanding was produced in 1998 on behalf of the Information Society and Media Directorate General of the European Commission under its former name , DGXIII . The aim of the CD - ROM was to demonstrate the importance of HLT in helping to realise the benefits of the Multilingual Information Society , in particular forming a review and record of the Language Engineering Sector of the Fourth Framework Programme of the European Union ( 1994 - 98 ) . 1.1 Introduction to HLT . [ ... ] there is no doubt that the development of tools ( technology ) depends on language - it is difficult to imagine how any tool - from a chisel to a CAT scanner - could be built without communication , without language . What is less obvious is that the development and the evolution of language - its effectiveness in communicating faster , with more people , and with greater clarity - depends more and more on sophisticated tools . ( European Commission : Language and technology 1996:1 ) . Language and technology lists the following examples of language technology ( using an admittedly broad understanding of the term ) : . photocopier ( p. 10 ) . laser printer ( p. 11 ) . fax machine ( p. 12 ) . desktop publishing ( p. 13 ) . scanner , modem ( p. 15 ) . electronic mail ( p. 16 ) . machine translation ( p. 17 ) . translator 's workbench ( p. 18 ) . tape recorder , database search engines ( p. 19 ) . telephone ( p. 25 ) . Many of these are already being used in language learning and teaching . The field of human language technology covers a broad range of activities with the eventual goal of enabling people to communicate with machines using natural communication skills . Research and development activities include the coding , recognition , interpretation , translation , and generation of language . [ ... ] Advances in human language technology offer the promise of nearly universal access to online information and services . Since almost everyone speaks and understands a language , the development of spoken language systems will allow the average person to interact with computers without special skills or training , using common devices such as the telephone . These systems will combine spoken language understanding and generation to allow people to interact with computers using speech to obtain information on virtually any topic , to conduct business and to communicate with each other more effectively . [ Source : Foreword to ( Cole 1997 ) ] . Facilitating and supporting all aspects of human communication through machines has interested researchers for a number of centuries . The use of mechanical devices to overcome language barriers was proposed first in the seventeenth century . Then , suggestions for numerical codes to be used to mediate between languages were made by Leibnitz , Descartes and others ( v. Hutchins 1986:21 ) . The beginnings of what we describe today as Human Language Technologies are , of course , closely connected to the advent of computers . ( i ) Various games , e.g. chess , noughts and crosses , bridge , poker ( ii ) The learning of languages ( iii ) Translation of languages ( iv ) Cryptography ( v ) Mathematics . Of these ( i ) , ( iv ) , and to a lesser extent ( iii ) and ( v ) are good in that they require little contact with the outside world . For instance in order that the machine should be able to play games its only organs need be ' eyes ' capable of distinguishing the various positions on a specially made board , and means for announcing its own moves . Mathematics should preferably be resticted to branches where diagrams are not much used . Of the above possible fields the learning of languages would be the most impressive , since it is the most human of these activities . This field sees however to depend too much on sense organs and locomotion to be feasible . ( Turing 1948:9 ) . Later on , Machine Translation enjoyed a period of popularity with researchers and funding bodies in the United States and the Soviet Union : . From 1956 onwards , the dollars ( and roubles ) really started to flow . Between 1956 and 1959 , no less than twelve research groups became established at various US universities and private corporations and research centres . Although linguists , language teachers and computer users today may find these predictions ridiculous , it was the enthusiasm and the work during this time that form the basis of many developments in HLT today . Research and development in HLT is nowadays more rapidly transferred into commercial systems than was the case up until the 1980s . Indeed HLT is becoming increasingly pervasive in our everyday lives . Here are some examples : . Machine Translation ( Section 3 ): There are many online translation systems that can be accessed free of charge , causing headaches for teachers whose students thought that they could save themselves time and who were blissfully unaware of the unreliability of their output ( Section 3.2 ) . Speech synthesis ( Section 4.1 ): Satellite navigation ( satnav ) devices for motor vehicles use systems that read out road numbers , street names and directions for the driver , and their output is surprisingly good . Speech recognition ( Section 4.2 ): If you make a telephone call to a customer support service you may hear a telephone recording that asks you to say a word or short phrase so that you can be connected to the appropriate department . Other previously unexpected areas of use are emerging . It is now , for instance , common for mobile phones to have what is known as predictive text input to aid the writing of short text messages . Instead of having to press one of the nine keys a number of times to produce the correct letter in a word , software in the phone compares users ' key presses to a linguistic database to determine the correct ( or most likely ) word . Most Internet search engines also now incorporate some kind of linguistic technology to enable users to enter a query in natural language , for example \\\" What is meant by log - likelihood ratio ? \\\" is as acceptable a query as simply \\\" log - likelihood ratio \\\" . What are the possible benefits for language teaching and learning of using HLT ? Here are some examples : . Teachers might want to preprocess a text to highlight certain grammatical phenomena or patterns . This can easily be done with a word - processor . Teachers might use part - of - speech taggers ( see Section 5 ) which could save them the trouble of having to manually annotate a text . Parsers available either on the Web or for local use on PCs can generate a graphical representation of sentence structure that may be useful for grammatical analysis for more advanced learners . Machine Translation ( MT ) has been the dream of computer scientists since the 1940s . The student 's attention is drawn in particular to the following publications , which provide a very useful introduction to MT : . Hutchins ( 1999 ) \\\" The development and use of machine translation systems and computer - based translation tools \\\" . Paper given at the International Symposium on Machine Translation and Computer Language Information Processing , 26 - 28 June 1999 , Beijing , China . 3.1 Machine Translation : a brief history . Initial work on Machine Translation ( MT ) systems was typified by what we would now consider to be a naive approach to the \\\" problem \\\" of natural language translation . Successful decoding of encrypted messages by machines during World War II led some scientists , most notably Warren Weaver , to view the translation process as essentially analogous with decoding . The concept of Machine Translation in the modern age can be traced back to the 1940s . Warren Weaver , Director of the Natural Sciences Division of the Rockefeller Foundation , wrote to his friend Norbert Wiener on 4 March 1947 - short ly after the first computers and computer programs had been produced : . Recognising fully , even though necessarily vaguely , the semantic difficulties because of multiple meanings , etc . , I have wondered if it were unthinkable to design a computer which would translate . Even if it would translate only scientific material ( where the semantic difficulties are very notably less ) , and even if it did produce an inelegant ( but intelligible ) result , it would seem to me worth while . When I look at an article in Russian , I say \\\" This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode \\\" . Have you ever thought about this ? As a linguist and expert on computers , do you think it is worth thinking about ? Cited in Hutchins ( 1997 ) . Weaver was possibly chastened by Wiener 's pessimistic reply : . I frankly am afraid the boundaries of words in different languages are too vague and the emotional and international connotations are too extensive to make any quasi - mechanical translation scheme very hopeful . But Weaver remained undeterred and composed his famous 1949 Memorandum , titled simply \\\" Translation \\\" , which he sent to some 30 noteworthy minds of the time . It posited in more detail the need for and possibility of MT . Thus began the first era of MT research . A direct system would comprise a bilingual dictionary containing potential replacements or target language equivalents for each word in the source language . A restriction of such MT systems was therefore that they were unidirectional and could not accommodate many languages unlike the systems that followed . Rules for choosing correct replacements were incorporated but functioned on a basic level ; although there was some initial morphological analysis prior to dictionary lookup , subsequent local re - ordering and final generation of the target text , there was no scope for syntactic analysis let alone semantic analysis ! Inevitably this often led to poor quality output , which certainly contributed to the severe criticism of MT in the 1966 Automatic Language Processing Advisory Committee ( ALPAC ) report which stated that it saw little use for MT in the foreseeable future . The damning judgment of the ALPAC report effectively halted research funding for machine translation in the USA throughout the 1960s and 1970s . We can say that both technical constraints and the lack of a linguistic basis hampered MT systems . The system developed at Georgetown University , Washington DC , and first demonstrated at IBM in New York in 1954 had no clear separation of translation knowledge and processing algorithms , making modification of the system difficult . In the period following the ALPAC report the need was increasingly felt for an approach to MT system design which would avoid many of the pitfalls of 1 G systems . By this time opinion had shifted towards the view that linguistic developments should influence system design and development . Indeed it can be said that the second generation ( 2 G ) of \\\" indirect \\\" systems owed much to linguistic theories of the time . 2 G systems can be divided essentially into \\\" interlingual \\\" and \\\" transfer \\\" systems . We will look first of all at interlingual systems , or rather those claiming to adopt an interlingual approach . Although Warren Weaver had put forward the idea of an intermediary \\\" universal \\\" language as a possible route to machine translation in his 1947 letter to Norbert Wiener , linguistics was unable to offer any models to apply until the 1960s . By virtue of its introduction of the concept of \\\" deep structure \\\" , Noam Chomsky 's theory of transformational generative grammar appeared to offer a route towards \\\" universal \\\" semantic representations and thus appeared to provide a model for the structure of a so - called interlingua . An interlingua is not a natural language , rather it can be seen as a meaning representation which is independent of both the source and the target language of translation . An interlingua system maps from a language 's surface structure to the interlingua and vice versa . A truly interlingual approach to system design has obvious advantages , the most important of which is economy , since an interlingual representation can be applied for any language pair and facilitates addition of other language pairs without major additions to the system . The next section looks at \\\" transfer \\\" systems . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . ( For n languages the number of transfer modules required would be n ( n -1 ) or n ( n -1 ) /2 if the modules are reversible ) . An important advance in 2 G systems when compared to 1 G was the separation of algorithms ( software ) from linguistic data ( lingware ) . In a system such as the Georgetown model the program mixed language modelling , translation and the processing thereof in one program . This meant that the program was monolithic and it was easy to introduce errors when trying to rectify an existing shortcoming . The move towards separating software and lingware was hastened by parallel advances in both computational and linguistic techniques . The adoption of linguistic formalisms in the design of systems and the development of high level programming languages enabled MT workers to code in a more problem - oriented way . The development in programming languages meant that it was becoming ever easier to code rules for translation in a meaningful manner and arguably improved the quality of these rules . The declarative nature of linguistic description could now be far more explicitly reflected in the design of programs for MT . . Early MT systems were predominantly parser - based , one of the first steps in such a system being to parse and tag the source language : see Section 5 on Parsing and Tagging . More recent current approaches to MT rely less on formal linguistic descriptions than the transfer approach described above . Translation Memory ( TM ) systems are now in widespread commercial use : see below and Chapter 10 of Arnold et al . ( 1994 ) . Example - Based Machine Translation ( EBMT ) is a relatively new technology which aims to combine both traditional MT and more recent TM paradigms by reusing previous translations and applying various degrees of linguistic knowledge to convert fuzzy matches into exact ones : see the Wikipedia article on EBMT . However , some early definitions of EBMT refer to what is now known as TM and they often exclude the concept of fuzzy matches . Essentially , Google Translat e begins by examining and comparing massive corpora of texts on the Web that have already been translated by human beings . It looks for matches between source and target texts and uses complex statistical analysis routines to look for statistically significant patterns , i.e. it works out the rules of the interrelationships between source and target texts for itself . As more and more corpora are added to the Web this means that Google Translate will keep improving until it reaches a point where it will be very difficult to tell that a machine has done the translation . I remember early machine translation tools translating \\\" Wie geht es dir ? \\\" as \\\" How goes it you ? \\\" Now Google Translate gets it right : \\\" How are you ? \\\" Thus we have , in a sense , come full circle in that Weaver 's ideas of applying statistical techniques are seen as a fruitful basis for MT . . 3.2 Commercial MT packages . There are many automatic translation packages on the market - as well as free packages on the Web . While such packages may be useful for extracting the gist of a text they should not be seen as a serious replacement for the human translator . Some are not all that bad , producing translations that are half - intelligible , letting you know whether a text is worth having translated properly . See : . Professional human translators are making increasing use of Translation Memory ( TM ) packages . TM packages store texts that have previously been translated , together with their source texts , in a large database . Chunks of new texts to be translated are then matched against the translated texts in the database and suggested translations are offered to the human translator wherever a match is found . The human translator has to intervene regularly in this process of translation , making corrections and amendments as necessary . TM systems can save hours of time ( estimated at up to 80 % of a translator 's time ) , especially when translating texts that are repetitive or that use lots of standard phrases and sentence formulations . Producing updates of technical manuals is a typical application of TM systems . Examples of TM systems include : . An example of automatic translations can be found at the Newstran website . This site is extremely useful for locating newspapers in a wide range of languages . You can also locate selected newspapers that have been translated using a Machine Translation system . Another approach to translation is the stored phrase bank , for example LinguaWrite , which was aimed at the business user and contained a large database of equivalent phrases and sentences in different languages to facilitate the writing of business letters . LinguaWrite was programmed by Marco Bruzzone in the 1980s and marketed by Camsoft , but it is no longer available and has not been updated . David Sephton 's Tick - Tack ( Primrose Publishing ) adopted a similar approach , beginning as a package consisting of \\\" building blocks \\\" of language for business communication , but it now embraces other topics . 3.3 Just for fun . 3.3.1 Some apocryphal stuff . The following examples have often been cited as mistakes made by machine translation ( MT ) systems . Whether they are real examples or not can not be verified . Russian - English : In a technical text that had been translated from Russian into English the term water sheep kept appearing . When the Russian source text was checked it was found that it was actually referring to a hydraulic ram . Russian - English : Idioms are often a problem . Russian - English : Another example , similar to the one above , is where out of sight , out of mind ended up being translated as the equivalent of blind and stupid . MT systems do , however , often make mistakes . The Systran MT system , which has been used by the European Commission , translated the English phrase pregnant women and children into des femmes et enfants enceints , which implies that both the women and the children are pregnant . Although it is an interpretation of the original phrase that is theoretically possible , it is also clearly wrong . 3.3.2 Translations of nursery rhymes . Try using an online machine translator to translate a text from English into another language and then back again . The results are often amusing , especially if you are translating nursery rhymes ! ( i ) Bah , bah , black sheep translated into French and then back again into English , using Babel Fish . English source text : Bah , bah , black sheep , have you any wool ? Yes sir , yes sir , three bags full . One for the master , one for the dame , and one for the little boy who lives down the lane . French translation : Bah , bah , mouton noir , vous ont n'importe quelles laines ? Oui monsieur , oui monsieur , trois sacs compl\\u00e8tement . Un pour le ma\\u00eetre , un pour dame , et un pour le petit gar\\u00e7on qui vit en bas de la ruelle . And back into English again : Bah , bah , black sheep , have you n ' imports which wools ? Yes Sir , yes Sir , three bags completely . For the Master , for lady , and for the little boy who lives in bottom of the lane . ( ii ) Humpty Dumpty translated into Italian and then back again into English , using Babel Fish . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty si \\u00e8 seduto su una parete . Humpty Dumpty ha avuto una grande caduta . I cavalli di tutto il re e gli uomini di tutto il re non hanno potuto un Humpty ancora . And back into English again : Humpty Dumpty has been based on a wall . Humpty Dumpty has had a great fall . The horses of all the king and the men of all the king have not been able a Humpty still . ( iii ) Humpty Dumpty translated into Italian and then back again into English , using Google Translate . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty sedeva su un muro . Humpty Dumpty ha avuto un grande caduta . Tutti i cavalli del re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme . And back into English again : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . All the king 's horses and all the king 's men could not put Humpty together again . Now , the above is an interesting result ! Google Translate used to be a very unreliable MT tool . It drives language teachers mad , as their students often use it to do their homework , e.g. translating from a given text into a foreign language or drafting their own compositions and then translating them . Mistakes made by Google Translate used to be very easy to spot , but ( as indicated above in Section 3.1 ) Google changed its translation engine a few years ago and now uses a Statistical Machine Translation ( SMT ) approach . The Humpty Dumpty translation back into English from the Italian appears to indicate that Google Translate has matched the whole text and got it right . Clever ! 3.3.3 Translations of business and journalistic texts . ( i ) A business text , translated with Google Translate and Babel Fish . Google Translate was used to translate the following text from German into English : Die Handelskammer in Saarbr\\u00fccken hat uns Ihre Anschrift zur Verf\\u00fcgung gestellt . Wir sind ein mittelgro\\u00dfes Fachgesch\\u00e4ft in Stuttgart , und wir spezialisieren uns auf den Verkauf von Personalcomputern . This was rendered as : The Chamber of Commerce in Saarbr\\u00fccken has provided us your address is available . We are a medium sized shop in Stuttgart , and we specialize in sales of personal computers . Babel Fish produced a better version : The Chamber of Commerce in Saarbruecken put your address to us at the disposal . We are a medium sized specialist shop in Stuttgart , and we specialize in the sale of personal computers . ( ii ) A journalistic text , translated with Google Translate and Babel Fish . Die deutsche Exportwirtschaft k\\u00e4mpft mit der weltweiten Konjunkturflaute und muss deshalb von den Zeiten zweistelligen Wachstums Abschied nehmen . [ Ludolf Wartenberg vom Bundesverband der Deutschen Industrie ] . This was rendered by Google Translate as : The German export economy is struggling with the global downturn and must therefore take the times of double - digit growth goodbye . [ Ludolf Wartenberg from the Federation of German Industry ] . The German export trade and industry fights with the world - wide recession and must take therefore from the times of two digit growth parting . [ Ludolf waiting mountain of the Federal association of the German industry . ] Computers are normally associated with two standard input devices , the keyboard and the mouse , and two standard output devices , the display screen and the printer . All these restrict language input and output . However , computer programs and hardware devices that enable the computer to handle human speech are now commonplace . All modern computers allow the user to plug in a microphone and record his / her own voice . A variety of other sources of sound recordings can also be used . Storing these sound files is not a problem anymore as a result of the immensely increased capacity and reduced cost of storage media and improved compression techniques that enable the size of sound files to be substantially reduced . For further information on the applications of sound recording and playback technology to CALL see Module 2.2 , Introduction to multimedia CALL . A range of computer software is available for speech analysis . Spoken input can be analysed according to a wide variety of parameters and the analysis can be represented graphically or numerically . Of course , graphic output is not immediately useful for the uninitiated viewer , and hence we are not arguing that this kind of graphical representation will prove useful to the language learner . On the other hand , specialists are well capable of interpreting this speech analysis data . The information we get from speech analysis has proven very valuable indeed for speech synthesis and speech recognition , which are dealt with in the following two sections . 4.1 Speech synthesis . Speech synthesis describes the process of generating human - like speech by computer . Producing natural sounding speech is a complex process in which one has to consider a range of factors that go beyond just converting characters to sounds because very often there is no one - to - one relation between them . The intonation of particular sentence types and the rhythm of particular utterances also have to be considered . Currently speech synthesis is far more advanced and more robust than speech recognition ( see Section 4.2 below ) . The naturalness of artificially produced utterances is now very impressive compared to what used to be produced by earlier speech synthesis systems in which the intonation and timing were far from natural and resulted in the production of monotonous , robot - like speech . Many people are now unaware that so - called talking dictionaries use speech synthesis software rather than recordings of human voices . In - car satellite navigation ( satnav ) systems can produce a range of different types of human voices , both male and female in a number of different languages , and \\\" talk \\\" to the car driver guiding him / her to a chosen destination . So far , however , speech synthesis has not been as widely used in CALL as speech recognition . This is probably due to the fact that language teachers ' requirements regarding the presentation of spoken language are very demanding . Anything that sounds artificial is likely to be rejected . Some language teachers even reject speakers whose regional accent is too far from what is considered standard or received pronunciation . There is , however , a category of speech synthesis technology known as Text To Speech ( TTS ) technology that is widely used for practical purposes . TTS software falls into the category of assistive technology , which has a vital role in improving accessiblity for a wide range of computer users with special needs , which is now governed by legislation in the UK . The Special Educational Needs and Disability Act ( SENDA ) of 2001 covers educational websites and obliges their designers \\\" to make reasonable adjustments to ensure that people who are disabled are not put at a substantial disadvantage compared to people who are not disabled . \\\" See JISC 's website on Disability Legislation and ICT in Further and Higher Education - Essentials . See the Glossary for definitions of assistive technology and accessiblity . TTS is important in making computers accessible to blind or partially sighted people as it enables them to \\\" read \\\" from the screen . TTS technology can be linked to any written input in a variety of languages , e.g. automatic pronunciation of words from an online dictionary , reading aloud of a text , etc . These are examples of TTS software : . Festival Speech Synthesis System : From the Centre for Speech Technology Research at the University of Edinburgh . Festival offers a general framework for building speech synthesis systems as well as including examples of various modules . Just for fun I entered the phrase \\\" Pas d'elle yeux Rh\\u00f4ne que nous \\\" into a couple of French language synthesisers . It 's a nonsense sentence in French but it comes out sounding like a French person trying to pronounce a well - known expression in English . Try it ! There are also Web - based tools that enable you to create animated cartoons or movies incorporating TTS , for example : . Voki enables you to create and customise your own speaking cartoon character . You can choose the TTS option ( as in Graham Davies 's example on the right ) to give the character a voice , or you can record your own voice . ReadTheWords : A tool that works in much the same way as Voki , but without the option of recording one 's own voice . An excellent tool that helps people with hearing impairments to learn how to articulate is the CSLU Speech Toolkit . To what extent speech synthesis systems are suitable for CALL is a matter for further discussion . See the article by Handley & Hamel ( 2005 ) , who report on their progress towards the development of a benchmark for determining the adequacy of speech synthesis systems for use in CALL . The article mentions a Web - based package called FreeText , for advanced learners of French , the outcome of a project funded by the European Commission . 4.2 Speech recognition . Speech recognition describes the use of computers to recognise spoken words . Speech recognition has not reached such a high level of performance as speech synthesis ( see Section 4.1 above ) , but it has certainly become usable in CALL in recent years . EyeSpeak English is a typical example of the use of speech recognition software for helping students improve their English pronunciation . Speech recognition is a non - trivial task because the same spoken word does not produce entirely the same sound waves when uttered by different people or even when uttered by the same person on different occasions . The process is complex : the computer has to digitise the sound , transform it to discard unneeded information , and then try to match it with words stored in a dictionary . The most efficient speech recognition systems are speaker - dependent , i.e. they are trained to recognise a particular person 's speech and can then distinguish thousands of words uttered by that person . If one remembers that each of the parameters analysed could have been affected by some speaker - independent background noise or by some idiosyncratic pronunciation features of this particular speaker then it already becomes clear how difficult the interpretation of the analysis data is for a speech recognition program . The following information is taken from an article written by Norman Harris of DynEd , a publisher of CALL software incorporating ASR : . Speech recognition technology has finally come of age - at least for language training purposes for young adults and adults . The essence of real language is not in discrete single words - language students need to practice complete phrases and sentences in realistic contexts . Moreover , programs which were trained to accept a speaker 's individual pronunciation quirks were not ideally suited to helping students move toward more standard pronunciation . These technologies also failed if the speaker 's voice changed due to common colds , laryngitis and other throat ailments , rendering them useless until the speaker recovered or retrained the speech engine . The solution to these problems came with the development of continuous speech recognition engines that were speaker independent . These programs are able to deal with complete sentences spoken at a natural pace , not just isolated words . Such flexibility with regard to pronunciation paradigms means that today 's speaker - independent speech recognition programs are not ideal for direct pronunciation practice . Nonetheless , exercises which focus on fluency and word order , and with native speaker models which are heard immediately after a student 's utterance had been successfully recognized , have been shown to indirectly result in much improved pronunciation . Another trade off is that the greater flexibility and leniency which allows these programs to \\\" recognize \\\" sentences spoken by students with a wide variety of accents , also limits the accuracy of the programs , especially for similar sounding words and phrases . Some errors may be accepted as correct . Native speakers testing the \\\" understanding \\\" of programs \\\" tuned \\\" to the needs of non - native speakers may be bothered by this , but most teachers , after careful consideration of the different needs and psychologies of native speakers and learners , will accept the trade off . Students do not expect to be understood every time . If they are required occasionally to repeat a sentence which the program has not recognized or which the program has misinterpreted , there may be some small frustration , but language students are much more likely to take this in their stride than would native speakers . On the other hand , if the program does \\\" understand \\\" such students , however imperfect their pronunciation , they typically experience a huge sense of satisfaction , a feel good factor native speakers simply can not enjoy to anywhere near the same degree . The worst thing for a student is a program that is too demanding of perfection - such programs will quickly lead to student frustration or the kind of embarrassed , hesitant unwillingness to speak English typical of many classrooms . Even if we accept that accuracy needs to be responsive to proficiency in order to encourage students to speak , we must , as teachers , be concerned that errors do not become reinforced . A recent breakthrough is the implementation of apps such as Apple 's Siri on the iPhone 4S and Evi , which is available for the iPhone and the Android . These apps are quite impressive at recognising speech and providing answers to questions submitted by the user . Evi 's performance was tested by the author of this paragraph . \\\" She \\\" immediately provided correct answers to these questions submitted by voice input : . In which American state is Albuquerque ? In addition , Evi may link to relevant websites that provide further information . Text input is also accepted . In this section we outline the essentials of parsing , first of all by describing the components of a parsing system and then discussing different kinds of parser . We look at one linguistic phenomenon which causes problems for parsing and finally examine potential solutions to the difficulties raised by parsing . Put in simple terms , a parser is a program that maps strings of a language into its structures . The most basic components needed by a parser are a lexicon containing words that may be parsed and a grammar , consisting of rules which determine grammatical structures . The first parsers were developed for the analysis of programming languages ; obviously as artificial , regular languages they present fewer problems than a natural language . It is most useful to think of parsing as a search problem which has to be solved . It can be solved using an algorithm which can be defined as : . [ ... ] a formal procedure that always produces a correct or optimal result . An algorithm applies a step - by - step procedure that guarantees a specific outcome or solves a specific problem . The procedure of an algorithm performs a computation in a finite amount of time . Programmers specify the algorithm the program will follow when they develop a conventional program . ( Smith 1990 ) . Parsing algorithms define a procedure that looks for the optimum combination of grammatical rules that generate a tree structure for the input sentence . How might we define these grammatical rules in a concise way that is amenable to computer processing ? A useful construct for our purposes is a so - called context - free grammar ( CFG ) . A CFG consists of rules containing a single symbol on the left - hand side and one or more on the right - hand side . For example , the statement that a sentence can consist of : . a noun phrase and a verb phrase can be expressed by the following rewrite rule . S \\u00ae NP VP . This means that a sentence S can be ' rewritten ' as a noun phrase NP followed by a verb phrase VP which are in their turn defined in the grammar . A noun phrase , for example , can consist of a determiner DET and a noun N. These symbols are known as non - terminals and the words represented by these symbols are terminal symbols . Parsing algorithms can proceed top - down or bottom - up . In some cases , top - down and bottom - up algorithms can be combined . Below are simple descriptions of two parsing strategies . 5.1.1 Top Down ( depth first ) . Top down strategy works from non - terminal symbols : . S \\u00ae NP VP . and then breaks them down into constituents . The strategy assumes we have an S and tries to fit it in . If we choose to search depth first , then we proceed down one side of the tree at a time . The search will end successfully if it manages to break down the sentence into all its terminal symbols ( words ) . 5.1.2 Bottom up ( breadth first ) . A bottom up strategy looks at elements of an S and assigns categories to them to form larger constituents until we arrive at an S. If we choose to search breadth first , then we proceed consecutively through each layer and stop successfully once we have constructed a sentence . Let 's look now at one linguistic phenomenon which causes problems for parsers - that of so - called attachment ambiguity . Consider the following sentence : . The man saw the man in the park with a telescope . Parser output can be represented as a bracketed list or , more commonly , a tree structure . Here is the output of two possible parses for the sentence above . One way of dealing with the problem of sentences which have more than one possible parse is to concentrate on specific elements of the parser input and to not deal with such phenomena as attachment ambiguity . Ideally we expect a parser to successfully analyse a sentence on the basis of its grammar , but often there are problems caused by errors in the text or incompleteness of grammar and lexicon . Also the length of sentences and ambiguity of grammars often make it hard to successfully parse unrestricted text . An approach which addresses some of these issues is partial or shallow parsing . Abney ( 1997:125 ) succinctly describes partial parsing thus : . \\\" Partial parsing techniques aim to recover syntactic information efficiently and reliably from unrestricted text , by sacrificing completeness and depth of analysis . \\\" Partial parsers concentrate on recovering pieces of sentence structure which do not require large amounts of information ( such as lexical association information ) ; attachment remains unresolved for instance . We can see that in this way parsing efficiency is greatly improved . Another strategy for analysing language is part - of - speech tagging , in which we do not seek to find larger structures such as noun phrases but instead label each word in a sentence with its appropriate part of speech . Here is the original paragraph from Section 3 of this document : . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . The following table shows the tagger output , and we can see that most of the words have been correctly identified . additional . languages . NNS . language . required . VBN . require . . . SENT . . . As with partial parsing , we are not trying to find correct attachments and since it is a limited task the success rate is quite high . The information derived from tagging can itself have input into partial parsing or into improving the performance of traditional parsers . Some of the decision task as to what is the correct part of speech to assign to a word is based on the probability of two or three word sequences ( bigrams and trigrams ) occurring , even where words can be assigned more than one part of speech . For instance , in our example tagged text the sequence ' the transfer module ' occurs . Transfer is of course also a verb , but the likelihood of a determiner ( the ) being followed by a verb is lower than the likelihood of a determiner noun sequence . See als o the Visual Interactive Syntax Learning ( VISL ) website . An online parser and a variety of other tools concerned with English grammar , including games and quizzes , can be found here . 5.1.3 Parsing erroneous input . Of course , in CALL we are dealing with texts that have been produced by language learners at various levels of proficiency and accuracy . It is therefore reasonable to assume that the parser has to be prepared to deal with linguistic errors in the input . One thing we could do is to complement our grammar for correct sentences with a grammar of incorrect sentences - an error grammar , i.e. we capture individual and/or typical errors in a separate rule system . The advantage of this error grammar approach is that the feedback can be very specific and is normally fairly reliable because this feedback can be attached to a very specific rule . The big drawback , however , is that individual learner errors have to be anticipated in the sense that each error needs to be covered by an adequate rule . However , as already stated it is not only in texts that have been produced by language learners that we find erroneous structures . Machine translation is facing similar problems . Dina & Malnati review approaches \\\" concerning the design and the implementation of grammars able to deal with ' real input ' . \\\" ( Dina & Malnati 1993:75 ) . They list four approaches : . The rule - based approach which relies on two sets of rules : one for grammatical input and the other for ungrammatical input . Dina & Malnati point out quite rightly that normally well - formedness conditions should be sufficient and the second set of rules results in linguistic redundancy . The main problem with using this approach in a parser - based CALL system is the problem of having to anticipate the errors learners are likely to make . The metarule - based approach uses a set of well - formedness rules and if none of them can be applied calls an algorithm that relaxes some constraints and records the kind of violation . Dina & Malnati note the procedurality of the algorithm causes problems when confronted with multiple errors - something very likely in any text produced by a language learner . The preference - based approach comprises an overgenerating grammar and a set of preference rules . \\\" [ ... ] each time a formal condition is removed from a b - rule to make its applicability context wider , a preference rule must be added to the grammar . Such a p - rule must be able to state - in the present context of the b - rule - the condition that has been previously removed . \\\" ( Dina & Malnati 1993:78 ) Again a source for linguistic redundancy which might result in inconsistencies in the grammar . They claim that , due to the overgeneration of possible interpretations , \\\" the system would be completely unusable in an applied context . \\\" ( ibid.:79 ) . The constraint - based approach is based on the following assumptions : . each ( sub)tree is marked by an index of error ( initially set to 0 ) ; . the violation of a constraint in a rule does not block the application , but increases the error index of the generated ( sub)tree ; . at the end of parsing the object marked by the smallest index is chosen . Consequently , the \\\" most plausible interpretation of a [ ... ] sentence is the one which satisfies the largest number of constraints . \\\" ( Dina & Malnati 1993:80 ) . We have seen in Section 3 that Machine Translation ( MT ) and the political and scientific interest in machine translation played a significant role in the acceptance ( or non - acceptance ) as well as the general development of Human Language Technologies . By 1964 , however , the promise of operational MT systems still seemed distant and the sponsors set up a committee , which recommended in 1966 that funding for MT should be reduced . It brought to an end a decade of intensive MT research activity . ( Hutchins 1986:39 ) . It is then perhaps not surprising that the mid-1960s saw the birth of another discipline : Computer Assisted Language Learning ( CALL ) . The PLATO project , which was initiated at the University of Illinois in 1960 , is widely regarded as the beginning of CALL - although CALL was just part of this huge package of general Computer Assisted Learning ( CAL ) programs running on mainframe computers . PLATO IV ( 1972 ) was probably the version of this project that had the biggest impact on the development of CALL ( Hart 1995 ) . At the same time , another American university , Brigham Young University , received government funding for a CALL project , TICCIT ( Time - Shared Interactive , Computer Controlled Information Television ) ( Jones 1995 ) . Other well - known and still widely used programs were developed soon afterwards : . The CALIS / WinCALIS ( Computer Aided Language Instruction System ) authoring tools , Duke University ( Borchardt 1995 ) . The TUCO package for learners of German , developed by Heimy Taylor and Werner Haas , Ohio State University . See Module 3.2 , Section 5.9 . The CLEF package for learners of French , which was produced by a consortium of Canadian universities in the late 1970s and is still going strong today . See Module 3.2 , Section 5.9 . In the UK , John Higgins developed Storyboard in the early 1980s , a total Cloze text reconstruction program for the microcomputer ( Higgins & Johns 1984:57 ) . ( Levy 1997:24 - 25 ) describes how other programs extended this idea further . See Section 8.3 , Module 1.4 , headed Total text reconstruction : total Cloze , for further information on total Cloze programs . In recent years the development of CALL has been greatly influenced by the technology and by our knowledge of and our expertise in using it , so that not only the design of most CALL software , but also its classification has been technology - driven . For example , Wolff ( 1993:21 ) distinguished five groups of applications : . The late 1980s saw the beginning of attempts which are mostly subsumed under Intelligent CALL ( ICALL ) , a \\\" mix of AI [ Artificial Intelligence ] techniques and CALL \\\" ( Matthews 1992b : i ) . Early AI - based CALL was not without its critics , however : . And that , fundamentally , is why my initial enthusiasm has now turned so sour . ( Last 1989:153 ) . For a more up - to - date and positive point of view of Artifical Intelligence , see Dodigovic ( 2005 ) . Bowerman ( 1993:31 ) notes : \\\" Weischedel et al . ( 1978 ) produced the first ICALL [ Intelligent CALL ] system which dealt with comprehension exercises . It made use of syntactic and semantic knowledge to check students ' answers to comprehension questions . \\\" As far as could be ascertained , this was just the early swallow that did not create a summer . Kr\\u00fcger - Thielmann ( 1992:51ff . ) lists and summarises the following early projects in ICALL : ALICE , ATHENA , BOUWSTEEN & COGO , EPISTLE , ET , LINGER , VP2 , XTRA - TE , Zock . Matthews ( 1993:5 ) identifies Linguistic Theory and Second Language Acquisition Theory as the two main disciplines which inform Intelligent CALL and which are ( or will be ) informed by Intelligent CALL . He adds : \\\" the obvious AI research areas from which ICALL should be able to draw the most insights are Natural Language Processing ( NLP ) and Intelligent Tutoring Systems ( ITS ) \\\" ( Matthews1993:6 ) . Matthews shows that it is possible to \\\" conceive of an ICALL system in terms of the classical ITS architecture \\\" ( ibid . ) The system consists of three modules - expert , student and teacher module - and an interface . The expert module is the one that \\\" houses \\\" the language knowledge of the system . It is this part which can process any piece of text produced by a learner - in an ideal system . This is usually done with the help of a parser of some kind : . ( Holland et al . 1993:28 ) . This notion of parser - based CALL not only captures the nature of the field much better than the somewhat misleading term \\\" Intelligent CALL \\\" ( Is all other CALL un - intelligent ? ) , it also identifies the use of Human Language Technologies as one possible approach in CALL alongside others such as multimedia - based CALL and Web - based CALL and thus identifies parser - based CALL as one possible way forward for CALL . In some cases , the ( technology - defined ) borders between these sub - fields of CALL are not even clearly identifiable , as we will see in some of the projects mentioned in the following paragraphs . To exemplify recent advances in the use of sophisticated human language technology in CALL , let us have a look at some of the projects that were presented at two conferences in the late 1990s . The first one is the Language Teaching and Language Technology conference in Groningen in 1997 ( Jager et al . 1998 ) . Witt & Young ( 1998 ) , on the other hand , are concerned with assessing pronunciation . They implemented and tested a pronunciation scoring algorithm which is based on speech recognition ( see Section 4.2 ) and uses hidden Markov models . \\\" The results show that - at least for this setup with artificially generated pronunciation errors - the GOP [ goodness of pronunciation ] scoring method is a viable assessment tool . \\\" A third paper on pronunciation at this conference , by Skrelin & Volskaja ( 1998 ) outlined the use of speech synthesis ( see Section 4.1 ) in language learning and lists dictation , distinction of homographs , a sound dictionary and pronunciation drills as possible applications . \\\" The project vision foresees two main areas where GLOSSER applications can be used . First , in language learning and second , as a tool for users that have a bit of knowledge of a foreign language , but can not read it easily or reliably \\\" ( Dokter & Nerbonne 1998:88 ) . Dokter & Nerbonne report on the French - Dutch demonstrator running under UNIX . The demonstrator : . uses morphological analysis to provide additional grammatical information on individual words and to simplify dictionary look - up ; . relies on automatic word selection ; . offers the opportunity to insert glosses ( taken form the dictionary look - up ) into the text ; . relies on string - based word sense disambiguation ( \\\" Whenever a lexical context is found in the text that is also provided in the dictionary , the example in the dictionary is highlighted . \\\" ( op.cit.:93 ) . Roosma & Pr\\u00f3sz\\u00e9ky ( 1998 ) draw attention to the fact that GLOSSER works with the following language pairs : English - Estonian - Hungarian , English - Bulgarian , French - Dutch and describe a demonstrator version running under Windows . Dokter et al ( 1998 ) conclude in their user study \\\" that Glosser - RuG improves the ease with which language students can approach a foreign language text \\\" ( Dokter et al . 1998:175 ) . The latter project relies on a spellchecker , morphological analyser , syntactic parser and a lexical database for Basque , and the authors report on the development of an interlanguage model . At another conference ( UMIST , May 1998 ) , which brought together a group of researchers who are exploring the use of HLT in CALL software , Schulze et al . ( 1999 ) and Tschichold ( 1999 ) discussed strategies for improving the success rate of grammar checkers . Menzel & Schr\\u00f6der ( 1999 ) described error diagnosis in a multi - level representation . The demonstration system captures the relations of entities in a simple town scenery . The available syntactic , semantic and pragmatic information is checked simultaneously for constraint violations , i.e. errors made by the language learners . Visser ( 1999 ) introduced CALLex , a program for learning vocabulary based on lexical functions . Diaz de Ilarraza et al . ( 1999 ) described aspects of IDAZKIDE , a learning environment for Spanish learners of Basque . The program contains the following modules : wide - coverage linguistic tools ( lexical database with 65,000 entries ; spell checker ; a word form proposer and a morphological analyser ) , an adaptive user interface and a student modelling system . The model of the students ' language knowledge , i.e. their interlanguage , is based on a corpus analysis ( 300 texts produced by learners of Basque ) . Foucou & K\\u00fcbler ( 1999 ) presented a Web - based environment for teaching technical English to students of computing . Ward et al . ( 1999 ) showed that Natural Language Processing techniques combined with a graphical interface can be used to produce meaningful language games . Davies & Poesio ( 1998 ) reported on tests of simple CALL prototypes that have been created using CSLUrp , a graphical authoring system for the creation of spoken dialogue systems . They argue that since it is evident that today 's dialogue systems are usable in CALL software , it is now possible and necessary to study the integration of corrective feedback in these systems . Mitkov ( 1998 ) outlined early plans for a new CALL project , The Language Learner 's Workbench . It is the aim of this project to incorporate a number of already available HLT tools and to package them for language learners . These examples of CALL applications that make use of Human Language Technologies are by no means exhaustive . They not only illustrate that research in HLT in CALL is vibrant , but also that HLT has an important contribution to make in the further development of CALL . Of course , both disciplines are still rather young and many projects in both areas , CALL and HLT , have not even reached the stage of the implementation of a fully functional prototype yet . A number of CALL packages that make use of speech recognition have reached the commercial market and are being used successfully by learners all over the world ( see Section 4.2 ) . Speech synthesis , certainly at word level , has achieved a clarity of pronunciation that makes it a viable tool for language learning ( see Section 4.1 ) . Many popular electronic dictionaries now incorporate speech synthesis systems . Part - of - speech taggers have reached a level of accuracy that makes them usable in the automatic pre - processing of learner texts . Morphological analysers for a number of languages automatically provide grammatical information on vocabulary items in context and make automatic dictionary look - ups of inflected or derived word forms possible . This progress in HLT and CALL has mainly been possible as the result of our better understanding of the structures of language - our understanding of linguistics . The lack of linguistic modelling and the insufficient deployment of Natural Language Processing techniques has sometimes been given as one reason for the lack of progress in some areas of CALL : see , for example , Levy ( 1997:3 ) , citing Kohn ( 1994 ) . [ ... ] Kohn suggests that current CALL is lacking because of poor linguistic modelling , insufficient deployment of natural language processing techniques , an emphasis on special - purpose rather than general - purpose technology , and a neglect of the ' human ' dimnesion of CALL ( Kohn 1994:32 ) . The examples in the previous section have shown that it is possible to apply certain linguistic theories ( e.g. phonology and morphology ) to Human Language Technologies and implement this technology in CALL software . This is , of course , true . However , it does not mean that interesting fragments or aspects of a given language can not be captured by a formal linguistic theory and hence implemented in a CALL application . In other words , if one can not capture the German language in its entirety in order to implement this linguistic knowledge in a computer program , this does not mean that one can not capture interesting linguistic phenomena of that language . This means even if we are only able to describe a fragment of a given language adequately we can still make very good use of this description in computer applications for language learning . What is the kind of knowledge we ought to have about language before we can attempt to produce an HLT tool that can be put to effective use in CALL ? Let us look at one particular aspect of language - grammar . In recent years , the usefulness of conscious learning of grammar has been discussed time and again , very often in direct opposition to what has been termed \\\" the communicative approach \\\" . ( ibid.:6 ) This assumption leads to the question of what role exactly the computer ( program ) has to play in a sensitive , rich and enjoyable grammar - learning process . The diversity of approaches outlined in this special issue of ReCALL on grammar illustrates that there are many different roads to successful grammar learning that will need to be explored . In this module , only the example of parser - based CALL will be discussed . Let us take a grammar checker for language learners as a specific example in point . This grammar checker could then be integrated into a CALL program , a word - processor , an email editor , a Web page editor etc . The design of such a grammar checker is mainly based on findings in theoretical linguistics and second language acquisition theory . Let us start with second language acquisition theory . Research in second language acquisition has proved that grammar learning can lead to more successful language acquisition . Here learners have the opportunity to correct grammatical errors and mistakes that they have made while concentrating on the subject matter and the communicative function of the text . It is at this stage that a grammar checker for language learners can provide useful and stimulating guidance . In order to ascertain the computational features of such a grammar checker , let us first consider what exactly we mean by \\\" grammar \\\" in a language learning context . Helbig discusses possible answers to this question from the point of view of the teaching and learning of foreign languages in general : . As a starting point for answering our question concerning the relevance ( and necessity ) of grammar in foreign language teaching we used a differentiation of what was and is understood by the term \\\" grammar \\\" : . Grammar A : the system of rules that is inherent to the object language itself and is independent of the fact whether it has been captured by Linguistics or not ; . Grammar B : the scientific - linguistic description of the language inherent system of rules , the modelling of Grammar A by Linguistics ; . Grammar C : the system of rules intern to the speaker and listener which is formed in the head of the learner during language acquisition and which forms the basis for him / her to produce and understand correct sentences and texts and to use them appropriately in communication . ( Helbig:1975 ) . Helbig identifies further a Grammar B1 and a Grammar B2 - the former being a linguistic grammar and the latter being a learner grammar . The description of grammar B1c is a literal translation of Helbig 's wording - in the terminology used now , the term \\\" interlanguage \\\" appears to be the most appropriate . The application of Helbig 's grammar classification to CALL produces the following results : . Grammar A remains as defined by Helbig . In other words it refers to the target grammar of the interlanguage continuum . Grammar B1a is the grammar which enables the parser to process grammatically well - formed sentences in the target language . Grammars B1b , B1c and B2 enable the grammar checking CALL tool to detect errors in the learner input and provide the linguistic information to generate feedback . Grammar C is the grammar system which the CALL tool should help to correct and expand . Additionally , the grammar checker will gather data for learner profiles which should allow useful insights into the development of Grammar C of learners you have used the program . Consequently , Grammar B in its entirety and Grammar C will have to be considered first and foremost when developing the grammar checker . The question then arises : If Grammar A provides the linguistic data for the parser developer , how can we \\\" feed \\\" these different grammars into a computer program ? The computer requires that any grammar which we intend to use in any program ( or programming language , for that matter ) be mathematically exact . Grammars which satisfy this condition are normally referred to as formal grammars . The mathematical description of these grammars uses set theory . Therefore , a language L is said to have a vocabulary V . If there were no restrictions on how to construct strings , the number of possible strings is infinite . This becomes clear when one considers that each vocabulary item of V could be repeated infinitely in order to construct a string . However , as language learners in particular know any language L adheres to a finite set of ( grammar ) rules . This explains why grammar teaching software that attempts to anticipate possible incorrect answers can only do this successfully if the answer domain is severely restricted and the anticipation process will therefore much simpler . Could a computer program perform this task - a task based on infinite possibilities ? Yes , it could - but not based on infinite possibilities . That is why it will be necessary to look for an approach which is based on a finite set of possibilities , which can then be pre - programmed . Let us therefore consider L the set of strings that can be constructed using the ( formal ) grammar G . A formal grammar can be defined as follows ( see e.g. Allen 1995 ): . G ( VN , VT , R , S ) . And here we are already dealing with sets which have a finite number of members . The number of grammatical rules is fairly limited . This is certainly the case when we only consider the basic grammar rules of a language that will have to be learned by the intermediate to early advanced learner . ( Note here what we said earlier about Grammar B2 - the learner grammar : It was only a subset of Grammar 1 - the linguistic grammar . ) Formal grammars have been used in a number of CALL projects . Matthews ( 1993 ) continues his discussion of grammar frameworks for CALL which he started in 1992 ( Matthews 1992a ) . He lists eight major grammar frameworks that have been used in CALL : . Of course , these are only some examples . More recently , Tschichold et al . ( 1994 ) reported on a prototype for correcting English texts produced by French learners . This system relies on a number of different finite state automata for pre - processing , filtering and detecting ( Tschichold et al.1994 ) . Brehony & Ryan ( 1994 ) report on \\\" Francophone Stylistic Grammar Checking ( FSGC ) using Link Grammars \\\" . They adapted the post - processing section of an existing parser so that it would detect stylistic errors in English input produced by French learners . His plea is for the use of the PPT ( Principles and Parameters Theory ( Chomsky 1986 ) as a grammar framework for CALL applications , basing his judgement on three criteria : computational effectiveness , linguistic perspicuity and acquisitional perspicuity ( Matthews 1993:9 ) . In later parts of his paper , Matthews compares rule- and principle - based frameworks using DCGs ( Definite Clause Grammars ) as the example for the latter . He concludes that principle - based frameworks ( and consequently principle - based parsing ) are the most suitable grammar frameworks for what he calls Intelligent CALL . Recently , other unification - based grammar frameworks not included in Matthews ' list have been used in CALL . Hagen , for instance , describes \\\" an object - oriented , unification - based parser called HANOI \\\" ( Hagen 1995 ) which uses formalisms developed in Head - Driven Phrase Structure Grammar ( HPSG ) . He quotes Zajac : . Combining object - oriented approaches to linguistic description with unification - based grammar formalisms [ ... ] is very attractive . On one hand , we gain the advantages of the object - oriented approach : abstraction and generalisation through the use of inheritance . On the other hand , we gain a fully declarative framework , with all the advantages of logical formalisms [ ... ] . Of course , not even this extended list is comprehensive - at best it could be described as indicative of the variety of linguistic approaches used in parser - based Computer Assisted Language Learning and , in particular , in the field of grammar checking . At the end of this short excursion into formal grammar(s ) it can be concluded that any CALL grammar checker component needs as its foundation a formal grammar describing as comprehensively as possible the knowledge we have about the target language grammar . This was the grammar that Helbig ( 1975 ) refers to as Grammar B1a . But what part do the other grammars play in a CALL environment ? Let us stay with the example of a parser - based grammar checker for language learners . Hence , the provision of adequate feedback on the morpho - syntactic structure of parts of the text produced by learners is the most important task for this parser - based grammar checker . Let us therefore consider the place of feedback provision within a parser grammar . In other words , as a good teacher would do - the grammar checker would offer advice on how to change an ungrammatical structure into a corresponding grammatically well - formed structure . As stated earlier this approach would be based on an infinite number of construction possibilities . Therefore , the provision of adequate feedback and help to the learner appears to be difficult if not impossible . However , it has been indicated above that feedback could be linked to the finite sets on which the formal grammar relies . How can this be done ? Each member of the three sets which will have to be considered here . The non - terminal symbols like NP and VP , the words and the set of morpho - syntactic rules carry certain features that determine their behaviour in a sentence and determine their relation to other signs within the sentence . These features which restrict what the text producer can do with a given ( terminal or non - terminal ) symbol in a sentence and under what conditions a particular grammatical rule has to be applied will be labelled constraints . Let us return to our provisional description of feedback , which can now be formulated more precisely . Feedback shows the relation . by explaining the underlying constraint of the anticipated construction in L based on Grammar B2 . to support production of construction in L . and by reasoning about the likely cause of the rule violation . to extend Grammar C - the learner - inherent grammar . And secondly , this above description of feedback given by a grammar checker which is based on a modified parser shows that it is possible to construct tools that support the focus on form by learners during the reflection stage of a text production process . Even if the grammar checker were only to detect a small number of morpho - syntactic errors , this would be beneficial for the learners as long as they were aware of the limitations of this CALL tool . On the other hand , the feedback description contains still a number of question marks in parentheses after some of the important keywords - whether the intended aims of grammar checking can be achieved can only be validated through the use and thorough testing of such a grammar checker . We should better not make any such assumption ( in the scientific sense - we do hope for these improvements , of course ) and better wait until such a parser - based grammar checker is actually tested in a series of proper learning experiments . Let us now leave the discussion of some of the underlying linguistics behind and discuss the role of parser - based applications in language learning . Natural language parsers take written language as their input and produce a formal representation of the syntactic and sometimes semantic structure of this input . The role they have to play in computer - assisted language learning has been under scrutiny in the last decade : ( Matthews 1992a ) ; ( Holland et al . 1993 ) ; ( Nagata 1996 ) . See also Heift ( 2001 ) . Holland et al . discussed the \\\" possibilities and limitations of parser - based language tutors \\\" ( Holland et al . 1993:28 ) . Comparing parser - based CALL to what they label as conventional CALL they come to the conclusion that : . [ ... ] in parser - based CALL the student has relatively free rein and can write a potentially huge variety of sentences . ICALL thus permits practice of production skills , which require recalling and constructing , not just recognising [ as in conventional CALL ] , words and structures . ( Holland et al.1993:31 ) . However , at the same time , parsing imposes certain limitations . Parsers tend to concentrate on the syntax of the textual input , thus \\\" ICALL may actually subvert a principal goal of language pedagogy , that of communicating meanings rather than producing the right forms \\\" ( Holland et al.1993:32 ) . This disadvantage can be avoided by a \\\" focus on form \\\" which is mainly achieved by putting the parser / grammar checker to use within a relevant , authentic communicative task and at a time chosen by and convenient to the learner / text producer . Juozulynas ( 1994 ) evaluated the potential usefulness of syntactic parsers in error diagnosis . He analysed errors in an approximately 400 page corpus of German essays by American college students in second - year language courses . His study shows that : . [ ... ] syntax is the most problematic area , followed by morphology . ( Juozulynas 1994:5 ) . Juozulynas adapted a taxonomic schema by Hendrickson which comprises four categories : syntax , morphology , orthography , lexicon . Juozulynas ' argument for splitting orthography into spelling and punctuation is easily justified in the context of syntactic parsing . Parts of punctuation can be described by using syntactic bracketing rules , and punctuation errors can consequently be dealt with by a syntactic parser . Lexical and spelling errors form , according to Juozulynas , a rather small part of the overall number of learner errors . Some of these errors will , of course , be identified during dictionary look - up , but if words that are in the dictionary are used in a nonsensical way , the parser will not recognise them unless specific error rules ( e.g. for false friends ) are built in . Consequently , a parser - based CALL application can play a useful role in detecting many of the morpho - syntactic errors which constitute a high percentage of learner errors in freely produced texts . Nevertheless , the fact remains : . A second limitation of ICALL is that parsers are not foolproof . Because no parser today can accurately analyse all the syntax of a language , false acceptance and false alarms are inevitable . ( Holland et al . 1993:33 ) . This is something not only developers of parser - based CALL , but also language learners using such software have to take into account . In other words , this limitation of parser - based CALL has to be taken into consideration during the design and implementation process and when integrating this kind of CALL software in the learning process . A final limitation of ICALL is the cost of developing NLP systems . By comparison with simple CALL , NLP development depends on computational linguists and advanced programmers as well as on extensive resources for building and testing grammars . Beyond this , instructional shells and lessons must be built around NLP , incurring the same expense as developing shells and lessons for CALL . ( Holland et al . 1993:33 ) . It is mainly this disadvantage of parser - based CALL that explains the lack of commercially available ( and commercially viable ) CALL applications which make good use of HLT . However , it is to be hoped that this hurdle can be overcome in the not too distant future because sufficient expertise in the area has accumulated over recent years . More and more computer programs make good use of this technology , and many of these have already \\\" entered \\\" the realm of computer - assisted language learning , as can be seen from the examples in the previous section . Holland et al . ( 1993 ) answer their title question \\\" What are parsers good for ? \\\" on the basis of their own experience with BRIDGE , a parser - based CALL program for American military personnel learning German , and on the basis of some initial testing with a small group of experienced learners of German . They present the following points : . ICALL appears to be good for form - focused instruction offering learners the chance to work on their own linguistic errors by this method , not only to improve their performance in the foreign language but also to improve their language awareness . ICALL appears to be good for selected kinds of students . The authors list the following characteristics which might influence the degree of usefulness of ICALL for certain students : . i. intermediate proficiency . ii . analytical orientation . iii . tolerance of ambiguity . iv . confidence as learners . ICALL is good for research because the parser automatically tracks whole sentence responses and detects , classifies , and records errors . It might facilitate the assessment of the students grammatical competency and thus help us discover patterns of acquisition . ICALL [ ... ] can play a role in communicative practice . The authors argue for the embedding of parser - based CALL in \\\" graphics microworlds \\\" which help to capture some basic semantics . Given that we would like to harness the advantages of parser - based CALL , how do we take the limitations into consideration in the process of designing and implementing a parser - based CALL system ? What implications does the use of parsing technology have for human - computer interaction ? [ I]n discourse analytic terms ( Grice 1975 ) , the nature of the contract between student and CALL tutor is straightforward , respecting the traditional assumption that the teacher is right , whereas the ICALL contract is less well defined . ( Holland et al . 1993:33f . ) . The rigidity of traditional CALL in which the program controls the linguistic input by the learner to a very large extent has often given rise to a criticism of CALL which accuses CALL developers and practitioners of relying on behaviourist programmed instruction . If one wants to give learners full control over their linguistic input , for example , by relying on parsing technology , what are then the terms according to which the communicative interaction of computer ( program ) and learner can be defined ? The differences between humans and machines have obviously to be taken into consideration in order to understand the interaction of learners with CALL programs : . Machines are compiled out of individual parts for a very specific purpose ( totum fix et partibus ) ; whereas humans are holistic entities whose parts can be differentiated ( partes fiunt ex toto ) . Humans process all sorts of experiences and repeatedly and interactively create their own environment - something machines can not do . They \\\" calculate \\\" a problem on the basis of pre - wired rules . The main features of human thoughts are the inherent contradictions and the ability to cope with them , something that will not be calculable due to its complexity , variety and degree of detail ( Schmitz 1992:209f . ) . These differences between humans and machines can for our purposes , i.e. the theoretical description of human - computer interaction , be legitimately reduced to the distinction between actions and operations as is done in Activity Theory . The main point of this theory for our consideration here is that communicative activities can be divided into actions which are intentional , i.e. goal - driven ; and these can be sub - divided into operations which are condition - triggered . These operations are normally learnt as actions . In the example referred to in the quotation above , the gear - switching is learnt as an action . The learner - driver is asked by the driving instructor to change gear and this becomes the goal of the learner . Once the learner - driver has performed this action a sufficient number of times , this action becomes more and more automated and in the process loses more and more of its intentionality . A proficient driver might have the goal to accelerate the car which will necessitate switching into higher gear , but this is now triggered by a condition ( the difference between engine speed and speed of the car ) . It can thus be argued that humans learn to perform complex actions by learning to perform certain operations in a certain order . Machines , on the other hand , are only made to perform certain ( sometimes rather complex ) operations . This has some bearing on our understanding of the human - computer interaction that takes place when a learner uses language - learning software . When , for instance , the spell checker is started in a word - processing package , the software certainly does not have ' proof - read ' the learner 's document . The computer just responds to the clicking of the spellchecker menu item and performs the operation of checking the strings in the document against the entries in a machine dictionary . For the computer user ( in our case a learner ) , it might look like the computer is proof - reading the document . Normally , one only realises that no \\\" proper \\\" document checking is going on if a correctly spelled word is not found in the dictionary or nonsensical alternatives are given for a simple spelling error . Person X interacts with Person Y in that he observes Person Y 's action , reasons about the likely intention for that action and reacts according to this assumed intention . This , for example , explains why many learners get just as frustrated when an answer they believe to be right is rejected by the computer as they would get if it were rejected by their tutor . Of course , an ideal computer - assisted language learning system would avoid such pitfalls and not reject a correct response or overlook an incorrect one . Since any existing system can only approximate to this ideal , researchers and developers in parser - based CALL can only attempt to build systems that can perform complex structured sequences of ( linguistic ) operations so that learners can interact meaningfully and successfully with the computer . Grammatica is able to identify parts of speech in English and French with a fair degree of accuracy and show , for example , how verbs are conjugated in different tenses and how plurals of nouns are formed . Abeill\\u00e9 A. ( 1992 ) \\\" A lexicalised tree adjoining grammar for French and its relevance to language teaching \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Abney S. ( 1997 ) \\\" Part - of - speech tagging and partial parsing \\\" . In Young S. & Bloothooft G. ( eds . ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Allen J. ( 1995 ) Natural language understanding , New York : John Benjamins Publishing Company . Alwang G. ( 1999 ) \\\" Speech recognition \\\" , PC Magazine , 10 November 1999 . Antos G. ( 1982 ) Grundlagen einer Theorie des Formulierens . Textherstellung in geschriebener und gesprochener Sprache , T\\u00fcbingen : Niemeyer . Arnold D. , Balkan . L , Meijer S. , Humphreys R. L. & Sadler L. ( 1994 ) Machine Translation : an introductory guide , Manchester : NEC Blackwell . Bellos D. ( 2011 ) Is that a fish in your ear ? Translation and the meaning of everything , Harlow : Penguin / Particular Books . Bennett P. ( 1997 ) Feature - based approaches to grammar , Manchester : UMIST , Unpublished Manuscript . Bennett P. & Paggio P. ( eds . ) ( 1993 ) Preference in EUROTRA , Luxembourg : European Commission . Bloothooft G. , Dommelen W. , van Espain C. , Green P. , Hazan V. & Wigforss E. ( eds . ) ( 1997 ) The landscape of future education in speech communication sciences : ( 1 ) analysis , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bloothooft G. , van Dommelen W. , Espain C. , Hazan V. , Huckvale M. & Wigforss E. ( eds . ) ( 1998 ) The landscape of future education in speech communication sciences : ( 2 ) proposals , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bolt P. & Yazdani M. ( 1998 ) \\\" The evolution of a grammar - checking program : LINGER to ISCA \\\" , CALL 11 , 1 : 55 - 112 . Borchardt F. ( 1995 ) \\\" Language and computing at Duke University : or , Virtue Triumphant , for the time being \\\" , CALICO Journal 12 , 4 : 57 - 83 . Bowerman C. ( 1993 ) Intelligent computer - aided language learning . LICE : a system to support undergraduates writing in German , Manchester : UMIST , Unpublished doctoral dissertation . Brehony T. & Ryan K. ( 1994 ) \\\" Francophone stylistic grammar checking ( FSGC ) : using link grammars \\\" , CALL 7 , 3 : 257 - 269 . Brocklebank P. ( 1998 ) An experiment in developing a prototype intelligent teaching system from a parser written in Prolog , Manchester , UMIST , Unpublished MPhil dissertation . Brown P.F. , Della Pietra S.A. , Della Pietra V.J. & Mercer R.L. ( 1993 ) \\\" The mathematics of statistical Machine Translation : parameter estimation \\\" , Computational Linguistics 19 , 2 : 263 - 311 . Buchmann B. ( 1987 ) \\\" Early history of Machine Translation \\\" . In King M. ( ed . ) Machine Translation today : the state of the art , Edinburgh : University Press . Bull S. ( 1994 ) \\\" Learning languages : implications for student modelling in ICALL \\\" , ReCALL 6 , 1 : 34 - 39 . Bureau Lingua / DELTA ( 1993 ) Foreign language learning and the use of new technologies , Brussels : European Commission . Cameron K. ( ed . ) ( 1989 ) Computer Assisted Language Learning , Oxford : Intellect . Carson - Berndsen J. ( 1998 ) \\\" Computational autosegmental phonology in pronunciation teaching \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Chanier D. , Pengelly M. , Twidale M. & Self J. ( 1992 ) \\\" Conceptual modelling in error analysis in computer - assisted language learning \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Chen L. & Barry L. ( 1989 ) \\\" XTRA - TE : Using natural language processing software to develop an ITS for language learning \\\" . In Fourth International Conference on Artificial Intelligence and Education : 54 - 70 . Chomsky N. ( 1986 ) Knowledge of language : its nature , origin , and use , New York : Praeger . CLEF ( Computer - assisted Learning Exercises for French ) ( 1985 ) Developed by the CLEF Group , Canada , including authors at the University of Guelph , the University of Calgary and the University of Western Ontario . Also published by Cambridge University Press , 1998 : ISBN 0 - 521 - 59277 - 1 . Curzon L. B. ( 1985 ) Teaching in further education : an outline of principles and practice . London : Holt , Rinehart & Winston , ( 3rd edition ) . Davies G. ( 1988 ) \\\" CALL software development \\\" . In Jung Udo O.H .. ( ed . ) Computers in applied linguistics and language learning : a CALL handbook , Frankfurt : Peter Lang . Davies G. ( 1996 ) Total - text reconstruction programs : a brief history , Maidenhead : Camsoft . Davies S. & Poesio M. ( 1998 ) \\\" The provision of corrective feedback in a spoken dialogue system \\\" . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) ( 1991 ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Diaz de Ilarranza A. , Maritxalar M. & Oronoz M. ( 1998 ) \\\" Reusability of language technology in support of corpus studies in an ICALL environment \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Diaz de Ilarranza A. , Maritxalar A. , Maritxalar M. & Oronoz M. ( 1999 ) \\\" IDAZKIDE : An intelligent computer - assisted language learning environment for Second Language Acquisition \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 12 - 19 . Dodigovic M. ( 2005 ) Artificial intelligence in second language learning : raising error awarenes s , Clevedon : Multilingual Matters . Dokter D. & Nerbonne J. ( 1998 ) \\\" A session with Glosser - RuG \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Dokter D. , Nerbonne J. , Schurcks - Grozeva L. & Smit P. ( 1998 ) \\\" Glosser - RuG : a user study \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Ehsani F. & Knodt E. ( 1998 ) \\\" Speech technology in computer - aided language learning : strengths and limitations of a new CALL paradigm \\\" , Language Learning and Technology 2 , 1 : 45 - 60 . Ellis R. ( 1994 ) The study of Second Language Acquisition , Oxford : OUP . European Commission ( 1996 ) Language and technology : from the Tower of Babel to the Global Village , Luxembourg : European Commission . ISBN 92 - 827 - 6974 - 7 . Fechner J. ( ed . ) ( 1994 ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Feuerman K. , Marshall C. , Newman D. & Rypa M. ( 1987 ) \\\" The CALLE project \\\" , CALICO Journal 4 : 25 - 34 . Foucou P - Y. & K\\u00fcbler N. ( 1999 ) \\\" A Web - based language learning environment : general architecture \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 31 - 39 . Fum D. , Pani B. & Tasso C. ( 1992 ) \\\" Native vs. formal grammars : a case for integration in the design of a foreign language tutor \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hagen L.K. ( 1995 ) \\\" Unification - based parsing applications for intelligent foreign language tutoring systems , CALICO Journal 12 , 2 - 3 : 5 - 31 . Hamilton S. ( 1998 ) \\\" A CALL user study \\\" . In Jager S. , Nerbonne J. & van Essen A.(eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Handke J. ( 1992 ) \\\" WIZDOM : a multiple - purpose language tutoring system based on AI techniques \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hart R. ( 1995 ) \\\" The Illinois PLATO foreign languages project \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 15 - 37 . Heift T. ( 2001 ) \\\" Error - specific and individualised feedback in a Web - based language tutoring system : Do they read it ? \\\" ReCALL 13 , 1 : 99 - 109 . Heift T. & Schulze M. ( eds . ) ( 2003 ) Error diagnosis and error correction in CALL , CALICO Journal Special Issue 20 , 3 . Heift T. & Schulze M. ( 2007 ) Errors and intelligence in CALL : parsers and pedagogues , London and New York : Routledge . Helbig G. ( 1975 ) \\\" Bemerkungen zum Problem von Grammatik und Fremdsprachenunterricht \\\" , Deutsch als Fremdsprache 6 , 12 : 325 - 332 . Higgins J. & Johns T. ( 1984 ) Computers in language learning , London : Collins . Holland M. , Maisano R. , Alderks C. & Martin J. ( 1993 ) \\\" Parsers in tutors : what are they good for ? \\\" CALICO Journal 11 , 1 : 28 - 46 . Holland M. & Fisher F.P. ( eds . ) ( 2007 ) T he path of speech technologies in Computer Assisted Language Learning : from research toward practice , London and New York : Routledge . Hutchins W.J. ( 1986 ) Machine Translation : past , present , future , Chichester : Ellis Horwood . Hutchins W.J. ( 1997 ) \\\" Fifty years of the computer and translation \\\" , Machine Translation Review 6 , October 1997 : 22 - 24 . Hutchins W.J. & Somers H.L. ( 1992 ) An introduction to Machine Translation , London : Academic Press . Jager S. ( 2001 ) \\\" From gap - filling to filling the gap : a re - assessment of Natural Language Processing in CALL \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Jager S. , Nerbonne J. & van Essen A. ( eds . ) ( 1998 ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Jones R. ( 1995 ) \\\" TICCIT and CLIPS : The early years \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 84 - 96 . Jung Udo O.H. & Vanderplank R. ( eds . ) ( 1994 ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Juozulynas V. ( 1994 ) \\\" Errors in the composition of second - year German students : an empirical study of parser - based ICALI \\\" , CALICO Journal 12 , 1 : 5 - 17 . King M. ( ed . ) ( 1987 ) Machine Translation today : the state of the art , Edinburgh : Edinburgh University Press . Klein W. & Dittmar N. ( 1979 ) Developing grammars : the acquisition of German syntax by foreign workers , Heidelberg : Springer . Klein W. & Perdue C. ( 1992 ) Utterance structure ( developing grammars again ) , Amsterdam : John Benjamins Publishing Company . Klein W. ( 1986 ) Second language acquisition , Cambridge : Cambridge University Press . Kohn K. ( 1994 ) \\\" Distributive language learning in a computer - based multilingual communication environment \\\" . In Jung Udo O.H. & Vanderplank R. ( eds . ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Krashen S. ( 1981 ) Second language acquisition and second language learning , Oxford : Pergamon . Krashen S. ( 1982 ) Principles and practice in second language acquisition , Oxford : Pergamon . Kr\\u00fcger - Thielmann K. ( 1992 ) Wissensbasierte Sprachlernsysteme . Neue M\\u00f6glichkeiten f\\u00fcr den computergest\\u00fctzten Sprachunterricht , T\\u00fcbingen : Gunter Narr . Labrie G. & Singh L. ( 1991 ) \\\" Parsing , error diagnosis and instruction in a French tutor \\\" , CALICO Journal 9 : 9 - 25 . Last R. ( 1989 ) Artificial Intelligence techniques in language learning , Chichester : Ellis Horwood . Last R. ( 1992 ) \\\" Computers and language learning : past , present - and future ? \\\" In Butler C. ( ed . ) Computers and written texts , Oxford : Blackwell . Levin L. , Evans D. & Gates D. ( 1991 ) \\\" The Alice system : a workbench for learning and using language \\\" , CALICO Journal 9 : 27 - 55 . Levy M. ( 1997 ) Computer - assisted language learning : context and conceptualisation , Oxford : Oxford University Press . Lightbown P.M. & Spada N. ( 1993 ) How languages are learned , Oxford : Oxford University Press . Long M. ( 1991 ) \\\" Focus on form : a design feature in language teaching methodology \\\" . In de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Manning C. & Sch\\u00fctze H. ( 1999 ) Foundations of statistical Natural Language Processing , Cambridge MA , MIT Press . Matthews C. ( 1992a ) \\\" Going AI : foundations of ICALL \\\" , CALL 5 , 1 - 2 : 13 - 31 . Matthews C. ( 1992b ) Intelligent CALL ( ICALL ) bibliography , Hull : University of Hull , CTI Centre for Modern Languages . Matthews C. ( 1993 ) \\\" Grammar frameworks in Intelligent CALL \\\" , CALICO Journal 11 , 1 : 5 - 27 . Matthews C. ( 1994 ) \\\" Intelligent Computer Assisted Language Learning as cognitive science : the choice of syntactic frameworks for language tutoring \\\" , Journal of Artificial Intelligence in Education 5 , 4 : 533 - 56 . Menzel W. & Schr\\u00f6der I. ( 1999 ) \\\" Error diagnosis for language learning systems \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 20 - 30 . Michel G. ( ed . ) ( 1985 ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . Mitkov R. ( 1998 ) Language Learner 's Workbench . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . Mitkov R. & Nicolov N. ( eds . ) ( 1997 ) Recent advances in Natural Language Processing . Amsterdam : John Benjamins Publishing Company . Murphy M. , Kr\\u00fcger A. & Griesz A. , ( 1998 ) \\\" RECALL \\\" -towards a knowledge - based approach to CALL . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nagata N. ( 1996 ) \\\" Computer vs. workbook instruction in second language acquisition \\\" , CALICO Journal 14 , 1 : 53 - 75 . Nerbonne J. , Jager S. & van Essen A. ( 1998 ) Introduction . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nirenburg S. ( ed . ) ( 1986 ) Machine Translation : theoretical and methodological issues , Cambridge : Cambridge University Press . Pijls F. , Daelmans W. & Kempen G. ( 1987 ) \\\" Artificial intelligence tools for grammar and spelling instruction , Instructional Science 16 : 319 - 336 . Pollard C. & Sag I.A. ( 1987 ) Information - Based Syntax and Semantics , Chicago : University Press . Pollard C. & Sag I.A. ( 1994 ) Head - Driven Phrase Structure Grammar , Chicago : University Press . Ramsay A. & Sch\\u00e4ler R. ( 1997 ) \\\" Case and word order in English and German \\\" . In Mitkov R. & Nicolov N. ( eds . ) Recent advances in Natural Language Processing , Amsterdam : John Benjamins Publshing Company : 15 - 34 . Ramsay A. & Schulze M. ( 1999 ) \\\" Die Struktur deutscher Lexeme \\\" , German Linguistic and Cultural Studies , Peter Lang , Submitted Manuscript . Reifler E. ( 1958 ) \\\" The Machine Translation project at the University of Washington , Seattle , Washington , USA \\\" . In Proceedings of the Eighth International Congress of Linguists , Oslo University Press : 514 - 518 . Roosmaa T. & Pr\\u00f3sz\\u00e9ky G. ( 1998 ) \\\" GLOSSER - using language technology tools for reading texts in a foreign language \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Salaberry R. ( 1996 ) \\\" A theoretical foundation for the development of pedagogical tasks in computer mediated communication \\\" , CALICO Journal 14 , 1 : 5 - 34 . Sanders R. ( 1991 ) \\\" Error analysis in purely syntactic parsing of free input : the example of German \\\" , CALICO Journal 9 , 1 : 72 - 89 . Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 . Schmitz U. ( 1992 ) Computerlinguistik , Opladen : Westdeutscher Verlag . Schulze M. ( 1997 ) \\\" Textana - text production in a hypertext environment \\\" , CALL 10 , 1 : 71 - 82 . Schulze M. ( 1998 ) \\\" Teaching grammar - learning grammar . Aspects of Second Language Acquisition in CALL \\\" , CALL 11 , 2 : 215 - 228 . Schulze M. ( 2001 ) \\\" Human Language Technologies in Computer Assisted Language Learning \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Schulze M. , Hamel M - J. & Thompson J. ( eds . ) ( 1999 ) Language processing in CALL , ReCALL Special Issue . Schumann J.H. & Stenson N. ( eds . ) ( 1975 ) New frontiers in second language learning , Rowley : Newbury House . Schwind C. ( 1990 ) \\\" An intelligent language tutoring system \\\" , International Journal of Man - Machine Studies 33 : 557 - 579 . Selinker L. ( 1992 ) Rediscovering interlanguage , London : Longman . Skrelin P. & Volskaja N. ( 1998 ) \\\" The application of new technologies in the development of education programs \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Smith R. ( 1990 ) Dictionary of Artificial Intelligence , London : Collins . Sp\\u00e4th P. ( 1994 ) \\\" Hypertext und Expertensysteme i m Sprachunterricht \\\" . In Fechner J. ( ed . ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Stenzel B. ( ed . ) ( 1985 ) Computergest\\u00fctzter Fremdsprachenunterricht . Ein Handbuch , Berlin : Langenscheidt . Swartz M. & Yazdani M. ( eds . ) ( 1992 ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Taylor H. ( 1987 ) TUCO II . Published by Gessler Educational Software , New York . Based on earlier programs developed by Taylor H. & Haas W. at Ohio State University in the 1970s : DECU ( Deutscher Computerunterricht ) and TUCO ( Tutorial Computer ) . Taylor H. ( 1998 ) Computer assisted text production : feedback on grammatical errors made by learners of English as a Foreign Language , Manchester : UMIST , MSc Dissertation . Tschichold C. ( 1999 ) \\\" Intelligent grammar checking for CALL \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 5 - 11 . Tschichold C. , Bodme F. , Cornu E. , Grosjean F. , Grosjean L. , K\\u00fcbler N. & Tschumi C. ( 1994 ) \\\" Detecting and correcting errors in second language texts \\\" , CALL 7 , 2 : 151 - 160 . Visser H. ( 1999 ) \\\" CALLex ( Computer - Aided Learning of Lexical functions ) - a CALL game to study lexical relationships based on a semantic database \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 50 - 56 . Ward R. , Foot R. & Rostron A.B. ( 1999 ) \\\" Language processing in computer - assisted language learning : language with a purpose \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 40 - 49 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . Reproduced in Locke W.N. & Booth A.D. ( eds . ) ( 1955 ) Machine translation of languages : fourteen essays , Cambridge , Mass : Technology Press of the Massachusetts Institute of Technology : 15 - 23 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . See \\\" 50th anniversary of machine translation \\\" , MT News International , Issue 22 ( Vol . 8 , 1 ) , July 1999 : 5 - 6 . Weischedel R. , Voge W. & James M. ( 1978 ) \\\" An artificial intelligence approach to language instruction \\\" , Artificial Intelligence 10 : 225 - 240 . Wilks Y. & Farwell D. ( 1992 ) \\\" Building an intelligent second language tutoring system from whatever bits you happen to have lying around \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag , . Whitelock P.J. & Kilby K. ( 1995 ) Linguistic and computational techniques in Machine Translation system design , London : University College Press . Witt S. & Young S. ( 1998 ) \\\" Computer - assisted pronunciation teaching based on automatic speech recognition \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Wolff D. ( 1993 ) \\\" New technologies for foreign language teaching \\\" . In Foreign language learning and the use of new technologies , Bureau Lingua / DELTA , Brussels , European Commission . Young S. & Bloothooft G. ( eds . ) ( 1997 ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Z\\u00e4hner C. ( 1991 ) \\\" Word grammars in ICALL \\\" . In Savolainen H. & Telenius J. ( eds . ) EuroCALL 91 proceedings , Helsinki : Helsinki School of Economics . Zech J. ( 1985 ) \\\" Methodische Probleme einer t\\u00e4tigkeitsorientierten Ausbildung des sprachlich - kommunikativen K\\u00f6nnens \\\" . In Michel G. ( ed . ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . CALICO ( Computer Assisted Language Instruction Consortium ) : CALICO is a professional association devoted to promoting the use of technology enhanced language learning . CALICO 's sister association in Europe is EUROCALL . EUROCALL : EUROCALL is a professional association devoted to promoting the use of technology enhanced language learning , based at the University of Ulster , Northern Ireland . EUROCALL 's sister association in the USA is CALICO . ICALL is an interdisciplinary research field integrating insights from computational linguistics and artificial intelligence into computer - aided language learning . Such integration is needed for CALL systems to be able to analyze language automatically , to make them aware of language as such . This makes it possible to provide individualized feedback to learners working on exercises , to ( semi-)automatically prepare or enhance texts for learners , and to automatically create and use detailed learner models . See NLP SIG , the Special Interest Group within EUROCALL , with which ICALL closely collaborates . InSTIL : The name of a now defunct Special Interest Group dedicated to Integrating Speech Technology in Language Learning , which was set up within the EUROCALL and CALICO professional associations . A good deal of the work undertaken by InSTIL has now been taken over by ICALL and NLP SIG . NLP SIG : The name of the Special Interest Group for Natural Language Processing within the EUROCALL professional association . See ICALL , the Special Interest Group within CALICO , with which NLP SIG closely collaborates . Virtual Linguistics Campus : It includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Document last updated 29 April 2012 . This page is maintained by Graham Davies . Please cite this Web page as : Gupta P. & Schulze M. ( 2012 ) Human Language Technologies ( HLT ) . Module 3.5 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \"}",
        "_version_":1692580922897989632,
        "score":20.861294},
      {
        "id":"f16f649b-a89e-43cf-b0d8-017e5364bfd6",
        "_src_":"{\"url\": \"http://bulbapedia.bulbagarden.net/w/index.php?title=Talk:Future_Sight_(move)&diff=prev&oldid=1544802\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158481.37/warc/CC-MAIN-20160205193918-00277-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"6.864 Advanced Natural Language Processing . This course is a graduate introduction to natural language processing - the study of human language from a computational perspective . It covers syntactic , semantic and discourse processing models , emphasizing machine learning or corpus - based methods and algorithms . It also covers applications of these methods and models in syntactic parsing , information extraction , statistical machine translation , dialogue systems , and summarization . The subject qualifies as an Artificial Intelligence and ... More . This course is a graduate introduction to natural language processing - the study of human language from a computational perspective . It covers syntactic , semantic and discourse processing models , emphasizing machine learning or corpus - based methods and algorithms . It also covers applications of these methods and models in syntactic parsing , information extraction , statistical machine translation , dialogue systems , and summarization . The subject qualifies as an Artificial Intelligence and Applications concentration subject . Pick a Bookmark Collection or Course ePortfolio to put this material in or scroll to the bottom to create a new Bookmark Collection . Name the Bookmark Collection to represent the materials you will add . Describe the Bookmark Collection so other MERLOT users will know what it contains and if it has value for their work or teaching . Other users can copy your Bookmark Collection to their own profile and modify it to save time . Submitting Bookmarks ... . Select this link to open drop down to add material 6.864 Advanced Natural Language Processing to your Bookmark Collection or Course ePortfolio . Select this link to close drop down of your Bookmark Collection or Course ePortfolio for material 6.864 Advanced Natural Language Processing . Select this link to open drop down to add material 6.864 Advanced Natural Language Processing to your Bookmark Collection or Course ePortfolio \"}",
        "_version_":1692670070427222016,
        "score":20.783295},
      {
        "id":"e326311c-beec-42a9-b2a8-0360d587a93c",
        "_src_":"{\"url\": \"http://www.walkoffwalk.com/2010/07/angry-old-hall-of-famer-now-ha.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166141.55/warc/CC-MAIN-20160205193926-00223-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Natural Language Processing in Textual Information Retrieval and Related Topics . Mari Vallez ; Rafael Pedraza - Jimenez . Citaci\\u00f3n recomendada : Mari Vallez ; Rafael Pedraza - Jimenez . Natural Language Processing in Textual Information Retrieval and Related Topics [ en linea]. \\\" Hipertext.net \\\" , num . \\\" Natural Language Processing \\\" ( NLP ) as a discipline has been developing for many years . It was formed in 1960 as a sub - field of Artificial Intelligence and Linguistics , with the aim of studying problems in the automatic generation and understanding of natural language . At first its methods were widely accepted and successful . However , when applied in controlled environments and with a generic vocabulary , many problems arose . Among those problems were polysemy and synonymy . In recent years contributions to this field have improved substantially , allowing for the processing of huge amounts of textual information with an acceptable level of efficacy . An example of this is the application of these techniques as an essential component in web search engines , in automated translation tools or in summary generators [ Baeza - Yates , 2004]. Problems with natural language processing : linguistic variation and ambiguity . Natural language , understood as a tool that people use to express themselves , has specific properties that reduce the efficacy of textual information retrieval systems . These properties are linguistic variation and ambiguity . By linguistic variation we mean the possibility of using different words or expressions to communicate the same idea . Linguistic ambiguity is when a word or phrase allows for more than one interpretation . Both phenomenons affect the information retrieval process , even though in different ways . Linguistic variation provokes document silence , that is , the omission of relevant documents that fulfil information needs , because the same terms were not used as those found in the document . Ambiguity , on the other hand , implies document noise , or the inclusion of non - meaningful documents , since documents were retrieved that used the same term but with a different meaning . These characteristics make automated language processing considerably difficult . The following is a set of examples that show the repercussions of these phenomena in information retrieval : . At a morphological level , the same word may play different morph - syntactic roles relative to the context in which they appear , causing ambiguity problems ( example 1 ) . Example 1 . A notebook was the present that his wife gave him when all of us were present at the party . In this case , the word \\\" present \\\" acts both as an adjective and noun , and with different meanings . At a syntactic level , focusing on the study of established relations between words to form larger linguistic units , phrases and sentences , ambiguities are produced as a consequence of the possibility of associating a sentence with more than one syntactic structure . On the other hand , this variation supposes the possibility of expressing the same idea , but changing the order of the sentence 's syntactic structure . ( example 2 ) . Example 2 . He ate the chocolates on the plane . This example could mean that \\\" He ate the chocolates that were in the plane \\\" or that \\\" He ate the chocolates when he was flying in the plane . \\\" At a semantic level we study the meaning of a word and sentence by studying the meaning of each of the words in it . Ambiguity is produced because a word can have one or various meanings , which is known as polysemy ( example 3 ) . Example 3 . Paul was reading a newspaper in the bank . The term \\\" bank \\\" could refer to a financial institution or a mound . And we must also keep in mind lexical variation which refers to the possibility of using different terms for the same meaning , that is , a synonymy ( example 4 ) : . Example 4 : Car / Auto / Automobile . At a pragmatic level , based on a language 's relationship to its context , we often can not use a literal and automated interpretation of the terms used . In specific circumstances , the sense of the words in the sentence must be interpreted at a level that includes the context in which the sentence is found . ( example 5 ) . Example 5 . Give me a break . Here we are asking for rest from work , or we could be asking the perceiver to leave us alone . Another important topic is ambiguity provoked by an anaphora , for example , the presence of pronouns and adverbs that refer to something that was previously mentioned ( example 6 ) . Example 6 . It was terrible for him she had not to manipulate it . Who is he ? And she ? What was not manipulated ? It is impossible to understand this sentence out of context . All of these examples demonstrate the complexity of language , and that any automated processing is not easy or obvious . Natural Language processing in textual information retrieval . As the reader has probably already deduced , the complexity associated with natural language is especially key when retrieving textual information [ Baeza - Yates , 1999 ] to satisfy a user 's information needs . In other words , a textual information retrieval system carries out the following tasks in response to a user 's query ( image 1 ) : . Indexing the collection of documents : in this phase , NLP techniques are applied to generate an index containing document descriptions . Normally each document is described through a set of terms that , in theory , best represents its content . When a user formulates a query , the system analyses it , and if necessary , transforms it with the hope of representing the user 's information needs in the same way as the document content is represented . The system compares the description of each document with that of the query , and presents the user with those documents whose descriptions are closest to the query description . The results are usually listed in order of relevancy , that is , by the level of similarity between the document and query descriptions . As of now there are no NLP techniques that allow us to extract a document 's or query 's meaning without any mistakes . In fact , the scientific community is divided on the procedure to follow in reaching this goal . In the following section we will explain the functions and peculiarities of the two key approaches to natural language processing : a statistical approach and a linguistic focus . Both proposals differ considerably , even though in practice natural language processing systems use a mixed approach , combining techniques from both focuses . Statistical processing of natural language . Statistical processing of natural language [ Manning , 1999 ] represents the classical model of information retrieval systems , and is characterised from each document 's set of key words , known as the terms index . This is a very simple focus based on the \\\" bag of words . \\\" In this approach , all words in a document are treated as its index terms . Moreover , each term is assigned a weight in function of its importance , usually determined by its appearance frequency within the document . This way the word 's order , structure , meaning , etc , are not taken into consideration . These models are then limited to pairing the documents ' words with that of the query 's . Its simplicity and efficacy has become the most commonly used contemporary models in textual information retrieval systems . This document processing model involves the following stages : . a)Document pre - processing : fundamentally consisting in preparing the documents for its parameterisation , eliminating any elements considered as superfluous . b)Parameterisation : a stage of minimal complexity once the relevant terms have been identified . This consists in quantifying the document 's characteristics ( that is , the terms ) . Below we will illustrate their function using this paper 's first paragraph as an example , assuming that it is XML tagged . So the document on which we would apply the pre - processed and parameterisation techniques would be the following : . ( Example 6 ) . Stemming terms is a linguistic process that attempts to determine the base ( lemma ) of each word in a text . Its aim is to reduce a word to its root , so that the key words in a query or document are represented by their roots instead of the original words . The lemma of word is its basic form along with its inflected forms . For example , \\\" inform \\\" could be the lemma of \\\" information \\\" or \\\" inform . \\\" However , these algorithms have the -inconvenience of sometimes not grouping words that should be grouped , and vice versa : erroneously presenting words as equals . Parametrising documents consists in assigning a weight to each one of the relevant terms associated to a document . A term 's weight is usually calculated as a function of its appearance frequency in the document , indicating the importance of these terms as the document 's content description ( example 8) . Example 8 . Fragment of a parametrised document ( see how the frequencies of each term changes as the quantification of the remaining terms in the document continues ) . One of the most often used methods to estimate the importance of a term is the TF.IDF system ( Term Frequency , Inverse Document Frequency ) . It is designed to calculate the importance of a term relative to its appearance frequency in a document , but as a function of the total appearance frequency for all of the corpus ' documents . That is , the fact that a term appears often in one document is indicative that that term is representative of the content , but only when that term does not appear frequently in all documents . If it appeared frequently in all documents , it would not have any discriminatory value ( for example , it would be absurd to represent the content of a document in a recipe database by the frequency of the word food , even though it appears often ) . Finally , and as we have already mentioned , we must describe two commonly used techniques in the statistical processing of natural language : . a ) Detecting N - Grams : this consists in identifying words that are usually together ( compound words , proper nouns , etc . ) to be able to process them as a single conceptual unit . This is usually done by estimating the probability of two words that are often together make up a single term ( compound ) . These techniques attempt to identify compound terms such as \\\" accommodation service \\\" or \\\" European Union . \\\" b ) Stopwords lists : a list of empty words in a terms list ( prepositions , determiners , pronouns , etc . ) considered to have little semantic value , and are eliminated when found in document , leaving them out of the terms index to be analysed . Deleting all of these terms avoids document noise problems and saves on resources , since in documents few elements are repeated frequently . Linguistic processing of natural language . This approach is based on the application of different techniques and rules that explicitly encode linguistic knowledge [ Sanderson , 2000]. The documents are analysed through different linguistic levels ( as previously mentioned ) by linguistic tools that incorporate each level 's own annotations to the text . Below we show the different steps to take in a linguistic analysis of documents , even though not all systems use them . The morphological analysis is performed by taggers that assign each word to a grammatical category according to the morphological characteristics found . After having identified and analysed the words in a text , the next step is to see how they are related and used together in making larger grammatical units , phrases and sentences . Therefore a syntax analysis of the text is performed . This is when parsers are applied : descriptive formalism that demonstrate the text 's syntax structure . The techniques used to apply and create parsers vary and depend on the aim of the syntax analysis . For information retrieval it is often used for a superficial analysis aiming to only identify the most meaningful structures : nominal sentences , verbal and prepositional sentence , values , etc . This level of analysis is usually used to optimise resources and not slow down the system 's response . From the text 's syntax structure , the next aim is to obtain the meaning of the sentences within it . The aim is to obtain the sentence 's semantic representation from the elements that make it up . One of the most often used tools in semantic processing is the lexicographic database WordNet . This is an annotated semantic lexicon in different languages made up of synonym groups called synsets which provide short definitions along with the different semantic relationships between synonym groups . There are different fields of research relative to information retrieval and natural language processing that focus on the problem from other perspectives , but whose final aim is to facilitate information access . Information extraction consists in extracting entities , events and existing relationships between elements in a text or group of texts . This is one way of efficiently accessing large documents since it extracts parts of the document shown in its content . The information generated can be used as knowledge and ontology databases . Summary generators compress a text 's most relevant information . The techniques most often used vary according to the rate of compression , the summary 's aim , the text 's genre and language ( or languages ) of the original text , among other factors . Question answering aims to give a specific response to the formulated query . The information needs must be well - defined : dates , places , etc . Here the processing of natural language attempts to identify the type of response to provide ( by disambiguating the question , analysing the set restrictions , and the use of information extraction techniques . These systems are considered to be the potential successors to the current information retrieval systems . START natural language system is an example of one of these systems . Retrieving multi - language information involves the possibility of retrieving information even though the question and/or documents are in different languages . Automatic translators are used on the documents and/or questions , or the use of interlingua mechanisms to interpret documents . These systems are still a great challenge to researches since they combine two key aspects of the Web 's current context : retrieving information and processing multilingual information . Finally , we must cite the automatic text classification techniques , which automatically assign a set of documents into categories within predefined classifications . The correct description of the document 's characteristics ( usually through the use of statistical techniques -- pre - processed and parametrisation ) strongly influences the quality of the grouping / categorization by these techniques . Conclusions . With the aim of understanding the current Natural Language Process , we have concisely defined the key concepts and techniques associated with this field , along with some simple examples to help the reader better understand . Moreover , we have shown how , despite its years of experience , NLP is a very live and developing field of linguistics , with its many challenges still to overcome due to natural language 's ambiguity . We have paid special attention to the differences between statistical and linguistic methods in natural language processing . Even the scientific communities that support each approach are usually at odds , and NLP is often applied by using a combination of techniques from both approaches . Our experience in this field has made us conclude that it is not possible to claim one approach is better than the other ; this even includes the use of a mixed approach . Relative to information retrieval , the statistical processing techniques are more often used in commercial applications . However , in our opinion , the behaviour and efficacy of the different NLP techniques vary depending on the nature of the task at hand , the type of documents to analyse and the computational cost to assume . Overall , we can deduce the need to continue working on this with the hope of creating new techniques or focuses that will help us overcome these existing shortcomings . This is the only way we can finally reach what seems like the impossible dream of automatic comprehension of natural language . Finally , and as an Annexe ( Annexe 1 ) , we have described some of the unique aspects of processing Spanish , including the mention of some of the key initiatives developed to process this language . Acknowledgments . This project has been partially financed by the Ministerio de Educaci\\u00f3n y Ciencia ( Spain ) as part of the HUM2004 - 03162/FILO project . References . [ Allan , 1995 ] J. Allan [ et al.]. Recent experiments with INQUERY , in : HARMAN , D.K. from : The Fourth Text Retrieval Conference , NIST SP 500 - 236 , Gaithersburg , Maryland . [ Baeza - Yates , 1999 ] Baeza - Yates , R. and Ribeiro - Neto , Berthier . Modern information retrieval . Addison - Wesley Longman . [ Baeza - Yates , 2004 ] Baeza - Yates , R. ( 2004 ) . Challenges in the Interaction of Information Retrieval and Natural Language Processing . in Proc . 5 th International Conference on Computational Linguistics and Intelligent Text Processing ( CICLing 2004 ) , Seoul , Corea . Lecture Notes in Computer Science vol . 2945 , pages 445 - 456 , Springer . [ Carmona , 1998 ] J. Carmona [ et al.]. An environment for morphosyntactic processing of unrestricted spanish text . In : LREC 98 : Procedings of the First International Conference on Language Resources and Evaluation , Granada , Espa\\u00f1a . [ Figuerola , 2000 ] C. G. Figuerola . La investigaci\\u00f3n sobre recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol . In : C.Gonzalo Garc\\u00eda and V. Garc\\u00eda Yedra , editors , Documentaci\\u00f3n , Terminolog\\u00eda y Traducci\\u00f3n , pages 73 - 82 . S\\u00edntesis , Madrid . [ Figuerola , 2004 ] C. G. Figuerola [ et al.]. La recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol y la normalizaci\\u00f3n de t\\u00e9rminos , in : Revista Iberoamericana de Inteligencia Artificial , vol VIII , n\\u00ba 22 , pp . 135 - 145 . [ Manning , 1999 ] Manning , C. D. and Sch\\u00fctze , H. ( 1999 ) . Foundations of statistical natural language processing . MIT Press . Cambridge , MA : May , p. 680 . [ Rodr\\u00edguez , 1996 ] S. Rodr\\u00edguez y J. Carretero . A formal approach to Spanish morphology : the COES tools . En : XII Congreso de la SEPLN , Sevilla , pp . 118 - 126 . [ Sanderson , 2000 ] Sanderson , M. ( 2000 ) . Retrieving with good sense , In : Information Retrieval , 2 , 49 - 69 . [ Santana , 1997 ] O. Santana [ et al.]. Flexionador y lematizador autom\\u00e1tico de formas verbales , In : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XIX(2 ) , pp . 229 - 282 . [ Santana , 1999 ] O. Santana [ et al.]. Flexionador y lematizador de formas nominales , en : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XXI(2 ) , pp . 253 - 297 . [ Strzalkowski , 1999 ] Strzalkowski , T. ( 1999 ) . Natural Language Information Retrieval . Netherlands : Kluwer Academic Publishers . As an annexe we will go on to show some of the most important characteristics of natural language processing in Spanish : . Empty words lists [ Figuerola , 2000 ] : creating these tools for Spanish is quite a challenge , mainly due to the lack of collections and statistical studies in Spanish that would advise for or against its use . Furthermore , creating these lists varies in function of whether or not they are used in processing general or specific information . If our corpus is not field specific , then the list of empty words should mainly include : determiners , pronouns , adverbs , prepositions and conjunctions . But if the information to be analysed is field specific , this list should be modified and/or extended by an expert of the field in question . We must also mention that many researches have noted the advantage of using fixed expressions as elements in empty words lists . Specifically [ Allan , 1995 ] recommends using a short list of \\\" empty phrases \\\" : indication of , which are , how are , information on . Stemming techniques : the majority of information retrieval techniques use frequency counts of terms found in the documents and queries . This implies the need to standardise these terms in order for the count to be carried out properly , taking into consideration those terms with the same lemma or root . There are various lemma and morphological analysers for Spanish . Finally , it is worth noting that experiments of this kind of algorithms in Spanish has shown that standardising terms through stemming techniques provides improved results . The S - stemmer algorithm comes up with surprising results . This algorithm is very simple and basically it simply reduces plural words to their singular form . In its original version ( for English ) , this algorithm only eliminates the last \\\" s \\\" of each word . For Spanish , this algorithm could be reinforced by including plural forms of nouns and adjectives with consonants , thus ending in \\\" es . \\\" Eliminating the \\\" es \\\" suffixes could produce inconsistencies with words ending in \\\" e \\\" in their singular form , which would require the elimination of \\\" e \\\" endings . We have also shown that eliminating gender specific \\\" a \\\" or \\\" o \\\" endings improves results . The greatest advantage of this algorithm is its simplicity . However , the inconvenience is that the S - stemmer is incapable of distinguishing nouns and adjectives from other grammatical categories , thus applying it to all words ; it also does not distinguish between irregular plural forms . But on the other hand , by treating all words in the same form , it does not introduce additional noise . University Pompeu Fabra . Department of Communication . DigiDoc Research Group Campus de la Comunicaci\\u00f3 . Roc Boronat , 138 , office 53804 . Barcelona 08018 Tel : 93 542 13 11 . E - mail : cristofol.rovira@upf.edu Legal deposit B-49106 - 2002 - ISSN 1695 - 5498 \"}",
        "_version_":1692669059566403584,
        "score":20.108679},
      {
        "id":"9e40f1f0-4308-4ccb-9b33-f7513a6a6abe",
        "_src_":"{\"url\": \"http://www.shareable.net/blog/differentiating-the-sharing-economy-from-the-anarchy-economy\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168065.93/warc/CC-MAIN-20160205193928-00322-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Affiliated with . Abstract . Background . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system . It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources , integrating diverse text processing applications , and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Evaluations were performed on each system , resulting in F - scores ranging from .26 - .72 ( precision .39 - .85 , recall .16 - .85 ) . Additionally , each of these systems was run over all abstracts in MEDLINE , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . Conclusion . OpenDMAP advances the performance standards for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . The output of the information extraction system is always constructed from elements of an ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction . Electronic supplementary material . The online version of this article ( doi : 10 . 1186/\\u200b1471 - 2105 - 9 - 78 ) contains supplementary material , which is available to authorized users . Background . Conceptual analysis is the process of mapping from natural language texts to a formal representation of the objects and predicates ( together , the concepts ) meant by the text . The history of attempts to build programs to do conceptual analysis dates back to at least 1967 [ 1 ] . Recent advances in the availability of high quality ontologies , in the ability to accurately recognize named entities in texts , and in language processing methods generally have made possible a significant advance in concept analysis , arguably the most difficult and general natural language processing task . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system that significantly advances the state of the art . We also discuss its application to three important information extraction tasks in molecular biology . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . In a recent PLoS Biology essay Rebholz - Schuhmann [ 2 ] argued , \\\" It is only a matter of time and effort before we are able to extract facts [ from articles in the primary literature ] automatically . The consequences are likely to be profound . \\\" Existing examples include extraction of information about gene - gene interactions [ 3 ] , alternative splicing [ 4 ] , functional analysis of mutations [ 5 ] , phosphorylation sites [ 6 ] , and regulatory sites [ 7 ] . The primary significance of OpenDMAP to these efforts is that it leverages the large - scale efforts being made in biomedical ontology development , such as the Open Biomedical Ontologies Foundry ( OBO Foundry ) [ 8 ] . Logical representations of reality , such as those built on the OBO Foundry , use a set of predicates that formally describe properties of , or relationships among , objects . Predicates are defined with a specific number and type of admissible arguments . For example , the predicate expresses might be specified to take two arguments , a gene and a cell type , meaning that the specified gene is expressed in all normal cells of the specified type . Such predicates can also be related to each other through abstraction ( \\\" is a \\\" ) and packaging ( \\\" part of \\\" ) hierarchies , as done in the OBO Foundry . The semantics defined by the predicates and hierarchies in such ontologies provide a powerful tool for natural language processing . Independently constructed ontologies have played at best a modest role in prior natural language processing systems . Other language processing systems have used either small , ad hoc conceptual representations developed specifically for the application , or structured linguistic resources , such as WordNet [ 11 ] , which do not meet the logical requirements for an ontology . While the implementation reported below exploits only a small portion of the OBO Foundry , and the crucial Relationship Ontology component of the Foundry is still in an early stage of development , the organizing principles of OpenDMAP generalize straightforwardly . These systems and their extensions have been used to extract semantic relationships relevant to pharmacogenomics [ 16 ] and to compare alternative sources of information [ 17 ] , among other applications . OpenDMAP is like MetaMap and its descendents in that it can only produce output drawn from a predefined semantic representation . The main difference is that MetaMap , SemRep and SemGen are structured as traditional NLP systems , with a lexicon that enumerates possible concepts that might be associated with a word or phrase . Multiple possible mappings are returned , with rankings . OpenDMAP provides an alternative method of organizing knowledge about language , so that each concept has associated with it a set of patterns that describe how that concept can be realized in language ; there is no explicit lexicon . To appreciate the differences between OpenDMAP and previous work in biomedical text mining , it is also useful to contrast its handling of syntactic structure and of semantic content with other systems . At one end of the spectrum are systems that employ essentially asyntactic representations . Early in the modern period of genomic natural language processing , some such systems were able to achieve significant ( and in some cases ground - breaking ) results using techniques based on text literals only . These include [ 18 - 20 ] . One line of subsequent work has attempted to increase the coverage of these early systems , which utilized manually - built patterns , by automatically acquiring considerably larger sets of patterns - see , for example , Huang et al . 2004 [ 21 ] . Another line of subsequent work has focused on adding a modest , but still useful , level of linguistic abstraction by explicitly including either lexical categories ( parts of speech ) , word stems , or both [ 22 , 23 ] . These systems were essentially agrammatical ; in contrast , OpenDMAP utilizes a classic form of \\\" semantic grammar , \\\" freely mixing text literals , semantically typed basal syntactic constituents , and semantically defined classes of entities . Although OpenDMAP is capable of utilizing full syntactic parses , the patterns for the three separate tasks discussed in this paper utilize primarily shallow syntactic parses ( the development phase of the transport project reports results using syntactic dependency information ) . It remains to be seen what depth of syntactic parsing is useful in biomedical text mining . All of the systems discussed thus far have in common the fact that they employ some notion of explicit patterns , be they agrammatical , syntactic , or semantic . In a separate line of work , patterns are entirely implicit - that is , they exist only to the extent that they are captured by orthogonal features . This work approaches relation extraction as a classification problem ; a classic example is the work of Craven and Kumlein 1999 [ 32 ] . Bunescu et al . 2005 [ 33 ] presents a detailed analysis of a number of classification - based approaches ; the state of the art is characterized by the participants in the recent BioCreative protein - protein interaction shared task [ 34 ] . OpenDMAP has been applied in three domains : protein transport , protein - protein interaction and the expression of a gene in a particular cell type . The three application domains are independently significant . Protein transport , the directed movement of proteins from one cellular compartment to another , is a broadly important biological phenomenon . Although protein subcellular localization information is centralized ( e.g. through ontological annotations at NCBI and in various model organism databases ) , information about transport is not . Protein transport information is published throughout the scientific literature , but no previous method was able to capture it systematically . Protein - protein interaction extraction has been the subject of dozens of systems ( see , e.g. a review in [ 35 ] ) . Widely used web resources such as IHOP [ 3 ] and Chilibot [ 36 ] are based entirely on automated extraction of protein - protein interactions from text . This task was used in the BioCreative community evaluation , described below . The protein transport task is illustrative of another distinguishing aspect of the OpenDMAP approach : it provides mechanisms for handling relationships involving more than two entities . Note that the protein transport predicate has at least three arguments : what protein is transported , from where , and to where ( our model also includes a fourth argument : the transporting protein ) . Although some linguistic expressions of the concept may elide an argument , the predicate itself inherently describes a greater than binary relationship . Wattarujeekrit et al . [ 38 ] and Cohen and Hunter [ 39 ] present evidence that many important predicates in biomedicine require more than two arguments . However , most previous efforts at extracting relationships from biomedical text have addressed exclusively binary relationships . Geneways [ 40 ] and RLMPS - P [ 41 ] are the only other biomedical IE systems of which we are aware that extracted greater than binary relationships , and neither is ontology - driven . Assessing the accuracy of an information extraction system is a very labor - intensive activity . In order to identify information that could have been extracted , but was not ( a \\\" false negative \\\" ) , a person must go through a large volume of text to determine all of the relevant assertions . To estimate the reliability of these manually derived assertions , at least two people must complete that task to assess inter - rater reliability . Once such data is used for one evaluation and system developers have seen it , further use of the data will generate upwardly biased accuracy estimates as system developers fit their systems to it . For these reasons , large - scale community evaluations of information extraction systems are particularly important . The second Critical Assessment of Information Extraction in Biology , ( BioCreative ) [ 34 , 42 ] , community evaluation included a test of systems designed to extract human protein - protein interaction information from the full texts of hundreds of journal articles , called the IPS task . Human curators from the IntAct database [ 43 ] manually extracted interaction assertions from these articles using the same curatorial standards as for the database . The results produced by human experts were compared to the results submitted from 45 systems developed by laboratories around the world , providing the best current assessment of the accuracy of protein interaction information extraction systems . The performance of OpenDMAP on the protein interaction task was evaluated as part of this shared task . More limited evaluations of the accuracy in the other applications are also reported in the results section . The accuracy of an information extraction system depends on the genre of texts on which it operates [ 44 ] . This report demonstrates the application of OpenDMAP to full texts of scientific journal articles , to Medline abstracts , and to GeneRIFs ( single sentences or sentence fragments that are selected by human curators for relevance to the function of a particular gene product ) . GeneRIFs are particularly attractive targets for information extraction , due to their roughly sentential length ( identified by [ 44 ] as the optimum ) , breadth of coverage , manual preselection for relevance , and association with at least one normalized gene reference . Despite these attractive features , this is the first report of an information extraction system targeting them . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Each of these systems was run over all abstracts in Medline as of June 18 , 2007 , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . These results are provided in RDF format in the Additional Files 1 , 2 , 3 , 4 . One particularly striking result is the diversity of journals from which these assertions were mined . The transport relationships were extracted from 2,340 different journals ; the interaction relationships from 4,103 different journals ; and the expression relationships from 2,984 different journals . A total of 4,434 unique journals contributed to these results , nearly 40 % of the journals indexed in Medline each year ( see Figure 1 ) . OpenDMAP coverage of MEDLINE . The gray bars indicate the number of journals indexed by MEDLINE each year . The red bars indicate the number of journal abstracts from which OpenDMAP extracted at least one assertion regarding transport , interaction or expression . In recent years , more than 40 % of biomedical journals contain such information . 2007 is partial data ( through July 1 ) . For the BioCreative evaluation , the interaction system was run on the full texts of all of the 359 articles in the test set , producing 385 interaction assertions . Performance was averaged per article , since a few articles had a very large number of interactions and would have dominated a per assertion calculation . OpenDMAP 's average F - measure of 0.29 was 10 % higher than the next best scoring system , and more than three standard deviations above the mean performance . OpenDMAP 's recall was similar to the other high scoring systems ; its advantage arose from being substantially more precise ( fewer false positives ) , achieving an average precision of 0.39 , more than 20 % better than the next best system . Due to IntAct 's curation criteria , which require clear experimental evidence for an interaction in the text , these results are quite conservative . Many \\\" false positives \\\" were in fact assertions of interactions , but fell short of the evidential requirements for IntAct curation . A manual evaluation of the performance of the protein transport recognition system was based on all 570 GeneRIFs containing a form of the word \\\" translocate \\\" ( 382 of which were about protein transport , and 188 were about the transport of something else ) . Since transport is a greater than binary relationship , the extraction was only counted as correct if all of the components extracted matched the human annotation . For that strict criterion , OpenDMAP achieved precision of 0.75 and a recall of 0.49 ( F - score of 0.59 ) . If incomplete extractions are counted as correct , precision is unchanged at 0.75 and recall rises to 0.67 ( F - score of 0.71 ) . A substantial proportion of the errors were due to imperfect recognition of proteins ; if OpenDMAP is given correct protein identifications as inputs , precision is 0.77 , strict recall is 0.67 ( F - score of 0.72 ) and incomplete recall is 0.85 ( F - score of 0.81 ) . A manual evaluation of the performance of the expression recognition system was based on 324 GeneRIFs containing a form of the word \\\" express , \\\" ( these sentences contained 469 assertions about expression , 205 of which were about gene expression in 178 different cell types ) . Open DMAP had a precision of 0.64 , but missed many statements that annotators identified as expression assertions , achieving a recall of only 0.16 ( F - score of 0.26 ) . A substantial portion of these errors were due to imperfect recognition of gene names ; if OpenDMAP is given correct gene identifications as input , precision is 0.85 and recall is 0.36 ( F - score of 0.51 ) . Many other failures to identify expression assertions were related to coordination ; the test set had an average of more than two expression assertions per sentence , but the IE system extracted only about 1.3 assertions per sentence . Discussion . As demonstrated by its performance in the community evaluation , OpenDMAP advances the state of the art for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . There are several reasons why OpenDMAP exhibits better performance than any other biomedical information extraction system to date . OpenDMAP is an extension of the Direct Memory Access Parsing ( DMAP ) paradigm described in [ 45 ] and [ 46 ] . Three innovations distinguish the present work from those prior efforts . First , the ontology component of OpenDMAP is independent of the rest of the system . The knowledge representation component is the well - established , open source Prot\\u00e9g\\u00e9 ontology development system [ 47 , 48 ] , and OpenDMAP concept analyzers can be associated with any ontology compatible with Prot\\u00e9g\\u00e9 , for example , the OBO Foundry . Second , OpenDMAP is fully integrated with the open source Unstructured Information Management Architecture , ( UIMA ) [ 49 - 51 ] , which allows the results of any text processing application interfaced to UIMA to be exploited by the OpenDMAP system . As demonstrated below , this mechanism facilitates the use of many external language processing systems , including tokenizers , sentence boundary detectors , entity recognition systems , and syntactic parsers . Since the inputs and outputs of each system are mapped by UIMA to a common annotation structure accessed by OpenDMAP , the use , comparison and combination of various approaches to language processing can all be fully integrated into OpenDMAP patterns . The third innovation in the OpenDMAP system is an expanded pattern language for specifying how concepts can be expressed in text . The intimate connection between the ontology and the natural language processing system provides two significant advantages over prior information extraction systems generally . First , the output of the information extraction system is always constructed from elements of the ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . In contrast , the outputs of most natural language processing systems are grounded only in substrings of text , not normalized to any model at all . Progress in normalizing biological entities recognized in text to specific database identifiers [ 52 - 54 ] has made the output of text processing systems much more valuable . Mapping the properties and relationships extracted to a community ontology similarly provides a significant increment in the value of the output from text processing systems . The second advantage of the OpenDMAP approach is that all of the knowledge used by the system to recognize concepts is structured by the ontology . In contrast , the nearly universal alternative approach is to embody knowledge of language into a lexicon , which associates individual lexical items with their possible semantic interpretations . In the OpenDMAP approach , information about which concepts are potentially relevant to the analysis of a particular text passage straightforwardly places limits on the linguistic knowledge relevant to analyzing that passage . This approach finesses many difficult ambiguity resolution problems faced by lexicon - driven systems , since these limits on the knowledge applied to conceptual analysis prevent many multiple interpretation problems from arising at all . For example , the string \\\" hunk \\\" refers to a cell type ( human natural killer cells ) , a gene ( hormonally upregulated Neu - associated kinase ) , and the general English word meaning a large piece of something without definite shape . A traditional , lexicon - driven system would have an explicit method for assigning the correct word sense to any occurrence of the string \\\" hunk . \\\" The fact that there might be possible alternative interpretations of the matching string has no consequence , and no explicit ambiguity resolution step is necessary . Ambiguity is a leading cause of errors in text processing systems , and this approach is one of the contributing factors to OpenDMAP 's superior performance . Our top - down approach to restricting possible interpretations does not address all problems due to ambiguity in language ; for example , errors in preprocessing systems ( e.g. syntactic parsing , see below ) are not effected . The use of UIMA greatly facilitates the incorporation of various applications as input to OpenDMAP . The outputs of NLP tools integrated into the system are described by the extensible UIMA type system . In the case that a new type of information is produced by a preprocessor , OpenDMAP patterns would have to be modified to take advantage of the new type of information available . For example , the first time an external cell type tagging system is added , the UIMA type of the result of that processor must be linked to a cell - type concept in an OpenDMAP ontology in order for it to be used in patterns . However , if a new NLP tool produces a UIMA output type that has been used by OpenDMAP previously , then no changes in the ontology or patterns are needed . We believe that the outputs of information extraction systems are not likely to useful until the F - score ( or at least the precision ) is greater than about 0.85 [ 34 ] , so the various sources of error in these systems must be addressed . A significant cause of errors in the OpenDMAP system as evaluated is incorrect identification of gene and protein names . The UIMA architecture makes it trivial to adopt and exploit better gene / protein recognition systems as they are developed . Use of such a system should improve the performance of OpenDMAP . Error analysis of the false positives in the transport data set indicates that more than 80 % are due to errors in the syntactic analysis . For example , in the sentence \\\" Rho protein regulates the tyrosine phosphorylation of FAK through translocation from the nucleus to the membrane , \\\" the subject of the translocation was incorrectly identified as FAK ( rather than Rho ) by the Stanford parser . That parser was developed for general English rather than biomedical text , so using specialized syntactic analysis systems may improve the precision of OpenDMAP . Remaining problems in false positives are due to problematic tokenization , failures to properly resolve anaphoric reference , and , rarely , negation . False negatives are due to gaps in concept recognition patterns , more than half of which arise from a failure to properly handle coordinated clauses and conjunctions . Addressing these issues remains an open area of research . Another issue was that the Stanford parser was too slow to use in the application of the transport system to all of Medline , so it was n't run . OpenDMAP ignores aspects of patterns that require inputs that are n't present , so the patterns that contained syntactic dependencies did not have to be altered . These syntactic constraints are important for accuracy , however . Tested on the gold standard set for the system without the parser precision drops to 0.62 , while strict recall remains largely unchanged , rising to 0.51 . Conclusion . Despite OpenDMAP elevating the state of the art for biomedical information extraction significantly beyond previous levels , error rates remain high . In the most challenging BioCreative task , finding curatable assertions in full text documents , only about 29 % of the relevant assertions were found , and only about 39 % of the extracted assertions were completely correct . Such error rates mean that automatically generated databases can not replace manual curation efforts . However , the evidence is quite clear that manual curation can not keep up with the rate of data generation [ 56 ] . The surprisingly large number of journals that contained information relevant to these three IE tasks suggests that the temporal approach taken in [ 56 ] may actually underestimate the severity of the problem . Although the outputs produced by large - scale IE systems are not yet suitable for producing factual databases for direct use by biomedical researchers , the current level of performance provides two important facilities to the research community . First , the results of these efforts can be used to significantly increase the efficiency of manual curation efforts . Each extracted assertion is tied to a specific text , which can be used to direct the attention of manual curators both to relevant documents and to specific relevant passages within a document . Effective integration of IE results into curatorial workflows will require the development of new tools . OpenDMAP developers are working with curators at IntAct to address these issues . The open source availability of OpenDMAP will facilitate the work of others addressing this issue as well . The second important use of the sorts of results that IE systems are currently able to generate is in statistical integration with multiple sources of noisy data , such as those described in [ 57 ] and [ 58 ] . As demonstrated in the latter , the proper addition of even noisy data from the literature substantially improves the quality and coverage of protein - protein interaction networks for several species . Methods . OpenDMAP uses Prot\\u00e9g\\u00e9 [ 47 ] to provide an object model for the possible concepts ( predicates and objects ) that might be found in a text . Prot\\u00e9g\\u00e9 models concepts ( including actions ) as classes that participate in abstraction and packaging hierarchies , and relationships as class - specific slots . For example , protein transport is modeled as a class ( called PROTEIN - TRANSPORT ) and the relationship between a transport event and the protein transported in that event is represented as a slot in that class ( called [ TRANSPORTED - ENTITY ] ) . Slots can take on values , which can be constrained to be instances of other classes . For example , the [ TRANSPORTED - ENTITY ] slot of the PROTEIN - TRANSPORT class is constrained to be an instance of either of the classes PROTEIN or MOLECULAR - COMPLEX . Figure 2 shows a portion of the model used for the transport , which includes biological entities , such as molecular complexes and cellular components , and biological processes , particularly protein transport . This model is drawn almost entirely from the Gene Ontology ( GO ) [ 53 ] although the relationships that define the four slots shown in Figure 2 are from a provisional submission to the OBO Foundry Relationship Ontology and are not official . Preprocessing tools ( ABNER [ 55 ] and LingPipe [ 59 ] ) were applied to tag instances of proteins , genes , and cell types . Screenshot of the Prot\\u00e9g\\u00e9 ontology for the protein transport task . The slots of the protein transport class are shown in the lower right panel of this screen shot . Note that the subclasses of Cellular Component and Protein Transport are not shown . For the transport task , patterns were produced for 30 ontology concepts ; eight directly related to transport and 22 others for cellular components that are the sources and destinations of transport . A large number of other concepts ( e.g. genes , proteins and cell types ) do not have explicit patterns associated with them , but are instead tagged as such by UIMA tools during preprocessing . The protein - protein interaction task involved producing patterns for nine concepts , and the cell expression task required patterns for six additional concepts . The UIMA architecture [ 49 ] manages the processing of document sets . Various combinations of these tools were used in the different applications . For example , the GeneRIFs used in the transport application did not require sentence segmentation , and the applications to all medline abstracts did not use syntactic elements in patterns because the Stanford Parser was too slow to run over all of Medline . The results of this preprocessing are stored in UIMA 's common annotation structure . In order to be able to recognize a concept in text , OpenDMAP associates one or more patterns with each concept . A pattern describes the words , phrases , parts of speech , syntactic structures or concepts that should cause an instance of the associated concept to be recognized . A simple pattern , such as the one shown in equation 1 , enumerates a disjunction of words that should trigger recognition of a concept . The patterns for all of the CELLULAR - COMPONENT concepts were derived from the GO term names and synonyms , supplemented with derivational variants , such as the adjectival \\\" nuclear \\\" in equation 1 . Twenty - two GO cellular component terms were used , along with 19 synonyms associated with the GO terms , and 78 additional derivational variants generated by inspection of the training corpus . More complex patterns can include references to non - terminals , particularly other concepts . Equation 2 is one of the patterns for recognizing instances of the PROTEIN - TRANSPORT concept . When a pattern that includes a slot name is matched , the instance created has its slots filled with the concepts that matched the slot names in the pattern . For example , the above pattern matches the GeneRIF that contains \\\" ... Bax translocation to mitochondia ... \\\" ( from Entrez GeneID 27113 ) . Since the entire pattern matches , an instance of PROTEIN - TRANSPORT is created , with the Bax protein concept in its [ TRANSPORTED - ENTITY ] slot and an instance of the mitochondria concept ( from GO 's cellular component hierarchy ) in its [ TRANSPORT - DESTINATION ] slot . OpenDMAP patterns can express variability in word and phrase order . Note , for example , that equation 2 would fail to match the phrase \\\" Bax translocation to mitochondria from the cytosol . \\\" The special pattern marker @ is used to identify a set of subpatterns that are both optional and can occur before or after a required phrase ; multiple @ marked phrases can occur in any order . For example , equation 2 can be modified with this marker to recognize the above text : . Many sentences in the literature express multiple concepts , making extraction of even simple assertions problematic . Consider the following GeneRIF from GeneID:29560 : \\\" ... HIF-1alpha which is present in glomus cells translocates to the nucleus .... \\\" The intervening phrase \\\" which is present in glomus cells \\\" prevents the pattern in equation 3 from matching that sentence . OpenDMAP does have a wildcard character ( underscore ) that could be added to the pattern in equation 3 , between the [ TRANSPORTED - ENTITY ] concept and the word \\\" translocation , \\\" allowing this sentence to be matched . However , using such a wild card would make any protein mentioned before the word \\\" translocation \\\" match the pattern , which is too promiscuous . To address this problem , OpenDMAP allows patterns to specify syntactic constraints on potential matches . Furthermore , the reliance on the exact word \\\" translocation \\\" can be relaxed to be any reference to a transport action word , including both verbal and nominal forms of multiple terms ( e.g. , transported , translocation ) . The PROTEIN - TRANSPORT class is extended to have an [ action ] slot that specifies the type of transportation action , to keep track of the term that was used . Equation 4 demonstrates the pattern language for specifying syntactic constraints : . The use of the variable \\\" x \\\" in the specification identifies a specific syntactic unit , linking the dependency to the head of a phrase . Multiple variables can be used to specify constraints on different syntactic units within a sentence . OpenDMAP patterns are very powerful . Only five such patterns , shown in equations 5 - 9 were required for the transport extraction system performance noted above . These patterns were devised manually , based on expert knowledge of the domain and on a small training set of sample GeneRIFs . The test data used in the transport and expression evaluations were marked up by domain experts trained in conceptual annotation , using the Knowtator annotation tool [ 62 ] . Availability of data and software . The results of the information extraction effort are available as RDF format files in the Additional Files 1 , 2 , 3 , 4 . Declarations . Acknowledgements . This work was funded by NIH grants R01NLM008111 and R01NLM009254 to LH . ZL was also supported in part by the Intramural Research Program of the NIH , NLM . Electronic supplementary material . 1471 - 2105 - 9 - 78-S1.GZ Additional file 1 : Transport instances from MEDLINE . This file contains the RDF formatted instances of transport , mined from MEDLINE with OpenDMAP . ( GZ 2785 kb ) . 1471 - 2105 - 9 - 78-S2.GZ Additional file 2 : Interaction instances from MEDLINE , part 1 . The interaction data set is very large . This file contains the first half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 6035 kb ) . 1471 - 2105 - 9 - 78-S3.GZ Additional file 3 : Interaction instances from MEDLINE , part 2 . The interaction data set is very large . This file contains the second half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 5994 kb ) . 1471 - 2105 - 9 - 78-S4.GZ Additional file 4 : Expression instances from MEDLINE . This file contains the RDF formatted instances of expression , mined from MEDLINE with OpenDMAP . ( GZ 7154 kb ) . Authors ' contributions . LH conceived of the project , supervised the design and implementation of the system and wrote the manuscript . ZL was responsible for the transport project , including writing the patterns and analyzing the results ; he also contributed suggestions for the interaction task . JRF implemented the OpenDMAP pattern recognition engine and its UIMA wrapper . WAB wrote all other infrastructure software , including the other UIMA wrappers , managed the data , applied OpenDMAP to all of MEDLINE , and designed and built other software . HLJ was responsible for the interaction and expression projects , including writing the patterns , analyzing the results , and doing the associated error analyses . PVO managed the creation of the gold standard data for transport and contributed to the design of the pattern language syntax . KBC managed the team , selected the preprocessing tools , coordinated and supervised the interaction and expression task efforts , and provided linguistic and software design contributions during all phases of the project . All authors have read and approved this manuscript . Authors ' Affiliations . Center for Computational Pharmacology , University of Colorado School of Medicine . National Center for Biotechnology Information , National Library of Medicine . PowerSet , Inc. . Department of Computer Science , University of Colorado . References . Sparck Jones K : Natural language processing : A historical review . Current Issues in Computational Linguistics : in Honour of Don Walker ( Ed Zampolli , Calzolari and Palmer ) , Amsterdam : Kluwer 1994 . Rebholz - Schuhmann D , Kirsch H , Couto F : Facts from text -- is text mining ready to deliver ? PLoS Biol 2005 , 3 ( 2 ) : e65 . View Article PubMed . Hoffmann R , Valencia A : A gene network for navigating the literature . Nat Genet 2004/07/01 Edition 2004 , 36 ( 7 ) : 664 . View Article PubMed . Shah PK , Jensen LJ , Bou\\u00e9 S , Bork P : Extraction of transcript diversity from scientific literature . PLoS Comput Biol 2005 , 1 ( 1 ) : e10 . View Article PubMed . Horn F , Lau AL , Cohen FE : Automated extraction of mutation data from the literature : application of MuteXt to G protein - coupled receptors and nuclear hormone receptors . Bioinformatics 2004 , 20 ( 4 ) : 557 - 568 . View Article PubMed . Hu ZZ , Narayanaswamy M , Ravikumar KE , Vijay - Shanker K , Wu CH : Literature mining and database annotation of protein phosphorylation using a rule - based system . Bioinformatics 2005/04/09 Edition 2005 , 21 ( 11 ) : 2759 - 2765 . View Article PubMed . Saric J , Jensen LJ , Ouzounova R , Rojas I , Bork P : Extraction of regulatory gene / protein networks from Medline . Bioinformatics 2005/07/28 Edition 2006 , 22 ( 6 ) : 645 - 650 . View Article PubMed . Guarino N : Formal ontology in information systems . Trento , Italy , IOS Press 1998 , 3 - 15 . Hersh W , Bhupatiraju R , Ross L , Johnson P , Cohen A , Kraemer D : TREC 2004 Genomics track overview . National Institute of Standards and Technology 2004 . Fellbaum C : WordNet : An Electronic Lexical Database ( Language , Speech , and Communication ) . MIT Press 1998 . Aronson A : Effective Mapping of Biomedical Text to the UMLS Metathesaurus : The MetaMap Program . AMIA Annu Symp Proc 2001 , 17 - 21 . Rindflesch TC , Fiszman M : The interaction of domain knowledge and linguistic structure in natural language processing : interpreting hypernymic propositions in biomedical text . J Biomed Inform 2003 , 36 ( 6 ) : 462 - 477 . View Article PubMed . Rindflesch TC , Libbus B , Hristovski D , Aronson AR , Kilicoglu H : Semantic relations asserting the etiology of genetic diseases . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 554 - 558 . Masseroli M , Kilicoglu H , Lang FM , Rindflesch TC : Argument - predicate distance as a filter for enhancing precision in extracting predications on the genetic etiology of disease . BMC Bioinformatics 2006/06/10 Edition 2006 , 7 : 291 . View Article PubMed . Ahlers CB , Fiszman M , Demner - Fushman D , Lang FM , Rindflesch TC : Extracting semantic predications from Medline citations for pharmacogenomics . Pac Symp Biocomput 2007/11/10 Edition 2007 , 209 - 220 . Libbus B , Kilicoglu H , Rindflesch TC , Mork JG , Aronson AR , Hirschman L , Pustejovsky J : Using Natural Language Processing , LocusLink and the Gene Ontology to Compare OMIM to MEDLINE . HLT - NAACL 2004 Workshop : BioLINK 2004 , Linking Biological Literature , Ontologies and Databases 2004 , 69 - 76 . Blaschke C , Andrade MA , Ouzounis C , Valencia A : Automatic extraction of biological information from scientific text : protein - protein interactions . Proc Int Conf Intell Syst Mol Biol 2000/04/29 Edition 1999 , 60 - 67 . Blaschke C , Oliveros JC , Valencia A : Mining functional information associated with expression arrays . Funct Integr Genomics 2002/01/17 Edition 2001 , 1 ( 4 ) : 256 - 268 . View Article PubMed . Blaschke C , Valencia A : Can bibliographic pointers for known biological data be found automatically ? Protein interactions as a case study . Comparative and Functional Genomics 2001 , 2 ( 4 ) : 196 - 206 . View Article PubMed . Huang M , Zhu X , Hao Y , Payan DG , Qu K , Li M : Discovering patterns to extract protein - protein interactions from biomedical full texts . Proc JNLPBA , COLING 2004 , 22 - 28 . Temkin JM , Gilder MR : Extraction of protein interaction information from unstructured text using a context - free grammar . Bioinformatics Oxford Univ Press 2003 , 19 ( 16 ) : 2046 - 2053 . Corney DP , Buxton BF , Langdon WB , Jones DT : BioRAT : extracting biological information from full - length papers . Bioinformatics 2004/07/03 Edition 2004 , 20 ( 17 ) : 3206 - 3213 . View Article PubMed . Park JC , Kim HS , Kim JJ : Bidirectional incremental parsing for automatic pathway identification with combinatory categorial grammar . Pac Symp Biocomput 2001 , 396 - 407 . Yakushiji A , Tateisi Y , Miyao Y , Tsujii J : Event extraction from biomedical papers using a full parser . Pac Symp Biocomput 2001 , 408 - 419 . Gaizauskas R , Demetriou G , Artymiuk PJ , Willett P : Protein structures and information extraction from biological texts : the PASTA system . Bioinformatics 2002/12/25 Edition 2003 , 19 ( 1 ) : 135 - 143 . View Article PubMed . Leroy G , Chen H , Martinez JD : A shallow parser based on closed - class words to capture relations in biomedical text . J Biomed Inform 2003 , 36 ( 3 ) : 145 - 158 . View Article PubMed . Koike A , Niwa Y , Takagi T : Automatic extraction of gene / protein biological functions from biomedical text . Bioinformatics 2004/10/29 Edition 2005 , 21 ( 7 ) : 1227 - 1236 . View Article PubMed . Rinaldi F , Schneider G , Kaljurand K , Hess M , Romacker M : An environment for relation mining over richly annotated corpora : the case of GENIA . BMC Bioinformatics 2006 , 7 Suppl 3 : S3 . View Article PubMed . McInnes BT , Pedersen T , Pakhomov SV : Determining the Syntactic Structure of Medical Terms in Clinical Notes . Proc Assoc Comp Ling 2007 . Pyysalo S , Ginter F , Haverinen K , Heimonen J , Salakoski T , Laippala V : On the unification of syntactic annotations under the Stanford dependency scheme : A case study on BioInfer and GENIA . Association for Computational Linguistics 2007 . Craven M , Kumlien J : Constructing biological knowledge bases by extracting information from text sources . Proc Int Conf Intell Syst Mol Biol 1999 , 77 - 86 . Bunescu R , Ge R , Kate RJ , Marcotte EM , Mooney RJ , Ramani AK , Wong YW : Comparative experiments on learning information extractors for proteins and their interactions . Artif Intell Med 2005/04/07 Edition 2005 , 33 ( 2 ) : 139 - 155 . View Article PubMed . Krallinger M , Leitner F , Valencia A : Assessment of the second BioCreative PPI task : automatic extraction of protein - protein interactions . Hunter L , Cohen KB : Biomedical Language Processing : What 's Beyond PubMed ? Molecular Cell Cell 2006 , 21 : 589 - 594 . Chen H , Sharp BM : Content - rich biological network constructed by mining PubMed abstracts . BMC Bioinformatics 2004 , 5 : 147 . View Article PubMed . Wattarujeekrit T , Shah PK , Collier N : PASBio : predicate - argument structures for event extraction in molecular biology . BMC Bioinformatics 2004 , 5 : 155 - 175 . View Article PubMed . Cohen KB , Hunter L : A critical review of PASBio 's argument structures for biomedical verbs . BMC Bioinformatics 2006 , 7 ( Suppl . 3 ) : S5 . View Article PubMed . J Biomed Inform 2004/03/16 Edition 2004 , 37 ( 1 ) : 43 - 53 . View Article PubMed . Narayanaswamy M , Ravikumar KE , Vijay - Shanker K : Beyond the clause : extraction of phosphorylation information from medline abstracts . Bioinformatics 2005 . , 21 Suppl 1 : . Nucleic Acids Res 2003/12/19 Edition 2004 , 32 ( Database issue ) : D452 - 5 . View Article PubMed . Ding J , Berleant D , Nettleton D , Wurtele E : Mining MEDLINE : Abstracts , Sentences , or Phrases ? Pac Symp Biocomput 2002 , 7 : 326 - 337 . Martin C : Direct Memory Access Parsing . Yale University 1992 . Fitzgerald W : Building Embedded Conceptual Parsers . Northwestern University 1994 . Noy NF , Crubezy M , Fergerson RW , Knublauch H , Tu SW , Vendetti J , Musen MA : Protege-2000 : an open - source ontology - development and knowledge - acquisition environment . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 953 . Hirschman L , Colosimo M , Morgan A , Yeh A : Overview of BioCreAtIvE task 1B : normalized gene lists . BMC Bioinformatics 2005/06/18 Edition 2005 , 6 Suppl 1 : S11 . View Article PubMed . The Gene Ontology Consortium . Nat Genet 2000/05/10 Edition 2000 , 25 ( 1 ) : 25 - 29 . View Article PubMed . Genome Biology 2008 . Settles B : ABNER : an open source tool for automatically tagging genes , proteins and other entity names in text . Bioinformatics 2005 , 21 ( 14 ) : 3191 - 3192 . View Article PubMed . Baumgartner WA Jr. , Cohen KB , Fox LM , Acquaah - Mensah G , Hunter L : Manual curation is not sufficient for annotation of genomic databases . Bioinformatics 2007 , 23 ( 14 ) : e. . Leach SM , Gabow AP , Hunter L , Goldberg D : Assessing and combining reliability of protein interaction sources . Pac Symp Biocomp 2007 , 12 : 433 - -444 . View Article . Gabow AP , Leach SM , Baumgartner WA Jr. , Hunter L , Goldberg D : Improving Protein Function Prediction Methods with Integrated Literature Data . Carpenter B : Phrasal queries with LingPipe and Lucene : ad hoc genomics text retrieval . 13th Annual Text Retrieval Conference 2004 . Klein D , Manning CD : Fast Exact Inference with a Factored Model for Natural Language Parsing . Advances in Neural Information Processing Systems MIT Press 2003 . , 15 : . Kehler A , Appelt D , Taylor L , Simma A : The ( non ) utility of predicate - argument frequencies for pronoun interpretation . Proc of HLT - NAACL 2004 , 4 : 289 - 296 . Ogren P : Knowtator : a Protege plugin for annotated copus construction . HLT - NAACL 2006 , 273 . Copyright . \\u00a9 Hunter et al . 2008 . This article is published under license to BioMed Central Ltd. \"}",
        "_version_":1692580791959158784,
        "score":19.683487},
      {
        "id":"dd1551ef-e927-4afe-8fc9-4894e45b1672",
        "_src_":"{\"url\": \"http://www.pekingduck.org/2006/12/killing-dogs/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701165484.60/warc/CC-MAIN-20160205193925-00266-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Manuscript received October 27 , 2010 . Manuscript accepted for publication January 14 , 2011 . Externai sandhi is a linguistic phenomenon which refers to a set of sound changes that occur at word boundaries . These changes are similar to phonological processes such as assimilation and fusion when they apply at the level of prosody , such as in connected speech . External sandhi formation can be orthographically reflected in some languages . External sandhi formation in such languages , causes the occurrence of forms which are morphologically unanalyzable , thus posing a problem for all kind of NLP applications . In this paper , we discuss the implications that this phenomenon has for the syntactic annotation of sentences in Telugu , an Indian language with agglutinative morphology . We describe in detail , how external sandhi formation in Telugu , if not handled prior to dependency annotation , leads either to loss or misrepresentation of syntactic information in the treebank . This phenomenon , we argue , necessitates the introduction of a sandhi splitting stage in the generic annotation pipeline currently being followed for the treebanking of Indian languages . We identify one type of external sandhi widely occurring in the previous version of the Telugu treebank ( version 0.2 ) and manually split all its instances leading to the development of a new version 0.5 . We also conduct an experiment with a statistical parser to empirically verify the usefulness of the changes made to the treebank . Comparing the parsing accuracies obtained on versions 0 . 2 and 0 . 5 of the treebank , we observe that splitting even just one type of external sandhi leads to an increase in the overall parsing accuracies . [ 1 ] M. Marcus , M. Marcinkiewicz , and B. Santorini , \\\" Building a large annotated corpus of English : The Penn Treebank , \\\" Computational linguistics , vol . [ Links ] . [ 2 ] A. Bharati , V. Chaitanya , and R. Sangal , Natural language processing : a Paninian perspective . Prentice Hall of India , 1995 . [ Links ] . [ 3 ] J. Hajic , \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank , \\\" Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 , 1998 . [ Links ] . [ Links ] . [ Links ] . [ 6 ] A. Culotta and J. Sorensen , \\\" Dependency tree kernels for relation extraction , \\\" in Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics . Association for Computational Linguistics , 2004 , p. 423 . [ Links ] . [ Links ] . [ Links ] . [ Links ] . [ Links ] . [ Links ] . [ 12 ] A. Bharati , M. Bhatia , V. Chaitanya , and R. Sangal , \\\" Paninian Grammar Framework Applied to English , \\\" South Asian Language Review , 1997 . [ Links ] . [ Links ] . [ Links ] . [ 15 ] B. Krishnamurti , The Dravidian languages . Cambridge Univ Press , 2003 . [ Links ] . [ 16 ] E. Sapir , Language : An introduction to the study of speech . Dover Publications , 1921 . [ Links ] . [ 17 ] J. Greenberg , \\\" A quantitative approach to the morphological typology of language , \\\" International Journal of American Linguistics , vol . [ Links ] . [ 18 ] S. Husain , \\\" Dependency Parsers for Indian Languages , \\\" Proceedings of ICON09 NLP Tools Contest : Indian Language Dependency Parsing , 2009 . [ Links ] . [ Links ] . [ Links ] . Association for Computational Linguistics , 2007 . [ Links ] . [ 22 ] V. Mittal , \\\" Automatic Sanskrit segmentizer using finite state transducers , \\\" in Proceedings of the ACL 2010 Student Research Workshop . [ Links ] . [ Links ] . [ 25 ] A. A. Macdonell , A Sanskrit Grammar for students . New Delhi , India : D.K. Printworld ( P ) Ltd. , 1926 . [ Links ] . [ Links ] . [ 27 ] A. Zwicky , \\\" Stranded to and phonological phrasing in english , \\\" Linguistics , vol . [ Links ] . [ 28 ] H. Andersen , Sandhi phenomena in the languages of Europe . Mouton de Gruyter , 1986 . [ Links ] . [ Links ] . [ Links ] \"}",
        "_version_":1692671067547500544,
        "score":19.614225},
      {
        "id":"bfdc7bfe-8f1f-4d1c-8639-3037543b201e",
        "_src_":"{\"url\": \"https://www.behance.net/gallery/YWFT-Mimic/6892595\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153585.76/warc/CC-MAIN-20160205193913-00244-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"We present a method for recognizing semantic roles for Spanish sentences . If a complete parse can not be produced , a partial structure is built with some ( if not all ) dependency relations identified . Evaluation shows that in spite of its simplicity , the parser 's accuracy is superior to the available existing parsers for Spanish . A particularly interesting ambiguity which we have decided to analyze deeper , is the Prepositional Phrase Attachment Disambiguation . The system uses an ordered set of simple heuristic rules for determining iteratively the relationships between words to which a governor has not been yet assigned . For resolving certain cases of ambiguity we use cooccurrence statistics of words collected previously in an unsupervised manner , whether it be from big corpora , or from the Web ( through a search engine such as Google ) . Collecting these statistics is done by using Selectional Preferences . In order to evaluate our system , we developed a Method for Converting a Gold Standard from a constituent format to a dependency format . Additionally , each one of the modules of the system ( Selectional Preferences Acquisition and Prepositional Phrase Attachment Disambiguation ) , is evaluated in a separate and independent way to verify that they work properly . Finally we present some Applications of our system : Word Sense Disambiguation and Linguistic Steganography . Keywords : dependency parsing , pp attachment disambiguation , constituent to dependency conversion , heuristic rules , hybrid parser , selectional preferences . Resumen . Se presenta un m\\u00e9todo para reconocer los roles sem\\u00e1nticos de las oraciones en espa\\u00f1ol , es decir , identificar el papel que tiene cada uno de los elementos de la oraci\\u00f3n . Si no se puede producir un an\\u00e1lisis completo , se construye una estructura parcial con algunas ( si no todas ) relaciones de dependencia identificadas . La evaluaci\\u00f3n muestra que a pesar de su simplicidad , la precisi\\u00f3n del analizador es superior a aquella de los analizadores existentes actuales para el espa\\u00f1ol . A pesar de que ciertas reglas gramaticales y los recursos l\\u00e9xicos usados son espec\\u00edficos para el espa\\u00f1ol , el enfoque sugerido es independiente del lenguaje . Una ambig\\u00fcedad interesante que hemos decidido analizar a mayor profundidad , es la desambiguaci\\u00f3n de sintagma preposicional . El sistema usa un conjunto ordenado de reglas heur\\u00edsticas simples para determinar iterativamente las relaciones entre palabras para las cuales no se les ha asignado a\\u00fan un gobernante . Estas estad\\u00edsticas han sido obtenidas previamente de una manera no supervisada , ya sea a partir de grandes corpus de texto , o a trav\\u00e9s de Internet ( a trav\\u00e9s de un motor de b\\u00fasqueda como Google ) . Para evaluar este sistema , desarrollamos un m\\u00e9todo para convertir un est\\u00e1ndar existente , de un formato de constituyentes a un formato de dependencias . Adicionalmente , cada uno de los m\\u00f3dulos del sistema ( Adquisici\\u00f3n de Preferencias de Selecci\\u00f3n , Desambiguaci\\u00f3n de Sintagma Preposicional ) se eval\\u00faa de una forma separada e independiente para verificar su correcto funcionamiento . Finalmente , presentamos algunas aplicaciones de nuestro sistema : Desambiguaci\\u00f3n de sentidos de palabras y Estaganograf\\u00eda ling\\u00fc\\u00edstica . Palabras clave : an\\u00e1lisis de dependencias , desambiguaci\\u00f3n de frase preposicional , conversi\\u00f3n de constituyentes a dependencias , reglas heur\\u00edsticas , analizador sint\\u00e1ctico h\\u00edbrido , preferencias de selecci\\u00f3n . Agirre E. , D. Mart\\u00ednez . Unsupervised WSD based on automatically retrieved examples : The importance of bias . In Proceedings of the Conference on Empirical Methods in Natural Language Processing , EMNLP , Barcelona , Spain , 2004 . [ Links ] . Agirre , E. D. Martinez . [ Links ] . Agirre , E. , D. Martinez . Integrating selectional preferences in WordNet . [ Links ] . Apresyan , Yuri D. , Igor Boguslavski , Leonid Iomdin , Alexandr Lazurski , Nikolaj Pertsov , Vladimir Sannikov , Leonid Tsinman . Moscow , Nauka , 1989 . [ Links ] . Bolshakov , Igor A. [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . A Very Large Database of Collocations and Semantic Links . Proc . Conf . [ Links ] . Bolshakov , Igor A. , Alexander Gelbukh . On Detection of Malapropisms by Multistage Collocation Testing . Conf . on Application of Natural Language to Information Systems . [ Links ] . In Proceedings of the 6 th Applied Natural Language Processing Conference , Seattle , Washington , USA , 2000 . [ Links ] . Brants , Thorsten . In : Proc . [ Links ] . Brill , Eric , Philip Resnik . [ Links ] . Briscoe , Ted . John Carroll , Jonathan Graham and Ann Copestake . Relational evaluation schemes . In : Procs . [ Links ] . Calvo , Hiram , Alexander Gelbukh . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Improving Prepositional Phrase Attachment Disambiguation Using the Web as Corpus , In A. Sanfeliu and J. Shulcloper ( Eds . ) Calvo , Hiram , Alexander Gelbukh . Natural Language Interface Framework for Spatial Object Composition Systems . Procesamiento de Lenguaje Natural 31 , 2003 . [ Links ] . Calvo , Hiram , Alexander Gelbukh . Acquiring Selectional Preferences from Untagged Text for Prepositional Phrase Attachment Disambiguation . In : Proc . [ Links ] . Calvo , Hiram . Alexander Gelbukh , Adam Kilgarriff . Distributional Thesaurus versus WordNet : A Comparison of Backoff Techniques for Unsupervised PP Attachment . [ Links ] . Carreras , Xavier , Isaac Chao , Lluis Padr\\u00f3 , Muntsa Padr\\u00f3 . Proc . 4 th Intern . Conf . [ Links ] . Carroll , J. , D. McCarthy . Word sense disambiguation using automatically acquired verbal preferences . [ Links ] . Chomsky , Noam . Syntactic Structures . The Hague : Mouton & Co , 1957 . [ Links ] . Civit , Montserrat , and Maria Ant\\u00f2nia Mart\\u00ed . Est\\u00e1ndares de anotaci\\u00f3n morfosint\\u00e1ctica para el espa\\u00f1ol . Workshop of tools and resources for Spanish and Portuguese . IBERAMIA 04 , Mexico , 2004 . [ Links ] . Copestake , Ann , Dan Flickinger , Ivan A. Sag . Minimal Recursion Semantics . An introduction . CSLI , Stanford University , 1997 . [ Links ] . In : Recent Advances in Dependency Grammar . Proc . D\\u00edaz , Isabel , Lidia Moreno , Inmaculada Fuentes , Oscar Pastor . In : Alexander Gelbukh ( ed . ) [ Links ] . Dik , Simon C. , The Theory of Functional Grammar . Part I : The structure of the clause . Dordrecht , Foris , 1989 . [ Links ] . Dirk , L\\u00fcdtke , Satoshi Sato . [ Links ] . Gelbukh , A. , G. Sidorov , L. Chanona . Corpus virtual , virtual : Un diccionario grande de contextos de palabras espa\\u00f1olas compilado a trav\\u00e9s de Internet . In : Julio Gonzalo , Anselmo Pe\\u00f1as , Antonio Ferr\\u00e1ndez , eds . : Proc . [ Links ] . Gelbukh , A. , S. Torres , H. Calvo . Transforming a Constituency Treebank into a Dependency Treebank . Submitted to Procesamiento del Lenguaje Natural No . 34 , Spain , 2005 . [ Links ] . Gelbukh , Alexander , Grigori Sidorov , Francisco Vel\\u00e1squez . An\\u00e1lisis morfol\\u00f3gico autom\\u00e1tico del espa\\u00f1ol a trav\\u00e9s de generaci\\u00f3n . [ Links ] . Gladki , A. V. Syntax Structures of Natural Language in Automated Dialogue Systems ( in Russian ) . Moscow , Nauka , 1985 . [ Links ] . Kudo , T. , Y. Matsumoto . Use of Support Vector Learning for Chunk Identification . [ Links ] . Lara , Luis Fernando . Diccionario del espa\\u00f1ol usual en M\\u00e9xico . Digital edition . Colegio de M\\u00e9xico , Center of Linguistic and Literary Studies , 1996 . [ Links ] . [ Links ] . Mel'cuk , Igor A. Dependency Syntax : Theory and Practice . State U. Press of NY , 1988 . [ Links ] . Mel'cuk , Igor A. Lexical Functions : A Tool for the Description of Lexical Relations in the Lexicon . In : L. Wanner ( ed . ) [ Links ] . [ Links ] . Monedero , J. , Gonz\\u00e1lez , J. Go\\u00f1i , C. Iglesias , A. Nieto . Obtenci\\u00f3n autom\\u00e1tica de marcos de subcategorizaci\\u00f3n verbal a partir de texto etiquetado : el sistema SOAMAS . [ Links ] . Text Mining at Detail Level Using Conceptual Graphs . In : Uta Priss et al . ( Eds . ) : Conceptual Structures : Integration and Interfaces , 10 th Intern . Conf . [ Links ] . Information Retrieval with Conceptual Graph Matching . Proc . Conf . [ Links ] . Evaluation of TnT Tagger for Spanish . In Proc . [ Links ] . Pollard , Carl , and Ivan Sag . University of Chicago Press , Chicago , IL and London , UK , 1994 . [ Links ] . Prescher , Detlef , Stefan Riezler , and Mats Rooth . In Proceedings of the 18th International Conference on Computational Linguistics , Saarland University , Saarbr\\u00fccken , Germany , 2000 . [ Links ] . Ratnaparkhi , Adwait , Jeff Reynar , and Salim Roukos . A Maximum Entropy Model for Prepositional Phrase Attachment . [ Links ] . [ Links ] . Resnik , P. Selectional preference and sense disambiguation , ACL SIGLEX Workshop on Tagging Text with Lexical Semantics : Why , What , and How ? [ Links ] . Ph.D. Thesis , University of Pennsylvania , December , 1993 . [ Links ] . Sag , Ivan , Tom Wasow , and Emily M. Bender . Syntactic Theory . A Formal Introduction ( 2nd Edition ) . CSLI Publications , Stanford , CA , 2003 [ Links ] . Sebasti\\u00e1n , N. , M. A. Mart\\u00ed , M. F. Carreiras , and F. Cuestos . LEXESP , l\\u00e9xico informatizado del espa\\u00f1ol , Edicions de la Universitat de Barcelona , 2000 . [ Links ] . Sowa , John F. 1984 . Conceptual Structures : Information Processing in Mind and Machine . [ Links ] . Steele , James ( ed . ) Linguistics , Lexicography , and Implications . Ottawa : Univ . of Ottawa Press , 1990 . [ Links ] . Su\\u00e1rez , A. , M. Palomar . [ Links ] . Tapanainen , Pasi . Academic Dissertation . University of Helsinki , Language Technology , Department of General Linguistics , Faculty of Arts , 1999 . [ Links ] . Tesni\\u00e8re , Lucien . El\\u00e9ments de syntaxe structurale . Paris : Librairie Klincksieck , 1959 . [ Links ] . Volk , Martin . Exploiting the WWW as a corpus to resolve PP attachment ambiguities . In Proceeding of Corpus Linguistics 2001 . Lancaster , 2001 . [ Links ] . Weinreich , Uriel . Explorations in Semantic Theory , Mouton , The Hague , 1972 . [ Links ] . Yarowsky , D. , Hierarchical decision lists for word sense disambiguation . [ Links ] . Yarowsky , D. , S. Cucerzan , R. Florian , C. Schafer , R. Wicentowski . [ Links ] . Yuret , Deniz . Discovery of Linguistic Relations Using Lexical Attraction , PhD thesis , MIT , 1998 . [ Links ] \"}",
        "_version_":1692671237020450816,
        "score":19.261168},
      {
        "id":"fe955458-f0ee-4906-8213-8c3e4de97bc5",
        "_src_":"{\"url\": \"http://eol.org/data_objects/22903368\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168076.20/warc/CC-MAIN-20160205193928-00173-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Implementation of the DMV+CCM Parser . Please let me know if you download this software . Write an e - mail to this address . Introduction . This package includes implementations of the CCM , DMV and DMV+CCM parsers from Klein and Manning ( 2004 ) , and code for testing them with the WSJ , Negra and Cast3LB corpora ( English , German and Spanish respectively ) . A detailed description of the parsers can be found in Klein ( 2005 ) . This work was done as part of the PhD in Computer Science I am doing at FaMAF , Universidad Nacional de Cordoba , Argentina , under the supervision of Gabriel Infante - Lopez , with a research fellowship from CONICET . About Version 0.2.0 . This version is aimed at reproducing some of the results from Klein and Manning ( 2004 ) . The implemented DMV and DMV+CCM models are the versions with the one - side - first constraint . We could n't reproduce yet the results for these models without the one - side - first constraint . The following table shows the performance of the models over the WSJ10 corpus : . DMV reaches the given values at the 20th iteration . CCM converges to the given values since the 40th iteration . The results for DMV+CCM reported in the table were arbitrarily picked from the 10th iteration . Dependencies . Installation and Configuration . Extract lq-nlp-commons.tar.gz and lq-dmvccm.tar.gz into a folder and add the new folders to the Python PATH . For instance : . Usage . Quickstart . To train ( with 10 iterations ) and test the CCM parser with the WSJ10 do the following ( replace ' my_wsj_path ' with the path to the WSJ corpus ) : . To parse a sentence with the resulting parser do something like this : . Parser Instantiation , Training and Testing . The WSJ10 corpus is used by default to train the parsers . The WSJ10 corpus is automatically extracted from the WSJ corpus . You must place the WSJ corpus in a folder named wsj_comb ( or edit lq - nlp - commons / wsj . py , or read below ) . For instance , to train ( with 10 iterations ) and test the CCM parser with the WSJ10 do the following : . To give the treebanks explicitly : . The Negra corpus must be in a file negra - corpus / negra - corpus2 . penn ( in Penn format ) , and the Cast3LB corpus in a folder named 3lb - cast ( also in Penn format ) . To use alternative locations for the treebanks use the parameter basedir when creating the object . For instance : . ( similarly for Negra and Cast3LB ) . When loaded for the first time , the extracted corpora are saved into files to avoid having to process the entire treebanks again . The files are saved in the NLTK data path ( nltk.data.path[0 ] ) , usually $ HOME / nltk_data , to be loaded in future instantiations of the treebanks . The DMV and DMV+CCM parsers are in the classes dmv . DMV and dmvccm . DMVCCM . The current implementation of DMV+CCM has the one - side - first constraint . They are used in the same way CCM is . For example : . Take into account that training DMV and DMV+CCM is much slower than training CCM . A single training step can take more than 20 minutes . Parser Usage . Once you have a trained instance of CCM , DMV or DMV+CCM , you can parse sentences with the parse ( ) method . You may give a list of words or an instance of the sentence . Sentence class from lq - nlp - commons . The parse ( ) method returns a pair ( b , p ) , where b is an instance of bracketing . Bracketing and p is the probability of the bracketing . You can get the set of brackets from b.brackets or convert the bracketing to a tree with b.treefy ( ) . For instance : . In the case of DMV and DMV+CCM you can also use the method dep_parse ( ) to get the dependency structure parsed by these models ( Klein , 2005 ) . For instance : . References . Franco M. Luque . Una Implementaci\\u00f3n del Modelo DMV+CCM para Parsing No Supervisado ( 2011 ) . 2do Workshop Argentino en Procesamiento de Lenguaje Natural , C\\u00f3rdoba , Argentina . Klein , D. and Manning , C. D. ( 2004 ) . Corpus - based induction of syntactic structure : Models of dependency and constituency . In ACL , pages 478 - 485 . Klein , D. ( 2005 ) . The Unsupervised Learning of Natural Language Structure . PhD thesis , Stanford University . \"}",
        "_version_":1692669796068360192,
        "score":18.386873}]
  }}
