{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"och approach automatic generation evaluation corpus text zhao model ney proposed error employed previously rate maximum training berger standard chen",
      "fl":"*,score"}},
  "response":{"numFound":946850,"start":0,"maxScore":29.094374,"numFoundExact":true,"docs":[
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":29.094374},
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":21.564936},
      {
        "id":"7aa16a17-1269-468f-b6a7-ec1a0840c797",
        "_src_":"{\"url\": \"http://alotarchives.org/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166739.77/warc/CC-MAIN-20160205193926-00147-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"This document shows the results of our Speaker Verification System under two scenarios : the Face and Speaker Verification Evaluation organized by MOBIO ( MObile BIOmetric consortium ) and the results for the Speaker Recognition Evaluation 2010 organized by NIST . The core of our system is based on a Gaussian Mixture Model ( GMM ) and maximum likelihood ( ML ) framework . First , it extracts the important speech features by computing the Mel Frequency Cepstral Coefficients ( MFCC ) . Finally , those scores are tagged as target or impostor . We tried several system configurations and found that each database requires a specific tuning to improve the performance . For the MOBIO database we obtained an average equal error rate ( EER ) of 16.43 % . For the NIST 2010 database we accomplished an average EER of 16.61 % . NIST2010 database considers various conditions . From those conditions , the interview training and testing conditions showed the best EER of 10.94 % , followed by the phone call training phone call testing conditions of 13.35 % . Keywords : Speaker verification and authentication . Resumen . Este documento muestra los resultados de nuestro sistema de verificaci\\u00f3n de hablante bajo dos escenarios : la Evaluaci\\u00f3n Face and Speaker Verification Evaluation organizada por MOBIO ( MObile BIOmetric consortium ) y la Evaluaci\\u00f3n de Reconociemiento de personas 2010 organizada por NIST . La parte central de nuestro esquema se basa en un modelado de Mezclas de Gaussianas ( GMM ) y m\\u00e1xima verosimilitud . Primero , se extraen los par\\u00e1metros importantes de la voz calculando los coeficientes ceptrales en escala mel , Mel Frequency Cepstral Coefficients ( MFCC ) . Despu\\u00e9s , dichos MFFCs entrenan las mezclas de Gaussianas dependientes del g\\u00e9nero que posteriormente ser\\u00e1n adaptadas y se obtendr\\u00e1n los modelos de los usuarios objetivo . Para obtener estad\\u00edsticas confiables esos modelos objetivo son evaluados por un conjunto de se\\u00f1ales no conocidas y se obtienen puntuaciones finales . Por \\u00faltimo , esas puntuaciones son etiquetadas como usuario objetivo o impostor . Hemos analizado diferentes configuraciones y encontramos que cada base de datos requiere una sintonizaci\\u00f3n adecuada para mejorar su desempe\\u00f1o . Para la base de datos MOBIO , obtuvimos un porcentaje de error promedio de 16.43 % . Para la base de datos NIST2010 , logramos un promedio de error de 16.61 % . La base de datos NIST2010 considera varias condiciones . De esas condiciones , la condici\\u00f3n de entrevista para entrenamiento y prueba mostr\\u00f3 el mejor error con 10.94 % , seguida por la condici\\u00f3n de llamada telef\\u00f3nica en entrenamiento y llamada telef\\u00f3nica en prueba con 13.35 % . Bogert , B. P. , Healy , M. J. R. & Tukey , J. W. ( 1963 ) . [ Links ] . Burget , L. , Fapso , M. Hubeika , V. , Glembek , O. , Karafi\\u00e1t , M. , Kockmann , M. , Matejka , P. , Schwarz , P. , & Cernocky , J. ( 2009 ) . But system for nist 2008 speaker recognition evaluation . Interspeech 2009 . [ Links ] . Chen , S. S. , & Gopinath , R. A. ( 2001 ) . Gaussianization . In Todd K. Leen , Thomas G. Dietterich , Volker Tresp ( Eds . ) [ Links ] . Davis , S. & Mermelstein , P. ( 1980 ) . Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences . [ Links ] . Dempster , A. P. , Laird , N. M. , & Rubin , D. B. ( 1977 ) . Maximum likelihood from incomplete data via the EM algorithm . [ Links ] . Duda , R. O. & Hart , P. E. ( 1973 ) . Pattern classification and scene analysis . New York : Wiley . [ Links ] . Furui , S. ( 1981 ) . Cepstral analysis techniques for automatic speaker verification . [ Links ] . Gauvain , J. L. & Lee , C. H. ( 1994 ) . Maximum a posteriori estimation for multivariate Gaussian mixture observations of markov chains . [ Links ] . Hermansky , H. , Morgan , N. , Bayya , A. , & Kohn , P. ( 1992 ) . [ Links ] . A unified statistical hypothesis testing approach to speaker verification and verbal information verification . Proceedings COST , Workshop on Speech Technology in the Public Telephone Network : Where are we today ? [ Links ] . Mari\\u00e9thoz , J. & Bengio , S. ( 2005 ) . [ Links ] . Martin , A.F. & Greenberg , C.S. ( 2009 ) . NIST 2008 Speaker Recognition Evaluation : Performance Across Telephone and Room Microphone Channels . [ Links ] . Navratil , J. & Ramaswamy , G.N. ( 2003 ) . [ Links ] . Pelecanos , J. & Sridharan , S. ( 2001 ) . Feature warping for robust speaker verification . [ Links ] . Progress in nonlinear speech processing . [ Links ] . Reynolds , D.A. ( 1992 ) . Ph.D. dissertation , Georgia Institute of Technology , Atlanta , Georgia , USA . [ Links ] . Reynolds , D.A. ( 1995 ) , Speaker identification and verification using Gaussian mixture speaker models . [ Links ] . Reynolds , D.A. , Quatieri , T. F. & Dunn , R. B. ( 2000 ) . Speaker verification using adapted Gaussian mixture models . [ Links ] . [ Links ] . Viikki , O. & Laurila , K. ( 1998 ) . Cepstral domain segmental feature vector normalization for noise robust speech recognition . [ Links ] . Wald , A. ( 1947 ) . Sequential analysis . New York : John Wiley and Sons . [ Links ] \"}",
        "_version_":1692671107818061824,
        "score":21.03785},
      {
        "id":"54d25c03-e458-4702-9871-2935f55db0fc",
        "_src_":"{\"url\": \"https://kb.osu.edu/dspace/handle/1811/346\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166570.91/warc/CC-MAIN-20160205193926-00022-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Learning to Classify Text . Detecting patterns is a central part of Natural Language Processing . Words ending in -ed tend to be past tense verbs ( 5 . ) Frequent use of will is indicative of news text ( 3 ) . These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic . But how did we know where to start looking , which aspects of form to associate with which aspects of meaning ? The goal of this chapter is to answer the following questions : . How can we identify particular features of language data that are salient for classifying it ? How can we construct models of language that can be used to perform language processing tasks automatically ? What can we learn about language from these models ? Along the way we will study some important machine learning techniques , including decision trees , naive Bayes ' classifiers , and maximum entropy classifiers . We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) . Before looking at these methods , we first need to appreciate the broad scope of this topic . 1 Supervised Classification . Classification is the task of choosing the correct class label for a given input . In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance . Some examples of classification tasks are : . Deciding whether an email is spam or not . Deciding what the topic of a news article is , from a fixed list of topic areas such as \\\" sports , \\\" \\\" technology , \\\" and \\\" politics . \\\" Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution . The basic classification task has a number of interesting variants . For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified . A classifier is called supervised if it is built based on training corpora containing the correct label for each input . The framework used by supervised classification is shown in 1.1 . Figure 1.1 : Supervised Classification . ( a ) During training , a feature extractor is used to convert each input value to a feature set . These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section . Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model . ( b ) During prediction , the same feature extractor is used to convert unseen inputs to feature sets . These feature sets are then fed into the model , which generates predicted labels . In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks . Our discussion is not intended to be comprehensive , but to give a representative sample of tasks that can be performed with the help of text classifiers . 1.1 Gender Identification . In 4 we saw that male and female names have some distinctive characteristics . Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male . Let 's build a classifier to model these differences more precisely . The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features . For this example , we 'll start by just looking at the final letter of a given name . The following feature extractor function builds a dictionary containing relevant information about a given name : . The returned dictionary , known as a feature set , maps from feature names to their values . Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' . Feature values are values with simple types , such as booleans , numbers , and strings . Note . Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings . But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute . Indeed , it is even possible to use very complex and informative values , such as the output of a second supervised classifier , as features . Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels . Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set . The training set is used to train a new \\\" naive Bayes \\\" classifier . We will learn more about the naive Bayes classifier later in the chapter . For now , let 's just test it out on some names that did not appear in its training data : . Observe that these character names from The Matrix are correctly classified . Although this science fiction movie is set in 2199 , it still conforms with our expectations about names and genders . We can systematically evaluate the classifier on a much larger quantity of unseen data : . Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : . This listing shows that the names in the training set that end in \\\" a \\\" are female 33 times more often than they are male , but names that end in \\\" k \\\" are male 32 times more often than they are female . These ratios are known as likelihood ratios , and can be useful for comparing different feature - outcome relationships . Note . Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative . Retrain the classifier with these new features , and test its accuracy . When working with large corpora , constructing a single list that contains the features of every instance can use up a large amount of memory . In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2 Choosing The Right Features . Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model . Much of the interesting work in building a classifier is deciding what features might be relevant , and how we can represent them . Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand . Typically , feature extractors are built through a process of trial - and - error , guided by intuitions about what information is relevant to the problem . It 's common to start with a \\\" kitchen sink \\\" approach , including all the features that you can think of , and then checking to see which features actually are helpful . We take this approach for name gender features in 1.2 . def gender_features2 ( name ) : . lower ( ) . lower ( ) for letter in ' abcdefghijklmnopqrstuvwxyz ' : . count(letter ) . return features . Example 1.2 ( code_gender_features_overfitting . py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features . The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus . This problem is known as overfitting , and can be especially problematic when working with small training sets . Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis . First , we select a development set , containing the corpus data for creating the model . This development set is then subdivided into the training set and the dev - test set . The training set is used to train the model , and the dev - test set is used to perform error analysis . The test set serves in our final evaluation of the system . For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set . The division of the corpus data into different subsets is shown in 1.3 . Figure 1.3 : Organization of corpus data for training supervised classifiers . The corpus data is divided into two sets : the development set , and the test set . The development set is often further subdivided into a training set and a dev - test set . Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set . Using the dev - test set , we can generate a list of the errors that the classifier makes when predicting name genders : . We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) . The feature set can then be adjusted accordingly . The names classifier that we have built generates about 100 errors on the dev - test corpus : . Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders . For example , names ending in yn appear to be predominantly female , despite the fact that names ending in n tend to be male ; and names ending in ch are usually male , even though names that end in h tend to be female . We therefore adjust our feature extractor to include features for two - letter suffixes : . Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : . This error analysis procedure can then be repeated , checking for patterns in the errors that are made by the newly improved classifier . Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set . But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data . It is therefore important to keep the test set separate , and unused , until our model development is complete . At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification . In 1 , we saw several examples of corpora where documents have been labeled with categories . Using these corpora , we can build classifiers that will automatically tag new documents with appropriate category labels . First , we construct a list of documents , labeled with the appropriate categories . For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative . words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews . Next , we define a feature extractor for documents , so the classifier will know which aspects of the data it should pay attention to ( 1.4 ) . For document topic identification , we can define a feature for each word , indicating whether the document contains that word . To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus . We can then define a feature extractor that simply checks whether each of these words is present in a given document . words ( ) ) . return features . words ( ' pos / cv957_8737 . The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) . Now that we 've defined our feature extractor , we can use it to train a classifier to label new movie reviews ( 1.5 ) . To check how reliable the resulting classifier is , we compute its accuracy on the test set . And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging . In 5 . we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word . However , this regular expression tagger had to be hand - crafted . Instead , we can train a classifier to work out which suffixes are most informative . Let 's begin by finding out what the most common suffixes are : . Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features . Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties . The classifier will rely exclusively on these highlighted properties when determining how to label inputs . In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has . Now that we 've defined our feature extractor , we can use it to train a new \\\" decision tree \\\" classifier ( to be discussed in 4 ): . One nice feature of decision tree models is that they are often fairly easy to interpret - we can even instruct NLTK to print them out as pseudocode : . if endswith ( . ) Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \\\" , \\\" . Next , the classifier checks if the word ends in \\\" the \\\" , in which case it 's almost certainly a determiner . This \\\" suffix \\\" gets used early by the decision tree because the word \\\" the \\\" is so common . Continuing on , the classifier checks if the word ends in \\\" s \\\" . 1.5 Exploiting Context . By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix . However , as long as the feature extractor just looks at the target word , we have no way to add features that depend on the context that the word appears in . But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \\\" fly , \\\" knowing that the previous word is \\\" a \\\" will allow us to determine that it is functioning as a noun , not a verb . In order to accommodate features that depend on a word 's context , we must revise the pattern that we used to define our feature extractor . Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word . This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features . Example 1.6 ( code_suffix_pos_tag . py ) : Figure 1.6 : A part - of - speech classifier whose feature detector examines the context in which a word appears in order to determine which part of speech tag should be assigned . In particular , the identity of the previous word is included as a feature . It is clear that exploiting contextual features improves the performance of our part - of - speech tagger . For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \\\" large \\\" or the word \\\" gubernatorial \\\" . However , it is unable to learn the generalization that a word is probably a noun if it follows an adjective , because it does n't have access to the previous word 's part - of - speech tag . In general , simple classifiers always treat each input as independent from all other inputs . In many contexts , this makes perfect sense . For example , decisions about whether names tend to be male or female can be made on a case - by - case basis . However , there are often cases , such as part - of - speech tagging , where we are interested in solving classification problems that are closely related to one another . 1.6 Sequence Classification . In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs . In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence . One sequence classification strategy , known as consecutive classification or greedy sequence classification , is to find the most likely class label for the first input , then to use that answer to help find the best label for the next input . The process can then be repeated until all of the inputs have been labeled . This strategy is demonstrated in 1.7 . First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far . Each tag in history corresponds with a word in sentence . But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word . Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) . Having defined a feature extractor , we can proceed to build our sequence classifier . During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk . TaggerI ) : def _ _ init _ _ ( self , train_sents ) : . train_set . append ( ( featureset , tag ) ) . history.append(tag ) . history.append(tag ) . return zip(sentence , history ) . 1.7 Other Methods for Sequence Classification . One shortcoming of this approach is that we commit to every decision that we make . For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake . One solution to this problem is to adopt a transformational strategy instead . Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs . The Brill tagger , described in ( 1 ) , is a good example of this strategy . Another solution is to assign scores to all of the possible sequences of part - of - speech tags , and to choose the sequence whose overall score is highest . This is the approach taken by Hidden Markov Models . Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags . However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags . These probabilities are then combined to calculate probability scores for tag sequences , and the tag sequence with the highest probability is chosen . Unfortunately , the number of possible tag sequences is quite large . Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence . In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) . Given that restriction , it is possible to use dynamic programming ( 4.7 ) to efficiently find the most likely tag sequence . In particular , for each consecutive word index i , a score is computed for each possible current and previous tag . This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences . 2 Further Examples of Supervised Classification . 2.1 Sentence Segmentation . Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence . The first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features : . Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens . Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' : tokens[i-1]. Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : . To use this classifier to perform sentence segmentation , we simply check each punctuation mark to see whether it 's labeled as a boundary ; and divide the list of words at the boundary marks . The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) . return sents . 2.2 Identifying Dialogue Act Types . When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker . This interpretation is most straightforward for performative statements such as \\\" I forgive you \\\" or \\\" I bet you ca n't climb that hill . \\\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions . Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation . The NPS Chat Corpus , which was demonstrated in 1 , consists of over 10,000 posts from instant messaging sessions . These posts have all been labeled with one of 15 dialogue act types , such as \\\" Statement , \\\" \\\" Emotion , \\\" \\\" ynQuestion \\\" , and \\\" Continuer . \\\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts . The first step is to extract the basic messaging data . We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : . Next , we 'll define a simple feature extractor that checks what words the post contains : . Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment . Recognizing textual entailment ( RTE ) is the task of determining whether a given piece of text T entails another text called the \\\" hypothesis \\\" ( as already discussed in 5 ) . To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams . Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset . The label True indicates that the entailment holds , and False , that it fails to hold . Challenge 3 , Pair 34 ( True ) . T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism . H : China is a member of SCO . Challenge 3 , Pair 81 ( False ) . T : According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart . H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory . It should be emphasized that the relationship between text and hypothesis is not intended to be logical entailment , but rather whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true . We can treat RTE as a classification task , in which we try to predict the True / False label for each pair . Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level . In the ideal case , we would expect that if there is an entailment , then all the information expressed by the hypothesis should also be present in the text . Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment . Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) . In addition , some high frequency function words are filtered out as \\\" stopwords \\\" . def rte_features ( rtepair ) : . return features . Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \\\" Recognizing Text Entailment \\\" Feature Extractor . The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference . To illustrate the content of these features , we examine some attributes of the text / hypothesis Pair 34 shown earlier : . These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True . The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these . Although this figure is not very impressive , it requires significant effort , and more linguistic processing , to achieve much better results . 2.4 Scaling Up to Large Datasets . Python provides an excellent environment for performing basic text processing and feature extraction . If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages . Once these packages have been installed , NLTK can transparently invoke them ( via system calls ) to train classifier models significantly faster than the pure - Python classifier implementations . See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK . 3 Evaluation . In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model . The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it . Evaluation can also be an effective tool for guiding us in making future improvements to the model . 3.1 The Test Set . Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs . This test set typically has the same format as the training set . When building the test set , there is often a trade - off between the amount of data available for testing and the amount available for training . For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances . But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times . Additionally , if the test set contains many closely related instances - such as instances drawn from a single document - then the size of the test set should be increased to ensure that this lack of diversity does not skew the evaluation results . When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation . Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set . The more similar these two datasets are , the less confident we can be that evaluation results will generalize to other datasets . For example , consider the part - of - speech tagging task . At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : . In this case , our test set will be very similar to our training set . The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres . What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training . If there is any consistent pattern within a document - say , if a given word appears with a particular part - of - speech tag especially frequently - then that difference will be reflected in both the development set and the test set . A somewhat better approach is to ensure that the training set and test set are taken from different documents : . If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : . If we build a classifier that performs well on this test set , then we can be confident that it has the power to generalize well beyond the data that it was trained on . 3.2 Accuracy . The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled . The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 . When interpreting the accuracy score of a classifier , it is important to take into consideration the frequencies of the individual class labels in the test set . For example , consider a classifier that determines the correct word sense for each occurrence of the word bank . If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 . In that case , an accuracy of 95 % would hardly be impressive , since we could achieve that accuracy with a model that always returns the financial - institution sense . However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result . ( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall . Another instance where accuracy scores can be misleading is in \\\" search \\\" tasks , such as information retrieval , where we are attempting to find documents that are relevant to a particular task . Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % . It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : . True positives are relevant items that we correctly identified as relevant . True negatives are irrelevant items that we correctly identified as irrelevant . False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant . False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant . Given these four numbers , we can define the following metrics : . Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) . Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) . The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \\u00d7 Precision \\u00d7 Recall ) / ( Precision + Recall ) . 3.4 Confusion Matrices . When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made . A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i . In the following example , we generate a confusion matrix for the bigram tagger developed in 4 : . The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) . Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets . XXX explain use of \\\" reference \\\" in the legend above . 3.5 Cross - Validation . In order to evaluate our models , we must reserve a portion of the annotated data for the test set . As we already mentioned , if the test set is too small , then our evaluation may not be accurate . However , making the test set larger usually means making the training set smaller , which can have a significant impact on performance if a limited amount of annotated data is available . One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation . In particular , we subdivide the original corpus into N subsets called folds . For each of these folds , we train a model using all of the data except the data in that fold , and then test that model on the fold . Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable . A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets . If we get very similar scores for all N training sets , then we can be fairly confident that the score is accurate . On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score . 4 Decision Trees . In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers . As we 've seen , it 's possible to treat these learning methods as black boxes , simply training models and using them for prediction without understanding how they work . But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set . An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded . And an understanding of the generated models can allow us to extract information about which features are most informative , and how those features relate to one another . A decision tree is a simple flowchart that selects labels for input values . This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels . To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node . This node contains a condition that checks one of the input value 's features , and selects a branch based on that feature 's value . Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features . We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value . 4.1 shows an example decision tree model for the name gender task . Once we have a decision tree , it is straightforward to use it to assign labels to new input values . What 's less straightforward is how we can build a decision tree that models a given training set . But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \\\" decision stump \\\" for a corpus . A decision stump is a decision tree with a single node that decides how to classify inputs based on a single feature . It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value . In order to build a decision stump , we must first decide which feature should be used . The simplest method is to just build a decision stump for each possible feature , and see which one achieves the highest accuracy on the training data , although there are other alternatives that we will discuss below . Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) . Given the algorithm for choosing decision stumps , the algorithm for growing larger decision trees is straightforward . We begin by selecting the overall best decision stump for the classification task . We then check the accuracy of each of the leaves on the training set . Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf . 4.1 Entropy and Information Gain . As was mentioned before , there are several methods for identifying the most informative feature for a decision stump . One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature . To measure how disorganized the original set of input values are , we calculate entropy of their labels , which will be high if the input values have highly varied labels , and low if many input values all have the same label . In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : . Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male . For example , 4.2 shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names . Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low . In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2 P(l ) is small ) . On the other hand , if the input values have a wide variety of labels , then there are many labels with a \\\" medium \\\" frequency , where neither P(l ) nor log 2 P(l ) is small , so the entropy is high . 4.3 demonstrates how to calculate the entropy of a list of labels . import math def entropy ( labels ) : . Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump . To do so , we calculate the entropy for each of the decision stump 's leaves , and take the average of those leaf entropy values ( weighted by the number of samples in each leaf ) . The information gain is then equal to the original entropy minus this new , reduced entropy . The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain . Another consideration for decision trees is efficiency . The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree . A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples . Decision trees have a number of useful qualities . To begin with , they 're simple to understand , and easy to interpret . This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features . Decision trees are especially well suited to cases where many hierarchical categorical distinctions can be made . For example , decision trees can be very effective at capturing phylogeny trees . However , decision trees also have a few disadvantages . One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small . As a result , these lower decision nodes may . overfit the training set , learning patterns that reflect idiosyncrasies of the training set rather than linguistically significant patterns in the underlying problem . One solution to this problem is to stop dividing nodes once the amount of training data becomes too small . Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test . A second problem with decision trees is that they force features to be checked in a specific order , even when features may act relatively independently of one another . For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are . Since there is limited space near the top of the decision tree , most of these features will need to be repeated on many different branches in the tree . And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large . A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label . Since these features make relatively small incremental improvements , they tend to occur very low in the decision tree . But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have . If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label . The fact that decision trees require that features be checked in a specific order limits their ability to exploit features that are relatively independent of one another . The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \\\" in parallel . \\\" 5 Naive Bayes Classifiers . In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value . To choose a label for an input value , the naive Bayes classifier begins by calculating the prior probability of each label , which is determined by checking frequency of each label in the training set . The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label . The label whose likelihood estimate is the highest is then assigned to the input value . 5.1 illustrates this process . Figure 5.1 : An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document . In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \\\" automotive \\\" label . But it then considers the effect of each feature . In this example , the input document contains the word \\\" dark , \\\" which is a weak indicator for murder mysteries , but it also contains the word \\\" football , \\\" which is a strong indicator for sports documents . After every feature has made its contribution , the classifier checks which label it is closest to , and assigns that label to the input . Individual features make their contribution to the overall decision by \\\" voting against \\\" labels that do n't occur with that feature very often . In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature . The overall effect will be to reduce the score of the murder mystery label slightly more than the score of the sports label , and to significantly reduce the automotive label with respect to the other two labels . This process is illustrated in 5.2 and 5.3 . Figure 5.2 : Calculating label likelihoods with naive Bayes . Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data . Every feature then contributes to the likelihood estimate for each label , by multiplying it by the probability that input values with that label will have that feature . The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent . 5.1 Underlying Probabilistic Model . Of course , this assumption is unrealistic ; features are often highly dependent on one another . We 'll return to some of the consequences of this assumption at the end of this section . This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another . Figure 5.3 : A Bayesian Network Graph illustrating the generative process that is assumed by the naive Bayes classifier . To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label . Every feature is assumed to be entirely independent of every other feature , given the label . Note . If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing . However , this simple approach can become problematic when a feature never occurs with a given label in the training set . Thus , the input will never be assigned this label , regardless of how well the other features fit the label . In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur . For example , we may not have seen any murder mystery documents that contained the word \\\" football , \\\" but we would n't want to conclude that it 's completely impossible for such documents to exist . For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities . The nltk.probability module provides support for a wide variety of smoothing techniques . 5.3 Non - Binary Features . We have assumed here that each feature is binary , i.e. that each input either has a feature or does not . Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \\\" color - is - red \\\" . Another alternative is to use regression methods to model the probabilities of numeric features . 5.4 The Naivete of Independence . The reason that naive Bayes classifiers are called \\\" naive \\\" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) . In particular , almost all real - world problems contain features with varying degrees of dependence on one another . If we had to avoid any features that were dependent on one another , it would be very difficult to construct good feature sets that provide the required information to the machine learning algorithm . So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ? One problem that arises is that the classifier can end up \\\" double - counting \\\" the effect of highly correlated features , pushing the classifier closer to a given label than is justified . To see how this can occur , consider a name gender classifier that contains two identical features , f 1 and f 2 . In other words , f 2 is an exact copy of f 1 , and contains no new information . When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose . Thus , the information content of these two features will be given more weight than it deserves . Of course , we do n't usually build naive Bayes classifiers that contain two identical features . However , we do build classifiers that contain features which are dependent on one another . For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature . For features like these , the duplicated information may be given more weight than is justified by the training set . 5.5 The Cause of Double - Counting . The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined . One solution , therefore , is to consider the possible interactions between feature contributions during training . We could then use those interactions to adjust the contributions that individual features make . To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : . Here , w[label ] is the \\\" starting score \\\" for a given label , and w[f , label ] is the contribution made by a given feature towards a label 's likelihood . We call these values w[label ] and w[f , label ] the parameters or weights for the model . Using the naive Bayes algorithm , we set each of these parameters independently : . However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values . 6 Maximum Entropy Classifiers . The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier . But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier . In particular , it looks for the set of parameters that maximizes the total likelihood of the training corpus , which is defined as : . Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set . Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution . These iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values , but do not necessarily provide a means of determining when those optimal values have been reached . Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn . This is especially true when the size of the training set , the number of features , and the number of labels are all large . Note . Some iterative optimization techniques are much faster than others . When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods . 6.1 The Maximum Entropy Model . The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier . Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label . The naive Bayes classifier model defines a parameter for each label , specifying its prior probability , and a parameter for each ( feature , label ) pair , specifying the contribution of individual features towards a label 's likelihood . In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters . In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label . This will sometimes allow the model to \\\" generalize \\\" over some of the differences between related labels or features . Each combination of labels and features that receives its own parameter is called a joint - feature . Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values . Note . In literature that describes and discusses Maximum Entropy models , the term \\\" features \\\" often refers to joint - features ; the term \\\" contexts \\\" refers to what we have been calling ( simple ) features . Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model . In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] . Given the joint - features for a Maximum Entropy model , the score assigned to a label for a given input is simply the product of the parameters associated with the joint - features that apply to that input and label : . 6.2 Maximizing Entropy . The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions . An example will help to illustrate this principle . Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) . At first , we are not told anything more about the word or the senses . There are many probability distributions that we could choose for the ten senses , such as : . Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other . On the other hand , distributions ( ii ) and ( iii ) reflect assumptions that are not supported by what we know . One way to capture this intuition that distribution ( i ) is more \\\" fair \\\" than the other two is to invoke the concept of entropy . In the discussion of decision trees , we described entropy as a measure of how \\\" disorganized \\\" a set of labels was . In particular , if a single label dominates then entropy is low , but if the labels are more evenly distributed then entropy is high . In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high . In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest . Next , suppose that we are told that sense A appears 55 % of the time . Once again , there are many distributions that are consistent with this new piece of information , such as : . But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) . Finally , suppose that we are told that the word \\\" up \\\" appears in the nearby context 10 % of the time , and that when it does appear in the context there 's an 80 % chance that sense A or C will be used . In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : . Furthermore , the remaining probabilities appear to be \\\" evenly distributed . \\\" Throughout this example , we have restricted ourselves to distributions that are consistent with what we know ; among these , we chose the distribution with the highest entropy . This is exactly what the Maximum Entropy classifier does as well . In particular , for each joint - feature , the Maximum Entropy model calculates the \\\" empirical frequency \\\" of that feature - i.e. , the frequency with which it occurs in the training set . It then searches for the distribution which maximizes entropy , while still predicting the correct frequency for each joint - feature . 6.3 Generative vs Conditional Classifiers . An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer . The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair . As a result , generative models can be used to answer the following questions : . How likely is a given input value with a given label ? What is the most likely label for an input that might have one of two values ( but we do n't know which ) ? The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier . Thus , conditional models can still be used to answer questions 1 and 2 . However , conditional models can not be used to answer the remaining questions 3 - 6 . However , this additional power comes at a price . Because the model is more powerful , it has more \\\" free parameters \\\" which need to be learned . However , the size of the training set is fixed . Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values . As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions . However , if we do need answers to questions like 3 - 6 , then we have no choice but to use a generative model . The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline . Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline . 7 Modeling Linguistic Patterns . Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns . Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models . Either way , these explicit models serve two important purposes : they help us to understand linguistic patterns , and they can be used to make predictions about new language data . The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used . Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another . Other models , such as multi - level neural networks , are much more opaque . Although it can be possible to gain insight by studying them , it typically takes a lot more work . But all explicit models can make predictions about new \\\" unseen \\\" language data that was not included in the corpus used to build the model . These predictions can be evaluated to assess the accuracy of the model . Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data . These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering . 7.1 What do models tell us ? It 's important to understand what we can learn about language from an automatically constructed model . One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models . Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns . For example , as we saw in 3.1 , the synonyms absolutely and definitely are not interchangeable : we say absolutely adore not definitely adore , and definitely prefer not absolutely prefer . In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns . For example , we might introduce the abstract concept of \\\" polar verb \\\" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar . Our explanatory model would contain the constraint that absolutely can only combine with polar verbs , and definitely can only combine with non - polar verbs . In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships . Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another . If our goal is to understand the linguistic patterns , then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns . 8 Summary . Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data . Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input . Supervised classifiers can perform a wide variety of NLP tasks , including document classification , part - of - speech tagging , sentence segmentation , dialogue act type identification , and determining entailment relations , and many other tasks . When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance . When evaluating a supervised classifier , it is important that you use fresh data , that was not included in the training or dev - test set . Otherwise , your evaluation results may be unrealistically optimistic . Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features . Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label . In naive Bayes classifiers , each feature independently contributes to the decision of which label should be used . This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another . Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set . Most of the models that are automatically constructed from a corpus are descriptive - they let us know which features are relevant to a given patterns or construction , but they do n't give any information about causal relationships between those features and patterns . 9 Further Reading . Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python . For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) . Examples of these challenge competitions include CoNLL Shared Tasks , the ACE competitions , the Recognizing Textual Entailment competitions , and the AQUAINT competitions . 10 Exercises . Find out what type and quantity of annotated data is required for developing such systems . Why do you think a large amount of data is required ? Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set . Then , starting with the example name gender classifier , make incremental improvements . Use the dev - test set to check your progress . Once you are satisfied with your classifier , check its final performance on the test set . How does the performance on the test set compare to the performance on the dev - test set ? Is this what you 'd expect ? It contains data for four words : hard , interest , line , and serve . Choose one of these four words , and load the corresponding data : . Using this dataset , build a classifier that predicts the correct sense tag for a given instance . Can you explain why these particular features are informative ? Do you find any of them surprising ? Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier . Compare the performance of the three classifiers on your selected task . How do you think that your results might be different if you used a different feature extractor ? What features are relevant in this distinction ? Build a classifier that predicts when each word should be used . However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others . For example , a ynQuestion dialog act is much more likely to be answered by a yanswer than by a greeting . Make use of this fact to build a consecutive classifier for labeling dialog acts . Be sure to consider what features might be useful . See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas . However , many words occur very infrequently , and some of the most informative words in a document may never have occurred in our training data . One solution is to make use of a lexicon , which describes how different words relate to one another . Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data . Each instance in the corpus is encoded as a PPAttachment object : . Select only the instances where inst.attachment is N : . Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns . For example , given the pair of nouns \\\" team \\\" and \\\" researchers , \\\" the classifier should predict the preposition \\\" of \\\" . Explore this issue by looking at corpus data ; writing programs as needed . \"}",
        "_version_":1692668188415754240,
        "score":20.184698},
      {
        "id":"ac717a6a-2e7b-4a9f-ba89-425864ab9974",
        "_src_":"{\"url\": \"http://www.peoplesworld.org/toussaint-released-contract-battle-continues/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146600.56/warc/CC-MAIN-20160205193906-00255-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"HOW - TO GUIDE : Installing and running the Joshua Decoder . Note : these instructions are several years out of date . This document gives instructions on how to install and use the Joshua decoder . Joshua is an open - source decoder for parsing - based machine translation . Joshua uses the synchronous context free grammar ( SCFG ) formalism in its approach to statistical machine translation , and the software implements the algorithms that underly the approach . The Berkeley Aligner - this software is used to align words across sentence pairs in a bilingual parallel corpus . Word alignment takes place before extracting an SCFG . After you have downloaded the srilm tar file , type the following commands to install it : . mkdir srilm mv srilm.tgz srilm/ cd srilm/ tar xfz srilm.tgz make . If the build fails , please follow the instructions in SRILM 's INSTALL file . After you successfully compile SRILM , Joshua will need to know what directory it is in . You can type pwd to get the absolute path to the sirlm/ directory that you created . Once you 've figured out the path , set an SRILM environment variable by typing : . Where \\\" /path / to / srilm \\\" is replaced with your path . You 'll also need to set a JAVA_HOME environment variable . For Mac OS X this usually is done by typing : . These variables will need to be set every time you use Joshua , so it 's useful to add them to your . bashrc , . bash_profile or . profile file . Download and Install Joshua . Running ant will compile the Java classes and link in srilm . If everything works properly , you should see the message BUILD SUCCESSFUL . If you get a BUILD FAILED message , it may be because you have not properly set the paths to SRILM and JAVA_HOME , or because srilm was not compiled properly , as described above . For the examples in this document , you will need to set a JOSHUA environment variable : . Run the example model . To test to make sure that the decoder is installed properly , we 'll translate 5 sentences using a small translation model that loads quickly . The sentences that we will translate are contained in example / example . test.in . The small translation grammar contains 15,939 rules -- you can get the count of the number of rules by running gunzip -c example / example . The first part of the rule is the left - hand side non - terminal . The second and third parts are the right - hand side . The three numbers listed after each translation rules are negative log probabilities that signify , in order : . You can use the grammar to translate the test set by running . config.srilm \\\\ example / example . test.in \\\\ example / example . nbest.srilm.out . For those of you who are n't very familiar with Java , the arguments are the following : . -Xmx1 g -- this tells Java to use 1 GB of memory . -cp $ JOSHUA / bin -- this specifies the directory that contains the Java class files . joshua.decoder.JoshuaDecoder -- This is the class that is run . If you want to look at the the source code for this class , you can find it in src / joshua / decoder / JoshuaDecoder.java . example / example . config.srilm -- This is the configuration file used by Joshua . example / example . test.in -- This is the input file containing the sentences to translate . example / example . nbest.srilm.out -- This is the output file that the n - best translations will be written to . You can inspect the output file by typing head example / example . nbest.srilm.out . This file contains the n - best translations , under the model . The first 10 lines that you see above are 10 best translations of the first sentence . Each line contains 4 fields . To get the 1-best translations for each sentence in the test set without all of the extra information , you can run the following command : . nbest.srilm.out \\\\ example / example . nbest.srilm.out.1best . You cat then look at the 1-best output file by typing cat example / example . nbest.srilm.out.1best : . the goal of gene scientists is to provide diagnostic tools to found of the flawed genes , are still provide a to stop these genes treatments . If your translations are identical to the ones above then Joshua is installed correctly . With this small model , there are many untranslated words , and the quality of the translations is very low . In the next steps , we 'll show you how to train a model for a new language pair , using a larger training corpus that will result in higher quality translations . Once you 've gathered your data , you will need to do several preprocess steps : sentence alignment , tokenization , normalization , and subsampling . Sentence alignment . In this exercise , we 'll start with an existing sentence - aligned parallel corpus . Download this tarball , which contains a Spanish - Engish parallel corpus , along with a dev and a test set : data.tar.gz . The data tarball contains two training directories training/ , which includes a subset of the corpus , and full - training , which includes the full corpus . I strongly recommend staring with the smaller set , and building an end - to - end system with it , since many steps take a very long time on the full data set . You should debug on the smaller set to avoid wasting time . Tokenization . Joshua uses whitespace to delineate words . For many languages , tokenization can be as simple as separating punctation off as its own token . For languages like Chinese , which do n't put spaces around words , tokenization can be more tricky . For this example we 'll use the simple tokenizer that is released as part of the WMT . It 's located in the tarball under the scripts directory . To use it type the following commands : . en.tok . Normalization . After tokenization , we recommend that you normalize your data by lowercasing it . The system treats words with variant capitalization as distinct , which can lead to worse probability estimates for their translation , since the counts are fragmented . For other languages you might want to normalize the text in other ways . You can lowercase your tokenized data with the following script : . cat es - en / full - training / training . en.tok.lc cat es - en / full - training / training . es.tok.lc . Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . Although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . After tokenization and lowercasing , the file looks like this ( head -3 es - en / full - training / training . en.tok.lc ): . resumption of the session i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period . although , as you will have seen , the dreaded ' millennium bug ' failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful . You must preprocess your dev and test sets in the same way you preprocess your training data . Run the following commands on the data that you downloaded : . cat es - en / dev / news - dev2009 . es.tok.lc cat es - en / dev / news - dev2009 . en.tok.lc cat es - en / test / newstest2009 . es.tok.lc cat es - en / test / newstest2009 . en.tok.lc . Subsampling ( optional ) . Sometimes the amount of training data is so large that it makes creating word alignments extremely time - consuming and memory - intesive . We therefore provide a facility for subsampling the training corpus to select sentences that are relevant for a test set . es.tok.lc es - en / test / newstest2009 . You can see how much the subsampling step reduces the training data , by yping wc -lw es - en / full - training / training . tok.lc es - en / full - training / subsampled / subsample . tok.lc : . 1411589 39411018 training / training . en.tok.lc 1411589 41042110 training / training . es.tok.lc 671429 16721564 training / subsampled / subsample . en.tok.lc 671429 17670846 training / subsampled / subsample . es.tok.lc . Step 3 : Create word alignments . Before extracting a translation grammar , we first need to create word alignments for our parallel corpus . In this example , we show you how to use the Berkeley aligner . You may also use Giza++ to create the alignments , although that program is a little unwieldy to install . To run the Berkeley aligner you first need to set up a configuration file , which defines the models that are used to align the data , how the program runs , and which files are to be aligned . Here is an example configuration file ( you should create your own version of this file and save it as training / word - align . conf ): . # # word - align . conf # # ---------------------- # # This is an example training script for the Berkeley # # word aligner . In this configuration it uses two HMM # # alignment models trained jointly and then decoded # # using the competitive thresholding heuristic . # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Training : Defines the training regimen # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # forwardModels MODEL1 HMM reverseModels MODEL1 HMM mode JOINT JOINT iters 5 5 # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # Execution : Controls output and program flow # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # execDir alignments create saveParams true numThreads 1 msPerLine 10000 alignTraining # # # # # # # # # # # # # # # # # # Language / Data # # # # # # # # # # # # # # # # # foreignSuffixes.tok.lc englishSuffixen.tok.lc . # Choose the training sources , which can either be directories or files that list files / directories trainSourcessubsampled/ . sentencesMAX . # 1-best output . competitiveThresholding . To run the Berkeley aligner , first set an environment variable saying where the aligner 's jar file is located ( this environment variable is just used for convenience in this document , and is not necessary for running the aligner in general : . You 'll need to create an empty directory called example / test . This is because the Berkeley aligner generally expects to test against a set of manually word - aligned data : . cd es - en / full - training/ mkdir -p example / test . After you 've created the word - align . config file , you can run the aligner with this command : . nohup java -d64 -Xmx10 g -jar $ BERKELEYALIGNER / berkeleyaligner . jar + + word - align . conf & . If the program finishes right away , then it probably terminated with an error . You can read the nohup.out file to see what went wrong . Common problems include a missing example / test directory , or a file not found exception . When you re - run the program , you will need to manually remove the alignments/ directory . When you are aligning tens of millions of words worth of data , the word alignment process will take several hours to complete . While it is running , you can skip ahead and complete step 4 , but not step 5 . Step 4 : Train a language model . Most translation models also make use of an n - gram language model as a way of assigning higher probability to hypothesis translations that look like fluent examples of the target language . Joshua provides support for n - gram language models , either through a built in data structure , or through external calls to the SRI language modeling toolkit ( srilm ) . To use large language models , we recommend srilm . If you successfully installed srilm in Step 1 , then you should be able to train a language model with the following command : . mkdir -p model / lm $ SRILM / bin / macosx64/ngram - count \\\\ -order 3 \\\\ -unk \\\\ -kndiscount1 -kndiscount2 -kndiscount3 \\\\ -text training / training . en.tok.lc \\\\ -lm model / lm / europarl . en.trigram.lm . ( Note : the above assumes that you are on a 64-bit machine running Mac OS X. If that 's not the case , your path to ngram - count will be slightly different . ) This will train a trigram language model on the English side of the parallel corpus . We use the . tok.lc file because it is important to have the input to the LM training be tokenized and normalized in the same way as the input data for word alignment and translation grammar extraction . The -order 3 tells srilm to produce a trigram language model . You can set this to a higher value , and srilm will happily output 4-gram , 5-gram or even higher order language models . The -kndiscount tells SRILM to use modified Kneser - Ney discounting as its smoothing scheme . Other smoothing schemes that are implemented in SRILM include Good - Turing and Witten - Bell . Given that the English side of the parallel corpus is a relatively small amount of data in terms of language modeling , it only takes a few minutes a few minutes to output the LM . The uncompressed LM is 144 megabytes large ( du -h europarl.en.trigram.lm ) . Step 5 : Extract a translation grammar . We 'll use the word alignments to create a translation grammar similar to the Chinese one shown in Step 1 . The translation grammar is created by looking for where the foreign language phrases from the test set occur in the training set , and then using the word alignments to figure out which foreign phrases are aligned . Create a suffix array index . To find the foreign phrases in the test set , we first create an easily searchable index , called a suffix array , for the training data . java -Xmx500 m -cp $ JOSHUA / bin/ \\\\ joshua.corpus.suffix_array.Compile \\\\ training / subsampled / subsample . es.tok.lc \\\\ training / subsampled / subsample . en.tok.lc \\\\ training / subsampled / training . en.tok.lc-es . tok.lc.align \\\\ model . This compiles the index that Joshua will use for its rule extraction , and puts it into a directory named model . Extract grammar rules for the dev set . The following command will extract a translation grammar from the suffix array index of your word - aligned parallel corpus , where the grammar rules apply to the foreign phrases in the dev set dev / news - dev2009 . es.tok.lc : . /model \\\\ mert / news - dev2009 . es.tok.lc.grammar.raw \\\\ dev / news - dev2009 . es.tok.lc & . Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u mert / news - dev2009 . es.tok.lc.grammar.raw \\\\ -o mert / news - dev2009 . es.tok.lc.grammar . You will also need to create a small \\\" glue grammar \\\" , in a file called model / hiero . glue that contains these rules that allow hiero - style grammars to reach the goal state : . Step 6 : Run minimum error rate training . After we 've extracted the grammar for the dev set we can run minimum error rate training ( MERT ) . MERT is a method for setting the weights of the different feature functions the translation model to maximize the translation quality on the dev set . Translation quality is calculated according to an automatic metric , such as Bleu . Our implementation of MERT allows you to easily implement some other metric , and optimize your paramters to that . There 's even a YouTube tutorial to show you how . A MERT configuration file . A separate file with the list of the feature functions used in your model , along with their possible ranges . Create a MERT configuration file . In this example we name the file mert / mert . config . Its contents are : . # # # MERT parameters # target sentences file name ( in this case , file name prefix ) -r dev / news - dev2009 . en.tok.lc -rps 1 # references per sentence -p mert / params . txt # parameter file -m BLEU 4 closest # evaluation metric and its options -maxIt 10 # maximum MERT iterations -ipi 20 # number of intermediate initial points per iteration -cmd mert / decoder_command # file containing commands to run decoder -decOut mert / news - dev2009 . output.nbest # file prodcued by decoder -dcfg mert / joshua . You can see a list of the other parameters available in our MERT implementation by running this command : . java -cp $ JOSHUA / bin joshua.zmert.ZMERT -h . Next , create a file called mert / params . txt that specifies what feature functions you are using in your mode . In our baseline model , this file should contain the following information : . Next , create a file called mert / decoder_command that contains the following command : . config \\\\ dev / news - dev2009 . es.tok.lc \\\\ mert / news - dev2009 . output.nbest . Next , create a configuration file for joshua at mert / joshua . config that contains the following : . es.tok.lc.grammar . glue . # lm config . # tm config . # pruning config . # nbest config . # remote lm server config , we should first prepare remote_symbol_tbl before starting any jobs . /voc.remote.sym . /remote.lm.server.list . # parallel deocoder : it can not be used together with remote lm . # # # # # # model weights . # lm order weight . lm 1.0 . # phrasemodel owner column(0-indexed ) weight . phrasemodel pt 0 1.4037585111897322 . phrasemodel pt 1 0.38379188013385945 . phrasemodel pt 2 0.47752204361625605 . # arityphrasepenalty owner start_arity end_arity weight . # arityphrasepenalty pt 0 0 1.0 . # arityphrasepenalty pt 1 2 -1.0 . # phrasemodel mono 0 0.5 . # wordpenalty weight . wordpenalty -2.721711092619053 . Finally , run the command to start MERT : . nohup java -cp $ JOSHUA / bin \\\\ joshua.zmert.ZMERT \\\\ -maxMem 1500 mert / mert . config & . While MERT is running , you can skip ahead to the first part of the next step and extract the grammar for the test set . Step 7 : Decode a test set . When MERT finishes , it will output a file mert / joshua . config . ZMERT.final that contains the news weights for the different feature functions . You can copy this config file and use it to decode the test set . Extract grammar rules for the test set . Before decoding the test set , you 'll need to extract a translation grammar for the foreign phrases in the test set test / newstest2009 . es.tok.lc : . /model \\\\ test / newstest2009 . es.tok.lc.grammar.raw \\\\ test / newstest2009 . es.tok.lc & . Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u test / newstest2009 . es.tok.lc.grammar.raw \\\\ -o test / newstest2009 . es.tok.lc.grammar . Once the grammar extraction has completed , you can edit the joshua.config file for the test set . cp mert / joshua . config . ZMERT.final test / joshua . config . es.tok.lc.grammar . After you have done that , you can decode the test set with the following command : . config \\\\ test / newstest2009 . es.tok.lc \\\\ test / newstest2009 . output.nbest . After the decoder has finished , you can extract the 1-best translations from the n - best list using the following command : . output.nbest \\\\ test / newstest2009 . output.1best . Step 8 : Recase and detokenize . You 'll notice that your output is all lowercased and has the punctuation split off . In order to make the output more readable to human beings ( remember us ? ) , it 'd be good to fix these problems and use proper punctuation and spacing . These are called recasing and detokenization , respectively . We can do recasing using SRILM , and can do detokenization with a perl script . To build a recasing model first train a language model on true cased English text : . $ SRILM / bin / macosx64/ngram - count \\\\ -unk \\\\ -order 5 \\\\ -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -kndiscount5 \\\\ -text training / training . en.tok \\\\ -lm model / lm / training . TrueCase.5gram.lm . Next , you 'll need to create a list of all of the alternative ways that each word can be capitalized . This will be stored in a map file that lists a lowercased word as the key and associates it with all of the variant capitalization of that word . Here 's an example perl script to create the map : . /usr / bin / perl # # truecase - map . cat training / training . map . Finally , recase the lowercased 1-best translation by running the SRILM disambig program , which takes the map of alternative capitalizations , creates a confusion network , and uses truecased LM to find the best path through it : . $ SRILM / bin / macosx / disambig \\\\ -lm model / lm / training . TrueCase.5gram.lm \\\\ -keep - unk \\\\ -order 5 \\\\ -map model / lm / true - case . map \\\\ -text test / mt09 . output.1best.recased . Where strip - sent - tags . perl is : . Step 9 : Score the translations . The quality of machine translation is commonly measured using the BLEU metric , which automatically compares a system 's output against reference human translations . You can score your output using the JoshuaEval class , Joshua 's built - in scorer : . en.output \\\\ -ref dev / dev2006 . en.small \\\\ -m BLEU 4 closest \"}",
        "_version_":1692668574655578112,
        "score":20.153507},
      {
        "id":"3339a7c3-a62e-46d8-b26c-bb1d3c5ec4d4",
        "_src_":"{\"url\": \"http://cyberinsecure.com/wordpress-doorway-spam-attacks/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454702039825.90/warc/CC-MAIN-20160205195359-00135-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Accepted papers . Conversational Speech Transcription Using Context - Dependent Deep Neural Networks . Dong Yu , Frank Seide , Gang Li . Abstract : Context - Dependent Deep - Neural - Network HMMs , or CD - DNN - HMMs , combine the classic artificial - neural - network HMMs with traditional context - dependent acoustic modeling and deep - belief - network pre - training . CD - DNN - HMMs greatly outperform conventional CD - GMM ( Gaussian mixture model ) HMMs : The word error rate is reduced by up to one third on the difficult benchmarking task of speaker - independent single - pass transcription of telephone conversations . Data - driven Web Design . Ranjitha Kumar , Jerry Talton , Salman Ahmad , Scott Klemmer . Abstract : This short paper summarizes challenges and opportunities of applying machine learning methods to Web design problems , and describes how structured prediction , deep learning , and probabilistic program induction can enable useful interactions for designers . We intend for these techniques to foster new work in data - driven Web design . Learning the Central Events and Participants in Unlabeled Text . Nathanael Chambers , Dan Jurafsky . Abstract : The majority of information on the Internet is expressed in written text . Understanding and extracting this information is crucial to building intelligent systems that can organize this knowledge . Today , most algorithms focus on learning atomic facts and relations . For instance , we can reliably extract facts like ' Annapolis is a City ' by observing redundant word patterns across a corpus . However , these facts do not capture richer knowledge like the way detonating a bomb is related to destroying a building , or that the perpetrator who was convicted must have been arrested . A structured model of these events and entities is needed for a deeper understanding of language . This talk describes unsupervised approaches to learning such rich knowledge . Exemplar - SVMs for Visual Ob ject Detection , Label Transfer and Image Retrieval . Tomasz Malisiewicz , Abhinav Shrivastava , Abhinav Gupta , Alexei Efros . While conventional wisdom tends to attribute the success of such methods to the ability of the classifier to generalize across the positive class instances , here we report on empirical findings suggesting that this might not necessarily be the case . We have experimented with a very simple idea : to learn a separate classifier for each positive object instance in the dataset . In this setup , no generalization across the positive instances is possible by definition , and yet , surprisingly , we did not observe any drastic drop in performance compared to the standard , category - based approaches . Capturing topical content with frequency and exclusivity . Jonathan Bischof , Edoardo Airoldi . HPC uses known hierarchical structure on human labeled topics to make focused comparisons of differential usage within each branch of the tree . We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation . TrueLabel + Confusions : A Spectrum of Probabilistic Models in Analyzing Multiple Ratings . Chao Liu , Yi - Min Wang . Abstract : This paper revisits the problem of analyzing multiple ratings given by different judges . Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings , we emphasize gaining diagnostic insights into our in - house well - trained judges . Robust Multiple Manifold Structure Learning . Dian Gong , Xuemei Zhao , Gerard Medioni . Abstract : We present a robust multiple manifold structure learning ( RMMSL ) scheme to robustly estimate data structures under the multiple low intrinsic dimensional manifolds assumption . In the local learning stage , RMMSL efficiently estimates local tangent space by weighted low - rank matrix factorization . In the global learning stage , we propose a robust manifold clustering method based on local structure learning results . The proposed clustering method is designed to get the flattest manifolds clusters by introducing a novel curved - level similarity function . Our approach is evaluated and compared to state - of - the - art methods on synthetic data , handwritten digit images , human motion capture data and motorbike videos . We demonstrate the effectiveness of the proposed approach , which yields higher clustering accuracy , and produces promising results for challenging tasks of human motion segmentation and motion flow learning from videos . Two Manifold Problems with Applications to Nonlinear System Identification . Byron Boots , Geoff Gordon . Abstract : Recently , there has been much interest in spectral approaches to learning manifolds - so - called kernel eigenmap methods . These methods have had some successes , but their applicability is limited because they are not robust to noise . To address this limitation , we look at two - manifold problems , in which we simultaneously reconstruct two related manifolds , each representing a different view of the same data . By solving these interconnected learning problems together , two - manifold algorithms are able to succeed where a non - integrated approach would fail : each view allows us to suppress noise in the other , reducing bias . We propose a class of algorithms for two - manifold problems , based on spectral decomposition of cross - covariance operators in Hilbert space and discuss when two - manifold problems are useful . Finally , we demonstrate that solving a two - manifold problem can aid in learning a nonlinear dynamical system from limited data . On the Difficulty of Nearest Neighbor Search . Junfeng He , Sanjiv Kumar , Shih - Fu Chang . Abstract : Fast approximate nearest neighbor search in large databases is becoming popular . Several powerful learning - based formulations have been proposed recently . However , not much attention has been paid to a more fundamental question : how difficult is ( approximate ) nearest neighbor search in a given data set ? More broadly , which data properties affect the nearest neighbor search and how ? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality , sparsity , and database size simultaneously in arbitrary normed metric spaces . To further justify why relative contrast is an important and effective measure , we present a theoretical analysis to prove how relative contrast determines / affects the performance / complexity of Locality Sensitive Hashing , a popular hashing based approximate nearest neighbor search method . Finally , relative contrast also provides an explanation for a family of heuristic hashing algorithms based on PCA with good practical performance . Learning Force Control Policies for Compliant Robotic Manipulation . Mrinal Kalakrishnan , Ludovic Righetti , Peter Pastor , Stefan Schaal . Abstract : Developing robots capable of fine manipulation skills is of major importance in order to build truly assistive robots . These robots need to be compliant in their actuation and control in order to operate safely in human environments . Manipulation tasks imply complex contact interactions with the external world , and involve reasoning about the forces and torques to be applied . Planning under contact conditions is usually impractical due to computational complexity , and a lack of precise dynamics models of the environment . We present an approach to acquiring manipulation skills on compliant robots through reinforcement learning . The initial position control policy for manipulation is initialized through kinesthetic demonstration . This policy is augmented with a force / torque profile to be controlled in combination with the position trajectories . The Policy Improvement with Path Integrals ( PI^2 ) algorithm is used to learn these force / torque profiles by optimizing a cost function that measures task success . We introduce a policy representation that ensures trajectory smoothness during exploration and learning . Our approach is demonstrated on the Barrett WAM robot arm equipped with a 6-DOF force / torque sensor on two different manipulation tasks : opening a door with a lever door handle , and picking up a pen off the table . We show that the learnt force control policies allow successful , robust execution of the tasks . Estimation of Simultaneously Sparse and Low Rank Matrices . Pierre - Andr\\u00e9 Savalle , Emile Richard , Nicolas Vayatis . Abstract : The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low - rank at the same time . Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block - diagonal in the appropriate basis . We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix . We bound generalization error in the link prediction problem . We also develop proximal descent strategies to solve the the optimization problem efficiently and evaluate performance on synthetic and real data sets . Online Structured Prediction via Coactive Learning . Pannaga Shivaswamy , Thorsten Joachims . Abstract : We propose Coactive Learning as a model of interaction between a learning system and a human user , where both have the common goal of providing results of maximum utility to the user . At each step , the system ( e.g. search engine ) receives a context ( e.g. query ) and predicts an object ( e.g. ranking ) . The user responds by correcting the system if necessary , providing a slightly improved - but not necessarily optimal - object as feedback . We argue that such feedback can be inferred from observable user behavior , specifically clicks in web search . We demonstrate the applicability of our model and learning algorithms on a movie recommendation task , as well as ranking for web search . Using CCA to improve CCA : A new spectral method for estimating vector models of words . Paramveer Dhillon , Jordan Rodu , Dean Foster , Lyle Ungar . Abstract : Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner . For example , for text applications where the words lie in a very high dimensional space ( the size of the vocabulary ) , one can learn a low rank \\\" dictionary \\\" by an eigen - decomposition of the word co - occurrence matrix ( e.g. using PCA or CCA ) . In this paper , we present a new spectral method based on CCA to learn an eigenword dictionary . Our improved procedure computes two set of CCAs , the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself . A Discrete Optimization Approach for Supervised Ranking with an Application to Reverse - Engineering Quality Ratings . Allison Chang , Cynthia Rudin , Dimitris Bertsimas , Michael Cavaretta , Robert Thomas , Gloria Chou . - Not for proceedings . Abstract : We present a new methodology based on mixed integer optimization ( MIO ) for supervised ranking tasks . Other methods for supervised ranking approximate ranking quality measures by convex functions in order to accommodate extremely large problems , at the expense of exact solutions . As our MIO approach provides exact modeling for ranking problems , our solutions are benchmarks for the other non - exact methods . We report computational results that demonstrate significant advantages for MIO methods over current state - of - the - art . We also use our technique for a new application : reverse - engineering quality rankings . A good or bad product quality rating can make or break an organization , and in order to invest wisely in product development , organizations are starting to use intelligent approaches to reverse - engineer the rating models . We present experiments on data from a major quality rating company , and provide new methods for evaluating the solution . In addition , we provide an approach to use the reverse - engineered model to achieve a top ranked product in a cost - effective way . Bounded Planning in Passive POMDPs . Roy Fox , Naftali Tishby . Abstract : In Passive POMDPs actions do not affect the world state , but still incur costs . When the agent is bounded by information - processing constraints , it can only keep an approximation of the belief . We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost , and introduce an efficient and simple algorithm for finding an optimum . Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss . Shai Ben - David , David Loker , Nathan Srebro , Karthik Sridharan . Abstract : We carefully study how well minimizing convex surrogate loss functions , corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors . In particular , we show that amongst all convex surrogate losses , the hinge loss gives essentially the best possible bound , of all convex loss functions , for the misclassification error rate of the resulting linear predictor in terms of the best possible margin error rate . We also provide lower bounds for specific convex surrogates that show how different commonly used losses qualitatively differ from each other . Bayesian Efficient Multiple Kernel Learning . Mehmet G\\u00f6nen . Abstract : Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources . Most of the previous research on such methods is focused on the computational efficiency issue . However , it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity . We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation , which allows us to combine hundreds or thousands of kernels very efficiently . We briefly explain how the proposed method can be extended for multiclass learning and semi - supervised learning . Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast , requiring less than a minute . On one bioinformatics and three image recognition data sets , our method outperforms previously reported results with better generalization performance . Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes . Murat Dundar , Ferit Akova , Alan Qi , Bartek Rajwa . Abstract : In this study we present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling . A Dirichlet process prior ( DPP ) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution . In an attempt to automatically discover potentially interesting class formations , the prior model is coupled with a suitably chosen data model , and sequential Monte Carlo sampling is used to perform online inference . Exact Soft Confidence - Weighted Learning . Steven C.H. Hoi , Jialei Wang , Peilin Zhao . Abstract : In this paper , we propose a new Soft Confidence - Weighted ( SCW ) online learning scheme , which enables the conventional confidence - weighted learning method to handle non - separable cases . Unlike the previous confidence - weighted learning algorithms , the proposed soft confidence - weighted learning method enjoys all the four salient properties : ( i ) large margin training , ( ii ) confidence weighting , ( iii ) capability to handle non - separable data , and ( iv ) adaptive margin . Our experimental results show that SCW performs significantly better than the original CW algorithm . When comparing with the state - of - the - art AROW algorithm , we found that SCW in general achieves better or at least comparable predictive performance , but enjoys considerably better efficiency performance ( i.e. , producing less number of updates and spending less time cost ) . Distributed Tree Kernels . Fabio Massimo Zanzotto , Lorenzo Dell'Arciprete . Abstract : In this paper , we propose the distributed tree kernels ( DTK ) as a novel method to reduce time and space complexity of tree kernels . Using a linear complexity algorithm to compute vectors for trees , we embed feature spaces of tree fragments in low - dimensional spaces where the kernel computation is directly done with dot product . We show that DTKs are faster , correlate with tree kernels , and obtain a statistically similar performance in two natural language processing tasks . Multiple Kernel Learning from Noisy Labels by Stochastic Programming . Tianbao Yang , Mehrdad Mahdavi , Rong Jin , Lijun Zhang , Yang Zhou , . Abstract : We study the problem of multiple kernel learning from noisy labels . This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples . Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments . We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem , and presenting a minimax formulation . We develop an efficient algorithm for solving the related convex - concave optimization problem with a fast convergence rate of O(1/T ) where T is the number of iterations . Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm . Improved Nystrom Low - rank Decomposition with Priors . Kai Zhang , Liang Lan , Jun Liu , andreas Rauber . Abstract : Low - rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amount of data . However , some limitations could prevent them from working effectively in certain domains . To solve these problems , in this paper we propose an \\\" inductive\\\"-flavored method for low - rank kernel decomposition with priors or side information . We achieve this by generalizing the Nystr\\u00f3m method in a novel way . Empirical results demonstrate the efficacy and efficiency of the proposed method . Active Learning for Matching Problems . Laurent Charlin , Rich Zemel , Craig Boutilier . Abstract : Effective learning of user preferences is critical to easing user burden in various types of matching problems . Equally important is active query selection to further reduce the amount of preference information users must provide . We address the problem of active learning of user preferences for matching problems , introducing a novel method for determining probabilistic matchings , and developing several new active learning strategies that are sensitive to the specific matching objective . Experiments with real - world data sets spanning diverse domains demonstrate that matching - sensitive active learning outperforms standard techniques . Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design . Lauren Hannah , David Dunson . Abstract : Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization . We develop a new piecewise linear convex regression method that uses the Convex Adaptive Partitioning ( CAP ) estimator in an ensemble setting , Ensemble Convex Adaptive Partitioning ( E - CAP ) . The ensembles alleviate some problems associated with convex piecewise linear estimators , such as instability when used to approximate constraints or objective functions for optimization , while maintaining desirable properties , such as consistency and O(n log(n)^2 ) computational complexity . We empirically demonstrate that E - CAP outperforms existing convex regression methods both when used for prediction and optimization . We then apply E - CAP to device modeling and constraint approximation for geometric programming based circuit design . Groupwise Constrained Reconstruction for Subspace Clustering . Ruijiang Li , Bin Li , Cheng Jin , Xiangyang Xue . Abstract : Recent proposed subspace clustering methods first compute a self reconstruction matrix for dataset , then converted it to an affinity matrix , before input to a spectral clustering method to obtain the final clustering result . Their success is largely based on the subspace independence assumption , which , however , does not always hold for the applications with increasing number of clusters such as face clustering . In this paper , we proposes a novel reconstruction based subspace clustering method without making the subspace independence assumption . In our model , certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators , and the affinity matrix input to the spectral clustering is built from the posterior of the cluster indicators . Evaluation on both synthetic and real - world datasets show that our method can outperform the state - of - the - arts . Stability of matrix factorization for collaborative filtering . Yu - Xiang Wang , Huan Xu . Abstract : We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion . We apply these results to the problem of collaborative filtering under manipulator attack , which leads to useful insights and guidelines for collaborative filtering system design . Adaptive Regularization for Similarity Measures . Koby Crammer , Gal Chechik . Abstract : Algorithms for learning distributions over weight - vectors , such as AROW were recently shown empirically to achieve state - of - the - art performance at various problems , with strong theoretical guaranties . Extending these algorithms to matrix models poses a challenge since the number of free parameters in the covariance of the distribution scales as n^4 with the dimension n of the matrix . We describe , analyze and experiment with two new algorithms for learning distribution of matrix models . Our first algorithm maintains a diagonal covariance over the parameters and is able to handle large covariance matrices . The second algorithm factores the covariance capturing some inter - features correlation while keeping the number of parameters linear in the size of the original matrix . We analyze the diagonal algorithm in the mistake bound model and show the superior precision of our approach over other algorithms in two tasks : retrieving similar images , and ranking similar documents . The second algorithms is shown to attain faster convergence rate . Linear Off - Policy Actor - Critic . Thomas Degris , Martha White , Richard Sutton . Abstract : This paper presents the first off - policy actor - critic reinforcement learning algorithm with a per - time - step complexity that scales linearly with the number of learned parameters . Previous work on actor - critic algorithms is limited to the on - policy setting and does not take advantage of the recent advances in off - policy gradient temporal - difference learning . Off - policy techniques , such as Greedy - GQ , enable a target policy to be learned while following and obtaining data from another ( behavior ) policy . For many problems , however , actor - critic methods are more practical than action value methods ( like Greedy - GQ ) because they explicitly represent the policy ; consequently , the policy can be stochastic and utilize a large action space . In this paper , we illustrate how to practically combine the generality and learning potential of off - policy learning with the flexibility in action selection given by actor - critic methods . We derive an incremental , linear time and space complexity algorithm that includes eligibility traces , prove convergence under assumptions similar to previous off - policy algorithms , and empirically show better or comparable performance to existing algorithms on standard reinforcement - learning benchmark problems . Modeling Latent Variable Uncertainty for Loss - based Learning . M. Pawan Kumar , Ben Packer , Daphne Koller . Abstract : We consider the problem of parameter estimation using weakly supervised datasets , where a training sample consists of the input and a partially specified annotation ( called the output ) . In addition , the missing information in the annotation is modeled using latent variables . Traditional methods , such as expectation - maximization , overburden a single distribution with two separate tasks : ( i ) modeling the uncertainty in the latent variables during training ; and ( ii ) making accurate predictions for the output and the latent variables during testing . During learning , we encourage agreement between the two distributions by minimizing a loss - based dissimilarity coefficient . We demonstrate the efficacy of our approach on two challenging problems - object detection and action detection - using publicly available datasets . Dimensionality Reduction by Local Discriminative Gaussians . Nathan Parrish , Maya Gupta . Abstract : We present local discriminative Gaussian ( LDG ) dimensionality reduction , a supervised linear dimensionality reduction technique that acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data . We focus on the classification setting ; however , our algorithm can be applied whenever training data is accompanied with similarity or dissimilarity constraints . Our experiments show that LDG is superior to other state - of - the - art linear dimensionality reduction techniques when the number of features in the original data is large . We also adapt LDG to the transfer learning setting , and show that it achieves good performance when the test data distribution differs from that of the training data . Learning to Label Aerial Images from Noisy Data . Volodymyr Mnih , Geoffrey Hinton . Abstract : When training a system to label images , the amount of labeled training data tends to be a limiting factor . We consider the task of learning to label aerial images from existing maps . These provide abundant labels , but the labels are often incomplete and sometimes poorly registered . We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets . The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider . The Most Persistent Soft - Clique in a Set of Sampled Graphs . Novi Quadrianto , Chao Chen , Christoph Lampert . Abstract : When searching for characteristic subpatterns in potentially noisy graph data , it appears self - evident that having multiple observations would be better than having just one . However , it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge . In this work we address this challenge for the problem of finding maximum weighted cliques . We introduce the concept of most persistent soft - clique . This is subset of vertices , that 1 ) is almost fully or at least densely connected , 2 ) occurs in all or almost all graph instances , and 3 ) has the maximum weight . We present a measure of clique - ness , that essentially counts the number of edge missing to make a subset of vertices into a clique . With this measure , we show that the problem of finding the most persistent soft - clique problem can be cast either as : a ) a max - min two person game optimization problem , or b ) a min - min soft margin optimization problem . Both formulations lead to the same solution when using a partial Lagrangian method to solve the optimization problems . By experiments on synthetic data and on real social network data , we show that the proposed method is able to reliably find soft cliques in graph data , even if that is distorted by random noise or unreliable observations . Learning Efficient Structured Sparse Models . Alex Bronstein , Pablo Sprechmann , Guillermo Sapiro . Abstract : We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes . For this purpose , we develop a novel block - coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems , and show an efficient feed forward architecture derived from its iteration . This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods . We also show that by using different training objective functions , learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre - given dictionary , as in earlier formulations , but can be rather used as full - featured sparse encoders or even modelers . A simple implementation shows several orders of magnitude speedup compared to the state - of - the - art at minimal performance degradation , making the proposed framework suitable for real time and large - scale applications . PAC Subset Selection in Stochastic Multi - armed Bandits . Shivaram Kalyanakrishnan , Ambuj Tewari , Peter Auer , Peter Stone . Abstract : We consider the problem of selecting , from among the arms of a stochastic n - armed bandit , a subset of size m of those arms with the highest expected rewards , based on efficiently sampling the arms . This \\\" subset selection \\\" problem finds application in a variety of areas . Kalyanakrishnan & Stone ( 2010 ) frame this problem under a PAC setting ( denoting it \\\" Explore - m \\\" ) and analyze corresponding sampling algorithms both formally and experimentally . Whereas their formal analysis is restricted to the worst case sample complexity of algorithms , in this paper , we design and analyze an algorithm ( \\\" LUCB \\\" ) with improved expected sample complexity . Interestingly LUCB bears a close resemblance to the well - known UCB algorithm for regret minimization . We also provide a lower bound on the worst case sample complexity of PAC algorithms for Explore - m . Nonparametric variational inference . Samuel Gershman , Matt Hoffman , David Blei . Abstract : Variational methods are widely used for approximate posterior inference . However , their use is typically limited to families of distributions that enjoy particular conjugacy properties . To circumvent this limitation , we propose a family of variational approximations inspired by nonparametric kernel density estimation . The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data . Using multiple kernels allows the approximation to capture multiple modes of the posterior , unlike most other variational approximations . We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model . We obtain predictive performance as good as or better than more specialized variational methods and sample - based approximations . The method is easy to apply to more general graphical models for which standard variational methods are difficult to derive . The Convexity and Design of Composite Multiclass Losses . Mark Reid , Robert Williamson , Peng Sun . Abstract : We consider composite loss functions for multiclass prediction comprising a proper ( i.e. , Fisher - consistent ) loss over probability distributions and an inverse link function . We establish conditions for their ( strong ) convexity and explore their implications . We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk . Finding Botnets Using Minimal Graph Clusterings . Peter Haider , Tobias Scheffer . Abstract : We study the problem of identifying botnets and the IP addresses which they comprise , based on the observation of a fraction of the global email spam traffic . Observed mailing campaigns constitute evidence for joint botnet membership , they are represented by cliques in the graph of all messages . No evidence against an association of nodes is ever available . We reduce the problem of identifying botnets to a problem of finding a minimal clustering of the graph of messages . We directly model the distribution of clusterings given the input graph ; this avoids potential errors caused by distributional assumptions of a generative model . We report on a case study in which we evaluate the model by its ability to predict the spam campaign that a given IP address is going to participate in . Learning the Experts for Online Sequence Prediction . Elad Eban , Aharon Birnbaum , Shai Shalev - Shwartz , Amir Globerson . Abstract : Online sequence prediction is the problem of predicting the next element of a sequence given previous elements . This problem has been extensively studied in the context of individual sequence prediction , where no prior assumptions are made on the origin of the sequence . Individual sequence prediction algorithms work quite well for long sequences , where the algorithm has enough time to learn the temporal structure of the sequence . However , they might give poor predictions for short sequences . A possible remedy is to rely on the general model of prediction with expert advice , where the learner has access to a set of r experts , each of which makes its own predictions on the sequence . It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of log ( r ) . But , without firm prior knowledge on the problem , it is not clear how to choose a small set of good experts . In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences . We demonstrate the merits of our approach by experimenting with the task of click prediction on the web . Efficient Active Algorithms for Hierarchical Clustering . Akshay Krishnamurthy , Sivaraman Balakrishnan , Min Xu , Aarti Singh . Abstract : Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets , while storage and processing power continue to lag behind . This motivates the need for algorithms that are efficient , both in terms of the number of measurements needed and running time . To combat the challenges associated with large datasets , we propose a general framework for active hierarchical clustering that repeatedly runs an off - the - shelf clustering algorithm on small subsets of the data and comes with guarantees on performance , measurement complexity and runtime complexity . Through extensive experimentation we also demonstrate that this framework is practically alluring . Copula Mixture Model for Dependency - seeking Clustering . Melanie Rey , Volker Roth . Abstract : We introduce a copula mixture model to perform dependency - seeking clustering when co - occurring samples from different data sources are available . The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities . We formulate our model as a non - parametric Bayesian mixture , while providing efficient MCMC inference . Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results . The Landmark Selection Method for Multiple Output Prediction . Krishnakumar Balasubramanian , Guy Lebanon . A substantial research effort is devoted to such modeling when x is high dimensional . We consider , instead , the case of a high dimensional y , where x is either low dimensional or high dimensional . Classification and regression experiments on multiple datasets show that this model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods . Subgraph Matching Kernels for Attributed Graphs . Nils Kriege , Petra Mutzel . Abstract : We propose graph kernels based on subgraph matchings , i.e. structure - preserving bijections between subgraphs . We show that subgraph matching kernels generalize several known kernels . To compute the kernel we propose a graph - theoretical algorithm inspired by a classical relation between common subgraphs of two graphs and cliques in their product graph observed by Levi ( 1973 ) . Adaptive Canonical Correlation Analysis Based On Matrix Manifolds . Florian Yger , Maxime Berar , Gilles Gasso , Alain Rakotomamonjy . Abstract : In this paper , we formulate the Canonical Correlation Analysis ( CCA ) problem on matrix manifolds . This framework provides a natural way for dealing with matrix constraints and tools for building efficient algorithms even in an adaptive setting . Finally , an adaptive CCA algorithm is proposed and applied to a change detection problem in EEG signals . Batch Active Learning via Coordinated Matching . Javad Azimi , Alan Fern , Xiaoli Zhang - Fern , Glencora Borradaile , Brent Heeringa . - Accepted . Abstract : Most prior work on active learning of classifiers has focused on sequentially selecting one unlabeled example at a time to be labeled in order to reduce the overall labeling effort . In many scenarios , however , it is desirable to label an entire batch of examples at once , for example , when labels can be acquired in parallel . We propose a novel batch active learning method that leverages the availability of high - quality and efficient sequential active - learning policies by attempting to approximate their behavior when applied for k steps . Specifically , our algorithm first uses Monte - Carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over k step executions . The algorithm then attempts to select a set of k examples that best matches this distribution , leading to a combinatorial optimization problem that we term \\\" bounded coordinated matching ' . While we show this problem is NP - hard in general , we give an efficient greedy solution , which inherits approximation bounds from supermodular minimization theory . Our experimental results on eight benchmark datasets show that the proposed approach is highly effective . Hybrid Batch Bayesian Optimization . Javad Azimi , Ali Jalali , Xiaoli Zhang - Fern . Abstract : Bayesian Optimization aims at optimizing an unknown non - convex / concave function that is costly to evaluate . We are interested in application scenarios where concurrent function evaluations are possible . Under such a setting , BO could choose to either sequentially evaluate the function , one input at a time and wait for the output of the function before making the next selection , or evaluate the function at a batch of multiple inputs at once . These two different settings are commonly referred to as the sequential and batch settings of Bayesian Optimization . In general , the sequential setting leads to better optimization performance as each function evaluation is selected with more information , whereas the batch setting has an advantage in terms of the total experimental time ( the number of iterations ) . In this work , our goal is to combine the strength of both settings . Specifically , we systematically analyze Bayesian optimization using Gaussian process as the posterior estimator and provide a hybrid algorithm that , based on the current state , dynamically switches between a sequential policy and a batch policy with variable batch sizes . We provide theoretical justification for our algorithm and present experimental results on eight benchmark BO problems . The results show that our method achieves substantial speedup ( up to % 78 ) compared to a pure sequential policy , without suffering any significant performance loss . Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization . Haim Avron , Satyen Kale , Shiva Kasiviswanathan , Vikas Sindhwani . Abstract : We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization . Unlike existing approaches , our method executes very cheap iterations by combining low - rank stochastic subgradients with efficient incremental SVD updates , made possible by highly optimized and parallelizable dense linear algebra operations on small matrices . Our practical algorithms always maintain a low - rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings . Empirical comparisons confirm that our approach is highly competitive with several recently proposed state - of - the - art solvers for such problems . Gap Filling in the Plant Kingdom - Trait Prediction Using Hierarchical Probabilistic Matrix Factorization . Hanhuai Shan , Jens Kattge , Peter Reich , Arindam Banerjee , Franziska Schrodt , Markus Reichstein . - Accepted . Abstract : Plant traits are a key to understand and predict the adaptation of ecosystems to environmental changes , which motivates the TRY project aiming at constructing a global database for plant traits and becoming a standard resource for the ecological community . Despite its unprecedented coverage , a large percentage of missing data substantially constrains joint trait analysis . Meanwhile , the trait data are characterized by the hierarchical phylogenetic structure of the plant kingdom . While factorization based matrix completion techniques have been widely used to address the missing data problem , traditional matrix factorization methods are unable to leverage the phylogenetic structure . We propose hierarchical probabilistic matrix factorization ( HPMF ) , which effectively uses hierarchical phylogenetic information for trait prediction . We demonstrate HPMF 's high accuracy , effectiveness of incorporating hierarchical structure and ability to capture trait correlation through experiments . Sparse Support Vector Infinite Push . Alain Rakotomamonjy . Abstract : In this paper , we address the problem of embedded feature selection for ranking on top of the list problems . We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer . Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator . Experimental results on toy , DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function . A Dantzig Selector Approach to Temporal Difference Learning . Matthieu Geist , Bruno Scherrer , Alessandro Lazaric , Mohammad Ghavamzadeh . - Accepted . Abstract : LSTD is one of the most popular reinforcement learning algorithms for value function approximation . Whenever the number of samples is larger than the number of features , LSTD must be paired with some form of regularization . In particular , L1-regularization methods tends to perform feature selection by promoting sparsity and thus they are particularly suited in high - dimensional problems . Nonetheless , since LSTD is not a simple regression algorithm but it solves a fixed - point problem , the integration with L1-regularization is not straightforward and it might come with some drawbacks ( see e.g. , the P - matrix assumption for LASSO - TD ) . In this paper we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector . In particular , we investigate the performance of the algorithm and its relationship with existing regularized approaches , showing how it overcomes some of the drawbacks of existing solutions . Chad Scherrer , Mahantesh Halappanavar , Ambuj Tewari , David Haglin . Abstract : We present a generic framework for parallel coordinate descent ( CD ) algorithms that has as special cases the original sequential algorithms of Cyclic CD and Stochastic CD , as well as the recent parallel Shotgun algorithm of Bradley et al . We introduce two novel parallel algorithms that are also special cases - Thread - Greedy CD and Coloring - Based CD - and give performance measurements for an OpenMP implementation of these . Cross - Domain Multitask Learning with Latent Probit Models . Shaobo Han , Xuejun Liao , Lawrence Carin . Abstract : Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks . We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model ( LPM ) to jointly learn the domain transforms , and the shared probit classifier in the common domain . To learn meaningful task relatedness and avoid over - fitting in classification , we introduce sparsity in the domain transforms matrices , as well as in the common classifier . We derive theoretical bounds for the estimation error of the classifier in terms of the sparsity of domain transforms . An expectation - maximization algorithm is derived for learning the LPM . The effectiveness of the approach is demonstrated on several real datasets . Structured Learning from Partial Annotations . Xinghua Lou , Fred Hamprecht . Abstract : Structured learning is appropriate when predicting structured outputs such as trees , graphs , or sequences . Most prior work requires the training set to consist of complete trees , graphs or sequences . Specifying such detailed ground truth can be tedious or infeasible for large outputs . Our main contribution is a large margin formulation that makes structured learning from only partially annotated data possible . The resulting optimization problem is non - convex , yet can be efficiently solve by concave - convex procedure ( CCCP ) with novel speedup strategies . We apply our method to a challenging tracking - by - assignment problem of a variable number of divisible objects . On this benchmark , using only 25 % of a full annotation we achieve a performance comparable to a model learned with a full annotation . Finally , we offer a unifying perspective of previous work using the hinge , ramp , or max loss for structured learning , followed by an empirical comparison on their practical performance . Maximum Margin Output Coding . Yi Zhang , Jeff Schneider . Abstract : In this paper we study output coding for multi - label prediction . For a multi - label output coding to be discriminative , it is important that codewords for different label vectors are significantly different from each other . In the meantime , unlike in traditional coding theory , codewords in output coding are to be predicted from the input , so it is also critical to have a predictable label encoding . To find output codes that are both discriminative and predictable , we first propose a max - margin formulation that naturally captures these two properties . We then convert it to a metric learning formulation , but with an exponentially large number of constraints as commonly encountered in structured prediction problems . Without a label structure for tractable inference , we use overgenerating ( i.e. , relaxation ) techniques combined with the cutting plane method for optimization . In our empirical study , the proposed output coding scheme outperforms a variety of existing multi - label prediction methods for image , text and music classification . Sequential Nonparametric Regression . Haijie Gu , John Lafferty . Abstract : We present algorithms for nonparametric regression in settings where the data are obtained sequentially . While traditional estimators select bandwidths that depend upon the sample size , for sequential data the effective sample size is dynamically changing . We propose a linear time algorithm that adjusts the bandwidth for each new data point , and show that the estimator achieves the optimal minimax rate of convergence . We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function . We provide simulations that confirm the theoretical results , and demonstrate the effectiveness of the methods . An Infinite Latent Attribute Model for Network Data . Konstantina Palla , David A. Knowles , Zoubin Ghahramani . Abstract : Latent variable models for network data extract a summary of the relational structure underlying an observed network . The simplest possible models subdivide nodes of the network into clusters ; the probability of a link between any two nodes then depends only on their cluster assignment . Currently available models can be classified by whether clusters are disjoint or are allowed to overlap . These models can explain a \\\" flat \\\" clustering structure . Hierarchical Bayesian models provide a natural approach to capture more complex dependencies . We propose a model in which objects are characterised by a latent feature vector . Each feature is itself partitioned into disjoint groups ( subclusters ) , corresponding to a second layer of hierarchy . In experimental comparisons , the model achieves significantly improved predictive performance on social and biological link prediction tasks . The results indicate that models with a single layer hierarchy over - simplify real networks . On Local Regret . Michael Bowling , Martin Zinkevich . Abstract : Online learning typically aims to perform nearly as well as the best hypothesis in hindsight . For some hypothesis classes , though , even finding the best hypothesis offline is challenging . In such offline cases , local search techniques are often employed and only local optimality guaranteed . For online decision - making with such hypothesis classes , we introduce local regret , a generalization of regret that aims to perform nearly as well as only nearby hypotheses . We then present a general algorithm that can minimize local regret for arbitrary locality graphs . We also show that certain forms of structure in the graph can be exploited to drastically simplify learning . These algorithms are then demonstrated on a diverse set of online problems ( some previously unexplored ) : online disjunct learning , online Max - SAT , and online decision tree learning . Smoothness and Structure Learning by Proxy . Benjamin Yackley , Terran Lane . Abstract : As data sets grow in size , the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account . For instance , Bayesian networks , the model chosen in this paper , have a super - exponentially large search space for a fixed number of variables . One possible method to alleviate this problem is to use a proxy , such as a Gaussian Process regressor , in place of the true scoring function , training it on a selection of sampled networks . We prove here that the use of such a proxy is well - founded , as we can bound the smoothness of a commonly - used scoring function for Bayesian network structure learning . We show here that , compared to an identical search strategy using the network 's exact scores , our proxy - based search is able to get equivalent or better scores on a number of data sets in a fraction of the time . A fast and simple algorithm for training neural probabilistic language models . Andriy Mnih , Yee Whye Teh . Abstract : Neural probabilistic language models ( NPLMs ) have recently superseded smoothed n - gram models as the best - performing model class for language modelling . Unfortunately , the adoption of NPLMs is held back by their notoriously long training times , which can be measured in weeks even for moderately - sized datasets . These are a consequence of the models being explicitly normalized , which leads to having to consider all words in the vocabulary when computing the log - likelihood gradients . We propose a fast and simple algorithm for training NPLMs based on noise - contrastive estimation , a newly introduced procedure for estimating unnormalized continuous distributions . We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models . The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well . We demonstrate the scalability of the proposed approach by training several neural language models on a 47M - word corpus with a 80K - word vocabulary , obtaining state - of - the - art results in the Microsoft Research Sentence Completion Challenge . Incorporating Causal Prior Knowledge as Path - Constraints in Bayesian Networks and Maximal Ancestral Graphs . Giorgos Borboudakis , Ioannis Tsamardinos . Abstract : We consider the incorporation of causal knowledge about the presence or absence of ( possibly indirect ) causal relations into a causal model . Such causal relations correspond to directed paths in a causal model . This type of knowledge naturally arises from experimental data , among others . Specifically , we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes : Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs . We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models . In simulated experiments , we show that often considering even a few causal facts leads to a significant number of new inferences . In a case study , we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network . High - Dimensional Covariance Decomposition into Sparse Markov and Independence Domains . Majid Janzamin , Animashree Anandkumar . Abstract : In this paper , we present a novel framework incorporating a combination of sparse models in different domains . We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model ( with a sparse precision matrix ) and a sparse Gaussian independence model ( with a sparse covariance matrix ) . We provide efficient methods for decomposition of the data into two domains , viz Markov and independence domains . We characterize a set of sufficient conditions for identifiability and model consistency . Latent Collaborative Retrieval . Jason Weston , Chong Wang , Ron Weiss , Adam Berenzweig . Abstract : Retrieval tasks typically require a ranking of items given a query . Collaborative filtering tasks , on the other hand , learn models comparing users with items . In this paper we study the joint problem of recommending items to a user with respect to a given query , which is a surprisingly common task . This setup differs from the standard collaborative filtering one in that we are given a query \\u00d7 user \\u00d7 item tensor for training instead of the more traditional user \\u00d7 item matrix . Compared to document retrieval we do have a query , but we may or may not have content features ( we will consider both cases ) and we can also take account of the user 's profile . We introduce a factorized model for this new task that optimizes the top ranked items returned for the given query and user . We report empirical results where it outperforms several baselines . Lightning Does Not Strike Twice : Robust MDPs with Coupled Uncertainty . Shie Mannor , Ofir Mebel , Huan Xu . Abstract : We consider Markov decision processes under parameter uncertainty . Previous studies all restrict to the case that uncertainties among different states are uncoupled , which leads to conservative solutions . In contrast , we introduce an intuitive concept , termed ' Lightning Does not Strike Twice , ' to model coupled uncertain parameters . Specifically , we require that the system can deviate from its nominal parameters only a bounded number of times . We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies using this concept . On causal and anticausal learning . Bernhard Schoelkopf , Dominik Janzing , Jonas Peters , Eleni Sgouritsa , Kun Zhang , Joris Mooij . - Accepted . Abstract : We consider the problem of function estimation in the case where an underlying causal model can be identified . This has implications for popular scenarios such as covariate shift , concept drift , transfer learning and semi - supervised learning . We argue that causal knowledge may facilitate some approaches for a given problem , and rule out others . In particular , we formulate a hypothesis for when semi - supervised learning can help , and corroborate it with empirical results . Compact Hyperplane Hashing with Bilinear Functions . Wei Liu , Jun Wang , Yadong Mu , Sanjiv Kumar , Shih - Fu Chang . Abstract : Hyperplane hashing aims at rapidly searching nearest points to a hyperplane , and has shown practical impact in scaling up active learning with SVMs . Unfortunately , the existing randomized methods need long hash codes and a number of hash tables to achieve reasonable search accuracy . Thus , they suffer from reduced search speed and large memory overhead . To this end , this paper proposes a novel hyperplane hashing technique which yields compact hash codes . The key idea is the bilinear form of the proposed hash functions , which leads to higher collision probability than the existing hyperplane hash functions when using random projections . To further increase the performance , we propose a learning based framework in which bilinear functions are directly learned from the data . This yields compact yet discriminative codes , and also increases the search performance over the random projection based solutions . Large - scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach . Continuous Inverse Optimal Control with Locally Optimal Examples . Sergey Levine , Vladlen Koltun . Abstract : Inverse optimal control , also known as inverse reinforcement learning , is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy . We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality , and is suitable for large , continuous domains where even computing a full policy is impractical . By using a local approximation of the reward function , our method can also drop the assumption that the demonstrations are globally optimal , requiring only local optimality . This allows it to learn from examples that are unsuitable for prior methods . Convex Multitask Learning with Flexible Task Clusters . Wenliang Zhong , James Kwok . Abstract : Traditionally , multitask learning ( MTL ) assumes that all the tasks are related . This can lead to negative transfer when tasks are indeed incoherent . Recently , a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships . However , they are limited to modeling these relationships at the task level , which may be restrictive in some applications . In this paper , we propose a novel MTL formulation that captures task relationships at the feature - level . Depending on the interactions among tasks and features , the proposed method construct different task clusters for different features , without even the need of pre - specifying the number of clusters . Computationally , the proposed formulation is strongly convex , and can be efficiently solved by accelerated proximal methods . Experiments are performed on a number of synthetic and real - world data sets . Under various degrees of task relationships , the accuracy of the proposed method is consistently among the best . Moreover , the feature - specific task clusters obtained agree with the known / plausible task structures of the data . A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling . Drausin Wulsin , Shane Jensen , Brian Litt . Abstract : Driven by the multi - level structure of human intracranial electroencephalogram ( iEEG ) recordings of epileptic seizures , we introduce a new variant of a hierarchical Dirichlet Process - the multi - level clustering hierarchical Dirichlet Process ( MLC - HDP)-that simultaneously clusters datasets on multiple levels . Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient . The MLC - HDP model clusters over channels - types , seizure - types , and patient - types simultaneously . We describe this model and its implementation in detail . We also present the results of a simulation study comparing the MLC - HDP to a similar model , the Nested Dirichlet Process and finally demonstrate the MLC - HDP 's use in modeling seizures across multiple patients . We find the MLC - HDP 's clustering to be comparable to independent human physician clusterings . To our knowledge , the MLC - HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients . Levy Measure Decompositions for the Beta and Gamma Processes . Yingjian Wang , Lawrence Carin . Abstract : We develop new representations for the Levy measures of the beta and gamma processes . These representations are manifested in terms of an infinite sum of well - behaved ( proper ) beta and gamma distributions . Further , we demonstrate how these infinite sums may be truncated in practice , and explicitly characterize truncation errors . We also perform an analysis of the characteristics of posterior distributions , based on the proposed decompositions . The decompositions provide new insights into the beta and gamma processes , and we demonstrate how the proposed representation unifies some properties of the two . This paper is meant to provide a rigorous foundation for and new perspectives on L\\u00b4evy processes , as these are of increasing importance in machine learning . Building high - level features using large scale unsupervised learning . Quoc Le , Marc'Aurelio Ranzato , Rajat Monga , Matthieu Devin , Greg Corrado , Kai Chen , Jeff Dean , Andrew Ng . - Accepted . Abstract : We consider the challenge of building feature detectors for high - level concepts from only unlabeled data . For example , we would like to understand if it is possible to learn a face detector using only unlabeled images downloaded from the Internet . To answer this question , we trained a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images ( which has 10 million images , each image has 200x200 pixels ) . On contrary to what appears to be a widely - held negative belief , our experimental results reveal that it is possible to achieve a face detector via only unlabeled data . Control experiments show that the feature detector is robust not only to translation but also to scaling and 3D rotation . Also via recognition and visualization , we find that the same network is sensitive to other high - level concepts such as cat faces and human bodies . Near - Optimal BRL using Optimistic Local Transitions . Mauricio Araya , Olivier Buffet , Vincent Thomas . Abstract : Model - based Bayesian Reinforcement Learning ( BRL ) allows a found formalization of the problem of acting optimally while facing an unknown environment , i.e. , avoiding the exploration - exploitation dilemma . However , algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms . This paper introduces BOLT , a simple and ( almost ) deterministic heuristic algorithm for BRL which is optimistic about the transition function . We analyze BOLT 's sample complexity , and show that under certain parameters , the algorithm is near - optimal in the Bayesian sense with high probability . Then , experimental results highlight the key differences of this method compared to previous work . A Unified Robust Classification Model . Akiko Takeda , Hiroyuki Mitsugi , Takafumi Kanamori . Abstract : A wide variety of machine learning algorithms such as support vector machine ( SVM ) , minimax probability machine ( MPM ) , and Fisher discriminant analysis ( FDA ) , exist for binary classification . The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach . This unified model has several benefits . One is that the extensions and improvements intended for SVM become applicable to MPM and FDA , and vice versa . Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model . We give a statistical interpretation of the unified classification model and propose a non - convex optimization algorithm that can be applied to non - convex variants of existing learning methods . Manifold Relevance Determination . Andreas Damianou , Carl Ek , Michalis Titsias , Neil Lawrence . Abstract : In this paper we present a fully Bayesian latent variable model which exploits conditional non- linear ( in)-dependence structures to learn an efficient latent representation . The model is capable of learning from extremely high - dimensional data such as directly modelling high resolution images . The latent representation is factorized to represent shared and private information from multiple views of the data . Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces . We demonstrate the model by prediction of human pose in an ambiguous setting . Our Bayesian representation allows us to perform disambiguation in a principled manner by including priors which incorporate the dynamics structure of the data . We demonstrate the ability of the model to capture structure underlying extremely high dimensional spaces by learning a low - dimensional representation of a set of facial images under different illumination conditions . The model correctly automatically creates a factorized representation where the lighting variance is represented in a separate latent space from the variance associated with different faces . We show that the model is capable of generating morphed faces and images from novel light directions . Residual Components Analysis . Alfredo Kalaitzis , Neil Lawrence . The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix . In this paper we consider the situation where the data variance is already partially explained by other factors , e.g. conditional dependencies between the covariates , or temporal correlations leaving some residual variance . We decompose the residual variance into its components through a generalised eigenvalue problem , which we call residual component analysis ( RCA ) . We explore a range of new algorithms that arise from the framework , including one that factorises the covariance of a Gaussian density into a low - rank and a sparse - inverse component . We illustrate the ideas on the recovery of a protein - signaling network , a gene expression time - series data set and the recovery of the human skeleton from motion capture 3-D cloud data . Clustering to Maximize the Ratio of Split to Diameter . Jiabing Wang , Jiaye Chen . The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster , and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster . In this paper , we propose a new criterion for measuring the goodness of clusters : ? the ratio of the minimum split to the maximum diameter , and the objective is to maximize the ratio . The worst - case runtime of both algorithms is O(n3 ) . We compare the proposed algorithms with the Normalized Cut by applying them to image segmentation . The experimental results on both natural and synthetic images demonstrate the effectiveness of the proposed algorithms . A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training . Aaron Defazio , Tiberio Caetano . Abstract : Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items , where each item is connected to those it is most similar to . The prediction of a user 's rating on an item is then given by that rating of neighbouring items , weighted by their similarity . This paper presents a new neighbourhood approach which we call item fields , whereby an undirected graphical model is formed over the item graph . The resulting prediction rule is a simple generalization of the classical approaches , which takes into account non - local information in the graph , allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches . A fast approximate maximum entropy training method based on the Bethe approximation is presented which utilizes a novel decomposition into tractable sub - problems . When using precomputed sufficient statistics on the Movielens dataset , our method outperforms maximum likelihood approaches by two orders of magnitude . On - Line Portfolio Selection with Moving Average Reversion . Bin Li , Steven C.H. Hoi . Abstract : On - line portfolio selection has attracted increasing interests in machine learning and AI communities recently . Empirical evidences show that stock 's high and low prices are temporary and stock price relatives are likely to follow the mean reversion phenomenon . While the existing mean reversion strategies are shown to achieve good empirical performance on many real datasets , they often make the single - period mean reversion assumption , which is not always satisfied in some real datasets , leading to poor performance when the assumption does not hold . From our empirical results , we found that OLMAR can overcome the drawback of existing mean reversion algorithms and achieve significantly better results , especially on the datasets where the existing mean reversion algorithms failed . In addition to superior trading performance , OLMAR also runs extremely fast , further supporting its practical applicability to a wide range of applications . Improved Information Gain Estimates for Decision Tree Induction . Sebastian Nowozin . Abstract : Ensembles of classification and regression trees remain popular machine learning methods because they define flexible non - parametric models that predict well and are computationally efficient both during training and testing . During induction of decision trees one aims to find predicates that are maximally informative about the prediction target . To select good predicates most approaches estimate an information - theoretic scoring function , the information gain , both for classification and regression problems . We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees . In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code . Influence Maximization in Continuous Time Diffusion Networks . Manuel Gomez Rodriguez , Bernhard Sch\\u00f6lkopf . Abstract : The problem of finding the optimal set of source nodes in a diffusion network that maximizes the spread of information , influence , and diseases in a limited amount of time depends dramatically on the underlying temporal dynamics of the network . However , this still remains largely unexplored to date . To this end , given a network and its temporal dynamics , we first describe how continuous time Markov chains allow us to analytically compute the average total number of nodes reached by a diffusion process starting in a set of source nodes . We then show that selecting the set of most influential source nodes in the continuous time influence maximization problem is NP - hard and develop an efficient approximation algorithm with provable near - optimal performance . Experiments on synthetic and real diffusion networks show that our algorithm outperforms other state of the art algorithms by at least 20 % and is robust across different network topologies . On the Size of the Online Kernel Sparsification Dictionary . Yi Sun , Faustino Gomez , Juergen Schmidhuber . Abstract : We analyze the size of the dictionary constructed from online kernel sparsification , using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator . Using this formula , we are able to connect the cardinality of the dictionary with the eigen - decay of the covariance operator . In particular , we show that for bounded kernels , the size of the dictionary always grows sub - linearly in the number of data points , and , as a consequence , the kernel linear regressor constructed from the resulting dictionary is consistent . Multi - level Lasso for Sparse Multi - task Regression . Aurelie Lozano , Grzegorz Swirszcz . Abstract : We present a flexible formulation for variable selection in multi - task regression to allow for discrepancies in the estimated sparsity patterns accross the multiple tasks , while leveraging the common structure among them . Our approach is based on an intuitive decomposition of the regression coefficients into a product between a component that is common to all tasks and another component that captures task - specificity . This decomposition yields the Multi - level Lasso objective that can be solved efficiently via alternating optimization . The analysis of the \\\" orthonormal design \\\" case reveals some interesting insights on the nature of the shrinkage performed by our method , compared to that of related work . Theoretical guarantees are provided on the consistency of Multi - level Lasso . Simulations and empirical study of micro - array data further demonstrate the value of our framework . Fast Computation of Subpath Kernel for Trees . Daisuke Kimura , Hisashi Kashima . Abstract : The kernel method is a potential approach to analyzing structured data such as sequences , trees , and graphs ; however , unordered trees have not been investigated extensively . Kimura et al . ( 2011 ) proposed a kernel function for unordered trees on the basis of their subpaths , which are vertical substructures of trees responsible for hierarchical information in them . Their kernel exhibits practically good performance in terms of accuracy and speed ; however , linear - time computation is not guaranteed theoretically , unlike the case of the other unordered tree kernel proposed by Vishwanathan and Smola ( 2003 ) . In this paper , we propose a theoretically guaranteed linear - time kernel computation algorithm that is practically fast , and we present an efficient prediction algorithm whose running time depends only on the size of the input tree . Experimental results show that the proposed algorithms are quite efficient in practice . Total Variation and Euler 's Elastica for Supervised Learning . Tong Lin , Hanlin Xue , Ling Wang , Hongbin Zha . Abstract : In recent years , total variation ( TV ) and Euler 's elastica ( EE ) have been successfully applied to image processing tasks such as denoising and inpainting . This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data . The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme , where the energy is composed of a squared loss and a total variation smoothing ( or Euler 's elastica smoothing ) . Its solution via variational principles leads to an Euler - Lagrange PDE . However , the PDE is always high - dimensional and can not be directly solved by common methods . Instead , radial basis functions are utilized to approximate the target function , reducing the problem to finding the linear coefficients of basis functions . We apply the proposed methods to supervised learning tasks ( including binary classification , multi - class classification , and regression ) on benchmark data sets . Extensive experiments have demonstrated promising results of the proposed methods . Learning the Dependence Graph of Time Series with Latent Factors . Ali Jalali , Sujay Sanghavi . Abstract : This paper considers the problem of learning , from samples , the dependency structure of a system of linear stochastic differential equations , when some of the variables are latent . We observe the time evolution of some variables , and never observe other variables ; from this , we would like to find the dependency structure of the observed variables - separating out the spurious interactions caused by the latent variables ' time series . We develop a new convex optimization based method to do so in the case when the number of latent variables is smaller than the number of observed ones . For the case when the dependency structure between the observed variables is sparse , we theoretically establish a high - dimensional scaling result for structure recovery . We verify our theoretical result with both synthetic and real data ( from the stock market ) . A Generalized Loop Correction Method for Approximate Inference in Graphical Models . Siamak Ravanbakhsh , Chun - Nam Yu , Russell Greiner . Abstract : Belief Propagation ( BP ) is one of the most popular methods for inference in probabilistic graphical models . BP is guaranteed to return the correct answer for tree structures , but can be incorrect or non - convergent for loopy graphical models . Recently , several new approximate inference algorithms based on cavity distribution have been proposed . These methods can account for the effect of loops by incorporating the dependency between BP messages . Alternatively , region - based approximations ( that lead to methods such as Generalized Belief Propagation ) improve upon BP by considering interactions within small clusters of variables , thus taking small loops within these clusters into account . This paper introduces an approach , Generalized Loop Correction ( GLC ) , that benefits from both of these types of loop correction . We show how GLC relates to these two families of inference methods , then provide empirical evidence that GLC works effectively in general , and can be significantly more accurate than both correction schemes . Consistent Covariance Selection From Data With Missing Values . Mladen Kolar , eric xing . Abstract : Data sets with missing values arise in many practical problems and domains . However , correct statistical analysis of these data sets is difficult . A popular likelihood approach to statistical inference from partially observed data is the expectation maximization ( EM ) algorithm , which leads to non - convex optimization and estimates that are difficult to analyze theoretically . We study a simple two step procedure for covariance selection , which is tractable in high - dimensions and does not require imputation of the missing values . Simulation studies show that this estimator compares favorably with the EM algorithm . Our results have important practical consequences as they show that standard tools for covariance selection can be used when data contains missing values , without resorting to the iterative EM algorithm that can be slow to converge in practice for large problems . Is margin preserved after random projection ? Qinfeng Shi , Chunhua Shen , Rhys Hill , Anton van den Hengel . Abstract : Random projections have been applied in many machine learning algorithms . However , whether margin is preserved after random projection is non - trivial and not well studied . In this paper we analyse margin distortion after random projection , and give the conditions of margin preservation for binary classification problems . We also extend our analysis to margin for multiclass problems , and provide theoretical bounds on multiclass margin on the projected data . A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices . Mingjun Zhong , Mark Girolami . Abstract : We present a fully Bayesian approach to simultaneously approximate diagonalization of several square matrices which are not necessary symmetric . A Gibbs sampler has been derived for simulating the common eigenvectors and the eigenvalues for these matrices . Several data are used to demonstrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms , which shows that the Gibbs sampler achieves the state - of - the - art performance . As a byproduct , the output of the Gibbs sampler could be used to estimate the log marginal likelihood by the Bayesian information criterion ( BIC ) which correctly located the number of common eigenvectors . We then applied the Gibbs sampler to the blind sources separation problem , the common principal component analysis and the common spatial pattern analysis . Predicting accurate probabilities with a ranking loss . Aditya Menon , Xiaoqian Jiang , Shankar Vembu , Charles Elkan , Lucila Ohno - Machado . - Accepted . Abstract : In many real - world applications of machine learning classifiers , it is essential to predict the probability of an example belonging to a particular class . This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss , followed by isotonic regression . This semi - parametric technique offers both good ranking and regression performance , and models a richer set of probability distributions than statistical workhorses such as logistic regression . We provide experimental results that show the effectiveness of this technique on real - world applications of probability prediction . Learning with Augmented Features for Heterogeneous Domain Adaptation . Lixin Duan , Dong Xu , Ivor Tsang . Abstract : We propose a new learning method for heterogeneous domain adaptation ( HDA ) , in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions . Using two different projection matrices , we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains . We then propose two new feature mapping functions to augment the transformed data with their original features and zeros . The existing learning methods ( e.g. , SVM and SVR ) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA . Using the hinge loss function in SVM as an example , we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation ( HFA ) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions . Moreover , we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method . Comprehensive experiments on two benchmark datasets clearly demonstrate that our HFA outperforms the existing HDA methods . Dirichlet Process with Mixed Random Measures : A Nonparametric Topic Model for Labeled Data . Dongwoo Kim , Suin Kim , Alice Oh . Abstract : We describe a nonparametric topic model for labeled data . The model uses a mixture of random measures ( MRM ) as a base distribution of the Dirichlet process ( DP ) of the HDP framework , so we call it the DP - MRM . To model labeled data , we define a DP distributed random measure for each label , and the resulting model generates an unbounded number of topics for each label . We apply DP - MRM on single - labeled and multi - labeled corpora of documents and compare the performance on label prediction with LDA - SVM and Labeled - LDA . We further enhance the model by incorporating ddCRP and modeling multi - labeled images for image segmentation and object labeling , comparing the performance with nCuts and rddCRP . Evaluating Bayesian and L1 Approaches for Sparse Unsupervised Learning . Shakir Mohamed , Katherine Heller , Zoubin Ghahramani . Abstract : The use of L_1 regularisation for sparse learning has generated immense research interest , with many successful applications in diverse areas such as signal acquisition , image coding , genomics and collaborative filtering . While existing work highlights the many advantages of L_1 methods , in this paper we find that L_1 regularisation often dramatically under - performs in terms of predictive performance when compared with other methods for inferring sparsity . We focus on unsupervised latent variable models , and develop L_1 minimising factor models , Bayesian variants of \\\" L_1 \\\" , and Bayesian models with a stronger L_0 -like sparsity induced through spike - and - slab distributions . These spike - and - slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner , and avoid unnecessary shrinkage of non - zero values . We demonstrate on a number of data sets that in practice spike - and - slab Bayesian methods outperform L_1 minimisation , even on a computational budget . We thus highlight the need to re - assess the wide use of L_1 methods in sparsity - reliant applications , particularly when we care about generalising to previously unseen data , and provide an alternative that , over many varying conditions , provides improved generalisation performance . Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems . Sanjay Purushotham , Yan Liu . Abstract : Social network websites , such as Facebook , YouTube , Lastfm etc , have become a popular platform for users to connect with each other and share content or opinions . They provide rich information to study the influence of user 's social circle in their decision process . In this paper , we are interested in examining the effectiveness of social network information to predict the user 's ratings of items . We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks . A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data . Empirical experiments on two large - scale datasets show that our algorithm provides a more effective recommendation system than the state - of - the art approaches . Our results also reveal interesting insight that the social circles have more influence on people 's decisions about the usefulness of information ( e.g. , bookmarking preference on Delicious ) than personal taste ( e.g. , music preference on Lastfm ) . LPQP for MAP : Putting LP Solvers to Better Use . Patrick Pletscher , Sharon Wulff . Abstract : MAP inference for general energy functions remains a challenging problem . While most efforts are channeled towards improving the linear programming ( LP ) based relaxation , this work is motivated by the quadratic programming ( QP ) relaxation . We propose a novel MAP relaxation that penalizes the Kullback - Leibler divergence between the LP pairwise auxiliary variables , and QP equivalent terms given by the product of the unaries . We develop two efficient algorithms based on variants of this relaxation . The algorithms minimize the non - convex objective using belief propagation and dual decomposition as building blocks . Experiments on synthetic and real - world data show that the solutions returned by our algorithms substantially improve over the LP relaxation . Clustering by Low - Rank Doubly Stochastic Matrix Decomposition . Zhirong Yang , Erkki Oja . Abstract : Clustering analysis by nonnegative low - rank approximations has achieved remarkable progress in the past decade . However , most approximation approaches in this direction are still restricted to matrix factorization . We propose a new low - rank learning method to improve the clustering performance , which is beyond matrix factorization . The approximation is based on a two - step bipartite random walk through virtual cluster nodes , where the approximation is formed by only cluster assigning probabilities . Minimizing the approximation error measured by Kullback - Leibler divergence is equivalent to maximizing the likelihood of a discriminative model , which endows our method with a solid probabilistic interpretation . The optimization is implemented by a relaxed Majorization - Minimization algorithm that is advantageous in finding good local minima . Furthermore , we point out that the regularized algorithm with Dirichlet prior only serves as initialization . Experimental results show that the new method has strong performance in clustering purity for various datasets , especially for large - scale manifold data . Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling . Changyou Chen , Nan Ding , Wray Buntine . Abstract : We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling . The dependency arises via superposition , subsampling and point transition on the underlying Poisson processes of these measures . The measures used include normalised generalised Gamma processes that demonstrate power law properties , unlike Dirichlet processes used previously in dynamic topic modeling . Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process . Experiments performed on news , blogs , academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models . State - Space Inference for Non - Linear Latent Force Models with Application to Satellite Orbit Prediction . Jouni Hartikainen , Mari Sepp\\u00e4nen , Simo S\\u00e4rkk\\u00e4 . Abstract : Latent force models ( LFMs ) are flexible models that combine mechanistic modelling principles ( i.e. , physical models ) with non - parametric data - driven components . Several key applications of LFMs need non - linearities , which results in analytically intractable inference . In this work we show how non - linear LFMs can be represented as non - linear white noise driven state - space models and present an efficient non - linear Kalman filtering and smoothing based method for approximate state and parameter inference . We illustrate the performance of the proposed methodology via two simulated examples , and apply it to a real - world problem of long - term prediction of GPS satellite orbits . Sparse Additive Functional and Kernel CCA . Sivaraman Balakrishnan , Kriti Puniyani , John Lafferty . Abstract : Canonical Correlation Analysis ( CCA ) is a classical tool for finding correlations among the components of two random vectors . In recent years , CCA has been widely applied to the analysis of genomic data , where it is common for researchers to perform multiple assays on a single set of patient samples . Recent work has proposed sparse variants of CCA to address the high dimensionality of such data . However , classical and sparse CCA are based on linear models , and are thus limited in their ability to find general correlations . In this paper , we present two approaches to high - dimensional nonparametric CCA , building on recent developments in high - dimensional nonparametric regression . We present estimation procedures for both approaches , and analyze their theoretical properties in the high - dimensional setting . We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations , as well as through experiments with genomic data . The Kernelized Stochastic Batch Perceptron . Andrew Cotter , Shai Shalev - Shwartz , Nathan Srebro . Abstract : We present a novel approach for training kernel Support Vector Machines , establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach , and show that our method works well in practice compared to existing alternatives . Fast classification using sparse decision DAGs . Robert Busa - Fekete , Djalel Benbouzid , Balazs Kegl . Abstract : In this paper we propose an algorithm that builds sparse decision DAGs ( directed acyclic graphs ) out of a list of base classifiers provided by an external learning method such as AdaBoost . The basic idea is to cast the DAG design task as a Markov decision process . Each instance can decide to use or to skip each base classifier , based on the current state of the classifier being built . The result is a sparse decision DAG where the base classifiers are selected in a data - dependent way . The method has a single hyperparameter with a clear semantics of controlling the accuracy / speed trade - off . The algorithm is competitive with state - of - the - art cascade detectors on three object - detection benchmarks , and it clearly outperforms them in the regime of low number of base classifiers . Unlike cascades , it is also readily applicable for multi - class classification . Using the multi - class setup , we show on a benchmark web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker . A Combinatorial Algebraic Approach for the Identifiability of Low - Rank Matrix Completion . Franz Kir\\u00e1ly , Ryota Tomioka . Abstract : In this paper , we review the problem of matrix completion and expose its intimate relations with algebraic geometry and combinatorial graph theory . We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries , yielding theoretical constraints and new algorithms for the problem of matrix completion . We conclude with algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes , showing that the algebraic - combinatoric approach can lead to improvements over state - of - the - art matrix completion methods . Rethinking Collapsed Variational Bayes Inference for LDA . Issei Sato , Hiroshi Nakagawa . Abstract : We provide a unified view for existing inference algorithms of latent Dirichlet allocation by using the alpha - divergence projection . In particular , we explain the collapsed variational Bayes inference with zero - order information , called the CVB0 inference , in terms of the local alpha - divergence projection . Exact Maximum Margin Structure Learning of Bayesian Networks . Robert Peharz , Franz Pernkopf . Abstract : Recently , there has been much interest in finding globally optimal Bayesian network structures . % , which is NP - hard in general . These techniques were developed for generative scores and can not be directly extended to discriminative scores , as desired for classification . In this paper , we propose an exact method for finding network structures maximizing the probabilistic soft margin , a successfully applied discriminative score . Our method is based on branch - and - bound techniques within a linear programming framework and maintains an any - time solution , together with worst - case suboptimality bounds . We introduce a set of order constraints for enforcing the network structure to be acyclic , which allows a compact problem representation and the use of general - purpose optimization techniques . In classification experiments , our methods clearly outperform generatively trained network structures and compete with support vector machines . AOSO - LogitBoost : Adaptive One - Vs - One LogitBoost for Multi - Class Problem . Peng Sun , Mark Reid , Jie Zhou . Abstract : This paper is dedicated to the improvement of model learning in multi - class LogitBoost for classification . Motivated by statistical view , LogitBoost can be seen as additive tree regression . Important facts in such a setting are 1 ) coupled classifier output as sum - to - zero constraint and 2 ) dense Hessian matrix arising in tree node split gain and node values fitting . On the one hand , the setting is too complicated for a tractable model learning algorithm ; On the other hand , too aggressive simplification of the setting may lead to degraded performance . For example , the original LogitBoost is outperformed by ABC - LogitBoost due to the later 's more careful treatment for the above two key points in problem settings . In this paper we propose improved methods to address the challenge : we adopt 1 ) vector tree ( i.e. node value is vector ) that enforces sum - to - zero constraint and 2 ) adaptive block coordinate descent exploiting dense Hessian when computing tree split gain and node values . Higher classification accuracy and faster convergence rate are observed for a range of public data sets when comparing to both original and ABC LogitBoost . Hypothesis testing using pairwise distances and associated kernels . Dino Sejdinovic , Arthur Gretton , Bharath Sriperumbudur , Kenji Fukumizu . - Accepted . The equivalence holds when energy distances are computed with semimetrics of negative type , in which case a kernel may be defined such that the RKHS distance between distributions corresponds exactly to the energy distance . We determine the class of probability distributions for which kernels induced by semimetrics are characteristic ( that is , for which embeddings of the distributions to an RKHS are injective ) . Monte Carlo Bayesian Reinforcement Learning . Yi Wang , Kok Sung Won , David Hsu , Wee Sun Lee . Abstract : Bayesian reinforcement learning ( BRL ) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a posterior distribution over them . This paper proposes a simple and general approach to BRL . The POMDP does not require conjugate distributions for belief representation , as earlier works do , and can be solved relatively easily with point - based approximation algorithms . Our approach naturally handles both fully and partially observable worlds . Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL problem well with guaranteed performance . A Topic Model for Melodic Sequences . Athina Spiliopoulou , Amos Storkey . Abstract : We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre . This is a challenging task as one needs to tackle not only the complex statistical dependencies that characterize a music genre , but also the element of novelty , as each music piece is a unique realization of a musical form . To address this problem we introduce the Variable - gram Topic Model , which couples the latent topic formalism with a systematic model for contextual information . We evaluate the model using a next - step prediction task . Additionally , we present a novel way of model evaluation , where we directly compare model samples with data sequences using the Maximum Mean Discrepancy ( MMD ) of string kernels , to assess how close is the model distribution to the data distribution . We show that the model has the highest performance under both evaluation measures when compared to LDA , the Topic Bigram and related non - topic models . Output Space Search for Structured Prediction . Janardhan Rao Doppa , Alan Fern , Prasad Tadepalli . Abstract : We consider a framework for structured prediction based on search in the space of complete structured outputs . Given a structured input , an output is produced by running a time - bounded search procedure , guided by a learned cost function , and then returning the least cost output uncovered during the search . This framework can be instantiated for a wide range of search spaces and search procedures , and easily incorporates arbitrary structured - prediction loss functions . In this paper , we make two main technical contributions within this framework . First , we describe a novel approach to automatically defining an effective search space over structured outputs , which is able to leverage the availability of powerful classification learning algorithms . In particular , we define the limited - discrepancy search space and relate the quality of that space to the quality of learned classifiers . Second , we give a generic cost function learning approach that can be instantiated for a wide range of search procedures . The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function . Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state - of - the - art structured - prediction performance . How To Grade a Test Without Knowing the Answers - A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing . Yoram Bachrach , Thore Graepel , Tom Minka , John Guiver . Abstract : We propose a new probabilistic graphical model that jointly models the difficulties of questions , the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings . We devise an active learning / adaptive testing scheme based on a greedy minimization of expected model entropy , which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses . We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario . Parallelizing Exploration - Exploitation Tradeoffs with Gaussian Process Bandit Optimization . Thomas Desautels , Andreas Krause , Joel Burdick . Abstract : Can one parallelize complex exploration - exploitation tradeoffs ? As an example , consider the problem of optimal high - throughput experimental design , where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response , as well as identifying the maximum of the function . We formalize the task as a multi - armed bandit problem , where the unknown payoff function is sampled from a Gaussian process ( GP ) , and instead of a single arm , in each round we pull a batch of several arms in parallel . We develop GP - BUCB , a principled algorithm for choosing batches , based on the GP - UCB algorithm for sequential GP optimization . We prove that , perhaps surprisingly , the cumulative regret of the parallel algorithm , compared to the sequential approach , only increases by a constant factor independent of the batch size B , for any fixed B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization . We demonstrate the effectiveness of our approach on two real - world applications . A Simple Algorithm for Semi - supervised Learning with Improved Generalization Error Bound . Ming Ji , Tianbao Yang , Binbin Lin , Rong Jin , Jiawei Han . Abstract : In this work , we develop a simple algorithm for semi - supervised regression . The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression . We show that under appropriate assumptions about the integral operator , this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning . We also verify the effectiveness of the proposed algorithm by an empirical study . Bayesian Optimal Active Search and Surveying . Roman Garnett , Yamuna Krishnamurthy , Xuehan Xiong , Jeff Schneider , Richard Mann . - Accepted . Abstract : We consider two active binary - classification problems with atypical objectives . In the first , active search , our goal is to actively uncover as many members of a given class as possible . In the second , active surveying , our goal is to actively query points to ultimately predict the class proportion of a given class . Numerous real - world problems can be framed in these terms , and in either case typical model - based concerns such as generalization error are only of secondary importance . We approach these problems via Bayesian decision theory ; after choosing natural utility functions , we derive the optimal policies . We provide three contributions . In addition to introducing the active surveying problem , we extend previous work on active search in two ways . First , we prove a novel theoretical result , that less - myopic approximations to the optimal policy can outperform more - myopic approximations by any arbitrary degree . We then derive bounds that for certain models allow us to reduce ( in practice dramatically ) the exponential search space required by a na\\u00efve implementation of the optimal policy , enabling further lookahead while still ensuring the optimal decisions are always made . Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis . Deepti Pachauri , Maxwell Collins , Vikas SIngh . Abstract : Matching one set of objects to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem ( QAP ) . The QAP is known to be notoriously hard , both in theory and in practice . We propose a new approach to accelerating the solution of QAPs based on learning parameters for a modified objective function from prior QAP instances . Experiments show that in practical domains the new method can significantly outperform existing approaches . Lognormal and Gamma Mixed Negative Binomial Regression . Mingyuan Zhou , Lingbo Li , David Dunson , Lawrence Carin . Abstract : In regression analysis of counts , a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped . By placing a gamma distribution prior on the NB dispersion parameter r , and connecting a lognormal distribution prior with the logit of the NB probability parameter p , efficient Gibbs sampling and variational Bayes inference are both developed . The closed - form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya - Gamma distribution based data augmentation approach . The proposed Bayesian inference can be implemented routinely , while being easily generalizable to more complex settings involving multivariate dependence structures . The algorithms are illustrated using real examples . Greedy Algorithms for Sparse Reinforcement Learning . Christopher Painter - Wakefield , Ronald Parr . Abstract : Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning ( RL ) community to expand the reach and applicability of RL . One approach to the problem of feature selection is to impose a sparsity - inducing form of regularization on the learning method . Recent work on L_1 regularization has adapted techniques from the supervised learning literature for use with RL . Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features . Such algorithms have many of the good properties of the L_1 regularization methods , while also being extremely efficient and , in some cases , allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data . This paper considers variants of orthogonal matching pursuit ( OMP ) applied to reinforcement learning . The resulting algorithms are analyzed and compared experimentally with existing L_1 regularized approaches . We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails ; however , one variant , OMP - BRM , provides promising theoretical guarantees under certain assumptions on the feature dictionary . Another variant , OMP - TD , empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems . Variance Function Estimation in High - dimensions . Mladen Kolar , James Sharpnack . Abstract : We consider the high - dimensional heteroscedastic regression model , where the mean and the log variance are modeled as a linear combination of input variables . Existing literature on high - dimensional linear regression models has largely ignored non - constant error variances , even though they commonly occur in a variety of applications ranging from biostatistics to finance . In this paper we study a class of non - convex penalized pseudolikelihood estimators for both the mean and variance parameters . We show that the Heteroscedastic Iterative Penalized Pseudolikelihood Optimizer ( HIPPO ) achieves the oracle property , that is , we prove that the rates of convergence are the same as if the true model was known . We demonstrate numerical properties of the procedure on a simulation study and real world data . Conditional Sparse Coding and Grouped Multivariate Regression . Min Xu , John Lafferty . Abstract : We study the problem of multivariate regression where the data are naturally grouped , and a regression matrix is to be estimated for each group . We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups , and a sparse linear combination of the dictionary elements is estimated to form a model within each group . We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group . It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience . We propose an algorithm for conditional sparse coding , analyze its theoretical properties in terms of predictive accuracy , and present the results of simulation experiments that compare the new technique to reduced rank regression . Apprenticeship Learning for Model Parameters of Partially Observable Environments . Takaki Makino , Johane Takeuchi . Abstract : We consider apprentice learning - i.e. , having an agent learn a task by observing an expert demonstrating the task - in a partially observable environment when the model of the environment is uncertain . This setting is useful in applications where the explicit modeling of the environment is difficult , such as a dialogue system . We show that we can extract information about the environment model by inferring action selection process behind the demonstration , under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment . We show that our proposed algorithms can estimate the parameters of the environment model with much shorter demonstration compared to learning the model only from the reaction from the environment . Modeling Images using Transformed Indian Buffet Processes . KE ZHAI , Yuening Hu , Jordan Boyd - Graber , Sinead Williamson . Abstract : Latent feature models are attractive for image modeling ; images generally contain multiple objects . However , many latent feature models ignore that objects can appear at different locations , or require pre - segmentation of images . While the transformed Indian buffet process ( tIBP ) provides a method for modeling transformation - invariant features in simple , unsegmented binary images , in its current form it is inappropriate for real images because of computational constraints and modeling assumptions . We combine the tIBP with likelihoods appropriate for real images . We also develop an efficient inference scheme using the cross - correlation between images and features that is both theoretically and empirically faster than existing inference techniques . We demonstrate that , using our method , we are able to discover reasonable components and achieve effective image reconstruction in natural images . Learning Object Arrangements in 3D Scenes using Human Context . Yun Jiang , Marcus Lim , Ashutosh Saxena . Abstract : We consider the problem of learning object arrangements in a 3D scene . The key idea here is to learn how objects relate to human skeletons based their affordances , ease of use and reachability . We design appropriate density functions based on 3D spatial features to capture this . We then learn the distribution of human poses in a scene using Dirichlet processes . This allows our algorithm to reason about arrangement of the objects in the room . This is in contrast to previous approaches that learn context by learning object - object context such as co - occurrence relationships . We tested our algorithm extensively on 20 different rooms with a total of 47 objects . Our algorithm predicted correct placements with an error of 1.6 meters from the ground truth and received a score of 4.3/5 in arranging five real scenes compared to 3.7 of the best baseline . Cross Language Text Classification via Subspace Co - regularized Multi - view Learning . Yuhong Guo , Min Xiao . Abstract : In many multilingual text classification problems , the documents in different languages often share the same set of categories . To reduce the labeling cost of training a classification model for each individual language , it is important to transfer knowledge gained from the labeled data in one language to another language by conducting cross language classification . In this paper we develop a novel subspace co - regularized multi - view learning method for cross language text classification . This method is built on parallel corpora produced by machine translation . It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents . Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms the alternative inductive methods , domain adaptation methods , and multi - view learning methods . Plug - in martingales for testing exchangeability on - line . Valentina Fedorova , Alex Gammerman , Ilia Nouretdinov , Volodya Vovk . Abstract : A standard assumption in machine learning is the exchangeability of data ; in other words , the examples are assumed to be generated from the same probability distribution independently . This paper is devoted to testing the assumption of exchangeability on - line : the examples arrive one by one , and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified . Such measure are provided by exchangeability martingales . We extend known techniques for constructing exchangeability martingales and show that our new method is competitive to martingales introduced before . Finally we investigate the performance of our testing method on two benchmark datasets , USPS and Statlog Satellite data ; for the former , the known techniques give satisfactory results , but for the latter our new more flexible method becomes necessary . An Iterative Locally Linear Embedding Algorithm . Deguang Kong , Chris H.Q. Ding . Abstract : Local Linear embedding(LLE ) is a popular dimension reduction method . In this paper , we first show LLE with nonnegative constraint is equivalent to the widely used Laplacian embedding . We further propose to iterate the two steps in LLE repeatedly to improve the results . Thirdly , we relax the kNN constraint of LLE and present a sparse similarity learning algorithm . The final Iterative LLE combines these three improvements . Extensive experiment results show that iterative LLE algorithm significantly improve both classification and clustering results . Gaussian Process Quantile Regression using Expectation Propagation . Alexis Boukouvalas , Remi Barillec , Dan Cornford . Abstract : Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables . We present a new framework for direct quantile regression where a Gaussian process model is learned , minimising the expected tilted loss function . The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm . We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets . Latent Multi - group Membership Graph Model . Myunghwan Kim , Jure Leskovec . Abstract : We develop the Latent Multi - group Membership Graph ( LMMG ) model , a model of networks with rich node feature structure . In the LMMG model , each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure . The LMMG can be used to summarize the network structure , to predict links between the nodes , and to predict missing features of a node . We derive efficient inference and learning algorithms and evaluate the predictive performance of the LMMG on several social and document network datasets . Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations . Nando de Freitas , Alex Smola , Masrour Zoghi . Abstract : This paper analyzes the problem of Gaussian process ( GP ) bandits with deterministic observations . The analysis uses a branch and bound algorithm that is related to the UCB algorithm of ( Srinivas et al , 2010 ) . To complement their result , we attack the deterministic case and attain a much faster exponential convergence rate . Here , d is the dimension of the search space and tau is a constant that depends on the behaviour of the objective function near its global maximum . Estimating the Hessian by Back - propagating Curvature . James Martens , Ilya Sutskever , Kevin Swersky . Abstract : In this work we develop Curvature Propagation ( CP ) , a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph . At the cost of roughly two gradient evaluations , CP can give a rank-1 approximation of the whole Hessian , and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian . Of particular interest is the diagonal of the Hessian , for which no general approach is known to exist that is both efficient and accurate . We show in experiments that CP turns out to work well in practice , giving very accurate estimates of the Hessian of neural networks , for example , with a relatively small amount of work . We also apply CP to Score Matching , where a diagonal of a Hessian plays an integral role in the Score Matching objective , and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models . Feature Selection via Probabilistic Outputs . Andrea Danyluk , Nicholas Arnosti . Abstract : This paper investigates two feature - scoring criteria that make use of estimated class probabilities : one method proposed by shen and a complementary approach proposed below . We develop a theoretical framework to analyze each criterion and show that both estimate the spread ( across all values of a given feature ) of the probability that an example belongs to the positive class . Based on our analysis , we predict when each scoring technique will be advantageous over the other and give empirical results validating those predictions . Hierarchical Exploration for Accelerating Contextual Bandits . Yisong Yue , Sue Ann Hong , Carlos Guestrin . Abstract : Contextual bandit learning is a popular approach to optimizing recommender systems , but can be slow to converge in practice due to the need for explorating a large feature space . In this paper , we propose a coarse - to - fine hierarchical approach for encoding prior knowl- 016 edge which drastically reduces the amount of exploration required . Intuitively , user preferences can be reasonably embedded in a coarse low - dimensional feature space that can be explored efficiently , requiring exploration in the high - dimensional space only as necessary . We introduce a bandit algorithm for exploring within this coarse - to - fine spectrum , and prove performance guarantees that depend on how well the coarser feature space captures the user 's preferences . We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation . Fast Training of Nonlinear Embedding Algorithms . Max Vladymyrov , Miguel Carreira - Perpinan . Abstract : Stochastic neighbor embedding and related nonlinear manifold learning algorithms achieve high - quality low - dimensional representations of similarity data , but are notoriously slow to train . We propose a generic formulation of embedding algorithms that includes SNE and other existing algorithms , and study their relation with spectral methods and graph Laplacians . This allows us to define several partial - Hessian optimization strategies , characterize their global and local convergence , and evaluate them empirically . We achieve up to two orders of magnitude speedup over existing training methods with a strategy that adds nearly no overhead to the gradient and yet is simple , scalable and applicable to several existing and future embedding algorithms . Fast Prediction of New Feature Utility . Hoyt Koepke , Mikhail Bilenko . Abstract : We study the new feature utility prediction problem : statistically testing whether adding a new feature to the data representation can improve predictive accuracy on a supervised learning task . In many applications , identifying new informative features is the primary pathway for improving performance . However , evaluating every potential feature by re - training the predictor with it can be costly computationally , logistically and financially . The paper describes an efficient , learner - independent technique for estimating new feature utility without re - training based on the current predictor 's outputs . The method is obtained by deriving a connection between loss reduction potential and the new feature 's correlation with the loss gradient of the current predictor . This leads to a simple yet powerful hypothesis testing procedure , for which we prove consistency . Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large - scale industrial dataset . Robust Classification with Adiabatic Quantum Optimization . Vasil Denchev , Nan Ding , SVN Vishwanathan , Hartmut Neven . Abstract : We propose a non - convex training objective for robust binary classification of data sets in which label noise is present . The design is guided by the intention of solving the resulting problem by adiabatic quantum optimization . Two choices are prompted by the engineering constraints of existing quantum hardware : training problems are formulated as quadratic unconstrained binary optimization ; and model parameters are represented as binary expansions of low bit - depth . In the present work we validate this approach by using a heuristic classical solver as a stand - in for quantum hardware . Testing on several popular data sets and comparing with a number of existing convex and non - convex losses solved by convex optimization , we find substantial advantages in robustness as measured by test error under increasing label noise . Robustness is enabled by the non - convexity of our hardware - compatible loss function , which we name q -loss . We emphasize that we do not claim any theoretical superiority of q -loss over other non - convex losses . In fact if solving them to optimality was possible , they could be just as good or better , but q -loss has one important property that all others are lacking - namely that it is compatible with quantum hardware . Agglomerative Bregman Clustering . Matus Telgarsky , Sanjoy Dasgupta . Abstract : This manuscript develops the theory of agglomerative clustering with Bregman divergences . Geometric smoothing techniques are developed to deal with degenerate clusters . To allow for cluster models based on exponential families with overcomplete representations , Bregman divergences are developed for nondifferentiable convex functions . Isoelastic Agents and Wealth Updates in Machine Learning Markets . Amos Storkey , Jono Millin , Krzysztof Geras . Abstract : Recently , prediction markets have shown considerable promise for developing flexible mechanisms for machine learning . In this paper agents with isoelastic utilities are considered , and it is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha - mixtures , with a particular form of mixing component relating to each agent 's wealth . We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents ( through payoffs on prediction of training targets ) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures . An efficient variational algorithm is given for market equilibrium computation . Quasi - Newton Methods : A New Direction . Philipp Hennig , Martin Kiefel . Abstract : Four decades after their invention , quasi - Newton methods are still state of the art in unconstrained numerical optimization . Although not usually interpreted thus , these are learning algorithms that fit a local quadratic approximation to the objective function . We show that many , including the most popular , quasi - Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions . This new notion elucidates some shortcomings of classical algorithms , and lights the way to a novel nonparametric quasi - Newton method , which is able to make more efficient use of available information at computational cost similar to its predecessors . No - Regret Learning in Extensive - Form Games with Imperfect Recall . Marc Lanctot , Richard Gibson , Neil Burch , Michael Bowling . Abstract : Counterfactual Regret Minimization ( CFR ) is an efficient no - regret learning algorithm for decision problems modeled as extensive games . CFR 's regret bounds depend on the requirement of perfect recall : players always remember information that was revealed to them and the order in which it was revealed . In games without perfect recall , however , CFR 's guarantees do not apply . In this paper , we present the first regret bound for CFR when applied to a general class of games with imperfect recall . In addition , we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game , but for the full game as well . We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains : die - roll poker , phantom tic - tac - toe , and Bluff . Information - theoretic Semi - supervised Metric Learning via Entropy Regularization . Gang Niu , Bo Dai , Makoto Yamada , Masashi Sugiyama . Abstract : We propose a general information - theoretic approach called Seraph for semi - supervised metric learning that does not rely upon the manifold assumption . It maximizes / minimizes entropies of labeled / unlabeled data pairs in the supervised / unsupervised part , which allows these two parts to be integrated in a natural and meaningful way . Furthermore , it is equipped with the hyper - sparsity : Given a certain probabilistic model parameterized by the learned metric , the posterior distribution and the resultant projection matrix are both ' sparse ' . Consequently , the metric learned by Seraph possesses high discriminability even under a noisy environment . The optimization problem of Seraph can be solved efficiently and stably by an EM - like scheme , where the E - Step is analytical and the M - Step is convex and ' smooth ' . Experiments demonstrate that Seraph compares favorably with many well - known metric learning methods . Fast approximation of matrix coherence and statistical leverage . Michael Mahoney , Petros Drineas , Malik Magdon - Ismail , David Woodruff . Abstract : The statistical leverage scores of a matrix A are the squared row - norms of the matrix containing its ( top ) left singular vectors and the coherence is the largest leverage score . These quantities have been of interest in recently - popular problems such as matrix completion and Nystrom - based low - rank matrix approximation ; in large - scale statistical data analysis applications more generally ; and since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms . The proposed algorithm runs in O(nd log n ) time , as opposed to the O(nd2 ) time required by the na\\u0131ve algorithm that involves computing an orthogonal basis for the range of A. This resolves an open question from ( Drineas et al . , 2006b ) and ( Mohri & Talwalkar , 2011 ) ; and our result leads to immediate improvements in coreset - based L2-regression , the estimation of the coherence of a matrix , and several related low - rank matrix problems . Interestingly , to achieve our result we judiciously apply random projections on both sides of A. . Predicting Manhole Events in New York City . Cynthia Rudin , Rebecca Passonneau , Axinia Radeva , Steve Ierome , Delfina Isaac . - Not for proceedings . Abstract : We present a knowledge discovery and data mining process developed as part of the Columbia / Con Edison project on manhole event prediction . This process can assist with real - world prioritization problems that involve raw data in the form of noisy documents requiring significant amounts of pre - processing . The documents are linked to a set of instances to be ranked according to prediction criteria . Our ranking results are currently used to help prioritize repair work on the Manhattan , Brooklyn , and Bronx electrical grids . Artist Agent : A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting . Ning Xie , Hirotaka Hachiya , Masashi Sugiyama . Abstract : Oriental ink painting , called Sumi - e , is one of the most appealing painting styles that has attracted artists around the world . Major challenges in computer - based Sumi - e simulation are to abstract complex scene information and draw smooth and natural brush strokes . To automatically find such strokes , we propose to model the brush as a reinforcement learning agent , and learn desired brush - trajectories by maximizing the sum of rewards in the policy search framework . We also provide elaborate design of actions , states , and rewards tailored for a Sumi - e agent . The effectiveness of our proposed approach is demonstrated through simulated Sumi - e experiments . On multi - view feature learning . Roland Memisevic . Abstract : Sparse coding is a common approach to learning local features for object recognition . Recently , there has been an increasing interest in learning features from spatio - temporal , binocular , or other multi - observation data , where the goal is to encode the relationship between images rather than the content of a single image . We discuss the role of multiplicative interactions and of squaring non - linearities in learning such relations . In particular , we show that training a sparse coding model whose filter responses are multiplied or squared amounts to jointly diagonalizing a set of matrices that encode image transformations . Inference amounts to detecting rotations in the shared eigenspaces . Our analysis helps explain recent experimental results showing that Fourier features and circular Fourier features emerge when training complex cell models on translating or rotating images . It also shows how learning about transformations makes it possible to learn invariant features . Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel - Based Online Learning . Steven C.H. Hoi , Jialei Wang , Peilin Zhao , Rong Jin , Pengcheng Wu . Abstract : Although kernel - based online learning has shown promising performance in a number of studies , its main shortcoming is the unbounded number of support vectors , making it non - scalable and unsuitable for large - scale datasets . In this work , we study the problem of bounded kernel - based online learning that aims to constrain the number of support vectors by a predefined budget . Although several algorithms have been proposed , they are mostly based on the Perceptron algorithm and only provide bounds for the number of classification mistakes . To address this limitation , we propose a framework for bounded kernel - based online learning based on online gradient descent approach . We propose two efficient algorithms of bounded online gradient descent ( BOGD ) for bounded kernel - based online learning : ( i ) BOGD using uniform sampling and ( ii ) BOGD++ using non - uniform sampling . We present theoretical analysis of regret bound for both algorithms , show the promising empirical performance of the proposed algorithms by comparing them to the state - of - the - art algorithms for bounded kernel - based online learning . A Hybrid Algorithm for Convex Semidefinite Optimization . Soeren Laue . Abstract : We present a hybrid algorithm for optimizing a convex , smooth function over the cone of positive semidefinite matrices . Our algorithm converges to the global optimal solution and can be used to solve general large - scale semidefinite programs and hence can be readily applied to a variety of machine learning problems . We show experimental results on three machine learning problems ( matrix completion , metric learning , and sparse PCA ) . Our approach outperforms state - of - the - art algorithms . A Complete Analysis of the l_1,p Group - Lasso . Julia Vogt , Volker Roth . Abstract : The Group - Lasso is a well - known tool for joint regularization in machine learning methods . For all p - norms , a highly efficient projected gradient algorithm is presented . This new algorithm enables us to compare the prediction performance of many variants of the Group - Lasso in a multi - task learning setting , where the aim is to solve many learning problems in parallel which are coupled via the Group - Lasso constraint . We conduct large - scale experiments on synthetic data and on two real - world data sets . Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models . Jean Honorio . Abstract : We study the convergence rate of stochastic optimization of exact ( NP - hard ) objectives , for which only biased estimates of the gradient are available . We motivate this problem in the context of learning the structure and parameters of Ising models . We first provide a convergence - rate analysis of deterministic errors for forward - backward splitting ( FBS ) . We then extend our analysis to biased stochastic errors , by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS , but also proximal gradient ( PG ) methods . Decoupling Exploration and Exploitation in Multi - Armed Bandits . Orly Avner , Shie Mannor , Ohad Shamir . Abstract : We consider a multi - armed bandit problem where the decision maker can explore and exploit different arms at every round . The exploited arm adds to the decision maker 's cumulative reward ( without necessarily observing the reward ) while the explored arm reveals its value . We devise algorithms for this setup and show that the dependence on the number of arms can be much better than the standard square root of the number of arms dependence , according to the behavior of the arms ' reward sequences . For the important case of piecewise stationary stochastic bandits , we show a significant improvement over existing algorithms . Our algorithms are based on a non - uniform sampling policy , which we show is essential to the success of any algorithm in the adversarial setup . We finally show some simulation results on an ultra - wide band channel selection inspired setting indicating the applicability of our algorithms . Learning to Identify Regular Expressions that Describe Email Campaigns . Paul Prasse , Christoph Sawade , Niels Landwehr , Tobias Scheffer . Abstract : This paper addresses the problem of inferring a regular expression from a given set of strings that resembles , as closely as possible , the regular expression that a human expert would have written to identify the language . This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns . Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist . We model this task as a learning problem with structured output spaces and an appropriate loss function , derive a decoder and the resulting optimization problem , and a report on a case study conducted with an email service . Deep Mixtures of Factor Analysers . Yichuan Tang , Ruslan Salakhutdinov , Geoffrey Hinton . Abstract : An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables . After learning each layer , samples from the posterior distributions for that layer are used as training data for learning the next layer . This approach is commonly used with Restricted Boltzmann Machines , which are undirected graphical models with a single hidden layer , but it can also be used with Mixtures of Factor Analysers ( MFAs ) which are directed graphical models . In this paper , we present a greedy layer - wise learning algorithm for Deep Mixtures of Factor Analysers ( DMFAs ) . We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machines on a wide variety of datasets . Revisiting k - means : New Algorithms via Bayesian Nonparametrics . Brian Kulis , Michael Jordan . Abstract : Bayesian models offer great flexibility for clustering applications - Bayesian nonparametrics can be used for modeling infinite mixtures , and hierarchical Bayesian models can be utilized for shared clusters across multiple data sets . For the most part , such flexibility is lacking in classical clustering methods such as k - means . In this paper , we revisit the k - means clustering algorithm from a Bayesian nonparametric viewpoint . We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process . We also discuss further extensions that highlight the benefits of our analysis : i ) a spectral relaxation involving thresholded eigenvectors , and ii ) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph . Gaussian Process Regression Networks . Andrew Wilson , David A. Knowles , Zoubin Ghahramani . Abstract : We introduce a new regression framework , Gaussian process regression networks ( GPRN ) , which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian processes . GPRN accommodates input ( predictor ) dependent signal and noise correlations between multiple output ( response ) variables , input dependent length - scales and amplitudes , and heavy - tailed predictive distributions . We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN . We apply GPRN as a multiple output regression and multivariate volatility model , demonstrating substantially improved performance over eight popular multiple output ( multi - task ) Gaussian process models and three multivariate volatility models on real datasets , including a 1000 dimensional gene expression dataset . Analysis of Kernel Mean Matching under Covariate Shift . Yaoliang Yu , Csaba Szepesvari . Abstract : In real supervised learning scenarios , it is not uncommon that the training and test sample follow different probability distributions , thus rendering the necessity to correct the sampling bias . Focusing on a particular covariate shift problem , we derive high probability confidence bounds for the kernel mean matching ( KMM ) estimator , whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel . By comparing KMM with the natural plug - in estimator , we establish the superiority of the former hence provide concrete evidence / understanding to the effectiveness of KMM under covariate shift . Tighter Variational Representations of f - Divergences via Restriction to Probability Measures . Avraham Ruderman , Mark Reid , Dar\\u00edo Garc\\u00eda - Garc\\u00eda , James Petterson . Abstract : We show that the variational representations for f - divergences currently used in the literature can be tightened . This has implications to a number of methods recently proposed based on this representation . As an example application we use our tighter representation to derive a general f - divergence estimator based on two i.i.d . samples and derive the dual program for this estimator that performs well empirically . We also point out a connection between our estimator and MMD . Training Restricted Boltzmann Machines on Word Observations . George Dahl , Ryan Adams , Hugo Larochelle . Abstract : The restricted Boltzmann machine ( RBM ) is a flexible tool for modeling complex data , however there have been significant computational difficulties in using RBMs to model high - dimensional multinomial observations . In natural language processing applications , words are naturally modeled by K - ary discrete distributions , where K is determined by the vocabulary size and can easily be in the hundreds of thousands . The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K - way softmax visible units during block Gibbs updates , an operation that takes time linear in K. In this work , we address this issue by employing a more general class of Markov chain Monte Carlo operators on the visible units , yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n - grams using larger vocabularies than previously feasible and using the learned features to improve performance on chunking and sentiment classification tasks , achieving state - of - the - art results on the latter . Bayesian Watermark Attacks . Ivo Shterev , David Dunson . Abstract : This paper presents an application of statistical machine learning to the field of watermarking . We develop a new attack model on spread - spectrum watermarking systems , using Bayesian statistics . Our model jointly infers the watermark signal and embedded message bitstream , directly from the watermarked signal . No access to the watermark decoder is required . We develop an efficient Markov chain Monte Carlo sampler for updating the model parameters from their conjugate full conditional posteriors . We also provide a variational Bayesian solution , which further increases the convergence speed of the algorithm . Experiments with synthetic and real image signals demonstrate that the attack model is able to correctly infer a large part of the message bitstream , while at the same time obtaining a very accurate estimate of the watermark signal . Max - Margin Nonparametric Latent Feature Models for Link Prediction . Jun Zhu . Abstract : Recent work on probabilistic latent feature models have shown great promise in predicting unseen links in social network and relational data . We present a max - margin nonparametric latent feature model , which unites the ideas of max - margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension . By minimizing a hinge - loss using the linear expectation operator , we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function ; by using a fully - Bayesian formulation , we can avoid tuning regularization constants . Experimental results on real datasets appear to demonstrate the benefits inherited from max - margin learning and fully - Bayesian nonparametric inference . Discriminative Probabilistic Prototype Learning . Edwin Bonilla , Antonio Robles - Kelly . Abstract : In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where each original input datapoint is described by a set of vectors and their associated outputs may be given by soft labels indicating , for example , class probabilities . We represent an input datapoint as a mixture of probabilities over the corresponding set of feature vectors where each probability indicates how likely each vector is to belong to an unknown prototype pattern . We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization . More importantly , both the model parameters and the prototype patterns can be learned from data in a discriminative way . We show that our model can be seen as a probabilistic generalization of learning vector quantization ( LVQ ) . We apply our method to the problems of shape classification , hyperspectral imaging classification and people 's work class categorization , showing the superior performance of our method compared to the standard prototype - based classification approach and other competitive benchmark methods . Sparse - GEV : Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling . Yan Liu , Taha Bahadori , Hongfei Li . Abstract : In many applications of time series models , such as climate analysis or social media analysis , we are often interested in extreme events , such as heatwave , wind gust , and burst of topics . These time series data usually exhibit a heavy - tailed distribution rather than a normal distribution . This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events . In this paper , we propose the sparse - GEV model , a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions . Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies between multivariate extreme value time series . We demonstrate the superior performance of our algorithm compared with state - of - art methods , including Granger causality , copula approach , and transfer entropy , on one synthetic dataset , one climate dataset and one Twitter dataset . Factorized Asymptotic Bayesian Hidden Markov Models . Ryohei Fujimaki , Kohei Hayashi . Abstract : This paper addresses the issue of model selection for hidden Markov models ( HMMs ) . We generalize factorized asymptotic Bayesian inference ( FAB ) , which has been recently developed for model selection on independent hidden variables ( i.e. , mixture models ) , for time - dependent hidden variables . As with FAB in mixture models , FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion ( FIC ) . It inherits , from FAB for mixture models , several desirable properties for learning HMMs , such as asymptotic consistency of FIC with marginal log - likelihood , a shrinkage effect for hidden state selection , monotonic increase of the lower FIC bound through the iterative optimization . Further , it does not have a tunable hyper - parameter , and thus its model selection process can be fully automated . Experimental results shows that FAB outperforms states - of - the - art variational Bayesian HMM and non - parametric Bayesian HMM in terms of model selection accuracy and computational efficiency . PAC - Bayesian Generalization Bound on Confusion Matrix for Multi - Class Classification . Emilie Morvant , Sokol Ko\\u00e7o , Liva Ralaivola . Abstract : In this paper , we propose a PAC - Bayes bound for the generalization risk of the Gibbs classifier in the multi - class classification framework . The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure ; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate . Thanks to very recent and beautiful results on matrix concentration inequalities , we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper - bounded by its empirical risk plus a term depending on the number of training examples in each class . To the best of our knowledge , this is the first PAC - Bayes bounds based on confusion matrices . Semi - Supervised Learning of Class Balance under Class - Prior Change by Distribution Matching . Marthinus Du Plessis , Masashi Sugiyama . Abstract : In real - world classification problems , the class balance in the training dataset does not necessarily reflect that of the test dataset , which can cause significant estimation bias . If the class ratio of the test dataset is known , instance re - weighting or resampling allows systematical bias correction . However , learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain . In this paper , we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data . We demonstrate the utility of the proposed approach through experiments . A Proximal - Gradient Homotopy Method for the L1-Regularized Least - Squares Problem . Lin Xiao , Tong Zhang . Since the objective function is not strongly convex , standard proximal gradient methods only achieve sublinear convergence . We propose a homotopy continuation strategy , which employs a proximal gradient method to solve the problem with a sequence of decreasing regularization parameters . It is shown that under common assumptions in compressed sensing , the proposed method ensures that all iterates along the homotopy solution path are sparse , and the objective function is effectively strongly convex along the solution path . This observation allows us to obtain a global geometric convergence rate for the procedure . Empirical results are presented to support our theoretical analysis . Comparison - Based Learning with Rank Nets . Amin Karbasi , Stratis Ioannidis , laurent Massoulie . Abstract : We consider the problem of search through comparisons , where a user is presented with two candidate objects and reveals which is closer to her intended target . We study adaptive strategies for finding the target , that require knowledge of rank relationships but not actual distances between objects . We propose a new strategy based on rank nets , and show that for target distributions with a bounded doubling constant , it finds the target in a number of comparisons close to the entropy of the target distribution and , hence , of the optimum . We extend these results to the case of noisy oracles , and compare this strategy to prior art over multiple datasets . Semi - Supervised Collective Classification via Hybrid Label Regularization . Luke McDowell , David Aha . Abstract : Many classification problems involve data instances that are interlinked with each other , such as webpages connected by hyperlinks . Techniques for ' collective classification ' ( CC ) often increase accuracy for such data graphs , but usually require a fully - labeled training graph . In contrast , we examine how to improve the semi - supervised learning of CC models when given only a sparsely - labeled graph , a common situation . We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non - relational features . We also extend the ideas of ' label regularization ' to such hybrid classifiers , enabling them to leverage the unlabeled data to bias the learning process . We find that these techniques , which are efficient and easy to implement , significantly increase accuracy on three real datasets . In addition , our results explain conflicting findings from prior related studies . Shortest path distance in random k - nearest neighbor graphs . Morteza Alamgir , Ulrike von Luxburg . Abstract : Consider a weighted or unweighted k - nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R^d . We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity . We prove that for unweighted kNN graphs , this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning . We also study the behavior of the shortest path distance in weighted kNN graphs . A Split - Merge Framework for Comparing Clusterings . Qiaoliang Xiang , Qi Mao , Kian Ming Chai , Hai Leong Chieu , Ivor Tsang , Zhenddong Zhao . - Accepted . Abstract : Clustering evaluation measures are frequently used to evaluate the performance of algorithms . However , most measures are not suitable for this task mainly because they are not normalized properly and ignore some information in the inherent structure of clusterings such as dependency and consistency . We model two clusterings as a bipartite graph and propose a general component - based decomposition formula based on the components of the graph . Under this formula , most existing measures can be reinterpreted . In order to capture dependency and satisfy consistency in the component , we further propose a split - merge framework for comparing clusterings of different data sets . The proposed framework is conditionally normalized , flexible to utilize previous measures , and extensible to take into account data point information , such as feature vectors and pairwise distances . Moreover , experimental results on a real world data set demonstrate that one instantiated measure of the framework outperforms other measures . Compositional Planning Using Optimal Option Models . David Silver , Kamil Ciosek . Abstract : In this paper we introduce a framework for option model composition . Option models are temporal abstractions that , like macro - operators in classical planning , jump directly from a start state to an end state . Prior work has focused on constructing option models from primitive actions , by intra - option model learning ; or on using option models to construct a value function , by inter - option planning . We present a unified view of intra- and inter - option model learning , based on a major generalisation of the Bellman equation . Our fundamental operation is the recursive composition of option models into other option models . This key idea enables compositional planning over many levels of abstraction . We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals , and also searches over those option models to provide rapid progress towards other subgoals . Information - Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation . Yuan Shi , Fei Sha . Abstract : We study the problem of unsupervised domain adaptation , which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain . Many existing approaches first learn domain - invariant features and then construct classifiers with them . We propose a novel approach that jointly learn the both . Specifically , the proposed method identifies a feature space where data in the source and the target domains are similarly distributed . More importantly , our method learns a discriminative feature space , optimizing an information - theoretic metric as an proxy to the expected misclassification error on the target domain . We show how this optimization can be effectively carried out with simple gradient - based methods and how hyperparameters can be cross validated without demanding any labeled data from the target domain . Empirical studies on the benchmark tasks of visual object recognition and sentiment analysis validate our modeling assumptions and demonstrate significant improvement of our method over competing ones in classification accuracies . Flexible Modeling of Latent Task Structures in Multitask Learning . Alexandre Passos , Piyush Rai , Jacques Wainer , Hal Daume III . Abstract : When learning multiple related tasks with limited training data per task , it is natural to share parameters across tasks to improve generalization performance . Imposing the correct notion of task relatedness is critical to achieve this goal . However , one rarely knows the correct relatedness notion a priori . We present a generative model that is based on the weight vector of each task being drawn from a nonparametric mixture of nonparametric factor analyzers . The nonparametric aspect of the model makes it broad enough to subsume many types of latent task structures previously considered in the literature as special cases , and also allows it to learn more general task structures , addressing their individual shortcomings . This brings in considerable flexibility as compared to the commonly used multitask learning models that are based on some a priori fixed notion of task relatedness . We also present an efficient variational inference algorithm for our model . Experimental results on synthetic and real - world datasets , on both regression and classification problems , establish the efficacy of the proposed method . An Efficient Approach to Sparse Linear Discriminant Analysis . Luis Francisco S\\u00e1nchez Merchante , Yves Grandvalet , G\\u00e9rrad Govaert . Abstract : We present the Group - Lasso Optimal Scoring Solver ( GLOSS ) , a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis ( LDA ) . Our formulation , which is based on penalized Optimal Scoring , preserves an exact equivalence with sparse LDA , contrary to the multi - class approaches based on the regression of class indicator that have been proposed so far . Additionally , the versatility of the implementation allows to impose some particular structure on the within - class covariance matrix . Computationally , the optimization algorithm considers two nested convex problems , the main one being a linear regression regularized by a quadratic penalty implementing the group - lasso penalty . As group - Lasso selects the same features in all discriminant directions , it generates extremely parsimonious models without compromising the prediction performances . Moreover , the resulting sparse discriminant directions are amenable to low - dimensional representations . Our algorithm is highly efficient for medium to large number of variables , and is thus particularly well suited to the analysis of gene expression data . The Greedy Miser : Learning under Test - time Budgets . Zhixiang Xu , Kilian Weinberger , Olivier Chapelle . Abstract : As machine learning algorithms enter applications in industrial settings , there is increased interest in controlling their cpu - time during testing . The cpu - time consists of the running time of the algorithm and the extraction time of the features . The latter can vary drastically when the feature set is diverse . In this paper , we propose an algorithm , the Greedy Miser , that incorporates the feature extraction cost during training to explicitly minimize the cpu - time during testing . The algorithm is a straightforward extension of stage - wise regression and is equally suitable for regression or multi - class classification . Compared to prior work , it is significantly more cost - effective and scales to larger data sets . Canonical Trends : Detecting Trend Setters in Web Data . Felix Biessmann , Jens - Michalis Papaioannou , Mikio Braun , Andreas Harth . - Accepted . Abstract : The web offers large amounts of information most of which is just copied or rephrased , a phenomenon that is often called trend . A central problem in the context of web data mining is to detect those web sources that publish information which will give rise to a trend first . Here we present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to this trend before other websites do so . We validate our approach on real data collected from the most influential technology news feeds . A convex relaxation for weakly supervised classifiers . Armand Joulin , Francis Bach . Abstract : This paper introduces a general multi - class approach to weakly supervised classification . Inferring the labels and learning the parameters of the model is usually done jointly through a block - coordinate descent algorithm such as expectation - maximization ( EM ) , which may lead to local minima . To avoid this problem , we propose a cost function based on a convex relaxation of the soft - max loss . We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program ( SDP ) . Empirically , our method compares favorably to standard ones on different datasets for multiple instance learning and semi - supervised learning as well as on clustering tasks . Demand - Driven Clustering in Relational Domains for Predicting Adverse Drug Events . Jesse Davis , Vitor Santos Costa , Elizabeth Berg , David Page , Peggy Peissig , Michael Caldwell . - Accepted . Abstract : Learning from electronic medical records ( EMR ) is challenging due to their relational nature and the need to capture the uncertain dependence between a patient 's past and future health status . Statistical relational learning ( SRL ) , which combines relational and probabilistic representations , is a natural fit for analyzing EMRs . SRL is less adept at handling the inherent latent structure , such as connections between related medications or diseases , in EMRs . One solution is to capture the latent structure via a relational clustering of objects . We propose a novel approach that , instead of pre - clustering the objects , performs a demand - driven clustering during the learning process . We evaluate our algorithm on three real - world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication . We find that the proposed approach is more accurate than performing no clustering , pre - clustering and using expert - constructed medical heterarchies . A Binary Classification Framework for Two - Stage Multiple Kernel Learning . Abhishek Kumar , Alexandru Niculescu - Mizil , Koray Kavukcuoglu , Hal Daume III . - Accepted . Abstract : With the advent of kernel methods , automating the task of specifying a suitable kernel has become increasingly important . In this context , the Multiple Kernel Learning ( MKL ) problem of finding a combination of pre - specified base kernels that is suitable for the task at hand has received significant attention from researchers . In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel . Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler , and , arguably , more accessible to practitioners . Experiments on nine data sets from different domains show that , despite their simplicity , the proposed techniques can significantly outperform current leading MKL approaches . Learning Invariant Representations with Local Transformations . Kihyuk Sohn , Honglak Lee . Abstract : The difficulty of developing feature learning algorithms that are robust to the novel transformations ( e.g. , scale , rotation , or translation ) has been a challenge in many applications ( e.g. , object recognition problems ) . In this paper , we address this important problem of transformation invariant feature learning by introducing the transformation matrices into the energy function of the restricted Boltzmann machines . Consistent Multilabel Ranking through Univariate Losses . Krzysztof Dembczy\\u0144ski , Wojciech Kot\\u0142owski , Eyke Huellermeier . Abstract : We consider the problem of rank loss minimization in the setting of multilabel classification , which is commonly tackled by means of convex surrogate losses defined on pairs of labels . Very recently , this approach was put into question by the negative result showing that any such loss function is necessarily inconsistent . In this paper , we show a positive result which is arguably surprising in light of the previous one : despite being simpler , common convex surrogates for binary classification , defined on single labels instead of label pairs , are consistent for rank loss minimization . Instead of directly proving convergence , we give a much stronger result by deriving regret bounds and convergence rates . The proposed losses suggest efficient and scalable algorithms , which are tested experimentally . On the Equivalence between Herding and Conditional Gradient Algorithms . Francis Bach , Simon Lacoste - Julien , Guillaume Obozinski . Abstract : We show the equivalence of the herding procedure of Welling ( 2009 ) with a standard convex optimization algorithm - namely a conditional gradient algorithm minimizing a quadratic moment discrepancy . This link enables us to harness convergence results from convex optimization as well as suggest faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space . We study the behavior of the different variants with numerical simulations . The experiments indicate that while we can improve on the task of approximating integrals , the original herding algorithm tends to approach more often the maximum entropy distribution , shedding more light on the learning bias behind herding . Variational Bayesian Inference with Stochastic Search . John Paisley , David Blei , Michael Jordan . Abstract : Mean - field variational inference is an approximate posterior inference method for Bayesian models . It approximates the full posterior distribution of a model 's variables with a factorized set of distributions by maximizing a lower bound on the marginal likelihood . This requires the ability to integrate the log joint likelihood of the model with respect to the factorized approximation . Often not all integrals are closed - form , which is traditionally handled using lower bound approximations . We present an algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound in all models . This method uses control variates for variance reduction of the stochastic search gradient , in which existing lower bounds can play an important role . We demonstrate the approach on logistic regression and the hierarchical Dirichlet process . Small - sample brain mapping : sparse recovery on spatially correlated designs with randomization and clustering . Gael Varoquaux , Alexandre Gramfort , Bertrand Thirion . Abstract : Functional neuroimaging can measure the brain 's response to an external stimulus . It is used to perform brain mapping : identifying from these observations the brain regions involved . This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus . Brain mapping is then seen as a support recovery problem . On functional MRI ( fMRI ) data , this problem is particularly challenging as i ) the number of samples is small due to limited acquisition time and ii ) the variables are strongly correlated . We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables . The use of randomization techniques , e.g. bootstrap samples , and hierarchical clustering of the variables improves the recovery properties of sparse methods . We demonstrate the benefit of our approach on an extensive simulation study as well as two publicly available fMRI datasets . Joint Optimization and Variable Selection of High - dimensional Gaussian Processes . Bo Chen , Rui Castro , Andreas Krause . Abstract : Maximizing high - dimensional , non - convex functions through noisy observations is a notoriously hard problem , but one that arises in many applications . In this paper , we tackle this challenge by modeling the unknown function as a sample from a high - dimensional Gaussian process ( GP ) distribution . Assuming that the unknown function only depends on few relevant variables , we show that it is possible to perform joint variable selection and GP optimization . We provide strong performance guarantees for our algorithm , bounding the sample complexity of variable selection , and as well as providing cumulative regret bounds . We further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems . Large - Scale Feature Learning With Spike - and - Slab Sparse Coding . Ian Goodfellow , Aaron Courville , Yoshua Bengio . Abstract : We consider the problem of object recogni- tion with a large number of classes . In or- der to scale existing feature learning algo- rithms to this setting , we introduce a new feature learning and extraction procedure based on a factor model we call spike - and- slab sparse coding ( S3C ) . Prior work on this model has not prioritized the ability to ex- ploit parallel architectures and scale to the enormous problem sizes needed for object recognition . We present an inference proce- dure appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors . We demonstrate that this approach improves upon the supervised learning ca- pabilities of both sparse coding and the ss- RBM on the CIFAR-10 dataset . We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes bet- ter than previous methods . Finally , we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Mod- els ' Transfer Learning Challenge . Anytime Marginal MAP Inference . Denis Maua , Cassio De Campos . Abstract : This paper presents a new anytime algorithm for the marginal MAP problem in graphical models . The algorithm is described in detail , its complexity and convergence rate are studied , and relations to previous theoretical results for the problem are discussed . It is shown that the algorithm runs in polynomial - time if the underlying graph of the model has bounded tree - width , and that it provides guarantees to the lower and upper bounds obtained within a fixed amount of computational resources . Experiments with both real and synthetic generated models highlight its main characteristics and show that it compares favorably against Park and Darwiche 's systematic search , particularly in the case of problems with many MAP variables and moderate tree - width . Distributed Parameter Estimation via Pseudo - likelihood . Qiang Liu , Alexander Ihler . Abstract : Estimating statistical models within sensor networks requires distributed algorithms , in which both data and computation are distributed across the nodes of the network . We propose a general approach for distributed learning based on combining local estimators defined by pseudo - likelihood components , encompassing a number of combination methods , and provide both theoretical and experimental analysis . We show that simple linear combination or max - voting methods , when combined with second - order information , are statistically competitive with more advanced and costly joint optimization . Our algorithms have many attractive properties including low communication and computational cost and \\\" any - time \\\" behavior . The Nonparametric Metadata Dependent Relational Model . Dae Il Kim , Michael Hughes , Erik Sudderth . Abstract : We introduce the nonparametric metadata dependent relational ( NMDR ) model , a Bayesian nonparametric extension of previous stochastic block models . The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities . Learned regression models allow these memberships to depend on , and be predicted from , arbitrary node metadata . We develop efficient MCMC algorithms for learning NMDR models from ( partially observed ) node relationships . Retrospective MCMC methods allow our sampler to work directly with the infinite stick - breaking representation of the NMDR , avoiding the need for finite truncations . Our results demonstrate recovery of useful latent communities from real - world social and ecological networks , and the usefulness of metadata in link prediction tasks . Deep Lambertian Networks . Yichuan Tang , Ruslan Salakhutdinov , Geoffrey Hinton . Abstract : Visual perception is a challenging problem in part due to illumination variations . A possible solution is to first estimate an illumination invariant representation before using it for recognition . The object albedo and surface normals are examples of such representation . In this paper , we introduce a multilayer generative model where the latent variables include the albedo , surface normals , and the light source . Combining Deep Belief Nets with the Lambertian reflectance assumption , our model can learn good priors over the albedo from 2D images . Illumination variations can be explained by changing only the lighting latent variable in our model . By transferring learned knowledge from similar objects , albedo and surface normals estimation from a single image is possible in our model . Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one - shot face recognition . Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients . Iftekhar Naim , Daniel Gildea . Abstract : The speed of convergence of the Expectation Maximization ( EM ) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components . In this paper , we study the impact of mixing coefficients on the convergence of EM . We show that when the mixture components exhibit some overlap , the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases . We propose a deterministic anti - annealing algorithm , that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients . The proposed algorithm is compared against other standard optimization techniques like LBFGS , Conjugate Gradient , and the traditional EM algorithm . Finally , we propose a similar deterministic anti - annealing based algorithm for the Dirichlet process mixture model fitting and demonstrate its advantages over the conventional variational Bayesian approach . A Joint Model of Language and Perception for Grounded Attribute Learning . Cynthia Matuszek , Nicholas FitzGerald , Luke Zettlemoyer , Liefeng Bo , Dieter Fox . - Accepted . Abstract : As robots become more ubiquitous and capable , it becomes ever more important to enable untrained users to easily interact with them . Recently , this has led to study of the language grounding problem , where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world . In this paper , we present an approach for joint learning of language and perception models for grounded attribute induction . Our perception model includes attribute classifiers , for example to detect object color and shape , and the language model is based on a probabilistic categorial grammar that enables the construction of rich , compositional meaning representations . The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace . We demonstrate accurate task performance and effective latent - variable concept induction in physical grounded scenes . Learning Parameterized Skills . Bruno Da Silva , George Konidaris , Andrew Barto . Abstract : We introduce a method for constructing a single skill capable of solving any tasks drawn from a distribution of parameterized reinforcement learning problems . The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the geometry of the lower - dimensional manifold on which the skill policies lie . This manifold models how policy parameters change as the skill parameters are varied . We then apply non - linear regression to construct a parameterized skill by predicting policy parameters from task parameters . We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at any target location around it . Safe Exploration in Markov Decision Processes . Teodor Mihai Moldovan , Pieter Abbeel . Abstract : In unknown or partially unknown environments exploration is necessary to learn how to perform well . Existing reinforcement learning algorithms provide strong exploration guarantees , but they tend to rely on an ergodicity assumption . While ergodicity is a more generally applicable property , it is simplest to state for discrete state spaces , and its essence is that any state is reachable from any other state within some finite amount of time with non - zero probability by following a suitable policy . This assumption allows for exploration algorithms that operate by favoring visiting previously unvisited ( or rarely visited ) states . For most physical systems this assumption is completely impractical as the systems would break before any reasonable exploration has taken place , i.e. , most physical systems do n't satisfy the ergodicity assumption . In this paper we address the need for safe exploration methods in Markov decision processes . We first propose a general formulation of safety that can be thought of as forcing ergodicity . We show that imposing the safety constraint exactly is NP - hard . We present an efficient approximation algorithm for guaranteed safe , but potentially suboptimal , exploration . At the core of our approach is an optimization formulation in which the constraints restrict attention to guaranteed safe policies . The objective favors exploration policies . Our framework is compatible with the majority of previously proposed exploration methods , which rely on an exploration bonus , which we can replicate in the objective . Our experiments show that our method is able to safely explore state - spaces in which classical exploration methods get stuck . Improved Estimation in Time Varying Models . Doina Precup , Philip Bachman . Abstract : Locally adapted parametrizations of a model ( such as locally weighted regression ) are expressive but often suffer from high variance . We describe an approach for reducing the variance , based on the idea of estimating simultaneously a transformed space for the model , as well as locally adapted parameterizations in this new space . We present a new problem formulation that captures this idea and illustrate it in the important context of time - varying models . We develop an algorithm for learning a set of bases for approximating a time - varying sparse network ; each learned basis constitutes an archetypal sparse network structure . We also provide an extension for learning task - specific bases . We present empirical results on synthetic data sets , as well as on a BCI EEG classification task . Poisoning Attacks against Support Vector Machines . Battista Biggio , Blaine Nelson , Pavel Laskov . Abstract : We investigate a family of poisoning attacks against Support Vector Machines ( SVM ) . Such attacks amount to injecting specially crafted training data so as to increase the test error of the trained SVM . Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well - behaved distribution . However , this iassumption does not generally hold in security - sensitive settings . As we demonstrate in this contribution , an intelligent adversary can to some extent predict the change of the SVM decision function in response to malicious input and use this ability to construct malicious data points . The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM 's optimal solution . The gradient ascent method can be easily kernelized and enables the attack to be constructed in the input space even for non - linear kernels . We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non - convex validation error surface and inflicts a significant damage on the test error of the trained classifier . Regularizers versus Losses for Nonlinear Dimensionality Reduction : A Factored View with New Convex Relaxations . James Neufeld , Yaoliang Yu , Xinhua Zhang , Ryan Kiros , Dale Schuurmans . Abstract : We demonstrate that almost all non - parametric dimensionality reduction methods can be expressed by a simple procedure : regularized loss minimization plus singular value truncation . By distinguishing the role of the loss and regularizer in such a process , we recover a factored perspective that reveals some gaps in the current literature . Beyond identifying a useful new loss for manifold unfolding , a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction . These regularizers can be applied to any loss . Utilizing Static Analysis and Code Generation to Accelerate Neural Networks . Lawrence McAfee , Kunle Olukotun . Abstract : As datasets continue to grow , neural network ( NN ) applications are becoming increasingly limited by both the amount of available computational power and the ease of developing high - performance applications . Researchers often must have expert systems knowledge to make their algorithms run efficiently . Although available computing power approximately doubles every two years , algorithm efficiency is not able to keep pace due to the use of general purpose compilers , which are not able to fully optimize specialized application domains . Within the domain of NNs , we have the added knowledge that network architecture remains constant during training , meaning the architecture 's data structure can be statically optimized by a compiler . In this paper , we present SONNC , a compiler for NNs that utilizes static analysis to generate optimized parallel code . We show that SONNC 's use of static optimizations make it able to outperform optimized MATLAB code by up to 242X. Additionally , we show that use of SONNC significantly reduces code complexity when using structurally sparse networks . Similarity Learning for Provably Accurate Sparse Linear Classification . Aur\\u00e9lien Bellet , Amaury Habrard , Marc Sebban . Abstract : In recent years , the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions . Most of the state of the art focus on learning Mahalanobis distances ( requiring to fulfill a constraint of positive semi - definiteness ) for use in a local k - NN algorithm . However , no theoretical link is established between the learned metrics and their performance in classification . In this paper , we make use of the formal framework of good similarities introduced by Balcan et al . to propose an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space , tailored to a use in a global linear classifier . We show that our approach has the property of stability , allowing us to derive a generalization bound on the classification error . Experiments performed on various datasets confirm the effectiveness of our approach compared to state - of - the - art methods and provide evidence that ( i ) it is fast , ( ii ) robust to overfitting and ( iii ) produces very sparse models . Variational Inference in Non - negative Factorial Hidden Markov Models for Efficient Audio Source Separation . Gautham Mysore , Maneesh Sahani . Abstract : The last decade has seen substantial work on the use of non - negative matrix factorization ( NMF ) and its probabilistic counterparts for audio source separation . Although able to capture audio spectral structure well , these models neglect the non - stationarity and temporal dynamics that are important properties of audio . The recently proposed non - negative factorial hidden Markov model ( N - FHMM ) introduces a temporal dimension and improves source separation performance . However , the factorial nature of this model makes the complexity of inference exponential in the number of sound sources . Here , we present a Bayesian variant of the N - FHMM suited to an efficient variational inference algorithm , whose complexity is linear in the number of sound sources . Our algorithm performs comparably to exact inference in the original NFHMM but is significantly faster . In typical configurations of the N - FHMM , our method achieves around a 30x increase in speed . Conditional Likelihood Maximization : A Unifying Framework for Information Theoretic Feature Selection . Gavin Brown , Adam Pocock , Ming - Jie Zhao , Mikel Lujan . Abstract : We present a unifying framework for information theoretic feature selection , bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation . This is in response to the question : \\\" what are the implicit statistical assumptions of feature selection criteria based on mutual information ? \\\" While many hand - designed heuristic criteria try to optimize a definition of feature ' relevancy ' and ' redundancy ' , our approach leads to a probabilistic framework which naturally incorporates these concepts . As a result we unify the numerous criteria published over the last two decades , and show them to be low - order approximations to the exact ( but intractable ) optimization problem . The primary contribution is to show that common heuristics for information based feature selection ( including Markov Blanket algorithms as a special case ) are approximate iterative maximizers of the conditional likelihood . A large empirical study provides strong evidence to favor certain classes of criteria , in particular those that balance the relative size of the relevancy / redundancy terms . Overall we conclude that the JMI criterion ( Yang and Moody , 1999 ; Meyer et al . , 2008 ) provides the best tradeoff in terms of accuracy , stability , and flexibility with small data samples . Communications Inspired Linear Discriminant Analysis . Minhua Chen , William Carson , Miguel Rodrigues , Lawrence Carin , Robert Calderbank . - Accepted . Abstract : We study the problem of supervised linear dimensionality reduction , taking an information - theoretic viewpoint . The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label ( based on a Shannon entropy measure ) . By harnessing a recent theoretical result on the gradient of mutual information , the above optimization problem can be solved directly using gradient descent , without requiring simplification of the objective function . Relative to these alternative approaches , the proposed method achieves promising results on real datasets . Sparse stochastic inference for latent Dirichlet allocation . David Mimno , Matt Hoffman , David Blei . Abstract : We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference . The resulting method scales to a corpus of 1.2 million books comprising 33 billion words with thousands of topics on one CPU . This approach reduces the bias of variational inference and generalizes to many Bayesian hidden - variable models . Stochastic Smoothing for Nonsmooth Minimizations : Accelerating SGD by Exploiting Structure . Hua Ouyang , Alexander Gray . Abstract : In this work we consider the stochastic minimization of nonsmooth convex loss functions , a central problem in machine learning . We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent ( ANSGD ) , which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs . It is the first stochastic algorithm that can achieve the optimal O(1/t ) rate for minimizing nonsmooth loss functions . The fast rates are confirmed by empirical comparisons , in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD . A Convex Feature Learning Formulation for Latent Task Structure Discovery . Pratik Jawanpuria , J. Saketha Nath . Abstract : This paper considers the multi - task learning problem and in the setting where some relevant features could be shared across few related tasks . Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori . In real - world applications however , it is desirable to automatically discover the groups of related tasks that share a feature space . In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space . The main contribution is a convex formulation that employs a graph - based regularizer and simultaneously discovers few groups of related tasks , having close - by task parameters , as well as the feature space shared within each group . An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented . The algorithm is guaranteed to solve the proposed formulation ( within some precision ) in a time polynomial in the number of groups of related tasks discovered . Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state - of - the - art multi - task learning algorithms in some cases . Efficient Decomposed Learning for Structured Prediction . Rajhans Samdani , Dan Roth . Abstract : Structured prediction is the cornerstone of several machine learning applications . Unfortunately , in structured prediction settings with expressive inter - variable interactions , inference and hence exact learning is often intractable . We present a new way , Decomposed Learning ( DecL ) , for performing efficient learning over structured output spaces . In DecL , we restrict the inference step to a limited part of the output space . We use characterizations based on the structure , target parameters , and gold labels to guarantee that DecL with limited inference is equivalent to exact learning . We also show that in real world settings , where our theoretical assumptions may not hold exactly , DecL - based algorithms are significantly more efficient and provide accuracies close to exact learning . Path Integral Policy Improvement with Covariance Matrix Adaptation . Freek Stulp , Olivier Sigaud . Abstract : There has been a recent focus in reinforcement learning ( RL ) on addressing continuous state and action problems by optimizing parameterized policies . PI2 is a recent example of this approach . It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory . In this paper , we consider PI2 as a member of the wider family of methods which share the concept of probability - weighted averaging to iteratively update parameters to optimize a cost function . We compare PI2 to other members of the same family - Cross - Entropy Methods and CMA - ES - at the conceptual level and in terms of performance . The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for \\\" Path Integral Policy Improvement with Covariance Matrix Adaptation \\\" . PI2-CMA 's main advantage is that it determines the magnitude of the exploration noise automatically . Optimizing F - measure : A Tale of Two Approaches . Ye Nan , Kian Ming Chai , Wee Sun Lee , Hai Leong Chieu . Abstract : F - measures are popular performance metrics , particularly for tasks with imbalanced data sets . Algorithms for learning to maximize F - measures follow two approaches : the empirical utility maximization ( EUM ) approach learns a classifier having optimal performance on training data , while the decision - theoretic approach learns a probabilistic model and then predict labels with maximum expected F - measure . In this paper , we investigate the theoretical justifications and connections for these two approaches , and we study the conditions under which one approach is preferable to the other using synthetic and real datasets . Given accurate models , our results suggest that the two approaches are asymptotically equivalent given large training and test sets . Nevertheless , the EUM approach appears to be more robust against model misspecification , and given a good model , the decision - theoretic approach appears to be better for handling rare classes and for a common domain adaptation scenario . Efficient Euclidean Projections onto the Intersection of Norm Balls . Adams Wei Yu , Hao Su , Li Fei - Fei . Abstract : Using sparse - inducing norms to learn robust models has received increasing attention from many fields for its attractive properties . Projection - based methods have been widely applied to learning tasks constrained by such norms . We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic . Hence , a bisection algorithm is sufficient to solve the problem . Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage . We further show that embedded with our efficient projection operator , projection - based algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy . Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization . Alexander Rakhlin , Ohad Shamir , Karthik Sridharan . Abstract : Stochastic gradient descent ( SGD ) is a simple and popular method to solve stochastic optimization problems which arise in machine learning . For strongly convex problems , its convergence rate was known to be O(log ( T)/T ) , by running SGD for T iterations and returning the average point . However , recent results showed that using a different algorithm , one can get an optimal O1/T ) rate . This might lead one to believe that standard SGD is suboptimal , and maybe should even be replaced as a method of choice . In this paper , we investigate the optimality of SGD in a stochastic setting . We show that for smooth problems , the algorithm attains the optimal O(1/T ) rate . However , for non - smooth problems , the convergence rate with averaging might really be \\u03a9(log ( T)/T ) , and this is not just an artifact of the analysis . On the flip side , we show that a simple modification of the averaging step suffices to recover the O(1/T ) rate , and no other change of the algorithm is necessary . We also present experimental results which support our findings , and point out open problems . Clustering using Max - norm Constrained Optimization . Ali Jalali , Nathan Srebro . Abstract : We suggest using the max - norm as a convex surrogate constraint for clustering . We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear - norm relaxation , and study the effectiveness of our method , and other related convex relaxations , compared to other approaches . Submodular Inference of Diffusion Networks from Multiple Trees . Manuel Gomez Rodriguez , Bernhard Sch\\u00f6lkopf . Abstract : Diffusion and propagation of information , influence and diseases take place over increasingly larger networks . We observe when a node copies information , makes a decision or becomes infected but networks are often hidden or unobserved . Since networks are highly dynamic , changing and growing rapidly , we only observe a relatively small set of cascades before a network changes significantly . Scalable network inference based on a small cascade set is then necessary for understanding the rapidly evolving dynamics that govern diffusion . In this article , we develop a scalable approximation algorithm with provable near - optimal performance which achieves a high accuracy in such scenario , solving an open problem first introduced by Gomez - Rodriguez et al ( 2010 ) . Experiments on synthetic and real diffusion data show that our algorithm in practice achieves an optimal trade - off between accuracy and running time . Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds . Marek Petrik . Abstract : Approximate dynamic programming is a popular method for solving large Markov decision processes . This paper describes a new class of approximate dynamic programming ( ADP ) methods - distributionally robust ADP - that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss . This approach turns ADP into an optimization problem , for which we derive new mathematical program formulations and analyze itsp properties . DRADP improves on the theoretical guarantees of existing ADP methods - it guarantees convergence and L_1 norm - based error bounds . The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems . Modelling transition dynamics in MDPs with RKHS embeddings . Steffen Grunewalder , Guy Lever , Luca Baldassarre , Massi Pontil , Arthur Gretton . - Accepted . Abstract : We propose a new , nonparametric approach to learning and representing transition dynamics in Markov decision processes ( MDPs ) , which can be combined easily with dynamic programming methods for policy optimisation and value estimation . This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space ( RKHS ) . Such representations bypass the need for estimating transition probabilities or densities , and apply to any domain on which kernels can be defined . This avoids the need to calculate intractable integrals , since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding . In experiments , we investigate a learning task in a typical classical control setting ( the under - actuated pendulum ) , and on a navigation problem where only images from a sensor are observed . For policy optimisation we compare with least - squares policy iteration where a Gaussian process is used for value function estimation . For value estimation we also compare to the recent NPDP method . Our approach achieves better performance in all experiments . Approximate Principal Direction Trees . Mark McCartin - Lim , Andrew McGregor , Rui Wang . Abstract : We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree ( APD tree ) that adapts to the intrinsic dimension of the data . Our algorithm ensures vector - quantization accuracy similar to that of computationally - expensive PCA trees with similar time - complexity to that of lower - accuracy RP trees . APD trees use a small number of power - method iterations to find splitting planes for recursively partitioning the data . As such they provide a natural trade - off between the running - time and accuracy achieved by RP and PCA trees . Our theoretical results establish a ) strong performance guarantees regardless of the convergence rate of the power - method and b ) that O(log d ) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d . We demonstrate this trade - off and the efficacy of our data structure on both the CPU and GPU . Unachievable Region in Precision - Recall Space and Its Effect on Empirical Evaluation . Kendrick Boyd , Jesse Davis , David Page , Vitor Santos Costa . Abstract : Precision - recall ( PR ) curves and the areas under them are widely used to summarize machine learning results , especially for data sets exhibiting class skew . They are often used analogously to ROC curves and the area under ROC curves . It is already known that PR curves vary as class skew varies . What was not recognized before this paper is that there is a region of PR space that is completely unachievable , and the size of this region varies only with the skew . This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning . Randomized Smoothing for ( Parallel ) Stochastic Optimization . John Duchi , Martin Wainwright , Peter Bartlett . Abstract : By combining randomized smoothing techniques with accelerated gradient methods , we obtain convergence rates for stochastic optimization procedures , both in expectation and with high probability , that have optimal dependence on the variance of the gradient estimates . To the best of our knowledge , these are the first variance - based convergence guarantees for non - smooth optimization . A combination of our techniques with recent work on decentralized optimization yields order - optimal parallel stochastic optimization algorithms . We give applications of our results to several statistical machine learning problems , providing experimental results demonstrating the effectiveness of our algorithms . Marginalized Denoising Autoencoders for Domain Adaptation . Minmin Chen , Zhixiang Xu , Kilian Weinberger , Fei Sha . Abstract : Glorot et al . ( 2011 ) have successfully used Stacked Denoising Autoencoders ( SDAs ) ( Vincent et al . , 2008 ) to learn new representations for domain adaptation , resulting in record accuracy levels on well - known benchmark datasets for sentiment analysis . The representations are learned by reconstructing input data from partial corruption . In this paper , we introduce a variation , marginalized SDA ( mSDA ) . In contrast to the original SDA , mSDA requires no training through back - propagation as we explicitly marginalize out the feature corruption and solve for the parameters in closed form . mSDA learns representations that lead to comparable accuracy levels , but can be implemented in only 20 lines of MATLAB and reduces the computation time on large data sets from several days to mere minutes . Copula - based Kernel Dependency Measures . Barnabas Poczos , Zoubin Ghahramani , Jeff Schneider . Abstract : The paper presents a new copula based method for measuring dependence between random variables . Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution . We prove that this approach has several advantageous properties . Similarly to Shannon mutual information , the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables . This is important in many applications , for example in feature selection . The estimator is consistent , robust to outliers , and uses rank statistics only . We derive upper bounds on the convergence rate and propose independence tests too . We illustrate the theoretical contributions through a series of experiments in feature selection and low - dimensional embedding of distributions using real and toy datasets . Group Sparse Additive Models . Junming Yin , Xi Chen , eric xing . Abstract : We consider the problem of sparse variable selection in nonparametric additive models , with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly . Previous works either study the group sparsity in the parametric setting ( e.g. , group lasso ) , or address the variable selection problem in the nonparametric setting without exploiting the structural information ( e.g. , sparse additive models ( SpAM ) ) . In this paper , we present a new method , called group sparse additive models ( GroupSpAM ) , which can handle group sparsity in nonparametric additive models . We generalize the l1/l2 norm to Hilbert spaces as the sparsity - inducing penalty in GroupSpAM . Moreover , we derive a novel thresholding condition for identifying the functional sparsity at the group level , and propose an efficient block coordinate descent algorithm for constructing the estimate . We demonstrate by simulation that GroupSpAM substantially outperforms competing methods in terms of support recovery and prediction accuracy in additive models , and also conduct a comparative experiment on a real breast cancer dataset . Policy Gradients with Variance Related Risk Criteria . Dotan Di Castro , Aviv Tamar , Shie Mannor . Abstract : Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control . The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward . It is known that optimizing many of the variance related risk criteria is NP - hard . In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria . Our starting point is a new formula for the variance of the cost - to - go in episodic tasks . Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost . We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem . Efficient Structured Prediction with Latent Variables for General Graphical Models . Alexander Schwing , Tamir Hazan , Marc Pollefeys , Raquel Urtasun . Abstract : In this paper we propose a unified framework for structured prediction with hidden variables which includes hidden conditional random fields and latent structural support vector machines as special cases . We describe an approximation for this general structured prediction formulation using duality , which is based on a local entropy approximation . On the Partition Function and Random Maximum A - Posteriori Perturbations . Tamir Hazan , Tommi Jaakkola . Abstract : n this paper we relate the partition function to the max - statistics of random variables . In particular , we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models . As a result , we can directly use efficient MAP solvers such as graph - cuts to evaluate the corresponding partition function . We show that our method excels in the typical \\\" high signal - high coupling \\\" regime that results in ragged energy landscapes difficult for alternative approaches . Infinite Tucker Decomposition : Nonparametric Bayesian Models for Multiway Data Analysis . Zenglin Xu , Feng Yan , Alan Qi . Abstract : Tensor decomposition is a powerful computational tool for multiway data analysis . Many popular tensor decomposition approaches - such as the Tucker decomposition and CANDECOMP / PARAFAC ( CP)-amount to multi - linear factorization . They are insufficient to model ( i ) complex interactions between data entities , ( ii ) various data types ( eg missing data and binary data ) , and ( iii ) noisy observations and outliers . To address these issues , we propose tensor - variate latent nonparametric Bayesian models , coupled with efficient inference methods , for multiway data analysis . We name these models InfTucker . Using these InfTucker , we conduct Tucker decomposition in an infinite feature space . Unlike classical tensor decomposition models , our new approaches handle both continuous and binary data in a probabilistic framework . Unlike previous Bayesian models on matrices and tensors , our models are based on latent Gaussian or t processes with nonlinear covariance functions . To efficiently learn the InfTucker from data , we develop a variational inference technique on tensors . Compared with classical implementation , the new technique reduces both time and space complexities by several orders of magnitude . Our experimental results on chemometrics and social network datasets demonstrate that our new models achieved significantly higher prediction accuracy than the most state - of - art tensor decomposition approaches . Inferring Latent Structure From Mixed Real and Categorical Relational Data . Esther Salazar , Lawrence Carin . Abstract : We consider analysis of relational data ( a matrix ) , in which the rows correspond to subjects ( e.g. , people ) and the columns correspond to attributes . The elements of the matrix may be a mix of real and categorical . Each subject and attribute is characterized by a latent binary feature vector , and an inferred matrix maps each row - column pair of binary feature vectors to an observed matrix element . The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low - rank covariance matrix , and the Gaussian random variables are mapped to latent binary features via a probit link . The same type construction is applied jointly to the columns . The model infers latent , low - dimensional binary features associated with each row and each column , as well correlation structure between all rows and between all columns . The Bayesian construction is successfully applied to real - world data , demonstrating an ability to infer meaningful low - dimensional structure from high - dimensional relational data . Bayesian Conditional Cointegration . Chris Bracegirdle , David Barber . Abstract : Cointegration is an important topic for time - series , and describes a relationship between two series in which a linear combination is stationary . Classically , the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares . Subsequently a unit root test is performed on the residuals . A well - known deficiency of this classical approach is that is can lead to erroneous conclusions about the presence of cointegration . As an alternative , we present a coherent framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach . Finally , we apply our technique to model segmented cointegration in which cointegration may exist only for limited time . In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments . Online Alternating Direction Method . Huahua Wang , Arindam Banerjee . Abstract : Online optimization has emerged as powerful tool in large scale optimization . In this paper , we introduce efficient online algorithms based on the alternating directions method ( ADM ) . We introduce a new proof technique for ADM in the batch setting , which yields a linear rate of convergence of ADM and forms the basis of regret analysis in the online setting . We consider two scenarios in the online setting , based on whether the solution needs to lie in the feasible set or not . In both settings , we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions . Preliminary results are presented to illustrate the performance of the proposed algorithms . On the Sample Complexity of Reinforcement Learning with a Generative Model . Mohammad Gheshlaghi Azar , Remi Munos , Bert Kappen . Abstract : We consider the problem of learning the optimal action - value function in the discounted - reward Markov decision processes ( MDPs ) . We also prove a matching lower bound of \\u0398 ( N log ( N / \\u03b4)/ ( ( 1-\\u03b3)^3\\u03b5^2 ) ) on the sample complexity of estimating the optimal action - value function by every RL algorithm . To the best of our knowledge , this is the first matching result on the sample complexity of estimating the optimal ( action-)value function in which the upper bound matches the lower bound of RL in terms of N , \\u03b5 , \\u03b4 and 1-\\u03b3 . Also , both our lower bound and our upper bound significantly improve on the state - of - the - art in terms of 1/(1-\\u03b3 ) . Convergence Rates for Differentially Private Statistical Estimation . Kamalika Chaudhuri , Daniel Hsu . Abstract : Differential privacy is a cryptographically - motivated privacy definition which has gained significant attention over the past few years . Differentially private solutions enforce privacy by adding random noise to the data or a function computed on the data , and the challenge in designing such algorithms is to optimize the privacy - accuracy - sample size tradeoff . This work studies differentially - private statistical estimation , and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators . Our results reveal a connection between differential privacy and the notion of B - robustness in robust statistics , by showing that unless an estimator is B - robust , we can not approximate it well with differential privacy over a large class of distributions . We then provide an upper bound on the convergence rate of a differentially private approximation to a B - robust estimator with a bounded range . We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy . Learning Task Grouping and Overlap in Multi - task Learning . Abhishek Kumar , Hal Daume III . Abstract : In the paradigm of multi - task learning , mul- tiple related prediction tasks are learned jointly , sharing information across the tasks . We propose a framework for multi - task learn- ing that enables one to selectively share the information across the tasks . We assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks . The coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these . Our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases . Experimental results on four datasets show that our approach outperforms competing methods . High Dimensional Semiparametric Gaussian Copula Graphical Models . Han Liu , Fang Han , Ming Yuan , John Lafferty , Larry Wasserman . Abstract : In this paper , we propose a semiparametric approach named nonparanormal SKEPTIC for efficiently and robustly estimating high dimensional undirected graphical models . To achieve modeling flexibility , we consider Gaussian Copula graphical models ( or the nonparanormal ) as proposed by Liu et al . ( 2009 ) . To achieve estimation robustness , we exploit nonparametric rank - based correlation coefficient estimators , including Spearman 's rho and Kendall 's tau . In high dimensional settings , we prove that the nonparanormal SKEPTIC achieves the optimal parametric rate of convergence in both graph and parameter estimation . This celebrating result suggests that the Gaussian copula graphical models can be used as a safe replacement of the popular Gaussian graphical models , even when the data are truly Gaussian . Besides theoretical analysis , we also conduct thorough numerical simulations to compare different estimators for their graph recovery performance under both ideal and noisy settings . The proposed methods are then applied on a large - scale genomic dataset to illustrate their empirical usefulness . Discovering Support and Affiliated Features from Very High Dimensions . Yiteng Zhai , Mingkui Tan , Ivor Tsang , Yew Soon Ong . Abstract : In this paper , a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions . Specifically , we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy . The benefits of the proposed algorithm are two - folds . First , it can identify the optimal discriminative and uncorrelated feature subset to the output labels , denoted here as Support Features , which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper . Second , during the learning process , the underlying group structures of correlated features associated with each support feature , denoted as Affiliated Features , can also be discovered without any additional cost . These affiliated features serve to improve the interpretations on the learning tasks . Extensive empirical studies on both synthetic and very high dimensional real - world datasets verify the validity and efficiency of the proposed method . Online Bandit Learning against an Adaptive Adversary : from Regret to Policy Regret . Ofer Dekel , Ambuj Tewari , Raman Arora . Abstract : Online learning algorithms are designed to learn even when their input is generated by an adversary . The widely - accepted formal definition of an online algorithm 's ability to learn is the game - theoretic notion of regret . We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm 's actions . We define the alternative notion of pol- icy regret , which attempts to provide a more meaningful way to measure an online algorithm 's performance against adaptive adversaries . Focusing on the online bandit setting , we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory . On the other hand , if the adversary 's memory is unbounded , we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound . We extend this result to other variants of regret , such as switching regret , internal regret , and swap regret . Statistical linear estimation with penalized estimators : an application to reinforcement learning . Bernardo Avila Pires , Csaba Szepesvari . Abstract : Motivated by value function estimation in reinforcement learning , we study statistical linear inverse problems , i.e. , problems where the coefficients of a linear system to be solved are observed in noise . We consider penalized estimators , where performance is evaluated using a matrix - weighted two - norm of the defect of the estimator measured with respect to the true , unknown coefficients . Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared . We propose simple , yet novel and theoretically well - founded data - dependent choices for the regularization parameters for both cases that avoid data - splitting . A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients , thus allowing the complete separation of the analysis of the stochastic properties of these errors . We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning . Large Scale Variational Bayesian Inference for Structured Scale Mixture Models . Young Jun Ko , Matthias Seeger . Abstract : Natural image statistics exhibit hierarchical dependencies across multiple scales . Representing such prior knowledge in non - factorial latent tree models can boost performance of image denoising , inpainting , deconvolution or reconstruction substantially , beyond standard factorial \\\" sparse \\\" methodology . We derive a large scale approximate Bayesian inference algorithm for linear models with non - factorial ( latent tree - structured ) scale mixture priors . Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors . Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring . Sungjin Ahn , Anoop Korattikara , Max Welling . Abstract : In this paper we address the following question : \\\" Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini - batch of data - items for every sample we generate ? \\\" An algorithm based on the Langevin equation with stochastic gradients ( SGLD ) was previously proposed to solve this , but its mixing rate was slow . By leveraging the Bayesian Central Limit Theorem , we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior , while for slow mixing rates it will mimic the behavior of SGLD with a pre - conditioner matrix . As a bonus , the proposed algorithm is reminiscent of Fisher scoring ( with stochastic gradients ) and as such an efficient optimizer during burn - in . An adaptive algorithm for finite stochastic partial monitoring . Gabor Bartok , Navid Zolghadr , Csaba Szepesvari . Abstract : We present a new anytime algorithm that achieves near - optimal regret for any instance of finite stochastic partial monitoring . In particular , the new algorithm achieves the minimax regret , within logarithmic factors , for both \\\" easy \\\" and \\\" hard \\\" problems . For easy problems , it additionally achieves logarithmic individual regret . Most importantly , the algorithm is adaptive in the sense that if the opponent strategy is in an \\\" easy region \\\" of the strategy space then the regret grows as if the problem was easy . The Big Data Bootstrap . Ariel Kleiner , Ameet Talwalkar , Purnamrita Sarkar , Michael Jordan . Abstract : The bootstrap provides a simple and powerful means of assessing the quality of estimators . However , in settings involving large datasets , the computation of bootstrap - based quantities can be prohibitively demanding . As an alternative , we present BLB , a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust , computationally efficient means of assessing the quality of estimators . BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability , statistical efficiency , and favorable theoretical properties of the bootstrap . We provide a theoretical analysis elucidating the properties of BLB , as well as an extensive empirical investigation , including a study of its statistical correctness , its large - scale implementation and performance , selection of hyperparameters , and performance on real data . Predicting Consumer Behavior in Commerce Search . Or Sheffet , Nina Mishra , Samuel Ieong . Abstract : Traditional approaches to ranking in web search follow the paradigm of rank - by - score : a learned function gives each query - URL combination an absolute score and URLs are ranked according to this score . This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other . Scoring contradicts prior work in behavioral economics that showed that users ' preferences between two items depend not only on the items but also on the presented alternatives . Thus , for the same query , users ' preference between items A and B depends on the presence / absence of item C. We propose a new model of ranking , the Random Shopper Model , that allows and explains such behavior . In this model , each feature is viewed as a Markov chain over the items to be ranked , and the goal is to find a weighting of the features that best reflects their importance . We show that our model can be learned under the empirical risk minimization framework , and give an efficient learning algorithm . Experiments on commerce search logs demonstrate that our algorithm outperforms scoring - based approaches including regression and listwise ranking . Conditional mean embeddings as regressors . Steffen Grunewalder , Guy Lever , Arthur Gretton , Luca Baldassarre , Sam Patterson , Massi Pontil . - Accepted . Abstract : We demonstrate an equivalence between reproducing kernel Hilbert space ( RKHS ) embeddings of conditional distributions and vector - valued regressors . This connection introduces a natural regularized loss function which the RKHS embeddings minimise , providing an intuitive understanding of the embeddings and a solid justification for their use . Furthermore , the equivalence allows the application of vector - valued regression methods and results to the problem of learning conditional distributions . Using this link we derive a sparse version of the embedding by considering alternative formulations . These minimax lower rates coincide with upper rates up to a logarithmic factor and show that the embedding method achieves nearly optimal rates . We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over a Cholesky decomposition . A Generative Process for Contractive Auto - Encoders . Salah Rifai , Yann Dauphin , Pascal Vincent , Yoshua Bengio . Abstract : The contractive auto - encoder learns a representation of the input data that captures the local manifold structure around each data point , through the leading singular vectors of the Jacobian of the transformation from input to representation . The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors , while remaining in a high - density region of the input space . This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto - encoder . The associated stochastic process defines a distribution from which one can sample , and which experimentally appears to converge quickly and mix well between modes , compared to Restricted Boltzmann Machines and Deep Belief Networks . The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower - level features and learns to be invariant to the local directions of variation discovered in the first layer . We show that this can help learn and represent invariances present in the data and improve classification error . Local Loss Optimization in Operator Models : A New Insight into Spectral Learning . Borja Balle , Ariadna Quattoni , Xavier Carreras . Abstract : This paper re - visits the spectral method for learning latent variable models defined in terms of observable operators . We give a new perspective on the method , showing that operators can be recovered by minimizing a loss defined on a finite subspace of the domain . A non - convex optimization similar to the spectral method is derived . We also propose a regularized convex relaxation of this optimization . We show that in practice the availabilty of a continuous regularization parameter ( in contrast with the discrete number of states in the original method ) allows a better trade - off between accuracy and model complexity . We also prove that in general , a randomized strategy for choosing the local loss will succeed with high probability . We also prove that in general , a randomized strategy for choosing the local loss will succeed with high probability . Robust PCA in High - dimension : A Deterministic Approach . Jiashi Feng , Huan Xu , Shuicheng Yan . Abstract : We consider principal component analysis for contaminated data - set in the high dimensional regime , where the number of observations is comparable or more than the dimensionality of each observation . More importantly , the proposed method exhibits significantly better computational efficiency , which makes it suitable for large - scale real applications . Complexity Analysis of the Lasso Regularization Path . Julien Mairal , Bin Yu . Abstract : The regularization path of the Lasso can be shown to be piecewise linear , making it possible to \\\" follow \\\" and explicitly compute the entire path . We analyze in this paper this popular strategy , and prove that its worst case complexity is exponential in the number of variables . We complete our theoretical analysis with a practical algorithm to compute these approximate paths . Projection - free Online Learning . Elad Hazan , Satyen Kale . Abstract : We present efficient online learning algorithms that eschew projections in favor of linear optimizations using the Frank - Wolfe technique . Besides the computational advantage , other desirable features of our algorithms are that they are parameter - free in the stochastic case and produce sparse decisions . Machine Learning that Matters . Kiri Wagstaff . Abstract : Much of current machine learning ( ML ) research has lost its connection to problems of import to the larger world of science and society . From this perspective , there exist glaring limitations in the data sets we investigate , the metrics we employ for evaluation , and the degree to which results are communicated back to their originating domains . What changes are needed to how we conduct research to increase the impact that ML has ? We present six Impact Challenges to explicitly focus the field 's energy and attention , and we discuss existing obstacles that must be addressed . We aim to inspire ongoing discussion and focus on ML that matters . Scene parsing with Multiscale Feature Learning , Purity Trees , and Optimal Covers . Cl\\u00e9ment Farabet , Camille Couprie , Laurent Najman , Yann LeCun . Abstract : Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to . We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel . The method alleviates the need for engineered features . In parallel to feature extraction , a tree of segments is computed from a graph of pixel dissimilarities . The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment . A subset of tree nodes that cover the image are then selected so as to maximize the average ' purity ' of the class distributions , hence maximizing the overall likelihood that each segment will contain a single object . Linear Regression with Limited Observation . Elad Hazan , Tomer Koren . Abstract : We consider the most common variants of linear regression , including Ridge , Lasso and Support - vector regression , in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time . We present simple and efficient algorithms for these problems : for Lasso and Ridge regression they need the same number of attributes ( up to constants ) as do full - information algorithms , for reaching a certain accuracy . For Support - vector regression , we require exponentially less attributes compared to the state of the art . By that , we resolve an open problem recently posed by ( Cesa - Bianchi et al . , 2010 ) . Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art . An Online Boosting Algorithm with Theoretical Justifications . Shang - Tse Chen , Hsuan - Tien Lin , Chi - Jen Lu . Abstract : We study the task of online boosting , which aims to combine online weak learners into an online strong learner . While boosting in the batch setting has a sound theoretical foundation , not much theory is known for the online setting . In this paper , we propose an online boosting algorithm and we provide a theoretical guarantee . In addition , we also perform experiments on real - world datasets and the results show that our algorithm compares favorably with existing algorithms . Modeling Temporal Dependencies in High - Dimensional Sequences : Application to Polyphonic Music Generation and Transcription . Nicolas Boulanger - Lewandowski , Yoshua Bengio , Pascal Vincent . Abstract : We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano - roll representation . We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high - dimensional sequences . Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets . We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription . Approximate Modified Policy Iteration . Bruno Scherrer , Victor Gabillon , Mohammad Ghavamzadeh , Matthieu Geist . Abstract : Modified policy iteration ( MPI ) is a dynamic programming ( DP ) algorithm that contains the two celebrated policy and value iteration methods . Despite its generality , MPI has not been thoroughly studied , especially its approximation form which is used when the state and/or action spaces are large or infinite . In this paper , we propose three approximate MPI ( AMPI ) algorithms that are extensions of the well - known approximate DP algorithms : fitted - value iteration , fitted - Q iteration , and classification - based policy iteration . We provide an error propagation analysis for AMPI that unifies those for approximate policy and value iteration . We also provide a finite - sample analysis for the classification - based implementation of AMPI ( CBMPI ) , which is more general ( and somehow contains ) than the analysis of the other presented AMPI algorithms . An interesting observation is that the MPI 's parameter allows us to control the balance of errors ( in value function approximation and in estimating the greedy policy ) in the final performance of the CBMPI algorithm . Nonparametric Link Prediction in Dynamic Networks . Purnamrita Sarkar , Deepayan Chakrabarti , Michael Jordan . Abstract : We propose a non - parametric link prediction algorithm for a sequence of graph snapshots over time . The model predicts links based on the features of its endpoints , as well as those of the local neighborhood around the endpoints . This allows for different types of neighborhoods in a graph , each with its own dynamics ( e.g , growing or shrinking communities ) . We prove the consistency of our estimator , and give a fast implementation based on locality - sensitive hashing . Experiments with simulated as well as five real - world dynamic graphs show that we outperform the state of the art , especially when sharp fluctuations or non - linearities are present . Agnostic System Identification for Model - Based Reinforcement Learning . Stephane Ross , Drew Bagnell . Abstract : A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis . To provide good performance guarantees , existing methods must assume that the real system is in the class of models considered during learning . We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class . In particular , we show that any no - regret online learning algorithm can be used to obtain a near - optimal policy , provided some model achieves low training error and access to a good exploration distribution . Our approach applies to both discrete and continuous domains . We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature . \"}",
        "_version_":1692670882082717696,
        "score":19.76438},
      {
        "id":"0f1ddc31-8f02-4a08-82b5-53feba1f695f",
        "_src_":"{\"url\": \"https://www.law.cornell.edu/uscode/html/uscode50a/usc_sec_50a_00001637----000-.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701157212.22/warc/CC-MAIN-20160205193917-00082-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Applicable services include voice dialing , banking over a telephone network , telephone shopping , database access services , information and reservation services , voice mail , security control for confidential information , and remote access to computers . Another important application of speaker recognition technology is as a forensics tool . Speaker identity is correlated with physiological and behavioral characteristics of the speech production system of an individual speaker . These characteristics derive from both the spectral envelope ( vocal tract characteristics ) and the supra - segmental features ( voice source characteristics ) of speech . The most commonly used short - term spectral measurements are cepstral coefficients and their regression coefficients . As for the regression coefficients , typically , the first- and second - order coefficients , that is , derivatives of the time functions of cepstral coefficients , are extracted at every frame period to represent spectral dynamics . These regression coefficients are respectively referred to as the delta - cepstral and delta - delta - cepstral coefficients . Speaker Identification and Verification . Speaker recognition can be classified into speaker identification and speaker verification . Speaker identification is the process of determining from which of the registered speakers a given utterance comes . Speaker verification is the process of accepting or rejecting the identity claimed by a speaker . Most of the applications in which voice is used to confirm the identity of a speaker are classified as speaker verification . In the speaker identification task , a speech utterance from an unknown speaker is analyzed and compared with speech models of known speakers . The unknown speaker is identified as the speaker whose model best matches the input utterance . In speaker verification , an identity is claimed by an unknown speaker , and an utterance of this unknown speaker is compared with a model for the speaker whose identity is being claimed . If the match is good enough , that is , above a threshold , the identity claim is accepted . A high threshold makes it difficult for impostors to be accepted by the system , but with the risk of falsely rejecting valid users . Conversely , a low threshold enables valid users to be accepted consistently , but with the risk of accepting impostors . To set the threshold at the desired level of customer rejection ( false rejection ) and impostor acceptance ( false acceptance ) , data showing distributions of customer and impostor scores are necessary . The fundamental difference between identification and verification is the number of decision alternatives . In identification , the number of decision alternatives is equal to the size of the population , whereas in verification there are only two choices , acceptance or rejection , regardless of the population size . Therefore , speaker identification performance decreases as the size of the population increases , whereas speaker verification performance approaches a constant independent of the size of the population , unless the distribution of physical characteristics of speakers is extremely biased . There is also a case called \\\" open set \\\" identification , in which a reference model for an unknown speaker may not exist . In this case , an additional decision alternative , \\\" the unknown does not match any of the models \\\" , is required . Verification can be considered a special case of the \\\" open set \\\" identification mode in which the known population size is one . In either verification or identification , an additional threshold test can be applied to determine whether the match is sufficiently close to accept the decision , or if not , to ask for a new trial . The effectiveness of speaker verification systems can be evaluated by using the receiver operating characteristics ( ROC ) curve adopted from psychophysics . The ROC curve is obtained by assigning two probabilities , the probability of correct acceptance ( 1 ? false rejection rate ) and the probability of incorrect acceptance ( false acceptance rate ) , to the vertical and horizontal axes respectively , and varying the decision threshold . The detection error trade - off ( DET ) curve is also used , in which false rejection and false acceptance rates are assigned to the vertical and horizontal axes respectively . The error curve is usually plotted on a normal deviate scale . With this scale , a speaker recognition system whose true speaker and impostor scores are Gaussians with the same variance will result in a linear curve with a slope equal to ? The DET curve representation is therefore more easily readable than the ROC curve and allows for a comparison of the system 's performance over a wide range of operating conditions . The equal - error rate ( EER ) is a commonly accepted overall measure of system performance . It corresponds to the threshold at which the false acceptance rate is equal to the false rejection rate . Text - Dependent , Text - Independent and Text - Prompted Methods . Speaker recognition methods can also be divided into text - dependent ( fixed passwords ) and text - independent ( no specified passwords ) methods . The former require the speaker to provide utterances of key words or sentences , the same text being used for both training and recognition , whereas the latter do not rely on a specific text being spoken . The text - dependent methods are usually based on template / model - sequence - matching techniques in which the time axes of an input speech sample and reference templates or reference models of the registered speakers are aligned , and the similarities between them are accumulated from the beginning to the end of the utterance . Since this method can directly exploit voice individuality associated with each phoneme or syllable , it generally achieves higher recognition performance than the text - independent method . There are several applications , such as forensics and surveillance applications , in which predetermined key words can not be used . Moreover , human beings can recognize speakers irrespective of the content of the utterance . Therefore , text - independent methods have attracted more attention . Another advantage of text - independent recognition is that it can be done sequentially , until a desired significance level is reached , without the annoyance of the speaker having to repeat key words again and again . Both text - dependent and independent methods have a serious weakness . That is , these security systems can easily be circumvented , because someone can play back the recorded voice of a registered speaker uttering key words or sentences into the microphone and be accepted as the registered speaker . Another problem is that people often do not like text - dependent systems because they do not like to utter their identification number , such as their social security number , within the hearing of other people . To cope with these problems , some methods use a small set of words , such as digits as key words , and each user is prompted to utter a given sequence of key words which is randomly chosen every time the system is used . Yet even this method is not reliable enough , since it can be circumvented with advanced electronic recording equipment that can reproduce key words in a requested order . Therefore , a text - prompted speaker recognition method has been proposed in which password sentences are completely changed every time . Text - Dependent Speaker Recognition Methods . Text - dependent speaker recognition methods can be classified into DTW ( dynamic time warping ) or HMM ( hidden Markov model ) based methods . DTW - Based Methods . The overall distance between the test utterance and the template is used for the recognition decision . When multiple templates are used to represent spectral variation , distances between the test utterance and the templates are averaged and then used to make the decision . The DTW approach has trouble modeling the statistical variation in spectral features . HMM - Based Methods . An HMM can efficiently model the statistical variation in spectral features . Therefore , HMM - based methods have achieved significantly better recognition accuracies than DTW - based methods . Text - Independent Speaker Recognition Methods . In text - independent speaker recognition , generally the words or sentences used in recognition trials can not be predicted . Since it is impossible to model or match speech events at the word or sentence level , the following four kinds of methods have been investigated . Long - Term - Statistics - Based Methods . Long - term sample statistics of various spectral features , such as the mean and variance of spectral features over a series of utterances , have been used . Long - term spectral averages are extreme condensations of the spectral characteristics of a speaker 's utterances and , as such , lack the discriminating power of the sequences of short - term spectral features used as models in text - dependent methods . VQ - Based Methods . A set of short - term training feature vectors of a speaker can be used directly to represent the essential characteristics of that speaker . However , such a direct representation is impractical when the number of training vectors is large , since the memory and amount of computation required become prohibitively large . Therefore , attempts have been made to find efficient ways of compressing the training data using vector quantization ( VQ ) techniques . In this method , VQ codebooks , consisting of a small number of representative feature vectors , are used as an efficient means of characterizing speaker - specific features . In the recognition stage , an input utterance is vector - quantized by using the codebook of each reference speaker ; the VQ distortion accumulated over the entire input utterance is used for making the recognition determination . In contrast with the memoryless ( frame - by - frame ) VQ - based method , non - memoryless source coding algorithms have also been studied using a segment ( matrix ) quantization technique . The advantage of a segment quantization codebook over a VQ codebook representation is its characterization of the sequential nature of speech events . A segment modeling procedure for constructing a set of representative time normalized segments called \\\" filler templates \\\" has been proposed . The procedure , a combination of K - means clustering and dynamic programming time alignment , provides a means for handling temporal variation . Ergodic - HMM - Based Methods . The basic structure is the same as the VQ - based method , but in this method an ergodic HMM is used instead of a VQ codebook . Over a long timescale , the temporal variation in speech signal parameters is represented by stochastic Markovian transitions between states . This method uses a multiple - state ergodic HMM ( i.e. , all possible transitions between states are allowed ) to classify speech segments into one of the broad phonetic categories corresponding to the HMM states . The automatically obtained categories are often characterized as strong voicing , silence , nasal / liquid , stop burst /post silence , frication , etc . . The VQ - based method has been compared with the discrete / continuous ergodic HMM - based method , particularly from the viewpoint of robustness against utterance variations . It was found that the continuous ergodic HMM method is far superior to the discrete ergodic HMM method and that the continuous ergodic HMM method is as robust as the VQ - based method when enough training data is available . However , when little data is available , the VQ - based method is more robust than the continuous HMM method . Speaker identification rates using the continuous HMM were investigated as a function of the number of states and mixtures . It was shown that the speaker recognition rates were strongly correlated with the total number of mixtures , irrespective of the number of states . This means that using information on transitions between different states is ineffective for text - independent speaker recognition . A technique based on maximum likelihood estimation of a Gaussian mixture model ( GMM ) representation of speaker identity is one of the most popular methods . This method corresponds to the single - state continuous ergodic HMM . Gaussian mixtures are noted for their robustness as a parametric model and for their ability to form smooth estimates of rather arbitrary underlying densities . The VQ - based method can be regarded as a special ( degenerate ) case of a single - state HMM with a distortion measure being used as the observation probability . Speech - Recognition - Based Methods . The VQ- and HMM - based methods can be regarded as methods that use phoneme - class - dependent speaker characteristics contained in short - term spectral features through implicit phoneme - class recognition . In other words , phoneme - classes and speakers are simultaneously recognized in these methods . On the other hand , in the speech - recognition - based methods , phonemes or phoneme - classes are explicitly recognized , and then each phoneme / phoneme - class segment in the input speech is compared with speaker models or templates corresponding to that phoneme / phoneme - class . A five - state ergodic linear predictive HMM for broad phonetic categorization has been investigated . In this method , after frames that belong to particular phonetic categories have been identified , feature selection is performed . In the training phase , reference templates are generated and verification thresholds are computed for each phonetic category . In the verification phase , after phonetic categorization , a comparison with the reference template for each particular category provides a verification score for that category . The final verification score is a weighted linear combination of the scores for each category . The weights are chosen to reflect the effectiveness of particular categories of phonemes in discriminating between speakers and are adjusted to maximize the verification performance . Experimental results showed that verification accuracy can be considerably improved by this category - dependent weighted linear combination method . A speaker verification system using 4-digit phrases has also been tested in actual field conditions with a banking application , where input speech was segmented into individual digits using a speaker - independent HMM . The frames within the word boundaries for a digit were compared with the corresponding speaker - specific HMM digit model and the Viterbi likelihood score was computed . This was done for each of the digits making up the input utterance . The verification score was defined to be the average normalized log - likelihood score over all the digits in the utterance . A large vocabulary speech recognition system has also been used for speaker verification . With this approach a set of speaker - independent phoneme models were adapted to each speaker . Speaker verification consisted of two stages . First , speaker - independent speech recognition was run on each of the test utterances to obtain phoneme segmentation . In the second stage , the segments were scored against the adapted models for a particular target speaker . The scores were normalized by those with speaker - independent models . The system was evaluated using the 1995 NIST - administered speaker verification database , which consists of data taken from the Switchboard corpus . The results showed that this method did not out - perform Gaussian mixture models . Text - Prompted Speaker Recognition . In this method , key sentences are completely changed every time . The system accepts the input utterance only when it determines that the registered speaker uttered the prompted sentence . Because the vocabulary is unlimited , prospective impostors can not know in advance the sentence they will be prompted to say . This method not only accurately recognizes speakers , but can also reject an utterance whose text differs from the prompted text , even if it is uttered by a registered speaker . Thus , a recorded and played back voice can be correctly rejected . This method uses speaker - specific phoneme models as basic acoustic units . One of the major issues in this method is how to properly create these speaker - specific phoneme models when using training utterances of a limited size . The phoneme models are represented by Gaussian - mixture continuous HMMs or tied - mixture HMMs , and they are made by adapting speaker - independent phoneme models to each speaker 's voice . In the recognition stage , the system concatenates the phoneme models of each registered speaker to create a sentence HMM , according to the prompted text . Then the likelihood of the input speech against the sentence model is calculated and used for speaker verification . High - level Speaker Recognition . High - level features such as word idiolect , pronunciation , phone usage , prosody , etc . have also been successfully used in text - independent speaker verification . Typically , high - level - feature recognition systems produce a sequence of symbols from the acoustic signal and then perform recognition using the frequency and co - occurrence of symbols . In an idiolect approach , word unigrams and bigrams from manually transcribed conversations are used to characterize a particular speaker in a traditional target / background likelihood ratio framework . The use of support vector machines for performing the speaker verification task based on phone and word sequences obtained using phone recognizers has been proposed . The benefit of these features was demonstrated in the \\\" NIST extended data \\\" task for speaker verification ; with enough conversational data , a recognition system can become \\\" familiar \\\" with a speaker and achieve excellent accuracy . The corpus was a combination of phases 2 and 3 of the Switchboard-2 corpora . Each training utterance in the corpus consisted of a conversation side that was nominally of length 5 minutes ( approximately 2.5 minutes of speech ) recorded over a land - line telephone . Speaker models were trained using 1 ? 16 conversation sides . These methods need utterances of at least several minutes long , much longer than those used in conventional speaker recognition methods . Normalization and Adaptation Techniques . How can we normalize intra - speaker variation of likelihood ( similarity ) values in speaker verification ? The most significant factor affecting automatic speaker recognition performance is variation in signal characteristics from trial to trial ( inter - session variability , or variability over time ) . Variations arise from the speaker him / herself , from differences in recording and transmission conditions , and from noise . Speakers can not repeat an utterance precisely the same way from trial to trial . It is well known that samples of the same utterance recorded in one session are much more highly correlated than tokens recorded in separate sessions . There are also long term trends in voices . It is important for speaker recognition systems to accommodate these variations . Adaptation of the reference model as well as the verification threshold for each speaker is indispensable to maintaining a high recognition accuracy over a long period . In order to compensate for the variations , two types of normalization techniques have been tried ? one in the parameter domain , and the other in the distance / similarity domain . The latter technique uses the likelihood ratio or a posteriori probability . To adapt HMMs for noisy conditions , various techniques including the HMM composition ( PMC : parallel model combination ) method , have proved successful . Parameter - Domain Normalization . As one typical normalization technique in the parameter domain , spectral equalization , the so - called \\\" blind equalization \\\" method , has been confirmed to be effective in reducing linear channel effects and long - term spectral variation . This method is especially effective for text - dependent speaker recognition applications using sufficiently long utterances . In this method , cepstral coefficients are averaged over the duration of an entire utterance , and the averaged values are subtracted from the cepstral coefficients of each frame ( CMS ; cepstral mean subtraction ) . This method can compensate fairly well for additive variation in the log spectral domain . However , it unavoidably removes some text - dependent and speaker - specific features , so it is inappropriate for short utterances in speaker recognition applications . It has also been shown that time derivatives of cepstral coefficients ( delta - cepstral coefficients ) are resistant to linear channel mismatches between training and testing . Likelihood Normalization . A normalization method for likelihood ( similarity or distance ) values that uses a likelihood ratio has been proposed . The likelihood ratio is the ratio of the conditional probability of the observed measurements of the utterance given the claimed identity is correct , to the conditional probability of the observed measurements given the speaker is an impostor ( normalization term ) . Generally , a positive log - likelihood ratio indicates a valid claim , whereas a negative value indicates an imposter . The likelihood ratio normalization approximates optimal scoring in Bayes ' sense . This normalization method is , however , unrealistic because conditional probabilities must be calculated for all the reference speakers , which requires large computational cost . Therefore , a set of speakers , \\\" cohort speakers \\\" , who are representative of the population distribution near the claimed speaker has been chosen for calculating the normalization term . Another way of choosing the cohort speaker set is to use speakers who are typical of the general population . It was reported that a randomly selected , gender - balanced background speaker population outperformed a population near the claimed speaker . A normalization method based on a posteriori probability has also been proposed . Experimental results indicate that both normalization methods almost equally improve speaker separability and reduce the need for speaker - dependent or text - dependent thresholding , compared with scoring using only the model of the claimed speaker . A method in which the normalization term is approximated by the likelihood for a world model representing the population in general has also been proposed . This method has an advantage in that the computational cost for calculating the normalization term is much smaller than the original method since it does not need to sum the likelihood values for cohort speakers . A method based on tied - mixture HMMs in which the world model is made as a pooled mixture model representing the parameter distribution for all the registered speakers has been proposed . The use of a single background model for calculating the normalization term has become the predominate approach used in speaker verification systems . Since these normalization methods neglect absolute deviation between the claimed speaker 's model and the input speech , they can not differentiate highly dissimilar speakers . It has been reported that a multilayer network decision algorithm makes effective use of the relative and absolute scores obtained from the matching algorithm . A family of normalization techniques has been proposed , in which the scores are normalized by subtracting the mean and then dividing by standard deviation , both terms having been estimated from the ( pseudo ) imposter score distribution . Different possibilities are available for computing the imposter score distribution : Znorm , Hnorm , Tnorm , Htnorm , Cnorm and Dnorm ( Bimbot et al . , 2004 ) . The state - of - the - art text - independent speaker verification techniques associate one or more parameterization level normalization approaches ( CMS , feature variance normalization , feature warping , etc . ) with world model normalization and one or more score normalizations . Updating Models and A Priori Threshold for Speaker Verification . How to update speaker models to cope with the gradual changes in people 's voices is an important issue . How to set the a priori decision threshold for speaker verification is another important issue . In most laboratory speaker recognition experiments , the threshold is set a posteriori to the system 's equal error rate ( EER ) . Since the threshold can not be set a posteriori in real situations , we have to have practical ways to set the threshold before verification . It must be set according to the relative importance of the two errors , which depends on the application . These two problems are intrinsically related each other . Methods for updating reference templates and the threshold in DTW - based speaker verification were proposed . An optimum threshold was estimated based on the distribution of overall distances between each speaker 's reference template and a set of utterances of other speakers ( interspeaker distances ) . The interspeaker distance distribution was approximated by a normal distribution , and the threshold was calculated by the linear combination of its mean value and standard deviation . The intraspeaker distance distribution was not taken into account in the calculation , mainly because it is difficult to obtain stable estimates of the intraspeaker distance distribution from small numbers of training utterances . The reference template for each speaker was updated by averaging new utterances and the present template after time registration . These methods have been extended and applied to text - independent and text - prompted speaker verification using HMMs . Model - based Compensation Techniques . Various model - based compensation techniques for \\\" mismatch factors \\\" including channel , additive noise , linguistic content and intra - speaker variation have recently been proposed ( e. g. , Fauve et al . , 2007 ; Yin et al . , 2007 ) . Key developments include support vector machines ( SVMs ) , associated nuisance attribute projection compensation ( NAP ) and factor analysis ( FA ) . They have been shown to provide significant improvements in GMM - based text - independent speaker verification . These approaches involve estimating the variability from a large database in which each speaker is recorded across multiple sessions . The underlying hypothesis is that a low - dimensional \\\" session variability \\\" subspace exists with only limited overlap on speaker - specific information . The goal of NAP is to project out a subspace from the original expanded space , where information has been affected by nuisance effects . This is performed by learning on a background set of recordings , without explicit labeling , from many different speakers ' recordings . The most straightforward approach is to use the difference between a given session and the mean across sessions for each speaker . This information is pooled across speakers to form a combined matrix . An eigen problem is solved on the corresponding covariance matrix to find the dimensions of high variability for the pooled set . The resulting vectors are used in a SVM framework . FA shares similar characteristics to that of NAP , and it operates on generative models with traditional statistical approaches , such as EM , to model intersession variability . References . on Applied Signal Processing , pp . 430 - 451 . Fauve , B. G. B. , Matrouf , D. , Scheffer , N. , and Bonastre , J.-F ( 2007 ) \\\" State - of - the - Art Performance in Text - Independent Speaker Verification through Open - Source Software , \\\" IEEE Trans . On Audio , Speech , and Language Process . , 15 , 7 , pp . 1960 - 1968 . Furui , S. ( 1991 ) \\\" Speaker - Independent and Speaker - Adaptive Recognition Techniques , \\\" in Furui , S. and Sondhi , M. M. ( Eds . ) Advances in Speech Signal Processing , New York : Marcel Dekker , pp . 597 - 622 . Furui , S. ( 1997 ) \\\" Recent Advances in Speaker Recognition \\\" , Proc . First Int . Conf . Audio- and Video - based Biometric Person Authentication , Crans - Montana , Switzerland , pp . 237 - 252 . Furui , S. ( 2000 ) Digital Speech Processing , Synthesis , and Recognition , 2nd Edition , New York : Marcel Dekker . Yin , S.-C. , Rose , R. and Kenny , P. ( 2007 ) \\\" A Joint factor Analysis Approach to Progressive Model Adaptation in Text - Independent Speaker Verification , \\\" IEEE Trans . On Audio , Speech , and Language Process . , 15 , 7 , pp . 1999 - 2010 . \"}",
        "_version_":1692670764295127040,
        "score":19.071482},
      {
        "id":"16a8ba9e-d601-4561-bc48-af12821fa72f",
        "_src_":"{\"url\": \"http://collections.mun.ca/cdm/search/collection/theses/searchterm/revised/field/all/mode/any/conn/and/cosuppress/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701160918.28/warc/CC-MAIN-20160205193920-00225-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"The Gibbs sampler is one of the most popular algorithms for inference in statistical models . In this thesis , we introduce a herding variant of this algorithm that is entirely deterministic . We demonstrate , with simple examples , that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling . In particular , image denoising exemplifies the effectiveness of herded Gibbs as an inference technique for Markov Random Fields ( MRFs ) . Also , we adopt herded Gibbs as the inference engine for Conditional Random Fields ( CRFs ) in Named Entity Recognition ( NER ) and show that it is competitive with the state of the art . The conclusion is that herded Gibbs , for graphical models with nodes of low degree , is very close to Gibbs sampling in terms of the complexity of the code and computation , but that it converges much faster . In this thesis , we introduce a herding variant of this algorithm that is entirely deterministic . We demonstrate , with simple examples , that herded Gibbs exhibits better convergence behavior for approximating the marginal distributions than Gibbs sampling . In particular , image denoising exemplifies the effectiveness of herded Gibbs as an inference technique for Markov Random Fields ( MRFs ) . Also , we adopt herded Gibbs as the inference engine for Conditional Random Fields ( CRFs ) in Named Entity Recognition ( NER ) and show that it is competi- tive with the state of the art . The conclusion is that herded Gibbs , for graphical models with nodes of low degree , is very close to Gibbs sampling in terms of the complexity of the code and computation , but that it converges much faster . ii Table of Contents Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ii Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . v List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ix Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 6 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 iv List of Tables Table 5.1 Joint distribution of the two - variable model . . . . . . . . . . . 16 Table 5.2 Joint distribution of the three - variable model . For each \\u03c3 , we generated 10 corrupted images by adding Gaussian noise . The final results shown here are averages and standard deviations ( in the parentheses ) across the 10 corrupted images . I : in - place updates ; P : parallel updates ; D : damping factor ; R : random step size . . . . . . . . . . . . . . . . . . . . . . . . . 25 v Table 5.4 Comparisons of Gibbs , herded Gibbs , herded Gibbs with a ran- domized step size , and Viterbi for the NER task . We used the pre - trained 3-class CRF model in the Stanford NER package [ 7]. For the test set , we used the corpus for the NIST 1999 IE - ER Evaluation . For all the methods except Viterbi , we show F1 scores after 100 , 400 and 800 iterations . For Gibbs and herded Gibbs with a random step size , the results shown are the averages and standard deviations ( in the parentheses ) over 5 random runs . We used a linear annealing schedule for Gibbs and in - place updates for both versions of herded Gibbs . The average computational time each approach took to do inference for the entire test set is also listed ( in the brackets ) . R : random step size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 vi List of Figures Figure 2.1 Two iterations corresponding to two versions of the conditional gradient algorithm . 6 Figure 5.1 Two - variable model . . . . . . . . . . . . . . . . . . . . . . . 16 Figure 5.2 Approximate marginals obtained via Gibbs and herded Gibbs for an MRF of two variables , constructed so as to make the move from state ( 0,0 ) to ( 1,1 ) progressively more difficult as \\u03b5 decreases . Table 5.1 provides the joint distribution for these variables . The error bars correspond to one standard deviation . The plots are best viewed in color . . . . . . . . . . . . . . . . 16 Figure 5.3 Four - variable model . . . . . . . . . . . . . . . . . . . . . . . 17 Figure 5.4 The error in approximating the marginals ( top ) and joint ( bot- tom ) for an MRF of four variables . When the ratios of condi- tional distributions are rational , the deterministic herded Gibbs fails to converge to the joint . This is however not a problem when the ratios are irrational . In both cases , herded Gibbs with a randomized step size converges well , while providing a very significant improvement over standard Gibbs sampling . . . . . 18 Figure 5.5 Three - variable model . . . . . . . . . . . . . . . . . . . . . . 19 Figure 5.6 The marginal of the first node of a three - node MRF and its esti- mates . Herded Gibbs with a random step size converges to the correct value at an impressive rate . The left plot shows 5,000 iterations while the right plots shows 50,000 iterations . . . . . 20 vii Figure 5.7 The MRF employed for the task of image denoising . We show the results after 1 , 10 and 30 iterations across the image and also the posterior means computed by averaging over the last 10 iterations . The results are averaged across 10 corrupted images with Gaussian noise N ( 0,16 ) . The error bars correspond to one standard devi- ation . Here we introduce a shared version of herded Gibbs , which stores conditionals that have the same potential values together . As shown , the shared version of herded Gibbs outper- forms all the other approaches . In general , the in - place update versions of each approach perform better than the correspond- ing parallel update versions . Herded Gibbs with a random- ized step size has almost the same performance with standard Herded Gibbs under the same setting . For better display , we do not show them in the plots . . . . . . . . . . . . . . . . . . 24 Figure 5.10 A representative application of NER to a random Wikipedia sample [ 1]. First and foremost , my supervisor Nando de Freitas for his invaluable guidance and support throughout my graduate study . He devoted numerous hours to my work and gave me the freedom to explore my own ideas . Dr. Luke Bornn and Dr. David Poole for their collaboration and valuable feedback on this work . Mareija Eskelinen for being a wonderful collaborator and for the time and efforts she devoted to proofread my thesis . Misha Denil , Masrour Zoghi , Ziyu Wang , Matt Hoffman , David Buchman , Shakir Mohamed , Xing Chen , Shafiq Joty , Kevin Swersky and other fellow UBC CS students for discussing ideas , answering questions , and sharing code with me . I highly value their openness and willingness to help . Finally , I would like to thank my parents for their tireless love and support . It means very much to me that they are proud to have me as their daughter . x Chapter 1 Introduction Computing the posterior distribution is the core component of Bayesian analysis . This computation involves the integration of high - dimensional functions , which can be computationally very difficult . Several approaches that avoid direct integra- tion have been proposed over the decades [ 6 , 14]. Markov chain Monte Carlo ( MCMC ) methods are one of the most popular classes of such algorithms , which attempt to simulate direct draws from some com- plex distribution of interest [ 2 , 15]. They generate samples based on constructing a Markov chain whose equilibrium distribution is exactly the target distribution . As the transition probabilities between sample values are only a function of the most recent sample values , it only needs the previous sample values in order to generate the next . The states of the chain after a large number of steps ( the so - called burn - in period ) can then be used as a sample of the target distribution . The quality of the sample improves as a function of the number of steps . One particular MCMC method , the Gibbs sampler [ 11 ] , was realized to be widely applicable to a broad class of Bayesian problems in the early 1990s [ 10]. It was first introduced in the context of image processing as a special case of the Metropolis - Hastings algorithm , and was then extended to be a general framework for sampling from a large set of variables . Gibbs sampling is commonly used for statistical inference . It is applicable when the joint distribution is not known ex- plicitly or is difficult to sample from directly , but the conditional distribution of each variable is known and is easy to sample from . 1 Over the decades , numerous variations and extensions of the basic Gibbs sam- pler have arisen . In this thesis , we introduce a novel variant of the Gibbs sampler that is entirely deterministic - herded Gibbs . It builds upon a recently published deterministic herding technique and is shown empirically to be an effective and valuable algorithm for statistical inference . 1.1 Thesis Contributions In this thesis , we present herded Gibbs , a deterministic variant of the well - known and widely - used Gibbs sampler . We provide an empirical study of the proposed algorithm and show that it is a valuable algorithm that performs better than the standard Gibbs sampler in several different tasks of great interest , including image denoising and Named Entity Recognition . Our algorithm is a novel and key step in the design of herding methods for sampling from large discrete distributions . By an image denoising example , we show that a shared version of herded Gibbs requires less storage while performing even better than comparable inference tech- niques . This to some extent resolves the storage issue of herded Gibbs and makes it a potentially more useful algorithm that is capable of being applied to more general models . We also show by experiments that , for herded Gibbs to converge to both the marginals and joint , we require the ratios of the conditional distributions to be ir- rational . If this is satisfied , herded Gibbs should converge to the correct marginals and joint . Otherwise , it may fail to converge . Furthermore , by simply randomizing the step size of herded Gibbs , we will be able to deliver the correct marginal and joint distributions , while retaining improved rates of convergence over the tradi- tional Gibbs sampling . This thesis presents a very effective , useful and straightforward algorithm as well as important methodological and empirical contributions to research on herd- ing . We believe this contribution will be of a wide interest to researchers in the machine learning community . 2 1.2 Thesis Organization This thesis is organized as follows : in Chapter 2 and 3 , we give an overview of the herding algorithm and the Gibbs algorithm respectively , which are the bases of our algorithm . Then we introduce in Chapter 4 the proposed algorithm , herded Gibbs , which is a variant of the Gibbs sampler and is entirely deterministic . In Chapter 5 , we use several experiments to show the effectiveness of herded Gibbs as an inference technique and compare it with several existing popular algorithms for inference . Finally we discuss the limitations and extensions of herded Gibbs and conclude in Chapter 6 . A moment \\u00b5i is the expectation of some feature function \\u03c6i over data and can be used as a quantitative measure of the data set . Some examples include the mean and the variance . As presented , herding is a moment matching algorithm . However , it can also be used to produce samples of normalized probability distri- butions . That is , \\u00b5i is the probability of the i - th bin . The corresponding pseudocode is shown in Algorithm 2.1 . Note that this is a simple instance of the algorithm of equation ( 2.1 ) . Set \\u03c6 to be a vector of zeros of size n Set the i ? Unfortu- nately , this only applies to normalized distributions or to problems where the ob- jective is not to guarantee that the samples come from the right target , but simply to ensure that some moments are matched . An interpretation of herding in terms of quadrature has been put forward recently by Husza\\u0301r and Duvenaud [ 12]. In this thesis , we will show that it is possible to use herding to generate samples from more complex unnormalized probability distributions . 5 2.1 Herding and Frank - Wolfe Algorithms 2.1.1 Frank - Wolfe Algorithms The Frank - Wolfe algorithm , also known as the reduced gradient method and the convex combination algorithm , is a class of iterative optimization algorithms . It was first proposed by Marguerite Frank and Philip Wolfe in 1956 [ 8 ] as an algo- rithm for quadratic programming and was later proven to be an iterative method for nonlinear programming that need not be restricted to quadratic programming . The iterations corresponding to these two choices are illustrated in Fig- ure 2.1 . As t increases , g(t ) approaches the optimal solution \\u00b5 . \\u00b5 g(1 ) g(2 ) g(3 ) \\u00b5 g(1 ) g(2 ) g(3 ) Figure 2.1 : Two iterations corresponding to two versions of the conditional gradient algorithm . 6 2.1.2 Herding as a Frank - Wolfe Algorithm As described by Bach et al . [ 3 ] , herding can be reformulated as a conditional gra- dient algorithm . An alternative choice of step size is to use line search to find the point in the segment with optimal value . 2.2 Herding and Maximum / Minimum Entropy In the maximum entropy estimation setting , given a moment vector \\u00b5 , the goal of herding is to produce a pseudo - sample drawn from a distribution whose moments match \\u00b5 . For an introduction to maximum entropy estimation , see the book by Bernardo and Smith [ 4 , Section 4.5.4]. On the contrary , it happens to be close to a minimum entropy solution , where many states have probability zero . In addition , the work also mentions that the conclusions can not be further generalized to other cases . In general , the herding procedure , while leading to a consistent estimation of the mean vector \\u00b5 , does not converge to the maximum entropy distribution . 8 Chapter 3 Gibbs Sampling Gibbs sampling ( popularized in the context of image processing by Geman and Geman 1984 [ 11 ] ) is a simple MCMC method used to perform approximate infer- ence in factored probabilistic models . In many cases it is not possible to do direct simulation from the posterior distributions . In these cases , Gibbs sampling may prove useful . The idea behind Gibbs sampling is that it alternates ( either systematically or randomly ) the sampling of each variable while conditioning on the values of all the other variables in the distribution . Typically , we do not sample those visible variables whose values are already known . The pseudocode of Gibbs sampling is shown in Algorithm 3.1 . In general , x j may only depend on some of the other variables . If we represent p(x ) as a graphical model , we can infer the dependencies by looking at x j 's Markov blanket , which are its neighbors in the graph . Thus to sample x j , we only need to know the values of x j 's neighbors . In this sense , Gibbs sampling is a distributed algorithm . However , it is not a parallel algorithm , since the samples must be generated sequentially . 2 : Initialize x(0 ) . We then consider only every mth sample when averaging values to compute an expectation , because suc- cessive samples are not independent of each other but from a Markov chain with some amount of correlation . In the early phase of the sampling process , the algorithm tends to move slowly around the sample space rather than moving around quickly as is desired . As a result , the samples generated in this stage tend to have a high amount of autocor- relation . Simulated annealing is often used to reduce the random walk behavior and to reduce autocorrelations between samples . Other techniques that may reduce autocorrelation include collapsed Gibbs sampling , blocked Gibbs sampling , and ordered overrelaxation . The sequence obtained by Gibbs sampling can be used to approximate the joint distribution , to approximate the marginal distribution of one of the variables or some subset of the variables , or to compute an integral . 10 Chapter 4 Herded Gibbs In this chapter , we will introduce the herded Gibbs sampler , a variant of the popular Gibbs sampling algorithm that is completely deterministic . While Gibbs relies on drawing samples from the full - conditionals in a random way , herded Gibbs generates samples by matching the full - conditionals using the herding procedure . Given that Gibbs sampling is one of the work - horses of statistical inference , it is remarkable that it is possible to construct an entirely deterministic version of this algorithm . 4.1 Example For ease of presentation , we introduce the herded Gibbs sampler with a simple , but very popular example : Image denoising with a Markov Random Field ( MRF ) . Let us assume that we have a binary image corrupted by noise , and that we want to infer the original clean image . We take advantage of the fact that neighboring pixels are likely to have the same label by defining an MRF with an Ising prior . The known pa- rameters Ji j establish the coupling strength between nodes i and j. Note that the matrix J is symmetric . It alternates ( either systematically or randomly ) the sampling of each pixel xi while conditioning on the observations y and the neighbors of xi , de- noted xN ( i ) . That is , it generates each sample from its full - conditional distribution . The pseudocode of Gibbs sampling for this example is shown in Algorithm 4.1 . This is because we already have an updated value of x1 at time t , and x1 has been assumed to be a neighbor of x2 . The herded Gibbs sampler replaces the sampling from full - conditionals with herding at the level of the full - conditionals , as shown in Algorithm 4.2 . 12 Algorithm 4.1 Systematic - scan Gibbs sampler 1 : Input : An image y with M pixels , and T . Only the weight vector corresponding to the current instantiation of the neighbors is up- dated , as illustrated in Algorithm 4.3 . Hence , herded Gibbs effectively trades off memory for increased statistical ef- ficiency . The memory complexity of herded Gibbs is exponential in the maximum node degree . However , the cost for one iteration is not significantly more expensive than that of Gibbs and the convergence rate of herded Gibbs is faster than that of Gibbs , as shown by means of experiments in the following section . Moreover , by comparing various models for fitting the convergence curves , we conjecture that the rate of convergence of herded Gibbs is O(1/T ) . Proving this fact mathemati- cally is an open problem . Furthermore , if the conditions are met , then we can recover the joint distribution from the conditional distributions which allows us to recover the marginal dis- tributions ( though this last step is computationally expensive ) . In the following examples , we assess whether herded Gibbs is capable of estimating the correct conditionals and marginals . 5.1.1 Two - Node Example First , we consider a model of two variables , x1 and x2 , as shown in Figure 5.1 ; the joint distribution of these variables is shown in Table 5.1 . As \\u03b5 decreases , both approaches require more iterations to converge . Alternately , as \\u03b5 approaches zero , Gibbs struggles to converge , whereas , herded Gibbs continues to provide good deterministic convergence behavior . In this and subsequent experiments , we set the initial parameters w to a constant vector . We observed that the performance of herded Gibbs does not change when we initialize w at random . 15 x1 x2 Figure 5.1 : Two - variable model . Table 5.1 pro- vides the joint distribution for these variables . The error bars correspond to one standard deviation . The plots are best viewed in color . 16 5.1.2 Four - Node Example Next , we study a model of four variables with a joint distribution as shown in Equation 4.2 and illustrated in Figure 5.3 . The left column of Figure 5.4 shows the 1-norm error of approximating the marginals and the joint distribution when the coupling parameters Ji j are set to 1 . ( For this choice of coupling parameters , the ratios of the conditional distributions are rational numbers . ) While herded Gibbs approximates the marginal distributions well , it fails to approximate the joint dis- tribution . x1 x2 x3 x4 Figure 5.3 : Four - variable model . This finding is not new and has in fact been reported in a simple maximum- entropy setting [ 3]. There , the authors conjecture that almost sure convergence of the herding algorithm to the maximum entropy distribution can be established when \\u00b5 is a random vector . In our setting , by drawing the full conditionals uni- formly at random in the unit interval , we are creating irrational ratios among these full conditionals . This follows from the fact that the rationals have measure zero in the unit interval . When the ratios of full conditionals are irrational , determin- istic cycles in the state ( sample ) sequence are broken ( or be made very large in digital computers ) and , hence , herded Gibbs converges to the right marginals and joint distribution . The right hand side of Figure 5.4 illustrates this . These results add further evidence to the conjecture made by Bach et al . [ 3]. The choice of irrational \\u00b5 's has also appeared in the work by Welling and Chen [ 18]. Clearly , however , we should avoid irrational \\u00b5 's whose ratios are rational . When the ratios of conditional distribu- tions are rational , the deterministic herded Gibbs fails to converge to the joint . This is however not a problem when the ratios are irrational . In both cases , herded Gibbs with a randomized step size converges well , while providing a very significant improvement over standard Gibbs sampling . The randomization of the step size effectively breaks cycles and results in ergodicity of the marginals and joint as illustrated in 18 Figure 5.4 . This four - node experiment has shown that herded Gibbs can yield fast and ac- curate estimates of the marginals and joint distribution when its step size is ran- domized . The experiment also seems to indicate that the deterministic herded Gibbs can result in good estimates of the marginal distributions . The following three - node experiment shows that this is not always the case . 5.1.3 Three - Node Example Consider a three - node model as illustrated in Figure 5.5 , whose joint distribution is shown in Table 5.2 . In this case , the deterministic herded Gibbs fails to converge to the correct marginals . This is depicted in Figure 5.6 . x1 x3 x2 Figure 5.5 : Three - variable model . Table 5.2 : Joint distribution of the three - variable model . Herded Gibbs with a random step size converges to the correct value at an impressive rate . The left plot shows 5,000 iterations while the right plots shows 50,000 iterations . Even if all states can be reached , the deterministic algorithm can still fail . In our three - node example , all states can be reached if the herding parameters for x1 and x2 are initialized to different values . However , this does not break state cycles and the algorithm still fails to converge to the right marginals . Once again , to obtain fast convergence , we simply have to randomize the step size . 5.2 Markov Random Field ( MRF ) for Image Denoising As discussed in Chapter 4 , herded Gibbs performs deterministic sampling which in itself can provide computational benefits over comparable stochastic techniques . Furthermore , this derandomized sampling algorithm is further shown to provide performance gains beyond that of similar widely - adapted stochastic techniques . Here , herded Gibbs is compared to both Gibbs sampling and mean field inference . In particular , the herded Gibbs algorithm 's performance is exemplified for image denoising of the \\\" NIPS \\\" image ( as seen in Figure 5.8 ) . Various noisy \\\" NIPS \\\" images were created through the addition of Gaussian noise , with varying \\u03c3 , to the original \\\" NIPS \\\" image . For each noisy image a set of observed variables is 20 xi yi y j x j Figure 5.7 : The MRF employed for the task of image denoising . The vari- ables xi , x j are hidden variables representing the true pixel values , and the variables yi , y j are observed variables representing the noisy pixel values . Accurately reproducing the hidden variables will result in the construction of an image much like the original \\\" NIPS \\\" image of Figure 5.8a . Gibbs sampling proceeds by sequentially resampling each hidden variable given the values of its neighbors ( in the MRF ) at the previous iteration . Though herded Gibbs performs inference in much the same way , the samples drawn via herded Gibbs are computed deterministically . Such a similarity establishes Gibbs sam- pling as a natural choice for evaluating the performance of herded Gibbs . Analo- gously , mean field inference is well - suited for evaluating the performance of herded Gibbs in that it performs inference by employing the mean values of neighboring nodes in the MRF . Furthermore , both mean field inference and Gibbs sampling are widely used and natural choices for the task of image denoising with MRFs . Recall that herded Gibbs employs coupling parameters in the joint distribu- tion . We show the results after 1 , 10 and 30 it- erations across the image and also the posterior means computed by averaging over the last 10 iterations . If we store the conditionals for configurations with the same sum together , we only need to store as many conditionals as differ- ent possible values that the sum on the right hand side could take . This gives us inspiration to develop a shared version of herded Gibbs . The shared version of herded Gibbs presents additional opportunities for op- timization . In particular , the difference in performance between in - place updates and parallel updates was evaluated . For in - place updates ( Gauss - Seidel updates ) , we compute the value of each node on the go . For parallel updates ( Jacobi updates ) , we compute all the nodes synchronously at the end of each iteration . In general , the in - place update versions of each approach perform better than the correspond- ing parallel update versions , but this conclusion might change if one has access to a large cluster or processors . The performance of inference techniques in this case is indicative of their average performance and demonstrates the impres- sive convergence of herded Gibbs . After ten iterations of the algorithms , the herded Gibbs techniques already demonstrate improvements over comparable techniques . After thirty iterations , the herded Gibbs techniques have effectively converged and are significantly beyond the comparable techniques . A long - term evaluation of the inference techniques for image denoising of \\\" NIPS \\\" is shown in Figure 5.9 . The shared version of herded Gibbs converges faster than all other approaches with in - place herded Gibbs following as a close second . Herded Gibbs ' superior perfor- mance to comparable stochastic techniques makes the algorithm a promising new inference technique . The results are averaged across 10 corrupted images with Gaussian noise N ( 0,16 ) . The error bars correspond to one standard deviation . Here we intro- duce a shared version of herded Gibbs , which stores conditionals that have the same potential values together . As shown , the shared version of herded Gibbs outperforms all the other approaches . In general , the in - place update versions of each approach perform better than the cor- responding parallel update versions . Herded Gibbs with a randomized step size has almost the same performance with standard Herded Gibbs under the same setting . For better display , we do not show them in the plots . 24 A comparison of the image denoising results over different \\u03c3 's is shown in Table 5.3 . Overall , herded Gibbs is demonstrated to provide consistently superior performance in that it converges much faster than comparable techniques and does so consistently over a range of \\u03c3 ' s. As such , herded Gibbs provides a potential platform for improving upon applications employing Gibbs sampling or other sim- ilar sampling techniques . For each \\u03c3 , we generated 10 corrupted images by adding Gaussian noise . The final results shown here are averages and standard deviations ( in the parentheses ) across the 10 corrupted images . I : in - place updates ; P : parallel updates ; D : damping factor ; R : random step size . Information extraction , such as auto- matically identifying and extracting calendar dates or phone numbers from email messages , employs NER as a vital component of its computation . Historically , NER was performed by techniques like Viterbi that , though accurate and efficient under the Markov assumption , become intractable if the model attempts to account for long - term dependencies such as non - local information . As shown by Finkel et al . [ 7 ] , Gibbs sampling and simulated annealing can incorporate long distance struc- ture while preserving tractable inference in factored probabilistic models . Like Gibbs , herded Gibbs is capable of accommodating long term dependencies and the performance improvements it can obtain over Gibbs ( as demonstrated in Sec- tion 5.2 ) make it an interesting candidate for NER . We show here that herded Gibbs can be easily adapted to NER and requires fewer iterations to find the maximum . Figure 5.10 provides a representative NER example of the performance of Gibbs , herded Gibbs , herded Gibbs with a randomized step size , and Viterbi ( all methods produced the same annotation for this short example ) . They decide to rob it after realizing they could make money off the customers as well as the business , as they did during their previous heist . Moments after they initiate the hold - up , the scene breaks off and the title credits roll . As Jules Winnfield ( Samuel L. Jackson ) drives , Vincent Vega ( John Travolta ) talks about his experiences in Europe , from where he has just returned : the hash bars in Amsterdam , the French McDonald 's and its \\\" Royale with Cheese \\\" . Figure 5.10 : A representative application of NER to a random Wikipedia sample [ 1]. Entities are identified as follows : Person , Location , Organization . 26 for long - term dependencies in the text . The results of Table 5.4 indicate the perfor- mance gain associated with herded Gibbs ; herded Gibbs achieves accuracy equal to that of Viterbi and a faster convergence rate than annealed Gibbs . Table 5.4 : Comparisons of Gibbs , herded Gibbs , herded Gibbs with a ran- domized step size , and Viterbi for the NER task . We used the pre - trained 3-class CRF model in the Stanford NER package [ 7]. For the test set , we used the corpus for the NIST 1999 IE - ER Evaluation . For all the methods except Viterbi , we show F1 scores after 100 , 400 and 800 iterations . For Gibbs and herded Gibbs with a random step size , the results shown are the averages and standard deviations ( in the parentheses ) over 5 random runs . We used a linear annealing schedule for Gibbs and in - place updates for both versions of herded Gibbs . The average computational time each approach took to do inference for the entire test set is also listed ( in the brackets ) . R : random step size . While Gibbs relies on drawing samples from the full- conditionals in a random way , herded Gibbs generates the samples by matching the full - conditionals . Several experiments are employed to indicate the effectiveness of herded Gibbs over multiple domains and in tricky situations that can inhibit the performance of Gibbs sampling . The experiments also showed that the determin- istic algorithm can fail to converge to the joint distribution when the ratios of the conditionals are rational , but that it converges when the same ratios are irrational . This motivated the introduction of a herded Gibbs algorithm with random step size , which retained improved rates of convergence over Gibbs , while delivering the cor- rect marginal and joint distributions . Proving consistency and rates of convergence of the deterministic herded Gibbs method for irrational ratios and the random step size herded Gibbs is an open problem . As demonstrated Chapter 4 , herded Gibbs trades storage cost for faster conver- gence by storing a parameter for each conditional . Without resolving the storage issue , it will be hard for herded Gibbs to be applicable to more general models , such as densely connected models . However , by the image denoising experiment we show that a shared version of herded Gibbs requires less storage while perform- ing even better than comparable inference techniques . Further exploration of the shared version of herded Gibbs and its applications to other models is a promising avenue for further research . 28 Bibliography [ 1 ] Pulp fiction - wikipedia , the free encyclopedia @ONLINE , June 2012 . An introduction to mcmc for machine learning . Machine Learning , 50:5 - 43 , 2003 . ISSN 0885 - 6125 . 10.1023/A:1020281327116 . On the equivalence between herding and conditional gradient algorithms . In International Conference on Machine Learning , 2012 . Bayesian Theory , pages 207 - 209 . Wiley Series in Probability and Statistics . John Wiley & Sons Ltd. , Chichester , 1994 . Supersamples from kernel - herding . In Uncertainty in Artificial Intelligence , pages 109 - 116 , 2010 . Methods for approximating integrals in statistics with special emphasis on bayesian integration problems . Incorporating non - local information into information extraction systems by Gibbs sampling . In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , ACL ' 05 , pages 363 - 370 , Stroudsburg , PA , USA , 2005 . Association for Computational Linguistics . doi:10.3115/1219840.1219885 . An algorithm for quadratic programming . Naval Research Logistics Quarterly , 3(1 - 2):95 - 110 , 1956 . ISSN 1931 - 9193 . 29 doi:10.1002/nav.3800030109 . On herding and the perceptron cycling theorem . In Advances in Neural Information Processing Systems , pages 694 - 702 , 2010 . Sampling - based approaches to calculating marginal densities . Journal of the American Statistical Association , 85(410 ) : 398 - 409 , 1990 . doi:10.1080/01621459.1990.10476213 . Stochastic relaxation , gibbs distributions , and the bayesian restoration of images . Pattern Analysis and Machine Intelligence , IEEE Transactions on , PAMI-6(6):721 -741 , nov . 1984 . ISSN 0162 - 8828 . doi:10.1109/TPAMI.1984.4767596 . Optimally - weighted herding is Bayesian quadrature . Arxiv preprint arXiv:1204.1664 , 2012 . Canonical representation of conditionally specified multivariate discrete distributions . Journal of Multivariate Analysis , 100(6 ) : 1282 - 1290 , 2009 . Bayesian computational methods . Philosophical Transactions of the Royal Society of London . Series A : Physical and Engineering Sciences , 337(1647):369 - 386 , 1991 . doi:10.1098/rsta.1991.0130 . Markov chain monte carlo and gibbs sampling . Herding dynamical weights to learn . In International Conference on Machine Learning , pages 1121 - 1128 , 2009 . Herding dynamic weights for partially observed random field models . In Uncertainty in Artificial Intelligence , pages 599 - 606 , 2009 . Statistical inference using weak chaos and infinite memory . Journal of Physics : Conference Series , 233(1 ) , 2010 . Citation Scheme : APA APA ( 5th Edition ) APA No DOI , No Issue BibTeX Chicago , Author , Date IEEE LexisNexis ( Guide Lluelles , 7th Edition ) Modern Language Association , With Url Turabian , Full Note Bibliography . Feedback on Open Collections Website . Open Collections is an initiative to bring together locally created and managed content from the University of British Columbia Library 's open access repositories . The Library welcomes questions and comments about Open Collections . If you notice any bugs , display issues , or data issues - or just want to say hi - you 're in the right place ! Thanks for visiting Open Collections . \"}",
        "_version_":1692669279360516096,
        "score":19.01115},
      {
        "id":"f173ddf0-ea74-4edd-814e-fa77f92d9bc6",
        "_src_":"{\"url\": \"https://noisebridge.net/index.php?title=Category:Events&oldid=34235\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162808.51/warc/CC-MAIN-20160205193922-00100-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"1999 IEEE Workshop on Automatic Speech Recognition and Understanding , Keystone , Colorado , December 1999 . Publisher : . One approach to speaker adaptation for the neural - network acoustic models of a hybrid connectionist - HMM speech recognizer is to adapt a speaker - independent network by performing a small amount of additional training using data from the target speaker , giving an acoustic model specifically tuned to that speaker . This adapted model might be useful for speaker recognition too , especially since state - of - the - art speaker recognition typically performs a speech - recognition labelling of the input speech as a first stage . We present the results of using such an approach for a set of 12 speakers selected from the DARPA / NIST Broadcast News corpus . The speaker - adapted nets showed a 17 % relative improvement in worderror rate on their target speakers , and were able to identify among the 12 speakers with an average equal - error rate of 6.6 % . \"}",
        "_version_":1692670204453060609,
        "score":18.780962},
      {
        "id":"b4fc5b8f-d0c0-462a-b4b1-a25f97e042e0",
        "_src_":"{\"url\": \"http://traumamanagement.biomedcentral.com/articles/10.1186/1752-2897-4-7\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701165697.9/warc/CC-MAIN-20160205193925-00026-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Fuzzy Pruning Based LS - SVM Modeling Development for a Fermentation Process . 1 Key Laboratory of Advanced Process Control for Light Industry ( Ministry of Education ) , Jiangnan University , Wuxi 214122 , China 2 School of Internet of Things Engineering , Jiangnan University , Wuxi 214122 , China . Received 16 December 2013 ; Revised 14 January 2014 ; Accepted 14 January 2014 ; Published 27 February 2014 . Academic Editor : Shen Yin . Copyright \\u00a9 2014 Weili Xiong et al . This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited . Abstract . Due to the complexity and uncertainty of microbial fermentation processes , data coming from the plants often contain some outliers . However , these data may be treated as the normal support vectors , which always deteriorate the performance of soft sensor modeling . Since the outliers also contaminate the correlation structure of the least square support vector machine ( LS - SVM ) , the fuzzy pruning method is provided to deal with the problem . Furthermore , by assigning different fuzzy membership scores to data samples , the sensitivity of the model to the outliers can be reduced greatly . The effectiveness and efficiency of the proposed approach are demonstrated through two numerical examples as well as a simulator case of penicillin fermentation process . Introduction . For the limitation of advanced measurement techniques , some important process variables in biochemical industrial processes , such as product composition , product concentration , and biomass concentration , are difficult or impossible to measure online . However , these variables are very important for the products quality and the result of the whole reaction process . A soft sensor model is always needed to construct between variables which are easy to measure online and one which is difficult to measure . Then a value of an objective variable can be inferred by this model . The approaches and corresponding applications of soft sensors have been discussed in some literature [ 1 - 4 ] . For example , partial least squares ( PLS ) and principal component analysis ( PCA ) [ 5 , 6 ] are the most popular projection based soft sensor modeling methods for modeling and prediction . However , a drawback of these models is their linear nature . If it is known that the relation between the easy - to - measure and the difficult - to - measure variables is nonlinear , then a nonlinear modeling method should be used . Although the NPCA is a well - established and powerful algorithm , it has several drawbacks . One of them is that the principal components describe very well the input space but do not reflect the relation between the input and the output data space . A solution to this drawback is given by the NPLS method . NPLS models are appropriate to study the behavior of the process . Unfortunately , sometimes the algorithm of NPLS is available only for specific nonlinear relationships . To break through the limitation of NPLS , ANN is adopted to solve the complexity and highly nonlinear problem in the case of the sample data tending to infinity . The disadvantage of ANNs is that during their learning they are prone to get stuck in local minima , which can result in suboptimal performance . Meanwhile , SVM has been demonstrated to work very well for a wide spectrum of applications under the limited training data samples , so it is not surprising that it has also been successfully applied as soft sensor . Support vector machine ( SVM ) proposed by Vapnik [ 11 , 12 ] , which is based on statistical learning theory , obtains the optimal classification of the sample data through a quadratic programming . So it can balance the risk of learning algorithm and promotion of the extension ability . As a sophisticated soft sensor modeling method , SVM has a lot of advantages in solving small sample data and nonlinear and high dimensional pattern recognition and has been applied to the fermentation process successfully [ 13 , 14 ] . Least squares support vector machine ( LS - SVM ) proposed by Suykens and Vandewalle [ 15 ] is an extension of the standard SVM . It can solve linear equations with faster solution speed and figure out the robustness , sparseness , and large - scale computing problems . However , all training data are treated as the normal support vector which loses the sparseness of SVM [ 16 - 19 ] . In this paper , the effective work addressed in Section 3 could improve the performance of the standard LS - SVM effectively . For this process , key variables are concentration of the biomass , product , and substrate which are difficult to measure directly . However , some other auxiliary variables are easy to measure . So we choose aeration rate , dissolved oxygen concentration , agitator power , and others as auxiliary variables and the concentration of penicillin as the quality variable in this process . The next step is to construct the inferred model between the auxiliary variables and the quality variable . Outliers are commonly encountered in penicillin fermentation process which may be treated as the normal support vector and always has a bad influence on the precision of the soft sensor model . So applying the idea of fuzzy pruning for LS - SVM algorithm to cut off these outliers and reduce the number of support vectors will improve the sparseness and precision of the original LS - SVM model . Also assigning different fuzzy membership scores to sample data , the sensitivity to the outliers is reduced and the accuracy of the model is further improved as well . Finally , the LS - SVM and fuzzy pruning based LS - SVM soft sensor models for the penicillin fermentation process are constructed based on the optimal parameters obtained by using particle swarm optimization algorithm [ 21 , 22 ] . Thus a soft sensor model with higher prediction precision and better generalization capability for penicillin fermentation process is completed . The remainder of this paper is organized as follows . Section 2 begins with the revisit of LS - SVM algorithm and lays out the mathematical formulations . Detailed descriptions of improved LS - SVM based on fuzzy pruning algorithm are provided in Section 3 . Two numerical simulation examples are illustrated in Section 4 which aims to demonstrate the effectiveness of the proposed method in developing soft sensors . Thereafter , a soft sensor application for the penicillin fermentation process using the proposed approach is presented in Section 5 . Section 6 draws conclusions based on the results obtained in this paper . The main difference between LS - SVM and SVM is that LS - SVM adopts the equality constraints instead of inequality constraints , and empirical risk is the deviation of the quadratic rather than one square deviation . By introducing the Kernel function . Improved LS - SVM with Fuzzy Pruning Algorithm . The Idea of Fuzzy Pruning Algorithm . Compared with SVM , the computational load of LS - SVM is reduced greatly . However , LS - SVM loses its sparseness because all training data are treated as support vectors even the outliers which always have a bad influence on the precision of the soft sensor model . In this paper , aiming to minimize effects of the outliers as well as the antidisturbance ability of sampling data [ 23 , 24 ] , fuzzy pruning approach is employed to handle the problem . The number of the support vectors is reduced which improves the sparseness of LS - SVM and model accuracy as well . Furthermore , the sensitivity to outliers of the proposed algorithm can be reduced through the fuzzy membership score assigned to the data samples . The absolute value of Lagrange multiplier determines the importance of data in the training process which means the higher the absolute value , the greater the influence degree . The absolute value of Lagrange multiplier of outliers is often higher than that of the normal data . Based on this situation , the data which have the higher absolute value of Lagrange multiplier will be cut off according to certain proportion ( e.g. , 5 % ) . When these data are cut off , the impact of outlier data is minimized , and the model sparseness and accuracy are improved simultaneously . Since Lagrange multiplier plays an important role in constructing model , a fuzzy membership score is introduced to adjust the weight of data for modeling . Fuzzy membership value is defined as . It is noticed that the fuzzy membership score is near to zero when Lagrange multiplier is very small . So the corresponding sampling data may play no role in modeling , which means a part of sample data can be cut off according to the absolute value of Lagrange multiplier that is very small . As a result , the sparseness of the proposed LS - SVM algorithm is further improved . , and cut off the data taking larger Lagrange multiplier according to certain proportion ( e.g. , 5 % ) . ( 5 ) Then the fuzzy pruning based LS - SVM algorithm is applied to train the current data set . If the fitting performance degrades , the training procedure is done . Otherwise , switch to ( 4 ) . Two Numerical Simulations . One - Dimension Function . The effectiveness and efficiency of handing the outliers through the proposed approach are evaluated through two numerical functions . All the simulation experiments are run on a 2.8 GH CPU with 1024 MB RAM PC using Matlab 7.11 . randomly as the training data set . To test the performance of detecting outliers , 30 % disturbance is added to the 20th , 40th , 60th , 80th , and 100th data sample , respectively . And another 100 data are collected for evaluation . It can be seen from Figure 1 that the outliers have the higher value of Lagrange multiplier as mentioned above . Using PSO algorithm ( . , then the LS - SVM and fuzzy pruning LS - SVM models are constructed to predict and compare ( Figures 2 and 3 ) . Figure 3 is the 45-degree line comparison between different measurements . If two measurements agree with the true outputs , then all data points will fall into the black 45-degree line . The blue circles denote the LS - SVM measurements and the pink asterisks denote the model predictions of fuzzy pruning LS - SVM . We can see that the estimation with the fuzzy pruning LS - SVM fits the black line better and thus provides a superior performance compared to the LS - SVM observation . The detailed results such as the maximum absolute error ( Max EE ) , the mean absolute error ( Mean EE ) , and root mean square error ( RMSE ) are calculated and listed in Table 1 . The RMSE decreased from 1.21 % to 0.052 % , which indicates the fuzzy pruning LS - SVM has higher prediction performance and better antidisturbance . Two - Dimension Function . , which makes up a training data set . Then the 20th , 40th , 60th , 80th , and 100th data points are added with 30 % disturbance separately and the performance is tested by using another different 100 data . As is shown in Figure 4 , Lagrange multiplier value of data points that corrupted by some disturbance always has the higher value . Compared results are shown in Figure 5 . From Table 2 , prediction accuracy of fuzzy pruning LS - SVM is much higher than LS - SVM , which indicates the five outliers have been detected and cut off effectively using the proposed method . An Experiment Simulation . The Pensim simulator provides a simulation of a fed - batch fermentation process for penicillin production . The main component of the process is a fermenter , where the biological reaction takes place . It fully considers the most factors influencing the penicillin fermentation process , such as PH , aeration rate , substrate feed rate , carbon dioxide , and penicillin production . The practicability and validity of the platform have been fully verified [ 25 - 27 ] and it has been a benchmark problem for modeling and diagnosis detection . In this paper Pensim simulation platform is used to generate the original 100 training data . Then 30 % disturbance is added to the 20th , 30th , 40th , 60th , and 85th , respectively , and another 100 data are used as test data to verify the constructed model . The simulation results are shown in Figures 7 and 8 . To further exhibit the difference of the two methods , the indexes of Max EE , Mean EE , and RMSE of each method are also calculated and listed in Table 3 . Compared to LS - SVM , the proposed approach makes RMSE decrease from 2.44 % to 0.97 % , which indicates the fuzzy pruning LS - SVM has better prediction performance . Lagrange multiplier values according to each data point are shown in Figure 6 , and we can easily find out the outliers obviously have much bigger Lagrange multiplier . Figure 8 is the 45-degree line comparison between two different soft sensors . Clearly , the fuzzy pruning based LS - SVM exhibits the better capability of approximating the true process . It has effectively handled the disturbance of the outliers so that their impact on modeling is minimized to lowest . Conclusions . A novel LS - SVM method based on fuzzy pruning technique is investigated in this paper . Pruning algorithm is applied to cut off the outliers . Therefore the number of support vectors is reduced which improves the sparseness and accuracy of LS - SVM algorithm . On the other hand , assigning different fuzzy membership score to each of the sample data makes those sample data that play a small role in soft sensor modeling not participate in the construction of the model . Furthermore , the sensitivity to the outliers of the proposed algorithm can be reduced through the fuzzy membership score . The simulation examples demonstrated that the proposed method can effectively handle the outliers and achieved satisfied performance of modeling and prediction . Conflict of Interests . The authors declare that there is no conflict of interests regarding the publication of this paper . Acknowledgments . The authors thank the financial support by the National Natural Science Foundation of China ( nos . 21206053 , 21276111 , and 61273131 ) and partial support by the 111 Project ( B12018 ) and the Priority Academic Program Development of Jiangsu Higher Education Institutions ( PAPD ) . References . S. Yin , H. Luo , and S. Ding , \\\" . Real - time implementation of fault - tolerant control systems with performance optimization , \\\" IEEE Transactions on Industrial Electronics , vol . 64 , no . 5 , pp . 2402 - 2411 , 2014 . View at Google Scholar . Q.-D. Yang , F.-L. Wang , and Y.-Q. Chang , \\\" Soft sensor of biomass based on improved BP neural network , \\\" Control and Decision , vol . 23 , no . 8 , pp . 869 - 878 , 2008 . View at Google Scholar \\u00b7 View at Scopus . G. Liu , D. Zhou , H. Xu , and C. Mei , \\\" Soft sensor modeling using SVM in fermentation process , \\\" Chinese Journal of Scientific Instrument , vol . 30 , no . 6 , pp . 1228 - 1232 , 2009 . View at Google Scholar \\u00b7 View at Scopus . L. Huang , Y. Sun , X. Ji , Y. Huang , and B. Wang , \\\" Soft sensor of lysine fermentation based on tPSO - BPNN , \\\" Chinese Journal of Scientific Instrument , vol . 31 , no . 10 , pp . 2317 - 2321 , 2010 . View at Google Scholar \\u00b7 View at Scopus . S. J. Qin and T. J. McAvoy , \\\" Nonlinear PLS modeling using neural networks , \\\" Computers and Chemical Engineering , vol . 16 , no . 4 , pp . 379 - 391 , 1992 . View at Google Scholar \\u00b7 View at Scopus . D. Dong and T. J. Mcavoy , \\\" Nonlinear principal component analysis : based on principal curves and neural networks , \\\" Computers and Chemical Engineering , vol . 20 , no . 1 , pp . 65 - 78 , 1996 . View at Google Scholar \\u00b7 View at Scopus . J. C. B. Gonzaga , L. A. C. Meleiro , C. Kiang , and R. Maciel Filho , \\\" ANN - based soft - sensor for real - time process monitoring and control of an industrial polymerization process , \\\" Computers and Chemical Engineering , vol . 33 , no . 1 , pp . 43 - 49 , 2009 . View at Publisher \\u00b7 View at Google Scholar \\u00b7 View at Scopus . G. Liu , D. Zhou , H. Xu , and C. Mei , \\\" Microbial fermentation process soft sensors modeling research based on the SVM , \\\" Chinese Journal of Scientific Instrument , vol . 30 , no . 6 , pp . 1228 - 1232 , 2009 . View at Google Scholar \\u00b7 View at Scopus . X.-J. Gao , P. Wang , C.-Z. Sun , J.-Q. Yi , Y.-T. Zhang , and H.-Q. Zhang , \\\" Modeling for Penicillin fermentation process based on support vector machine , \\\" Journal of System Simulation , vol . 18 , no . 7 , pp . 2052 - 2055 , 2006 . View at Google Scholar \\u00b7 View at Scopus . J. A. K. Suykens and J. Vandewalle , \\\" Least squares support vector machine classifiers , \\\" Neural Processing Letters , vol . 9 , no . 3 , pp . 293 - 300 , 1999 . View at Google Scholar \\u00b7 View at Scopus . X. Wang , J. Chen , C. Liu , and F. Pan , \\\" Hybrid modeling of penicillin fermentation process based on least square support vector machine , \\\" Chemical Engineering Research and Design , vol . 88 , no . 4 , pp . 415 - 420 , 2010 . View at Publisher \\u00b7 View at Google Scholar \\u00b7 View at Scopus . R.-Q. Chen and J.-S. Yu , \\\" Soft sensor modeling based on particle swarm optimization and least squares support vector machines , \\\" Journal of System Simulation , vol . 19 , no . 22 , pp . 5307 - 5310 , 2007 . View at Google Scholar \\u00b7 View at Scopus . L. Huang , Y. Sun , X. Ji , Y. Huang , and T. Du , \\\" Soft sensor modeling of fermentation process based on the combination of CPSO and LSSVM , \\\" Chinese Journal of Scientific Instrument , vol . 32 , no . 9 , pp . 2066 - 2070 , 2011 . View at Google Scholar \\u00b7 View at Scopus . X. Zhang , \\\" Using class - center vectors to build support vector machines , \\\" in Proceedings of the 9th IEEE Workshop on Neural Networks for Signal Processing ( NNSP ' 99 ) , pp . 3 - 11 , August 1999 . View at Scopus . S. Yin , S. Ding , A. Haghani , and H. Hao , \\\" Data - driven monitoring for stochastic systems and its application on batch process , \\\" International Journal of Systems Science , vol . 44 , no . 7 , pp . 1366 - 1376 , 2013 . View at Google Scholar . Y. Liu and H.-Q. Wang , \\\" Pensim simulator and its application in penicillin fermentation process , \\\" Journal of System Simulation , vol . 18 , no . 12 , pp . 3524 - 3527 , 2006 . View at Google Scholar \\u00b7 View at Scopus . S. Yin , S. Ding , A. Haghani , H. Hao , and P. Zhang , \\\" A comparison study of basic data driven fault diagnosis and process monitoring methods on the benchmark Tennessee Eastman process , \\\" Journal of Process Control , vol . 22 , no . 9 , pp . 1567 - 1581 , 2012 . View at Google Scholar \"}",
        "_version_":1692580761594494977,
        "score":18.376247}]
  }}
