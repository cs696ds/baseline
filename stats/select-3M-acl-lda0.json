{
  "responseHeader":{
    "status":0,
    "QTime":4,
    "params":{
      "q":"lexical language rules information natural analysis text e.g lin processing rule extraction general question found speech chen applications work computational",
      "fl":"*,score"}},
  "response":{"numFound":1862747,"start":0,"maxScore":31.90664,"numFoundExact":true,"docs":[
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":31.90664},
      {
        "id":"5f453de1-8993-46be-880e-a4d8ed7472fd",
        "_src_":"{\"url\": \"http://www.disappearednews.com/2012/11/will-honolulu-pull-its-head-out-of-sand.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701160950.71/warc/CC-MAIN-20160205193920-00188-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"SWEETpedia . A Periodic Update of Semantic Web - related Research using Wikipedia . One of the more popular posts of this AI3 blog was a listing of 99 research articles that used Wikipedia in one way or another to do semantic - Web related research . It was first posted in February 2008 . Many of you suggested additions to that listing . Wikipedia continues to be an effective and unique source for many information extraction and semantic Web purposes . We thus decided to make this listing a permanent resource of this blog and to update it on a periodic basis . If you have additions to this listing , please suggest them here and they will be added to the next periodic update . UPDATE : This alphabetical listing now contains 246 articles , and was posted on Jan. 25 , 2010 . This update adds 19 articles since the last listing of 227 entries posted on Sept. 10 , 2009 . All new entries since the last update are noted with [ NEW ] . Eytan Adar , Michael Skinner and Daniel S. Weld . Information Arbitrage across Multi - lingual Wikipedia , in Proceedings of the Second ACM International Conference on Web Search and Data Mining ( WSDM'09 ) , February 9 - 12 , 2009 , Barcelona , Spain , pages 94 - 103 . S\\u00f6ren Auer , Chris Bizer , Jens Lehmann , Georgi Kobilarov , Richard Cyganiak and Zachary Ives , 2007 . DBpedia : A nucleus for a web of open data , in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ( ISWC / ASWC2007 ) , Busan , South Korea , volume 4825 of LNCS , pages 715728 , November 2007 . S\\u00f6ren Auer and Jens Lehmann , 2007 . What Have Innsbruck and Leipzig in Common ? Extracting Semantics from Wiki Content , in The Semantic Web : Research and Applications , pages 503 - 517 , 2007 . Dominic Balasuriya , Nicky Ringland , Joel Nothman , Tara Murphy and James R. Curran , 2009 . Named Entity Recognition in Wikipedia , in Proceedings of the 2009 Workshop on the People 's Web Meets NLP , ACL - IJCNLP 2009 , pages 10 - 18 , Suntec , Singapore , 7 August 2009 . Somnath Banerjee , Krishnan Ramanathan , Ajay Gupta , 2007 . Clustering Short Texts using Wikipedia , poster presented at Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , Amsterdam , The Netherlands , pp . 787 - 788 . Sebastian Blohm , Philipp Cimiano , 2007 . Using the Web to Reduce Data Sparseness in Pattern - based Information Extraction , in Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases ( PKDD ) , pp . 18 - 29 . Springer , Warsaw , Poland , September 2007 . Gosse Bouma , Sergio Duarte and Zahurul Islam , 2009 . Cross - lingual Alignment and Completion of Wikipedia Templates , in Proceedings of CLIAWS3 , Third International Cross Lingual Information Access Workshop , pages 21 - 29 , Boulder , Colorado , June 2009 . Bryan Chan , Leslie Wu , Justin Talbot , Mike Cammarano and Pat Hanrahan , 2008 . Vispedia : Interactive Visual Exploration of Wikipedia Data via Search - based Integration , in IEEE Transactions on Visualization and Computer Graphics , 14(6):1213 - 1220 , 2008 . Kino Coursey and Rada Mihalcea , 2009 . Topic Identification Using Wikipedia Graph Centrality , in Proceedings of Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics , June , 2009 , Boulder , Colorado . Kino Coursey , Rada Mihalcea and William Moen . Using Encyclopedic Knowledge for Automatic Topic Identification , in Proceedings of the Conference on Natural Language Learning , Boulder , CO . Gaoying Cui , Qin Lu , Wenjie Li and Yirong Chen , 2009 . Automatic Acquisition of Attributes for Ontology Construction , in Computer Processing of Oriental Languages . Language Technology for the Knowledge - based Economy , Springer . [ NEW ] Gaoying Cui , Qin Lu , Wenjie Li and Yirong Chen , 2009 . Mining Concepts from Wikipedia for Ontology Construction , in Proceedings of the 2009 IEEE / WIC / ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology , p.287 - 290 , September 15 - 18 , 2009 . Silviu Cucerzan , 2007 . Large - Scale Named Entity Disambiguation Based on Wikipedia Data , in Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) . Maike Erdmann , Kotaro Nakayama , Takahiro Hara , Sojiro Nishio , 2008 . An Approach for Extracting Bilingual Terminology from Wikipedia , in Proceedings of the 13th International Conference on Database Systems for Advanced Applications ( DASFAA ) . Sergio Ferr\\u00e1ndez , Antonio Toral , \\u00d3scar Ferr\\u00e1ndez , Antonio Ferr\\u00e1ndez and Rafael Mu\\u00f1oz , 2007 . Applying Wikipedias Multilingual Knowledge to CrossLingual Question Answering , in Proceedings of the 12th International Conference on Applications of Natural Language to Information Systems , Paris , France , pp . 352363 . June 2007 . [ NEW ] Angel Fogarolli and Marco Ronchetti , 2008 . Intelligent Mining and Indexing of Multi - Language e - Learning Material , in Proceedings of the 1st International Symposium on Intelligent Interactive Multimedia Systems and Services , KES IIMS 2008 , 9 - 11 July 2008 Piraeus , Greece Studies in Computational Intelligence , Springer - Verlag . Angela Fogarolli and Marco Ronchetti , 2009 . Extracting Semantics from Multimedia Content using Wikipedia , in the Special Issue of Scalable Computing : Practice and Experience , v. 1895 - 1767 . Y. Ganjisaffar , S. Javanmardi and C. Lopes , 2009a . Using User Reviews to Improve Search in Wikipedia , SIGIR 2009 Workshop on Search in Social Media , Boston , USA , 2009 . Y. Ganjisaffar , S. Javanmardi and C. Lopes , 2009b . Leveraging Crowdsourcing Heuristics to Improve Search in Wikipedia , in Proceedings of the 5th International Symposium on Wikis and Open Collaboration ( WikiSym ) , Florida , USA , 2009 . Y. Ganjisaffar , S. Javanmardi and C. Lopes , 2009c . Review - based Ranking of Wikipedia Articles , in Proceedings of the International Conference on Computational Aspects of Social Networks , Fontainebleau , France , 2009 . Zeno Gantner and Lars Schmidt - Thieme , 2009 . Automatic Content - based Categorization of Wikipedia Articles , in Proceedings of the 2009 Workshop on the People 's Web Meets NLP , ACL - IJCNLP 2009 , pages 32 - 37 , Suntec , Singapore , 7 August 2009 . Maxim Grinev , Dmitry Lizorkin , Denis Turdakov and Pavel Velikhov , 2008 . Efficient Ranking and Computation of Semantic Relatedness and its Application to Word Sense Disambiguation , Institute for System Programming , Russian Academy of Sciences Technical Report , 2008 . A. Herbelot and Ann Copestake , 2006 . Acquiring Ontological Relationships from Wikipedia Using RMRS , in Proc . International Semantic Web Conference 2006 Workshop , Web Content Mining with Human Language Technologies , Athens , GA , 2006 . Ryuichiro Higashinaka , Kohji Dohsaka and Hideki Isozaki , 2007 . Raphael Hoffmann , Saleema Amershi , Kayur Patel , Fei Wu , James Fogarty and Daniel S. Weld , 2009 . Amplifying Community Content Creation Using Mixed - Initiative Information Extraction , in CHI2009 : Conference on Computer Human Interaction . Todd Holloway , Miran Bozicevic , and Katy B\\u00f6rner , 2005 . Analyzing and Visualizing the Semantic Coverage of Wikipedia and Its Authors . ArXiv Computer Science e - prints , cs/0512085 . Wei Che Huang , Andrew Trotman , and Shlomo Geva , 2007 . Collaborative Knowledge Management : Evaluation of Automated Link Discovery in the Wikipedia , in SIGIR 2007 Workshop on Focused Retrieval , July 27 , 2007 , Amsterdam , The Netherlands . Masahiro Ito , Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2008 . Association Thesaurus Construction Methods based on Link Co - occurrence Analysis For Wikipedia , in Proceedings of ACM International Conference on Information and Knowledge Management ( CIKM ) . [ NEW ] Rianne Kaptein and Jaap Kamps , 2009a . Using Links to Classify Wikipedia Pages , in Shlomo Geva , Jaap Kamps , and Andrew Trotman , editors , Advances in Focused Retrieval : 7th International Workshop of the Initiative for the Evaluation of XML Retrieval ( INEX 2008 ) , volume 5631 of LNCS . Springer Verlag , Berlin , Heidelberg . [ NEW ] Rianne Kaptein and Jaap Kamps , 2009b . Finding Entities in Wikipedia using Links and Categories , in Shlomo Geva , Jaap Kamps , and Andrew Trotman , editors , Advances in Focused Retrieval : 7th International Workshop of the Initiative for the Evaluation of XML Retrieval ( INEX 2008 ) , volume 5631 of LNCS . Springer Verlag , Berlin . [ NEW ] Rianne Kaptein , Marijn Koolen , and Jaap Kamps , 2009a . Experiments with Result Diversity and Entity Ranking : Text , Anchors , Links , and Wikipedia , in The Eighteenth Text REtrieval Conference ( TREC 2009 ) Notebook . National Institute for Standards and Technology . Gjergji Kasneci , Fabian M. Suchanek , Georgiana Ifrim , Maya Ramanath and Gerhard Weikum , 2007 . NAGA : Searching and Ranking Knowledge , Technical Report , Max - Planck - Institut f\\u00a8ur Informatik , MPII20075001 , March 2007 , 42 pp . Junichi Kazama and Kentaro Torisawa , 2007 . Exploiting Wikipedia as External Knowledge for Named Entity Recognition , in Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp . 698707 , Prague , June 2007 . Jun'ichi Kazama and Kentaro Torisawa , 2008 . Inducing Gazetteers for Named Entity Recognition by Large - scale Clustering of Dependency Relations , in Proceedings of ACL-08 : HLT , pages 407 - 415 , Columbus , Ohio , USA , June 2008 . [ NEW ] Yusuke Kiritani , Qiang Ma and Masatoshi Yoshikawa , 2009 . Classifying Web Pages by Using Knowledge Bases for Entity Retrieval , in Lecture Notes In Computer Science ; Vol . 5690 , Proceedings of the 20th International Conference on Database and Expert Systems Applications , Linz , Austria . A. Krizhanovsky , 2006 . Synonym Search in Wikipedia : Synarcher , in 11th International Conference Speech and Computer SPECOM2006 . Russia , St. Petersburg , June 25 - 29 , 2006 , pp . 474 - 477 . Andrew Krizhanovsky and Feiyu Lin , 2009 . Related Terms Search based on WordNet / Wiktionary and its Application in Ontology Matching . Miro Lehtonen and Antoine Doucet , 2007 . EXTIRP : Baseline Retrieval from Wikipedia , in Comparative Evaluation of XML Information Retrieval Systems , pp . 115120 . Yinghao Li , Wing Pong Robert Luk , Kei Shiu Edward Ho and Fu Lai Korris Chung , 2007 . Improving Weak Ad - Hoc Queries Using Wikipedia As External Corpus , in Kraaij et al . ( editors ) Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , SIGIR07 , Amsterdam , The Netherlands , July 2327 , 2007 , pp . 797798 . ACM Press . Olena Medelyan , Catherine Legg , David Milne and Ian H. Witten , 2008 . Mining Meaning from Wikipedia , Working Paper Series ISSN 1177 - 777X , Department of Computer Science , The University of Waikato ( New Zealand ) , September 2008 , 82 pp . Lev Muchnik , Royi Itzhack , Sorin Solomon and Yoram Louzoun , 2007 . Self - emergence of Knowledge Trees : Extraction of the Wikipedia Hierarchies , in Physical Review E ( Statistical , Nonlinear , and Soft Matter Physics ) , Vol . 76 , No . 1 . David Nadeau , Peter D. Turney and Stan Matwin , 2006 . Unsupervised Named - Entity Recognition : Generating Gazetteers and Resolving Ambiguity , at 19th Canadian Conference on Artificial Intelligence . Qu\\u00e9bec City , Qu\\u00e9bec , Canada . June 7 , 2006 . Does n't specifically use Wikipedia , but techniques are applicable . Kotaro Nakayama , Takahiro Hara and Shojiro Nishio , 2007a . Wikipedia Mining for an Association Web Thesaurus Construction , in Web Information Systems Engineering WISE 2007 , Vol . 4831 ( 2007 ) , pp . 322 - 334 . Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2007a . Wikipedia : A New Frontier for AI Researches , in Journal of the Japanese Society for Artificial Intelligence 22(5 ) , pp . 693701 . Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2007b . A Thesaurus Construction Method from Large Scale Web Dictionaries , in Proceedings of the 21st IEEE International Conference on Advanced Information Networking and Applications , AINA07 , May 2123 , 2007 , Niagara Falls , Canada , pp . 932939 , iEEE Computer Society . Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2008a . A Search Engine for Browsing the Wikipedia Thesaurus , in Proceedings of the 13th International Conference on Database Systems for Advanced Applications , Demo session ( DASFAA08 ) , pp . 690693 . Kotaro Nakayama , Takahiro Hara and Shojiro Nishio , 2008c . Kotaro Nakayama , Masahiro Ito , Takahiro Hara and Sojiro Nishio , 2008 . Wikipedia Mining for Huge Scale Japanese Association Thesaurus Construction , in Workshop Proceedings of the 22nd International Conference on Advanced Information Networking and Applications , AINA08 , GinoWan , Okinawa , Japan , March 25 28 , 2008 , pp . 11501155 , iEEE Computer Society . Vivi Nastase and Michael Strube , 2009 . Combining Collocations , Lexical and Encyclopedic Knowledge for Metonymy Resolution , in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing , pages 910 - 918 , Singapore , 6 - 7 August 2009 . Rani Nelken and Elif Yamangil , 2008 . Mining Wikipedias Article Revision History for Traning Computational Lingustic Algorithms , in Proceedings of the WIKI - AI : Wikipedia and AI Workshop at the AAAI08 Conference , Chicago , US . Simon Overell , B\\u00f6rkur Sigurbj\\u00f6rnsson and Roelof van Zwol , 2009 . Classifying Tags using Open Content Resources , in Proceedings of the Second ACM International Conference on Web Search and Data Mining ( WSDM'09 ) , February 9 - 12 , 2009 , Barcelona , Spain . Marius Pasca , 2009 . Outclassing Wikipedia in Open - Domain Information Extraction : Weakly - Supervised Acquisition of Attributes over Conceptual Hierarchies , in Proceedings of the 12th Conference of the European Chapter of the ACL , pages 639 - 647 , Athens , Greece , 30 March - 3 April 2009 . Minghua Pei , Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2008a . Constructing a Global Ontology by Concept Mapping using Wikipedia Thesaurus , in Proceedings of the 22nd International Conference on Advanced Information Networking and Applications , AINA08 , GinoWan , Okinawa , Japan , March 2528 , 2008 , pp . 12051210 , iEEE Computer Society . Minghua Pei , Kotaro Nakayama , Takahiro Hara and Sojiro Nishio , 2008b . An Integrated Method for Web Resource Categorization , in Proceedings of IEEE International Symposium on Mining And Web ( IEEE MAW ) . Simone Paolo Ponzetto and Michael Strube , 2007c . Simone Paolo Ponzetto and Roberto Navigli , 2009 . Large - Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia , in Proceedings of the International Joint Conference on Artificial Intelligence ( IJCAI-09 ) , Pasadena , CA , July 2009 . [ NEW ] Andrea Prato and Marco Ronchetti , 2009 . Using Wikipedia as a Reference for Extracting Semantic Information from a Text , in The Third International Conference on Advances in Semantic Processing SEMAPRO 2009 , Malta . Nils Reiter , Matthias Hartung and Anette Frank , 2009 . A Resource - Poor Approach for Linking Ontology Classes to Wikipedia Articles , in Semantics in Text Processing . STEP 2008 Conference . Maria Ruiz - Casado , Enrique Alfonseca and Pablo Castells , 2007 . Automatising the Learning of Lexical Patterns : an Application to the Enrichment of WordNet by Extracting Semantic Relationships from Wikipedia . Samuel Sarjant , Catherine Legg , Michael Robinson and Olena Medelyan , 2009 . \\\" All You Can Eat \\\" Ontology - Building : Feeding Wikipedia to Cyc , in 2009 IEEE / WIC / ACM International Conference on Web Intelligence ( WI-09 ) , 15 - 18 September 2009 Universit\\u00e0 degli Studi di Milano Bicocca , Milano , Italy . Christina Sauper and Regina Barzilay , 2009 . Automatically Generating Wikipedia Articles : Structure - Aware Approach , in Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP , pages 208 - 216 , Suntec , Singapore , 2 - 7 August 2009 . [ NEW ] Masumi Shirakawa , Kotaro Nakayama , Takahiro Hara and Shojiro Nishio , 2009 . Concept Vector Extraction from Wikipedia Category Network , Conference On Ubiquitous Information Management And Communication , in Proceedings of the 3rd International Conference on Ubiquitous Information Management and Communication , Suwon , Korea . Eyal Shnarch , Libby Barak and Ido Dagan , 2009 . Extracting Lexical Reference Rules from Wikipedia , in Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP , pages 450 - 458 , Suntec , Singapore , 2 - 7 August 2009 . B\\u00f6rkur Sigurbj\\u00f6rnsson , Jaap Kamps , and Maarten de Rijke . Focused Access to Wikipedia , in Proceedings DIR-2006 . Julia Stoyanovich , Srikanta J. Bedathur , Klaus Berberich and Gerhard Weikum , 2007 . EntityAuthority : Semantically Enriched Graph - Based Authority Propagation , in Tenth International Workshop on the Web and Databases , WebDB 2007 , Beijing , China , June 15 , 2007 . Besiki Stvilia , Michael B. Twidale , Linda C. Smith and Les Gasser , 2005 , information Quality Discussions in Wikipedia . Technical Report ISRN UIUCLIS2005/2+CSCW , Graduate School of Library and Information Science , University of Illinois at Urbana- Champaign . S. Suh , H. Halpin and E. Klein , 2006 . Extracting Common Sense Knowledge from Wikipedia , in Proc . International Semantic Web Conference 2006 Workshop , Web Content Mining with Human Language Technologies , Athens , GA , 2006 . Eugenio Tacchini , Andreas Schultz and Christian Bizer , 2009 . Experiments with Wikipedia Cross - Language Data Fusion , in 5th Workshop on Scripting and Development for the Semantic Web , 31st May , 2009 , Crete , Greece . [ NEW ] Texterra is a toolkit for text mining . Texterra is based on novel text processing methods that exploit semantics extracted from Wikipedia . Texterra delivers a solution for organizing and monitoring collections of documents without the expensive customization that is present in contemporary systems . Noriko Tomuro and Andriy Shepitsen , 2009 . Construction of Disambiguated Folksonomy Ontologies Using Wikipedia , in Proceedings of the 2009 Workshop on the People 's Web Meets NLP , ACL - IJCNLP 2009 , pages 42 - 50 , Suntec , Singapore , 7 August 2009 . Marieke van Erp , Piroska Lendvai , and Antal van den Bosch , 2009 . Comparing Alternative Data - Driven Ontological Vistas of Natural History , in Proceedings of the 8th International Conference on Computational Semantics , pages 282 - 285 , Tilburg , January 2009 . Anne - Marie Vercoustre , Jovan Pehcevski and James A. Thom , 2007 . Using Wikipedia Categories and Links in Entity Ranking , in Pre - proceedings of the Sixth International Workshop of the Initiative for the Evaluation of XML Retrieval ( INEX 2007 ) , Dec 17 , 2007 . Max V\\u00f6lkel , Markus Kr\\u00f6tzsch , Denny Vrandecic , Heiko Haller and Rudi Studer , 2006 . Semantic Wikipedia , in Proceedings of the 15th International Conference on World Wide Web , WWW06 , Edinburgh , Scotland , May 2326 , 2006 . Denny Vrandecic , Markus Kr\\u00f6tzsch and Max V\\u00f6lkel , 2007 . Wikipedia and the Semantic Web , Part II , in Phoebe Ayers and Nicholas Boalch , Proceedings of Wikimania 2006 - The Second International Wikimedia Conference , Wikimedia Foundation , Cambridge , MA , USA , August 2007 . Gang Wang and Huajie Zhang and Haofen Wang and Yong Yu , 2007 . Enhancing Relation Extraction by Eliciting Selectional Constraint Features from Wikipedia , in Proceedings of the Natural Language Processing and Information Systems Conference , pp . 329340 . Timothy Weale , Chris Brew , Eric Fosler - Lussier , 2009 . Using the Wiktionary Graph Structure for Synonym Detection , in Proceedings of the 2009 Workshop on the People 's Web Meets NLP , ACL - IJCNLP 2009 , pages 28 - 31 , Suntec , Singapore , 7 August 2009 . Gabriel Weaver , Barbara Strickland and Gregory Crane , 2006 . Quantifying the Accuracy of Relational Statements in Wikipedia : A Methodology , in Proceedings of the 6th ACM / IEEE - CS Joint Conference on Digital Libraries ( JCDL 06 ) . Daniel S. Weld , Fei Wu , Eytan Adar , Saleema Amershi , James Fogarty , Raphael Hoffmann , Kayur Patel and Michael Skinner , 2008 . Intelligence in Wikipedia , in Proceedings of the 23rd AAAI Conference , ( AAAI-08 ) , Chicago , USA , July , 2008 . WikiXMLDB provides a way of querying Wikipedia with XQuery . The authors have also parsed the English Wikipedia into a well - structured XML document ( 21 GB ) , with a demo available . Fei Wu and Daniel S. Weld , 2007 . Autonomously Semantifying Wikipedia , in Proceedings of the 16th ACM Conference on Information and Knowledge Management , CIKM07 , Lisbon , Portugal , November 68 , 2007 , pp . 4150 . Fei Wu , Raphael Hoffmann and Daniel S. Weld , 2008b . Information Extraction from Wikipedia : Moving Down the Long Tail , in Proceedings of the 14th ACM SigKDD International Conference on Knowledge Discovery and Data Mining ( KDD-08 ) , Las Vegas , NV , August 24 - 27 , 2008 , pp . 635 - 644 . Elif Yamangil and Rani Nelken , 2008 . Mining Wikipedia Revision Histories for Improving Sentence Compression , in Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , Columbus , Ohio , June 15 - 20 , 2008 . Yulan Yan , Naoaki Okazaki , Yutaka Matsuo , Zhenglu Yang and Mitsuru Ishizuka , 2009 . Junghoon Yang , Jangwhan Han , Inseok Oh and Mingyung Kwak , 2007 . Using Wikipedia Technology for Topic Maps Design , in Proceedings of the ACM Southeast Regional Conference , pp . 106110 . Xiaofeng Yang and Jian Su , 2007 . Coreference Resolution Using Semantic Relatedness Information from Automatically Discovered Patterns , in Proceedings of the 45th Annual meeting of the Association for Computational Linguistics , ACL07 , Prague , Czech Republic , pp . 528535 . Eric Yeh , Daniel Ramage , Christopher D. Manning , Eneko Agirre , Aitor Soroa and Ixa Taldea , 2009 . WikiWalk : Random Walks on Wikipedia for Semantic Relatedness , in Proceedings of the 2009 Workshop on Graph - based Methods for Natural Language Processing , ACL - IJCNLP 2009 , pages 41 - 49 , Suntec , Singapore , 7 August 2009 . Jonathan Yu , James A. Thom and Audrey Tam , 2007 . Ontology Evaluation Using Wikipedia Categories for Browsing , in Proceedings of the 16th ACM Conference on Information and Knowledge Management , CIKM07 , Lisbon , Portugal , November 68 , 2007 , pp . 223232 . Hugo Zaragoza , Henning Rode , Peter Mika , Jordi Atserias , Massimiliano Ciaramita & Giuseppe Attardi , 2007 . Ranking Very Many Typed Entities on Wikipedia , in CIKM 07 : Proceedings of the Sixteenth ACM International Conference on Information and Knowledge Management . Torsten Zesch , Iryna Gurevych , Max M\\u00fchlh\\u00e4user , 2007a . Comparing Wikipedia and German WordNet by Evaluating Semantic Relatedness on Multiple Datasets , in Proceedings of Human Language Technologies : The Annual Conference of the North American Chapter of the Association for Computational Linguistics , NAACL - HLT07 , pp . 205208 . Ziqi Zhang and Jos\\u00e9 Iria , 2009 . A Novel Approach to Automatic Gazetteer Generation using Wikipedia , in Proceedings of the 2009 Workshop on the People 's Web Meets NLP , ACL - IJCNLP 2009 , pages 1 - 9 , Suntec , Singapore , 7 August 2009 . \"}",
        "_version_":1692670803070418944,
        "score":30.519888},
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":29.051907},
      {
        "id":"961622a0-ed96-497a-a087-32d2b5ce3ddb",
        "_src_":"{\"url\": \"http://battellemedia.com/archives/2003/11/ask_searches_for_whats_next_in_search.php\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163438.83/warc/CC-MAIN-20160205193923-00085-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Browse Contents . Search the University website . An intelligent question : answering system for natural language . Abstract . As applications of information storage and retrieval systems are becoming more widespread , there is an increased need to be able to communicate with these systems in a natural way . Natural Language applications in the 1990s , as well as in the foreseeable future , have more demanding requirements . Current Natural Language Processing approaches alone have proven to be insufficient as they lack to obtain linguistic understanding . A more suitable approach would be to adopt Computational Linguistics theories , such as the Lexical - Functional Grammar ( LFG ) theory complemented with Artificial Intelligence representation and processing techniques . A prototype Question - Answering System has been developed . It takes Natural Language parsed interrogatives , produces the Functional and Semantic structures according to the LFG representation . It compares the functional behaviour of verbs and their linguistic associations in a given query with a general Object Model in that specific domain . It will then attempt to deduce more information from the given processed text and represent it for possible queries . The structural rules of the LFG and the deduced common - sense domain specific information resolve most of the common ambiguities found in Natural Languages and enhance the understanding ability of the proposed prototype . \"}",
        "_version_":1692669216871677952,
        "score":28.711916},
      {
        "id":"9e40f1f0-4308-4ccb-9b33-f7513a6a6abe",
        "_src_":"{\"url\": \"http://www.shareable.net/blog/differentiating-the-sharing-economy-from-the-anarchy-economy\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168065.93/warc/CC-MAIN-20160205193928-00322-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Affiliated with . Abstract . Background . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system . It significantly advances the state of the art in information extraction by leveraging knowledge in ontological resources , integrating diverse text processing applications , and using an expanded pattern language that allows the mixing of syntactic and semantic elements and variable ordering . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Evaluations were performed on each system , resulting in F - scores ranging from .26 - .72 ( precision .39 - .85 , recall .16 - .85 ) . Additionally , each of these systems was run over all abstracts in MEDLINE , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . Conclusion . OpenDMAP advances the performance standards for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . The output of the information extraction system is always constructed from elements of an ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . The results of these efforts can be used to increase the efficiency of manual curation efforts and to provide additional features in systems that integrate multiple sources for information extraction . Electronic supplementary material . The online version of this article ( doi : 10 . 1186/\\u200b1471 - 2105 - 9 - 78 ) contains supplementary material , which is available to authorized users . Background . Conceptual analysis is the process of mapping from natural language texts to a formal representation of the objects and predicates ( together , the concepts ) meant by the text . The history of attempts to build programs to do conceptual analysis dates back to at least 1967 [ 1 ] . Recent advances in the availability of high quality ontologies , in the ability to accurately recognize named entities in texts , and in language processing methods generally have made possible a significant advance in concept analysis , arguably the most difficult and general natural language processing task . Here we report on the design , implementation and several evaluations of OpenDMAP , an ontology - driven , integrated concept analysis system that significantly advances the state of the art . We also discuss its application to three important information extraction tasks in molecular biology . Information extraction ( IE ) efforts are widely acknowledged to be important in harnessing the rapid advance of biomedical knowledge , particularly in areas where important factual information is published in a diverse literature . In a recent PLoS Biology essay Rebholz - Schuhmann [ 2 ] argued , \\\" It is only a matter of time and effort before we are able to extract facts [ from articles in the primary literature ] automatically . The consequences are likely to be profound . \\\" Existing examples include extraction of information about gene - gene interactions [ 3 ] , alternative splicing [ 4 ] , functional analysis of mutations [ 5 ] , phosphorylation sites [ 6 ] , and regulatory sites [ 7 ] . The primary significance of OpenDMAP to these efforts is that it leverages the large - scale efforts being made in biomedical ontology development , such as the Open Biomedical Ontologies Foundry ( OBO Foundry ) [ 8 ] . Logical representations of reality , such as those built on the OBO Foundry , use a set of predicates that formally describe properties of , or relationships among , objects . Predicates are defined with a specific number and type of admissible arguments . For example , the predicate expresses might be specified to take two arguments , a gene and a cell type , meaning that the specified gene is expressed in all normal cells of the specified type . Such predicates can also be related to each other through abstraction ( \\\" is a \\\" ) and packaging ( \\\" part of \\\" ) hierarchies , as done in the OBO Foundry . The semantics defined by the predicates and hierarchies in such ontologies provide a powerful tool for natural language processing . Independently constructed ontologies have played at best a modest role in prior natural language processing systems . Other language processing systems have used either small , ad hoc conceptual representations developed specifically for the application , or structured linguistic resources , such as WordNet [ 11 ] , which do not meet the logical requirements for an ontology . While the implementation reported below exploits only a small portion of the OBO Foundry , and the crucial Relationship Ontology component of the Foundry is still in an early stage of development , the organizing principles of OpenDMAP generalize straightforwardly . These systems and their extensions have been used to extract semantic relationships relevant to pharmacogenomics [ 16 ] and to compare alternative sources of information [ 17 ] , among other applications . OpenDMAP is like MetaMap and its descendents in that it can only produce output drawn from a predefined semantic representation . The main difference is that MetaMap , SemRep and SemGen are structured as traditional NLP systems , with a lexicon that enumerates possible concepts that might be associated with a word or phrase . Multiple possible mappings are returned , with rankings . OpenDMAP provides an alternative method of organizing knowledge about language , so that each concept has associated with it a set of patterns that describe how that concept can be realized in language ; there is no explicit lexicon . To appreciate the differences between OpenDMAP and previous work in biomedical text mining , it is also useful to contrast its handling of syntactic structure and of semantic content with other systems . At one end of the spectrum are systems that employ essentially asyntactic representations . Early in the modern period of genomic natural language processing , some such systems were able to achieve significant ( and in some cases ground - breaking ) results using techniques based on text literals only . These include [ 18 - 20 ] . One line of subsequent work has attempted to increase the coverage of these early systems , which utilized manually - built patterns , by automatically acquiring considerably larger sets of patterns - see , for example , Huang et al . 2004 [ 21 ] . Another line of subsequent work has focused on adding a modest , but still useful , level of linguistic abstraction by explicitly including either lexical categories ( parts of speech ) , word stems , or both [ 22 , 23 ] . These systems were essentially agrammatical ; in contrast , OpenDMAP utilizes a classic form of \\\" semantic grammar , \\\" freely mixing text literals , semantically typed basal syntactic constituents , and semantically defined classes of entities . Although OpenDMAP is capable of utilizing full syntactic parses , the patterns for the three separate tasks discussed in this paper utilize primarily shallow syntactic parses ( the development phase of the transport project reports results using syntactic dependency information ) . It remains to be seen what depth of syntactic parsing is useful in biomedical text mining . All of the systems discussed thus far have in common the fact that they employ some notion of explicit patterns , be they agrammatical , syntactic , or semantic . In a separate line of work , patterns are entirely implicit - that is , they exist only to the extent that they are captured by orthogonal features . This work approaches relation extraction as a classification problem ; a classic example is the work of Craven and Kumlein 1999 [ 32 ] . Bunescu et al . 2005 [ 33 ] presents a detailed analysis of a number of classification - based approaches ; the state of the art is characterized by the participants in the recent BioCreative protein - protein interaction shared task [ 34 ] . OpenDMAP has been applied in three domains : protein transport , protein - protein interaction and the expression of a gene in a particular cell type . The three application domains are independently significant . Protein transport , the directed movement of proteins from one cellular compartment to another , is a broadly important biological phenomenon . Although protein subcellular localization information is centralized ( e.g. through ontological annotations at NCBI and in various model organism databases ) , information about transport is not . Protein transport information is published throughout the scientific literature , but no previous method was able to capture it systematically . Protein - protein interaction extraction has been the subject of dozens of systems ( see , e.g. a review in [ 35 ] ) . Widely used web resources such as IHOP [ 3 ] and Chilibot [ 36 ] are based entirely on automated extraction of protein - protein interactions from text . This task was used in the BioCreative community evaluation , described below . The protein transport task is illustrative of another distinguishing aspect of the OpenDMAP approach : it provides mechanisms for handling relationships involving more than two entities . Note that the protein transport predicate has at least three arguments : what protein is transported , from where , and to where ( our model also includes a fourth argument : the transporting protein ) . Although some linguistic expressions of the concept may elide an argument , the predicate itself inherently describes a greater than binary relationship . Wattarujeekrit et al . [ 38 ] and Cohen and Hunter [ 39 ] present evidence that many important predicates in biomedicine require more than two arguments . However , most previous efforts at extracting relationships from biomedical text have addressed exclusively binary relationships . Geneways [ 40 ] and RLMPS - P [ 41 ] are the only other biomedical IE systems of which we are aware that extracted greater than binary relationships , and neither is ontology - driven . Assessing the accuracy of an information extraction system is a very labor - intensive activity . In order to identify information that could have been extracted , but was not ( a \\\" false negative \\\" ) , a person must go through a large volume of text to determine all of the relevant assertions . To estimate the reliability of these manually derived assertions , at least two people must complete that task to assess inter - rater reliability . Once such data is used for one evaluation and system developers have seen it , further use of the data will generate upwardly biased accuracy estimates as system developers fit their systems to it . For these reasons , large - scale community evaluations of information extraction systems are particularly important . The second Critical Assessment of Information Extraction in Biology , ( BioCreative ) [ 34 , 42 ] , community evaluation included a test of systems designed to extract human protein - protein interaction information from the full texts of hundreds of journal articles , called the IPS task . Human curators from the IntAct database [ 43 ] manually extracted interaction assertions from these articles using the same curatorial standards as for the database . The results produced by human experts were compared to the results submitted from 45 systems developed by laboratories around the world , providing the best current assessment of the accuracy of protein interaction information extraction systems . The performance of OpenDMAP on the protein interaction task was evaluated as part of this shared task . More limited evaluations of the accuracy in the other applications are also reported in the results section . The accuracy of an information extraction system depends on the genre of texts on which it operates [ 44 ] . This report demonstrates the application of OpenDMAP to full texts of scientific journal articles , to Medline abstracts , and to GeneRIFs ( single sentences or sentence fragments that are selected by human curators for relevance to the function of a particular gene product ) . GeneRIFs are particularly attractive targets for information extraction , due to their roughly sentential length ( identified by [ 44 ] as the optimum ) , breadth of coverage , manual preselection for relevance , and association with at least one normalized gene reference . Despite these attractive features , this is the first report of an information extraction system targeting them . Results . OpenDMAP information extraction systems were produced for extracting protein transport assertions ( transport ) , protein - protein interaction assertions ( interaction ) and assertions that a gene is expressed in a cell type ( expression ) . Each of these systems was run over all abstracts in Medline as of June 18 , 2007 , producing a total of 72,460 transport instances , 265,795 interaction instances and 176,153 expression instances . These results are provided in RDF format in the Additional Files 1 , 2 , 3 , 4 . One particularly striking result is the diversity of journals from which these assertions were mined . The transport relationships were extracted from 2,340 different journals ; the interaction relationships from 4,103 different journals ; and the expression relationships from 2,984 different journals . A total of 4,434 unique journals contributed to these results , nearly 40 % of the journals indexed in Medline each year ( see Figure 1 ) . OpenDMAP coverage of MEDLINE . The gray bars indicate the number of journals indexed by MEDLINE each year . The red bars indicate the number of journal abstracts from which OpenDMAP extracted at least one assertion regarding transport , interaction or expression . In recent years , more than 40 % of biomedical journals contain such information . 2007 is partial data ( through July 1 ) . For the BioCreative evaluation , the interaction system was run on the full texts of all of the 359 articles in the test set , producing 385 interaction assertions . Performance was averaged per article , since a few articles had a very large number of interactions and would have dominated a per assertion calculation . OpenDMAP 's average F - measure of 0.29 was 10 % higher than the next best scoring system , and more than three standard deviations above the mean performance . OpenDMAP 's recall was similar to the other high scoring systems ; its advantage arose from being substantially more precise ( fewer false positives ) , achieving an average precision of 0.39 , more than 20 % better than the next best system . Due to IntAct 's curation criteria , which require clear experimental evidence for an interaction in the text , these results are quite conservative . Many \\\" false positives \\\" were in fact assertions of interactions , but fell short of the evidential requirements for IntAct curation . A manual evaluation of the performance of the protein transport recognition system was based on all 570 GeneRIFs containing a form of the word \\\" translocate \\\" ( 382 of which were about protein transport , and 188 were about the transport of something else ) . Since transport is a greater than binary relationship , the extraction was only counted as correct if all of the components extracted matched the human annotation . For that strict criterion , OpenDMAP achieved precision of 0.75 and a recall of 0.49 ( F - score of 0.59 ) . If incomplete extractions are counted as correct , precision is unchanged at 0.75 and recall rises to 0.67 ( F - score of 0.71 ) . A substantial proportion of the errors were due to imperfect recognition of proteins ; if OpenDMAP is given correct protein identifications as inputs , precision is 0.77 , strict recall is 0.67 ( F - score of 0.72 ) and incomplete recall is 0.85 ( F - score of 0.81 ) . A manual evaluation of the performance of the expression recognition system was based on 324 GeneRIFs containing a form of the word \\\" express , \\\" ( these sentences contained 469 assertions about expression , 205 of which were about gene expression in 178 different cell types ) . Open DMAP had a precision of 0.64 , but missed many statements that annotators identified as expression assertions , achieving a recall of only 0.16 ( F - score of 0.26 ) . A substantial portion of these errors were due to imperfect recognition of gene names ; if OpenDMAP is given correct gene identifications as input , precision is 0.85 and recall is 0.36 ( F - score of 0.51 ) . Many other failures to identify expression assertions were related to coordination ; the test set had an average of more than two expression assertions per sentence , but the IE system extracted only about 1.3 assertions per sentence . Discussion . As demonstrated by its performance in the community evaluation , OpenDMAP advances the state of the art for extracting protein - protein interaction predications from the full texts of biomedical research articles . Furthermore , this level of performance appears to generalize to other information extraction tasks , including extracting information about predicates of more than two arguments . There are several reasons why OpenDMAP exhibits better performance than any other biomedical information extraction system to date . OpenDMAP is an extension of the Direct Memory Access Parsing ( DMAP ) paradigm described in [ 45 ] and [ 46 ] . Three innovations distinguish the present work from those prior efforts . First , the ontology component of OpenDMAP is independent of the rest of the system . The knowledge representation component is the well - established , open source Prot\\u00e9g\\u00e9 ontology development system [ 47 , 48 ] , and OpenDMAP concept analyzers can be associated with any ontology compatible with Prot\\u00e9g\\u00e9 , for example , the OBO Foundry . Second , OpenDMAP is fully integrated with the open source Unstructured Information Management Architecture , ( UIMA ) [ 49 - 51 ] , which allows the results of any text processing application interfaced to UIMA to be exploited by the OpenDMAP system . As demonstrated below , this mechanism facilitates the use of many external language processing systems , including tokenizers , sentence boundary detectors , entity recognition systems , and syntactic parsers . Since the inputs and outputs of each system are mapped by UIMA to a common annotation structure accessed by OpenDMAP , the use , comparison and combination of various approaches to language processing can all be fully integrated into OpenDMAP patterns . The third innovation in the OpenDMAP system is an expanded pattern language for specifying how concepts can be expressed in text . The intimate connection between the ontology and the natural language processing system provides two significant advantages over prior information extraction systems generally . First , the output of the information extraction system is always constructed from elements of the ontology , ensuring that the knowledge representation is grounded with respect to a carefully constructed model of reality . In contrast , the outputs of most natural language processing systems are grounded only in substrings of text , not normalized to any model at all . Progress in normalizing biological entities recognized in text to specific database identifiers [ 52 - 54 ] has made the output of text processing systems much more valuable . Mapping the properties and relationships extracted to a community ontology similarly provides a significant increment in the value of the output from text processing systems . The second advantage of the OpenDMAP approach is that all of the knowledge used by the system to recognize concepts is structured by the ontology . In contrast , the nearly universal alternative approach is to embody knowledge of language into a lexicon , which associates individual lexical items with their possible semantic interpretations . In the OpenDMAP approach , information about which concepts are potentially relevant to the analysis of a particular text passage straightforwardly places limits on the linguistic knowledge relevant to analyzing that passage . This approach finesses many difficult ambiguity resolution problems faced by lexicon - driven systems , since these limits on the knowledge applied to conceptual analysis prevent many multiple interpretation problems from arising at all . For example , the string \\\" hunk \\\" refers to a cell type ( human natural killer cells ) , a gene ( hormonally upregulated Neu - associated kinase ) , and the general English word meaning a large piece of something without definite shape . A traditional , lexicon - driven system would have an explicit method for assigning the correct word sense to any occurrence of the string \\\" hunk . \\\" The fact that there might be possible alternative interpretations of the matching string has no consequence , and no explicit ambiguity resolution step is necessary . Ambiguity is a leading cause of errors in text processing systems , and this approach is one of the contributing factors to OpenDMAP 's superior performance . Our top - down approach to restricting possible interpretations does not address all problems due to ambiguity in language ; for example , errors in preprocessing systems ( e.g. syntactic parsing , see below ) are not effected . The use of UIMA greatly facilitates the incorporation of various applications as input to OpenDMAP . The outputs of NLP tools integrated into the system are described by the extensible UIMA type system . In the case that a new type of information is produced by a preprocessor , OpenDMAP patterns would have to be modified to take advantage of the new type of information available . For example , the first time an external cell type tagging system is added , the UIMA type of the result of that processor must be linked to a cell - type concept in an OpenDMAP ontology in order for it to be used in patterns . However , if a new NLP tool produces a UIMA output type that has been used by OpenDMAP previously , then no changes in the ontology or patterns are needed . We believe that the outputs of information extraction systems are not likely to useful until the F - score ( or at least the precision ) is greater than about 0.85 [ 34 ] , so the various sources of error in these systems must be addressed . A significant cause of errors in the OpenDMAP system as evaluated is incorrect identification of gene and protein names . The UIMA architecture makes it trivial to adopt and exploit better gene / protein recognition systems as they are developed . Use of such a system should improve the performance of OpenDMAP . Error analysis of the false positives in the transport data set indicates that more than 80 % are due to errors in the syntactic analysis . For example , in the sentence \\\" Rho protein regulates the tyrosine phosphorylation of FAK through translocation from the nucleus to the membrane , \\\" the subject of the translocation was incorrectly identified as FAK ( rather than Rho ) by the Stanford parser . That parser was developed for general English rather than biomedical text , so using specialized syntactic analysis systems may improve the precision of OpenDMAP . Remaining problems in false positives are due to problematic tokenization , failures to properly resolve anaphoric reference , and , rarely , negation . False negatives are due to gaps in concept recognition patterns , more than half of which arise from a failure to properly handle coordinated clauses and conjunctions . Addressing these issues remains an open area of research . Another issue was that the Stanford parser was too slow to use in the application of the transport system to all of Medline , so it was n't run . OpenDMAP ignores aspects of patterns that require inputs that are n't present , so the patterns that contained syntactic dependencies did not have to be altered . These syntactic constraints are important for accuracy , however . Tested on the gold standard set for the system without the parser precision drops to 0.62 , while strict recall remains largely unchanged , rising to 0.51 . Conclusion . Despite OpenDMAP elevating the state of the art for biomedical information extraction significantly beyond previous levels , error rates remain high . In the most challenging BioCreative task , finding curatable assertions in full text documents , only about 29 % of the relevant assertions were found , and only about 39 % of the extracted assertions were completely correct . Such error rates mean that automatically generated databases can not replace manual curation efforts . However , the evidence is quite clear that manual curation can not keep up with the rate of data generation [ 56 ] . The surprisingly large number of journals that contained information relevant to these three IE tasks suggests that the temporal approach taken in [ 56 ] may actually underestimate the severity of the problem . Although the outputs produced by large - scale IE systems are not yet suitable for producing factual databases for direct use by biomedical researchers , the current level of performance provides two important facilities to the research community . First , the results of these efforts can be used to significantly increase the efficiency of manual curation efforts . Each extracted assertion is tied to a specific text , which can be used to direct the attention of manual curators both to relevant documents and to specific relevant passages within a document . Effective integration of IE results into curatorial workflows will require the development of new tools . OpenDMAP developers are working with curators at IntAct to address these issues . The open source availability of OpenDMAP will facilitate the work of others addressing this issue as well . The second important use of the sorts of results that IE systems are currently able to generate is in statistical integration with multiple sources of noisy data , such as those described in [ 57 ] and [ 58 ] . As demonstrated in the latter , the proper addition of even noisy data from the literature substantially improves the quality and coverage of protein - protein interaction networks for several species . Methods . OpenDMAP uses Prot\\u00e9g\\u00e9 [ 47 ] to provide an object model for the possible concepts ( predicates and objects ) that might be found in a text . Prot\\u00e9g\\u00e9 models concepts ( including actions ) as classes that participate in abstraction and packaging hierarchies , and relationships as class - specific slots . For example , protein transport is modeled as a class ( called PROTEIN - TRANSPORT ) and the relationship between a transport event and the protein transported in that event is represented as a slot in that class ( called [ TRANSPORTED - ENTITY ] ) . Slots can take on values , which can be constrained to be instances of other classes . For example , the [ TRANSPORTED - ENTITY ] slot of the PROTEIN - TRANSPORT class is constrained to be an instance of either of the classes PROTEIN or MOLECULAR - COMPLEX . Figure 2 shows a portion of the model used for the transport , which includes biological entities , such as molecular complexes and cellular components , and biological processes , particularly protein transport . This model is drawn almost entirely from the Gene Ontology ( GO ) [ 53 ] although the relationships that define the four slots shown in Figure 2 are from a provisional submission to the OBO Foundry Relationship Ontology and are not official . Preprocessing tools ( ABNER [ 55 ] and LingPipe [ 59 ] ) were applied to tag instances of proteins , genes , and cell types . Screenshot of the Prot\\u00e9g\\u00e9 ontology for the protein transport task . The slots of the protein transport class are shown in the lower right panel of this screen shot . Note that the subclasses of Cellular Component and Protein Transport are not shown . For the transport task , patterns were produced for 30 ontology concepts ; eight directly related to transport and 22 others for cellular components that are the sources and destinations of transport . A large number of other concepts ( e.g. genes , proteins and cell types ) do not have explicit patterns associated with them , but are instead tagged as such by UIMA tools during preprocessing . The protein - protein interaction task involved producing patterns for nine concepts , and the cell expression task required patterns for six additional concepts . The UIMA architecture [ 49 ] manages the processing of document sets . Various combinations of these tools were used in the different applications . For example , the GeneRIFs used in the transport application did not require sentence segmentation , and the applications to all medline abstracts did not use syntactic elements in patterns because the Stanford Parser was too slow to run over all of Medline . The results of this preprocessing are stored in UIMA 's common annotation structure . In order to be able to recognize a concept in text , OpenDMAP associates one or more patterns with each concept . A pattern describes the words , phrases , parts of speech , syntactic structures or concepts that should cause an instance of the associated concept to be recognized . A simple pattern , such as the one shown in equation 1 , enumerates a disjunction of words that should trigger recognition of a concept . The patterns for all of the CELLULAR - COMPONENT concepts were derived from the GO term names and synonyms , supplemented with derivational variants , such as the adjectival \\\" nuclear \\\" in equation 1 . Twenty - two GO cellular component terms were used , along with 19 synonyms associated with the GO terms , and 78 additional derivational variants generated by inspection of the training corpus . More complex patterns can include references to non - terminals , particularly other concepts . Equation 2 is one of the patterns for recognizing instances of the PROTEIN - TRANSPORT concept . When a pattern that includes a slot name is matched , the instance created has its slots filled with the concepts that matched the slot names in the pattern . For example , the above pattern matches the GeneRIF that contains \\\" ... Bax translocation to mitochondia ... \\\" ( from Entrez GeneID 27113 ) . Since the entire pattern matches , an instance of PROTEIN - TRANSPORT is created , with the Bax protein concept in its [ TRANSPORTED - ENTITY ] slot and an instance of the mitochondria concept ( from GO 's cellular component hierarchy ) in its [ TRANSPORT - DESTINATION ] slot . OpenDMAP patterns can express variability in word and phrase order . Note , for example , that equation 2 would fail to match the phrase \\\" Bax translocation to mitochondria from the cytosol . \\\" The special pattern marker @ is used to identify a set of subpatterns that are both optional and can occur before or after a required phrase ; multiple @ marked phrases can occur in any order . For example , equation 2 can be modified with this marker to recognize the above text : . Many sentences in the literature express multiple concepts , making extraction of even simple assertions problematic . Consider the following GeneRIF from GeneID:29560 : \\\" ... HIF-1alpha which is present in glomus cells translocates to the nucleus .... \\\" The intervening phrase \\\" which is present in glomus cells \\\" prevents the pattern in equation 3 from matching that sentence . OpenDMAP does have a wildcard character ( underscore ) that could be added to the pattern in equation 3 , between the [ TRANSPORTED - ENTITY ] concept and the word \\\" translocation , \\\" allowing this sentence to be matched . However , using such a wild card would make any protein mentioned before the word \\\" translocation \\\" match the pattern , which is too promiscuous . To address this problem , OpenDMAP allows patterns to specify syntactic constraints on potential matches . Furthermore , the reliance on the exact word \\\" translocation \\\" can be relaxed to be any reference to a transport action word , including both verbal and nominal forms of multiple terms ( e.g. , transported , translocation ) . The PROTEIN - TRANSPORT class is extended to have an [ action ] slot that specifies the type of transportation action , to keep track of the term that was used . Equation 4 demonstrates the pattern language for specifying syntactic constraints : . The use of the variable \\\" x \\\" in the specification identifies a specific syntactic unit , linking the dependency to the head of a phrase . Multiple variables can be used to specify constraints on different syntactic units within a sentence . OpenDMAP patterns are very powerful . Only five such patterns , shown in equations 5 - 9 were required for the transport extraction system performance noted above . These patterns were devised manually , based on expert knowledge of the domain and on a small training set of sample GeneRIFs . The test data used in the transport and expression evaluations were marked up by domain experts trained in conceptual annotation , using the Knowtator annotation tool [ 62 ] . Availability of data and software . The results of the information extraction effort are available as RDF format files in the Additional Files 1 , 2 , 3 , 4 . Declarations . Acknowledgements . This work was funded by NIH grants R01NLM008111 and R01NLM009254 to LH . ZL was also supported in part by the Intramural Research Program of the NIH , NLM . Electronic supplementary material . 1471 - 2105 - 9 - 78-S1.GZ Additional file 1 : Transport instances from MEDLINE . This file contains the RDF formatted instances of transport , mined from MEDLINE with OpenDMAP . ( GZ 2785 kb ) . 1471 - 2105 - 9 - 78-S2.GZ Additional file 2 : Interaction instances from MEDLINE , part 1 . The interaction data set is very large . This file contains the first half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 6035 kb ) . 1471 - 2105 - 9 - 78-S3.GZ Additional file 3 : Interaction instances from MEDLINE , part 2 . The interaction data set is very large . This file contains the second half of RDF formatted instances of interaction , mined from MEDLINE with OpenDMAP . ( GZ 5994 kb ) . 1471 - 2105 - 9 - 78-S4.GZ Additional file 4 : Expression instances from MEDLINE . This file contains the RDF formatted instances of expression , mined from MEDLINE with OpenDMAP . ( GZ 7154 kb ) . Authors ' contributions . LH conceived of the project , supervised the design and implementation of the system and wrote the manuscript . ZL was responsible for the transport project , including writing the patterns and analyzing the results ; he also contributed suggestions for the interaction task . JRF implemented the OpenDMAP pattern recognition engine and its UIMA wrapper . WAB wrote all other infrastructure software , including the other UIMA wrappers , managed the data , applied OpenDMAP to all of MEDLINE , and designed and built other software . HLJ was responsible for the interaction and expression projects , including writing the patterns , analyzing the results , and doing the associated error analyses . PVO managed the creation of the gold standard data for transport and contributed to the design of the pattern language syntax . KBC managed the team , selected the preprocessing tools , coordinated and supervised the interaction and expression task efforts , and provided linguistic and software design contributions during all phases of the project . All authors have read and approved this manuscript . Authors ' Affiliations . Center for Computational Pharmacology , University of Colorado School of Medicine . National Center for Biotechnology Information , National Library of Medicine . PowerSet , Inc. . Department of Computer Science , University of Colorado . References . Sparck Jones K : Natural language processing : A historical review . Current Issues in Computational Linguistics : in Honour of Don Walker ( Ed Zampolli , Calzolari and Palmer ) , Amsterdam : Kluwer 1994 . Rebholz - Schuhmann D , Kirsch H , Couto F : Facts from text -- is text mining ready to deliver ? PLoS Biol 2005 , 3 ( 2 ) : e65 . View Article PubMed . Hoffmann R , Valencia A : A gene network for navigating the literature . Nat Genet 2004/07/01 Edition 2004 , 36 ( 7 ) : 664 . View Article PubMed . Shah PK , Jensen LJ , Bou\\u00e9 S , Bork P : Extraction of transcript diversity from scientific literature . PLoS Comput Biol 2005 , 1 ( 1 ) : e10 . View Article PubMed . Horn F , Lau AL , Cohen FE : Automated extraction of mutation data from the literature : application of MuteXt to G protein - coupled receptors and nuclear hormone receptors . Bioinformatics 2004 , 20 ( 4 ) : 557 - 568 . View Article PubMed . Hu ZZ , Narayanaswamy M , Ravikumar KE , Vijay - Shanker K , Wu CH : Literature mining and database annotation of protein phosphorylation using a rule - based system . Bioinformatics 2005/04/09 Edition 2005 , 21 ( 11 ) : 2759 - 2765 . View Article PubMed . Saric J , Jensen LJ , Ouzounova R , Rojas I , Bork P : Extraction of regulatory gene / protein networks from Medline . Bioinformatics 2005/07/28 Edition 2006 , 22 ( 6 ) : 645 - 650 . View Article PubMed . Guarino N : Formal ontology in information systems . Trento , Italy , IOS Press 1998 , 3 - 15 . Hersh W , Bhupatiraju R , Ross L , Johnson P , Cohen A , Kraemer D : TREC 2004 Genomics track overview . National Institute of Standards and Technology 2004 . Fellbaum C : WordNet : An Electronic Lexical Database ( Language , Speech , and Communication ) . MIT Press 1998 . Aronson A : Effective Mapping of Biomedical Text to the UMLS Metathesaurus : The MetaMap Program . AMIA Annu Symp Proc 2001 , 17 - 21 . Rindflesch TC , Fiszman M : The interaction of domain knowledge and linguistic structure in natural language processing : interpreting hypernymic propositions in biomedical text . J Biomed Inform 2003 , 36 ( 6 ) : 462 - 477 . View Article PubMed . Rindflesch TC , Libbus B , Hristovski D , Aronson AR , Kilicoglu H : Semantic relations asserting the etiology of genetic diseases . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 554 - 558 . Masseroli M , Kilicoglu H , Lang FM , Rindflesch TC : Argument - predicate distance as a filter for enhancing precision in extracting predications on the genetic etiology of disease . BMC Bioinformatics 2006/06/10 Edition 2006 , 7 : 291 . View Article PubMed . Ahlers CB , Fiszman M , Demner - Fushman D , Lang FM , Rindflesch TC : Extracting semantic predications from Medline citations for pharmacogenomics . Pac Symp Biocomput 2007/11/10 Edition 2007 , 209 - 220 . Libbus B , Kilicoglu H , Rindflesch TC , Mork JG , Aronson AR , Hirschman L , Pustejovsky J : Using Natural Language Processing , LocusLink and the Gene Ontology to Compare OMIM to MEDLINE . HLT - NAACL 2004 Workshop : BioLINK 2004 , Linking Biological Literature , Ontologies and Databases 2004 , 69 - 76 . Blaschke C , Andrade MA , Ouzounis C , Valencia A : Automatic extraction of biological information from scientific text : protein - protein interactions . Proc Int Conf Intell Syst Mol Biol 2000/04/29 Edition 1999 , 60 - 67 . Blaschke C , Oliveros JC , Valencia A : Mining functional information associated with expression arrays . Funct Integr Genomics 2002/01/17 Edition 2001 , 1 ( 4 ) : 256 - 268 . View Article PubMed . Blaschke C , Valencia A : Can bibliographic pointers for known biological data be found automatically ? Protein interactions as a case study . Comparative and Functional Genomics 2001 , 2 ( 4 ) : 196 - 206 . View Article PubMed . Huang M , Zhu X , Hao Y , Payan DG , Qu K , Li M : Discovering patterns to extract protein - protein interactions from biomedical full texts . Proc JNLPBA , COLING 2004 , 22 - 28 . Temkin JM , Gilder MR : Extraction of protein interaction information from unstructured text using a context - free grammar . Bioinformatics Oxford Univ Press 2003 , 19 ( 16 ) : 2046 - 2053 . Corney DP , Buxton BF , Langdon WB , Jones DT : BioRAT : extracting biological information from full - length papers . Bioinformatics 2004/07/03 Edition 2004 , 20 ( 17 ) : 3206 - 3213 . View Article PubMed . Park JC , Kim HS , Kim JJ : Bidirectional incremental parsing for automatic pathway identification with combinatory categorial grammar . Pac Symp Biocomput 2001 , 396 - 407 . Yakushiji A , Tateisi Y , Miyao Y , Tsujii J : Event extraction from biomedical papers using a full parser . Pac Symp Biocomput 2001 , 408 - 419 . Gaizauskas R , Demetriou G , Artymiuk PJ , Willett P : Protein structures and information extraction from biological texts : the PASTA system . Bioinformatics 2002/12/25 Edition 2003 , 19 ( 1 ) : 135 - 143 . View Article PubMed . Leroy G , Chen H , Martinez JD : A shallow parser based on closed - class words to capture relations in biomedical text . J Biomed Inform 2003 , 36 ( 3 ) : 145 - 158 . View Article PubMed . Koike A , Niwa Y , Takagi T : Automatic extraction of gene / protein biological functions from biomedical text . Bioinformatics 2004/10/29 Edition 2005 , 21 ( 7 ) : 1227 - 1236 . View Article PubMed . Rinaldi F , Schneider G , Kaljurand K , Hess M , Romacker M : An environment for relation mining over richly annotated corpora : the case of GENIA . BMC Bioinformatics 2006 , 7 Suppl 3 : S3 . View Article PubMed . McInnes BT , Pedersen T , Pakhomov SV : Determining the Syntactic Structure of Medical Terms in Clinical Notes . Proc Assoc Comp Ling 2007 . Pyysalo S , Ginter F , Haverinen K , Heimonen J , Salakoski T , Laippala V : On the unification of syntactic annotations under the Stanford dependency scheme : A case study on BioInfer and GENIA . Association for Computational Linguistics 2007 . Craven M , Kumlien J : Constructing biological knowledge bases by extracting information from text sources . Proc Int Conf Intell Syst Mol Biol 1999 , 77 - 86 . Bunescu R , Ge R , Kate RJ , Marcotte EM , Mooney RJ , Ramani AK , Wong YW : Comparative experiments on learning information extractors for proteins and their interactions . Artif Intell Med 2005/04/07 Edition 2005 , 33 ( 2 ) : 139 - 155 . View Article PubMed . Krallinger M , Leitner F , Valencia A : Assessment of the second BioCreative PPI task : automatic extraction of protein - protein interactions . Hunter L , Cohen KB : Biomedical Language Processing : What 's Beyond PubMed ? Molecular Cell Cell 2006 , 21 : 589 - 594 . Chen H , Sharp BM : Content - rich biological network constructed by mining PubMed abstracts . BMC Bioinformatics 2004 , 5 : 147 . View Article PubMed . Wattarujeekrit T , Shah PK , Collier N : PASBio : predicate - argument structures for event extraction in molecular biology . BMC Bioinformatics 2004 , 5 : 155 - 175 . View Article PubMed . Cohen KB , Hunter L : A critical review of PASBio 's argument structures for biomedical verbs . BMC Bioinformatics 2006 , 7 ( Suppl . 3 ) : S5 . View Article PubMed . J Biomed Inform 2004/03/16 Edition 2004 , 37 ( 1 ) : 43 - 53 . View Article PubMed . Narayanaswamy M , Ravikumar KE , Vijay - Shanker K : Beyond the clause : extraction of phosphorylation information from medline abstracts . Bioinformatics 2005 . , 21 Suppl 1 : . Nucleic Acids Res 2003/12/19 Edition 2004 , 32 ( Database issue ) : D452 - 5 . View Article PubMed . Ding J , Berleant D , Nettleton D , Wurtele E : Mining MEDLINE : Abstracts , Sentences , or Phrases ? Pac Symp Biocomput 2002 , 7 : 326 - 337 . Martin C : Direct Memory Access Parsing . Yale University 1992 . Fitzgerald W : Building Embedded Conceptual Parsers . Northwestern University 1994 . Noy NF , Crubezy M , Fergerson RW , Knublauch H , Tu SW , Vendetti J , Musen MA : Protege-2000 : an open - source ontology - development and knowledge - acquisition environment . AMIA Annu Symp Proc 2004/01/20 Edition 2003 , 953 . Hirschman L , Colosimo M , Morgan A , Yeh A : Overview of BioCreAtIvE task 1B : normalized gene lists . BMC Bioinformatics 2005/06/18 Edition 2005 , 6 Suppl 1 : S11 . View Article PubMed . The Gene Ontology Consortium . Nat Genet 2000/05/10 Edition 2000 , 25 ( 1 ) : 25 - 29 . View Article PubMed . Genome Biology 2008 . Settles B : ABNER : an open source tool for automatically tagging genes , proteins and other entity names in text . Bioinformatics 2005 , 21 ( 14 ) : 3191 - 3192 . View Article PubMed . Baumgartner WA Jr. , Cohen KB , Fox LM , Acquaah - Mensah G , Hunter L : Manual curation is not sufficient for annotation of genomic databases . Bioinformatics 2007 , 23 ( 14 ) : e. . Leach SM , Gabow AP , Hunter L , Goldberg D : Assessing and combining reliability of protein interaction sources . Pac Symp Biocomp 2007 , 12 : 433 - -444 . View Article . Gabow AP , Leach SM , Baumgartner WA Jr. , Hunter L , Goldberg D : Improving Protein Function Prediction Methods with Integrated Literature Data . Carpenter B : Phrasal queries with LingPipe and Lucene : ad hoc genomics text retrieval . 13th Annual Text Retrieval Conference 2004 . Klein D , Manning CD : Fast Exact Inference with a Factored Model for Natural Language Parsing . Advances in Neural Information Processing Systems MIT Press 2003 . , 15 : . Kehler A , Appelt D , Taylor L , Simma A : The ( non ) utility of predicate - argument frequencies for pronoun interpretation . Proc of HLT - NAACL 2004 , 4 : 289 - 296 . Ogren P : Knowtator : a Protege plugin for annotated copus construction . HLT - NAACL 2006 , 273 . Copyright . \\u00a9 Hunter et al . 2008 . This article is published under license to BioMed Central Ltd. \"}",
        "_version_":1692580791959158784,
        "score":28.580257},
      {
        "id":"25cee640-ac85-48bc-a7e9-7f941fa9a37e",
        "_src_":"{\"url\": \"http://www.hcpl.net/content/case-missing-servant\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148558.5/warc/CC-MAIN-20160205193908-00177-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Semantic Web tool sets span from comprehensive engineering environments to specific converters and editors and the like . The entire workflow extends from getting the initial content , annotating or tagging it according to existing or built ontologies , reconciling heterogeneities , and then storing and managing the RDF or OWL with subsequent querying and inferencing . There are certainly more tools extant , and I made some choices to exclude some marginal tools ( Sourceforge , for example , has more than 200 semantic Web - related projects , but the vast majority appear moribund with no actual software to download ) . Thus , listed below , are today 's current , most comprehensive list of 175 semantic Web software tools and applications . I am now further characterizing these offline as to open source v. proprietary and categorizing according to SW - related workflow . I may later post those expansions . I also welcome tool suggestions . I think the ESW tools listing is the best place ongoing for such a compilation , but so far I am not liking what I am seeing in vendors using hype to characterize their tools versus more dispassionate descriptions by practitioners . ActiveRDF is a library for accessing RDF data from Ruby programs . It can be used as data layer in Ruby - on - Rails . You can address RDF resources , classes , properties , etc . programmatically , without queries . The Aduna Metadata Server automatically extracts metadata from information sources , like a file server , an intranet or public web sites . The Aduna Metadata Server is a powerful and scalable store for metadata . Aperture is a Java framework for extracting and querying full - text content and metadata from various information systems ( e.g. file systems , web sites , mail boxes ) and the file formats ( e.g. documents , images ) occurring in these systems . Corese stands for Conceptual Resource Search Engine . It is an RDF engine based on Conceptual Graphs ( CG ) and written in Java . It enables the processing of RDF Schema and RDF statements within the CG formalism , provides a rule engine and a query engine accepting the SPARQL syntax . The Closed World Machine ( CWM ) data manipulator , rules processor and query system mostly using using the Notation 3 textual RDF syntax . It also has an incomplete OWL Full and a SPARQL access . It is written in Python . Euler is an inference engine supporting logic based proofs . It is a backward - chaining reasoner enhanced with Euler path detection . It has implementations in Java , C # , Python , Javascript and Prolog . Via N3 it is interoperable with W3C Cwm . FreeLing is an open source language analysis tool suite . GATE is a stable , robust , and scalable open - source infrastructure which allows users to build and customise language processing components , while it handles mundane tasks like data storage , format analysis and data visualisation . Heart of Gold is a middleware for the integration of deep and shallow natural language processing components . It provides a uniform and flexible infrastructure for building applications that use Robust Minimal Recursion Semantics ( RMRS ) and/or general XML standoff annotation produced by NLP components . IF - Map is an Information Flow based ontology mapping method . It is based on the theoretical grounds of logic of distributed systems and provides an automated streamlined process for generating mappings between ontologies of the same domain . Jena is a Java framework to construct Semantic Web Applications . It provides a programmatic environment for RDF , RDFS and OWL , SPARQL and includes a rule - based inference engine . It also has the ability to be used as an RDF database via its Joseki layer . See the jena discussion list for more information . LingPipe is a suite of Java tools designed to perform linguistic analysis on natural language data . LingPipe 's flexibility and included source make it appropriate for research use . Version 1.0 tools include a statistical named - entity detector , a heuristic sentence boundary detector , and a heuristic within - document coreference resolution engine . LinguaStream is an integrated experimentation environment ( IEE ) targeted to researchers in Natural Language Processing . LinguaStream allows processing streams to be assembled visually , picking individual components in a \\\" palette \\\" ( the standard set contains about fifty components , and is easily extensible using a Java API , a macro - component system , and templates ) . Some components are specifically targeted to NLP , while others solve various issues related to document engineering ( especially to XML processing ) . Other components are to be used in order to perform computations on the annotations produced by the analysers , to visualise annotated documents , to generate charts , etc . . Language & Computing 's LinKFactory is an ontology management tool , it provides an effective and user - friendly way to create , maintain and extend extensive multilingual terminology systems and ontologies ( English , Spanish , French , etc . ) . It is designed to build , manage and maintain large , complex , language independent ontologies . Apache Lucene is a high - performance , full - featured text search engine library written entirely in Java . It is a technology suitable for nearly any application that requires full - text search , especially cross - platform . It is open source . Magpie supports the interpretation of web documents through on - the - fly ontologically based enrichment . Semantic services can be invoked either by the user or be automatically triggered by patterns of browsing activity . MnM is an annotation tool which provides both automated and semi - automated support for annotating web pages with semantic contents . MnM integrates a web browser with an ontology editor and provides open APIs to link to ontology servers and for integrating information extraction tool . ODS is a distributed collaborative application platform for creating Semantic Web applications such as : blogs , wikis , feed aggregators , etc . , with built - in SPARQL support and incorporation of shared ontologies such as SIOC , FOAF , and Atom OWL . ODS is an application of OpenLink Virtuoso and is available in Open Source and Commercial Editions . OWLJessKB is a description logic reasoner for OWL . The semantics of the language is implemented using Jess , the Java Expert System Shell . Currently most of the common features of OWL lite , plus some and minus some . The ICS - FORTH RDFSuite open source , high - level scalable tools for the Semantic Web . This suite includes Validating RDF Parser ( VRP ) , a RDF Schema Specific DataBase ( RSSDB ) and supporting RDF Query Language ( RQL ) . The Redland RDF Application Framework is a set of free software libraries that provide support for RDF . It provides parser for RDF / XML , Turtle , N - triples , Atom , RSS ; has a SPARQL and GRDDL implementation , and has language interfaces to C # , Python , Obj - C , Perl , PHP , Ruby , Java and Tcl . ReTAX is an aide to help a taxonomist create a consistent taxonomy and in particular provides suggestions as to where a new entity could be placed in the taxonomy whilst retaining the integrity of the revised taxonomy ( c.f . , problems in ontology modelling ) . The Web Service Modeling Toolkit ( WSMT ) is a collection of tools for use with the Web Service Modeling Ontology ( WSMO ) , the Web Service Modeling Language ( WSML ) and the Web Service Execution Environment ( WSMX ) . YARS ( Yet Another RDF Store ) is a data store for RDF in Java and allows for querying RDF based on a declarative query language , which offers a somewhat higher abstraction layer than the APIs of RDF toolkits such as Jena or Redland . Schema.org Markup . This AI3 blog maintains Sweet Tools , the largest listing of about 800 semantic Web and -related tools available . Most are open source . Click here to see the current listing ! I have been assembling for some time a listing of semantic Web - related software applications and tools . My first partial listing had about 50 sources . I recently [ ... ] . 5 thoughts on \\\" Comprehensive Listing of 175 Semantic Web Tools \\\" . TermExtractor is a FREE and high - performing software package for Terminology Extraction . TermExtractor extracts terminology consensually referred in a specific application domain . The software takes as input a corpus of domain documents , parses the documents , and extracts a list of \\\" syntactically plausible \\\" terms ( e.g. compounds , adjective - nouns , etc . ) . Documents parsing assigns a greater importance to terms with text layouts ( title , bold , italic , underlined , etc . ) . Two entropy - based measures , called Domain Relevance and Domain Consensus , are then used . Domain Consensus is used to select only the terms which are consensually referred throughout the corpus documents . Domain Relevance to select only the terms which are relevant to the domain of interest , Domain Relevance is computed with reference to a set of contrastive terminologies from different domains . Finally , extracted terms are further filtered using Lexical Cohesion , that measures the degree of association of all the words in a terminological string . \"}",
        "_version_":1692669334284926977,
        "score":27.039305},
      {
        "id":"32c9f64f-c981-4437-9424-056c60cde52d",
        "_src_":"{\"url\": \"http://ancienthebrewpoetry.typepad.com/ancient_hebrew_poetry/2009/01/why-i-love-virginia-woolf.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148758.73/warc/CC-MAIN-20160205193908-00166-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"It is the aim of this module to explore some of the aspects and challenges in Human Language Technologies ( HLT ) that are of relevance to Computer Assisted Language Learning ( CALL ) . Starting with a brief outline of some of the early attempts in HLT , using the example of Machine Translation ( MT ) , it will become apparent that experiences and results in this area had a direct bearing on some of the developments in CALL . CALL soon became a multi - disciplinary field of research , development and practice . Some researchers began to develop CALL applications that made use of Human Language Technologies , and a few such applications will be introduced in this module . The advantages and limitations of applying HLT to CALL will be discussed , using the example of parser - based CALL . This brief discussion will form the basis for first hypotheses about the nature of human - computer interaction ( HCI ) in parser - based CALL . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . Piklu Gupta : At this time of writing this module Piklu was a lecturer in German Linguistics at the University of Hull , UK . He is now working for Fraunhofer IPSI . Mathias Schulze : At this time of writing this module Mathias was a lecturer in German at UMIST , now merged with the University of Manchester , UK . He is now working at the University of Waterloo , Canada . His main research interest is in parser - based CALL and linguistics . He is an active member of the NLP SIG within the EUROCALL professional association and ICALL within the CALICO professional association . Graham Davies , ICT4LT Editor , Thames Valley University , UK . Graham has been interested in Machine Translation since 1976 . Human Language Technologies ( HLT ) is a relatively new term that embraces a wide range of areas of research and development in the sphere of what used to be called Language Technologies or Language Engineering . The aim of this module is to familiarise the student with key areas of HLT , including a range of Natural Language Processing ( NLP ) applications . NLP is a general term used to describe the use of computers to process information expressed in natural ( i.e. human ) languages . The term NLP is used in a number of different contexts in this document and is one of the most important branches of HLT . There is a Special Interest Group in Language Processing , NLP SIG , within the EUROCALL professional association , and a Special Interest Group in Intelligent Computer Assisted Language Instruction ( ICALL ) within the CALICO professional association . Both have similar aims , namely to further research in a number of areas that are mentioned in this module , such as : . Artificial Intelligence ( AI ) . Computational Linguistics . Corpus - Driven and Corpus Linguistics . Formal Linguistics . Machine Aided Translation ( MAT ) . Machine Translation ( MT ) . Natural Language Interfaces . Natural Language Processing ( NLP ) . Theoretical Linguistics . All of the above are areas of research that have produced results which have proven , are proving and will prove very useful in the field of Computer Assisted Language Learning . Of course , this module can not teach you everything there is to know about HLT . This is neither necessary nor possible . The two main authors of this module are living proof of that ; they both started off as language teachers and then got interested in HLT . A multilingual CD - ROM titled A world of understanding was produced in 1998 on behalf of the Information Society and Media Directorate General of the European Commission under its former name , DGXIII . The aim of the CD - ROM was to demonstrate the importance of HLT in helping to realise the benefits of the Multilingual Information Society , in particular forming a review and record of the Language Engineering Sector of the Fourth Framework Programme of the European Union ( 1994 - 98 ) . 1.1 Introduction to HLT . [ ... ] there is no doubt that the development of tools ( technology ) depends on language - it is difficult to imagine how any tool - from a chisel to a CAT scanner - could be built without communication , without language . What is less obvious is that the development and the evolution of language - its effectiveness in communicating faster , with more people , and with greater clarity - depends more and more on sophisticated tools . ( European Commission : Language and technology 1996:1 ) . Language and technology lists the following examples of language technology ( using an admittedly broad understanding of the term ) : . photocopier ( p. 10 ) . laser printer ( p. 11 ) . fax machine ( p. 12 ) . desktop publishing ( p. 13 ) . scanner , modem ( p. 15 ) . electronic mail ( p. 16 ) . machine translation ( p. 17 ) . translator 's workbench ( p. 18 ) . tape recorder , database search engines ( p. 19 ) . telephone ( p. 25 ) . Many of these are already being used in language learning and teaching . The field of human language technology covers a broad range of activities with the eventual goal of enabling people to communicate with machines using natural communication skills . Research and development activities include the coding , recognition , interpretation , translation , and generation of language . [ ... ] Advances in human language technology offer the promise of nearly universal access to online information and services . Since almost everyone speaks and understands a language , the development of spoken language systems will allow the average person to interact with computers without special skills or training , using common devices such as the telephone . These systems will combine spoken language understanding and generation to allow people to interact with computers using speech to obtain information on virtually any topic , to conduct business and to communicate with each other more effectively . [ Source : Foreword to ( Cole 1997 ) ] . Facilitating and supporting all aspects of human communication through machines has interested researchers for a number of centuries . The use of mechanical devices to overcome language barriers was proposed first in the seventeenth century . Then , suggestions for numerical codes to be used to mediate between languages were made by Leibnitz , Descartes and others ( v. Hutchins 1986:21 ) . The beginnings of what we describe today as Human Language Technologies are , of course , closely connected to the advent of computers . ( i ) Various games , e.g. chess , noughts and crosses , bridge , poker ( ii ) The learning of languages ( iii ) Translation of languages ( iv ) Cryptography ( v ) Mathematics . Of these ( i ) , ( iv ) , and to a lesser extent ( iii ) and ( v ) are good in that they require little contact with the outside world . For instance in order that the machine should be able to play games its only organs need be ' eyes ' capable of distinguishing the various positions on a specially made board , and means for announcing its own moves . Mathematics should preferably be resticted to branches where diagrams are not much used . Of the above possible fields the learning of languages would be the most impressive , since it is the most human of these activities . This field sees however to depend too much on sense organs and locomotion to be feasible . ( Turing 1948:9 ) . Later on , Machine Translation enjoyed a period of popularity with researchers and funding bodies in the United States and the Soviet Union : . From 1956 onwards , the dollars ( and roubles ) really started to flow . Between 1956 and 1959 , no less than twelve research groups became established at various US universities and private corporations and research centres . Although linguists , language teachers and computer users today may find these predictions ridiculous , it was the enthusiasm and the work during this time that form the basis of many developments in HLT today . Research and development in HLT is nowadays more rapidly transferred into commercial systems than was the case up until the 1980s . Indeed HLT is becoming increasingly pervasive in our everyday lives . Here are some examples : . Machine Translation ( Section 3 ): There are many online translation systems that can be accessed free of charge , causing headaches for teachers whose students thought that they could save themselves time and who were blissfully unaware of the unreliability of their output ( Section 3.2 ) . Speech synthesis ( Section 4.1 ): Satellite navigation ( satnav ) devices for motor vehicles use systems that read out road numbers , street names and directions for the driver , and their output is surprisingly good . Speech recognition ( Section 4.2 ): If you make a telephone call to a customer support service you may hear a telephone recording that asks you to say a word or short phrase so that you can be connected to the appropriate department . Other previously unexpected areas of use are emerging . It is now , for instance , common for mobile phones to have what is known as predictive text input to aid the writing of short text messages . Instead of having to press one of the nine keys a number of times to produce the correct letter in a word , software in the phone compares users ' key presses to a linguistic database to determine the correct ( or most likely ) word . Most Internet search engines also now incorporate some kind of linguistic technology to enable users to enter a query in natural language , for example \\\" What is meant by log - likelihood ratio ? \\\" is as acceptable a query as simply \\\" log - likelihood ratio \\\" . What are the possible benefits for language teaching and learning of using HLT ? Here are some examples : . Teachers might want to preprocess a text to highlight certain grammatical phenomena or patterns . This can easily be done with a word - processor . Teachers might use part - of - speech taggers ( see Section 5 ) which could save them the trouble of having to manually annotate a text . Parsers available either on the Web or for local use on PCs can generate a graphical representation of sentence structure that may be useful for grammatical analysis for more advanced learners . Machine Translation ( MT ) has been the dream of computer scientists since the 1940s . The student 's attention is drawn in particular to the following publications , which provide a very useful introduction to MT : . Hutchins ( 1999 ) \\\" The development and use of machine translation systems and computer - based translation tools \\\" . Paper given at the International Symposium on Machine Translation and Computer Language Information Processing , 26 - 28 June 1999 , Beijing , China . 3.1 Machine Translation : a brief history . Initial work on Machine Translation ( MT ) systems was typified by what we would now consider to be a naive approach to the \\\" problem \\\" of natural language translation . Successful decoding of encrypted messages by machines during World War II led some scientists , most notably Warren Weaver , to view the translation process as essentially analogous with decoding . The concept of Machine Translation in the modern age can be traced back to the 1940s . Warren Weaver , Director of the Natural Sciences Division of the Rockefeller Foundation , wrote to his friend Norbert Wiener on 4 March 1947 - short ly after the first computers and computer programs had been produced : . Recognising fully , even though necessarily vaguely , the semantic difficulties because of multiple meanings , etc . , I have wondered if it were unthinkable to design a computer which would translate . Even if it would translate only scientific material ( where the semantic difficulties are very notably less ) , and even if it did produce an inelegant ( but intelligible ) result , it would seem to me worth while . When I look at an article in Russian , I say \\\" This is really written in English , but it has been coded in some strange symbols . I will now proceed to decode \\\" . Have you ever thought about this ? As a linguist and expert on computers , do you think it is worth thinking about ? Cited in Hutchins ( 1997 ) . Weaver was possibly chastened by Wiener 's pessimistic reply : . I frankly am afraid the boundaries of words in different languages are too vague and the emotional and international connotations are too extensive to make any quasi - mechanical translation scheme very hopeful . But Weaver remained undeterred and composed his famous 1949 Memorandum , titled simply \\\" Translation \\\" , which he sent to some 30 noteworthy minds of the time . It posited in more detail the need for and possibility of MT . Thus began the first era of MT research . A direct system would comprise a bilingual dictionary containing potential replacements or target language equivalents for each word in the source language . A restriction of such MT systems was therefore that they were unidirectional and could not accommodate many languages unlike the systems that followed . Rules for choosing correct replacements were incorporated but functioned on a basic level ; although there was some initial morphological analysis prior to dictionary lookup , subsequent local re - ordering and final generation of the target text , there was no scope for syntactic analysis let alone semantic analysis ! Inevitably this often led to poor quality output , which certainly contributed to the severe criticism of MT in the 1966 Automatic Language Processing Advisory Committee ( ALPAC ) report which stated that it saw little use for MT in the foreseeable future . The damning judgment of the ALPAC report effectively halted research funding for machine translation in the USA throughout the 1960s and 1970s . We can say that both technical constraints and the lack of a linguistic basis hampered MT systems . The system developed at Georgetown University , Washington DC , and first demonstrated at IBM in New York in 1954 had no clear separation of translation knowledge and processing algorithms , making modification of the system difficult . In the period following the ALPAC report the need was increasingly felt for an approach to MT system design which would avoid many of the pitfalls of 1 G systems . By this time opinion had shifted towards the view that linguistic developments should influence system design and development . Indeed it can be said that the second generation ( 2 G ) of \\\" indirect \\\" systems owed much to linguistic theories of the time . 2 G systems can be divided essentially into \\\" interlingual \\\" and \\\" transfer \\\" systems . We will look first of all at interlingual systems , or rather those claiming to adopt an interlingual approach . Although Warren Weaver had put forward the idea of an intermediary \\\" universal \\\" language as a possible route to machine translation in his 1947 letter to Norbert Wiener , linguistics was unable to offer any models to apply until the 1960s . By virtue of its introduction of the concept of \\\" deep structure \\\" , Noam Chomsky 's theory of transformational generative grammar appeared to offer a route towards \\\" universal \\\" semantic representations and thus appeared to provide a model for the structure of a so - called interlingua . An interlingua is not a natural language , rather it can be seen as a meaning representation which is independent of both the source and the target language of translation . An interlingua system maps from a language 's surface structure to the interlingua and vice versa . A truly interlingual approach to system design has obvious advantages , the most important of which is economy , since an interlingual representation can be applied for any language pair and facilitates addition of other language pairs without major additions to the system . The next section looks at \\\" transfer \\\" systems . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . ( For n languages the number of transfer modules required would be n ( n -1 ) or n ( n -1 ) /2 if the modules are reversible ) . An important advance in 2 G systems when compared to 1 G was the separation of algorithms ( software ) from linguistic data ( lingware ) . In a system such as the Georgetown model the program mixed language modelling , translation and the processing thereof in one program . This meant that the program was monolithic and it was easy to introduce errors when trying to rectify an existing shortcoming . The move towards separating software and lingware was hastened by parallel advances in both computational and linguistic techniques . The adoption of linguistic formalisms in the design of systems and the development of high level programming languages enabled MT workers to code in a more problem - oriented way . The development in programming languages meant that it was becoming ever easier to code rules for translation in a meaningful manner and arguably improved the quality of these rules . The declarative nature of linguistic description could now be far more explicitly reflected in the design of programs for MT . . Early MT systems were predominantly parser - based , one of the first steps in such a system being to parse and tag the source language : see Section 5 on Parsing and Tagging . More recent current approaches to MT rely less on formal linguistic descriptions than the transfer approach described above . Translation Memory ( TM ) systems are now in widespread commercial use : see below and Chapter 10 of Arnold et al . ( 1994 ) . Example - Based Machine Translation ( EBMT ) is a relatively new technology which aims to combine both traditional MT and more recent TM paradigms by reusing previous translations and applying various degrees of linguistic knowledge to convert fuzzy matches into exact ones : see the Wikipedia article on EBMT . However , some early definitions of EBMT refer to what is now known as TM and they often exclude the concept of fuzzy matches . Essentially , Google Translat e begins by examining and comparing massive corpora of texts on the Web that have already been translated by human beings . It looks for matches between source and target texts and uses complex statistical analysis routines to look for statistically significant patterns , i.e. it works out the rules of the interrelationships between source and target texts for itself . As more and more corpora are added to the Web this means that Google Translate will keep improving until it reaches a point where it will be very difficult to tell that a machine has done the translation . I remember early machine translation tools translating \\\" Wie geht es dir ? \\\" as \\\" How goes it you ? \\\" Now Google Translate gets it right : \\\" How are you ? \\\" Thus we have , in a sense , come full circle in that Weaver 's ideas of applying statistical techniques are seen as a fruitful basis for MT . . 3.2 Commercial MT packages . There are many automatic translation packages on the market - as well as free packages on the Web . While such packages may be useful for extracting the gist of a text they should not be seen as a serious replacement for the human translator . Some are not all that bad , producing translations that are half - intelligible , letting you know whether a text is worth having translated properly . See : . Professional human translators are making increasing use of Translation Memory ( TM ) packages . TM packages store texts that have previously been translated , together with their source texts , in a large database . Chunks of new texts to be translated are then matched against the translated texts in the database and suggested translations are offered to the human translator wherever a match is found . The human translator has to intervene regularly in this process of translation , making corrections and amendments as necessary . TM systems can save hours of time ( estimated at up to 80 % of a translator 's time ) , especially when translating texts that are repetitive or that use lots of standard phrases and sentence formulations . Producing updates of technical manuals is a typical application of TM systems . Examples of TM systems include : . An example of automatic translations can be found at the Newstran website . This site is extremely useful for locating newspapers in a wide range of languages . You can also locate selected newspapers that have been translated using a Machine Translation system . Another approach to translation is the stored phrase bank , for example LinguaWrite , which was aimed at the business user and contained a large database of equivalent phrases and sentences in different languages to facilitate the writing of business letters . LinguaWrite was programmed by Marco Bruzzone in the 1980s and marketed by Camsoft , but it is no longer available and has not been updated . David Sephton 's Tick - Tack ( Primrose Publishing ) adopted a similar approach , beginning as a package consisting of \\\" building blocks \\\" of language for business communication , but it now embraces other topics . 3.3 Just for fun . 3.3.1 Some apocryphal stuff . The following examples have often been cited as mistakes made by machine translation ( MT ) systems . Whether they are real examples or not can not be verified . Russian - English : In a technical text that had been translated from Russian into English the term water sheep kept appearing . When the Russian source text was checked it was found that it was actually referring to a hydraulic ram . Russian - English : Idioms are often a problem . Russian - English : Another example , similar to the one above , is where out of sight , out of mind ended up being translated as the equivalent of blind and stupid . MT systems do , however , often make mistakes . The Systran MT system , which has been used by the European Commission , translated the English phrase pregnant women and children into des femmes et enfants enceints , which implies that both the women and the children are pregnant . Although it is an interpretation of the original phrase that is theoretically possible , it is also clearly wrong . 3.3.2 Translations of nursery rhymes . Try using an online machine translator to translate a text from English into another language and then back again . The results are often amusing , especially if you are translating nursery rhymes ! ( i ) Bah , bah , black sheep translated into French and then back again into English , using Babel Fish . English source text : Bah , bah , black sheep , have you any wool ? Yes sir , yes sir , three bags full . One for the master , one for the dame , and one for the little boy who lives down the lane . French translation : Bah , bah , mouton noir , vous ont n'importe quelles laines ? Oui monsieur , oui monsieur , trois sacs compl\\u00e8tement . Un pour le ma\\u00eetre , un pour dame , et un pour le petit gar\\u00e7on qui vit en bas de la ruelle . And back into English again : Bah , bah , black sheep , have you n ' imports which wools ? Yes Sir , yes Sir , three bags completely . For the Master , for lady , and for the little boy who lives in bottom of the lane . ( ii ) Humpty Dumpty translated into Italian and then back again into English , using Babel Fish . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty si \\u00e8 seduto su una parete . Humpty Dumpty ha avuto una grande caduta . I cavalli di tutto il re e gli uomini di tutto il re non hanno potuto un Humpty ancora . And back into English again : Humpty Dumpty has been based on a wall . Humpty Dumpty has had a great fall . The horses of all the king and the men of all the king have not been able a Humpty still . ( iii ) Humpty Dumpty translated into Italian and then back again into English , using Google Translate . English source text : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . Italian translation : Humpty Dumpty sedeva su un muro . Humpty Dumpty ha avuto un grande caduta . Tutti i cavalli del re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme . And back into English again : Humpty Dumpty sat on a wall . Humpty Dumpty had a great fall . All the king 's horses and all the king 's men could not put Humpty together again . Now , the above is an interesting result ! Google Translate used to be a very unreliable MT tool . It drives language teachers mad , as their students often use it to do their homework , e.g. translating from a given text into a foreign language or drafting their own compositions and then translating them . Mistakes made by Google Translate used to be very easy to spot , but ( as indicated above in Section 3.1 ) Google changed its translation engine a few years ago and now uses a Statistical Machine Translation ( SMT ) approach . The Humpty Dumpty translation back into English from the Italian appears to indicate that Google Translate has matched the whole text and got it right . Clever ! 3.3.3 Translations of business and journalistic texts . ( i ) A business text , translated with Google Translate and Babel Fish . Google Translate was used to translate the following text from German into English : Die Handelskammer in Saarbr\\u00fccken hat uns Ihre Anschrift zur Verf\\u00fcgung gestellt . Wir sind ein mittelgro\\u00dfes Fachgesch\\u00e4ft in Stuttgart , und wir spezialisieren uns auf den Verkauf von Personalcomputern . This was rendered as : The Chamber of Commerce in Saarbr\\u00fccken has provided us your address is available . We are a medium sized shop in Stuttgart , and we specialize in sales of personal computers . Babel Fish produced a better version : The Chamber of Commerce in Saarbruecken put your address to us at the disposal . We are a medium sized specialist shop in Stuttgart , and we specialize in the sale of personal computers . ( ii ) A journalistic text , translated with Google Translate and Babel Fish . Die deutsche Exportwirtschaft k\\u00e4mpft mit der weltweiten Konjunkturflaute und muss deshalb von den Zeiten zweistelligen Wachstums Abschied nehmen . [ Ludolf Wartenberg vom Bundesverband der Deutschen Industrie ] . This was rendered by Google Translate as : The German export economy is struggling with the global downturn and must therefore take the times of double - digit growth goodbye . [ Ludolf Wartenberg from the Federation of German Industry ] . The German export trade and industry fights with the world - wide recession and must take therefore from the times of two digit growth parting . [ Ludolf waiting mountain of the Federal association of the German industry . ] Computers are normally associated with two standard input devices , the keyboard and the mouse , and two standard output devices , the display screen and the printer . All these restrict language input and output . However , computer programs and hardware devices that enable the computer to handle human speech are now commonplace . All modern computers allow the user to plug in a microphone and record his / her own voice . A variety of other sources of sound recordings can also be used . Storing these sound files is not a problem anymore as a result of the immensely increased capacity and reduced cost of storage media and improved compression techniques that enable the size of sound files to be substantially reduced . For further information on the applications of sound recording and playback technology to CALL see Module 2.2 , Introduction to multimedia CALL . A range of computer software is available for speech analysis . Spoken input can be analysed according to a wide variety of parameters and the analysis can be represented graphically or numerically . Of course , graphic output is not immediately useful for the uninitiated viewer , and hence we are not arguing that this kind of graphical representation will prove useful to the language learner . On the other hand , specialists are well capable of interpreting this speech analysis data . The information we get from speech analysis has proven very valuable indeed for speech synthesis and speech recognition , which are dealt with in the following two sections . 4.1 Speech synthesis . Speech synthesis describes the process of generating human - like speech by computer . Producing natural sounding speech is a complex process in which one has to consider a range of factors that go beyond just converting characters to sounds because very often there is no one - to - one relation between them . The intonation of particular sentence types and the rhythm of particular utterances also have to be considered . Currently speech synthesis is far more advanced and more robust than speech recognition ( see Section 4.2 below ) . The naturalness of artificially produced utterances is now very impressive compared to what used to be produced by earlier speech synthesis systems in which the intonation and timing were far from natural and resulted in the production of monotonous , robot - like speech . Many people are now unaware that so - called talking dictionaries use speech synthesis software rather than recordings of human voices . In - car satellite navigation ( satnav ) systems can produce a range of different types of human voices , both male and female in a number of different languages , and \\\" talk \\\" to the car driver guiding him / her to a chosen destination . So far , however , speech synthesis has not been as widely used in CALL as speech recognition . This is probably due to the fact that language teachers ' requirements regarding the presentation of spoken language are very demanding . Anything that sounds artificial is likely to be rejected . Some language teachers even reject speakers whose regional accent is too far from what is considered standard or received pronunciation . There is , however , a category of speech synthesis technology known as Text To Speech ( TTS ) technology that is widely used for practical purposes . TTS software falls into the category of assistive technology , which has a vital role in improving accessiblity for a wide range of computer users with special needs , which is now governed by legislation in the UK . The Special Educational Needs and Disability Act ( SENDA ) of 2001 covers educational websites and obliges their designers \\\" to make reasonable adjustments to ensure that people who are disabled are not put at a substantial disadvantage compared to people who are not disabled . \\\" See JISC 's website on Disability Legislation and ICT in Further and Higher Education - Essentials . See the Glossary for definitions of assistive technology and accessiblity . TTS is important in making computers accessible to blind or partially sighted people as it enables them to \\\" read \\\" from the screen . TTS technology can be linked to any written input in a variety of languages , e.g. automatic pronunciation of words from an online dictionary , reading aloud of a text , etc . These are examples of TTS software : . Festival Speech Synthesis System : From the Centre for Speech Technology Research at the University of Edinburgh . Festival offers a general framework for building speech synthesis systems as well as including examples of various modules . Just for fun I entered the phrase \\\" Pas d'elle yeux Rh\\u00f4ne que nous \\\" into a couple of French language synthesisers . It 's a nonsense sentence in French but it comes out sounding like a French person trying to pronounce a well - known expression in English . Try it ! There are also Web - based tools that enable you to create animated cartoons or movies incorporating TTS , for example : . Voki enables you to create and customise your own speaking cartoon character . You can choose the TTS option ( as in Graham Davies 's example on the right ) to give the character a voice , or you can record your own voice . ReadTheWords : A tool that works in much the same way as Voki , but without the option of recording one 's own voice . An excellent tool that helps people with hearing impairments to learn how to articulate is the CSLU Speech Toolkit . To what extent speech synthesis systems are suitable for CALL is a matter for further discussion . See the article by Handley & Hamel ( 2005 ) , who report on their progress towards the development of a benchmark for determining the adequacy of speech synthesis systems for use in CALL . The article mentions a Web - based package called FreeText , for advanced learners of French , the outcome of a project funded by the European Commission . 4.2 Speech recognition . Speech recognition describes the use of computers to recognise spoken words . Speech recognition has not reached such a high level of performance as speech synthesis ( see Section 4.1 above ) , but it has certainly become usable in CALL in recent years . EyeSpeak English is a typical example of the use of speech recognition software for helping students improve their English pronunciation . Speech recognition is a non - trivial task because the same spoken word does not produce entirely the same sound waves when uttered by different people or even when uttered by the same person on different occasions . The process is complex : the computer has to digitise the sound , transform it to discard unneeded information , and then try to match it with words stored in a dictionary . The most efficient speech recognition systems are speaker - dependent , i.e. they are trained to recognise a particular person 's speech and can then distinguish thousands of words uttered by that person . If one remembers that each of the parameters analysed could have been affected by some speaker - independent background noise or by some idiosyncratic pronunciation features of this particular speaker then it already becomes clear how difficult the interpretation of the analysis data is for a speech recognition program . The following information is taken from an article written by Norman Harris of DynEd , a publisher of CALL software incorporating ASR : . Speech recognition technology has finally come of age - at least for language training purposes for young adults and adults . The essence of real language is not in discrete single words - language students need to practice complete phrases and sentences in realistic contexts . Moreover , programs which were trained to accept a speaker 's individual pronunciation quirks were not ideally suited to helping students move toward more standard pronunciation . These technologies also failed if the speaker 's voice changed due to common colds , laryngitis and other throat ailments , rendering them useless until the speaker recovered or retrained the speech engine . The solution to these problems came with the development of continuous speech recognition engines that were speaker independent . These programs are able to deal with complete sentences spoken at a natural pace , not just isolated words . Such flexibility with regard to pronunciation paradigms means that today 's speaker - independent speech recognition programs are not ideal for direct pronunciation practice . Nonetheless , exercises which focus on fluency and word order , and with native speaker models which are heard immediately after a student 's utterance had been successfully recognized , have been shown to indirectly result in much improved pronunciation . Another trade off is that the greater flexibility and leniency which allows these programs to \\\" recognize \\\" sentences spoken by students with a wide variety of accents , also limits the accuracy of the programs , especially for similar sounding words and phrases . Some errors may be accepted as correct . Native speakers testing the \\\" understanding \\\" of programs \\\" tuned \\\" to the needs of non - native speakers may be bothered by this , but most teachers , after careful consideration of the different needs and psychologies of native speakers and learners , will accept the trade off . Students do not expect to be understood every time . If they are required occasionally to repeat a sentence which the program has not recognized or which the program has misinterpreted , there may be some small frustration , but language students are much more likely to take this in their stride than would native speakers . On the other hand , if the program does \\\" understand \\\" such students , however imperfect their pronunciation , they typically experience a huge sense of satisfaction , a feel good factor native speakers simply can not enjoy to anywhere near the same degree . The worst thing for a student is a program that is too demanding of perfection - such programs will quickly lead to student frustration or the kind of embarrassed , hesitant unwillingness to speak English typical of many classrooms . Even if we accept that accuracy needs to be responsive to proficiency in order to encourage students to speak , we must , as teachers , be concerned that errors do not become reinforced . A recent breakthrough is the implementation of apps such as Apple 's Siri on the iPhone 4S and Evi , which is available for the iPhone and the Android . These apps are quite impressive at recognising speech and providing answers to questions submitted by the user . Evi 's performance was tested by the author of this paragraph . \\\" She \\\" immediately provided correct answers to these questions submitted by voice input : . In which American state is Albuquerque ? In addition , Evi may link to relevant websites that provide further information . Text input is also accepted . In this section we outline the essentials of parsing , first of all by describing the components of a parsing system and then discussing different kinds of parser . We look at one linguistic phenomenon which causes problems for parsing and finally examine potential solutions to the difficulties raised by parsing . Put in simple terms , a parser is a program that maps strings of a language into its structures . The most basic components needed by a parser are a lexicon containing words that may be parsed and a grammar , consisting of rules which determine grammatical structures . The first parsers were developed for the analysis of programming languages ; obviously as artificial , regular languages they present fewer problems than a natural language . It is most useful to think of parsing as a search problem which has to be solved . It can be solved using an algorithm which can be defined as : . [ ... ] a formal procedure that always produces a correct or optimal result . An algorithm applies a step - by - step procedure that guarantees a specific outcome or solves a specific problem . The procedure of an algorithm performs a computation in a finite amount of time . Programmers specify the algorithm the program will follow when they develop a conventional program . ( Smith 1990 ) . Parsing algorithms define a procedure that looks for the optimum combination of grammatical rules that generate a tree structure for the input sentence . How might we define these grammatical rules in a concise way that is amenable to computer processing ? A useful construct for our purposes is a so - called context - free grammar ( CFG ) . A CFG consists of rules containing a single symbol on the left - hand side and one or more on the right - hand side . For example , the statement that a sentence can consist of : . a noun phrase and a verb phrase can be expressed by the following rewrite rule . S \\u00ae NP VP . This means that a sentence S can be ' rewritten ' as a noun phrase NP followed by a verb phrase VP which are in their turn defined in the grammar . A noun phrase , for example , can consist of a determiner DET and a noun N. These symbols are known as non - terminals and the words represented by these symbols are terminal symbols . Parsing algorithms can proceed top - down or bottom - up . In some cases , top - down and bottom - up algorithms can be combined . Below are simple descriptions of two parsing strategies . 5.1.1 Top Down ( depth first ) . Top down strategy works from non - terminal symbols : . S \\u00ae NP VP . and then breaks them down into constituents . The strategy assumes we have an S and tries to fit it in . If we choose to search depth first , then we proceed down one side of the tree at a time . The search will end successfully if it manages to break down the sentence into all its terminal symbols ( words ) . 5.1.2 Bottom up ( breadth first ) . A bottom up strategy looks at elements of an S and assigns categories to them to form larger constituents until we arrive at an S. If we choose to search breadth first , then we proceed consecutively through each layer and stop successfully once we have constructed a sentence . Let 's look now at one linguistic phenomenon which causes problems for parsers - that of so - called attachment ambiguity . Consider the following sentence : . The man saw the man in the park with a telescope . Parser output can be represented as a bracketed list or , more commonly , a tree structure . Here is the output of two possible parses for the sentence above . One way of dealing with the problem of sentences which have more than one possible parse is to concentrate on specific elements of the parser input and to not deal with such phenomena as attachment ambiguity . Ideally we expect a parser to successfully analyse a sentence on the basis of its grammar , but often there are problems caused by errors in the text or incompleteness of grammar and lexicon . Also the length of sentences and ambiguity of grammars often make it hard to successfully parse unrestricted text . An approach which addresses some of these issues is partial or shallow parsing . Abney ( 1997:125 ) succinctly describes partial parsing thus : . \\\" Partial parsing techniques aim to recover syntactic information efficiently and reliably from unrestricted text , by sacrificing completeness and depth of analysis . \\\" Partial parsers concentrate on recovering pieces of sentence structure which do not require large amounts of information ( such as lexical association information ) ; attachment remains unresolved for instance . We can see that in this way parsing efficiency is greatly improved . Another strategy for analysing language is part - of - speech tagging , in which we do not seek to find larger structures such as noun phrases but instead label each word in a sentence with its appropriate part of speech . Here is the original paragraph from Section 3 of this document : . In a transfer model the intermediate representation is language dependent , there being a bilingual module whose function it is to interpose between source language and target language intermediate representations . Thus we can not say that the transfer module is language independent . The following table shows the tagger output , and we can see that most of the words have been correctly identified . additional . languages . NNS . language . required . VBN . require . . . SENT . . . As with partial parsing , we are not trying to find correct attachments and since it is a limited task the success rate is quite high . The information derived from tagging can itself have input into partial parsing or into improving the performance of traditional parsers . Some of the decision task as to what is the correct part of speech to assign to a word is based on the probability of two or three word sequences ( bigrams and trigrams ) occurring , even where words can be assigned more than one part of speech . For instance , in our example tagged text the sequence ' the transfer module ' occurs . Transfer is of course also a verb , but the likelihood of a determiner ( the ) being followed by a verb is lower than the likelihood of a determiner noun sequence . See als o the Visual Interactive Syntax Learning ( VISL ) website . An online parser and a variety of other tools concerned with English grammar , including games and quizzes , can be found here . 5.1.3 Parsing erroneous input . Of course , in CALL we are dealing with texts that have been produced by language learners at various levels of proficiency and accuracy . It is therefore reasonable to assume that the parser has to be prepared to deal with linguistic errors in the input . One thing we could do is to complement our grammar for correct sentences with a grammar of incorrect sentences - an error grammar , i.e. we capture individual and/or typical errors in a separate rule system . The advantage of this error grammar approach is that the feedback can be very specific and is normally fairly reliable because this feedback can be attached to a very specific rule . The big drawback , however , is that individual learner errors have to be anticipated in the sense that each error needs to be covered by an adequate rule . However , as already stated it is not only in texts that have been produced by language learners that we find erroneous structures . Machine translation is facing similar problems . Dina & Malnati review approaches \\\" concerning the design and the implementation of grammars able to deal with ' real input ' . \\\" ( Dina & Malnati 1993:75 ) . They list four approaches : . The rule - based approach which relies on two sets of rules : one for grammatical input and the other for ungrammatical input . Dina & Malnati point out quite rightly that normally well - formedness conditions should be sufficient and the second set of rules results in linguistic redundancy . The main problem with using this approach in a parser - based CALL system is the problem of having to anticipate the errors learners are likely to make . The metarule - based approach uses a set of well - formedness rules and if none of them can be applied calls an algorithm that relaxes some constraints and records the kind of violation . Dina & Malnati note the procedurality of the algorithm causes problems when confronted with multiple errors - something very likely in any text produced by a language learner . The preference - based approach comprises an overgenerating grammar and a set of preference rules . \\\" [ ... ] each time a formal condition is removed from a b - rule to make its applicability context wider , a preference rule must be added to the grammar . Such a p - rule must be able to state - in the present context of the b - rule - the condition that has been previously removed . \\\" ( Dina & Malnati 1993:78 ) Again a source for linguistic redundancy which might result in inconsistencies in the grammar . They claim that , due to the overgeneration of possible interpretations , \\\" the system would be completely unusable in an applied context . \\\" ( ibid.:79 ) . The constraint - based approach is based on the following assumptions : . each ( sub)tree is marked by an index of error ( initially set to 0 ) ; . the violation of a constraint in a rule does not block the application , but increases the error index of the generated ( sub)tree ; . at the end of parsing the object marked by the smallest index is chosen . Consequently , the \\\" most plausible interpretation of a [ ... ] sentence is the one which satisfies the largest number of constraints . \\\" ( Dina & Malnati 1993:80 ) . We have seen in Section 3 that Machine Translation ( MT ) and the political and scientific interest in machine translation played a significant role in the acceptance ( or non - acceptance ) as well as the general development of Human Language Technologies . By 1964 , however , the promise of operational MT systems still seemed distant and the sponsors set up a committee , which recommended in 1966 that funding for MT should be reduced . It brought to an end a decade of intensive MT research activity . ( Hutchins 1986:39 ) . It is then perhaps not surprising that the mid-1960s saw the birth of another discipline : Computer Assisted Language Learning ( CALL ) . The PLATO project , which was initiated at the University of Illinois in 1960 , is widely regarded as the beginning of CALL - although CALL was just part of this huge package of general Computer Assisted Learning ( CAL ) programs running on mainframe computers . PLATO IV ( 1972 ) was probably the version of this project that had the biggest impact on the development of CALL ( Hart 1995 ) . At the same time , another American university , Brigham Young University , received government funding for a CALL project , TICCIT ( Time - Shared Interactive , Computer Controlled Information Television ) ( Jones 1995 ) . Other well - known and still widely used programs were developed soon afterwards : . The CALIS / WinCALIS ( Computer Aided Language Instruction System ) authoring tools , Duke University ( Borchardt 1995 ) . The TUCO package for learners of German , developed by Heimy Taylor and Werner Haas , Ohio State University . See Module 3.2 , Section 5.9 . The CLEF package for learners of French , which was produced by a consortium of Canadian universities in the late 1970s and is still going strong today . See Module 3.2 , Section 5.9 . In the UK , John Higgins developed Storyboard in the early 1980s , a total Cloze text reconstruction program for the microcomputer ( Higgins & Johns 1984:57 ) . ( Levy 1997:24 - 25 ) describes how other programs extended this idea further . See Section 8.3 , Module 1.4 , headed Total text reconstruction : total Cloze , for further information on total Cloze programs . In recent years the development of CALL has been greatly influenced by the technology and by our knowledge of and our expertise in using it , so that not only the design of most CALL software , but also its classification has been technology - driven . For example , Wolff ( 1993:21 ) distinguished five groups of applications : . The late 1980s saw the beginning of attempts which are mostly subsumed under Intelligent CALL ( ICALL ) , a \\\" mix of AI [ Artificial Intelligence ] techniques and CALL \\\" ( Matthews 1992b : i ) . Early AI - based CALL was not without its critics , however : . And that , fundamentally , is why my initial enthusiasm has now turned so sour . ( Last 1989:153 ) . For a more up - to - date and positive point of view of Artifical Intelligence , see Dodigovic ( 2005 ) . Bowerman ( 1993:31 ) notes : \\\" Weischedel et al . ( 1978 ) produced the first ICALL [ Intelligent CALL ] system which dealt with comprehension exercises . It made use of syntactic and semantic knowledge to check students ' answers to comprehension questions . \\\" As far as could be ascertained , this was just the early swallow that did not create a summer . Kr\\u00fcger - Thielmann ( 1992:51ff . ) lists and summarises the following early projects in ICALL : ALICE , ATHENA , BOUWSTEEN & COGO , EPISTLE , ET , LINGER , VP2 , XTRA - TE , Zock . Matthews ( 1993:5 ) identifies Linguistic Theory and Second Language Acquisition Theory as the two main disciplines which inform Intelligent CALL and which are ( or will be ) informed by Intelligent CALL . He adds : \\\" the obvious AI research areas from which ICALL should be able to draw the most insights are Natural Language Processing ( NLP ) and Intelligent Tutoring Systems ( ITS ) \\\" ( Matthews1993:6 ) . Matthews shows that it is possible to \\\" conceive of an ICALL system in terms of the classical ITS architecture \\\" ( ibid . ) The system consists of three modules - expert , student and teacher module - and an interface . The expert module is the one that \\\" houses \\\" the language knowledge of the system . It is this part which can process any piece of text produced by a learner - in an ideal system . This is usually done with the help of a parser of some kind : . ( Holland et al . 1993:28 ) . This notion of parser - based CALL not only captures the nature of the field much better than the somewhat misleading term \\\" Intelligent CALL \\\" ( Is all other CALL un - intelligent ? ) , it also identifies the use of Human Language Technologies as one possible approach in CALL alongside others such as multimedia - based CALL and Web - based CALL and thus identifies parser - based CALL as one possible way forward for CALL . In some cases , the ( technology - defined ) borders between these sub - fields of CALL are not even clearly identifiable , as we will see in some of the projects mentioned in the following paragraphs . To exemplify recent advances in the use of sophisticated human language technology in CALL , let us have a look at some of the projects that were presented at two conferences in the late 1990s . The first one is the Language Teaching and Language Technology conference in Groningen in 1997 ( Jager et al . 1998 ) . Witt & Young ( 1998 ) , on the other hand , are concerned with assessing pronunciation . They implemented and tested a pronunciation scoring algorithm which is based on speech recognition ( see Section 4.2 ) and uses hidden Markov models . \\\" The results show that - at least for this setup with artificially generated pronunciation errors - the GOP [ goodness of pronunciation ] scoring method is a viable assessment tool . \\\" A third paper on pronunciation at this conference , by Skrelin & Volskaja ( 1998 ) outlined the use of speech synthesis ( see Section 4.1 ) in language learning and lists dictation , distinction of homographs , a sound dictionary and pronunciation drills as possible applications . \\\" The project vision foresees two main areas where GLOSSER applications can be used . First , in language learning and second , as a tool for users that have a bit of knowledge of a foreign language , but can not read it easily or reliably \\\" ( Dokter & Nerbonne 1998:88 ) . Dokter & Nerbonne report on the French - Dutch demonstrator running under UNIX . The demonstrator : . uses morphological analysis to provide additional grammatical information on individual words and to simplify dictionary look - up ; . relies on automatic word selection ; . offers the opportunity to insert glosses ( taken form the dictionary look - up ) into the text ; . relies on string - based word sense disambiguation ( \\\" Whenever a lexical context is found in the text that is also provided in the dictionary , the example in the dictionary is highlighted . \\\" ( op.cit.:93 ) . Roosma & Pr\\u00f3sz\\u00e9ky ( 1998 ) draw attention to the fact that GLOSSER works with the following language pairs : English - Estonian - Hungarian , English - Bulgarian , French - Dutch and describe a demonstrator version running under Windows . Dokter et al ( 1998 ) conclude in their user study \\\" that Glosser - RuG improves the ease with which language students can approach a foreign language text \\\" ( Dokter et al . 1998:175 ) . The latter project relies on a spellchecker , morphological analyser , syntactic parser and a lexical database for Basque , and the authors report on the development of an interlanguage model . At another conference ( UMIST , May 1998 ) , which brought together a group of researchers who are exploring the use of HLT in CALL software , Schulze et al . ( 1999 ) and Tschichold ( 1999 ) discussed strategies for improving the success rate of grammar checkers . Menzel & Schr\\u00f6der ( 1999 ) described error diagnosis in a multi - level representation . The demonstration system captures the relations of entities in a simple town scenery . The available syntactic , semantic and pragmatic information is checked simultaneously for constraint violations , i.e. errors made by the language learners . Visser ( 1999 ) introduced CALLex , a program for learning vocabulary based on lexical functions . Diaz de Ilarraza et al . ( 1999 ) described aspects of IDAZKIDE , a learning environment for Spanish learners of Basque . The program contains the following modules : wide - coverage linguistic tools ( lexical database with 65,000 entries ; spell checker ; a word form proposer and a morphological analyser ) , an adaptive user interface and a student modelling system . The model of the students ' language knowledge , i.e. their interlanguage , is based on a corpus analysis ( 300 texts produced by learners of Basque ) . Foucou & K\\u00fcbler ( 1999 ) presented a Web - based environment for teaching technical English to students of computing . Ward et al . ( 1999 ) showed that Natural Language Processing techniques combined with a graphical interface can be used to produce meaningful language games . Davies & Poesio ( 1998 ) reported on tests of simple CALL prototypes that have been created using CSLUrp , a graphical authoring system for the creation of spoken dialogue systems . They argue that since it is evident that today 's dialogue systems are usable in CALL software , it is now possible and necessary to study the integration of corrective feedback in these systems . Mitkov ( 1998 ) outlined early plans for a new CALL project , The Language Learner 's Workbench . It is the aim of this project to incorporate a number of already available HLT tools and to package them for language learners . These examples of CALL applications that make use of Human Language Technologies are by no means exhaustive . They not only illustrate that research in HLT in CALL is vibrant , but also that HLT has an important contribution to make in the further development of CALL . Of course , both disciplines are still rather young and many projects in both areas , CALL and HLT , have not even reached the stage of the implementation of a fully functional prototype yet . A number of CALL packages that make use of speech recognition have reached the commercial market and are being used successfully by learners all over the world ( see Section 4.2 ) . Speech synthesis , certainly at word level , has achieved a clarity of pronunciation that makes it a viable tool for language learning ( see Section 4.1 ) . Many popular electronic dictionaries now incorporate speech synthesis systems . Part - of - speech taggers have reached a level of accuracy that makes them usable in the automatic pre - processing of learner texts . Morphological analysers for a number of languages automatically provide grammatical information on vocabulary items in context and make automatic dictionary look - ups of inflected or derived word forms possible . This progress in HLT and CALL has mainly been possible as the result of our better understanding of the structures of language - our understanding of linguistics . The lack of linguistic modelling and the insufficient deployment of Natural Language Processing techniques has sometimes been given as one reason for the lack of progress in some areas of CALL : see , for example , Levy ( 1997:3 ) , citing Kohn ( 1994 ) . [ ... ] Kohn suggests that current CALL is lacking because of poor linguistic modelling , insufficient deployment of natural language processing techniques , an emphasis on special - purpose rather than general - purpose technology , and a neglect of the ' human ' dimnesion of CALL ( Kohn 1994:32 ) . The examples in the previous section have shown that it is possible to apply certain linguistic theories ( e.g. phonology and morphology ) to Human Language Technologies and implement this technology in CALL software . This is , of course , true . However , it does not mean that interesting fragments or aspects of a given language can not be captured by a formal linguistic theory and hence implemented in a CALL application . In other words , if one can not capture the German language in its entirety in order to implement this linguistic knowledge in a computer program , this does not mean that one can not capture interesting linguistic phenomena of that language . This means even if we are only able to describe a fragment of a given language adequately we can still make very good use of this description in computer applications for language learning . What is the kind of knowledge we ought to have about language before we can attempt to produce an HLT tool that can be put to effective use in CALL ? Let us look at one particular aspect of language - grammar . In recent years , the usefulness of conscious learning of grammar has been discussed time and again , very often in direct opposition to what has been termed \\\" the communicative approach \\\" . ( ibid.:6 ) This assumption leads to the question of what role exactly the computer ( program ) has to play in a sensitive , rich and enjoyable grammar - learning process . The diversity of approaches outlined in this special issue of ReCALL on grammar illustrates that there are many different roads to successful grammar learning that will need to be explored . In this module , only the example of parser - based CALL will be discussed . Let us take a grammar checker for language learners as a specific example in point . This grammar checker could then be integrated into a CALL program , a word - processor , an email editor , a Web page editor etc . The design of such a grammar checker is mainly based on findings in theoretical linguistics and second language acquisition theory . Let us start with second language acquisition theory . Research in second language acquisition has proved that grammar learning can lead to more successful language acquisition . Here learners have the opportunity to correct grammatical errors and mistakes that they have made while concentrating on the subject matter and the communicative function of the text . It is at this stage that a grammar checker for language learners can provide useful and stimulating guidance . In order to ascertain the computational features of such a grammar checker , let us first consider what exactly we mean by \\\" grammar \\\" in a language learning context . Helbig discusses possible answers to this question from the point of view of the teaching and learning of foreign languages in general : . As a starting point for answering our question concerning the relevance ( and necessity ) of grammar in foreign language teaching we used a differentiation of what was and is understood by the term \\\" grammar \\\" : . Grammar A : the system of rules that is inherent to the object language itself and is independent of the fact whether it has been captured by Linguistics or not ; . Grammar B : the scientific - linguistic description of the language inherent system of rules , the modelling of Grammar A by Linguistics ; . Grammar C : the system of rules intern to the speaker and listener which is formed in the head of the learner during language acquisition and which forms the basis for him / her to produce and understand correct sentences and texts and to use them appropriately in communication . ( Helbig:1975 ) . Helbig identifies further a Grammar B1 and a Grammar B2 - the former being a linguistic grammar and the latter being a learner grammar . The description of grammar B1c is a literal translation of Helbig 's wording - in the terminology used now , the term \\\" interlanguage \\\" appears to be the most appropriate . The application of Helbig 's grammar classification to CALL produces the following results : . Grammar A remains as defined by Helbig . In other words it refers to the target grammar of the interlanguage continuum . Grammar B1a is the grammar which enables the parser to process grammatically well - formed sentences in the target language . Grammars B1b , B1c and B2 enable the grammar checking CALL tool to detect errors in the learner input and provide the linguistic information to generate feedback . Grammar C is the grammar system which the CALL tool should help to correct and expand . Additionally , the grammar checker will gather data for learner profiles which should allow useful insights into the development of Grammar C of learners you have used the program . Consequently , Grammar B in its entirety and Grammar C will have to be considered first and foremost when developing the grammar checker . The question then arises : If Grammar A provides the linguistic data for the parser developer , how can we \\\" feed \\\" these different grammars into a computer program ? The computer requires that any grammar which we intend to use in any program ( or programming language , for that matter ) be mathematically exact . Grammars which satisfy this condition are normally referred to as formal grammars . The mathematical description of these grammars uses set theory . Therefore , a language L is said to have a vocabulary V . If there were no restrictions on how to construct strings , the number of possible strings is infinite . This becomes clear when one considers that each vocabulary item of V could be repeated infinitely in order to construct a string . However , as language learners in particular know any language L adheres to a finite set of ( grammar ) rules . This explains why grammar teaching software that attempts to anticipate possible incorrect answers can only do this successfully if the answer domain is severely restricted and the anticipation process will therefore much simpler . Could a computer program perform this task - a task based on infinite possibilities ? Yes , it could - but not based on infinite possibilities . That is why it will be necessary to look for an approach which is based on a finite set of possibilities , which can then be pre - programmed . Let us therefore consider L the set of strings that can be constructed using the ( formal ) grammar G . A formal grammar can be defined as follows ( see e.g. Allen 1995 ): . G ( VN , VT , R , S ) . And here we are already dealing with sets which have a finite number of members . The number of grammatical rules is fairly limited . This is certainly the case when we only consider the basic grammar rules of a language that will have to be learned by the intermediate to early advanced learner . ( Note here what we said earlier about Grammar B2 - the learner grammar : It was only a subset of Grammar 1 - the linguistic grammar . ) Formal grammars have been used in a number of CALL projects . Matthews ( 1993 ) continues his discussion of grammar frameworks for CALL which he started in 1992 ( Matthews 1992a ) . He lists eight major grammar frameworks that have been used in CALL : . Of course , these are only some examples . More recently , Tschichold et al . ( 1994 ) reported on a prototype for correcting English texts produced by French learners . This system relies on a number of different finite state automata for pre - processing , filtering and detecting ( Tschichold et al.1994 ) . Brehony & Ryan ( 1994 ) report on \\\" Francophone Stylistic Grammar Checking ( FSGC ) using Link Grammars \\\" . They adapted the post - processing section of an existing parser so that it would detect stylistic errors in English input produced by French learners . His plea is for the use of the PPT ( Principles and Parameters Theory ( Chomsky 1986 ) as a grammar framework for CALL applications , basing his judgement on three criteria : computational effectiveness , linguistic perspicuity and acquisitional perspicuity ( Matthews 1993:9 ) . In later parts of his paper , Matthews compares rule- and principle - based frameworks using DCGs ( Definite Clause Grammars ) as the example for the latter . He concludes that principle - based frameworks ( and consequently principle - based parsing ) are the most suitable grammar frameworks for what he calls Intelligent CALL . Recently , other unification - based grammar frameworks not included in Matthews ' list have been used in CALL . Hagen , for instance , describes \\\" an object - oriented , unification - based parser called HANOI \\\" ( Hagen 1995 ) which uses formalisms developed in Head - Driven Phrase Structure Grammar ( HPSG ) . He quotes Zajac : . Combining object - oriented approaches to linguistic description with unification - based grammar formalisms [ ... ] is very attractive . On one hand , we gain the advantages of the object - oriented approach : abstraction and generalisation through the use of inheritance . On the other hand , we gain a fully declarative framework , with all the advantages of logical formalisms [ ... ] . Of course , not even this extended list is comprehensive - at best it could be described as indicative of the variety of linguistic approaches used in parser - based Computer Assisted Language Learning and , in particular , in the field of grammar checking . At the end of this short excursion into formal grammar(s ) it can be concluded that any CALL grammar checker component needs as its foundation a formal grammar describing as comprehensively as possible the knowledge we have about the target language grammar . This was the grammar that Helbig ( 1975 ) refers to as Grammar B1a . But what part do the other grammars play in a CALL environment ? Let us stay with the example of a parser - based grammar checker for language learners . Hence , the provision of adequate feedback on the morpho - syntactic structure of parts of the text produced by learners is the most important task for this parser - based grammar checker . Let us therefore consider the place of feedback provision within a parser grammar . In other words , as a good teacher would do - the grammar checker would offer advice on how to change an ungrammatical structure into a corresponding grammatically well - formed structure . As stated earlier this approach would be based on an infinite number of construction possibilities . Therefore , the provision of adequate feedback and help to the learner appears to be difficult if not impossible . However , it has been indicated above that feedback could be linked to the finite sets on which the formal grammar relies . How can this be done ? Each member of the three sets which will have to be considered here . The non - terminal symbols like NP and VP , the words and the set of morpho - syntactic rules carry certain features that determine their behaviour in a sentence and determine their relation to other signs within the sentence . These features which restrict what the text producer can do with a given ( terminal or non - terminal ) symbol in a sentence and under what conditions a particular grammatical rule has to be applied will be labelled constraints . Let us return to our provisional description of feedback , which can now be formulated more precisely . Feedback shows the relation . by explaining the underlying constraint of the anticipated construction in L based on Grammar B2 . to support production of construction in L . and by reasoning about the likely cause of the rule violation . to extend Grammar C - the learner - inherent grammar . And secondly , this above description of feedback given by a grammar checker which is based on a modified parser shows that it is possible to construct tools that support the focus on form by learners during the reflection stage of a text production process . Even if the grammar checker were only to detect a small number of morpho - syntactic errors , this would be beneficial for the learners as long as they were aware of the limitations of this CALL tool . On the other hand , the feedback description contains still a number of question marks in parentheses after some of the important keywords - whether the intended aims of grammar checking can be achieved can only be validated through the use and thorough testing of such a grammar checker . We should better not make any such assumption ( in the scientific sense - we do hope for these improvements , of course ) and better wait until such a parser - based grammar checker is actually tested in a series of proper learning experiments . Let us now leave the discussion of some of the underlying linguistics behind and discuss the role of parser - based applications in language learning . Natural language parsers take written language as their input and produce a formal representation of the syntactic and sometimes semantic structure of this input . The role they have to play in computer - assisted language learning has been under scrutiny in the last decade : ( Matthews 1992a ) ; ( Holland et al . 1993 ) ; ( Nagata 1996 ) . See also Heift ( 2001 ) . Holland et al . discussed the \\\" possibilities and limitations of parser - based language tutors \\\" ( Holland et al . 1993:28 ) . Comparing parser - based CALL to what they label as conventional CALL they come to the conclusion that : . [ ... ] in parser - based CALL the student has relatively free rein and can write a potentially huge variety of sentences . ICALL thus permits practice of production skills , which require recalling and constructing , not just recognising [ as in conventional CALL ] , words and structures . ( Holland et al.1993:31 ) . However , at the same time , parsing imposes certain limitations . Parsers tend to concentrate on the syntax of the textual input , thus \\\" ICALL may actually subvert a principal goal of language pedagogy , that of communicating meanings rather than producing the right forms \\\" ( Holland et al.1993:32 ) . This disadvantage can be avoided by a \\\" focus on form \\\" which is mainly achieved by putting the parser / grammar checker to use within a relevant , authentic communicative task and at a time chosen by and convenient to the learner / text producer . Juozulynas ( 1994 ) evaluated the potential usefulness of syntactic parsers in error diagnosis . He analysed errors in an approximately 400 page corpus of German essays by American college students in second - year language courses . His study shows that : . [ ... ] syntax is the most problematic area , followed by morphology . ( Juozulynas 1994:5 ) . Juozulynas adapted a taxonomic schema by Hendrickson which comprises four categories : syntax , morphology , orthography , lexicon . Juozulynas ' argument for splitting orthography into spelling and punctuation is easily justified in the context of syntactic parsing . Parts of punctuation can be described by using syntactic bracketing rules , and punctuation errors can consequently be dealt with by a syntactic parser . Lexical and spelling errors form , according to Juozulynas , a rather small part of the overall number of learner errors . Some of these errors will , of course , be identified during dictionary look - up , but if words that are in the dictionary are used in a nonsensical way , the parser will not recognise them unless specific error rules ( e.g. for false friends ) are built in . Consequently , a parser - based CALL application can play a useful role in detecting many of the morpho - syntactic errors which constitute a high percentage of learner errors in freely produced texts . Nevertheless , the fact remains : . A second limitation of ICALL is that parsers are not foolproof . Because no parser today can accurately analyse all the syntax of a language , false acceptance and false alarms are inevitable . ( Holland et al . 1993:33 ) . This is something not only developers of parser - based CALL , but also language learners using such software have to take into account . In other words , this limitation of parser - based CALL has to be taken into consideration during the design and implementation process and when integrating this kind of CALL software in the learning process . A final limitation of ICALL is the cost of developing NLP systems . By comparison with simple CALL , NLP development depends on computational linguists and advanced programmers as well as on extensive resources for building and testing grammars . Beyond this , instructional shells and lessons must be built around NLP , incurring the same expense as developing shells and lessons for CALL . ( Holland et al . 1993:33 ) . It is mainly this disadvantage of parser - based CALL that explains the lack of commercially available ( and commercially viable ) CALL applications which make good use of HLT . However , it is to be hoped that this hurdle can be overcome in the not too distant future because sufficient expertise in the area has accumulated over recent years . More and more computer programs make good use of this technology , and many of these have already \\\" entered \\\" the realm of computer - assisted language learning , as can be seen from the examples in the previous section . Holland et al . ( 1993 ) answer their title question \\\" What are parsers good for ? \\\" on the basis of their own experience with BRIDGE , a parser - based CALL program for American military personnel learning German , and on the basis of some initial testing with a small group of experienced learners of German . They present the following points : . ICALL appears to be good for form - focused instruction offering learners the chance to work on their own linguistic errors by this method , not only to improve their performance in the foreign language but also to improve their language awareness . ICALL appears to be good for selected kinds of students . The authors list the following characteristics which might influence the degree of usefulness of ICALL for certain students : . i. intermediate proficiency . ii . analytical orientation . iii . tolerance of ambiguity . iv . confidence as learners . ICALL is good for research because the parser automatically tracks whole sentence responses and detects , classifies , and records errors . It might facilitate the assessment of the students grammatical competency and thus help us discover patterns of acquisition . ICALL [ ... ] can play a role in communicative practice . The authors argue for the embedding of parser - based CALL in \\\" graphics microworlds \\\" which help to capture some basic semantics . Given that we would like to harness the advantages of parser - based CALL , how do we take the limitations into consideration in the process of designing and implementing a parser - based CALL system ? What implications does the use of parsing technology have for human - computer interaction ? [ I]n discourse analytic terms ( Grice 1975 ) , the nature of the contract between student and CALL tutor is straightforward , respecting the traditional assumption that the teacher is right , whereas the ICALL contract is less well defined . ( Holland et al . 1993:33f . ) . The rigidity of traditional CALL in which the program controls the linguistic input by the learner to a very large extent has often given rise to a criticism of CALL which accuses CALL developers and practitioners of relying on behaviourist programmed instruction . If one wants to give learners full control over their linguistic input , for example , by relying on parsing technology , what are then the terms according to which the communicative interaction of computer ( program ) and learner can be defined ? The differences between humans and machines have obviously to be taken into consideration in order to understand the interaction of learners with CALL programs : . Machines are compiled out of individual parts for a very specific purpose ( totum fix et partibus ) ; whereas humans are holistic entities whose parts can be differentiated ( partes fiunt ex toto ) . Humans process all sorts of experiences and repeatedly and interactively create their own environment - something machines can not do . They \\\" calculate \\\" a problem on the basis of pre - wired rules . The main features of human thoughts are the inherent contradictions and the ability to cope with them , something that will not be calculable due to its complexity , variety and degree of detail ( Schmitz 1992:209f . ) . These differences between humans and machines can for our purposes , i.e. the theoretical description of human - computer interaction , be legitimately reduced to the distinction between actions and operations as is done in Activity Theory . The main point of this theory for our consideration here is that communicative activities can be divided into actions which are intentional , i.e. goal - driven ; and these can be sub - divided into operations which are condition - triggered . These operations are normally learnt as actions . In the example referred to in the quotation above , the gear - switching is learnt as an action . The learner - driver is asked by the driving instructor to change gear and this becomes the goal of the learner . Once the learner - driver has performed this action a sufficient number of times , this action becomes more and more automated and in the process loses more and more of its intentionality . A proficient driver might have the goal to accelerate the car which will necessitate switching into higher gear , but this is now triggered by a condition ( the difference between engine speed and speed of the car ) . It can thus be argued that humans learn to perform complex actions by learning to perform certain operations in a certain order . Machines , on the other hand , are only made to perform certain ( sometimes rather complex ) operations . This has some bearing on our understanding of the human - computer interaction that takes place when a learner uses language - learning software . When , for instance , the spell checker is started in a word - processing package , the software certainly does not have ' proof - read ' the learner 's document . The computer just responds to the clicking of the spellchecker menu item and performs the operation of checking the strings in the document against the entries in a machine dictionary . For the computer user ( in our case a learner ) , it might look like the computer is proof - reading the document . Normally , one only realises that no \\\" proper \\\" document checking is going on if a correctly spelled word is not found in the dictionary or nonsensical alternatives are given for a simple spelling error . Person X interacts with Person Y in that he observes Person Y 's action , reasons about the likely intention for that action and reacts according to this assumed intention . This , for example , explains why many learners get just as frustrated when an answer they believe to be right is rejected by the computer as they would get if it were rejected by their tutor . Of course , an ideal computer - assisted language learning system would avoid such pitfalls and not reject a correct response or overlook an incorrect one . Since any existing system can only approximate to this ideal , researchers and developers in parser - based CALL can only attempt to build systems that can perform complex structured sequences of ( linguistic ) operations so that learners can interact meaningfully and successfully with the computer . Grammatica is able to identify parts of speech in English and French with a fair degree of accuracy and show , for example , how verbs are conjugated in different tenses and how plurals of nouns are formed . Abeill\\u00e9 A. ( 1992 ) \\\" A lexicalised tree adjoining grammar for French and its relevance to language teaching \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Abney S. ( 1997 ) \\\" Part - of - speech tagging and partial parsing \\\" . In Young S. & Bloothooft G. ( eds . ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Allen J. ( 1995 ) Natural language understanding , New York : John Benjamins Publishing Company . Alwang G. ( 1999 ) \\\" Speech recognition \\\" , PC Magazine , 10 November 1999 . Antos G. ( 1982 ) Grundlagen einer Theorie des Formulierens . Textherstellung in geschriebener und gesprochener Sprache , T\\u00fcbingen : Niemeyer . Arnold D. , Balkan . L , Meijer S. , Humphreys R. L. & Sadler L. ( 1994 ) Machine Translation : an introductory guide , Manchester : NEC Blackwell . Bellos D. ( 2011 ) Is that a fish in your ear ? Translation and the meaning of everything , Harlow : Penguin / Particular Books . Bennett P. ( 1997 ) Feature - based approaches to grammar , Manchester : UMIST , Unpublished Manuscript . Bennett P. & Paggio P. ( eds . ) ( 1993 ) Preference in EUROTRA , Luxembourg : European Commission . Bloothooft G. , Dommelen W. , van Espain C. , Green P. , Hazan V. & Wigforss E. ( eds . ) ( 1997 ) The landscape of future education in speech communication sciences : ( 1 ) analysis , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bloothooft G. , van Dommelen W. , Espain C. , Hazan V. , Huckvale M. & Wigforss E. ( eds . ) ( 1998 ) The landscape of future education in speech communication sciences : ( 2 ) proposals , Utrecht , Institute of Linguistics , University of Utrecht : OTS Publications . Bolt P. & Yazdani M. ( 1998 ) \\\" The evolution of a grammar - checking program : LINGER to ISCA \\\" , CALL 11 , 1 : 55 - 112 . Borchardt F. ( 1995 ) \\\" Language and computing at Duke University : or , Virtue Triumphant , for the time being \\\" , CALICO Journal 12 , 4 : 57 - 83 . Bowerman C. ( 1993 ) Intelligent computer - aided language learning . LICE : a system to support undergraduates writing in German , Manchester : UMIST , Unpublished doctoral dissertation . Brehony T. & Ryan K. ( 1994 ) \\\" Francophone stylistic grammar checking ( FSGC ) : using link grammars \\\" , CALL 7 , 3 : 257 - 269 . Brocklebank P. ( 1998 ) An experiment in developing a prototype intelligent teaching system from a parser written in Prolog , Manchester , UMIST , Unpublished MPhil dissertation . Brown P.F. , Della Pietra S.A. , Della Pietra V.J. & Mercer R.L. ( 1993 ) \\\" The mathematics of statistical Machine Translation : parameter estimation \\\" , Computational Linguistics 19 , 2 : 263 - 311 . Buchmann B. ( 1987 ) \\\" Early history of Machine Translation \\\" . In King M. ( ed . ) Machine Translation today : the state of the art , Edinburgh : University Press . Bull S. ( 1994 ) \\\" Learning languages : implications for student modelling in ICALL \\\" , ReCALL 6 , 1 : 34 - 39 . Bureau Lingua / DELTA ( 1993 ) Foreign language learning and the use of new technologies , Brussels : European Commission . Cameron K. ( ed . ) ( 1989 ) Computer Assisted Language Learning , Oxford : Intellect . Carson - Berndsen J. ( 1998 ) \\\" Computational autosegmental phonology in pronunciation teaching \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Chanier D. , Pengelly M. , Twidale M. & Self J. ( 1992 ) \\\" Conceptual modelling in error analysis in computer - assisted language learning \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Chen L. & Barry L. ( 1989 ) \\\" XTRA - TE : Using natural language processing software to develop an ITS for language learning \\\" . In Fourth International Conference on Artificial Intelligence and Education : 54 - 70 . Chomsky N. ( 1986 ) Knowledge of language : its nature , origin , and use , New York : Praeger . CLEF ( Computer - assisted Learning Exercises for French ) ( 1985 ) Developed by the CLEF Group , Canada , including authors at the University of Guelph , the University of Calgary and the University of Western Ontario . Also published by Cambridge University Press , 1998 : ISBN 0 - 521 - 59277 - 1 . Curzon L. B. ( 1985 ) Teaching in further education : an outline of principles and practice . London : Holt , Rinehart & Winston , ( 3rd edition ) . Davies G. ( 1988 ) \\\" CALL software development \\\" . In Jung Udo O.H .. ( ed . ) Computers in applied linguistics and language learning : a CALL handbook , Frankfurt : Peter Lang . Davies G. ( 1996 ) Total - text reconstruction programs : a brief history , Maidenhead : Camsoft . Davies S. & Poesio M. ( 1998 ) \\\" The provision of corrective feedback in a spoken dialogue system \\\" . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) ( 1991 ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Diaz de Ilarranza A. , Maritxalar M. & Oronoz M. ( 1998 ) \\\" Reusability of language technology in support of corpus studies in an ICALL environment \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Diaz de Ilarranza A. , Maritxalar A. , Maritxalar M. & Oronoz M. ( 1999 ) \\\" IDAZKIDE : An intelligent computer - assisted language learning environment for Second Language Acquisition \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 12 - 19 . Dodigovic M. ( 2005 ) Artificial intelligence in second language learning : raising error awarenes s , Clevedon : Multilingual Matters . Dokter D. & Nerbonne J. ( 1998 ) \\\" A session with Glosser - RuG \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Dokter D. , Nerbonne J. , Schurcks - Grozeva L. & Smit P. ( 1998 ) \\\" Glosser - RuG : a user study \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Ehsani F. & Knodt E. ( 1998 ) \\\" Speech technology in computer - aided language learning : strengths and limitations of a new CALL paradigm \\\" , Language Learning and Technology 2 , 1 : 45 - 60 . Ellis R. ( 1994 ) The study of Second Language Acquisition , Oxford : OUP . European Commission ( 1996 ) Language and technology : from the Tower of Babel to the Global Village , Luxembourg : European Commission . ISBN 92 - 827 - 6974 - 7 . Fechner J. ( ed . ) ( 1994 ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Feuerman K. , Marshall C. , Newman D. & Rypa M. ( 1987 ) \\\" The CALLE project \\\" , CALICO Journal 4 : 25 - 34 . Foucou P - Y. & K\\u00fcbler N. ( 1999 ) \\\" A Web - based language learning environment : general architecture \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 31 - 39 . Fum D. , Pani B. & Tasso C. ( 1992 ) \\\" Native vs. formal grammars : a case for integration in the design of a foreign language tutor \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hagen L.K. ( 1995 ) \\\" Unification - based parsing applications for intelligent foreign language tutoring systems , CALICO Journal 12 , 2 - 3 : 5 - 31 . Hamilton S. ( 1998 ) \\\" A CALL user study \\\" . In Jager S. , Nerbonne J. & van Essen A.(eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Handke J. ( 1992 ) \\\" WIZDOM : a multiple - purpose language tutoring system based on AI techniques \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Hart R. ( 1995 ) \\\" The Illinois PLATO foreign languages project \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 15 - 37 . Heift T. ( 2001 ) \\\" Error - specific and individualised feedback in a Web - based language tutoring system : Do they read it ? \\\" ReCALL 13 , 1 : 99 - 109 . Heift T. & Schulze M. ( eds . ) ( 2003 ) Error diagnosis and error correction in CALL , CALICO Journal Special Issue 20 , 3 . Heift T. & Schulze M. ( 2007 ) Errors and intelligence in CALL : parsers and pedagogues , London and New York : Routledge . Helbig G. ( 1975 ) \\\" Bemerkungen zum Problem von Grammatik und Fremdsprachenunterricht \\\" , Deutsch als Fremdsprache 6 , 12 : 325 - 332 . Higgins J. & Johns T. ( 1984 ) Computers in language learning , London : Collins . Holland M. , Maisano R. , Alderks C. & Martin J. ( 1993 ) \\\" Parsers in tutors : what are they good for ? \\\" CALICO Journal 11 , 1 : 28 - 46 . Holland M. & Fisher F.P. ( eds . ) ( 2007 ) T he path of speech technologies in Computer Assisted Language Learning : from research toward practice , London and New York : Routledge . Hutchins W.J. ( 1986 ) Machine Translation : past , present , future , Chichester : Ellis Horwood . Hutchins W.J. ( 1997 ) \\\" Fifty years of the computer and translation \\\" , Machine Translation Review 6 , October 1997 : 22 - 24 . Hutchins W.J. & Somers H.L. ( 1992 ) An introduction to Machine Translation , London : Academic Press . Jager S. ( 2001 ) \\\" From gap - filling to filling the gap : a re - assessment of Natural Language Processing in CALL \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Jager S. , Nerbonne J. & van Essen A. ( eds . ) ( 1998 ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Jones R. ( 1995 ) \\\" TICCIT and CLIPS : The early years \\\" . In Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 : 84 - 96 . Jung Udo O.H. & Vanderplank R. ( eds . ) ( 1994 ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Juozulynas V. ( 1994 ) \\\" Errors in the composition of second - year German students : an empirical study of parser - based ICALI \\\" , CALICO Journal 12 , 1 : 5 - 17 . King M. ( ed . ) ( 1987 ) Machine Translation today : the state of the art , Edinburgh : Edinburgh University Press . Klein W. & Dittmar N. ( 1979 ) Developing grammars : the acquisition of German syntax by foreign workers , Heidelberg : Springer . Klein W. & Perdue C. ( 1992 ) Utterance structure ( developing grammars again ) , Amsterdam : John Benjamins Publishing Company . Klein W. ( 1986 ) Second language acquisition , Cambridge : Cambridge University Press . Kohn K. ( 1994 ) \\\" Distributive language learning in a computer - based multilingual communication environment \\\" . In Jung Udo O.H. & Vanderplank R. ( eds . ) Barriers and bridges : media technology in language learning : proceedings of the 1993 CETall Symposium on the occasion of the 10th AILA World Congress in Amsterdam , Frankfurt : Peter Lang . Krashen S. ( 1981 ) Second language acquisition and second language learning , Oxford : Pergamon . Krashen S. ( 1982 ) Principles and practice in second language acquisition , Oxford : Pergamon . Kr\\u00fcger - Thielmann K. ( 1992 ) Wissensbasierte Sprachlernsysteme . Neue M\\u00f6glichkeiten f\\u00fcr den computergest\\u00fctzten Sprachunterricht , T\\u00fcbingen : Gunter Narr . Labrie G. & Singh L. ( 1991 ) \\\" Parsing , error diagnosis and instruction in a French tutor \\\" , CALICO Journal 9 : 9 - 25 . Last R. ( 1989 ) Artificial Intelligence techniques in language learning , Chichester : Ellis Horwood . Last R. ( 1992 ) \\\" Computers and language learning : past , present - and future ? \\\" In Butler C. ( ed . ) Computers and written texts , Oxford : Blackwell . Levin L. , Evans D. & Gates D. ( 1991 ) \\\" The Alice system : a workbench for learning and using language \\\" , CALICO Journal 9 : 27 - 55 . Levy M. ( 1997 ) Computer - assisted language learning : context and conceptualisation , Oxford : Oxford University Press . Lightbown P.M. & Spada N. ( 1993 ) How languages are learned , Oxford : Oxford University Press . Long M. ( 1991 ) \\\" Focus on form : a design feature in language teaching methodology \\\" . In de Bot K. , Coste D. , Ginsberg R. & Kramsch C. ( eds . ) Foreign language research in cross - cultural perspectives , Amsterdam : John Benjamins Publishing Company . Manning C. & Sch\\u00fctze H. ( 1999 ) Foundations of statistical Natural Language Processing , Cambridge MA , MIT Press . Matthews C. ( 1992a ) \\\" Going AI : foundations of ICALL \\\" , CALL 5 , 1 - 2 : 13 - 31 . Matthews C. ( 1992b ) Intelligent CALL ( ICALL ) bibliography , Hull : University of Hull , CTI Centre for Modern Languages . Matthews C. ( 1993 ) \\\" Grammar frameworks in Intelligent CALL \\\" , CALICO Journal 11 , 1 : 5 - 27 . Matthews C. ( 1994 ) \\\" Intelligent Computer Assisted Language Learning as cognitive science : the choice of syntactic frameworks for language tutoring \\\" , Journal of Artificial Intelligence in Education 5 , 4 : 533 - 56 . Menzel W. & Schr\\u00f6der I. ( 1999 ) \\\" Error diagnosis for language learning systems \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 20 - 30 . Michel G. ( ed . ) ( 1985 ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . Mitkov R. ( 1998 ) Language Learner 's Workbench . Unpublished paper presented at the conference on NLP in CALL , May 1998 , Manchester : UMIST . Mitkov R. & Nicolov N. ( eds . ) ( 1997 ) Recent advances in Natural Language Processing . Amsterdam : John Benjamins Publishing Company . Murphy M. , Kr\\u00fcger A. & Griesz A. , ( 1998 ) \\\" RECALL \\\" -towards a knowledge - based approach to CALL . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nagata N. ( 1996 ) \\\" Computer vs. workbook instruction in second language acquisition \\\" , CALICO Journal 14 , 1 : 53 - 75 . Nerbonne J. , Jager S. & van Essen A. ( 1998 ) Introduction . In Jager S. , Nerbonne J. & van Essen A.(eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Nirenburg S. ( ed . ) ( 1986 ) Machine Translation : theoretical and methodological issues , Cambridge : Cambridge University Press . Pijls F. , Daelmans W. & Kempen G. ( 1987 ) \\\" Artificial intelligence tools for grammar and spelling instruction , Instructional Science 16 : 319 - 336 . Pollard C. & Sag I.A. ( 1987 ) Information - Based Syntax and Semantics , Chicago : University Press . Pollard C. & Sag I.A. ( 1994 ) Head - Driven Phrase Structure Grammar , Chicago : University Press . Ramsay A. & Sch\\u00e4ler R. ( 1997 ) \\\" Case and word order in English and German \\\" . In Mitkov R. & Nicolov N. ( eds . ) Recent advances in Natural Language Processing , Amsterdam : John Benjamins Publshing Company : 15 - 34 . Ramsay A. & Schulze M. ( 1999 ) \\\" Die Struktur deutscher Lexeme \\\" , German Linguistic and Cultural Studies , Peter Lang , Submitted Manuscript . Reifler E. ( 1958 ) \\\" The Machine Translation project at the University of Washington , Seattle , Washington , USA \\\" . In Proceedings of the Eighth International Congress of Linguists , Oslo University Press : 514 - 518 . Roosmaa T. & Pr\\u00f3sz\\u00e9ky G. ( 1998 ) \\\" GLOSSER - using language technology tools for reading texts in a foreign language \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) Language teaching and language technology , Lisse : Swets & Zeitlinger . Salaberry R. ( 1996 ) \\\" A theoretical foundation for the development of pedagogical tasks in computer mediated communication \\\" , CALICO Journal 14 , 1 : 5 - 34 . Sanders R. ( 1991 ) \\\" Error analysis in purely syntactic parsing of free input : the example of German \\\" , CALICO Journal 9 , 1 : 72 - 89 . Sanders R. ( ed . ) ( 1995 ) Thirty years of Computer Assisted Language Instruction , Festschrift for John R. Russell , CALICO Journal Special Issue 12 , 4 . Schmitz U. ( 1992 ) Computerlinguistik , Opladen : Westdeutscher Verlag . Schulze M. ( 1997 ) \\\" Textana - text production in a hypertext environment \\\" , CALL 10 , 1 : 71 - 82 . Schulze M. ( 1998 ) \\\" Teaching grammar - learning grammar . Aspects of Second Language Acquisition in CALL \\\" , CALL 11 , 2 : 215 - 228 . Schulze M. ( 2001 ) \\\" Human Language Technologies in Computer Assisted Language Learning \\\" . In Chambers A. & Davies G. ( eds . ) Information and Communications Technology : a European perspective , Lisse : Swets & Zeitlinger . Schulze M. , Hamel M - J. & Thompson J. ( eds . ) ( 1999 ) Language processing in CALL , ReCALL Special Issue . Schumann J.H. & Stenson N. ( eds . ) ( 1975 ) New frontiers in second language learning , Rowley : Newbury House . Schwind C. ( 1990 ) \\\" An intelligent language tutoring system \\\" , International Journal of Man - Machine Studies 33 : 557 - 579 . Selinker L. ( 1992 ) Rediscovering interlanguage , London : Longman . Skrelin P. & Volskaja N. ( 1998 ) \\\" The application of new technologies in the development of education programs \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Smith R. ( 1990 ) Dictionary of Artificial Intelligence , London : Collins . Sp\\u00e4th P. ( 1994 ) \\\" Hypertext und Expertensysteme i m Sprachunterricht \\\" . In Fechner J. ( ed . ) Neue Wege i m computergest\\u00fctzten Fremdsprachenunterricht , Berlin : Langenscheidt . Stenzel B. ( ed . ) ( 1985 ) Computergest\\u00fctzter Fremdsprachenunterricht . Ein Handbuch , Berlin : Langenscheidt . Swartz M. & Yazdani M. ( eds . ) ( 1992 ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag . Taylor H. ( 1987 ) TUCO II . Published by Gessler Educational Software , New York . Based on earlier programs developed by Taylor H. & Haas W. at Ohio State University in the 1970s : DECU ( Deutscher Computerunterricht ) and TUCO ( Tutorial Computer ) . Taylor H. ( 1998 ) Computer assisted text production : feedback on grammatical errors made by learners of English as a Foreign Language , Manchester : UMIST , MSc Dissertation . Tschichold C. ( 1999 ) \\\" Intelligent grammar checking for CALL \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 5 - 11 . Tschichold C. , Bodme F. , Cornu E. , Grosjean F. , Grosjean L. , K\\u00fcbler N. & Tschumi C. ( 1994 ) \\\" Detecting and correcting errors in second language texts \\\" , CALL 7 , 2 : 151 - 160 . Visser H. ( 1999 ) \\\" CALLex ( Computer - Aided Learning of Lexical functions ) - a CALL game to study lexical relationships based on a semantic database \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) , Language processing in CALL , ReCALL Special Issue : 50 - 56 . Ward R. , Foot R. & Rostron A.B. ( 1999 ) \\\" Language processing in computer - assisted language learning : language with a purpose \\\" . In Schulze M. , Hamel M - J. & Thompson J. ( eds . ) Language processing in CALL , ReCALL Special Issue : 40 - 49 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . Reproduced in Locke W.N. & Booth A.D. ( eds . ) ( 1955 ) Machine translation of languages : fourteen essays , Cambridge , Mass : Technology Press of the Massachusetts Institute of Technology : 15 - 23 . Weaver W. ( 1949 ) Warren Weaver Memorandum , \\\" Translation \\\" . See \\\" 50th anniversary of machine translation \\\" , MT News International , Issue 22 ( Vol . 8 , 1 ) , July 1999 : 5 - 6 . Weischedel R. , Voge W. & James M. ( 1978 ) \\\" An artificial intelligence approach to language instruction \\\" , Artificial Intelligence 10 : 225 - 240 . Wilks Y. & Farwell D. ( 1992 ) \\\" Building an intelligent second language tutoring system from whatever bits you happen to have lying around \\\" . In Swartz M. & Yazdani M. ( eds . ) Intelligent tutoring systems for foreign language learning : the bridge to international communication , Berlin : Springer - Verlag , . Whitelock P.J. & Kilby K. ( 1995 ) Linguistic and computational techniques in Machine Translation system design , London : University College Press . Witt S. & Young S. ( 1998 ) \\\" Computer - assisted pronunciation teaching based on automatic speech recognition \\\" . In Jager S. , Nerbonne J. & van Essen A. ( eds . ) , Language teaching and language technology , Lisse : Swets & Zeitlinger . Wolff D. ( 1993 ) \\\" New technologies for foreign language teaching \\\" . In Foreign language learning and the use of new technologies , Bureau Lingua / DELTA , Brussels , European Commission . Young S. & Bloothooft G. ( eds . ) ( 1997 ) Corpus - based methods in language and speech processing , Dordrecht : Kluwer AcademicPublishers . Z\\u00e4hner C. ( 1991 ) \\\" Word grammars in ICALL \\\" . In Savolainen H. & Telenius J. ( eds . ) EuroCALL 91 proceedings , Helsinki : Helsinki School of Economics . Zech J. ( 1985 ) \\\" Methodische Probleme einer t\\u00e4tigkeitsorientierten Ausbildung des sprachlich - kommunikativen K\\u00f6nnens \\\" . In Michel G. ( ed . ) Grundfragen der Kommunikationsbef\\u00e4higung , Leipzig : Bibliographisches Institut . CALICO ( Computer Assisted Language Instruction Consortium ) : CALICO is a professional association devoted to promoting the use of technology enhanced language learning . CALICO 's sister association in Europe is EUROCALL . EUROCALL : EUROCALL is a professional association devoted to promoting the use of technology enhanced language learning , based at the University of Ulster , Northern Ireland . EUROCALL 's sister association in the USA is CALICO . ICALL is an interdisciplinary research field integrating insights from computational linguistics and artificial intelligence into computer - aided language learning . Such integration is needed for CALL systems to be able to analyze language automatically , to make them aware of language as such . This makes it possible to provide individualized feedback to learners working on exercises , to ( semi-)automatically prepare or enhance texts for learners , and to automatically create and use detailed learner models . See NLP SIG , the Special Interest Group within EUROCALL , with which ICALL closely collaborates . InSTIL : The name of a now defunct Special Interest Group dedicated to Integrating Speech Technology in Language Learning , which was set up within the EUROCALL and CALICO professional associations . A good deal of the work undertaken by InSTIL has now been taken over by ICALL and NLP SIG . NLP SIG : The name of the Special Interest Group for Natural Language Processing within the EUROCALL professional association . See ICALL , the Special Interest Group within CALICO , with which NLP SIG closely collaborates . Virtual Linguistics Campus : It includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Document last updated 29 April 2012 . This page is maintained by Graham Davies . Please cite this Web page as : Gupta P. & Schulze M. ( 2012 ) Human Language Technologies ( HLT ) . Module 3.5 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \"}",
        "_version_":1692580922897989632,
        "score":26.447718},
      {
        "id":"196d7ee8-6c21-4795-accc-22c761580d50",
        "_src_":"{\"url\": \"https://interchangeableparts.wordpress.com/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701154221.36/warc/CC-MAIN-20160205193914-00035-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Software documentation is like sex : when it is good , it is very , very good ; and when it is bad , it is better than nothing . ( Anonymous . ) There are two ways of constructing a software design : one way is to make it so simple that there are obviously no deficiencies ; the other way is to make it so complicated that there are no obvious deficiencies . ( C.A.R. Hoare ) . A computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology . Thus , programs must be written for people to read , and only incidentally for machines to execute . ( The Structure and Interpretation of Computer Programs , H. Abelson , G. Sussman and J. Sussman , 1985 . ) If you try to make something beautiful , it is often ugly . If you try to make something useful , it is often beautiful . ( Oscar Wilde ) 1 . GATE 2 is an infrastructure for developing and deploying software components that process human language . It is nearly 15 years old and is in active use for all types of computational task involving human language . GATE excels at text analysis of all shapes and sizes . From large corporations to small startups , from \\u20ac multi - million research consortia to undergraduate projects , our user community is the largest and most diverse of any system of this type , and is spread across all but one of the continents 3 . GATE is open source free software ; users can obtain free support from the user and developer community via GATE.ac.uk or on a commercial basis from our industrial partners . We are the biggest open source language processing project with a development team more than double the size of the largest comparable projects ( many of which are integrated with GATE 4 ) . More than \\u20ac 5 million has been invested in GATE development 5 ; our objective is to make sure that this continues to be money well spent for all GATE 's users . GATE has grown over the years to include a desktop client for developers , a workflow - based web application , a Java library , an architecture and a process . GATE is : . One of our original motivations was to remove the necessity for solving common engineering problems before doing useful research , or re - engineering before deploying research results into applications . Core functions of GATE take care of the lion 's share of the engineering : . modelling and persistence of specialised data structures . measurement , evaluation , benchmarking ( never believe a computing researcher who has n't measured their results in a repeatable and open setting ! ) visualisation and editing of annotations , ontologies , parse trees , etc . . a finite state transduction language for rapid prototyping and efficient implementation of shallow analysis methods ( JAPE ) . extraction of training instances for machine learning . pluggable machine learning implementations ( Weka , SVM Light , ... ) . On top of the core functions GATE includes components for diverse language processing tasks , e.g. parsers , morphology , tagging , Information Retrieval tools , Information Extraction components for various languages , and many others . GATE Developer and Embedded are supplied with an Information Extraction system ( ANNIE ) which has been adapted and evaluated very widely ( numerous industrial systems , research systems evaluated in MUC , TREC , ACE , DUC , Pascal , NTCIR , etc . ) . ANNIE is often used to create RDF or OWL ( metadata ) for unstructured content ( semantic annotation ) . GATE version 1 was written in the mid-1990s ; at the turn of the new millenium we completely rewrote the system in Java ; version 5 was released in June 2009 . We invite you to give it a try , to get involved with the GATE community , and to contribute to human language science , engineering and development . This book describes how to use GATE to develop language processing components , test their performance and deploy them as parts of other applications . In the rest of this chapter : . ( Often the process of getting a new component is as simple as typing the URL into GATE Developer ; the system will do the rest . ) The material presented in this book ranges from the conceptual ( e.g. ' what is software architecture ? ' ) to practical instructions for programmers ( e.g. how to deal with GATE exceptions ) and linguists ( e.g. how to write a pattern grammar ) . Furthermore , GATE 's highly extensible nature means that new functionality is constantly being added in the form of new plugins . Important functionality is as likely to be located in a plugin as it is to be integrated into the GATE core . This presents something of an organisational challenge . Our ( no doubt imperfect ) solution is to divide this book into three parts . Part I covers installation , using the GATE Developer GUI and using ANNIE , as well as providing some background and theory . We recommend the new user to begin with Part I . Part II covers the more advanced of the core GATE functionality ; the GATE Embedded API and JAPE pattern language among other things . Part III provides a reference for the numerous plugins that have been created for GATE . Although ANNIE provides a good starting point , the user will soon wish to explore other resources , and so will need to consult this part of the text . We recommend that Part III be used as a reference , to be dipped into as necessary . In Part III , plugins are grouped into broad areas of functionality . Software Architecture ' is used rather loosely here to mean computer infrastructure for software development , including development environments and frameworks , as well as the more usual use of the term to denote a macro - level organisational structure for software systems [ Shaw & Garlan 96 ] . Language Engineering ( LE ) may be defined as : . ... the discipline or act of engineering software systems that perform tasks involving processing human language . Both the construction process and its outputs are measurable and predictable . The literature of the field relates to both application of relevant scientific results and a body of practice . [ Cunningham 99a ] . The relevant scientific results in this case are the outputs of Computational Linguistics , Natural Language Processing and Artificial Intelligence in general . Unlike these other disciplines , LE , as an engineering discipline , entails predictability , both of the process of constructing LE - based software and of the performance of that software after its completion and deployment in applications . Some working definitions : . Computational Linguistics ( CL ) : science of language that uses computation as an investigative tool . Natural Language Processing ( NLP ) : science of computation whose subject matter is data structures and algorithms for computer processing of human language . Language Engineering ( LE ) : building NLP systems whose cost and outputs are measurable and predictable . Software Architecture : macro - level organisational principles for families of systems . In this context is also used as infrastructure . Software Architecture for Language Engineering ( SALE ) : software infrastructure , architecture and development tools for applied CL , NLP and LE . ( Of course the practice of these fields is broader and more complex than these definitions . ) In the scientific endeavours of NLP and CL , GATE 's role is to support experimentation . In this context GATE 's significant features include support for automated measurement ( see Chapter 10 ) , providing a ' level playing field ' where results can easily be repeated across different sites and environments , and reducing research overheads in various ways . GATE as an architecture suggests that the elements of software systems that process natural language can usefully be broken down into various types of component , known as resources 8 . Components are reusable software chunks with well - defined interfaces , and are a popular architectural form , used in Sun 's Java Beans and Microsoft 's . Net , for example . GATE components are specialised types of Java Bean , and come in three flavours : . LanguageResources ( LRs ) represent entities such as lexicons , corpora or ontologies ; . ProcessingResources ( PRs ) represent entities that are primarily algorithmic , such as parsers , generators or ngram modellers ; . VisualResources ( VRs ) represent visualisation and editing components that participate in GUIs . These definitions can be blurred in practice as necessary . Collectively , the set of resources integrated with GATE is known as CREOLE : a Collection of REusable Objects for Language Engineering . All the resources are packaged as Java Archive ( or ' JAR ' ) files , plus some XML configuration data . The JAR and XML files are made available to GATE by putting them on a web server , or simply placing them in the local file space . Section 1.3.2 introduces GATE 's built - in resource set . When using GATE to develop language processing functionality for an application , the developer uses GATE Developer and GATE Embedded to construct resources of the three types . This may involve programming , or the development of Language Resources such as grammars that are used by existing Processing Resources , or a mixture of both . GATE Developer is used for visualisation of the data structures produced and consumed during processing , and for debugging , performance measurement and so on . For example , figure 1.1 is a screenshot of one of the visualisation tools . GATE Developer is analogous to systems like Mathematica for Mathematicians , or JBuilder for Java programmers : it provides a convenient graphical environment for research and development of language processing software . When an appropriate set of resources have been developed , they can then be embedded in the target client application using GATE Embedded . GATE Embedded is supplied as a series of JAR files . 9 To embed GATE - based language processing facilities in an application , these JAR files are all that is needed , along with JAR files and XML configuration files for the various resources that make up the new facilities . GATE includes resources for common LE data structures and algorithms , including documents , corpora and various annotation types , a set of language analysis components for Information Extraction and a range of data visualisation and editing components . GATE supports documents in a variety of formats including XML , RTF , email , HTML , SGML and plain text . In all cases the format is analysed and converted into a single unified model of annotation . The annotation format is a modified form the TIPSTER format [ Grishman 97 ] which has been made largely compatible with the Atlas format [ Bird & Liberman 99 ] , and uses the now standard mechanism of ' stand - off markup ' . GATE documents , corpora and annotations are stored in databases of various sorts , visualised via the development environment , and accessed at code level via the framework . See Chapter 5 for more details of corpora etc . . A family of Processing Resources for language analysis is included in the shape of ANNIE , A Nearly - New Information Extraction system . These components use finite state techniques to implement various tasks from tokenisation to semantic tagging or verb phrase chunking . All ANNIE components communicate exclusively via GATE 's document and annotation resources . See Chapter 6 for more details . Other CREOLE resources are described in Part III . JAPE , a Java Annotation Patterns Engine , provides regular - expression based pattern / action rules over annotations - see Chapter 8 . The ' annotation diff ' tool in the development environment implements performance metrics such as precision and recall for comparing annotations . Typically a language analysis component developer will mark up some documents by hand and then use these along with the diff tool to automatically measure the performance of the components . See Chapter 10 . GUK , the GATE Unicode Kit , fills in some of the gaps in the JDK 's 10 support for Unicode , e.g. by adding input methods for various languages from Urdu to Chinese . See Section 3.10.2 for more details . This section gives a very brief example of a typical use of GATE to develop and deploy language processing capabilities in an application , and to generate quantitative results for scientific publication . Let 's imagine that a developer called Fatima is building an email client 11 for Cyberdyne Systems ' large corporate Intranet . In this application she would like to have a language processing system that automatically spots the names of people in the corporation and transforms them into mailto hyperlinks . A little investigation shows that GATE 's existing components can be tailored to this purpose . Fatima starts up GATE Developer , and creates a new document containing some example emails . She then loads some processing resources that will do named - entity recognition ( a tokeniser , gazetteer and semantic tagger ) , and creates an application to run these components on the document in sequence . Having processed the emails , she can see the results in one of several viewers for annotations . The GATE components are a decent start , but they need to be altered to deal specially with people from Cyberdyne 's personnel database . Therefore Fatima creates new ' cyber- ' versions of the gazetteer and semantic tagger resources , using the ' bootstrap ' tool . This tool creates a directory structure on disk that has some Java stub code , a Makefile and an XML configuration file . After several hours struggling with badly written documentation , Fatima manages to compile the stubs and create a JAR file containing the new resources . She tells GATE Developer the URL of these files 12 , and the system then allows her to load them in the same way that she loaded the built - in resources earlier on . Fatima then creates a second copy of the email document , and uses the annotation editing facilities to mark up the results that she would like to see her system producing . She saves this and the version that she ran GATE on into her serial datastore . From now on she can follow this routine : . Run her application on the email test corpus . Check the performance of the system by running the ' annotation diff ' tool to compare her manual results with the system 's results . This gives her both percentage accuracy figures and a graphical display of the differences between the machine and human outputs . Make edits to the code , pattern grammars or gazetteer lists in her resources , and recompile where necessary . To make the alterations that she requires , Fatima re - implements the ANNIE gazetteer so that it regenerates itself from the local personnel data . She then alters the pattern grammar in the semantic tagger to prioritise recognition of names from that source . This latter job involves learning the JAPE language ( see Chapter 8 ) , but as this is based on regular expressions it is n't too difficult . Eventually the system is running nicely , and her accuracy is 93 % ( there are still some problem cases , e.g. when people use nicknames , but the performance is good enough for production use ) . Now Fatima stops using GATE Developer and works instead on embedding the new components in her email application using GATE Embeddded . She takes the accuracy measures that she has attained for her system and writes a paper for the Journal of Nasturtium Logarithm Encitement describing the approach used and the results obtained . Because she used GATE for development , she can cite the repeatability of her experiments and offer access to example binary versions of her software by putting them on an external web server . This section contains an incomplete list of publications describing systems that used GATE in competitive quantitative evaluation programmes . These programmes have had a significant impact on the language processing field and the widespread presence of GATE is some measure of the maturity of the system and of our understanding of its likely performance on diverse text processing tasks . describes the performance of an SVM - based learning system in the NTCIR-6 Patent Retrieval Task . The system achieved the best result on two of three measures used in the task evaluation , namely the R - Precision and F - measure . The system obtained close to the best result on the remaining measure ( A - Precision ) . describes a cross - source coreference resolution system based on semantic clustering . It uses GATE for information extraction and the SUMMA system to create summaries and semantic representations of documents . One system configuration ranked 4th in the Web People Search 2007 evaluation . describes a cross - lingual summarization system which uses SUMMA components and the Arabic plugin available in GATE to produce summaries in English from a mixture of English and Arabic documents . Open - Domain Question Answering : . The University of Sheffield has a long history of research into open - domain question answering . GATE has formed the basis of much of this research resulting in systems which have ranked highly during independent evaluations since 1999 . The first successful question answering system developed at the University of Sheffield was evaluated as part of TREC 8 and used the LaSIE information extraction system ( the forerunner of ANNIE ) which was distributed with GATE [ Humphreys et al . 99 ] . Further research was reported in [ Scott & Gaizauskas . 00 ] , [ Greenwood et al . 02 ] , [ Gaizauskas et al . 03 ] , [ Gaizauskas et al . 04 ] and [ Gaizauskas et al . 05 ] . In 2004 the system was ranked 9th out of 28 participating groups . describes techniques for answering definition questions . The system uses definition patterns manually implemented in GATE as well as learned JAPE patterns induced from a corpus . In 2004 , the system was ranked 4th in the TREC / QA evaluations . describes a multidocument summarization system implemented using summarization components compatible with GATE ( the SUMMA system ) . The system was ranked 2nd in the Document Understanding Evaluation programmes . describe participation in the TIDES surprise language program . ANNIE was adapted to Cebuano with four person days of effort , and achieved an F - measure of 77.5 % . Unfortunately , ours was the only system participating ! describe results obtained on systems designed for the ACE task ( Automatic Content Extraction ) . Although a comparison to other participating systems can not be revealed due to the stipulations of ACE , results show 82%-86 % precision and recall . To get HTML reports from profiled processing resources , there is a new menu item in the ' Tools ' menu called ' Profiling reports ' , see chapter 11 . To deal with quality assurance of annotations , one component has been updated and two new components have been added . The annotation diff tool has a new mode to copy annotations to a consensus set , see section 10.2.1 . An annotation stack view has been added in the document editor and it allows to copy annotations to a consensus set , see section 3.4.3 . A corpus view has been added for all corpus to get statistics like precision , recall and F - measure , see section 10.3 . An annotation stack view has been added in the document editor to make easier to see overlapping annotations , see section 3.4.3 . Added an isInitialised ( ) method to gate . Gate ( ) . The ontology API ( package gate.creole.ontology has been changed , the existing ontology implementation based on Sesame1 and OWLIM2 ( package gate.creole.ontology.owlim ) has been moved into the plugin Ontology_OWLIM2 . An upgraded implementation based on Sesame2 and OWLIM3 that also provides a number of new features has been added as plugin Ontology . See Section 14.12 for a detailed description of all changes . The new Imports : statement at the beginning of a JAPE grammar file can now be used to make additional Java import statements available to the Java RHS code , see 8.6.5 . The User Guide has been amalgamated with the Programmer 's Guide ; all material can now be found in the User Guide . The ' How - To ' chapter has been converted into separate chapters for installation , GATE Developer and GATE Embedded . Other material has been relocated to the appropriate specialist chapter . Plugin names have been rationalised . Mappings exist so that existing applications will continue to work , but the new names should be used in the future . Plugin name mappings are given in Appendix B . The Montreal Transducer has been made obsolete . The UIMA integration layer ( Chapter 18 ) has been upgraded to work with Apache UIMA 2.2.2 . The JAPE debugger has been removed . Debugging of JAPE has been made easier as stack traces now refer to the JAPE source file and line numbers instead of the generated Java source code . Oracle and PostGreSQL are no longer supported . The MIAKT Natural Language Generation plugin has been removed . The Minorthird plugin has been removed . Minorthird has changed significantly since this plugin was written . We will consider writing an up - to - date Minorthird plugin in the future . A new gazetteer , Large KB Gazetteer ( in the plugin ' Gazetteer_LKB ' ) has been added , see Section 13.9 for details . gate.creole.tokeniser.chinesetokeniser.ChineseTokeniser and related resources under the plugins / ANNIE / tokeniser / chinesetokeniser folder have been removed . Please refer to the Lang_Chinese plugin for resources related to the Chinese language in GATE . A number of improvements to the benchmarking support in GATE . JAPE transducers now log the time spent in individual phases of a multi - phase grammar and by individual rules within each phase . Other PRs that use JAPE grammars internally ( the pronominal coreferencer , English tokeniser ) log the time taken by their internal transducers . A reporting tool , called ' Profiling reports ' under the ' Tools ' menu makes summary information easily available . For more details , see chapter 11 . We have added a new PR called ' Segment Processing PR ' . As the name suggests this PR allows processing individual segments of a document independently of one other . For more details , please look at the section 16.2.8 . The gate . Controller implementations provided with the main GATE distribution now also implement the gate . ProcessingResource interface . This means that an application can now contain another application as one of its components . LingPipe is a suite of Java libraries for the linguistic analysis of human language . We have provided a plugin called ' LingPipe ' with wrappers for some of the resources available in the LingPipe library . For more details , see the section 19.15 . OpenNLP provides tools for sentence detection , tokenization , pos - tagging , chunking and parsing , named - entity detection , and coreference . The tools use Maximum Entropy modelling . We have provided a plugin called ' OpenNLP ' with wrappers for some of the resources available in the OpenNLP Tools library . For more details , see section 19.16 . A new plugin has been added to provide an easy route to integrate taggers with GATE . The Tagger_Framework plugin provides examples of incorporating a number of external taggers which should serve as a starting point for using other taggers . See Section 17.4 for more details . reviews the current state of the art in email processing and communication research , focusing on the roles played by email in information management , and commercial and research efforts to integrate a semantic - based approach to email . investigates two techniques for making SVMs more suitable for language learning tasks . Firstly , an SVM with uneven margins ( SVMUM ) is proposed to deal with the problem of imbalanced training data . Secondly , SVM active learning is employed in order to alleviate the difficulty in obtaining labelled training data . The algorithms are presented and evaluated on several Information Extraction ( IE ) tasks . presents a semantic - based prototype that is made for an open - source software engineering project with the goal of exploring methods for assisting open - source developers and software users to learn and maintain the system without major effort . discusses methods of measuring the performance of ontology - based information extraction systems , focusing particularly on the Balanced Distance Metric ( BDM ) , a new metric we have proposed which aims to take into account the more flexible nature of ontologically - based applications . describes the development of a system for content mining using domain ontologies , which enables the extraction of relevant information to be fed into models for analysis of financial and operational risk and other business intelligence applications such as company intelligence , by means of the XBRL standard . describes experiments for the cross - document coreference task in SemEval 2007 . Our cross - document coreference system uses an in - house agglomerative clustering implementation to group documents referring to the same entity . describes the application of ontology - based extraction and merging in the context of a practical e - business application for the EU MUSING Project where the goal is to gather international company intelligence and country / region information . studies Japanese - English cross - language patent retrieval using Kernel Canonical Correlation Analysis ( KCCA ) , a method of correlating linear relationships between two variables in kernel defined feature spaces . ( Proceedings of the 5th International Semantic Web Conference ( ISWC2006 ) ) In this paper the problem of disambiguating author instances in ontology is addressed . We describe a web - based approach that uses various features such as publication titles , abstract , initials and co - authorship information . describes work in progress concerning the application of Controlled Language Information Extraction - CLIE to a Personal Semantic Wiki - Semper- Wiki , the goal being to permit users who have no specialist knowledge in ontology tools or languages to semi - automatically annotate their respective personal Wiki pages . discusses existing evaluation metrics , and proposes a new method for evaluating the ontology population task , which is general enough to be used in a variety of situation , yet more precise than many current metrics . describes an approach that allows users to create and edit ontologies simply by using a restricted version of the English language . The controlled language described is based on an open vocabulary and a restricted set of grammatical constructs . ( Proceedings of Fifth International Conference on Recent Advances in Natural Language Processing ( RANLP2005 ) ) It is a full - featured annotation indexing and search engine , developed as a part of the GATE . It is powered with Apache Lucene technology and indexes a variety of documents supported by the GATE . ( Proceedings of Ninth Conference on Computational Natural Language Learning ( CoNLL-2005 ) ) uses the uneven margins versions of two popular learning algorithms SVM and Perceptron for IE to deal with the imbalanced classification problems derived from IE . ( Proceedings of the 2nd European Workshop on the Integration of Knowledge , Semantic and Digital Media Technologies ( EWIMT 2005))Digital Media Preservation and Access through Semantically Enhanced Web - Annotation . describes a sentence extraction system that produces two sorts of multi - document summaries ; a general - purpose summary of a cluster of related documents and an entity - based summary of documents related to a particular person . 8 The terms ' resource ' and ' component ' are synonymous in this context . ' Resource ' is used instead of just ' component ' because it is a common term in the literature of the field : cf . the Language Resources and Evaluation conference series [ LREC-1 98 , LREC-2 00 ] . 9 The main JAR file ( gate.jar ) supplies the framework . Built - in resources and various 3rd - party libraries are supplied as separate JARs ; for example ( guk.jar , the GATE Unicode Kit . ) contains Unicode support ( e.g. additional input methods for languages not currently supported by the JDK ) . They are separate because the latter has to be a Java extension with a privileged security profile . 10 JDK : Java Development Kit , Sun Microsystem 's Java implementation . Unicode support is being actively improved by Sun , but at the time of writing many languages are still unsupported . In fact , Unicode itself does n't support all languages , e.g. Sylheti ; hopefully this will change in time . 11 Perhaps because Outlook Express trashed her mail folder again , or because she got tired of Microsoft - specific viruses and had n't heard of Netscape or Emacs . 12 While developing , she uses a file:/ ... URL ; for deployment she can put them on a web server . 13 Languages other than Java require an additional interface layer , such as JNI , the Java Native Interface , which is in C. \"}",
        "_version_":1692670548559003648,
        "score":26.37238},
      {
        "id":"fd0e1ff0-392f-4b82-9652-d41fdff4f47f",
        "_src_":"{\"url\": \"http://www.ck12.org/geometry/Trapezoids/lesson/Trapezoids-Intermediate/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701147841.50/warc/CC-MAIN-20160205193907-00339-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Indicadores . Links relacionados . Compartir . Polibits no.41 M\\u00e9xico ene . /jun . 2010 . Editorial . THIS issue of Polibits includes a selection of papers related to the topic of processing of semantic information . Processing of semantic information involves usage of methods and technologies that help machines to understand the meaning of information . These methods automatically perform analysis , extraction , generation , interpretation , and annotation of information contained on the Web , corpus , natural language systems , and other data . The special section of this issue consists of six papers dedicated to processing of semantic information . The first four papers present new proposals on processing of semantic information using corpora . The fifth paper analyses opinions . The final paper of this section use classification rules for creation of conceptual graphs . The software package developed for processing of the corpora includes routines specially written for studying speech transcripts rather than written text . For example , speaker statistics function calculates number of words , number of pauses , their duration , and average speech time of a certain speaker . Also , the paper evaluates performance using 35 participants to determine the effectiveness of the proposed dialogue system . The paper \\\" Retrieving Lexical Semantics from Multilingual Corpora \\\" \\\" proposes an unsupervised technique for building a lexical resource like WordNet used for annotation of parallel corpora . The reported results are for English , German , French , and Greek using the Europarl parallel corpus . The multilingual aspect of the approach helps in reducing the ambiguity inherent in any words / phrases in the English language . The research presented in the paper \\\" Opinion Mining using Ontologies \\\" \\\" analyses opinions using an innovative approach based on ontology fusion and matching . The proposed method allows two enterprises to share and merge the results of opinion analyses on their own products and services . The paper \\\" Learning of Chained Rules for Construction of Conceptual Graphs \\\" studies chained rules for generating new rules that can help to construction of conceptual graphs . The proposed supervised method is based on the inclusion of chained rules . The rules are defined on the basis of three elements : the role of dialing or holding the word in the sentence , the standard conceptual graph , and the definition of an object that functions as a black box of graphs . The section of regular papers includes three papers . The proposed framework detects general and flexible event which can be adapted to specific requirements and situations . Within the framework , the main aspects of event management over distributed systems are treated , such as event definition , detection , production , notification and history management . Other aspects such as event composition are also discussed . The second paper \\\" Computer System for Analysis of Holter Cardiopathy \\\" describes a medical tool related to cardiopathy studies that is available and accessible to any hospital , medical center , or doctor 's office , has accessible cost , is a user friendly and understandable . As a benefit for patients , this tool allows major accessibility of such studies . Also , this paper reports how professional staff can obtain in certain cases a possible diagnosis . Finally , the paper \\\" Prediction of Failures in IP Networks using Artificial Neural Networks \\\" presents the implementation of a system for predicting timeout failures and rejection of connections in LAN , using multilayer perceptron configuration of neural networks . It describes the implementation of the system , experiments conducted for the selection of specific parameters of the neural network , training algorithm and results . Yulia Ledeneva Research Professor , Autonomous University of the State of Mexico , Mexico \"}",
        "_version_":1692671088761241600,
        "score":26.318996},
      {
        "id":"8841b732-73d5-4b93-9c89-b71737b7f380",
        "_src_":"{\"url\": \"http://www.locatetv.com/tv/2-broke-girls/season-1/7328131\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161946.96/warc/CC-MAIN-20160205193921-00023-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Latest News . January 2012 . Our paper on Action Science Explorer was accepted by JASIST , the Journal of the American Society for Information Science and Technology . See the Publications section below for more details . July 2010 . The iOpener Workbench has been renamed Action Science Explorer ( ASE ) . Description . The goal of the iOpener project is to generate readily - consumable surveys of different scientific domains and topics , targeted to different audiences and levels . Part of this is the Action Science Explorer ( ASE ) , a new tool which presents the academic literature for a field using many different modalities : lists of articles , their full texts , automatic text summaries , and visualizations of the structure of the citation network . SocialAction provides us with powerful network analysis capabilities including force - directed citation network visualization , ranking and filtering papers by statistical measures , scatterplots of paper attributes and statistics , categorical and numerical range coloring , and automatic cluster detection . Using visualizations of the citation network we can easily find unexpected trends , clusters , gaps and outliers . Additionally , visualizations can immediately identify invalid data that is easily missed in tabular views . It integrates with Microsoft Word , OpenOffice.org , and LaTeX / BibTeX , which allows quick adding of citations to discovered articles when writing survey papers . These tools are linked together to form multiple coordinated views of the data . Clicking on a node in the citation network selects it and its corresponding paper in the reference manager , displaying its abstract , review , and other data associated with it . Moreover , when clusters of nodes are selected their papers are floated to the top of the reference manager for easy perusal . The inverse is true as well , with any paper , group , or search term selected in the reference manager highlighting the corresponding nodes in the network . There are other coordinated views that provide the user with other aspects of the field . When any node or cluster is selected , the In - Cite Text window displays the text of all incoming citations to the paper(s ) , i.e. the whole sentences from the citing papers that include the citation to the selected paper(s ) . These are displayed in a hyperlinked list that allows the user to select any one of them to show their surrounding context in the Out - Cite Text window . This window shows the full text of the paper citing one of the selected papers , with highlighting showing the selected citation sentence as well as any other sentences that include hyperlinked citations to other papers . The last view is the summary window , which can contain various multi - document summaries of a selected cluster . Using automatic summarization techniques , we can summarize all of the incoming citations to papers within that cluster , hopefully providing key insights into that research community . Action Science Explorer integrates these many components in order to provide a tool that supports rapid understanding of scientific literature . Users can analyze the network of citations between papers , identify key papers and research clusters , automatically summarize them , dig into the full text of articles to extract context , make annotations , write reviews , and finally export their findings in many of document authoring formats . We hope this infrastructure will enable users to generate readily - consumable surveys of scientific fields . Data & Summarization . As part of the iOpener project we developed the ACL Anthology Network ( AAN ) reference dataset , which includes more than 16,000 papers , each distinguished with a unique ACL ID , together with their full - texts , abstracts , and citation information . It also includes other valuable meta - data such as author affiliations , citation and collaboration networks , and various centrality measures . Moreover , we automatically extracted all sentences in the collection that cite a particular target paper in order to create a citation context for it . We used this dataset to evaluate the effectiveness of ASE through our user studies , as well as to quantitatively test our text summarization approaches . We developed several techniques to summarize scientific literature , including C - LexRank , a novel graph - based method for the automatic creation of citation - based summaries of the target papers . C - LexRank is built on top of the Clairlib library and uses network community detection to identify the main contributions of the target papers and then produces a summary that highlights these contributions . Using numerical analysis and evaluations , we showed that the surveys generated using the citation network model of scientific literature have higher quality than summaries that only use abstracts and those generated using other state of the art summarization systems . Software . ASE currently requires substantial data processing for many of the views , as much of the data required is not available from publisher databases . ASE is available only to our collaborators . Those who wish to explore publication and other databases should review the currently available commercial , open source , and research tools listed below . Publications . Books and Other One - Time Publications . Abstract : Keeping up with rapidly growing research fields , especially when there are multiple interdisciplinary sources , requires substantial effort for researchers , program managers , or venture capital investors . Current theories and tools are directed at finding a paper or website , not gaining an understanding of the key papers , authors , controversies , and hypotheses . This report presents an effort to integrate statistics , text analysis , and visualization in a multiple coordinated window environment that supports exploration . Our prototype system , Action Science Explorer ( ASE ) , provides an environment for demonstrating principles of coordination and conducting iterative usability tests of them with interested and knowledgeable users . We developed an understanding of the value of reference management , statistics , citation context extraction , natural language summarization for single and multiple documents , filters to interactively select key papers , and network visualization to see citation patterns and identify clusters . The three - phase usability study guided our revisions to ASE and led us to improve the testing methods . BibTeX : . Gove , R. , Dunne , C. , Shneiderman , B. , Klavans , J. & Dorr , B. ( 2011 ) , \\\" Evaluating visual and statistical exploration of scientific literature networks \\\" , In VL / HCC ' 11 : Proc . 2011 IEEE Symposium on Visual Languages and Human - Centric Computing . Abstract : Action Science Explorer ( ASE ) is a tool designed to support users in rapidly generating readily consumable summaries of academic literature . It uses citation network visualization , ranking and filtering papers by network statistics , and automatic clustering and summarization techniques . We describe how early formative evaluations of ASE led to a mature system evaluation , consisting of an in - depth empirical evaluation with four domain experts . The evaluation tasks were of two types : predefined tasks to test system performance in common scenarios , and user - defined tasks to test the system 's usefulness for custom exploration goals . We contribute a taxonomy of features for literature search and exploration tools and describe exploration goals identified by our participants . BibTeX : . Gove , R. ( 2011 ) , \\\" Understanding scientific literature networks : case study evaluations of integrating vizualizations and statistics \\\" . School : University of Maryland , Department of Computer Science . Abstract : Investigators frequently need to quickly learn new research domains in order to advance their research . This thesis presents five contributions to understanding how software tools help researchers explore scientific literature networks . First , this thesis summarizes capabilities in existing bibliography tools , which reveals patterns of capabilities by system type . Next , six participants in two user studies evaluate Action Science Explorer ( ASE ) , which is designed to create surveys of scientific literature and integrates visualizations and statistics . Users found document - level statistics and attribute rankings to be convenient when beginning literature exploration . The user studies also identify users ? questions when exploring academic literature , which include examining the evolution of a field , identifying author relationships , and searching for review papers . The evaluations reveal some shortcomings of ASE , and this thesis outlines improvements to ASE and lists user requirements for bibliographic exploration . Finally , I recommend strategies for evaluating bibliographic exploration tools based on experiences evaluating ASE . BibTeX : . Dunne , C. , Shneiderman , B. , Dorr , B. & Klavans , J. ( 2010 ) , \\\" iOpener Workbench : tools for rapid understanding of scientific literature \\\" , In Proc . 27th Annual Human - Computer Interaction Lab Symposium . College Park , MD . May 2010 . Posters . Dunne , C. ( 2011 ) , \\\" Interactive data visualization for rapid understanding of scientific literature \\\" , Poster at VAC ' 11 : Visual Analytics Consortium Meeting . May , 2011 . Abstract : We developed Action Science Explorer ( ASE ) , a tool designed to support users in rapidly generating easily consumable summaries of academic literature . ASE uses bibliometric lexical link mining to create a citation network for a field and context for each citation , automatic clustering and multi - document summarization techniques to extract key points , and potent network analysis and visualization tools to aid in the exploration task . These techniques provide several coordinated views of the underlying data . BibTeX : . Gove , R. ( 2011 ) , \\\" Action Science Explorer \\\" , Poster at 28th Annual Human - Computer Interaction Lab Symposium . May , 2011 . Presentations . Dunne , C. ( 2011 ) , \\\" Visual analytic tools for monitoring and understanding the emergence and evolution of innovations in science & technology \\\" , Talk at OECD - KNOWINNO workshop on measuring the use and impact of knowledge exchange mechanisms . November , 2011 . Abstract : The internet and other ICTs have had an important role in promoting the use of datamining tools for assembling , interlinking and analysing information from diverse sources . In this session we will explore how advanced data analytics tools can be used for identifying and measuring knowledge flows between different parties and to what extent they can complement more traditional data sources such as patents , publications and surveys . BibTeX : . Dunne , C. ( 2011 ) , \\\" What researchers want \\\" , Talk at STM 3rd Master Class on Developing Leadership and Innovation . November , 2011 . Dunne , C. ( 2011 ) , \\\" Action Science Explorer : interactive data visualization for rapid understanding of scientific literature \\\" , Talk at STM Annual Spring Conference . April , 2011 . Abstract : We developed Action Science Explorer ( ASE ) , a tool designed to support users in rapidly generating easily consumable summaries of academic literature . ASE uses bibliometric lexical link mining to create a citation network for a field and context for each citation , automatic clustering and multi - document summarization techniques to extract key points , and potent network analysis and visualization tools to aid in the exploration task . These techniques provide several coordinated views of the underlying data . BibTeX : . Shneiderman , B. ( 2011 ) , \\\" Information visualization : A transformative technology for ACS \\\" , Talk at American Chemical Society . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Social discovery in an information abundant world \\\" , Talk at National Federation for Advanced Information Systems : Miles Conrad Award Lecture . February , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at University of North Carolina -- Charlotte , Dept of Computer Science . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Success stories in visual analytics \\\" , Panel at VAC ' 11 : Visual Analytics Consortium Meeting . May , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at Yale University , Dept of Computer Science . April , 2011 . Shneiderman , B. ( 2011 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at Georgia Tech Graphics , Visualization & Usability Brown Bag Lunch Seminar . March , 2011 . Shneiderman , B. ( 2010 ) , \\\" Information visualization for knowledge discovery \\\" , Talk at IBM Research Center . October , 2010 . Shneiderman , B. ( 2010 ) , \\\" Distinguished lecture in computational science : Information visualization for knowledge discovery \\\" , Talk at Harvard University , School of Engineering and Applied Sciences . October , 2010 . Abstract : The task of paraphrasing is inherently familiar to speakers of all languages . Moreover , the task of automatically generating or extracting semantic equivalences for the various units of language?words , phrases , and sentences?is an important part of natural language processing ( NLP ) and is being increasingly employed to improve the performance of several NLP applications . In this article , we attempt to conduct a comprehensive and application - independent survey of data - driven phrasal and sentential paraphrase generation methods , while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research . Recent work done in manual and automatic construction of paraphrase corpora is also examined . We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation . BibTeX : . Aris , A. , Shneiderman , B. , Qazvinian , V. & Radev , D. ( 2009 ) , \\\" Visual overviews for discovering key papers and influences across research fronts \\\" , JASIST : Journal of the American Society for Information Science and Technology . Vol . 60(11 ) , pp . 2219 - 2228 . Abstract : Gaining a rapid overview of an emerging scientific topic , sometimes called research fronts , is an increasingly common task due to the growing amount of interdisciplinary collaboration . Visual overviews that show temporal patterns of paper publication and citation links among papers can help researchers and analysts to see the rate of growth of topics , identify key papers , and understand influences across subdisciplines . This article applies a novel network - visualization tool based on meaningful layouts of nodes to present research fronts and show citation links that indicate influences across research fronts . To demonstrate the value of two - dimensional layouts with multiple regions and user control of link visibility , we conducted a design - oriented , preliminary case study with 6 domain experts over a 4-month period . The main benefits were being able ( a ) to easily identify key papers and see the increasing number of papers within a research front , and ( b ) to quickly see the strength and direction of influence across related research fronts . BibTeX : . Radev , D.R. , Joseph , M.T. , Gibson , B. & Muthukrishnan , P. ( 2009 ) , \\\" A bibliometric and network analysis of the field of computational linguistics \\\" , JASIST : Journal of the American Society for Information Science and Technology . John Wiley & Sons . Abstract : The ACL Anthology is a large collection of research papers in computational linguistics . Citation data was obtained using text extraction from a collection of PDF files with significant manual post - processing performed to clean up the results . Manual annotation of the references was then performed to complete the citation network . We analyzed the networks of paper citations , author citations , and author collaborations in an attempt to identify the most central papers and authors . Also , we propose an improved method for comparing different measures of impact based on correlation . The analysis includes general network statistics , PageRank , metrics across publication years and venues , impact factor and h - index , as well as other measures . BibTeX : . Elkiss , A. , Shen , S. , Fader , A. , Erkan , G. , States , D. & Radev , D.R. ( 2008 ) , \\\" Blind men and elephants : What do citation summaries tell us about a research article ? \\\" , JASIST : Journal of the American Society for Information Science and Technology . Vol . 59(1 ) , pp . 51 - 62 . Wiley Subscription Services , Inc. , A Wiley Company . Abstract : The old Asian legend about the blind men and the elephant comes to mind when looking at how different authors of scientific papers describe a piece of related prior work . It turns out that different citations to the same paper often focus on different aspects of that paper and that neither provides a full description of its full set of contributions . In this article , we will describe our investigation of this phenomenon . We studied citation summaries in the context of research papers in the biomedical domain . A citation summary is the set of citing sentences for a given article and can be used as a surrogate for the actual article in a variety of scenarios . It contains information that was deemed by peers to be important . Our study shows that citation summaries overlap to some extent with the abstracts of the papers and that they also differ from them in that they focus on different aspects of these papers than do the abstracts . In addition to this , co - cited articles ( which are pairs of articles cited by another article ) tend to be similar . We show results based on a lexical similarity metric called cohesion to justify our claims . BibTeX : . Books and Other One - Time Publications . Abu - Jbara , A. & Radev , D. ( 2011 ) , \\\" Coherent citation - based summarization of scientific papers \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 500 - 509 . Association for Computational Linguistics . Abstract : In citation - based summarization , text written by several researchers is leveraged to identify the important aspects of a target paper . Previous work on this problem focused almost exclusively on its extraction aspect ( i.e. selecting a representative set of citation sentences that highlight the contribution of the target paper ) . Meanwhile , the fluency of the produced summaries has been mostly ignored . For example , diversity , readability , cohesion , and ordering of the sentences included in the summary have not been thoroughly considered . This resulted in noisy and confusing summaries . In this work , we present an approach for producing readable and cohesive citation - based summaries . Our experiments show that the proposed approach outperforms several baselines in terms of both extraction quality and fluency . BibTeX : . Hu , Y. , Boyd - Graber , J. & Satinoff , B. ( 2011 ) , \\\" Interactive topic modeling \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 248 - 257 . Association for Computational Linguistics . Abstract : Topic models have been used extensively as a tool for corpus exploration , and a cottage industry has developed to tweak topic models to better encode human intuitions or to better model data . However , creating such extensions requires expertise in machine learning unavailable to potential end - users of topic modeling software . In this work , we develop a framework for allowing users to iteratively refine the topics discovered by models such as latent Dirichlet allocation ( LDA ) by adding constraints that enforce that sets of words must appear together in the same topic . We incorporate these constraints interactively by selectively removing elements in the state of a Markov Chain used for inference ; we investigate a variety of methods for incorporating this information and demonstrate that these interactively added constraints improve topic usefulness for simulated and actual user sessions . BibTeX : . Muthukrishnan , P. , Radev , D. & Mei , Q. ( 2011 ) , \\\" Simultaneous similarity learning and feature - weight learning for document clustering \\\" , In HLT - TextGraphs ' 11 : Proc . TextGraphs-6 workshop on Graph - based Methods for Natural Language Processing . Portland , OR . June 2011 . , pp . 42 - 50 . Association for Computational Linguistics . Abstract : A key problem in document classification and clustering is learning the similarity between documents . Traditional approaches include estimating similarity between feature vectors of documents where the vectors are computed using TF - IDF in the bag - of - words model . However , these approaches do not work well when either similar documents do not use the same vocabulary or the feature vectors are not estimated correctly . In this paper , we represent documents and keywords using multiple layers of connected graphs . We pose the problem of simultaneously learning similarity between documents and keyword weights as an edge - weight regularization problem over the different layers of graphs . Unlike most feature weight learning algorithms , we propose an unsupervised algorithm in the proposed framework to simultaneously optimize similarity and the keyword weights . We extrinsically evaluate the performance of the proposed similarity measure on two different tasks , clustering and classification . The proposed similarity measure outperforms the similarity measure proposed by ( Muthukrishnan et al . , 2010 ) , a state - of - the - art classification algorithm ( Zhou and Burges , 2007 ) and three different baselines on a variety of standard , large data sets . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2011 ) , \\\" Exploiting phase transition in similarity networks for clustering \\\" , In AAAI ' 11 : Proc . 25th Conference on Artificial Intelligence . August 2011 . Abstract : In this paper , we model the pair - wise similarities of a set of documents as a weighted network with a single cutoff parameter . Such a network can be thought of an ensemble of unweighted graphs , each consisting of edges with weights greater than the cutoff value . We look at this network ensemble as a complex system with a temperature parameter , and refer to it as a Latent Network . Our experiments on a number of datasets from two different domains show that certain properties of latent networks like clustering coef?cient , average shortest path , and connected components exhibit patterns that are signi?cantly divergent from randomized networks . We explain that these patterns re?ect the network phase transition as well as the existence of a community structure in document collections . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2011 ) , \\\" Learning from human collective behavior to introduce diversity in summary generation \\\" , In HLT ' 11 : Proc . 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies . June 2011 . , pp . 1098 - 1108 . Association for Computational Linguistics . Abstract : We analyze collective discourse , a collective human behavior in content generation , and show that it exhibits diversity , a property of general collective systems . Using extensive analysis , we propose a novel paradigm for designing summary generation systems that reflect the diversity of perspectives seen in reallife collective summarization . We analyze 50 sets of summaries written by human about the same story or artifact and investigate the diversity of perspectives across these summaries . We show how different summaries use various phrasal information units ( i.e. , nuggets ) to express the same atomic semantic units , called factoids . Finally , we present a ranker that employs distributional similarities to build a network of words , and captures the diversity of perspectives by detecting communities in this network . Our experiments show how our system outperforms a wide range of other document ranking systems that leverage diversity . BibTeX : . Satinoff , B. & Boyd - Graber , J. ( 2011 ) , \\\" Trivial classification : What features do humans use for classification ? \\\" , In Workshop on Crowdsourcing Technologies for Language and Cognition Studies . Whidby , M. , Zajic , D. & Dorr , B.J. ( 2011 ) , \\\" Citation handling for improved summarization of scientific documents \\\" . University of Maryland , Technical Report LAMP - TR-157 , 2011 . Abstract : In this paper we present the first steps toward improving summarization of scientific documents through citation analysis and parsing . Prior work ( Mohammad et al . , 2009 ) argues that citation texts ( sentences that cite other papers ) play a crucial role in automatic summarization of a topical area , but did not take into account the noise introduced by the citations themselves . We demonstrate that it is possible to improve summarization output through careful handling of these citations . We base our experiments on the application of an improved trimming approach to summarization of citation texts extracted from Question - Answering and Dependency - Parsing documents . We demonstrate that confidence scores from the Stanford NLP Parser ( Klein and Manning , 2003 ) are significantly improved , and that Trimmer ( Zajic et al . , 2007 ) , a sentence - compression tool , is able to generate higher - quality candidates . Our summarization output is currently used as part of a larger system , Action Science Explorer ( ASE ) ( Gove , 2011 ) . BibTeX : . Lin , J. , Madnani , N. & Dorr , B. ( 2010 ) , \\\" Putting the user in the loop : Interactive maximal marginal relevance for query - focused summarization \\\" , In HLT ' 10 : Proc . Human Language Technologies : The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics . June 2010 . , pp . 305 - 308 . Association for Computational Linguistics . Abstract : This work represents an initial attempt to move beyond \\\" single - shot \\\" summarization to interactive summarization . We present an extension to the classic Maximal Marginal Relevance ( MMR ) algorithm that places a user \\\" in the loop \\\" to assist in candidate selection . Experiments in the complex interactive Question Answering ( ciQA ) task at TREC 2007 show that interactively - constructed responses are significantly higher in quality than automatically - generated ones . This novel algorithm provides a starting point for future work on interactive summarization . BibTeX : . Qazvinian , V. , Radev , D.R. & \\u00d6zg\\u00fcr , A. ( 2010 ) , \\\" Citation summarization through keyphrase extraction \\\" , In COLING ' 10 : Proc . 23rd International Conference on Computational LInguistics . August 2010 . , pp . 895 - 903 . Abstract : This paper presents an approach to summarize single scientific papers , by extracting its contributions from the set of citation sentences written in other papers . Our methodology is based on extracting significant keyphrases from the set of citation sentences and using these keyphrases to build the summary . Comparisons show how this methodology excels at the task of single paper summarization , and how it out - performs other multi - document summarization methods . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2010 ) , \\\" Identifying non - explicit citing sentences for citation - based summarization \\\" , In ACL ' 10 : Proc . 48th Annual Meeting of the Association for Computational Linguistics . July 2010 . , pp . 555 - 564 . Abstract : Identifying background ( context ) information in scientific articles can help scholars understand major contributions in their research area more easily . In this paper , we propose a general framework based on probabilistic inference to extract such context information from scientific papers . We model the sentences in an article and their lexical similarities as a Markov Random Field tuned to detect the patterns that context data create , and employ a Belief Propagation mechanism to detect likely context sentences . We also address the problem of generating surveys of scientific papers . Our experiments show greater pyramid scores for surveys generated using such context information rather than citation sentences alone . BibTeX : . Mohammad , S. , Dunne , C. & Dorr , B. ( 2009 ) , \\\" Generating high - coverage semantic orientation lexicons from overtly marked words and a thesaurus \\\" , In EMNLP ' 09 : Proc . 2009 conference on Empirical Methods in Natural Language Processing . Morristown , NJ , USA . August 2009 . , pp . 599 - 608 . Association for Computational Linguistics . Abstract : Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words . A number of approaches have been proposed for creating such lexicons , but they tend to be computationally expensive , and usually rely on significant manual annotation and large corpora . Most of these methods use WordNet . In contrast , we propose a simple approach to generate a high - coverage semantic orientation lexicon , which includes both individual words and multi - word expressions , using only a Roget - like thesaurus and a handful of affixes . Further , the lexicon has properties that support the Polyanna Hypothesis . Using the General Inquirer as gold standard , we show that our lexicon has 14 percentage points more correct entries than the leading WordNet - based high - coverage lexicon ( SentiWordNet ) . In an extrinsic evaluation , we obtain significantly higher performance in determining phrase polarity using our thesaurus - based lexicon than with any other . Additionally , we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above . BibTeX : . Human Language Technologies : The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics . Stroudsburg , PA , USA . , pp . 584 - 592 . Association for Computational Linguistics . Abstract : The number of research publications in various disciplines is growing exponentially . Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material . In this paper we present the first steps in producing an automatically generated , readily consumable , technical survey . Specifically we explore the combination of citation information and summarization techniques . Even though prior work ( Teufel et al . , 2006 ) argues that citation text is unsuitable for summarization , we show that in the framework of multi - document survey creation , citation texts can play a crucial role . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2009 ) , \\\" The evolution of scientific title networks \\\" , In ICWSM ' 09 : Proc . 2009 International AAAI Conference on Weblogs and Social Media poster session . Abstract : In spite of enormous previous efforts to model the growth of various networks , there have only been a few works that successfully describe the evolution of latent networks . In a latent network edges do not represent interactions between nodes , but show some proximity values . In this paper we analyze the structure and evolution of a specific type of latent networks over time by looking at a wide range of document similarity networks , in which scientific titles are nodes and their similarities are weighted edges . We use scientific papers as the corpora in order to determine the behavior of authors in choosing words for article titles . The aim of our work is to see whether term selection for titles depends on earlier published titles . BibTeX : . Radev , D.R. , Muthukrishnan , P. & Qazvinian , V. ( 2009 ) , \\\" The ACL Anthology Network corpus \\\" , In NLPIR4DL ' 09 : Proc . ACL - IJCNLP 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries . Stroudsburg , PA , USA . , pp . 54 - 61 . Association for Computational Linguistics . Abstract : We introduce the ACL Anthology Network ( AAN ) , a manually curated networked database of citations , collaborations , and summaries in the field of Computational Linguistics . We also present a number of statistics about the network including the most cited authors , the most central collaborators , as well as network statistics about the paper citation , author citation , and author collaboration networks . BibTeX : . Aris , A. ( 2008 ) , \\\" Visualizing and exploring networks using Semantic Substrates \\\" . School : University of Maryland , Department of Computer Science . Abstract : Visualizing and exploring network data has been a challenging problem for HCI ( Human - Computer Interaction ) Information Visualization researchers due to the complexity of representing networks ( graphs ) . Research in this area has concentrated on improving the visual organization of nodes and links according to graph drawing aesthetics criteria , such as minimizing link crossings and the longest link length . Semantic substrates offer a different approach by which node locations represent node attributes . Users define semantic substrates for a given dataset according to the dataset characteristics and the questions , needs , and tasks of users . The substrates are typically 2 - 5 non - overlapping rectangular regions that meaningfully lay out the nodes of the network , based on the node attributes . Link visibility filters are provided to enable users to limit link visibility to those within or across regions . The reduced clutter and visibility of only selected links are designed to help users find meaningful relationships . Applications include legal precedent ( with court cases citing one another ) , food - web ( predator - prey relationships ) data , scholarly paper citations , and U. S. Senate voting patterns . These case studies , which had networks of up to 4,296 nodes and 16,385 links , helped refine NVSS and the semantic substrate approach , as well as understand its limitations . The case study approach enabled users to gain insights and form hypotheses about their data , while providing guidance for NVSS revisions . The proposed guidelines for semantic substrate definitions are potentially applicable to other datasets such as social networks , business networks , and email communication . NVSS appears to be an effective tool because it offers a user - controlled and understandable method of exploring networks . The main contributions of this dissertation include the extensive exploration of semantic substrates , implementation of software to define substrates , guidelines to design good substrates , and case studies to illustrate the applicability of the approach to various domains and its benefits . BibTeX : . Sixth International Language Resources and Evaluation . May 2008 . , pp . 1755 - 1759 . European Language Resources Association ( ELRA ) . Dorr , B. , Mohammad , S. & Onyshkevych , B. ( 2008 ) , \\\" From linguistic annotations to knowledge objects \\\" , In SKDOU ' 08 : Proc . Symposium on Semantic Knowledge Discovery , Organizaiton and Use . November 2008 . Klavans , Judith , Shneiderman , Ben & others ( 2008 ) , \\\" Motivating interactive summarizations : User guided exploration of new domains \\\" . Mohammad , S. , Dorr , B.J. & Hirst , G. ( 2008 ) , \\\" Computing word - pair antonymy \\\" , In EMNLP ' 08 : Proc . 2008 conference on Empirical Methods in Natural Language Processing . October 2008 . , pp . 982 - 991 . Association for Computational Linguistics . Abstract : Knowing the degree of antonymy between words has widespread applications in natural language processing . Manually - created lexicons have limited coverage and do not include most semantically contrasting word pairs . We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus . The approach is evaluated on a set of closest - opposite questions , obtaining a precision of over 80 % . Along the way , we discuss what humans consider antonymous and how antonymy manifests itself in utterances . BibTeX : . November 2008 . At the heart of our summarization system is Trimmer , which generates multiple alternative compressed versions of the source sentences that act as candidate sentences for inclusion in the summary . For the ? rst time , we investigated the use of automatically generated antonym pairs for both text summarization and recognizing textual entailment . The UMD summaries for the opinion task were especially effective in providing non - redundant information ( rank 3 out of a total 19 submissions ) . More coherent summaries resulted when using the antonymy feature as compared to when not using it . On the RTE task , even when using only automatically generated antonyms the system performed as well as when using a manually compiled list of antonyms . BibTeX : . Mohammad , S. , Dorr , B. & Hirst , G. ( 2008 ) , \\\" Towards antonymy - aware natural language applications \\\" , In SKDOU ' 08 : Proc . Symposium on Semantic Knowledge Discovery , Organizaiton and Use . November 2008 . Muthukrishnan , P. , Gerrish , J. & Radev , D.R. ( 2008 ) , \\\" Detecting multiple facets of an event using graph - based unsupervised methods \\\" , In COLING ' 08 : Proc . 22nd International Conference on Computational Linguistics . August 2008 . , pp . 609 - 616 . Abstract : We propose a new unsupervised method for topic detection that automatically identifies the different facets of an event . We use pointwise Kullback - Leibler divergence along with the Jaccard coefficient to build a topic graph which represents the community structure of the different facets . The problem is formulated as a weighted set cover problem with dynamically varying weights . The algorithm is domain - independent and generates a representative set of informative and discriminative phrases that cover the entire event . We evaluate this algorithm on a large collection of blog postings about different news events and report promising results . BibTeX : . Qazvinian , V. & Radev , D.R. ( 2008 ) , \\\" Scientific paper summarization using citation summary networks \\\" , In COLING ' 08 : Proc . 22nd International Conference on Computational Linguistics . Stroudsburg , PA , USA . , pp . 689 - 696 . Association for Computational Linguistics . Abstract : Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study . One possible way to overcome this problem is to summarize a scientific topic . In this paper , we propose a model of summarizing a single article , which can be further used to summarize an entire topic . Our model is based on analyzing others ' viewpoint of the target article 's contributions and the study of its citation summary network using a clustering approach . BibTeX : . Shneiderman , B. ( 2008 ) , \\\" Research agenda : Visual overviews for exploratory search \\\" , In National Science Foundation Workshop on Information Seeking Support Systems . June 2008 . Abstract : Exploratory search is necessary when users knowledge of the domain is incomplete or when initial user goals do not match available data or metadata that is the basis for search indexing attributes . Such mismatches mean that users need to learn more in order to develop a better understanding of the domain or to revise their search goals . Exploratory search processes may take weeks or months , so interfaces that support prolonged exploration are necessary . The attraction of exploratory search is that users can take on more ambitious goals that require substantial learning and creative leaps to bridge the gaps between what they know and that they seek . BibTeX : . Shneiderman , Ben , Aris , Aleks & others ( 2008 ) , \\\" Visual summarization of topic evaluation and topic dependencies \\\" . Joseph , M.T. & Radev , D.R. ( 2007 ) , \\\" Citation analysis , centrality , and the ACL Anthology \\\" . University of Michigan . Department of Electrical Engineering and Computer Science , Technical Report CSE - TR-535 - 07 , 2007 . Abstract : We analyze the ACL Anthology citation network in an attempt to identify the most ? central ? papers and authors using graph - based methods . Citation data was obtained using text extraction from the library of PDF files with some post - processing performed to clean up the results . Manual annotation of the references was then performed to complete the citation network . The analysis compares metrics across publication years and venues , such as citations in and out . The most cited paper , central papers , and papers with the highest impact factor are also established . BibTeX : . Radev , D.R. , Hodges , M. , Fader , A. , Joseph , M. , Gerrish , J. , Schaller , M. , dePeri , J. & Gibson , B. ( 2007 ) , \\\" CLAIRLIB documentation v1.03 \\\" . University of Michigan . Department of Electrical Engineering and Computer Science , Technical Report CSE - TR-536 - 07 , 2007 . \"}",
        "_version_":1692580798769659904,
        "score":25.788465}]
  }}
