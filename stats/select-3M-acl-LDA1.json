{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"linguistic document retrieval system english information problem test instance description pairs simple morphological language evidence source cfxxx inference task prosodic",
      "fl":"*,score"}},
  "response":{"numFound":1519677,"start":0,"maxScore":25.44051,"numFoundExact":true,"docs":[
      {
        "id":"e326311c-beec-42a9-b2a8-0360d587a93c",
        "_src_":"{\"url\": \"http://www.walkoffwalk.com/2010/07/angry-old-hall-of-famer-now-ha.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166141.55/warc/CC-MAIN-20160205193926-00223-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Natural Language Processing in Textual Information Retrieval and Related Topics . Mari Vallez ; Rafael Pedraza - Jimenez . Citaci\\u00f3n recomendada : Mari Vallez ; Rafael Pedraza - Jimenez . Natural Language Processing in Textual Information Retrieval and Related Topics [ en linea]. \\\" Hipertext.net \\\" , num . \\\" Natural Language Processing \\\" ( NLP ) as a discipline has been developing for many years . It was formed in 1960 as a sub - field of Artificial Intelligence and Linguistics , with the aim of studying problems in the automatic generation and understanding of natural language . At first its methods were widely accepted and successful . However , when applied in controlled environments and with a generic vocabulary , many problems arose . Among those problems were polysemy and synonymy . In recent years contributions to this field have improved substantially , allowing for the processing of huge amounts of textual information with an acceptable level of efficacy . An example of this is the application of these techniques as an essential component in web search engines , in automated translation tools or in summary generators [ Baeza - Yates , 2004]. Problems with natural language processing : linguistic variation and ambiguity . Natural language , understood as a tool that people use to express themselves , has specific properties that reduce the efficacy of textual information retrieval systems . These properties are linguistic variation and ambiguity . By linguistic variation we mean the possibility of using different words or expressions to communicate the same idea . Linguistic ambiguity is when a word or phrase allows for more than one interpretation . Both phenomenons affect the information retrieval process , even though in different ways . Linguistic variation provokes document silence , that is , the omission of relevant documents that fulfil information needs , because the same terms were not used as those found in the document . Ambiguity , on the other hand , implies document noise , or the inclusion of non - meaningful documents , since documents were retrieved that used the same term but with a different meaning . These characteristics make automated language processing considerably difficult . The following is a set of examples that show the repercussions of these phenomena in information retrieval : . At a morphological level , the same word may play different morph - syntactic roles relative to the context in which they appear , causing ambiguity problems ( example 1 ) . Example 1 . A notebook was the present that his wife gave him when all of us were present at the party . In this case , the word \\\" present \\\" acts both as an adjective and noun , and with different meanings . At a syntactic level , focusing on the study of established relations between words to form larger linguistic units , phrases and sentences , ambiguities are produced as a consequence of the possibility of associating a sentence with more than one syntactic structure . On the other hand , this variation supposes the possibility of expressing the same idea , but changing the order of the sentence 's syntactic structure . ( example 2 ) . Example 2 . He ate the chocolates on the plane . This example could mean that \\\" He ate the chocolates that were in the plane \\\" or that \\\" He ate the chocolates when he was flying in the plane . \\\" At a semantic level we study the meaning of a word and sentence by studying the meaning of each of the words in it . Ambiguity is produced because a word can have one or various meanings , which is known as polysemy ( example 3 ) . Example 3 . Paul was reading a newspaper in the bank . The term \\\" bank \\\" could refer to a financial institution or a mound . And we must also keep in mind lexical variation which refers to the possibility of using different terms for the same meaning , that is , a synonymy ( example 4 ) : . Example 4 : Car / Auto / Automobile . At a pragmatic level , based on a language 's relationship to its context , we often can not use a literal and automated interpretation of the terms used . In specific circumstances , the sense of the words in the sentence must be interpreted at a level that includes the context in which the sentence is found . ( example 5 ) . Example 5 . Give me a break . Here we are asking for rest from work , or we could be asking the perceiver to leave us alone . Another important topic is ambiguity provoked by an anaphora , for example , the presence of pronouns and adverbs that refer to something that was previously mentioned ( example 6 ) . Example 6 . It was terrible for him she had not to manipulate it . Who is he ? And she ? What was not manipulated ? It is impossible to understand this sentence out of context . All of these examples demonstrate the complexity of language , and that any automated processing is not easy or obvious . Natural Language processing in textual information retrieval . As the reader has probably already deduced , the complexity associated with natural language is especially key when retrieving textual information [ Baeza - Yates , 1999 ] to satisfy a user 's information needs . In other words , a textual information retrieval system carries out the following tasks in response to a user 's query ( image 1 ) : . Indexing the collection of documents : in this phase , NLP techniques are applied to generate an index containing document descriptions . Normally each document is described through a set of terms that , in theory , best represents its content . When a user formulates a query , the system analyses it , and if necessary , transforms it with the hope of representing the user 's information needs in the same way as the document content is represented . The system compares the description of each document with that of the query , and presents the user with those documents whose descriptions are closest to the query description . The results are usually listed in order of relevancy , that is , by the level of similarity between the document and query descriptions . As of now there are no NLP techniques that allow us to extract a document 's or query 's meaning without any mistakes . In fact , the scientific community is divided on the procedure to follow in reaching this goal . In the following section we will explain the functions and peculiarities of the two key approaches to natural language processing : a statistical approach and a linguistic focus . Both proposals differ considerably , even though in practice natural language processing systems use a mixed approach , combining techniques from both focuses . Statistical processing of natural language . Statistical processing of natural language [ Manning , 1999 ] represents the classical model of information retrieval systems , and is characterised from each document 's set of key words , known as the terms index . This is a very simple focus based on the \\\" bag of words . \\\" In this approach , all words in a document are treated as its index terms . Moreover , each term is assigned a weight in function of its importance , usually determined by its appearance frequency within the document . This way the word 's order , structure , meaning , etc , are not taken into consideration . These models are then limited to pairing the documents ' words with that of the query 's . Its simplicity and efficacy has become the most commonly used contemporary models in textual information retrieval systems . This document processing model involves the following stages : . a)Document pre - processing : fundamentally consisting in preparing the documents for its parameterisation , eliminating any elements considered as superfluous . b)Parameterisation : a stage of minimal complexity once the relevant terms have been identified . This consists in quantifying the document 's characteristics ( that is , the terms ) . Below we will illustrate their function using this paper 's first paragraph as an example , assuming that it is XML tagged . So the document on which we would apply the pre - processed and parameterisation techniques would be the following : . ( Example 6 ) . Stemming terms is a linguistic process that attempts to determine the base ( lemma ) of each word in a text . Its aim is to reduce a word to its root , so that the key words in a query or document are represented by their roots instead of the original words . The lemma of word is its basic form along with its inflected forms . For example , \\\" inform \\\" could be the lemma of \\\" information \\\" or \\\" inform . \\\" However , these algorithms have the -inconvenience of sometimes not grouping words that should be grouped , and vice versa : erroneously presenting words as equals . Parametrising documents consists in assigning a weight to each one of the relevant terms associated to a document . A term 's weight is usually calculated as a function of its appearance frequency in the document , indicating the importance of these terms as the document 's content description ( example 8) . Example 8 . Fragment of a parametrised document ( see how the frequencies of each term changes as the quantification of the remaining terms in the document continues ) . One of the most often used methods to estimate the importance of a term is the TF.IDF system ( Term Frequency , Inverse Document Frequency ) . It is designed to calculate the importance of a term relative to its appearance frequency in a document , but as a function of the total appearance frequency for all of the corpus ' documents . That is , the fact that a term appears often in one document is indicative that that term is representative of the content , but only when that term does not appear frequently in all documents . If it appeared frequently in all documents , it would not have any discriminatory value ( for example , it would be absurd to represent the content of a document in a recipe database by the frequency of the word food , even though it appears often ) . Finally , and as we have already mentioned , we must describe two commonly used techniques in the statistical processing of natural language : . a ) Detecting N - Grams : this consists in identifying words that are usually together ( compound words , proper nouns , etc . ) to be able to process them as a single conceptual unit . This is usually done by estimating the probability of two words that are often together make up a single term ( compound ) . These techniques attempt to identify compound terms such as \\\" accommodation service \\\" or \\\" European Union . \\\" b ) Stopwords lists : a list of empty words in a terms list ( prepositions , determiners , pronouns , etc . ) considered to have little semantic value , and are eliminated when found in document , leaving them out of the terms index to be analysed . Deleting all of these terms avoids document noise problems and saves on resources , since in documents few elements are repeated frequently . Linguistic processing of natural language . This approach is based on the application of different techniques and rules that explicitly encode linguistic knowledge [ Sanderson , 2000]. The documents are analysed through different linguistic levels ( as previously mentioned ) by linguistic tools that incorporate each level 's own annotations to the text . Below we show the different steps to take in a linguistic analysis of documents , even though not all systems use them . The morphological analysis is performed by taggers that assign each word to a grammatical category according to the morphological characteristics found . After having identified and analysed the words in a text , the next step is to see how they are related and used together in making larger grammatical units , phrases and sentences . Therefore a syntax analysis of the text is performed . This is when parsers are applied : descriptive formalism that demonstrate the text 's syntax structure . The techniques used to apply and create parsers vary and depend on the aim of the syntax analysis . For information retrieval it is often used for a superficial analysis aiming to only identify the most meaningful structures : nominal sentences , verbal and prepositional sentence , values , etc . This level of analysis is usually used to optimise resources and not slow down the system 's response . From the text 's syntax structure , the next aim is to obtain the meaning of the sentences within it . The aim is to obtain the sentence 's semantic representation from the elements that make it up . One of the most often used tools in semantic processing is the lexicographic database WordNet . This is an annotated semantic lexicon in different languages made up of synonym groups called synsets which provide short definitions along with the different semantic relationships between synonym groups . There are different fields of research relative to information retrieval and natural language processing that focus on the problem from other perspectives , but whose final aim is to facilitate information access . Information extraction consists in extracting entities , events and existing relationships between elements in a text or group of texts . This is one way of efficiently accessing large documents since it extracts parts of the document shown in its content . The information generated can be used as knowledge and ontology databases . Summary generators compress a text 's most relevant information . The techniques most often used vary according to the rate of compression , the summary 's aim , the text 's genre and language ( or languages ) of the original text , among other factors . Question answering aims to give a specific response to the formulated query . The information needs must be well - defined : dates , places , etc . Here the processing of natural language attempts to identify the type of response to provide ( by disambiguating the question , analysing the set restrictions , and the use of information extraction techniques . These systems are considered to be the potential successors to the current information retrieval systems . START natural language system is an example of one of these systems . Retrieving multi - language information involves the possibility of retrieving information even though the question and/or documents are in different languages . Automatic translators are used on the documents and/or questions , or the use of interlingua mechanisms to interpret documents . These systems are still a great challenge to researches since they combine two key aspects of the Web 's current context : retrieving information and processing multilingual information . Finally , we must cite the automatic text classification techniques , which automatically assign a set of documents into categories within predefined classifications . The correct description of the document 's characteristics ( usually through the use of statistical techniques -- pre - processed and parametrisation ) strongly influences the quality of the grouping / categorization by these techniques . Conclusions . With the aim of understanding the current Natural Language Process , we have concisely defined the key concepts and techniques associated with this field , along with some simple examples to help the reader better understand . Moreover , we have shown how , despite its years of experience , NLP is a very live and developing field of linguistics , with its many challenges still to overcome due to natural language 's ambiguity . We have paid special attention to the differences between statistical and linguistic methods in natural language processing . Even the scientific communities that support each approach are usually at odds , and NLP is often applied by using a combination of techniques from both approaches . Our experience in this field has made us conclude that it is not possible to claim one approach is better than the other ; this even includes the use of a mixed approach . Relative to information retrieval , the statistical processing techniques are more often used in commercial applications . However , in our opinion , the behaviour and efficacy of the different NLP techniques vary depending on the nature of the task at hand , the type of documents to analyse and the computational cost to assume . Overall , we can deduce the need to continue working on this with the hope of creating new techniques or focuses that will help us overcome these existing shortcomings . This is the only way we can finally reach what seems like the impossible dream of automatic comprehension of natural language . Finally , and as an Annexe ( Annexe 1 ) , we have described some of the unique aspects of processing Spanish , including the mention of some of the key initiatives developed to process this language . Acknowledgments . This project has been partially financed by the Ministerio de Educaci\\u00f3n y Ciencia ( Spain ) as part of the HUM2004 - 03162/FILO project . References . [ Allan , 1995 ] J. Allan [ et al.]. Recent experiments with INQUERY , in : HARMAN , D.K. from : The Fourth Text Retrieval Conference , NIST SP 500 - 236 , Gaithersburg , Maryland . [ Baeza - Yates , 1999 ] Baeza - Yates , R. and Ribeiro - Neto , Berthier . Modern information retrieval . Addison - Wesley Longman . [ Baeza - Yates , 2004 ] Baeza - Yates , R. ( 2004 ) . Challenges in the Interaction of Information Retrieval and Natural Language Processing . in Proc . 5 th International Conference on Computational Linguistics and Intelligent Text Processing ( CICLing 2004 ) , Seoul , Corea . Lecture Notes in Computer Science vol . 2945 , pages 445 - 456 , Springer . [ Carmona , 1998 ] J. Carmona [ et al.]. An environment for morphosyntactic processing of unrestricted spanish text . In : LREC 98 : Procedings of the First International Conference on Language Resources and Evaluation , Granada , Espa\\u00f1a . [ Figuerola , 2000 ] C. G. Figuerola . La investigaci\\u00f3n sobre recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol . In : C.Gonzalo Garc\\u00eda and V. Garc\\u00eda Yedra , editors , Documentaci\\u00f3n , Terminolog\\u00eda y Traducci\\u00f3n , pages 73 - 82 . S\\u00edntesis , Madrid . [ Figuerola , 2004 ] C. G. Figuerola [ et al.]. La recuperaci\\u00f3n de informaci\\u00f3n en espa\\u00f1ol y la normalizaci\\u00f3n de t\\u00e9rminos , in : Revista Iberoamericana de Inteligencia Artificial , vol VIII , n\\u00ba 22 , pp . 135 - 145 . [ Manning , 1999 ] Manning , C. D. and Sch\\u00fctze , H. ( 1999 ) . Foundations of statistical natural language processing . MIT Press . Cambridge , MA : May , p. 680 . [ Rodr\\u00edguez , 1996 ] S. Rodr\\u00edguez y J. Carretero . A formal approach to Spanish morphology : the COES tools . En : XII Congreso de la SEPLN , Sevilla , pp . 118 - 126 . [ Sanderson , 2000 ] Sanderson , M. ( 2000 ) . Retrieving with good sense , In : Information Retrieval , 2 , 49 - 69 . [ Santana , 1997 ] O. Santana [ et al.]. Flexionador y lematizador autom\\u00e1tico de formas verbales , In : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XIX(2 ) , pp . 229 - 282 . [ Santana , 1999 ] O. Santana [ et al.]. Flexionador y lematizador de formas nominales , en : Ling\\u00fc\\u00edstica Espa\\u00f1ola Actual , XXI(2 ) , pp . 253 - 297 . [ Strzalkowski , 1999 ] Strzalkowski , T. ( 1999 ) . Natural Language Information Retrieval . Netherlands : Kluwer Academic Publishers . As an annexe we will go on to show some of the most important characteristics of natural language processing in Spanish : . Empty words lists [ Figuerola , 2000 ] : creating these tools for Spanish is quite a challenge , mainly due to the lack of collections and statistical studies in Spanish that would advise for or against its use . Furthermore , creating these lists varies in function of whether or not they are used in processing general or specific information . If our corpus is not field specific , then the list of empty words should mainly include : determiners , pronouns , adverbs , prepositions and conjunctions . But if the information to be analysed is field specific , this list should be modified and/or extended by an expert of the field in question . We must also mention that many researches have noted the advantage of using fixed expressions as elements in empty words lists . Specifically [ Allan , 1995 ] recommends using a short list of \\\" empty phrases \\\" : indication of , which are , how are , information on . Stemming techniques : the majority of information retrieval techniques use frequency counts of terms found in the documents and queries . This implies the need to standardise these terms in order for the count to be carried out properly , taking into consideration those terms with the same lemma or root . There are various lemma and morphological analysers for Spanish . Finally , it is worth noting that experiments of this kind of algorithms in Spanish has shown that standardising terms through stemming techniques provides improved results . The S - stemmer algorithm comes up with surprising results . This algorithm is very simple and basically it simply reduces plural words to their singular form . In its original version ( for English ) , this algorithm only eliminates the last \\\" s \\\" of each word . For Spanish , this algorithm could be reinforced by including plural forms of nouns and adjectives with consonants , thus ending in \\\" es . \\\" Eliminating the \\\" es \\\" suffixes could produce inconsistencies with words ending in \\\" e \\\" in their singular form , which would require the elimination of \\\" e \\\" endings . We have also shown that eliminating gender specific \\\" a \\\" or \\\" o \\\" endings improves results . The greatest advantage of this algorithm is its simplicity . However , the inconvenience is that the S - stemmer is incapable of distinguishing nouns and adjectives from other grammatical categories , thus applying it to all words ; it also does not distinguish between irregular plural forms . But on the other hand , by treating all words in the same form , it does not introduce additional noise . University Pompeu Fabra . Department of Communication . DigiDoc Research Group Campus de la Comunicaci\\u00f3 . Roc Boronat , 138 , office 53804 . Barcelona 08018 Tel : 93 542 13 11 . E - mail : cristofol.rovira@upf.edu Legal deposit B-49106 - 2002 - ISSN 1695 - 5498 \"}",
        "_version_":1692669059566403584,
        "score":25.44051},
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":22.10058},
      {
        "id":"97fa5318-97b3-459e-90b9-be280724a2bf",
        "_src_":"{\"url\": \"https://www.opensecrets.org/lobby/billsum.php?id=hr5731-112\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163421.31/warc/CC-MAIN-20160205193923-00098-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Studies the relation between imaginative texts and the culture surrounding them . Emphasizes ... . Studies the relation between imaginative texts and the culture surrounding them . Emphasizes ways in which imaginative works absorb , reflect , and conflict with reigning attitudes and world views . Instruction and practice in oral and written communication . Topic for Fall : Ethical Interpretation . Topic for Spring : Women Reading , Women Writing . The course examines the earliest emergence of stories about King Arthur and the Knights of the Round Table in the context of the first wave of British Imperialism and the expanded powers of the Catholic Church during the twelfth and thirteenth centuries . The morphology of Arthurian romance will be set off against original historical documents and chronicle sources for the English conquests in Brittany , Wales , Scotland , and Ireland to understand the ways in which these new attitudes towards Empire were being mythologized . Authors will include Bede , Geoffrey of Monmouth , Chr\\u011a\\u0160tien de Troyes , Marie de France , Gerald of Wales , together with some lesser known works like the Perilous Graveyard , the Knight with the Sword , and Perlesvaus , or the High History of the Holy Graal . Special attention will be paid to how the narrative material of the story gets transformed according to the particular religious and political agendas of each new author . Biology 1B : General introduction to plant development , form , and function ; population genetics , ... . Biology 1B : General introduction to plant development , form , and function ; population genetics , ecology , and evolution . Intended for students majoring in the biological sciences , but open to all qualified students . Biology 1B : This course provides a general introduction to plant development , form , ... . Biology 1B : This course provides a general introduction to plant development , form , and function ; population genetics , ecology , and evolution . Intended for students majoring in the biological sciences , but open to all qualified students . Biology 1B : General introduction to plant development , form , and function ; population genetics , ... . Biology 1B : General introduction to plant development , form , and function ; population genetics , ecology , and evolution . Intended for students majoring in the biological sciences , but open to all qualified students . This online interface processes MSA using four different modes . The ' Resolve ' mode ... . This online interface processes MSA using four different modes . The ' Resolve ' mode provides tokenization and morphological analysis of the inserted text while the ' Inflect ' mode lets users inflect words into the forms required by context . The ' Derive ' mode allows users to derive words of similar meaning but different grammatical category . The ' Lookup ' mode can lookup lexical entries by the citation form and nests of entries by the root ; it also allows users to search in the English translations . Authored by Mohammed Jiyad , Professor of Arabic at Mount Holyoke College , this ... . Authored by Mohammed Jiyad , Professor of Arabic at Mount Holyoke College , this 69 page document contains detailed instructional notes on \\\" syntactic , morphological & phonological rules for novice & intermediate levels of proficiency . \\\" Available in PDF and Word formats , the document details grammar rules and provides examples for each rule . Its focus is entirely on grammar , and presupposes that students have intermediate - level , technical experience with the Arabic language . Students are introduced to the streaking technique to isolate colonies from a ... . Students are introduced to the streaking technique to isolate colonies from a mixed liquid bacteria culture and to several standard microbiological identification tests . The morphology of isolated colonies and the morphology and Gram stain reaction of the cells of each colony are examined . Each cell type is examined for the ability to use glucose as a carbon source , ability to form endospores , oxygen requirements for growth , and motility using prepared micrographs . Students compare the characteristics of the cells of the mixed bacterial culture with the same set of characteristics of six bacterial species of known identity , in an attempt at an initial identification of the bacteria of the mixed culture . This course studies what is language and what does knowledge of a language consist of . It asks how do children learn languages and is language unique to humans ; why are there many languages ; how do languages change ; is any language or dialect superior to another ; and how are speech and writing related . Context for these and similar questions is provided by basic examination of internal organization of sentences , words , and sound systems . No prior training in linguistics is assumed . Introduction to the current research questions in phonological theory . Topics include : metrical ... . Introduction to the current research questions in phonological theory . Topics include : metrical and prosodic structure ; features and their phonetic basis in speech ; acquisition and parsing ; phonological domains ; morphology ; and language change and reconstruction . Activities include problem solving , squibs , and data collection . The year - long Introduction to Phonology reviews at the graduate level fundamental notions of phonological analysis and introduces students to current debates , research and analytical techniques . The Fall term reviews issues pertaining to the nature of markedness and phonological representations - features , prosodies , syllables and stress - while the second term deals with the relation between the phonological component and the lexicon , morphology and syntax . The second term course will also treat in more detail certain phonological phenomena . Investigating a Deep Sea Mystery is based on Deep - sea mystery solved : astonishing ... . The deep sea fishes at the heart of the investigation and this activity were historically classified into three families or clades based on the obvious morphological differences between the members of each group . Over time , as new data was accumulated , a new hypothesis was generated ; the three fish clades were really one . In this activity students follow the steps of the science team to unravel the mystery of the fishes ' classification by analyzing some of the same morphological and phylogenetic data as the science team . Lectures , reading , and discussion of current theory and data concerning the psychology ... . Lectures , reading , and discussion of current theory and data concerning the psychology and biology of language acquisition . Emphasizes learning of syntax and morphology , together with some discussion of phonology , and especially research relating grammatical theory and learnability theory to empirical studies of children . Learn Arabic is a website that aims to teach Arabic via games and activities . Members can compete for top spots as they earn badges by completing lessons . The lessons start with the alphabet and all of its variations and move up through simple words and phrases . Plans are in the works to add more complicated lessons for intermediate and advanced learners . Lessons include interactive books , videos , games , vocabulary lists , and more depending on the lesson . Users can sign up for Arabic tips emailed to them . The site includes a blog as well . In modern systematics , both morphological features and DNA or amino acid sequences ... . In modern systematics , both morphological features and DNA or amino acid sequences are used to determine phylogenetic relationships . This two - week exercise demonstrates two methods used by systematists to create phylogenetic trees . In the first week students score morphological features of fictional and real organisms , create data matrices showing number of synapomorphies , and then use the matrices to draw phylogenetic trees . In the second week they use Bio Workbench , an online bioinformatics software package , to create phylogenetic trees based on nucleotide or amino acid sequences . Students learn how modern systematics helps answer questions about ecology , evolution , and behavior . Relationship between computer representation of knowledge and the structure of natural language . Relationship between computer representation of knowledge and the structure of natural language . Emphasizes development of the analytical skills necessary to judge the computational implications of grammatical formalisms , and uses concrete examples to illustrate particular computational issues . Efficient parsing algorithms for context - free grammars ; augmented transition network grammars . Question answering systems . Extensive laboratory work on building natural language processing systems . 6.863 is a laboratory - oriented course on the theory and practice of building computer systems for human language processing , with an emphasis on the linguistic , cognitive , and engineering foundations for understanding their design . This illustrated guide ( dorsal view ) to a male spider is designed to ... . This illustrated guide ( dorsal view ) to a male spider is designed to help students recognize and learn its common and unique body parts . This illustrated guide ( ventral view ) to a female spider is designed to ... . This illustrated guide ( ventral view ) to a female spider is designed to help students recognize and learn its common and unique body parts . The single Web page , which can be easily printed for use at field sites or in the lab , also includes a short description for the following labeled parts : chelicera fang endite labium sternum coxa lung slit epigynum spinnerets . In this laboratory exercise , learners will discover how many different plant hosts ... . In this laboratory exercise , learners will discover how many different plant hosts they can find that are infected by the same genus of a powdery mildew fungus , or how many different genera of powdery mildew fungi can be found on the same plant host . This exercise demonstrates the diversity that exists within a fungal order . With a good collection of leaves infected with different powdery mildew fungi ( collected by learners or instructor ) , learners use a written key and/or an illustrated key ( or could even make their own key ) to identify the powdery mildew fungus to genus . Since powdery mildew fungi reproduce by means of two spore types , asexual spores ( conidia ) and sexual spores ( ascospores ) , discussions of the types of reproduction in fungi can be facilitated . Note : This lab works best in regions that experience cold weather for part of the year , since this encourages production of cleistothecia ( sexual stage ) . This website is an attempt to create a concordance and lexicon of the Arabic language . The website explains how the corpus is being compiled , and also discusses issues such as word frequency counts , details about the concordance , and morphology analysis . The Creole languages spoken in the Caribbean are linguistic by - products of the ... . The Creole languages spoken in the Caribbean are linguistic by - products of the historical events triggered by colonization and the slave trade in Africa and the ' New World ' . In a nutshell , these languages are the results of language acquisition in the specific social settings defined by the history of contact between African and European peoples in 17th-/18th - century Caribbean colonies . One of the best known Creole languages , and the one with the largest community of speakers , is Haitian Creole . Its lexicon is primarily derived from varieties of French as spoken in 17th-/18th - century colonial Haiti ; yet some of its structures seem to have emerged under the influence of African languages , mostly from West and Central Africa . And yet other properties seem to have no analogues in any of the source languages . Through a sample of linguistic case studies focusing on Haitian Creole morphosyntax , we will explore creolization from a cognitive , historical and comparative perspective . Using Haitian Creole and some of its Caribbean congeners as test cases , we will evaluate various hypotheses about the development of Creole languages and about the role of first- and second - language acquisition in such development . We will also explore the concept of Creolization in its non - linguistic senses . Then we will address questions of \\\" Caribbean identities \\\" by examining a sample of Creole speakers ' attitudes toward the Creole language and the corresponding European language and toward the African and European components of their ethnic make - up . \"}",
        "_version_":1692670034178998272,
        "score":21.765707},
      {
        "id":"84430c3c-2aab-4bb6-8833-5f9bdec559f4",
        "_src_":"{\"url\": \"http://speedchange.blogspot.com/2008/06/coercive-technology.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701166570.91/warc/CC-MAIN-20160205193926-00126-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Museums world - wide are deploying both virtual exhibits and multimedia collections for use by researchers , educators and the general public . Two principal problems hinder support for location and access to multiple data sources . First , there is a lack of agreement on how semantically consistent metadata for description of data collections should be created . Thereafter a user - friendly query language and processing system must be developed to support the formulation of search criteria , search in a multi - database space , and integrate and present the search results . This paper presents the motivation , goals , and approach taken for a newly started project that aims to develop methods and tools to address these problems by integrating and extending existing methods and tools developed separately for metadata , multimedia , and multi - database management . The primary goal is to develop a system to support dynamic generation of specialized collections as the result of an easy - to - use query language to multiple museum collections , i.e. a system to support exhibits - on - demand . Keywords : Virtual exhibits , multimedia database management , metadata , Information retrieval , Query processing . Museums on the Web - an informal report from the perspective of information retrieval . Museums world - wide have been deploying virtual exhibits onto the Web since the mid 1990s . The basic structure used for virtual exhibits is a set of Web pages consisting primarily of text , images , and links from either the text or images to similar pages or image enlargements , respectively . Virtual exhibits are frequently large , often much more than 100 pages . Most virtual exhibits are self - contained in the sense that a viewer can select only the predefined exhibit pages using predefined links . Many museums , in addition to deploying virtual exhibits , have also made electronic versions of ( some of ) their collections accessible from their Web site . This makes large quantities of information available to researchers , educators , students , and the general public . A few examples of electronic museum collections , showing diversity of content themes , include : . Various search tools have been made available for these collections , including some combination of : . Find object , given the catalogue number . Key word search , for matching viewer specified search terms to terms used in the catalogue title , type , creator , date , and/or description fields . Find similar to input object , for matching a scanned or drawn image , input from outside of the collection , with collection objects , as used by the State Hermitage Museum Web site . a ) View similar search , based on matching catalogue descriptions to those of a previously retrieved object . b ) Refine search , for modifying a previous search to expand or restrict the result set . The results from a collection search are frequently listed without an apparent order . G. Catlin . F. Rinehart . G. Catlin . If the collection consists of images , the results commonly contain a list of thumbnail images , linked to a larger version of the image , plus an annotation giving the artist / creator , date , title , and perhaps location . If the collection consists of documents , the titles are listed and linked to the full text . Associations / relationships between the objects in the result set are not given , even though it is likely that there is a relationship between objects and documents describing them , particularly if the object has been used in an exhibit . 1.1 Information retrieval problems . The search strategies outlined above have a number of well - known problems based on the nature of communication and the mismatch between the knowledge level and language of the viewer and that of the creator of the collection . Another problem area lies in the ( lack of ) sophistication of the software search engines . For example : . Search by catalogue number assumes that the viewer has access to the catalogue . Given that the information in the electronic DB is frequently a version of the catalogue , there may be little new information to be attained . Matching a scanned or drawn image input object from outside of the collection , with collection objects requires a level of software sophistication that is not generally available today ( 2002 ) , except in very specific applications , such as fingerprints or retina images . Retrieving similar objects , based on matching catalogue descriptions , commonly gives a high number of matched objects unless the extended search is limited to only a few of the catalogue characteristics . These problems are well documented in the information retrieval literature , as presented by Kowalski & Maybury ( 2000 ) and Baeza - Yates & Ribeiro - Neto ( 1999 ) . There are also strategies for improving information retrieval from document collections that should be adapted to the more complex problem of information retrieval from multimedia museum collections . 1.2 Result presentation . Presentation of the results of an information search as an unordered list only indicates that each retrieved object has some relation to the selection criteria . This can be ok , if the user simply wants a list of all objects in a particular category , for example by a given artist . However , there is more information about the result collection available that could be utilized in presentation of the result set . Simple orderings could be by artist , chronologically by date of the objects , or by subject , style , or material . In addition , combining ordering criteria may give the viewer even more information . Again , from information retrieval research and practice , the results can be ordered by relevance to the search criteria , assuming that multiple criteria are given and/or there is an importance / relevance distinction between matches in the title , keywords , and description of the objects . Finally , the observed sites do not provide cross - referencing between collections , for example from an image collection to a document collection or to presentation within a virtual exhibit . Providing these links could significantly increase the information about the objects in the result set . Designing virtual exhibits on demand : applying a database management perspective . Development of an exhibit takes months and several man - years of effort . The resulting exhibit is static in the sense that users can not extend or tailor the content for their own special needs ; for example , to form a specialized exhibit as an element in an educational context . Supporting user - developed exhibits , or specialized user defined collections , requires user access to the underlying data collections , possibly from multiple museums , as well as tools for search , retrieval , and presentation . Museums maintain , and have made available on the Internet , an increasingly diverse set of electronic multimedia databases . As high quality recording and scanning equipment becomes affordable , document and image collections are being supplemented with audio , video , film , and 3D collections . Parallel digitalization activities have led to the development of sets of separate but related electronic data collections or databases . Given that museums house overlapping collections , the result is a large set of inter - related , Internet accessible multimedia , multi - database systems . Two principal problems hinder support for location and access to multiple data sources . First , there is a lack of agreement on how semantically consistent metadata for description of museum collections should be created . Thereafter , a query language and processing system must be developed to support the formulation of search criteria , search in a multi - database space , and integration and presentation of search results . Our project aims to develop methods and tools to address these problems by integrating and extending existing methods and tools developed separately for metadata , multimedia , and multi - database management . Our interest lies in developing methods and tools for assisting development of virtual exhibits from a collection of underlying multimedia databases . 2.1 A user scenario . A student or teacher wishes to locate information on . the use of precious metals and gems in royal jewelry in Europe during the middle ages . Relevant information is located in national museums , as well as national archives , libraries , and collections maintained for the royal families . Using current Internet search engines , our user must perform the following tasks : . Enter the words of the above requirement statement ( keywords ) as search criteria for the search engine , which will : . Match the keywords to site indexes created from site names and descriptions , and . Return a list of ( assumed ) relevant Web sites . Use the result reference list to access a sub - set of the sites . At each site : . Search via established links to locate relevant data , and/or . Finally , the user must construct a coherent presentation of the retrieved data and information , perhaps as a new virtual exhibit . Known problem points in the above scenario : . There are a number of search engines available , each giving a different set of responses to a query . There were 6 references in the first 20 that were found ( in different locations ) in the 2 result sets and no references to museum collections . Internet search engines use indexes of Web ( html ) sites but are not capable of searching underlying multimedia databases . There is little or no standardization of collection descriptions ( metadata ) , leading to . Site access from the result is an easy but tedious process , giving at best a mixed result consisting of segments of divers Web exhibits , and data from various museum collections . There is no uniform retrieval format to structure the collected information . There is no tool to aid construction of a new collection , as part of a document or perhaps a new virtual exhibit . The problems can be summarized as a need for methods and tools to aid users in locating , accessing , extracting , and presenting relevant information from multimedia , multi - database systems developed and maintained by autonomous museums . 2.2 Current status in multimedia , multi - database management . There are a number of theories , methods , tools , and IT systems that can be extended to provide a solution for accessing autonomous multimedia , multi - database systems . Resource Location & Metadata development . Location of relevant data and information requires that metadata describing the semantic content of each collection be made available in a standardized format that supports semantic integration [ Bearman and Trant 1998]. There are numerous metadata standardization activities in process , perhaps best known are the development efforts behind Dublin Core in which there are 13 working groups including one with museum representatives . The Dublin Core effort began from the requirements of the Digital Library community and proposals for extension to other cultural application areas have been made , for example in [ Bearman et al , 1999]. Other metadata proposals , developed for museum collection description include CIDOC [ Doerr 1998 ] and the Warwick Framework [ Lagoze1996]. Proposals for description of the semantic content of general multimedia can be found in [ Lu 1999 , Marcus 1996 , Subrahmanian 1998 , and Wu et.al 2000]. The Dublin Core proposal ( 1999 ) consists of 13 basic metadata elements for describing aspects of media objects or resources . Controlled vocabularies are recommended for certain descriptive values ; for example , for subject and type . However , it is well known that the use of controlled vocabularies for developing collection metadata / indexes requires trained users since the vocabulary seldom matches that of general public users [ Hillman 2001]. This problem is also well known in the information retrieval community where much research has gone into linguistic based methods for selecting descriptive document terms and establishing thesauruses to aid the match between user query terms and the collection index terms [ Baeza - Yates 1999 , Kowalski 2001]. The problem is also well known in the multi - database research community where a structural analysis of database schemas has been the dominant approach for synonym resolution [ Elmagarmid et.al 1999]. Data access and retrieval . As noted above , Internet search engines retrieve lists of Web site URLs after matching the user key word request with site indexes constructed from the metadata available to the search engine via crawler activity . No actual data is returned to the users who must then continue the search using the URL list and the established links on the referenced Web sites . Multimedia database management systems , including document retrieval systems , also use a keyword - based search , but can also return actual resources ( multimedia or document objects ) . These systems can also search for resources similar to a given resource , perhaps from the response to an initial keyword search [ Baeza - Yates 1999 , Kowalski 2001 , Lu 1999 , and Wu et.al 2000]. The problem with current approaches is that the search engine is specialized to one data type ( text , image , video , audio , or spatial ) , thus requiring multiple queries , one to each data collection . However , these systems do not address the autonomous multi - database problem . Current multi - database systems can search and retrieve data from multiple source databases , but only structured databases managed by relational or object - oriented DB management systems . In both the multimedia and multi - database approaches , extensions to SQL3 are proposed and used , perhaps with a form interface . The major problem here is also that SQL3 is not a user friendly language as it requires user knowledge of the structure of the underlying database to be searched , an impractical restriction for an environment with a large number of multimedia data collections . Presentation . Query results are generally given as a list of titles and/or thumbnail images linked to the objects / resources that are considered relevant to the search query . The list may be ordered by a relevance match between the query terms and the object descriptors . If the result is to be formed as an exhibit , the user must then do so . 2.3 A database perspective of exhibit construction . Figure 1 shows the main components for a planned ICT ( Information and Communication Technology ) system for construction of a virtual exhibit from the results of a multi - database query . The lower section of the figure illustrates the multimedia multi - database set , i.e. the environment of interest for retrieving information for an exhibit . Note that the figure shows only the databases for one museum / organization . The databases are assumed to be managed by an object - relational database system ( for example Oracle or DB2 ) containing SQL3 as its access language and search functions for document , image , video , audio , and spatial/3D media objects . A catalog describes the objects in the database set . Figure 1 : Virtual Exhibit system components . The upper part of Figure 1 shows the main components of a user interface for retrieving information from the database collection and presentation of the results as a virtual exhibit . Resource Location & Metadata development . The interface is based on a semantic model of the object catalogs so that similar objects can be identified in the separate source databases . Data access and retrieval . The SemQL query language is an extension of SQL3 . Based on information from the semantic schema , SemQL is able to construct local queries to those databases that have data relevant to the user query . SemQL also has a user interface that is similar to those used in the advanced search functions of Web search engines with the addition of an interactive dialog system to assist with early refinement of the user query . Presentation . A virtual exhibit construction module presents the results of queries to the system . The presentation module can construct a set of Web pages , a topic exhibit , for presentation of the query response . The resulting exhibit can be further modified by refining and resubmitting the SemQL query . 2.4 The virtual exhibit project . Bergen Museum , in Bergen , Norway , has embarked on a project , described briefly by Ramirez ( 2001 ) , to establish and publish a number of different multimedia databases implemented by theme and media type . The collection of databases currently includes 3D databases for sculpture and insects , a video database of centipedes , a film database from social anthropology projects , plus various text - document and image databases . At this time , the databases are not inter - connected and are not connected to virtual exhibits . When the first phase of the database set has been implemented , the system is to be integrated with similar systems in other Nordic museums . The framework for the project is given in Figure 1 above . This project is also supported by NFR and has funding for 6 researchers and graduate students . Project goals . The new methods and tools are to be an integration and extension of appropriate methods developed separately for metadata , multimedia , and multi - database management . Sub tasks for construction of a system for virtual exhibits on demand . The following methods and techniques will be developed in the coming 2 years : . A semantic model for the metadata required for description of semantic content metadata for museum data collections ( databases ) . SemQL as an extension of the SQL3 query language and processor to support search by semantic content , as defined in the metadata schema developed in task 3 , for electronic museum artifacts stored in multiple databases . An exhibit construction module for presentation of query search results . Finally , a prototype system will be constructed and tested to demonstrate the feasibility and utility of above techniques in prototype system for an educational application . The prototype will consist of 3 basic sub - systems : . A metadata system tailored to museum collection users . This media abstraction approach outlines a structure for capturing the semantic content of multimedia objects . It has not yet been developed into a working prototype , so that our result can be considered method development . Assuming this is successful , the result will be integrated with the semantic content descriptions as specified in the current Dublin Core standard for description of artifacts . The semantic query language SemQL , based on SQL3 + , will be developed . This language will combine the functionality of multimedia and multi - database access as outlined in Baeza - Yates ( 1999 ) and Elmagarmid ( 1999 ) , respectively . In addition , SemQL , will use the semantic schema structure developed under component 1 above . Currently , there is no known query language that combines multimedia search in multi - database systems . A generic exhibit generator will be developed for presentation of SemQL query results . We envision using a book metaphor for concurrent presentation of text and related image with embedded links so that the viewer can zoom from either the text or image presentation resulting in more detailed information presented in both text and image panels . Test and Evaluation . Three phases of evaluation are planned : . Prototype testing will be used to demonstrate the feasibility of combining the above elements into a museum resource retrieval system . Precision & Recall measures , common in document processing systems ( Kowalski 2001 ) , will be used for quality testing of SemQL . The test environment will consist of a carefully constructed set of data collections containing the scanned images and descriptive documents for a set of thematically close types of museum objects . Usability testing will be done by eliciting thematic descriptions of presentation exhibits , defined by educators interested in the thematic content of the test databases . These will form the source material for the SemQL query and the presentation model testing . Project status . We would appreciate any comments and suggestions that the reader can give . References . Baeza - Yates , R. & Ribeiro - Neto , B. ( 1999 ) . Modern Information Retrieval . Addison Wesley . Bearman , D. and Trant , J. ( 1998 ) . Unifying Cultural Memory . Information Landscapes for a Learning Society , 1998 . And presentation at UK Office of Library Networking Conference , July 1998 . Bearman , D. , Miller , E. , Rust , G. , Trant , J. , and Weibel , S. ( 1999 ) . A Common Model to Support Interoperable Metadata , Progress report on reconciling metadata requirements from the Dublin Core and INDECS / DOI Communities . Ramirez , E.A. ( 2001 ) Structuralizing Multimedia Data in Museums . The Use of Internet and Video and Scanned 3D Objects for Our Natural History and Science Museums . In Proc . Shneiderman , B. , et.al . Evaluating Three Museum Installations of a Hypertext System . Journal of the American . Society for Information Science , 40(3 ) , 172 - 182 . Subrahmanian , V. S. ( 1998 ) . Principles of Multimedia Database Systems . Morgan Kaufmann . Wu , J.K. , Kankanhalli , M.S. , Llim , J , and Hong , D. ( 2000 ) Perspectives on Content - Based Multimedia Systems . Kluwer Academic Publ . Yamada , S. , et.al . Development and evaluation of hypermedia for museum education : validation of metrics . ACM Trans . of Computer - Human Interaction , 2(4 ) , 284 - 307 . \"}",
        "_version_":1692668979511820289,
        "score":21.18959},
      {
        "id":"3d3475bd-f68c-49d5-b625-85987ca31725",
        "_src_":"{\"url\": \"http://sf.streetsblog.org/2009/08/07/penguins-to-penguins-sunday-streets-kicks-off-this-weekend/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156448.92/warc/CC-MAIN-20160205193916-00175-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Jahrestagung der Deutschen Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie ( gmds ) 19 . Jahrestagung der Schweizerischen Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) Jahrestagung 2004 des Arbeitskreises Medizinische Informatik ( \\u00d6AKMI ) . Deutsche Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie Schweizerische Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) . Article . Cross - Language Document Retrieval with MorphoSaurus . Search Medline for . Kornel Marko - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Stefan Schulz - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Joachim Wermter - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Michael Poprat - Arbeitsgruppe Computerlinguistik , Universit\\u00e4t Freiburg , Freiburg , Deutschland . Udo Hahn - Arbeitsgruppe Computerlinguistik , Universit\\u00e4t Freiburg , Freiburg , Deutschland . Kooperative Versorgung - Vernetzte Forschung - Ubiquit\\u00e4re Information . Jahrestagung der Deutschen Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie ( gmds ) , 19 . Jahrestagung der Schweizerischen Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) und Jahrestagung 2004 des Arbeitskreises Medizinische Informatik ( \\u00d6AKMI ) der \\u00d6sterreichischen Computer Gesellschaft ( OCG ) und der \\u00d6sterreichischen Gesellschaft f\\u00fcr Biomedizinische Technik ( \\u00d6GBMT ) . Innsbruck , 26.-30.09.2004 . D\\u00fcsseldorf , K\\u00f6ln : German Medical Science ; 2004 . Doc04gmds065 . \\u00a9 2004 Marko et al . You are free : to Share - to copy , distribute and transmit the work , provided the original author and source are credited . Outline . Introduction . Medical information retrieval ( IR ) presents a unique combination of challenges for the design and implementation of retrieval engines [ 1 ] . First of all , clinical document collections and medical databases are usually very large and dynamic . Second , medical document collections are truly multi - lingual . While clinical documents are typically written in the physicians ' native language , searches in major bibliographic databases such as MEDLINE require sophisticated knowledge of ( expert - level ) English medical terminology which most non - English speaking physicians do not have . Third , medical terminology is morphologically extremely productive , characterized by a typical mix of Latin and Greek roots with the corresponding host language , e.g. in words such as pseudohypoparathyroidism , Gastrointestinaltrakt , etc . Obviously , dealing with such phenomena is crucial for any medical IR system . We respond to these challenges in terms of the MORPHOSAURUS system ( an acronym for MORPHeme and the SAURUS ) . At its core lies a special type of dictionary , in which the entries are equivalence classes of subwords , i.e. , semantically minimal units [ 2 ] . These equivalence classes capture intralingual as well as interlingual synonymy . We evaluate these two fundamentally different approaches on a large medical document collection ( the Ohsumed corpus [ 3 ] ) . Morpho - semantic Normalization . Figure 1 [ Fig . 1 ] depicts how source documents are converted into a morpho - semantically normalized , interlingual representation by a three - step procedure . The first step deals with orthographic normalization . A preprocessor reduces all capitalized characters from input documents to lower - case characters and , additionally , performs language - specific character substitutions ( e.g. the replacement of German umlauts ) . The next step in the pipeline is concerned with morphological segmentation . The system segments the input stream into a sequence of semantically plausible sublexical items , corresponding to subwords as found in the lexicon . Currently , the subword lexicon contains about 57,000 entries with 21,000 for both German and English and 15,000 for Portuguese . In the final step , semantic normalization , each content bearing subword recognized is substituted by its corresponding equivalence class ( called MorphoSaurus identifier - MID ) . After that step , all synonyms within a language and all translations of semantically equivalent subwords from different languages are represented by the same code in that target representation . Experimental Settings . Our experiments were run on the Ohsumed corpus [ 3 ] , which constitutes one of the standard IR testbeds for the medical domain . Ohsumed is a subset of the MEDLINE database . Considering those documents which contained abstracts ( some did not ) , we obtained a document collection comprised of 233,445 texts with 41 million tokens , in total . Since the Ohsumed corpus was created specifically for IR studies , 106 queries are available , including associated relevance judgments . The following is a typical query : \\\" Are there adverse effects on lipids when progesterone is given with estrogen replacement therapy ? \\\" Since the Ohsumed corpus contains only English - language documents the question arises how this collection ( or MEDLINE , in general ) can be accessed from other languages as well . Query translation ( QTR ) can be regarded as a standard , and often preferred experimental procedure in the cross - language retrieval community [ 4 ] . In our experiments , the original English queries were first translated into Portuguese and German by medical experts ( native speakers of Portuguese or German , with a very good mastery of both general and medical English ) . In the second step , the manually translated queries were re - translated into English using the Google Translator . Additionally , for covering the medical sublanguage , we used a bilingual lexeme dictionary derived from the UMLS Metathesaurus [ 5 ] with about 26,000 German - English entries and 14,200 entries for Portuguese - English . As an alternative to QTR , we probed the MorphoSaurus indexing approach ( MSI ) . Unlike QTR , the normalization of documents and queries yields a language - independent , semantically normalized index format . As the baseline for our experiments , we provide a retrieval system operating with a word stemmer and a stopword list running on ( original ) English documents with ( original ) English queries . For an unbiased evaluation , we basically used a simple Boolean search approach incorporating adjacency metrics . Results . It is not surprising that the English - English baseline performs best with an 11pt average ( a standard metrics in IR ) of 0.14 ( cf . [ Tab . 1 ] ) . The German - English MSI approach result is almost on a par with the baseline ( 0.01 less ( 0.13 ) ) , whereas the German - English QTR result is more than 0.05 points worse ( 0.09 ) . This means that the MSI approach achieved 93 % of the baseline performance ( quite a high score given cross - language IR standards ) , whereas the QTR approach scored far lower ( 62 % ) . This difference turns out to be less dramatic , but still noticeable , in comparing the Portuguese - English MSI and QTR results with the baseline ( 68 % for MSI and 54 % for QTR , hence , 14 percentage points difference ) . Both the MSI and the QTR 11pt averages are much lower for the Portuguese - English retrieval case . In any case , it seems worth noting that at no single recall point QTR values were higher than MSI values . Hence , the latter consistently outperformed the former on both languages . Interesting from a realistic retrieval perspective is the average gain on the top two recall points . In Table 1 [ Tab . 1 ] the Portuguese - English MSI condition achieves a precision of 0.26 ( 72 % of the baseline ) , the German - English condition yields a precision value of 0.32 ( 90 % of the baseline ) for MSI . Conclusion . The success of dictionary - based cross - language IR largely depends on the coverage of underlying lexicons . We optimize the lexical coverage by limiting the lexicon to semantically relevant subwords . Based on this architecture we presented an interlingua approach to cross - language information retrieval on a medical document collection . Compared to state - of - the - art direct translation techniques we achieved a remarkable benefit , at least for German by reaching 93 % of the English baseline . Acknowledgments . This work was partly funded by Deutsche Forschungsgemeinschaft ( DFG ) , grant Klar 640/5 - 1 . Eichmann D , Ruiz ME , Srinivasan P. Cross - language Information Retrieval with the UMLS Metathesaurus . Proc . 21st Intl . ACM SIGIR Conference on Research and Development in Information Retrieval ; 1998:72 - 80 \"}",
        "_version_":1692668357486051328,
        "score":20.74475},
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":20.63976},
      {
        "id":"41197524-47a0-479d-a3de-765220c505b3",
        "_src_":"{\"url\": \"http://fair.org/topic/extra-november-2009/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701161718.0/warc/CC-MAIN-20160205193921-00035-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"The aim of this module is to introduce the student to corpus linguistics . Corpora are often used by linguists as the raw material from which language description may be fashioned - the role is no less relevant for CALL package designers . Corpora can provide the basis of accurate , empirically justified , linguistic observations on which to base CALL materials . Additionally , the corpora themselves , typically via concordancing , may become the raw material of CALL based teaching itself . The corpus may be viewed , in certain contexts , as an item bank . The use of corpora in CALL are many . A knowledge of the corpus method to CALL package designers is increasingly indispensable . This module extends and complements the section on corpus linguistics in Module 2.4 , which has been written by Marie - No\\u00eblle Lamy & Hans J\\u00f8rgen Klarskov Mortensen . Corpus linguistics also forms part of Natural Language Processing ( NLP ) , which is dealt with by Mathias Schulze & Piklu Gupta in Module 3.5 , Human Language Technologies ( HLT ) . This Web page is designed to be read from the printed page . Use File / Print in your browser to produce a printed copy . After you have digested the contents of the printed copy , come back to the onscreen version to follow up the hyperlinks . \\\" Early corpus linguistics \\\" is a term we use here to describe linguistics before the advent of Chomsky . Field linguists , for example Boas ( 1940 ) who studied American - Indian languages , and later linguists of the structuralist tradition all used a corpus - based methodology . However , that does not mean that the term \\\" corpus linguistics \\\" was used in texts and studies from this era . Below is a brief overview of some interesting corpus - based studies predating 1950 . The studies of child language in the diary studies period of language acquisition research ( roughly 1876 - 1926 ) were based on carefully composed parental diaries recording the child 's locutions . These primitive corpora are still used as sources of normative data in language acquisition research today , e.g. Ingram ( 1978 ) . Corpus collection continued and diversified after the diary studies period : large sample studies covered the period roughly from 1927 to 1957 - analysis was gathered from a large number of children with the express aim of establishing norms of development . Kading ( 1897 ) used a large corpus of German - 11 million words - to collate frequency distributions of letters and sequences of letters in German . The corpus , by size alone , is impressive for its time , and compares favourably in terms of size with modern corpora . Fries & Traver ( 1940 ) and Bongers ( 1947 ) are examples of linguists who used the corpus in research on foreign language pedagogy . Indeed , as noted by Kennedy ( 1992 ) , the corpus and second language pedagogy had a strong link in the early half of the twentieth century , with vocabulary lists for foreign learners often being derived from corpora . The word counts derived from such studies as Thorndike ( 1921 ) and Palmer ( 1933 ) were important in defining the goals of the vocabulary control movement in second language pedagogy . Chomsky changed the direction of linguistics away from empiricism and towards rationalism in a remarkably short space of time . In doing so he apparently invalidated the corpus as a source of evidence in linguistic enquiry . Chomsky suggested that the corpus could never be a useful tool for the linguist , as the linguist must seek to model language competence rather than performance . Competence is best described as our tacit , internalised knowledge of a language . Performance is external evidence of language competence , and is usage on particular occasions when , crucially , factors other than our linguistic competence may affect its form . Competence both explains and characterises a speaker 's knowledge of a language . Performance , however , is a poor mirror of competence . For examples , factors diverse as short - term memory limitations or whether or not we have been drinking can alter how we speak on any particular occasion . This brings us to the nub of Chomsky 's initial criticism : a corpus is by its very nature a collection of externalised utterances - it is performance data and is therefore a poor guide to modelling linguistic competence . Further to that , if we are unable to measure linguistic competence , how do we determine from any given utterance what are linguistically relevant performance phenomena ? This is a crucial question , for without an answer to this , we are not sure that what we are discovering is directly relevant to linguistics . We may easily be commenting on the effects of drink on speech production without knowing it . However , this was not the only criticism that Chomsky had of the early corpus linguistics approach . The non - finite nature of language . All the work of early corpus linguistics was underpinned by two fundamental , yet flawed assumptions : . The sentences of a natural language are finite . The sentences of a natural language can be collected and enumerated . The corpus was seen as the sole source of evidence in the formation of linguistic theory - \\\" This was when linguists [ ... ] regarded the corpus as the sole explicandum of linguistics \\\" ( Leech , 1991 ) . The number of sentences in a natural language is not merely arbitrarily large - it is potentially infinite . This is because of the sheer number of choices , both lexical and syntactic , which are made in the production of a sentence . Also , sentences can be recursive . Consider the sentence \\\" The man that the cat saw that the dog ate that the man knew that the ... \\\" This type of construct is referred to as centre embedding and can give rise to infinite sentences . ( This topic is discussed in further detail in McEnery & Wilson 1996:7 - 8 ) . The only way to account for a grammar of a language is by description of its rules - not by enumeration of its sentences . It is the syntactic rules of a language that Chomsky considers finite . These rules in turn give rise to infinite numbers of sentences . Even if language was a finite construct , would corpus methodology still be the best method of studying language ? Why bother waiting for the sentences of a language to enumerate themselves , when by the process of introspection we can delve into our own minds and examine our own linguistic competence ? At times intuition can save us time in searching a corpus . Without recourse to introspective judgements , how can ungrammatical utterances be distinguished from ones that simply have n't occurred yet ? If our finite corpus does not contain the sentence : . how do we conclude that it is ungrammatical ? Indeed , there may be persuasive evidence in the corpus to suggest that it is grammatical if we see sentences such as : . He gives Tony books He lends Tony books He owes Tony books . Introspection seems a useful and good tool for cases such as this . But early corpus linguistics denied its use . Also , ambiguous structures can only be identified and resolved with some degree of introspective judgement . An observation of physical form only seems inadequate . Consider the sentences : . Tony and Fido sat down - he read a book of recipes . Tony and Fido sat down - he ate a can of dog food . It is only with introspection that this pair of ambiguous sentences can be resolved e.g. we know that Fido is the name of a dog and it was therefore Fido who ate the dog food , and Tony who read the book . Apart from Chomsky 's theoretical criticisms , there were problems of practicality with corpus linguistics . Abercrombie ( 1963 ) criticisms of \\\" pseudo - procedures \\\" can , in the context of the pre - mass computing era , easily be applied to corpus linguistics . Can you imagine searching through an 11-million - word corpus such as that of Kading ( 1897 ) using nothing more than your eyes ? The whole undertaking becomes prohibitively time consuming , not to say error - prone and expensive . Whatever Chomsky 's criticisms were , Abercrombie 's observations about the nature of the pseudo - procedure were undoubtedly correct . Early corpus linguistics required data processing abilities that were simply not available at that time . The impact of the criticisms levelled at early corpus linguistics in the 1950s was immediate and profound . Corpus linguistics was largely abandoned during this period , although it never totally died . Although Chomsky 's criticisms did discredit corpus linguistics , they did not stop all corpus - based work . For example , in the field of phonetics , naturally observed data remained the dominant source of evidence with introspective judgements never making the impact they did on other areas of linguistic enquiry . Also , in the field of language acquisition the observation of naturally occurring evidence remained dominant . Introspective judgements are not available to the linguist / psychologist who is studying child language acquisition - try asking an eighteen - month - old child whether the word \\\" moo - cow \\\" is a noun or a verb ! Introspective judgements are only available to us when our meta - linguistic awareness has developed , and there is no evidence that a child at the one - word stage has meta - linguistic awareness . Even Chomsky ( 1964 ) cautioned the rejection of performance data as a source of evidence for language acquisition studies . Naturally occurring data is observable and verifiable by everyone . Introspective judgements are unobservable , therefore much more difficult to verify . Introspective data is artificial . Sampson ( 1992 ) argues that the type of sentence analysed by the introspective linguist is far away from the type of evidence we tend to see typically accruing in a corpus . By artificially manipulating the informant , we artificially manipulate the data itself . Human beings have only the vaguest notion of the frequency of a construct or a word . Corpora are sources of quantitative information beyond compare . However , frequency - based data is not available via introspective means . Leech ( 1992 ) argues that the corpus is a more powerful methodology from the point of view of the scientific method , as it is open to objective verification of results . Is language production really a poor reflection of language competence as Chomsky really argued ? Labov ( 1969 ) showed that \\\" the great majority of utterances in all contexts are grammatical \\\" . We are not saying that all sentences in a corpus are grammatically acceptable , but it seems probable that the Chomsky 's ( 1968 : 88 ) claim that performance data is ' degenerate ' is an exaggeration ( see Ingram 1989:223 for further criticisms of this view ) . Quantitative data is of use to linguistics . For example , Svartvik 's ( 1966 ) study of passivisation used quantitative data extracted from a corpora . Elsewhere , all successful approaches to automated part - of - speech analysis reply on quantitative data from corpora . The proof of the pudding is in the eating . Abercrombie 's observations that corpus research is time - consuming , expensive and error - prone are no longer applicable thanks to the development of powerful computers and software which is able to perform complex calculations in seconds , without error . It is a common belief that corpus linguistics was abandoned entirely in the 1950s , and then adopted once more almost as suddenly in the early 1980s . This is simply untrue , and does a disservice to those linguists who continued to pioneer corpus - based work during this interregnum . For example , Quirk ( 1960 ) planned and executed the construction of his ambitious Survey of English Usage ( SEU ) , which he began in 1961 . In the same year , Francis & Kucera began work on the now famous Brown corpus , a work which was to take almost two decades to complete . These researchers were in a minority , but they were not universally regarded as peculiar and others followed their lead . In 1975 Jan Svartvik started to build on the work of the SEU and the Brown corpus to construct the London - Lund corpus . During this period the computer slowly started to become the mainstay of corpus linguistics . Svartvik computerised the SEU , and as a consequence produced what some , including Leech ( 1991 ) , still believe to be \\\" to this day an unmatched resource for studying spoken English \\\" . The availability of the computerised corpus and the wider availability of institutional and private computing facilities do seem to have provided a spur to the revival of corpus linguistics . The term corpus is almost synonymous with the term machine - readable corpus . Considering the marriage of machine and corpus , it seems worthwhile to consider in slightly more detail what these processes that allow the machine to aid the linguist are . The computer has the ability to search for a particular word , sequence of words , or perhaps even a part of speech in a text . So if we are interested , say , in the usages of the word however in the text , we can simply ask the machine to search for this word in the text . The computer 's ability to retrieve all examples of this word , usually in context , is a further aid to the linguist . The machine can find the relevant text and display it to the user . It can also calculate the number of occurrences of the word so that information on the frequency of the word may be gathered . We may then be interested in sorting the data in some way - for example , alphabetically on words appearing to the right or left . We may even sort the list by searching for words accruing in the immediate context of the word . The processes described above are often included in a concordance program . This is the tool most often implemented in corpus linguistics to examine corpora . Whatever philosophical advantages we may eventually see in a corpus , it is the computer which allows us to exploit corpora on a large scale with speed and accuracy . See Module 2.4 , Using concordance programs in the Modern Foreign Languages classroom . The concept of carrying out research on written or spoken texts is not restricted to corpus linguistics . Indeed , individual texts are often used for many kinds of literary and linguistic analysis - the stylistic analysis of a poem , or a conversation analysis of a TV talk show . However , the notion of a corpus as the basis for a form of empirical linguistics is different from the examination of single texts in several fundamental ways . In principle , any collection of more than one text can be called a corpus , ( corpus being Latin for \\\" body \\\" , hence a corpus is any body of text ) . But the term \\\" corpus \\\" when used in the context of modern linguistics tends most frequently to have more specific connotations than this simple definition . The following list describes the four main characteristics of the modern corpus . Often in linguistics we are not merely interested in an individual text or author , but a whole variety of language . In such cases we have two options for data collection : . We could analyse every single utterance in that variety - however , this option is impracticable except in a few cases , for example with a dead language which only has a few texts . Usually , however , analysing every utterance would be an unending and impossible task . We could construct a smaller sample of that variety . This is a more realistic option . As discussed in Section 1.4 , one of Chomsky 's criticisms of the corpus approach was that language is infinite - therefore , any corpus would be skewed . In other words , some utterances would be excluded because they are rare , others which are much more common might be excluded by chance , and alternatively , extremely rare utterances might also be included several times . Although nowadays modern computer technology allows us to collect much larger corpora than those that Chomsky was thinking about , his criticisms still must be taken seriously . This does not mean that we should abandon corpus linguistics , but instead try to establish ways in which a much less biased and representative corpus may be constructed . We are therefore interested in creating a corpus which is maximally representative of the variety under examination , that is , which provides us with an as accurate a picture as possible of the tendencies of that variety , as well as their proportions . What we are looking for is a broad range of authors and genres which , when taken together , may be considered to \\\" average out \\\" and provide a reasonably accurate picture of the entire language population in which we are interested . The term \\\" corpus \\\" also implies a body of text of finite size , for example , one million words . This is not universally so . For example John Sinclair 's Cobuild team at the University of Birmingham initiated the construction and analysis of a monitor corpus in the 1980s . Such a \\\" collection of texts \\\" , as Sinclair 's team preferred to call the Cobuild corpus , is an open - ended entity - texts are constantly being added to it , so it gets bigger and bigger . Monitor corpora are of interest to lexicographers who can trawl a stream of new texts looking for the occurrence of new words , or for changing meanings of old words . Their main advantages are : . They are not static - new texts can always be added , unlike the synchronic \\\" snapshot \\\" provided by finite corpora . Their scope - they provide for a large and broad sample of language . Their main disadvantage is : . They are not such a reliable source of quantitative data ( as opposed to qualitative data ) because they are constantly changing in size and are less rigorously sampled than finite corpora . With the exception of monitor corpora , it should be noted that it is more often the case that a corpus consists of a finite number of words . Usually this figure is determined at the beginning of a corpus - building project . For example , the Brown Corpus contains 1,000,000 running words of text . Unlike the monitor corpus , when a corpus reaches its grand total of words , collection stops and the corpus is not increased in size . ( An exception is the London - Lund corpus , which was increased in the mid-1970s to cover a wider variety of genres . ) Nowadays the term \\\" corpus \\\" nearly always implies the additional feature \\\" machine - readable \\\" . This was not always the case as in the past the word \\\" corpus \\\" was only used in reference to printed text . Today few corpora are available in book form - one which does exist in this way is \\\" A Corpus of English Conversation \\\" ( Svartvik and Quirk 1980 ) which represents the \\\" original \\\" London - Lund corpus . Corpus data ( not excluding context - free frequency lists ) is occasionally available in other forms of media . For example , a complete key - word - in - context concordance of the LOB corpus is available on microfiche , and with spoken corpora copies of the actual recordings are sometimes available - this is the case with the Lancaster / IBM Spoken English Corpus but not with the London - Lund corpus . Machine - readable corpora possess the following advantages over written or spoken formats : . They can be searched and manipulated at speed . ( This is something which we covered at the end of Part One ) . They can easily be enriched with extra information . ( We will examine this in detail later . ) If you have n't already done so you can now read about other characteristics of the modern corpus . There is often a tacit understanding that a corpus constitutes a standard reference for the language variety that it represents . This presupposes that it will be widely available to other researchers , which is indeed the case with many corpora - e.g. the Brown Corpus , the LOB corpus and the London - Lund corpus . One advantage of a widely available corpus is that it provides a yardstick by which successive studies can be measured . So long as the methodology is made clear , new results on related topics can be directly compared with already published results without the need for re - computation . Also , a standard corpus also means that a continuous base of data is being used . This implies that any variation between studies is less likely to be attributed to differences in the data and more to the adequacy of the assumptions and methodology contained in the study . Not all corpora are monolingual , and an increasing amount of work in being carried out on the building of multilingual corpora , which contain texts of several different languages . For example , the Aarhus corpus of Danish , French and English contract law consists of a set of three monolingual law corpora , which is not comprised of translations of the same texts . The second type of multilingual corpora ( and the one which receives the most attention ) is parallel corpora . This refers to corpora which hold the same texts in more than one language . The parallel corpus dates back to mediaeval times when \\\" polyglot bibles \\\" were produced which contained the biblical texts side by side in Hebrew , Latin and Greek etc . . A parallel corpus is not immediately user - friendly . For the corpus to be useful it is necessary to identify which sentences in the sub - corpora are translations of each other , and which words are translations of each other . A corpus which shows these identifications is known as an aligned corpus as it makes an explicit link between the elements which are mutual translations of each other . For example , in a corpus the sentences \\\" Das Buch ist auf dem Tisch \\\" and \\\" The book is on the table \\\" might be aligned to one another . At a further level , specific words might be aligned , e.g. \\\" Das \\\" with \\\" The \\\" . This is not always a simple process , however , as often one word in one language might be equal to two words in another language , e.g. the German word \\\" raucht \\\" could be equivalent to \\\" is smoking \\\" in English . At present there are few cases of annotated parallel corpora , and those which exist tend to be bilingual rather than multilingual . However , two EU - funded projects ( CRATER and MULTEXT ) are aiming to produce genuinely multilingual parallel corpora . The Canadian Hansard corpus is annotated , and contains parallel texts in French and English , but it only covers a restricted range of text types ( proceedings of the Canadian Parliament ) . However , this is an area of growth , and the situation is likely to change dramatically in the near future . If corpora is said to be unannotated it appears in its existing raw state of plain text , whereas annotated corpora has been enhanced with various types of linguistic information . Unsurprisingly , the utility of the corpus is increased when it has been annotated , making it no longer a body of text where linguistic information is implicitly present , but one which may be considered a repository of linguistic information . The implicit information has been made explicit through the process of concrete annotation . For example , the form \\\" gives \\\" contains the implicit part - of - speech information \\\" third person singular present tense verb \\\" but it is only retrieved in normal reading by recourse to our pre - existing knowledge of the grammar of English . However , in an annotated corpus the form \\\" gives \\\" might appear as \\\" gives_VVZ \\\" , with the code VVZ indicating that it is a third person singular present tense ( Z ) form of a lexical verb ( VV ) . Such annotation makes it quicker and easier to retrieve and analyse information about the language contained in the corpus . Certain kinds of linguistic annotation , which involve the attachment of special codes to words in order to indicate particular features , are often known as \\\" tagging \\\" rather than annotation , and the codes which are assigned to features are known as \\\" tags \\\" . These terms will be used in the sections which follow . This is the most basic type of linguistic corpus annotation - the aim being to assign to each lexical unit in the text a code indicating its part of speech . Part - of - speech annotation is useful because it increases the specificity of data retrieval from corpora , and also forms an essential foundation for further forms of analysis ( such as syntactic parsing and semantic field annotation ) . Part - of - speech annotation also allows us to distinguish between homographs . Part - of - speech annotation was one of the first types of annotation to be formed on corpora and is the most common today . One reason for this is because it is a task that can be carried out to a high degree of accuracy by a computer . Greene & Rubin ( 1971 ) achieved a 71 % accuracy rate of correctly tagged words with their early part - of - speech tagging program ( TAGGIT ) . In the early 1980s the UCREL team at Lancaster University reported a success rate of 95 % using their program CLAWS . Spoken language corpora can also be transcribed using a form of phonetic transcription . Not many examples of publicly available phonetically transcribed corpora exist at the time of writing . This is possibly because phonetic transcription is a form of annotation which needs to be carried out by humans rather than computers . Such humans have to be well skilled in the perception and transcription of speech sounds . Phonetic transcription is therefore a very time consuming task . Nevertheless , phonetically transcribed corpora is extremely useful to the linguist who lacks the technological tools and expertise for the laboratory analysis of recorded speech . One such example is the MARSEC corpus ( which is derived from the Lancaster / IBM Spoken English Corpus ) and has been manipulated by the Universities of Lancaster and Leeds . The MARSEC corpus will include a phonetic transcription . Problem - oriented tagging , as described by de Haan ( 1984 ) , is the phenomenon whereby users will take a corpus , either already annotated , or unannotated , and add to it their own form of annotation , oriented particularly towards their own research goal . This differs in two ways from the other types of annotation we have examined in this section . It is not exhaustive . Not every word ( or sentence ) is tagged - only those which are directly relevant to the research . This is something which problem - oriented tagging has in common with anaphoric annotation . Annotation schemes are selected , not for broad coverage and theory - neutrality , but for the relevance of the distinctions which it makes to the specific questions that the researcher wishes to ask of his / her data . Although it is difficult to generalise further about this form of corpus annotation , it is an important type to keep in mind in the context of practical research using corpora . In this section we will examine a few of the roles which corpora may play in the study of language . The importance of corpora to language study is aligned to the importance of empirical data . Empirical data enable the linguist to make objective statements , rather than those which are subjective , or based upon the individual 's own internalised cognitive perception of language . Empirical data also allows us to study language varieties such as dialects or earlier periods in a language for which it is not possible to carry out a rationalist approach . Corpus linguistics , proper , should be seen as a subset of the activity within an empirical approach to linguistics . Although corpus linguistics entails an empirical approach , empirical linguistics does not always entail the use of a corpus . In the following pages we 'll consider the roles which corpora use may play in a number of different fields of study related to language . We will focus on the conceptual issues of why corpus data are important to these areas , and how they can contribute to the advancement of knowledge in each , providing real examples of corpus use . Empirical data has been used in lexicography long before the discipline of corpus linguistics was invented . Samuel Johnson , for example , illustrated his dictionary with examples from literature , and in the 19th Century the Oxford Dictionary used citation slips to study and illustrate word usage . Corpora , however , have changed the way in which linguists can look at language . A linguist who has access to a corpus , or other ( non - representative ) collection of machine readable text can call up all the examples of a word or phrase from many millions of words of text in a few seconds . Dictionaries can be produced and revised much more quickly than before , thus providing up - to - date information about language . Also , definitions can be more complete and precise since a larger number of natural examples are examined . Examples extracted from corpora can be easily organised into more meaningful groups for analysis . For example , by sorting the right - hand context of the word alphabetically so that it is possible to see all instances of a particular collocate together . Furthermore , because corpus data contains a rich amount of textual information - regional variety , author , date , genre , part - of - speech tags etc . it is easier to tie down usages of particular words or phrases as being typical of particular regional varieties , genres and so on . The open - ended ( constantly growing ) monitor corpus has its greatest role in dictionary building as it enables lexicographers to keep on top of new words entering the language , or existing words changing their meanings , or the balance of their use according to genre etc . However , finite corpora also have an important role in lexical studies - in the area of quantification . It is possible to rapidly produce reliable frequency counts and to subdivide these areas across various dimensions according to the varieties of language in which a word is used . Finally , the ability to call up word combinations rather than individual words , and the existence of mutual information tools which establish relationships between co - occurring words mean that we can treat phrases and collocations more systematically than was previously possible . A phraseological unit may constitute a piece of technical terminology or an idiom , and collocations are important clues to specific word senses . Grammatical ( or syntactic ) studies have , along with lexical studies , been the most frequent types of research which have used corpora . Corpora are a useful tool for syntactical research because of : . The potential for the representative quantification of a whole language variety . Their role as empirical data for the testing of hypotheses derived from grammatical theory . Many smaller - scale studies of grammar using corpora have included quantitative data analysis ( for example , Schmied 's 1993 study of relative clauses ) . There is now a greater interest in the more systematic study of grammatical frequency - for example , Oostdijk & de Haan ( 1994 ) are aiming to analyse the frequency of the various English clause types . Since the 1950s the rational - theory based / empiricist - descriptive division in linguistics ( see Section 1 ) has often meant that these two approaches have been viewed as separate and in competition with each other . However , there is a group of researchers who have used corpora in order to test essentially rationalist grammatical theory , rather than use it for pure description or the inductive generation of theory . At Nijmegen University , for instance , primarily rationalist formal grammars are tested on real - life language found in computer corpora ( Aarts 1991 ) . The formal grammar is first devised by reference to introspective techniques and to existing accounts of the grammar of the language . The grammar is then loaded into a computer parser and is run over a corpus to test how far it accounts for the data in the corpus : see Section 5 , Module 3.5 , headed Parsing and tagging . The grammar is then modified to take account of those analyses which it missed or got wrong . Although sociolinguistics is an empirical field of research it has hitherto relied primarily upon the collection of research - specific data which is often not intended for quantitative study and is thus not often rigorously sampled . Sometimes the data are also elicited rather than naturalistic data . A corpus can provide what these kinds of data can not provide - a representative sample of naturalistic data which can be quantified . Although corpora have not as yet been used to a great extent in sociolinguistics , there is evidence that this is a growing field . The majority of studies in this area have concerned themselves with lexical studies in the area of language and gender . Kjellmer ( 1986 ) , for example , used the Brown and LOB corpora to examine the masculine bias in American and British English . He looked at the occurrence of masculine and feminine pronouns , and at the occurrence of the items man / men and woman / women . As one would expect , the frequencies of the female items were much lower than the male items in both corpora . Interestingly , however , the female items were more common in British English than in American English . Another hypothesis of Kjellmer 's was not supported in the corpora - that woman would be less \\\" active \\\" , that is would be more frequently the objects rather than the subjects of verbs . In fact men and women had similar subject / object ratios . Holmes ( 1994 ) makes two important points about the methodology of these kinds of study , which are worth bearing in mind . First , when classifying and counting occurrences the context of the lexical item should be considered . For instance , whilst there is a non - gender marked alternative for policeman / policewoman , namely police officer , there is no such alternative for the -ess form in Duchess of York . The latter form should therefore be excluded from counts of \\\" sexist \\\" suffixes when looking at gender bias in writing . Second , Holmes points out the difficulty of classifying a form when it is actively undergoing semantic change . She argues that the word man can refer both to a single male ( such as in the phrase A 35 year old man was killed , or can have a generic meaning which refers to mankind ( such as Man has engaged in warfare for centuries . In phrases such as we need the right man for the job it is difficult to decide whether man is gender specific or could be replaced by person . These simple points should incite a more critical approach to data classification in further sociolinguistic work using corpora , both within and without the area of gender studies . In this section we have seen how language study has benefited from exploiting corpus data . To summarise , the main important advantages of corpora are : . Sampling and quantification : Because a corpus is sampled to maximally represent the population , any findings taken from the corpus can be generalised to the larger population . Hence quantification in corpus linguistics is more meaningful than other forms of linguistic quantification because it can tell us about a variety of language , not just that which is being analysed . Ease of access : As all of the data collection has been dealt with by someone else , the researcher does not have to go through the issues of sampling , collection and encoding . The majority of corpora are readily available , either free or at low - cost price . Once the corpora have been obtained , it is usually easy to access the data within it , e.g. by using a concordance program . Enriched data : Many corpora have already been enriched with additional linguistic information such as part - of - speech annotation , parsing and prosodic transcription . Hence data retrieval from annotated corpora can be easier and more specific than with unannotated data . Naturalistic data : Corpus data is not always completely unmonitored in the sense that the people producing the spoken or written texts are unaware until after the fact that they are being asked to participate in the building of a corpus . But for the most part , the data are largely naturalistic , unmonitored and the product of real social contexts . Thus the corpus provides one of the most reliable sources of naturally occurring data that can be examined . Imagine that you need to design an LSP corpus as part of the implementation of an LSP - oriented CALL program . Chose an area for the task and try to find appropriate texts from the web in order to populate such a corpus . Having gathered texts , what further processing might you be able to carry out on those texts ? Use a web browser to try to find sites on the web where you could carry out automated part - of - speech tagging . Imagine that you would now like to contrast your LSP corpus with a corpus of general English . Using a Web browser find what corpora of general English are available . What is the relative balance of British English corpora available in comparison to corpora of other varities of English ? Aarts J. ( 1991 ) \\\" Intuition - based and observation - based grammars \\\" . In Aijmer K. & Altenberg B. ( eds . ) English corpus linguistics . Studies in honour of Jan Svartvik , London : Longman : 44 - 62 . Aarts J. & Meijs W. ( eds . ) ( 1986 ) Corpus Linguistics II , Amsterdam : Rodopi . Abercrombie D. ( 1963 ) Studies in phonetics and linguistics , London : Oxford University Press . Baker P. , Hardie A. & McEnery T. ( 2006 ) A glossary of corpus linguistics , Edinburgh : Edinburgh University Press . Aijmer K. & Altenberg B. ( eds . ) ( 1991 ) English corpus linguistics . Studies in honour of Jan Svartvik , London : Longman . Beale A. ( 1987 ) \\\" Towards a distributional lexicon \\\" . In Garside R. , Leech G. & Sampson G. ( eds . ) The computational analysis of English : a corpus based approach . London : Longman . Bernardini S. ( 2000 ) \\\" Systematising serendipity : proposals for concordancing large corpora with language learners \\\" . In Burnard L. & McEnery T. ( eds . ) Rethinking language pedagogy from a corpus perspective : papers from the Third International Conference on Teaching and Language Corpora , Frankfurt am Main : Peter Lang : 225 - 34 . Bernardini , S. ( 2002 ) \\\" Exploring new directions for discovery learning \\\" . In Kettemann B. & Marko G. ( eds . ) Teaching and learning by doing corpus analysis , Amsterdam and New York : Rodopi : 165 - 82 . Biber D. ( 1993 ) \\\" Representativeness in corpus design \\\" , Literary and Linguistic Computing 8 , 4 : 243 - 57 . Bloom L. ( 1970 ) Language development : form and function in emerging grammars , Cambridge , MA : MIT Press . Boas F. ( 1940 ) Race , language and culture , New York : Macmillan . Bongers H. ( 1947 ) The history and principles of vocabulary control , Worden : Wocopi . Braun S. ( 2005 ) \\\" From pedagogically relevant corpora to authentic language learning contents \\\" , ReCALL 17 , 1 : 47 - 64 . Braun S. ( 2007 ) \\\" Integrating corpus work into secondary education : from data - driven learning to needs - driven corpora \\\" , ReCALL 19 , 3 : 307 - 328 . Brown R. ( 1973 ) A first language : the early stages , Cambridge , MA : Harvard University Press . Chambers A. ( 2005 ) \\\" Integrating corpus consultation in language studies \\\" , Language Learning and Technology 9 , 2 : 111 - 125 . Chambers A. , Farr F. & O'Riordan S. ( 2011 ) \\\" Language teachers with corpora in mind : from starting steps to walking tall \\\" , Language Learning Journal 39 , 1 : 85 - 103 . Chomsky N. ( 1964 ) \\\" Formal Discussion \\\" . In Bellugi U. & Brown R. ( eds . ) The acquisition of language . Monographs of the Society for Research in Child Developmen t 29 : 37 - 39 . Chomsky N. ( 1965 ) Aspects of the theory of syntax , Cambridge , MA : MIT Press . Chomsky N. ( 1968 ) Language and mind , New York : Harcourt Brace . Collins Cobuild English Language Dictionary ( 1987 ) ed . John Sinclair , London : Collins . Collins Cobuild English Grammar ( 1990 ) ed . John Sinclair , London : Collins . de Haan P. ( 1984 ) \\\" Problem - oriented tagging of English corpus data \\\" . In Aarts J. & Meijs W. ( eds . ) Corpus linguistics , Amsterdam : Rodopi . Farr F. ( 2007 ) \\\" Spoken language as an aid to reflective practice in language teacher education : using a specialised corpus to establish a generic fingerprint \\\" . In Campoy M.-C. & Luzon M.-J. ( eds . ) Spoken corpora in applied linguistics , Bern : Peter Lang : 235 - 258 . Farr F. ( 2008 ) \\\" Evaluating the use of corpus - based instruction in a language teacher education context : perspectives from the users \\\" , Language Awareness 17 , 1 : 25 - 43 . Farr F. , Chambers A. & O'Riordan S. ( 2010 ) \\\" Corpora for materials development in language teacher education : principles for development and useful data \\\" . In Mishan F. & Chambers A. ( eds . ) Perspectives on language learning materials development , Oxford : Peter Lang : 33 - 61 . Farr F. & Murphy B. ( 2009 ) \\\" Religious references in contemporary Irish - English : ' for the love of God almighty ... I 'm a holy terror for turf \\\" , Journal of Intercultural Pragmatics 6 , 3 : 535 - 560 . Farr F. , B. Murphy & A. O'Keeffe . ( 2004 ) \\\" The Limerick corpus of Irish English : design , description and application \\\" . In Farr F. & O'Keeffe A. ( eds . ) Corpora , Varieties and the Language Classroom , Special Edition of Teanga 21 , Dublin : IRAAL : 5 - 29 . Fligelstone S. ( 1993 ) \\\" Some reflections on the question of teaching , from a corpus linguistics perspective \\\" , ICAME Journal 17 : 97 - 109 . Fries C. & Traver A. ( 1940 ) English word lists : a study of their adaptability and instruction , Washington , DC : American Council of Education . Gaskell D. & T. Cobb . ( 2004 ) \\\" Can learners use concordance feedback for writing errors ? \\\" System 32 , 3 : 301 - 319 . Gavioli L. & Aston . G. ( 2001 ) \\\" Enriching reality : language corpora in language pedagogy \\\" , English Language Teaching Journal 55 , 3 : 238 - 246 . Granger S. ( 2002 ) \\\" A bird's - eye view of learner corpus research \\\" . In Granger S. , Hung J. & Petch - Tyson S. ( eds . ) Computer learner corpora , second language acquisition and foreign language teaching , Amsterdam : John Benjamins Publishing Company : 3 - 33 . Greene B. & Rubin G. ( 1971 ) Automatic grammatical tagging of English , Technical Report , Department of Linguistics , Brown University , RI . Halliday M. & Hasan R. ( 1976 ) Cohesion in English , London : Longman . Harris Z. ( 1951 ) Methods in structural linguistics , Chicago : University of Chicago Press . Hockett C. ( 1948 ) \\\" A note on structure \\\" , International Journal of American Linguistics 14 : 269 - 71 . Hockey S. ( 1980 A guide to computer applications in the humanities , London : Duckworth . Holmes J. ( 1994 ) \\\" Inferring language change from computer corpora : some methodological problems \\\" , ICAME Journal 18 : 27 - 40 . Hundt M. , N. Nesselhauf & C. Biewer ( 2007 ) \\\" Corpus linguistics and the Web \\\" . In Hundt M. , Nesselhauf N. & Biewer C. ( eds . ) Corpus linguistics and the Web , Amsterdam : Rodopi : 1 - 6 . Hunston S. ( 2002 ) Corpora in applied linguistics , Cambridge : Cambridge University Press . Ingram D. ( 1978 ) \\\" Sensori - motor development and language acquisition \\\" . In Lock A ( ed . ) Action , gesture and symbol : the emergence of language , London : Academic Press . Ingram D. ( 1989 ) First language acquisition , Cambride : Cambridge University Press . Johansson S. ( 1991 ) \\\" Times change and so do corpora \\\" . In Aijmer K. & Altenburg B. ( eds . ) English corpus linguistics : studies in honour of Jan Svartvik , London : Longman . Kading J. ( 1879 ) H\\u00e4ufigkeitsw\\u00f6rterbuch der deutschen Sprache , Steglitz : privately published . Karlsson F. , Voutilainen A. , Heikkil\\u00e4 J. & Anttila A. ( eds . ) ( 1995 ) Constraint grammar : a language - independent system for parsing unrestricted text , Berlin : Mouton de Gruyter . Kennedy C. & T. Miceli ( 2001 ) \\\" An evaluation of intermediate students ' approaches to corpus investigation \\\" , Language Learning and Technology 5 , 3 : 77 - 90 . Kennedy G. ( 1992 ) \\\" Preferred ways of putting things \\\" . In Svartvik J. ( ed ) Directions in corpus linguistics , Berlin : Mouton de Gruyter . Kjellmer G. ( 1986 ) \\\" The lesser man : observations on the role of women in modern English writings \\\" . In Aarts J. & Meijs W. ( eds . ) Corpus Linguistics II : 163 - 76 . Labov V. ( 1969 ) \\\" The logic of non - standard English \\\" , Georgetown Monographs on Language and Linguistics 22 . Leech G. ( 1991 ) \\\" The state of the art in corpus linguistics \\\" . In Aijmer K. & Altenberg B. ( eds . ) English corpus linguistics : studies in honour of Jan Svartvik , London : Longman . Leech G. ( 1992 ) \\\" Corpora and theories of linguistic performance \\\" . In Svartvik J. ( ed . ) Directions in corpus linguistics , Berlin : Mouton de Gruyter . Leech G. ( 1993 ) \\\" Corpus annotation schemes \\\" , Literary and Linguistic Computing 8 , 4 : 275 - 81 . McEnery T. & Wilson A. ( 1996 ) Corpus linguistics , Edinburgh : Edinburgh University Press . McEnery T. & Wilson A. ( 1997 ) \\\" Teaching and language corpora \\\" , ReCALL 9 , 1 : 5 - 14 . McEnery T. , Xiao R. & Tone Y. ( 2006 ) C orpus - based language studies . An advanced resource book , London and New York : Routledge . Mauranen A. ( 2003 ) \\\" The corpus of English as a lingua franca in academic setting \\\" , TESOL Quarterly 37 , 3 : 513 - 527 . O'Connor J. & Arnold G. ( 1961 ) Intonation of colloquial English , London : Longman . O'Keeffe A. & Farr F. ( 2003 ) \\\" Using language corpora in language teacher education : pedagogic , linguistic and cultural insights \\\" , TESOL Quarterly 37 , 3 : 389 - 418 . O'Keeffe A. , McCarthy M. & Carter R. ( 2007 ) From corpus to classroom , Cambridge : Cambridge University Press . Oostdijk N. & de Haan P. ( 1994 ) \\\" Clause patterns in Modern British English . A corpus - based ( quantitative ) study \\\" , ICAME Journal 18 . O'Sullivan I. & A. Chambers ( 2006 ) \\\" Learners ' writing skills in French : corpus consultation and learner evaluation \\\" , Journal of Second Language Writing 15 : 49 - 68 . Palmer H. ( 1933 ) Second interim report on English collocations , Tokyo : Institute for Research in English Teaching . Quirk R. ( 1960 ) \\\" Towards a description of English usage \\\" , Transactions of the Philological Society : 40 - 61 . Renouf A. , Kehoe A. & Banerjee J. ( 2007 ) \\\" WebCorp : an integrated system for Web text search \\\" . In Hundt M. , Nesselhauf N. & Biewer C. ( eds . ) Corpus linguistics and the Web , Amsterdam : Rodopi : 47 - 68 . R\\u00f6mer U. ( 2006 ) \\\" Pedagogical applications of corpora : some reflections on the current scope and a wish list for future developments \\\" , Zeitschrift f\\u00fcr Anglistik und Amerikanistik 54 , 2 : 121 - 134 . Sampson G. ( 1992 ) \\\" Probablistic parsing \\\" . In Svartvik , J. ( ed . ) Directions in corpus linguistics , Berlin : Mouton de Gruyter . Schmidt K. M. ( 1993 ) Begriffsglossar und Index zu Ulrichs von Zatzikhoven Lanzelet , T\\u00fcbingen : Niemeyer . Schmied J. ( 1993 ) \\\" Qualitative and quantitative research approaches to English relative constructions \\\" . In Souter C. & Atwell E. ( eds . ) Sedelow S & Sedelow W. ( 1969 ) \\\" Categories and procedures for content analysis in the humanities \\\" . In Gerbner G. , Holsti O. R. , Krippendorff K. , Paisley W.J. & Stone P.J. ( eds . ) The analysis of communication content , New York : John Wiley . Seidlhofer B. ( 2002 ) \\\" Pedagogy and local learner corpora . Working with learning - driven data \\\" . In Granger S. , Hung J. & Petch - Tyson S. ( eds . ) Computer learner corpora , second language acquisition and foreign language teaching , Amsterdam : John Benjamins Publishing Company : 231 - 234 . Seidlhofer B. ( 2004 ) \\\" In search of ' European English ' : or why the corpus ca n't tell us what to teach \\\" , Paper presented at EUROCALL 2004 . Sinclair J. ( 1991 ) Corpus , concordance , collocation , London : Longman . Sinclair J. ( 1995 ) \\\" Corpus typology - a framework for classification \\\" . In Melchers G. & Warren B. ( eds . ) Studies in Anglistics , Stockholm : Almqvist and Wiksell International : 17 - 46 . Sinclair J. ( ed . ) ( 2004 ) How to use corpora in language teaching , Amsterdam : John Benjamins Publishing Company . Souter C. ( 1993 ) \\\" Towards a standard format for parsed corpora \\\" . In Aarts J. , de Haan P. & Oostdijk N. ( eds . ) English language corpora : design , analysis and exploitation , Amsterdam : Rodopi . Souter C. & Atwell E. ( eds . ) ( 1993 ) Corpus - based computational linguistics , Amsterdam : Rodopi . Sperberg - McQueen C.M. & Burnard L. ( 1994 ) G uidelines for electronic text encoding and interchange ( P3 ) , Chicago and Oxford : Text Encoding Initiative . Stenstr\\u00f6m A - B. ( 1984 ) \\\" Discourse tags \\\" . In Aarts J. & Meijs W. ( eds . ) Corpus linguistics , Amsterdam : Rodopi . Svartvik J. ( 1966 ) On voice in the English verb , The Hague : Mouton . Svartvik J. & Quirk R. ( 1980 ) A corpus of English conversation , Lund : C.W.K. Gleerup . Thorndike E. ( 1921 ) A teacher 's wordbook , New York : Columbia Teachers College . Tribble C. ( 1997 ) \\\" Improvising corpora for ELT : quick - and - dirty ways of developing corpora for language teaching \\\" . In Melia J. & Lewandowska - Tomaszczyk B. ( eds . ) PALC 97 Proceedings , Lodz : Lodz University Press . Yoon H. & Hirvela A. ( 2004 ) \\\" ESC student attitudes towards corpus use in L2 writing \\\" , Journal of Second Language Writing 13 : 257 - 283 . Websites . See also the Websites listed in Module 2.4 , Using concordance programs in the Modern Foreign Languages classroom . American English : See ( COCA ) Corpus of Contemporary American English , Brigham Young University . See also American English : Google Books , a new interface for Google Books allows you to search more than 155 billion words in more than 1.3 million books of American English from 1810 - 2009 ( including 62 billion words from 1980 - 2009 ) . Collins Cobuild Bank of English : A project initiated by John Sinclair at the University of Birmingham in the 1980s that led to the publication of a series of dictionaries and reference books . Search for Cobuild at the HarperCollins website for the range of publications that are available both in book and in digital format . See also Wordbanks Online . Corpora4Learning.net : Links and references for the use of corpora , corpus linguistics and corpus analysis in the context of language learning and teaching . Created by Sabine Braun , University of Surrey . Google Books : American English : This new interface for Google Books allows you to search more than 155 billion words in more than 1.3 million books of American English from 1810 - 2009 ( including 62 billion words from 1980 - 2009 ) . University of Lancaster : University Centre for Computer Corpus Research on Language ( UCREL ) . Virtual Linguistics Campus , University of Marburg : Includes a virtual lecture hall where the student can attend linguistics courses , a linguistics lab , chat rooms , message boards , etc . Wordbanks Online is an online corpus that evolved out of the Collins Cobuild Bank of English corpus that forms the basis of the Collins range of dictionaries and reference books . Wordbanks Online is available by subscription , but there is also a trial version online . Document last updated 27 March 2012 . This page is maintained by Graham Davies . Please cite this Web page as : McEnery T. & Wilson A. ( 2012 ) Corpus linguistics . Module 3.4 in Davies G. ( ed . ) Information and Communications Technology for Language Teachers ( ICT4LT ) , Slough , Thames Valley University [ Online]. \"}",
        "_version_":1692669052091105280,
        "score":20.532017},
      {
        "id":"046ea5e9-6431-475e-9e48-aa76ffc1250a",
        "_src_":"{\"url\": \"http://www.davidbrewster.com/2007/05/07/in-fact-web-20-is-simplicity-defined/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168076.20/warc/CC-MAIN-20160205193928-00332-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Acronyms & Glossary . Glossary . There are terms relevant to artificial intelligence , knowledge bases and semantic technologies . The \\\" official \\\" ones used by Structured Dynamics in its various projects and products are provided in alphabetical order below . Most definitions are from Wikipedia or standards groups , except in those cases where they are terms of art of SD initiatives . Terms in bold are found elsewhere in the glossary . Acronyms . Listed at the bottom part of this page are acronyms and definitions related to artificial intelligence , knowledge bases and semantic technologies . Most definitions are from Wikipedia , with the remaining from the appropriate standards group . Glossary Listings . An ABox ( for assertions , the basis for A in ABox ) is an \\\" assertion component \\\" ; that is , a fact associated with a terminological vocabulary within a knowledge base . ABox are TBox -compliant statements about instances belonging to the concept of an ontology . Instances and instance records reside within the ABox . A statistical measure of how well a binary classification test correctly identifies or excludes a condition . It is calculated as the sum of true positives and true negatives divided by the total population . An annotation , specifically as an annotation property , is a way to provide metadata or to describe vocabularies and properties used within an ontology . Annotations do not participate in reasoning or coherency testing for ontologies . These are the aspects , features , characteristics , or descriptors that qualify individual entities . Attributes are the way we describe and characterize individual things . Key - value pairs match an attribute with a value ; the value may be a reference to another object , an actual value or a descriptive label or string . In an RDF statement , an attribute is expressed as a property ( or predicate or relation ) . In intensional logic , all attributes or characteristics of similarly classifiable items define the membership in that set . Attribute type . An aggregation ( or class ) of multiple attributes that have similar characteristics amongst themselves . As with other types , shared characteristics are subsumed over some essence ( s ) that give the type its unique character . Also called a bnode , a blank node in RDF is a resource for which a URI or literal is not given . A blank node indicates the existence of a thing , implied by the structure of the knowledge graph , but which was never explicitly identified by giving it a URI . Blank nodes have no meaning outside of their current graph and therefore can not be mapped to other resources or graphs . A class is a collection of sets or instances ( or sometimes other mathematical objects ) which can be unambiguously defined by a property that all of its members share . In ontologies , classes may also be known as sets , collections , concepts , types of objects , or kinds of things . CWA is the presumption that what is not currently known to be true , is false . CWA also has a logical formalization . CWA is the most common logic applied to relational database systems , and is particularly useful for transaction - type systems . See contrast to the open world assumption . A common - sense knowledge base that has been under development for over 20 years backed by 1000 person - years of effort . The smaller OpenCyc version is available in OWL as open source ; a ResearchCyc version of the entire system is available to researchers . The Cyc platform contains its own logic language , CycL , and has many buillt - in functions in areas such as natural language processing , search , inferencing and the like . UMBEL is based on a subset of Cyc . A project that extracts structured content from Wikipedia , and then makes that data available as linked data . There are millions of entities characterized by DBpedia in this way . As such , DBpedia is one of the largest - and most central - hubs for linked data on the Web . Description logics and their semantics traditionally split concepts and their relationship s from the different treatment of instances and their attributes and roles , expressed as fact assertions . The concept split is known as the TBox and represents the schema or taxonomy of the domain at hand . The TBox is the structural and intensional component of conceptual relationships . The second split of instances is known as the ABox and describes the attributes of instances ( and individuals ) , the roles between instances , and other assertions about instances regarding their class membership with the TBox concepts . Distant supervision . A method to use knowledge bases to label entities automatically in text through machine learning , which is then used to extract features and train a machine learning classifier . The knowledge bases provide coherent positive training examples and avoid the high cost and effort of manual labeling . The collection of objects and their relationships germane to a particular discourse or scope of inquiry . The domain bounds the scope of a given knowledge representation project . Scoping the domain is one of the first activities undertaken in a new KR project . Domain ( or content ) ontologies embody more of the traditional ontology functions such as information interoperability , inferencing , reasoning and conceptual and knowledge capture of the applicable domain . The basic , real things in our domain of interest . An entity is an individual object or member of a class ; when affixed with a proper name or label is also known as a named entity ( thus , named entities are a subset of all entities ) . Entities are described and characterized by attributes . Entities are connected or related to one another through relations . EAV is a data model to describe entities where the number of attributes ( properties , parameters ) that can be used to describe them is potentially vast , but the number that will actually apply to a given entity is relatively modest . In the EAV data model , each attribute - value pair is a fact describing an entity . EAV systems trade off simplicity in the physical and logical structure of the data for complexity in their metadata , which , among other things , plays the role that database constraints and referential integrity do in standard database designs . The extension of a class , concept , idea , or sign consists of the things to which it applies , in contrast with its intension . For example , the extension of the word \\\" dog \\\" is the set of all ( past , present and future ) dogs in the world . The extension is most akin to the attributes or characteristics of the instances in a set defining its class membership . An error where a test result indicates that a condition failed , while it actually was successful . That is , the test result indicates a negative , when the correct result should have been positive . Also known as a false negative error or Type II error in statistics . It is abbreviated FN . An error where a test result indicates that a condition was met or achieved , while it actually should have failed . That is , the test result indicates a positive , when the correct result should have been negative . Also known as a false positive error or Type I error in statistics . It is abbreviated FP . GRDDL is a markup format for Gleaning Resource Descriptions from Dialects of Languages ; that is , for getting RDF data out of XML and XHTML documents using explicitly associated transformation algorithms , typically represented in XSLT . A high - level subject is both a subject proxy and category label used in a hierarchical subject classification scheme ( taxonomy ) . Higher - level subjects are classes for more atomic subjects , with the height of the level representing broader or more aggregate classes . Inference is the act or process of deriving logical conclusions from premises known or assumed to be true . The logic within and between statements in an ontology is the basis for inferring new conclusions from it , using software applications known as inference engines or reasoners . Instances are the basic , \\\" ground level \\\" components of an ontology . An instance is an individual member of a class , also used synonomously with entity . The instances in an ontology may include concrete objects such as people , animals , tables , automobiles , molecules , and planets , as well as abstract instances such as numbers and words . An instance is also known as an individual , with member and entity also used somewhat interchangeably . irON ( instance record and Object Notation ) is a abstract notation and associated vocabulary for specifying RDF ( Resource Description Framework ) triples and schema in non - RDF forms . Its purpose is to allow users and tools in non - RDF formats to stage interoperable datasets using RDF . The intension of a class is what is intended as a definition of what characteristics its members should have ; it is akin to a definition of a concept and what is intended for a class to contain . It is therefore like the schema aspects ( or TBox ) in an ontology . Also known as a name - value pair or attribute - value pair , a key - value pair is a fundamental , open - ended data representation . The key is the defined attribute and the value may be a reference to another object or a literal string or value . In RDF triple terms , the subject is implied in a key - value pair by nature of the instance record at hand . A knowledge base ( abbreviated KB or kb ) is a special kind of database for knowledge management . A knowledge base provides a means for information to be collected , organized , shared , searched and utilized . Formally , the combination of a TBox and ABox is a knowledge base . A field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks . Knowledge supervision . A method of machine learning to use knowledge bases in a purposeful way to create features , and negative and positive training sets in order to train the classifiers or extractors . Distant supervision also uses knowledge bases , but not is such a purposeful , directed manner across multiple machine learning problems . Linked data is a set of best practices for publishing and deploying instance and class data using the RDF data model , and uses uniform resource identifiers ( URIs ) to name the data objects . The approach exposes the data for access via the HTTP protocol , while emphasizing data interconnections , interrelationships and context useful to both humans and machine agents . The construction of algorithms that can learn from and make predictions on data by building a model from example inputs . A wide variety of techniques and algorithms ranging from supervised to unsupervised may be employed . It is \\\" data about data \\\" , or the means by which data objects or aggregations can be described . Contrasted to an attribute , which is an individual characteristic intrinsic to a data object or instance , metadata is a description about that data , such as how or when created or by whom . Microdata is a proposed specification used to nest semantics within existing content on web pages . Microdata is an attempt to provide a simpler way of annotating HTML elements with machine - readable tags than the similar approaches of using RDFa or microformats . A microformat ( sometimes abbreviated \\u03bcF or uF ) is a piece of mark up that allows expression of semantics in an HTML ( or XHTML ) web page . Programs can extract meaning from a web page that is marked up with one or more microformats . NLP is the process of a computer extracting meaningful information from natural language input and/or producing natural language output . NLP is one method for assigning structured data characterizations to text content for use in semantic technologies . ( Hand assignment is another method . ) Information extraction ( IE ) is the task of automatically extracting structured information from unstructured and/or semi - structured machine - readable documents . Ontology - based information extraction ( OBIE ) is the use of an ontology to inform a \\\" tagger \\\" or information extraction program when doing natural language processing . Input ontologies thus become the basis for generating metadata tags when tagging text or documents . An object is anything we can think about or talk about . In their use in semantic technologies , objects are nouns and always given a URI ( bnodes can also act in the object position but they lack a persistent URI ) . An ontology is a data model that represents a set of concepts within a domain and the relationships between those concepts . Loosely defined , ontologies on the Web can have a broad range of formalism , or expressiveness or reasoning power . Ontology - driven applications ( or ODapps ) are modular , generic software applications designed to operate in accordance with the specifications contained in one or more ontologies . The relationships and structure of the information driving these applications are based on the standard functions and roles of ontologies ( namely as domain ontologies ) , as supplemented by UI and instruction sets and validations and rules . The open semantic framework , or OSF , is a combination of a layered architecture and an open - source , modular software stack . The stack combines many leading third - party software packages with open source semantic technology developments from Structured Dynamics . OWA is a formal logic assumption that the truth - value of a statement is independent of whether or not it is known by any single observer or agent to be true . OWA is used in knowledge representation to codify the informal notion that in general no single agent or observer has complete knowledge , and therefore can not make the closed world assumption . The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true . OWA is useful when we represent knowledge within a system as we discover it , and where we can not guarantee that we have discovered or will discover complete information . In the OWA , statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown , rather than wrong or false . Semantic Web languages such as OWL make the open world assumption . See contrast to the closed world assumption . The Web Ontology Language ( OWL ) is designed for defining and instantiating formal Web ontologies . An OWL ontology may include descriptions of classes , along with their related properties and instances . There are also a variety of OWL dialects . The fraction of retrieved documents that are relevant to the query . It is measured as true positives divided by all measured positives ( true and false ) . High precision indicates a high percentage of true positives in relation to all positive results . Properties are the ways in which classes and instances can be related to one another . Between objects , properties are thus a relationship , and are also known as predicates . Properties are used to define an attribute or relation for an instance . In computer science , punning refers to a programming technique that subverts or circumvents the type system of a programming language , by allowing a value of a certain type to be manipulated as a value of a different type . When used for ontologies , it means to treat a thing as both a class and an instance , with the use depending on context . Resource Description Framework ( RDF ) is a family of World Wide Web Consortium ( W3C ) specifications originally designed as a metadata model but which has come to be used as a general method of modeling information , through a variety of syntax formats . The RDF metadata model is based upon the idea of making statements about resources in the form of subject - predicate - object expressions , called triples in RDF terminology . The subject denotes the resource , and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object . RDFa uses attributes from meta and link elements , and generalizes them so that they are usable on all elements allowing annotation markup with semantics . RDFa 1.1 is a W3C Recommendation that removes prior dependence on the XML namespace and expands HTML5 and SVG support , among other changes . RDFS or RDF Schema is an extensible knowledge representation language , providing basic elements for the description of ontologies , otherwise called RDF vocabularies , intended to structure RDF resources . A semantic reasoner , reasoning engine , rules engine , or simply a reasoner , is a piece of software able to infer logical consequences from a set of asserted facts or axioms . The notion of a semantic reasoner generalizes that of an inference engine , by providing a richer set of mechanisms . Reasoning . Reasoning is one of many logical tests using inference rules as commonly specified by means of an ontology language , and often a description language . Many reasoners use first - order predicate logic to perform reasoning ; inference commonly proceeds by forward chaining or backward chaining . The fraction of the documents that are relevant to the query that are successfully retrieved . It is measured as true positives divided by all potential positives that could be returned from the corpus . High recall indicates a high yield in obtaining relevant results . Any of the noun objects within UMBEL , and abbreviated as RC . An RC may be either an entity , entity type , attribute , attribute type , relation , relation type , topic or abstract concept . There are presently about 35 K RCs in UMBEL . All RCs are objects . An aggregation ( or class ) of multiple relations that have similar characteristics amongst themselves . As with other types , shared characteristics are subsumed over some essence ( s ) that give the type its unique character . Schema.org is an initiative launched by the major search engines of Bing , Google and Yahoo ! , and later jointed by Yandex , in order to create and support a common set of schema for structured data markup on web pages . schema.org provided a starter set of schema and extension mechanisms for adding to them . schema.org supports markup in microdata , microformat and RDFa formats . Semantic technologies are a combination of software and semantic specifications that encode meanings separately from data and content files and separately from application code . This approach enables machines as well as people to understand , share and reason with data and specifications separately . With semantic technologies , adding , changing and implementing new relationships or interconnecting programs in a different way can be as simple as changing the external model that these programs share . New data can also be brought into the system and visualized or worked upon based on the existing schema . Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data , content , and processes . The Semantic Web is a collaborative movement led by the World Wide Web Consortium ( W3C ) that promotes common formats for data on the World Wide Web . By encouraging the inclusion of semantic content in web pages , the Semantic Web aims at converting the current web of unstructured documents into a \\\" web of data \\\" . It builds on the W3C 's Resource Description Framework ( RDF ) . A semset is the use of a series of alternate labels and terms to describe a concept or entity . These alternatives include true synonyms , but may also be more expansive and include jargon , slang , acronyms or alternative terms that usage suggests refers to the same concept . Semantically - Interlinked Online Communities Project ( SIOC ) is based on RDF and is an ontology defined using RDFS for interconnecting discussion methods such as blogs , forums and mailing lists to each other . SKOS or Simple Knowledge Organisation System is a family of formal languages designed for representation of thesauri , classification schemes , taxonomies , subject - heading systems , or any other type of structured controlled vocabulary ; it is built upon RDF and RDFS . A statement is a \\\" triple \\\" in an ontology , which consists of a subject - predicate - object ( S - P - O ) assertion . By definition , each statement is a \\\" fact \\\" or axiom within an ontology . Subject . A subject is always a noun or compound noun and is a reference or definition to a particular object , thing or topic , or groups of such items . Subjects are also often referred to as concepts or topics . Subject extraction . Subject extraction is an automatic process for retrieving and selecting subject names from existing knowledge bases or data sets . Extraction methods involve parsing and tokenization , and then generally the application of one or more information extraction techniques or algorithms . Subject proxy . A subject proxy as a canonical name or label for a particular object ; other terms or controlled vocabularies may be mapped to this label to assist disambiguation . A subject proxy is always representative of its object but is not the object itself . SuperType . One of about 30 segregated splits within UMBEL that are mostly disjoint from one another and mostly conform to broad groupings of entities . SuperTypes are a major organizational dimension of UMBEL . A machine learning task of inferring a function from labeled training data , which optimally consists of positive and negative training sets . The supervised learning algorithm analyzes the training data and produces an inferred function to correctly determine the class labels for unseen instances . A tag is a keyword or term associated with or assigned to a piece of information ( e.g. , a picture , article , or video clip ) , thus describing the item and enabling keyword - based classification of information . Tags are usually chosen informally by either the creator or consumer of the item . A TBox ( for terminological knowledge , the basis for T in TBox ) is a \\\" terminological component \\\" ; that is , a conceptualization associated with a set of facts . TBox statements describe a conceptualization , a set of concepts and properties for these concepts . The TBox is sufficient to describe an ontology ( best practice often suggests keeping a split between instance records - and ABox - and the TBox schema ) . In the context of knowledge systems , taxonomy is the hierarchical classification of entities of interest of an enterprise , organization or administration , used to classify documents , digital assets and other information . Taxonomies can cover virtually any type of physical or conceptual entities ( products , processes , knowledge fields , human groups , etc . ) at any level of granularity . The topic ( or theme ) is the part of the proposition that is being talked about ( predicated ) . In topic maps , the topic may represent any concept , from people , countries , and organizations to software modules , individual files , and events . Topics and subjects are closely related . Topic maps are an ISO standard for the representation and interchange of knowledge . A topic map represents information using topics , associations ( similar to a predicate relationship ) , and occurrences ( which represent relationships between topics and information resources relevant to them ) , quite similar in concept to the RDF triple . A set of data used to discover potentially predictive relationships . In supervised learning , a positive training set provides data that meets the training objectives ; a negative training set fails to meet the objectives . Triple . A basic statement in the RDF language , which is comprised of a subject - property - object construct , with the subject and property ( and object optionally ) referenced by URIs . Is a flat , hierarchical taxonomy comprised of related entity types within the context of a given UMBEL SuperType ( ST ) . Typologies are a critical connection point between the TBox and ABox . The link shown here uses an archaeology example . This vocabulary is also designed for interoperable domain ontologies . An upper ontology ( also known as a top - level ontology or foundation ontology ) is an ontology that describes very general concepts that are the same across all knowledge domains . An important function of an upper ontology is to support very broad semantic interoperability between a large number of ontologies that are accessible ranking \\\" under \\\" this upper ontology . A vocabulary in the sense of knowledge systems or ontologies are controlled vocabularies . They provide a way to organize knowledge for subsequent retrieval . They are used in subject indexing schemes , subject headings , thesauri , taxonomies and other form of knowledge organization systems . This is a crowdsourced , open knowledge base of ( currently ) about 18 million structured entity records . Each record consists of attributes and values with robust cross - links to multiple languages . Wikidata is a key entities source . Wikipedia is a crowdsourced , free - access and free - content knowledge base of human knowledge . It has nearly 5 million articles in its English version . Across all Wikipedias there are nearly 35 million articles in 288 different language versions . WordNet is a lexical database for the English language . It groups English words into sets of synonyms called synsets , provides short , general definitions , and records the various semantic relations between these synonym sets . The purpose is twofold : to produce a combination of dictionary and thesaurus that is more intuitively usable , and to support automatic text analysis and artificial intelligence applications . The database and software tools can be downloaded and used freely . Multiple language versions exist , and WordNet is a frequent reference structure for semantic applications . \"}",
        "_version_":1692670552485920769,
        "score":20.3211},
      {
        "id":"97283e74-30d0-410b-a366-4ca03d23c38d",
        "_src_":"{\"url\": \"http://www.starkravingmadmommy.com/2010/09/earlier-this-week-i-wrote-about-super.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162903.38/warc/CC-MAIN-20160205193922-00183-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Jahrestagung der Deutschen Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie ( gmds ) 19 . Jahrestagung der Schweizerischen Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) Jahrestagung 2004 des Arbeitskreises Medizinische Informatik ( \\u00d6AKMI ) . Deutsche Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie Schweizerische Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) . Artikel . Suche in Medline nach . Kornel Marko - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Stefan Schulz - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Joachim Wermter - Institut f\\u00fcr Med . Biometrie und Med . Informatik , Universit\\u00e4tsklinikum Freiburg , Freiburg , Deutschland . Michael Poprat - Arbeitsgruppe Computerlinguistik , Universit\\u00e4t Freiburg , Freiburg , Deutschland . Udo Hahn - Arbeitsgruppe Computerlinguistik , Universit\\u00e4t Freiburg , Freiburg , Deutschland . Kooperative Versorgung - Vernetzte Forschung - Ubiquit\\u00e4re Information . Jahrestagung der Deutschen Gesellschaft f\\u00fcr Medizinische Informatik , Biometrie und Epidemiologie ( gmds ) , 19 . Jahrestagung der Schweizerischen Gesellschaft f\\u00fcr Medizinische Informatik ( SGMI ) und Jahrestagung 2004 des Arbeitskreises Medizinische Informatik ( \\u00d6AKMI ) der \\u00d6sterreichischen Computer Gesellschaft ( OCG ) und der \\u00d6sterreichischen Gesellschaft f\\u00fcr Biomedizinische Technik ( \\u00d6GBMT ) . Innsbruck , 26.-30.09.2004 . D\\u00fcsseldorf , K\\u00f6ln : German Medical Science ; 2004 . Doc04gmds065 . \\u00a9 2004 Marko et al . Er darf vervielf\\u00e4ltigt , verbreitet und \\u00f6ffentlich zug\\u00e4nglich gemacht werden , vorausgesetzt dass Autor und Quelle genannt werden . Gliederung . Introduction . Medical information retrieval ( IR ) presents a unique combination of challenges for the design and implementation of retrieval engines [ 1 ] . First of all , clinical document collections and medical databases are usually very large and dynamic . Second , medical document collections are truly multi - lingual . While clinical documents are typically written in the physicians ' native language , searches in major bibliographic databases such as MEDLINE require sophisticated knowledge of ( expert - level ) English medical terminology which most non - English speaking physicians do not have . Third , medical terminology is morphologically extremely productive , characterized by a typical mix of Latin and Greek roots with the corresponding host language , e.g. in words such as pseudohypoparathyroidism , Gastrointestinaltrakt , etc . Obviously , dealing with such phenomena is crucial for any medical IR system . We respond to these challenges in terms of the MORPHOSAURUS system ( an acronym for MORPHeme and the SAURUS ) . At its core lies a special type of dictionary , in which the entries are equivalence classes of subwords , i.e. , semantically minimal units [ 2 ] . These equivalence classes capture intralingual as well as interlingual synonymy . We evaluate these two fundamentally different approaches on a large medical document collection ( the Ohsumed corpus [ 3 ] ) . Morpho - semantic Normalization . Figure 1 [ Fig . 1 ] depicts how source documents are converted into a morpho - semantically normalized , interlingual representation by a three - step procedure . The first step deals with orthographic normalization . A preprocessor reduces all capitalized characters from input documents to lower - case characters and , additionally , performs language - specific character substitutions ( e.g. the replacement of German umlauts ) . The next step in the pipeline is concerned with morphological segmentation . The system segments the input stream into a sequence of semantically plausible sublexical items , corresponding to subwords as found in the lexicon . Currently , the subword lexicon contains about 57,000 entries with 21,000 for both German and English and 15,000 for Portuguese . In the final step , semantic normalization , each content bearing subword recognized is substituted by its corresponding equivalence class ( called MorphoSaurus identifier - MID ) . After that step , all synonyms within a language and all translations of semantically equivalent subwords from different languages are represented by the same code in that target representation . Experimental Settings . Our experiments were run on the Ohsumed corpus [ 3 ] , which constitutes one of the standard IR testbeds for the medical domain . Ohsumed is a subset of the MEDLINE database . Considering those documents which contained abstracts ( some did not ) , we obtained a document collection comprised of 233,445 texts with 41 million tokens , in total . Since the Ohsumed corpus was created specifically for IR studies , 106 queries are available , including associated relevance judgments . The following is a typical query : \\\" Are there adverse effects on lipids when progesterone is given with estrogen replacement therapy ? \\\" Since the Ohsumed corpus contains only English - language documents the question arises how this collection ( or MEDLINE , in general ) can be accessed from other languages as well . Query translation ( QTR ) can be regarded as a standard , and often preferred experimental procedure in the cross - language retrieval community [ 4 ] . In our experiments , the original English queries were first translated into Portuguese and German by medical experts ( native speakers of Portuguese or German , with a very good mastery of both general and medical English ) . In the second step , the manually translated queries were re - translated into English using the Google Translator . Additionally , for covering the medical sublanguage , we used a bilingual lexeme dictionary derived from the UMLS Metathesaurus [ 5 ] with about 26,000 German - English entries and 14,200 entries for Portuguese - English . As an alternative to QTR , we probed the MorphoSaurus indexing approach ( MSI ) . Unlike QTR , the normalization of documents and queries yields a language - independent , semantically normalized index format . As the baseline for our experiments , we provide a retrieval system operating with a word stemmer and a stopword list running on ( original ) English documents with ( original ) English queries . For an unbiased evaluation , we basically used a simple Boolean search approach incorporating adjacency metrics . Results . It is not surprising that the English - English baseline performs best with an 11pt average ( a standard metrics in IR ) of 0.14 ( cf . [ Tab . 1 ] ) . The German - English MSI approach result is almost on a par with the baseline ( 0.01 less ( 0.13 ) ) , whereas the German - English QTR result is more than 0.05 points worse ( 0.09 ) . This means that the MSI approach achieved 93 % of the baseline performance ( quite a high score given cross - language IR standards ) , whereas the QTR approach scored far lower ( 62 % ) . This difference turns out to be less dramatic , but still noticeable , in comparing the Portuguese - English MSI and QTR results with the baseline ( 68 % for MSI and 54 % for QTR , hence , 14 percentage points difference ) . Both the MSI and the QTR 11pt averages are much lower for the Portuguese - English retrieval case . In any case , it seems worth noting that at no single recall point QTR values were higher than MSI values . Hence , the latter consistently outperformed the former on both languages . Interesting from a realistic retrieval perspective is the average gain on the top two recall points . In Table 1 [ Tab . 1 ] the Portuguese - English MSI condition achieves a precision of 0.26 ( 72 % of the baseline ) , the German - English condition yields a precision value of 0.32 ( 90 % of the baseline ) for MSI . Conclusion . The success of dictionary - based cross - language IR largely depends on the coverage of underlying lexicons . We optimize the lexical coverage by limiting the lexicon to semantically relevant subwords . Based on this architecture we presented an interlingua approach to cross - language information retrieval on a medical document collection . Compared to state - of - the - art direct translation techniques we achieved a remarkable benefit , at least for German by reaching 93 % of the English baseline . Acknowledgments . This work was partly funded by Deutsche Forschungsgemeinschaft ( DFG ) , grant Klar 640/5 - 1 . Eichmann D , Ruiz ME , Srinivasan P. Cross - language Information Retrieval with the UMLS Metathesaurus . Proc . 21st Intl . ACM SIGIR Conference on Research and Development in Information Retrieval ; 1998:72 - 80 \"}",
        "_version_":1692668192228376577,
        "score":20.122574},
      {
        "id":"37fb37df-baa0-4ca3-bfa6-10fa8492320b",
        "_src_":"{\"url\": \"http://winecast.net/2007/01/23/st-francis-zinfandel-old-vines-2004/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148558.5/warc/CC-MAIN-20160205193908-00235-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"The Sweet Compendium of Ontology Building Tools . 140 Tools : 20 Must Haves , 70 Possible Usefuls , and 50 Has Beens and Marginals . Well , for another client and another purpose , I was goaded into screening my Sweet Tools listing of semantic Web and -related tools and to assemble stuff from every other nook and cranny I could find . The net result is this enclosed listing of some 140 or so tools - most open source - related to semantic Web ontology building in one way or another . That interest , in no small measure , is why I continue to maintain the Sweet Tools listing . As far as I know , the following is the largest and most comprehensive listing of ontology building tools available . I broadly interpret the classification of ' ontology building ' ; I include , for example , vocabulary extraction and prompting tools , as well as ontology visualization and mapping . There are some 140 tools , perhaps 90 or so are still in active use . ( Given the scope , not every tool could be inspected in detail . Some listed as being perhaps inactive may not be so , and others not in that category perhaps should be . ) Of the entire roster of tools , somewhere on the order of 12 to 20 are quite impressive and deserving of local installation , test runs , and close inspection . There are relatively few tools useful to non - specialists ( or useful to engaging knowledgeable publics in the ontology - building exercise ) . There appear to be key gaps in the entire workflow from domain scoping and initial ontology definition and vocabulary candidates , to longer - term maintenance and revision . For example , spreadsheets would appear to be a possible useful first step in any workflow process ( which is why irON is listed ) , but the spreadsheet tool per se is not listed herein ( nor are text editors ) . I surely have missed some tools and likely improperly assigned others . Please drop me an email or comment on this post with any revisions or suggestions . Some Worth A Closer Look . In my own view , there are some tools that definitely deserve a closer look . Each one of these links is more fully described below . Also , all tools in the Vocabulary Prompting Tools category ( which also includes extraction ) are worth reviewing since all or nearly all have online demos . Other tools may also be deserving , depending on use case . Some of the more specific analysis and conversion tools , for example , are in the Miscellaneous category . Also , some purists may quibble with why some tools are listed here ( such as inclusion of some stuff related to Topic Maps ) . Well , my answer to that is there are no real complete solutions , and whatever we can pragmatically do today requires glueing together many disparate parts . Comprehensive Ontology Tools . Altova SemanticWorks is a visual RDF and OWL editor that auto - generates RDF / XML or nTriples based on visual ontology design . No open source version available . Amine is a rather comprehensive , open source platform for the development of intelligent and multi - agent systems written in Java . As one of its components , it has an ontology GUI with text- and tree - based editing modes , with some graph visualization . The Apelon DTS ( Distributed Terminology System ) is an integrated set of open source components that provides comprehensive terminology services in distributed application environments . DTS supports national and international data standards , which are a necessary foundation for comparable and interoperable health information , as well as local vocabularies . Typical applications for DTS include clinical data entry , administrative review , problem - list and code - set management , guideline creation , decision support and information retrieval . Though not strictly an ontology management system , Apelon DTS has plug - ins that provide visualization of concept graphs and related functionality that make it close to a complete solution . DOME is a programmable XML editor which is being used in a knowledge extraction role to transform Web pages into RDF , and available as Eclipse plug - ins . DOME stands for DERI Ontology Management Environment . FlexViz is a Flex - based , Prot\\u00e9g\\u00e9 - like client - side ontology creation , management and viewing tool ; very impressive . The code is distributed from Sourceforge ; there is a nice online demo available ; there is a nice explanatory paper on the system , and the developer , Chris Callendar , has a useful blog with Flex development tips . Knoodl facilitates community - oriented development of OWL based ontologies and RDF knowledge bases . It also serves as a semantic technology platform , offering a Java service - based interface or a SPARQL - based interface so that communities can build their own semantic applications using their ontologies and knowledgebases . It is hosted in the Amazon EC2 cloud and is available for free ; private versions may also be obtained . See especially the screencast for a quick introduction . The NeOn toolkit is a state - of - the - art , open source multi - platform ontology engineering environment , which provides comprehensive support for the ontology engineering life - cycle . The v2.3.0 toolkit is based on the Eclipse platform , a leading development environment , and provides an extensive set of plug - ins covering a variety of ontology engineering activities . You can add these plug - ins or get a current listing from the built - in updating mechanism . ontopia is a relative complete suite of tools for building , maintaining , and deploying Topic Maps - based applications ; open source , and written in Java . Could not find online demos , but there are screenshots and there is visualization of topic relationships . Prot\\u00e9g\\u00e9 is a free , open source visual ontology editor and knowledge - base framework . The Prot\\u00e9g\\u00e9 platform supports two main ways of modeling ontologies via the Prot\\u00e9g\\u00e9 - Frames and Prot\\u00e9g\\u00e9 - OWL editors . Prot\\u00e9g\\u00e9 ontologies can be exported into a variety of formats including RDF(S ) , OWL , and XML Schema . There are a large number of third - party plugins that extends the platform 's functionality . Prot\\u00e9g\\u00e9 Plugin Library - frequently consult this page to review new additions to the Prot\\u00e9g\\u00e9 editor ; presently there are dozens of specific plugins , most related to the semantic Web and most open source . Collaborative Prot\\u00e9g\\u00e9 is a plug - in extension of the existing Prot\\u00e9g\\u00e9 system that supports collaborative ontology editing as well as annotation of both ontology components and ontology changes . In addition to the common ontology editing operations , it enables annotation of both ontology components and ontology changes . It supports the searching and filtering of user annotations , also known as notes , based on different criteria . There is also an online demo . TopBraid Composer is an enterprise - class modeling environment for developing Semantic Web ontologies and building semantic applications . Fully compliant with W3C standards , Composer offers comprehensive support for developing , managing and testing configurations of knowledge models and their instance knowledge bases . It is based on the Eclipse IDE . There is a free version ( after registration ) for small ontologies . Not Apparently in Active Use . Adaptiva is a user - centred ontology building environment , based on using multiple strategies to construct an ontology , minimising user input by using adaptive information extraction . Exteca is an ontology - based technology written in Java for high - quality knowledge management and document categorisation , including entity extraction . Though code is still available , no updates have been provided since 2006 . It can be used in conjunction with search engines . IODT is IBM 's toolkit for ontology - driven development . The toolkit includes EMF Ontolgy Definition Metamodel ( EODM ) , EODM workbench , and an OWL Ontology Repository ( named Minerva ) . KAON is an open - source ontology management infrastructure targeted for business applications . It includes a comprehensive tool suite allowing easy ontology creation and management and provides a framework for building ontology - based applications . An important focus of KAON is scalable and efficient reasoning with ontologies . Ontolingua provides a distributed collaborative environment to browse , create , edit , modify , and use ontologies . The server supports over 150 active users , some of whom have provided us with descriptions of their projects . Provided as an online service ; software availability not known . Vocabulary Prompting Tools . AlchemyAPI from Orchestr8 provides an API based application that uses statistical and natural language processing methods . Applicable to webpages , text files and any input text in several languages . BooWa is a set expander for any language ( formerly known as SEALS ) ; developed by RC Wang of Carnegie Mellon . Google Keywords allows you to enter a few descriptive words or phrases or a site URL to generate keyword ideas . Google Sets for automatically creating sets of items from a few examples . The metadata results are stored centrally and returned to you as industry - standard RDF constructs accompanied by a Globally Unique Identifier ( GUID ) . Query - by - document from BlogScope has a nice phrase extraction service , with a choice of ranking methods . Can also be used in a Firefox plug - in ( not texted with 3.5 + ) . SemanticHacker ( from Textwise ) is an API that does a number of different things , including categorization , search , etc . By using ' concept tags ' , the API can be leveraged to generate metadata or tags for content . TagFinder is a Web service that automatically extracts tags from a piece of text . The tags are chosen based on both statistical and linguistic analysis of the original text . Tagthe.net has a demo and an API for automatic tagging of web documents and texts . Tags can be single words only . The tool also recognizes named entities such as people names and locations . TermExtractor extracts terminology consensually referred in a specific application domain . The software takes as input a corpus of domain documents , parses the documents , and extracts a list of \\\" syntactically plausible \\\" terms ( e.g. compounds , adjective - nouns , etc . ) . TermFinder uses Poisson statistics , the Maximum Likelihood Estimation and Inverse Document Frequency between the frequency of words in a given document and a generic corpus of 100 million words per language ; available for English , French and Italian . TerMine is an online and batch term extractor that emphasizes part of speech ( POS ) and n - gram ( phrase extraction ) . TerMine is the terminological management system with the C - Value term extraction and AcroMine acronym recognition integrated . Topicalizer is a service which automatically analyses a document specified by a URL or a plain text regarding its word , phrase and text structure . It provides a variety of useful information on a given text including the following : Word , sentence and paragraph count , collocations , syllable structure , lexical density , keywords , readability and a short abstract on what the given text is about . TrMExtractor does glossary extraction on pure text files for either English or Hungarian . Wikify ! is a system to automatically \\\" wikify \\\" a text by adding Wikipedia - like tags throughout the document . The system extracts keywords and then disambiguates and matches them to their corresponding Wikipedia definition . Yahoo ! Placemaker is a freely available geoparsing Web service . It helps developers make their applications location - aware by identifying places in unstructured and atomic content - feeds , web pages , news , status updates - and returning geographic metadata for geographic indexing and markup . Yahoo ! Term Extraction Service is an API to Yahoo 's term extraction service , as well as many other APIs and services in a variety of languages and for a variety of tasks ; good general resource . The service has been reported to be shut down numerous times , but apparently is kept alive due to popular demand . Initial Ontology Development . COE COE ( CmapTools Ontology Editor ) is a specialized version of the CmapTools from IMHC . COE - and its CmapTools parent - is based on the idea of concept maps . A concept map is a graph diagram that shows the relationships among concepts . Concepts are connected with labeled arrows , with the relations manifesting in a downward - branching hierarchical structure . COE is an integrated suite of software tools for constructing , sharing and viewing OWL encoded ontologies based on these constructs . Conzilla2 is a second generation concept browser and knowledge management tool with many purposes . It can be used as a visual designer and manager of RDF classes and ontologies , since its native storage is in RDF . It also has an online collaboration server . Does not appear to be code available anywhere . DogmaModeler is a free and open source , ontology modeling tool based on ORM . The philosophy of DogmaModeler is to enable non - IT experts to model ontologies with a little or no involvement of an ontology engineer ; project is quite old , but the software is still available and it may provide some insight into naive ontology development . Erca is a framework that eases the use of Formal and Relational Concept Analysis , a neat clustering technique . Erca is provided as an Eclipse plug - in . GraphMind is a mindmap editor for Drupal . It has the basic mindmap features and some Drupal specific enhancements . There is a quick screencast about how GraphMind looks like and what is does . The Flex source is also available from Github . GrOWL is the software framework to provide graphical , intuitive browsing and editing of knowledge maps . GrOWL is open source and is used in several projects worldwide . None of the online demos apparently work , but the screenshots look interesting and the code is still available . irON using spreadsheets , via its notation and specification . Spreadsheets can be used for initial authoring , esp if the irON guidelines are followed . See further this case study of Sweet Tools in a spreadsheet using irON ( commON ) . ITM T3 stands for Terminology , Thesaurus , Taxonomy , Metadata dictionary . ITM T3 includes a range of functions for managing enterprise shareable multilingual domain - specific taxonomies , thesaurus , terminologies in a unified way . It uses XML , SKOS and RDF standards . Commercial ; from Mondeca . MindRaider is Semantic Web outliner . It aims to connect the tradition of outline editors with emerging technologies . MindRaider mission is to organize not only the content of your hard drive but also your cognitive base and social relationships in a way that enables quick navigation , concise representation and inferencing . Topincs is a Topic Map authoring software that allows groups to share their knowledge over the web . It makes use of a variety of modern technologies . The most important are Topic Maps , REST and Ajax . It consists of three components : the Wiki , the Editor , and the Server . The servier requires AMP ; the Editor and Wiki are based on browser plug - ins . Ontology Editing . First , see all of the Comprehensive Tools listing above . Anzo for Excel includes an ( RDFS and OWL - based ) ontology editor that can be used directly within Excel . In addition to that , Anzo for Excel includes the capability to automatically generate an ontology from existing spreadsheet data , which is very useful for quick bootstrapping of an ontology . Hozo is an ontology visualization and development tool that brings version control constructs to group ontology development ; limited to a prototype , with no online demo . Lexaurus Editor is for off - line creation and editing of vocabularies , taxonomies and thesauri . It supports import and export in Zthes and SKOS XML formats , and allows hierarchical / poly - hierarchical structures to be loaded for editing , or even multiple vocabularies to be loaded simultaneously , so that terms from one taxonomy can be re - used in another , using drag and drop . Not available in open source . Model Futures OWL Editor combines simple OWL tools , featuring UML ( XMI ) , ErWin , thesaurus and imports . The editor is tree - based and has a \\\" navigator \\\" tool for traversing property and class - instance relationships . It can import XMI ( the interchange format for UML ) and Thesaurus Descriptor ( BT - NT XML ) , and EXPRESS XML files . It can export to MS Word . OntoTrack is a browsing and editing ontology authoring tool for OWL Lite . It combines a sophisticated graphical layout with mouse enabled editing features optimized for efficient navigation and manipulation of large ontologies . OWLViz is an attractive visual editor for OWL and is available as a Prot\\u00e9g\\u00e9 plug - in . PoolParty is a triple store - based thesaurus management environment which uses SKOS and text extraction for tag recommendations . See further this manual , which describes more fully the system 's functionality . Also , there is a PoolParty Web service that enables a Zthes thesaurus in XML format to be uploaded and converted to SKOS ( via skos : Concepts ) . SKOSEd is a plugin for Protege 4 that allows you to create and edit thesauri ( or similar artefacts ) represented in the Simple Knowledge Organisation System ( SKOS ) . TemaTres is a Web application to manage controlled vocabularies , taxonomies and thesaurus . The vocabularies may be exported in Zthes , Skos , TopicMap , etc . . ThManager is a tool for creating and visualizing SKOS RDF vocabularies . ThManager facilitates the management of thesauri and other types of controlled vocabularies , such as taxonomies or classification schemes . Vitro is a general - purpose web - based ontology and instance editor with customizable public browsing . Vitro is a Java web application that runs in a Tomcat servlet container . With Vitro , you can : 1 ) create or load ontologies in OWL format ; 2 ) edit instances and relationships ; 3 ) build a public web site to display your data ; and 4 ) search your data with Lucene . Still in somewhat early phases , with no online demos and with minimal interfaces . Not Apparently in Active Use . Omnigator The Omnigator is a form - based manipulaton tool centered on Topic Maps , though it enables the loading and navigation of any conforming topic map in XTM , HyTM , LTM or RDF formats . There is a free evaluation version . OntoGen is a semi - automatic and data - driven ontology editor focusing on editing of topic ontologies ( a set of topics connected with different types of relations ) . The system combines text - mining techniques with an efficient user interface . It requires . Net . OWL - S - editor is an editor for the development of services in OWL - S , with graphical , WSDL and import / export support . ReTAX+ is an aide to help a taxonomist create a consistent taxonomy and in particular provides suggestions as to where a new entity could be placed in the taxonomy whilst retaining the integrity of the revised taxonomy ( c.f . , problems in ontology modelling ) . SWOOP is a lightweight ontology editor . ( Swoop is no longer under active development at mindswap . WebOnto supports the browsing , creation and editing of ontologies through coarse grained and fine grained visualizations and direct manipulation . Ontology Mapping . COMA++ is a schema and ontology matching tool with a comprehensive infrastructure . Its graphical interface supports a variety of interaction . ConcepTool is a system to model , analyse , verify , validate , share , combine , and reuse domain knowledge bases and ontologies , reasoning about their implication . MatchIT automates and facilitates schema matching and semantic mapping between different Web vocabularies . MatchIT runs as a stand - alone or plug - in Eclipse application and can be integrated with popular third party applications . MatchIT 's uses Adaptive Lexicon \\u2122 as an ontology - driven dictionary and thesaurus of English language terminology to quantify and ank the semantic similarity of concepts . It apparently is not available in open source . myOntology is used to produce the theoretical foundations , and deployable technology for the Wiki - based , collaborative and community - driven development and maintenance of ontologies instance data and mappings . OLA / OLA2 ( OWL - Lite Alignment ) matches ontologies written in OWL . It relies on a similarity combining all the knowledge used in entity descriptions . It also deal with one - to - many relationships and circularity in entity descriptions through a fixpoint algorithm . Potluck is a Web - based user interface that lets casual users - those without programming skills and data modeling expertise - mash up data themselves . Potluck is novel in its use of drag and drop for merging fields , its integration and extension of the faceted browsing paradigm for focusing on subsets of data to align , and its application of simultaneous editing for cleaning up data syntactically . Potluck also lets the user construct rich visualizations of data in - place as the user aligns and cleans up the data . PRIOR+ is a generic and automatic ontology mapping tool , based on propagation theory , information retrieval technique and artificial intelligence model . The approach utilizes both linguistic and structural information of ontologies , and measures the profile similarity and structure similarity of different elements of ontologies in a vector space model ( VSM ) . Vine is a tool that allows users to perform fast mappings of terms across ontologies . Not Apparently in Active Use . ASMOV ( Automated Semantic Mapping of Ontologies with Validation ) is an automatic ontology matching tool which has been designed in order to facilitate the integration of heterogeneous systems , using their data source ontologies . Chimaera is a software system that supports users in creating and maintaining distributed ontologies on the web . Two major functions it supports are merging multiple ontologies together and diagnosing individual or multiple ontologies . CMS ( CROSI Mapping System ) is a structure matching system that capitalizes on the rich semantics of the OWL constructs found in source ontologies and on its modular architecture that allows the system to consult external linguistic resources . ConRef is a service discovery system which uses ontology mapping techniques to support different user vocabularies . DRAGO reasons across multiple distributed ontologies interrelated by pairwise semantic mappings , with a vision of peer - to - peer mapping of many distributed ontologies on the Web . It is implemented as an extension to an open source Pellet OWL Reasoner . Falcon - AO ( Finding , aligning and learning ontologies ) is an automatic ontology matching tool that includes the three elementary matchers of String , V - Doc and GMO . In addition , it integrates a partitioner PBM to cope with large - scale ontologies . FOAM is the Framework for ontology alignment and mapping . It is based on heuristics ( similarity ) of the individual entities ( concepts , relations , and instances ) . hMAFRA ( Harmonize Mapping Framework ) is a set of tools supporting semantic mapping definition and data reconciliation between ontologies . The targeted formats are XSD , RDFS and KAON . IF - Map is an Information Flow based ontology mapping method . It is based on the theoretical grounds of logic of distributed systems and provides an automated streamlined process for generating mappings between ontologies of the same domain . LILY is a system matching heterogeneous ontologies . LILY extracts a semantic subgraph for each entity , then it uses both linguistic and structural information in semantic subgraphs to generate initial alignments . The system is presently in a demo version only . MAFRA Toolkit - the Ontology MApping FRAmework Toolkit allows users to create semantic relations between two ( source and target ) ontologies , and apply such relations in translating source ontology instances into target ontology instances . OntoEngine is a step toward allowing agents to communicate even though they use different formal languages ( i.e. , different ontologies ) . It translates data from a \\\" source \\\" ontology to a \\\" target \\\" . OWLS - MX is a hybrid semantic Web service matchmaker . OWLS - MX 1.0 utilizes both description logic reasoning , and token based IR similarity measures . It applies different filters to retrieve OWL - S services that are most relevant to a given query . RiMOM ( Risk Minimization based Ontology Mapping ) integrates different alignment strategies : edit - distance based strategy , vector - similarity based strategy , path - similarity based strategy , background - knowledge based strategy , and three similarity - propagation based strategies . semMF is a flexible framework for calculating semantic similarity between objects that are represented as arbitrary RDF graphs . The framework allows taxonomic and non - taxonomic concept matching techniques to be applied to selected object properties . Snoggle is a graphical , SWRL - based ontology mapper . Snoggle attempts to solve the ontology mapping problem by providing a graphical user interface ( similar to which of the Microsoft Visio ) to guide the process of ontology vocabulary alignment . In Snoggle , user - defined mappings can be serialized into rules , which is expressed using SWRL . Terminator is a tool for creating term to ontology resource mappings ( documentation in Finnish ) . Ontology Visualization / Analysis . Cytoscape is a bioinformatics software platform for visualizing molecular interaction networks and integrating these interactions with gene expression profiles and other state data ; I have also written specifically about Cytoscape 's use in UMBEL . RDFScape is a project that brings Semantic Web \\\" features \\\" to the popular Systems Biology software Cytoscape . NetworkAnalyzer performs analysis of biological networks and calculates network topology parameters including the diameter of a network , the average number of neighbors , and the number of connected pairs of nodes . It also computes the distributions of more complex network parameters such as node degrees , average clustering coefficients , topological coefficients , and shortest path lengths . It displays the results in diagrams , which can be saved as images or text files ; used by SD . Graphl is a tool for collaborative editing and visualisation of graphs , representing relationships between resources or concepts of the real world . Graphl may be thought of as a visual wiki , a place where everybody can contribute to a shared repository of knowledge . igraph is a free software package for creating and manipulating undirected and directed graphs . Stanford Network Analysis Package ( SNAP ) is a general purpose network analysis and graph mining library . It is written in C++ and easily scales to massive networks with hundreds of millions of nodes . Social Networks Visualizer ( SocNetV ) is a flexible and user - friendly tool for the analysis and visualization of Social Networks . It lets you construct networks ( mathematical graphs ) with a few clicks on a virtual canvas or load networks of various formats ( GraphViz , GraphML , Adjacency , Pajek , UCINET , etc ) and modify them to suit your needs . SocNetV also offers a built - in web crawler , allowing you to automatically create networks from all links found in a given initial URL . VizierFX is a Flex library for drawing network graphs . The graphs are laid out using GraphViz on the server side , then passed to VizierFX to perform the rendering . The library also provides the ability to run ActionScript code in response to events on the graph , such as mousing over a node or clicking on it . Miscellaneous Ontology Tools . The Apolda processing resource ( PR ) annotates a document like a gazetteer , but takes the terms from an ( OWL ) ontology rather than from a list . DL - Learner is a tool for learning complex classes from examples and background knowledge . It extends Inductive Logic Programming to Description Logics and the Semantic Web . DL - Learner now has a flexible component based design , which allows to extend it easily with new learning algorithms , learning problems , reasoners , and supported background knowledge sources . A new type of supported knowledge sources are SPARQL endpoints , where DL - Learner can extract knowledge fragments , which enables learning classes even on large knowledge sources like DBpedia , and includes an OWL API reasoner interface and Web service interface . LexiLink is a tool for building , curating and managing multiple lexicons and ontologies in one enterprise - wide Web - based application . The core of the technology is based on RDF and OWL . mopy is the Music Ontology Python library , designed to provide easy to use python bindings for ontology terms for the creation and manipulation of music ontology data . mopy can handle information from several ontologies , including the Music Ontology , full FOAF vocab , and the timeline and chord ontologies . OBDA ( Ontology Based Data Access ) is a plugin for Prot\\u00e9g\\u00e9 aimed to be a full - fledged OBDA ontology and component editor . It provides data source and mapping editors , as well as querying facilities that , in sum , allow you to design and test every aspect of an OBDA system . It supports relational data sources ( RDBMS ) and GLAV - like mappings . In its current beta form , it requires Protege 3.3.1 , a reasoner implementing the OBDA extensions to DIG 1.1 ( e.g. , the DIG server for QuOnto ) and Jena 2.5.5 . OntoComP is a Prot\\u00e9g\\u00e9 4 plugin for completing OWL ontologies . It enables the user to check whether an OWL ontology contains \\\" all relevant information \\\" about the application domain , and extend the ontology appropriately if this is not the case . Ontology Metrics is a web - based tool that displays statistics about a given ontology , including the expressivity of the language it is written in . OntoSpec is a SWI - Prolog module , aiming at automatically generating XHTML specification from RDF - Schema or OWL ontologies . OWL API is a Java interface and implementation for the W3C Web Ontology Language ( OWL ) , used to represent Semantic Web ontologies . The API is focused towards OWL Lite and OWL DL and offers an interface to inference engines and validation functionality . OWL Module Extractor is a Web service that extracts a module for a given set of terms from an ontology . It is based on an implementation of locality - based modules that is part of the OWL API . OWL Syntax Converter is an online tool for converting ontologies between different formats , including several OWL syntaxes , RDF / XML , KRSS . OWL Verbalizer is an on - line tool that verbalizes OWL ontologies in ( controlled ) English . OwlSight is an OWL ontology browser that runs in any modern web browser ; it 's developed with Google Web Toolkit and uses Gwt - Ext , as well as OWL - API . OwlSight is the client component and uses Pellet as its OWL reasoner . Pellint is an open source lint tool for Pellet which flags and ( optionally ) repairs modeling constructs that are known to cause performance problems . Pellint recognizes several patterns at both the axiom and ontology level . PROMPT is a tab plug - in for Prot\\u00e9g\\u00e9 is for managing multiple ontologies by comparing versions of the same ontology , moving frames between included and including project , merging two ontologies into one , or extracting a part of an ontology . SETH is a software effort to deeply integrate Python with Web Ontology Language ( OWL - DL dialect ) . The idea is to import ontologies directly into the programming context so that its classes are usable alongside standard Python classes . SKOS2GenTax is an online tool that converts hierarchical classifications available in the W3C SKOS ( Simple Knowledge Organization Systems ) format into RDF - S or OWL ontologies . SpecGen ( v5 ) is an ontology specification generator tool . It 's written in Python using Redland RDF library and licensed under the MIT license . Text2Onto is a framework for ontology learning from textual resources that extends and re - engineers an earlier framework developed by the same group ( TextToOnto ) . Text2Onto offers three main features : it represents the learned knowledge at a metalevel by instantiating the modelling primitives of a Probabilistic Ontology Model ( POM ) , thus remaining independent from a specific target language while allowing the translation of the instantiated primitives . Thea is a Prolog library for generating and manipulating OWL ( Web Ontology Language ) content . Thea OWL parser uses SWI - Prolog 's Semantic Web library for parsing RDF / XML serialisations of OWL documents into RDF triples and then it builds a representation of the OWL ontology . TONES Ontology Repository is primarily designed to be a central location for ontologies that might be of use to tools developers for testing purposes ; it is part of the TONES project . Visual Ontology Manager ( VOM ) is a family of tools enables UML - based visual construction of component - based ontologies for use in collaborative applications and interoperability solutions . Web Ontology Manager is a lightweight , Web - based tool using J2EE for managing ontologies expressed in Web Ontology Language ( OWL ) . It enables developers to browse or search the ontologies registered with the system by class or property names . In addition , they can submit a new ontology file . RDF evoc ( external vocabulary importer ) is an RDF external vocabulary importer module ( evoc ) for Drupal caches any external RDF vocabulary and provides properties to be mapped to CCK fields , node title and body . This module requires the RDF and the SPARQL modules . Not Apparently in Active Use . Almo is an ontology - based workflow engine in Java supporting the ARTEMIS project ; part of the OntoWare initiative . ClassAKT is a text classification web service for classifying documents according to the ACM Computing Classification System . Elmo provides a simple API to access ontology oriented data inside a Sesame RDF repository . The domain model is simplified into independent concerns that are composed together for multi - dimensional , inter - operating , or integrated applications . ExtrAKT is a tool for extracting ontologies from Prolog knowledge bases . F - Life is a tool for analysing and maintaining life - cycle patterns in ontology development . Foxtrot is a recommender system which represents user profiles in ontological terms , allowing inference , bootstrapping and profile visualization . HyperDAML creates an HTML representation of OWL content to enable hyperlinking to specific objects , properties , etc . . LinKFactory is an ontology management tool , it provides an effective and user - friendly way to create , maintain and extend extensive multilingual terminology systems and ontologies ( English , Spanish , French , etc . ) . It is designed to build , manage and maintain large , complex , language independent ontologies . LSW - the Lisp semantic Web toolkit enables OWL ontologies to be visualized . It was written by Alan Ruttenberg . Ontodella is a Prolog HTTP server for category projection and semantic linking . OntoWeaver is an ontology - based approach to Web sites , which provides high level support for web site design and development . OWLLib is a PHP library for accessing OWL files . OWL is w3.org standard for storing semantic information . pOWL is a Semantic Web development platform for ontologies in PHP . pOWL consists of a number of components , including RAP . ROWL is the Rule Extension of OWL ; it is from the Mobile Commerce Lab in the School of Computer Science at Carnegie Mellon University . Semantic Net Generator is a utlity for generating Topic Maps automatically from different data sources by using rules definitions specified with Jelly XML syntax . This Java library provides Jelly tags to access and modify data sources ( also RDF ) to create a semantic network . SMORE is OWL markup for HTML pages . SMORE integrates the SWOOP ontology browser , providing a clear and consistent way to find and view Classes and Properties , complete with search functionality . SOBOLEO is a system for Web - based collaboration to create SKOS taxonomies and ontologies and to annotate various Web resources using them . SOFA is a Java API for modeling ontologies and Knowledge Bases in ontology and Semantic Web applications . It provides a simple , abstract and language neutral ontology object model , inferencing mechanism and representation of the model with OWL , DAML+OIL and RDFS languages ; from java.dev . WebScripter is a tool that enables ordinary users to easily and quickly assemble reports extracting and fusing information from multiple , heterogeneous DAMLized Web sources . Thanks to Enrico Motta for reminding me about the Neon Toolkit under the comprehensive tools section . This one is definitely worth a look ; it was a major brain wheeze for me to have missed it ! Sorry Neon project , and do n't miss the listing of useful plug - ins . Also , note there are 5 - 6 new vocabulary prompting tools now added since the first posting . You might include the Sigma knowledge engineering environment as well . It 's open source , includes ontology mapping , theorem proving , language generation in multiple languages , browsing , OWL read / write , and analysis . Most importantly , it includes the Suggested Upper Merged Ontology ( SUMO ) , a comprehensive formal ontology . It 's under active development and use . \"}",
        "_version_":1692670983307001856,
        "score":19.868727}]
  }}
