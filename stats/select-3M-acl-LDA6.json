{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"system coreference tasks method version semantic applied performance relations palmer xue relation recognition verb muc resolution conll chinese show recent",
      "fl":"*,score"}},
  "response":{"numFound":1303409,"start":0,"maxScore":27.766777,"numFoundExact":true,"docs":[
      {
        "id":"196d7ee8-6c21-4795-accc-22c761580d50",
        "_src_":"{\"url\": \"https://interchangeableparts.wordpress.com/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701154221.36/warc/CC-MAIN-20160205193914-00035-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Software documentation is like sex : when it is good , it is very , very good ; and when it is bad , it is better than nothing . ( Anonymous . ) There are two ways of constructing a software design : one way is to make it so simple that there are obviously no deficiencies ; the other way is to make it so complicated that there are no obvious deficiencies . ( C.A.R. Hoare ) . A computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology . Thus , programs must be written for people to read , and only incidentally for machines to execute . ( The Structure and Interpretation of Computer Programs , H. Abelson , G. Sussman and J. Sussman , 1985 . ) If you try to make something beautiful , it is often ugly . If you try to make something useful , it is often beautiful . ( Oscar Wilde ) 1 . GATE 2 is an infrastructure for developing and deploying software components that process human language . It is nearly 15 years old and is in active use for all types of computational task involving human language . GATE excels at text analysis of all shapes and sizes . From large corporations to small startups , from \\u20ac multi - million research consortia to undergraduate projects , our user community is the largest and most diverse of any system of this type , and is spread across all but one of the continents 3 . GATE is open source free software ; users can obtain free support from the user and developer community via GATE.ac.uk or on a commercial basis from our industrial partners . We are the biggest open source language processing project with a development team more than double the size of the largest comparable projects ( many of which are integrated with GATE 4 ) . More than \\u20ac 5 million has been invested in GATE development 5 ; our objective is to make sure that this continues to be money well spent for all GATE 's users . GATE has grown over the years to include a desktop client for developers , a workflow - based web application , a Java library , an architecture and a process . GATE is : . One of our original motivations was to remove the necessity for solving common engineering problems before doing useful research , or re - engineering before deploying research results into applications . Core functions of GATE take care of the lion 's share of the engineering : . modelling and persistence of specialised data structures . measurement , evaluation , benchmarking ( never believe a computing researcher who has n't measured their results in a repeatable and open setting ! ) visualisation and editing of annotations , ontologies , parse trees , etc . . a finite state transduction language for rapid prototyping and efficient implementation of shallow analysis methods ( JAPE ) . extraction of training instances for machine learning . pluggable machine learning implementations ( Weka , SVM Light , ... ) . On top of the core functions GATE includes components for diverse language processing tasks , e.g. parsers , morphology , tagging , Information Retrieval tools , Information Extraction components for various languages , and many others . GATE Developer and Embedded are supplied with an Information Extraction system ( ANNIE ) which has been adapted and evaluated very widely ( numerous industrial systems , research systems evaluated in MUC , TREC , ACE , DUC , Pascal , NTCIR , etc . ) . ANNIE is often used to create RDF or OWL ( metadata ) for unstructured content ( semantic annotation ) . GATE version 1 was written in the mid-1990s ; at the turn of the new millenium we completely rewrote the system in Java ; version 5 was released in June 2009 . We invite you to give it a try , to get involved with the GATE community , and to contribute to human language science , engineering and development . This book describes how to use GATE to develop language processing components , test their performance and deploy them as parts of other applications . In the rest of this chapter : . ( Often the process of getting a new component is as simple as typing the URL into GATE Developer ; the system will do the rest . ) The material presented in this book ranges from the conceptual ( e.g. ' what is software architecture ? ' ) to practical instructions for programmers ( e.g. how to deal with GATE exceptions ) and linguists ( e.g. how to write a pattern grammar ) . Furthermore , GATE 's highly extensible nature means that new functionality is constantly being added in the form of new plugins . Important functionality is as likely to be located in a plugin as it is to be integrated into the GATE core . This presents something of an organisational challenge . Our ( no doubt imperfect ) solution is to divide this book into three parts . Part I covers installation , using the GATE Developer GUI and using ANNIE , as well as providing some background and theory . We recommend the new user to begin with Part I . Part II covers the more advanced of the core GATE functionality ; the GATE Embedded API and JAPE pattern language among other things . Part III provides a reference for the numerous plugins that have been created for GATE . Although ANNIE provides a good starting point , the user will soon wish to explore other resources , and so will need to consult this part of the text . We recommend that Part III be used as a reference , to be dipped into as necessary . In Part III , plugins are grouped into broad areas of functionality . Software Architecture ' is used rather loosely here to mean computer infrastructure for software development , including development environments and frameworks , as well as the more usual use of the term to denote a macro - level organisational structure for software systems [ Shaw & Garlan 96 ] . Language Engineering ( LE ) may be defined as : . ... the discipline or act of engineering software systems that perform tasks involving processing human language . Both the construction process and its outputs are measurable and predictable . The literature of the field relates to both application of relevant scientific results and a body of practice . [ Cunningham 99a ] . The relevant scientific results in this case are the outputs of Computational Linguistics , Natural Language Processing and Artificial Intelligence in general . Unlike these other disciplines , LE , as an engineering discipline , entails predictability , both of the process of constructing LE - based software and of the performance of that software after its completion and deployment in applications . Some working definitions : . Computational Linguistics ( CL ) : science of language that uses computation as an investigative tool . Natural Language Processing ( NLP ) : science of computation whose subject matter is data structures and algorithms for computer processing of human language . Language Engineering ( LE ) : building NLP systems whose cost and outputs are measurable and predictable . Software Architecture : macro - level organisational principles for families of systems . In this context is also used as infrastructure . Software Architecture for Language Engineering ( SALE ) : software infrastructure , architecture and development tools for applied CL , NLP and LE . ( Of course the practice of these fields is broader and more complex than these definitions . ) In the scientific endeavours of NLP and CL , GATE 's role is to support experimentation . In this context GATE 's significant features include support for automated measurement ( see Chapter 10 ) , providing a ' level playing field ' where results can easily be repeated across different sites and environments , and reducing research overheads in various ways . GATE as an architecture suggests that the elements of software systems that process natural language can usefully be broken down into various types of component , known as resources 8 . Components are reusable software chunks with well - defined interfaces , and are a popular architectural form , used in Sun 's Java Beans and Microsoft 's . Net , for example . GATE components are specialised types of Java Bean , and come in three flavours : . LanguageResources ( LRs ) represent entities such as lexicons , corpora or ontologies ; . ProcessingResources ( PRs ) represent entities that are primarily algorithmic , such as parsers , generators or ngram modellers ; . VisualResources ( VRs ) represent visualisation and editing components that participate in GUIs . These definitions can be blurred in practice as necessary . Collectively , the set of resources integrated with GATE is known as CREOLE : a Collection of REusable Objects for Language Engineering . All the resources are packaged as Java Archive ( or ' JAR ' ) files , plus some XML configuration data . The JAR and XML files are made available to GATE by putting them on a web server , or simply placing them in the local file space . Section 1.3.2 introduces GATE 's built - in resource set . When using GATE to develop language processing functionality for an application , the developer uses GATE Developer and GATE Embedded to construct resources of the three types . This may involve programming , or the development of Language Resources such as grammars that are used by existing Processing Resources , or a mixture of both . GATE Developer is used for visualisation of the data structures produced and consumed during processing , and for debugging , performance measurement and so on . For example , figure 1.1 is a screenshot of one of the visualisation tools . GATE Developer is analogous to systems like Mathematica for Mathematicians , or JBuilder for Java programmers : it provides a convenient graphical environment for research and development of language processing software . When an appropriate set of resources have been developed , they can then be embedded in the target client application using GATE Embedded . GATE Embedded is supplied as a series of JAR files . 9 To embed GATE - based language processing facilities in an application , these JAR files are all that is needed , along with JAR files and XML configuration files for the various resources that make up the new facilities . GATE includes resources for common LE data structures and algorithms , including documents , corpora and various annotation types , a set of language analysis components for Information Extraction and a range of data visualisation and editing components . GATE supports documents in a variety of formats including XML , RTF , email , HTML , SGML and plain text . In all cases the format is analysed and converted into a single unified model of annotation . The annotation format is a modified form the TIPSTER format [ Grishman 97 ] which has been made largely compatible with the Atlas format [ Bird & Liberman 99 ] , and uses the now standard mechanism of ' stand - off markup ' . GATE documents , corpora and annotations are stored in databases of various sorts , visualised via the development environment , and accessed at code level via the framework . See Chapter 5 for more details of corpora etc . . A family of Processing Resources for language analysis is included in the shape of ANNIE , A Nearly - New Information Extraction system . These components use finite state techniques to implement various tasks from tokenisation to semantic tagging or verb phrase chunking . All ANNIE components communicate exclusively via GATE 's document and annotation resources . See Chapter 6 for more details . Other CREOLE resources are described in Part III . JAPE , a Java Annotation Patterns Engine , provides regular - expression based pattern / action rules over annotations - see Chapter 8 . The ' annotation diff ' tool in the development environment implements performance metrics such as precision and recall for comparing annotations . Typically a language analysis component developer will mark up some documents by hand and then use these along with the diff tool to automatically measure the performance of the components . See Chapter 10 . GUK , the GATE Unicode Kit , fills in some of the gaps in the JDK 's 10 support for Unicode , e.g. by adding input methods for various languages from Urdu to Chinese . See Section 3.10.2 for more details . This section gives a very brief example of a typical use of GATE to develop and deploy language processing capabilities in an application , and to generate quantitative results for scientific publication . Let 's imagine that a developer called Fatima is building an email client 11 for Cyberdyne Systems ' large corporate Intranet . In this application she would like to have a language processing system that automatically spots the names of people in the corporation and transforms them into mailto hyperlinks . A little investigation shows that GATE 's existing components can be tailored to this purpose . Fatima starts up GATE Developer , and creates a new document containing some example emails . She then loads some processing resources that will do named - entity recognition ( a tokeniser , gazetteer and semantic tagger ) , and creates an application to run these components on the document in sequence . Having processed the emails , she can see the results in one of several viewers for annotations . The GATE components are a decent start , but they need to be altered to deal specially with people from Cyberdyne 's personnel database . Therefore Fatima creates new ' cyber- ' versions of the gazetteer and semantic tagger resources , using the ' bootstrap ' tool . This tool creates a directory structure on disk that has some Java stub code , a Makefile and an XML configuration file . After several hours struggling with badly written documentation , Fatima manages to compile the stubs and create a JAR file containing the new resources . She tells GATE Developer the URL of these files 12 , and the system then allows her to load them in the same way that she loaded the built - in resources earlier on . Fatima then creates a second copy of the email document , and uses the annotation editing facilities to mark up the results that she would like to see her system producing . She saves this and the version that she ran GATE on into her serial datastore . From now on she can follow this routine : . Run her application on the email test corpus . Check the performance of the system by running the ' annotation diff ' tool to compare her manual results with the system 's results . This gives her both percentage accuracy figures and a graphical display of the differences between the machine and human outputs . Make edits to the code , pattern grammars or gazetteer lists in her resources , and recompile where necessary . To make the alterations that she requires , Fatima re - implements the ANNIE gazetteer so that it regenerates itself from the local personnel data . She then alters the pattern grammar in the semantic tagger to prioritise recognition of names from that source . This latter job involves learning the JAPE language ( see Chapter 8 ) , but as this is based on regular expressions it is n't too difficult . Eventually the system is running nicely , and her accuracy is 93 % ( there are still some problem cases , e.g. when people use nicknames , but the performance is good enough for production use ) . Now Fatima stops using GATE Developer and works instead on embedding the new components in her email application using GATE Embeddded . She takes the accuracy measures that she has attained for her system and writes a paper for the Journal of Nasturtium Logarithm Encitement describing the approach used and the results obtained . Because she used GATE for development , she can cite the repeatability of her experiments and offer access to example binary versions of her software by putting them on an external web server . This section contains an incomplete list of publications describing systems that used GATE in competitive quantitative evaluation programmes . These programmes have had a significant impact on the language processing field and the widespread presence of GATE is some measure of the maturity of the system and of our understanding of its likely performance on diverse text processing tasks . describes the performance of an SVM - based learning system in the NTCIR-6 Patent Retrieval Task . The system achieved the best result on two of three measures used in the task evaluation , namely the R - Precision and F - measure . The system obtained close to the best result on the remaining measure ( A - Precision ) . describes a cross - source coreference resolution system based on semantic clustering . It uses GATE for information extraction and the SUMMA system to create summaries and semantic representations of documents . One system configuration ranked 4th in the Web People Search 2007 evaluation . describes a cross - lingual summarization system which uses SUMMA components and the Arabic plugin available in GATE to produce summaries in English from a mixture of English and Arabic documents . Open - Domain Question Answering : . The University of Sheffield has a long history of research into open - domain question answering . GATE has formed the basis of much of this research resulting in systems which have ranked highly during independent evaluations since 1999 . The first successful question answering system developed at the University of Sheffield was evaluated as part of TREC 8 and used the LaSIE information extraction system ( the forerunner of ANNIE ) which was distributed with GATE [ Humphreys et al . 99 ] . Further research was reported in [ Scott & Gaizauskas . 00 ] , [ Greenwood et al . 02 ] , [ Gaizauskas et al . 03 ] , [ Gaizauskas et al . 04 ] and [ Gaizauskas et al . 05 ] . In 2004 the system was ranked 9th out of 28 participating groups . describes techniques for answering definition questions . The system uses definition patterns manually implemented in GATE as well as learned JAPE patterns induced from a corpus . In 2004 , the system was ranked 4th in the TREC / QA evaluations . describes a multidocument summarization system implemented using summarization components compatible with GATE ( the SUMMA system ) . The system was ranked 2nd in the Document Understanding Evaluation programmes . describe participation in the TIDES surprise language program . ANNIE was adapted to Cebuano with four person days of effort , and achieved an F - measure of 77.5 % . Unfortunately , ours was the only system participating ! describe results obtained on systems designed for the ACE task ( Automatic Content Extraction ) . Although a comparison to other participating systems can not be revealed due to the stipulations of ACE , results show 82%-86 % precision and recall . To get HTML reports from profiled processing resources , there is a new menu item in the ' Tools ' menu called ' Profiling reports ' , see chapter 11 . To deal with quality assurance of annotations , one component has been updated and two new components have been added . The annotation diff tool has a new mode to copy annotations to a consensus set , see section 10.2.1 . An annotation stack view has been added in the document editor and it allows to copy annotations to a consensus set , see section 3.4.3 . A corpus view has been added for all corpus to get statistics like precision , recall and F - measure , see section 10.3 . An annotation stack view has been added in the document editor to make easier to see overlapping annotations , see section 3.4.3 . Added an isInitialised ( ) method to gate . Gate ( ) . The ontology API ( package gate.creole.ontology has been changed , the existing ontology implementation based on Sesame1 and OWLIM2 ( package gate.creole.ontology.owlim ) has been moved into the plugin Ontology_OWLIM2 . An upgraded implementation based on Sesame2 and OWLIM3 that also provides a number of new features has been added as plugin Ontology . See Section 14.12 for a detailed description of all changes . The new Imports : statement at the beginning of a JAPE grammar file can now be used to make additional Java import statements available to the Java RHS code , see 8.6.5 . The User Guide has been amalgamated with the Programmer 's Guide ; all material can now be found in the User Guide . The ' How - To ' chapter has been converted into separate chapters for installation , GATE Developer and GATE Embedded . Other material has been relocated to the appropriate specialist chapter . Plugin names have been rationalised . Mappings exist so that existing applications will continue to work , but the new names should be used in the future . Plugin name mappings are given in Appendix B . The Montreal Transducer has been made obsolete . The UIMA integration layer ( Chapter 18 ) has been upgraded to work with Apache UIMA 2.2.2 . The JAPE debugger has been removed . Debugging of JAPE has been made easier as stack traces now refer to the JAPE source file and line numbers instead of the generated Java source code . Oracle and PostGreSQL are no longer supported . The MIAKT Natural Language Generation plugin has been removed . The Minorthird plugin has been removed . Minorthird has changed significantly since this plugin was written . We will consider writing an up - to - date Minorthird plugin in the future . A new gazetteer , Large KB Gazetteer ( in the plugin ' Gazetteer_LKB ' ) has been added , see Section 13.9 for details . gate.creole.tokeniser.chinesetokeniser.ChineseTokeniser and related resources under the plugins / ANNIE / tokeniser / chinesetokeniser folder have been removed . Please refer to the Lang_Chinese plugin for resources related to the Chinese language in GATE . A number of improvements to the benchmarking support in GATE . JAPE transducers now log the time spent in individual phases of a multi - phase grammar and by individual rules within each phase . Other PRs that use JAPE grammars internally ( the pronominal coreferencer , English tokeniser ) log the time taken by their internal transducers . A reporting tool , called ' Profiling reports ' under the ' Tools ' menu makes summary information easily available . For more details , see chapter 11 . We have added a new PR called ' Segment Processing PR ' . As the name suggests this PR allows processing individual segments of a document independently of one other . For more details , please look at the section 16.2.8 . The gate . Controller implementations provided with the main GATE distribution now also implement the gate . ProcessingResource interface . This means that an application can now contain another application as one of its components . LingPipe is a suite of Java libraries for the linguistic analysis of human language . We have provided a plugin called ' LingPipe ' with wrappers for some of the resources available in the LingPipe library . For more details , see the section 19.15 . OpenNLP provides tools for sentence detection , tokenization , pos - tagging , chunking and parsing , named - entity detection , and coreference . The tools use Maximum Entropy modelling . We have provided a plugin called ' OpenNLP ' with wrappers for some of the resources available in the OpenNLP Tools library . For more details , see section 19.16 . A new plugin has been added to provide an easy route to integrate taggers with GATE . The Tagger_Framework plugin provides examples of incorporating a number of external taggers which should serve as a starting point for using other taggers . See Section 17.4 for more details . reviews the current state of the art in email processing and communication research , focusing on the roles played by email in information management , and commercial and research efforts to integrate a semantic - based approach to email . investigates two techniques for making SVMs more suitable for language learning tasks . Firstly , an SVM with uneven margins ( SVMUM ) is proposed to deal with the problem of imbalanced training data . Secondly , SVM active learning is employed in order to alleviate the difficulty in obtaining labelled training data . The algorithms are presented and evaluated on several Information Extraction ( IE ) tasks . presents a semantic - based prototype that is made for an open - source software engineering project with the goal of exploring methods for assisting open - source developers and software users to learn and maintain the system without major effort . discusses methods of measuring the performance of ontology - based information extraction systems , focusing particularly on the Balanced Distance Metric ( BDM ) , a new metric we have proposed which aims to take into account the more flexible nature of ontologically - based applications . describes the development of a system for content mining using domain ontologies , which enables the extraction of relevant information to be fed into models for analysis of financial and operational risk and other business intelligence applications such as company intelligence , by means of the XBRL standard . describes experiments for the cross - document coreference task in SemEval 2007 . Our cross - document coreference system uses an in - house agglomerative clustering implementation to group documents referring to the same entity . describes the application of ontology - based extraction and merging in the context of a practical e - business application for the EU MUSING Project where the goal is to gather international company intelligence and country / region information . studies Japanese - English cross - language patent retrieval using Kernel Canonical Correlation Analysis ( KCCA ) , a method of correlating linear relationships between two variables in kernel defined feature spaces . ( Proceedings of the 5th International Semantic Web Conference ( ISWC2006 ) ) In this paper the problem of disambiguating author instances in ontology is addressed . We describe a web - based approach that uses various features such as publication titles , abstract , initials and co - authorship information . describes work in progress concerning the application of Controlled Language Information Extraction - CLIE to a Personal Semantic Wiki - Semper- Wiki , the goal being to permit users who have no specialist knowledge in ontology tools or languages to semi - automatically annotate their respective personal Wiki pages . discusses existing evaluation metrics , and proposes a new method for evaluating the ontology population task , which is general enough to be used in a variety of situation , yet more precise than many current metrics . describes an approach that allows users to create and edit ontologies simply by using a restricted version of the English language . The controlled language described is based on an open vocabulary and a restricted set of grammatical constructs . ( Proceedings of Fifth International Conference on Recent Advances in Natural Language Processing ( RANLP2005 ) ) It is a full - featured annotation indexing and search engine , developed as a part of the GATE . It is powered with Apache Lucene technology and indexes a variety of documents supported by the GATE . ( Proceedings of Ninth Conference on Computational Natural Language Learning ( CoNLL-2005 ) ) uses the uneven margins versions of two popular learning algorithms SVM and Perceptron for IE to deal with the imbalanced classification problems derived from IE . ( Proceedings of the 2nd European Workshop on the Integration of Knowledge , Semantic and Digital Media Technologies ( EWIMT 2005))Digital Media Preservation and Access through Semantically Enhanced Web - Annotation . describes a sentence extraction system that produces two sorts of multi - document summaries ; a general - purpose summary of a cluster of related documents and an entity - based summary of documents related to a particular person . 8 The terms ' resource ' and ' component ' are synonymous in this context . ' Resource ' is used instead of just ' component ' because it is a common term in the literature of the field : cf . the Language Resources and Evaluation conference series [ LREC-1 98 , LREC-2 00 ] . 9 The main JAR file ( gate.jar ) supplies the framework . Built - in resources and various 3rd - party libraries are supplied as separate JARs ; for example ( guk.jar , the GATE Unicode Kit . ) contains Unicode support ( e.g. additional input methods for languages not currently supported by the JDK ) . They are separate because the latter has to be a Java extension with a privileged security profile . 10 JDK : Java Development Kit , Sun Microsystem 's Java implementation . Unicode support is being actively improved by Sun , but at the time of writing many languages are still unsupported . In fact , Unicode itself does n't support all languages , e.g. Sylheti ; hopefully this will change in time . 11 Perhaps because Outlook Express trashed her mail folder again , or because she got tired of Microsoft - specific viruses and had n't heard of Netscape or Emacs . 12 While developing , she uses a file:/ ... URL ; for deployment she can put them on a web server . 13 Languages other than Java require an additional interface layer , such as JNI , the Java Native Interface , which is in C. \"}",
        "_version_":1692670548559003648,
        "score":27.766777},
      {
        "id":"4c2b5ac9-77aa-4806-8ec5-ec53295d7015",
        "_src_":"{\"url\": \"http://www.bbc.co.uk/music/reviews/3rgw\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701153736.68/warc/CC-MAIN-20160205193913-00054-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"List of accepted papers . Long papers . A Generative Joint , Additive , Sequential Model of Topics and Speech Acts in Patient - Doctor Communication Byron Wallace , Thomas Trikalinos , M Barton Laws , Ira Wilson and Eugene Charniak . A Multimodal LDA Model integrating Textual , Cognitive and Visual Modalities Stephen Roller and Sabine Schulte i m Walde . A Unified Model for Topics , Events and Users on Twitter Qiming Diao and Jing Jiang . Of words , eyes and brains : Correlating image - based distributional semantic models with neural representations of concepts Andrew J. Anderson , Elia Bruni , Ulisse Bordignon , Massimo Poesio and Marco Baroni . A Cognitive Model of Early Lexical Acquisition With Phonetic Variability Micha Elsner , Sharon Goldwater , Naomi Feldman and Frank Wood . A Constrained Latent Variable Model for Coreference Resolution Kai - Wei Chang , Rajhans Samdani and Dan Roth . A Convex Alternative to IBM Model 2 Andrei Simion , Michael Collins and Cliff Stein . A Dataset for Research on Short - Text Conversation Hao Wang , Zhengdong Lu , Hang Li and Enhong Chen . A Hierarchical Entity - based Approach to Structuralize User Generated Content in Social Media : A Case of Yahoo ! Answers Baichuan Li , Jing Liu , Chin - Yew Lin , Irwin King and Michael R. Lyu . A Laplacian Structured Sparsity Model for Computational Branding Analytics William Yang Wang , Eduard Lin and John Kominek . A Log - Linear Model for Unsupervised Text Normalization Yi Yang and Jacob Eisenstein . A Semantically Enhanced Approach to Determine Textual Similarity Eduardo Blanco and Dan Moldovan . A Study on Bootstrapping Bilingual Vector Spaces from Non - Parallel Data ( and Nothing Else ) Ivan Vuli\\u0107 and Marie - Francine Moens . A Systematic Exploration of Diversity in Machine Translation Kevin Gimpel , Dhruv Batra , Chris Dyer and Gregory Shakhnarovich . A discourse - driven content model for summarising scientific articles evaluated in a complex question answering task Maria Liakata , Simon Dobnik , Shyamasree Saha , Colin Batchelor and Dietrich Rebholz - Schuhmann . A multi - Teraflop Constituency Parser using GPUs John Canny , David Hall and Dan Klein . A temporal model of text periodicities using Gaussian Processes Daniel Preo\\u0163iuc - Pietro and Trevor Cohn . Adaptor Grammars for Learning non - concatenative Morphology Jan Botha and Phil Blunsom . An Efficient Language Model Using Double - Array Structures Makoto Yasuhara , Toru Tanaka , Jun - ya Norimatsu and Mikio Yamamoto . An Empirical Study Of Semi - Supervised Chinese Word Segmentation Using Co - Training Fan Yang and Paul Vozila . Anchor Graph : Global Reordering Contexts for Statistical Machine Translation Hendra Setiawan , Bowen Zhou and Bing Xiang . Assembling the Kazakh Language Corpus Olzhas Makhambetov , Aibek Makazhanov , Zhandos Yessenbayev , Bakhyt Matkarimov , Islam Sabyrgaliyev and Anuar Sharafudinov . Authorship Attribution of Micro - Messages Roy Schwartz , Oren Tsur , Ari Rappoport and Moshe Koppel . Automated Essay Scoring by Maximizing Human - machine Agreement Hongbo Chen and Ben He . Automatic Discovery of Pronunciation Dictionaries for ASR Chia - ying Lee , Yu Zhang and James Glass . Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora Ramy Eskander , Nizar Habash and Owen Rambow . Automatic Feature Engineering for Answer Selection and Extraction Aliaksei Severyn and Alessandro Moschitti . Automatic Knowledge Acquisition for Case Alternation between the Passive and Active Voices in Japanese Ryohei Sasano , Daisuke Kawahara , Sadao Kurohashi and Manabu Okumura . Automatically Classifying Edit Categories in Wikipedia Revisions Johannes Daxenberger and Iryna Gurevych . Automatically Detecting and Attributing Indirect Quotations Silvia Pareti , Tim O'Keefe , Ioannis Konstas , James R. Curran and Irena Koprinska . Automatically Determining a Proper Length for Multi - document Summarization : A Bayesian Nonparametric Approach Tengfei Ma and Hiroshi Nakagawa . Boosting Cross - Language Retrieval by Learning Bilingual Phrase Associations from Relevance Rankings Artem Sokokov , Laura Jehl , Felix Hieber and Stefan Riezler . Breaking Out of Local Optima with Count Transforms and Model Recombination : A Study in Grammar Induction Valentin Spitkovsky , Hiyan Alshawi and Daniel Jurafsky . Building Event Threads out of Multiple News Articles Xavier Tannier and V\\u00e9ronique Moriceau . Building Specialized Bilingual Lexicons Using Large Scale Background Knowledge Dhouha Bouamor , Adrian Popescu , Nasredine Semmar and Pierre Zweigenbaum . Centering Similarity Measures to Reduce Hubs Ikumi Suzuki , Kazuo Hara , Masashi Shimbo , Marco Saerens and Kenji Fukumizu . Collective Opinion Target Extraction in Chinese Microblogs Xinjie Zhou and Xiaojun Wan . Collective Personal Profile Summarization with Social Networks Zhongqing Wang , Shoushan LI and Guodong Zhou . Cross - Lingual Discriminative Learning of Sequence Models with Posterior Regularization Kuzman Ganchev and Dipanjan Das . Deep Learning for Chinese Word Segmentation and POS Tagging Xiaoqing Zheng , Hanyang Chen and Tianyu Xu . Dependency - Based Decipherment for Resource - Limited Machine Translation Qing Dou and Kevin Knight . Discourse Level Explanatory Relation Extraction from Product Reviews Using First - order Logic Qi ZHANG , Kang Han , Xuanjing Huang , Huan Chen and Yeyun Gong . Document Summarization via Guided Sentence Compression Chen Li , Fei Liu , Fuliang Weng and Yang Liu . Dynamic Feature Selection for Dependency Parsing He He , Hal Daum\\u00e9 III and Jason Eisner . Dynamic Programming for Optimal Best - First Shift - Reduce Parsing Kai Zhao , James Cross and Liang Huang . Easy Victories and Uphill Battles in Coreference Resolution Greg Durrett and Dan Klein . Effectiveness and Efficiency of Open Relation Extraction Filipe Mesquita , Jordan Schmidek and Denilson Barbosa . Efficient Collective Entity Linking with Stacking Zhengyan He . Efficient Higher - Order CRFs for Morphological Tagging Thomas Mueller , Hinrich Schuetze and Helmut Schmid . Efficient Left - to - Right Hierarchical Phrase - based Translation with Improved Reordering Maryam Siahbani , Baskaran Sankaran and Anoop Sarkar . Error - Driven Analysis of Challenges in Coreference Resolution Jonathan K. Kummerfeld and Dan Klein . Event Schema Induction with a Probabilistic Entity - Driven Model Nate Chambers . Event - based Time Label Propagation for Automatic Dating of News Articles Tao Ge , Baobao Chang , Sujian Li and Zhifang Sui . Exploiting Discourse Analysis for Article - Wide Temporal Classification Jun Ping Ng , Min - Yen Kan , Ziheng Lin , Vanessa Wei Feng , Bin Chen , Jian Su and Chew Lim Tan . Exploiting Domain Knowledge in Aspect Extraction Zhiyuan Chen , Arjun Mukherjee , Bing Liu , Meichun Hsu , Malu Castellanos and Riddhiman Ghosh . Exploiting Meta Features for Dependency Parsing Wenliang Chen , Min Zhang and Yue Zhang . Exploiting Multiple Sources for Open - domain Hypernym Discovery Ruiji Fu , Bing Qin and Ting Liu . Exploiting Zero Pronouns to Improve Chinese Coreference Resolution Fang Kong and Hwee Tou Ng . Exploiting language models for visual recognition Dieu - Thu Le , Jasper Uijlings and Raffaella Bernardi . Exploring Demographic Language Variations to Improve Multilingual Sentiment Analysis in Social Media Svitlana Volkova , Theresa Wilson and David Yarowsky . Exploring Representations from Unlabeled Data with Co - training for Chinese Word Segmentation Longkai Zhang and Houfeng Wang . Exploring the utility of joint morphological and syntactic learning from child - directed speech Stella Frank , Frank Keller and Sharon Goldwater . Factored Soft Source Syntactic Constraints for Hierarchical Machine Translation Zhongqiang Huang , Jacob Devlin and Rabih Zbib . Fast Joint Compression and Summarization via Graph Cuts Xian Qian and Yang Liu . Feature Noising for Log - linear Structured Prediction Sida Wang , Mengqiu Wang , Chris Manning , Percy Liang and Stefan Wager . Flexible and Efficient Hypergraph Interactions for Joint Hierarchical and Forest - to - String Decoding Martin Cmejrek , Haitao Mi and Bowen Zhou . Gender Inference of Twitter Users in Non - English Contexts Morgane Ciot , Morgan Sonderegger and Derek Ruths . Generating Coherent Event Schemas at Scale Niranjan Balasubramanian , Stephen Soderland , Mausam - and Oren Etzioni . Grounding Strategic Conversation : Using negotiation dialogues to predict trades in a win - lose game Anais Cadilhac , Nicholas Asher , Farah Benamara and Alex Lascarides . Growing Multi - Domain Glossaries from a Few Seeds with Probabilistic Topic Models Stefano Faralli and Roberto Navigli . Harvesting Parallel News Streams to Generate Paraphrases of Event Relations Congle Zhang and Daniel Weld . Identifying Manipulated Offerings on Review Portals Jiwei Li , Myle Ott and Claire Cardie . Identifying Multiple Userids of the Same Author Tieyun Qian and Bing Liu . Identifying Phrasal Verbs Using Many Bilingual Corpora Karl Pichotta and John DeNero . Identifying Web Search Query Reformulation using Concept based Matching Ahmed Hassan . Image Description using Visual Dependency Representations Desmond Elliott and Frank Keller . Improvements to the Bayesian Topic N - gram Models Hiroshi Noji , Daichi Mochihashi and Yusuke Miyao . Improving Alignment of System Combination by Using Multi - objective Optimization Tian Xia , Zongcheng Ji and Yidong Chen . Improving Pivot - Based Statistical Machine Translation Using Random Walk Xiaoning Zhu , Zhongjun He , Hua Wu , Haifeng Wang , Conghui Zhu and Tiejun Zhao . Improving Web Search Ranking by Incorporating Structured Annotation of Queries Xiao Ding , Zhicheng Dou , Bing Qin , Ting Liu and Ji - rong Wen . Inducing Document Plans for Concept - to - text Generation Ioannis Konstas and Mirella Lapata . Interactive Machine Translation using Hierarchical Translation Models Jes\\u00fas Gonz\\u00e1lez - Rubio , Daniel Ort\\u00edz - Martinez , Jos\\u00e9 - Miguel Bened\\u00ed and Francisco Casacuberta . Interpreting Anaphoric Shell Nouns using Cataphoric Shell Nouns as Training Data Varada Kolhatkar , Heike Zinsmeister and Graeme Hirst . Japanese Zero Reference Resolution Considering Exophora and Author / Reader Mentions Masatsugu Hangyo , Daisuke Kawahara and Sadao Kurohashi . Joint Bootstrapping of Corpus Annotations and Entity Types Hrushikesh Mohapatra , Siddhanth Jain and Soumen Chakrabarti . Joint Language and Translation Modeling with Recurrent Neural Networks Michael Auli , Michel Galley , Chris Quirk and Geoffrey Zweig . Joint Learning and Inference for Grammatical Error Correction Alla Rozovskaya and Dan Roth . Joint Word Segmentation and POS Tagging on Heterogeneous Annotated Corpora with Multiple Task Learning Xipeng Qiu and Xuanjing Huang . Joint segmentation and supertagging for English Rebecca Dridan . Latent Anaphora Resolution for Cross - Lingual Pronoun Prediction Christian Hardmeier , J\\u00f6rg Tiedemann and Joakim Nivre . Learning Biological Processes with Global Constraints Aju Thalappillil Scaria , Jonathan Berant , Mengqiu Wang , Peter Clark , Justin Lewis , Brittany Harding and Christopher Manning . Learning Distributions over Logical Forms for Referring Expression Generation Nicholas FitzGerald , Yoav Artzi and Luke Zettlemoyer . Learning Latent Word Representations for Domain Adaptation using Supervised Word Clustering Min Xiao , Feipeng Zhao and Yuhong Guo . Learning Topics and Positions from Debatepedia Swapna Gottipati , Minghui Qiu , Yanchuan Sim , Jing Jiang and Noah A. Smith . Learning to Freestyle : Hip Hop Challenge - Response Induction via Transduction Rule Chunking and Segmentation Dekai Wu , Karteek Addanki and Markus Saers . Leveraging Alternative Grammar Extraction Strategies using Lagrangian Relaxation with PCFG - LA Product Model Parsing : A Case Study with Function Labels and Binarization Joseph Le Roux , Antoine Rozenknop and Jennifer Foster . Leveraging lexical cohesion and disruption for topic segmentation Anca - Roxana Simon , Guillaume Gravier and Pascale S\\u00e9billot . Lexical Chain Based Cohesion Models for Document - Level Statistical Machine Translation Deyi Xiong , Yang Ding , Min Zhang and Chew Lim Tan . Log - linear Language Models based on Structured Sparsity Anil Nelakanti , Cedric Archambeau , Julien Mairal , Francis Bach and Guillaume Bouchard . MCTest : A Challenge Dataset for the Open - Domain Machine Comprehension of Text Matthew Richardson , Chris Burges and Erin Renshaw . Max - Margin Synchronous Grammar Induction for Machine Translation Xinyan Xiao and Deyi Xiong . Measuring Ideological Proportions in Political Speeches Yanchuan Sim , Brice Acree , Justin H. Gross and Noah A. Smith . Mining New Business Opportunities : Identifying Trend related Products by Leveraging Commercial Intents from Microblogs Jinpeng Wang , Wayne Xin Zhao and Xiaoming Li . Mining Scientific Terms and their Definitions : A Study of the ACL Anthology Yiping Jin , Min - Yen Kan , Jun - Ping Ng and Xiangnan He . Modeling Scientific Impact with Topical Influence Regression James Foulds and Padhraic Smyth . Modeling and Learning Semantic Co - Compositionality through Prototype Projections and Neural Networks Masashi Tsubaki , Kevin Duh , Masashi Shimbo and Yuji Matsumoto . Monolingual Marginal Matching for Translation Model Adaptation Ann Irvine , Chris Quirk and Hal Daum\\u00e9 III . Multi - Relational Latent Semantic Analysis Kai - Wei Chang , Wen - Tau Yih and Christopher Meek . Multi - domain Adaptation for SMT Using Multi - task Learning Lei Cui , Xilun Chen , Dongdong Zhang , Shujie Liu , Mu Li and Ming Zhou . Joint Coreference Resolution and Named - Entity Linking with Multi - pass Sieves Hannaneh Hajishirzi , Leila Zilles , Daniel S. Weld and Luke Zettlemoyer . Open Domain Targeted Sentiment Margaret Mitchell , Jacqui Aguilar , Theresa Wilson and Benjamin Van Durme . Open - Domain Fine - Grained Class Extraction from Web Search Queries Marius Pasca . Opinion Mining in Newspaper Articles by Entropy - based Word Connections Thomas Scholz and Stefan Conrad . Optimal Beam Search for Machine Translation Alexander Rush , Yin - Wen Chang and Michael Collins . Optimized Event Storyline Generation based on Mixture - Event - Aspect Model Lifu Huang and Lian'en Huang . Orthonormal explicit topic analysis for cross - lingual document matching John Philip McCrae , Philipp Cimiano and Roman Klinger . Overcoming the Lack of Parallel Data in Sentence Compression Katja Filippova and Yasemin Altun . Paraphrasing 4 Unsupervised Microblog Normalization Wang Ling , Chris Dyer , Alan Black and Isabel Trancoso . Predicting Success of Novels from Writing Styles Vikas Ashok , Song Feng and Yejin Choi . Predicting the Presence of Discourse Connectives Gary Patterson and Andrew Kehler . Prior Disambiguation of Word Tensors for Constructing Sentence Vectors Dimitri Kartsaklis and Mehrnoosh Sadrzadeh . Recursive Autoencoders for ITG - based Translation Peng Li , Yang Liu and Maosong Sun . Recursive Models for Semantic Compositionality Over a Sentiment Treebank Richard Socher , Alex Perelygin , Jean Wu , Christopher Manning , Andrew Ng and Jason Chuang . Regularized Minimum Error Rate Training Michel Galley , Chris Quirk , Colin Cherry and Kristina Toutanova . Relational Inference for Wikification Xiao Cheng and Dan Roth . Sarcasm as Contrast between a Positive Sentiment and Negative Situation Ellen Riloff , Ashequl Qadir , Prafulla Surve , Lalindra De Silva , Nathan Gilbert and Ruihong Huang . Scaling Semantic Parsers with On - the - fly Ontology Matching Tom Kwiatkowski , Eunsol Choi , Yoav Artzi and Luke Zettlemoyer . Semantic Parsing on Freebase from Question - Answer Pairs Jonathan Berant , Roy Frostig , Andrew Chou and Percy Liang . Semi - Markov Phrase - based Monolingual Alignment Xuchen Yao , Benjamin Van Durme , Chris Callison - Burch and Peter Clark . Sentiment Analysis : How to Derive Prior Polarities from SentiWordNet Marco Guerini , Lorenzo Gatti and Marco Turchi . Simulating Early - Termination Search for Verbose Spoken Queries Jerome White , Douglas Oard , Nitendra Rajput and Marion Zalk . Source - Side Classifier Preordering for Machine Translation Uri Lerner and Slav Petrov . Studying the recursive behaviour of adjectival modification with compositional distributional semantics Eva Maria Vecchi , Roberto Zamparelli and Marco Baroni . Summarizing Complex Events : a Cross - modal Solution of Storylines Extraction and Reconstruction Shize Xu and Yan Zhang . The Answer is at your Fingertips : Improving Passage Retrieval for Web Question Answering with Search Behavior Data Mikhail Ageev , Dmitry Lagun and Eugene Agichtein . The Effects of Syntactic Features in Automatic Prediction of Morphology Wolfgang Seeker and Jonas Kuhn . The Topology of Semantic Knowledge Jimmy Dubuisson , Jean - Pierre Eckmann , Christian Scheible and Hinrich Sch\\u00fctze . Towards Situated Dialogue : Revisiting Referring Expression Generation Rui Fang , Changsong Liu , Lanbo She and Joyce Chai . Translating into Morphologically Rich Languages with Synthetic Phrases Victor Chahuneau , Eva Schlinger , Chris Dyer and Noah A. Smith . Translation with Source Constituency and Dependency Trees Fandong Meng , Jun Xie , Linfeng Song , Yajuan Lv and Qun Liu . Tree Kernel - based Negation and Speculation Scope Detection with Structured Syntactic Parse Features Bowei Zou and Guodong Zhou . Two Recurrent Continuous Translation Models Nal Kalchbrenner and Phil Blunsom . Two - stage Method for Large - scale Acquisition of Contradiction Pattern Pairs using Entailment Julien Kloetzer , Stijn De Saeger , Kentaro Torisawa , Chikara Hashimoto , Jong - Hoon Oh , Motoki Sano and Kiyonori Ohtake . Understanding and Quantifying Creativity in Lexical Composition Polina Kuznetsova , Jianfu Chen and Yejin Choi . Unsupervised Induction of Contingent Event Pairs from Film Scenes Zhichao Hu , Elahe Rahimtoroghi , Larissa Munishkina , Reid Swanson and Marilyn Walker . Unsupervised Induction of Cross - lingual Semantic Relations Mike Lewis , Mark Steedman . Unsupervised Relation Extraction with General Domain Knowledge Oier Lopez de Lacalle and Mirella Lapata . Unsupervised Spectral Learning of WCFG as Low - rank Matrix Completion Franco M. Luque , Rapha\\u00ebl Bailly , Xavier Carreras and Ariadna Quattoni . Violation - Fixing Perceptron and Forced Decoding for Scalable MT Training Heng Yu , Liang Huang and Haitao Mi . Short papers . A Corpus Level MIRA Tuning Strategy for Machine Translation Ming Tan , Tian Xia , Shaojun Wang and Bowen Zhou . A Walk - based Semantically Enriched Tree Kernel Over Distributed Word Representations Shashank Srivastava , Dirk Hovy and Eduard Hovy . A synchronous context free grammar for time normalization Steven Bethard . Animacy Detection with Voting Models Joshua Moore , Chris Burges , Erin Renshaw and Wen - tau Yih . Application of Localized Similarity for Web Documents Peter Reber\\u0161ek and Mateja Verlic . Appropriately Incorporating Statistical Significance in PMI Om Damani and Shweta Ghonge . Automatic Domain Partitioning for Multi - Domain Learning Di Wang , Chenyan Xiong and William Yang Wang . Automatic Idiom Identification in Wiktionary Grace Muzny and Luke Zettlemoyer . Automatically Identifying Pseudepigraphic Texts Moshe Koppel and Shachar Seidman . Averaged Recursive Neural Networks for Semantic Relation Classification Kazuma Hashimoto , Makoto Miwa , Yoshimasa Tsuruoka and Takashi Chikayama . Bilingual Word Embeddings for Phrase - Based Machine Translation Will Zou , Richard Socher , Daniel Cer and Christopher Manning . Cascading Collective Classification for Bridging Anaphora Recognition using a Rich Linguistic Feature Set Yufang Hou , Katja Markert and Michael Strube . Classifying Message Board Posts with an Extracted Lexicon of Patient Attributes Ruihong Huang and Ellen Riloff . Combining Generative and Discriminative Model Scores for Distant Supervision Benjamin Roth and Dietrich Klakow . Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction Jason Weston , Antoine Bordes , Oksana Yakhnenko and Nicolas Usunier . Converting Continuous - Space Language Models into N - gram Language Models for Statistical Machine Translation Rui Wang , Masao Utiyama , Isao Goto , Eiichro Sumita , Hai Zhao and Bao - Liang Lu . Decipherment with a Million Random Restarts Taylor Berg - Kirkpatrick and Dan Klein . Decoding with Large - Scale Neural Language Models Improves Translation Ashish Vaswani , yinggong zhao , Victoria Fossum and David Chiang . Dependency language models for sentence completion Joseph Gubbins and Andreas Vlachos . Deriving adjectival scales from continuous space word representations Joo - Kyung Kim and Marie - Catherine de Marneffe . Detecting Compositionality of Multi - Word Expressions using Nearest Neighbours in Vector Space Models Douwe Kiela and Stephen Clark . Detecting Promotional Content in Wikipedia Shruti Bhosale , Heath Vinicombe and Ray Mooney . Detection of Product Comparisons -- How Far Does an Out - of - the - box Semantic Role Labeling System Take You ? Wiltrud Kessler and Jonas Kuhn . Discriminative Improvements to Distributional Sentence Similarity Yangfeng Ji and Jacob Eisenstein . Efficient Classification of Documents with Bursty Labels Sam Wiseman and Michael Crouse . Elephant : Sequence Labeling for Word and Sentence Segmentation Kilian Evang , Valerio Basile , Grzegorz Chrupa\\u0142a and Johan Bos . Fish transporters and miracle homes : How compositional distributional semantics can help NP bracketing Angeliki Lazaridou , Eva Maria Vecchi and Marco Baroni . Implicit Feature Detection via an Constrained Topic Model and SVM Wang Wei and Xu Hua . Improving Learning and Inference in a Large Knowledge - base using Latent Syntactic Cues Matt Gardner , Partha Talukdar , Bryan Kisiel and Tom Mitchell . Improving Statistical Machine Translation with Word Class Models Joern Wuebker , Stephan Peitz , Felix Rietig and Hermann Ney . Is Twitter A Better Corpus for Measuring Sentiment Similarity ? Shi Feng , Le Zhang , Kaisong Song and Daling Wang . Joint Parsing and Disfluency Detection in Linear Time Mohammad Sadegh Rasooli and Joel Tetreault . Learning to rank lexical substitutions Gy\\u00f6rgy Szarvas , R\\u00f3bert Busa - Fekete and Eyke H\\u00fcllermeier . Machine Learning for Chinese Zero Pronoun Resolution : Some Recent Advances Chen Chen and Vincent Ng . Microblog Entity Linking by Leveraging Multiple Posts Yuhang Guo , Bing Qin , Ting Liu and Sheng Li . Naive Bayes Word Sense Induction Do Kook Choe and Eugene Charniak . Noise - aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora Katsuhito Sudoh , Shinsuke Mori and Masaaki Nagata . Online Learning for Inexact Hypergraph Search Hao Zhang , Kai Zhao , Liang Huang and Ryan McDonald . Pair Language Models for Deriving Alternative Pronunciations and Spellings from Pronunciation Dictionaries Russell Beckley and Brian Roark . Predicting the resolution of referring expressions from user behavior Nikos Engonopoulos , Martin Villalba , Ivan Titov and Alexander Koller . Question Difficulty Estimation in Community Question Answering Services Jing Liu , Quan Wang and Chin - Yew Lin . Rule - based Information Extraction is Dead ! Long Live Rule - based Information Extraction Systems ! Laura Chiticariu , Yunyao Li and Frederick Reiss . Russian Stress Prediction using Maximum Entropy Ranking Richard Sproat and Keith Hall . Scaling to Large^3 Data : An efficient and effective method to compute Distributional Thesauri Martin Riedl and Chris Biemann . Shift - Reduce Word Reordering for Machine Translation Katsuhiko Hayashi , Katsuhito Sudoh , Hajime Tsukada , Jun Suzuki and Masaaki NAGATA . Single - Document Summarization as a Tree Knapsack Problem Tsutomu Hirao , Yasuhisa Yoshida , Masaaki Nishino , Norihito Yasuda and Masaaki Nagata . The VerbCorner Project : Toward an empirically - based semantic decomposition of verbs Joshua Hartshorne , Claire Bonial and Martha Palmer . Using Paraphrases and Lexical Semantics to Improve the Accuracy and the Robustness of Supervised Models in Situated Dialogue Systems Claire Gardent and Lina Maria Rojas Barahona . Using Soft Constraints in Joint Inference for Clinical Concept Recognition Prateek Jindal and Dan Roth . Using Topic Modeling to Improve Prediction of Neuroticism and Depression in College Students Philip Resnik , Anderson Garron and Rebecca Resnik . Using crowdsourcing to get representations based on regular expressions Anders S\\u00f8gaard , Hector Martinez , Jakob Elming and Anders Johannsen . Well - argued recommendation : adaptive models based on words in recommender systems Julien Gaillard , Marc El - Beze , Eitan Altman and Emmanuel Ethis . What is Hidden among Translation Rules Libin Shen and Bowen Zhou . Where Not to Eat ? Predicting Restaurant Inspections from Online Reviews Jun Seok Kang , Polina Kuznetsova , Michael Luca and Yejin Choi . With blinkers on : A robust model of eye movements across readers Franz Matthies and Anders S\\u00f8gaard . Word Level Language Identification in Online Multilingual Communication Dong Nguyen and Seza Dogruoz \"}",
        "_version_":1692669946971029505,
        "score":24.84444},
      {
        "id":"1dc09097-354c-45fd-8983-d8a1d471f3e9",
        "_src_":"{\"url\": \"https://channel9.msdn.com/Forums/Coffeehouse/Time-Lapse-Video-Control-Center\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158481.37/warc/CC-MAIN-20160205193918-00090-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Extracting Information from Text . For any given question , it 's likely that someone has written the answer down somewhere . The amount of natural language text that is available in electronic form is truly staggering , and is increasing every day . However , the complexity of natural language can make it very difficult to access the information in that text . The state of the art in NLP is still a long way from being able to build general - purpose representations of meaning from unrestricted text . If we instead focus our efforts on a limited set of questions or \\\" entity relations , \\\" such as \\\" where are different facilities located , \\\" or \\\" who is employed by what company , \\\" we can make significant progress . The goal of this chapter is to answer the following questions : . How can we build a system that extracts structured data , such as tables , from unstructured text ? What are some robust methods for identifying the entities and relationships described in a text ? Which corpora are appropriate for this work , and how do we use them for training and evaluating our models ? Along the way , we 'll apply techniques from the last two chapters to the problems of chunking and named - entity recognition . 1 Information Extraction . Information comes in many shapes and sizes . One important form is structured data , where there is a regular and predictable organization of entities and relationships . For example , we might be interested in the relation between companies and locations . Given a particular company , we would like to be able to identify the locations where it does business ; conversely , given a location , we would like to discover which companies do business in that location . If our data is in tabular form , such as the example in 1.1 , then answering these queries is straightforward . If this location data was stored in Python as a list of tuples ( entity , relation , entity ) , then the question \\\" Which organizations operate in Atlanta ? \\\" could be translated as follows : . The fourth Wells account moving to another agency is the packaged paper - products division of Georgia - Pacific Corp. , which arrived at Wells only last fall . Like Hertz and the History Channel , it is also leaving for an Omnicom - owned agency , the BBDO South unit of BBDO Worldwide . BBDO South in Atlanta , which handles corporate advertising for Georgia - Pacific , will assume additional duties for brands like Angel Soft toilet tissue and Sparkle paper towels , said Ken Haldin , a spokesman for Georgia - Pacific in Atlanta . If you read through ( 1 ) , you will glean the information required to answer the example question . But how do we get a machine to understand enough about ( 1 ) to return the answers in 1.2 ? This is obviously a much harder task . Unlike 1.1 , ( 1 ) contains no structure that links organization names with location names . One approach to this problem involves building a very general representation of meaning ( 10 . ) In this chapter we take a different approach , deciding in advance that we will only look for very specific kinds of information in text , such as the relation between organizations and locations . Rather than trying to use text like ( 1 ) to answer the question directly , we first convert the unstructured data of natural language sentences into the structured data of 1.1 . Then we reap the benefits of powerful query tools such as SQL . This method of getting meaning from text is called Information Extraction . Information Extraction has many applications , including business intelligence , resume harvesting , media analysis , sentiment detection , patent search , and email scanning . A particularly important area of current research involves the attempt to extract structured data out of electronically - available scientific literature , especially in the domain of biology and medicine . 1.1 Information Extraction Architecture . 1.1 shows the architecture for a simple information extraction system . It begins by processing a document using several of the procedures discussed in 3 and 5 . : first , the raw text of the document is split into sentences using a sentence segmenter , and each sentence is further subdivided into words using a tokenizer . Next , each sentence is tagged with part - of - speech tags , which will prove very helpful in the next step , named entity detection . In this step , we search for mentions of potentially interesting entities in each sentence . Finally , we use relation detection to search for likely relations between different entities in the text . Figure 1.1 : Simple Pipeline Architecture for an Information Extraction System . This system takes the raw text of a document as its input , and generates a list of ( entity , relation , entity ) tuples as its output . For example , given a document that indicates that the company Georgia - Pacific is located in Atlanta , it might generate the tuple ( [ ORG : ' Georgia - Pacific ' ] ' in ' [ LOC : ' Atlanta ' ] ) . To perform the first three tasks , we can define a simple function that simply connects together NLTK 's default sentence segmenter , word tokenizer , and part - of - speech tagger : . Note . Remember that our program samples assume you begin your interactive session or your program with : import nltk , re , pprint . Next , in named entity detection , we segment and label the entities that might participate in interesting relations with one another . Typically , these will be definite noun phrases such as the knights who say \\\" ni \\\" , or proper names such as Monty Python . In some tasks it is useful to also consider indefinite nouns or noun chunks , such as every student or cats , and these do not necessarily refer to entities in the same way as definite NP s and proper names . Finally , in relation extraction , we search for specific patterns between pairs of entities that occur near one another in the text , and use those patterns to build tuples recording the relationships between the entities . 2 Chunking . The basic technique we will use for entity detection is chunking , which segments and labels multi - token sequences as illustrated in 2.1 . The smaller boxes show the word - level tokenization and part - of - speech tagging , while the large boxes show higher - level chunking . Each of these larger boxes is called a chunk . Like tokenization , which omits whitespace , chunking usually selects a subset of the tokens . Also like tokenization , the pieces produced by a chunker do not overlap in the source text . Figure 2.1 : Segmentation and Labeling at both the Token and Chunk Levels . In this section , we will explore chunking in some depth , beginning with the definition and representation of chunks . We will see regular expression and n - gram approaches to chunking , and will develop and evaluate chunkers using the CoNLL-2000 chunking corpus . We will then return in ( 5 ) and 6 to the tasks of named entity recognition and relation extraction . 2.1 Noun Phrase Chunking . We will begin by considering the task of noun phrase chunking , or NP - chunking , where we search for chunks corresponding to individual noun phrases . For example , here is some Wall Street Journal text with NP -chunks marked using brackets : . [ The / DT market / NN ] for / IN [ system - management / NN software / NN ] for / IN [ Digital / NNP ] [ ' s / POS hardware / NN ] is / VBZ fragmented / JJ enough / RB that / IN [ a / DT giant / NN ] such / JJ as / IN [ Computer / NNP Associates / NNPS ] should / MD do / VB well / RB there / RB . As we can see , NP -chunks are often smaller pieces than complete noun phrases . For example , the market for system - management software for Digital 's hardware is a single noun phrase ( containing two nested noun phrases ) , but it is captured in NP -chunks by the simpler chunk the market . One of the motivations for this difference is that NP -chunks are defined so as not to contain other NP -chunks . Consequently , any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP -chunk , since they almost certainly contain further noun phrases . One of the most useful sources of information for NP -chunking is part - of - speech tags . This is one of the motivations for performing part - of - speech tagging in our information extraction system . We demonstrate this approach using an example sentence that has been part - of - speech tagged in 2.2 . In order to create an NP -chunker , we will first define a chunk grammar , consisting of rules that indicate how sentences should be chunked . In this case , we will define a simple grammar with a single regular - expression rule . This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner ( DT ) followed by any number of adjectives ( JJ ) and then a noun ( NN ) . Using this grammar , we create a chunk parser , and test it on our example sentence . The result is a tree , which we can either print , or display graphically . 2.2 Tag Patterns . The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words . Tag patterns are similar to regular expression patterns ( 3.4 ) . Now , consider the following noun phrases from the Wall Street Journal : . another / DT sharp / JJ dive / NN trade / NN figures / NNS any / DT new / JJ policy / NN measures / NNS earlier / JJR stages / NNS Panamanian / JJ dictator / NN Manuel / NNP Noriega / NNP . This will chunk any sequence of tokens beginning with an optional determiner , followed by zero or more adjectives of any type ( including relative adjectives like earlier / JJR ) , followed by one or more nouns of any type . However , it is easy to find many more complicated examples which this rule will not cover : . his / PRP$ Mansion / NNP House / NNP speech / NN the / DT price / NN cutting / VBG 3/CD % /NN to / TO 4/CD % /NN more / JJR than / IN 10/CD % /NN the / DT fastest / JJS developing / VBG trends / NNS ' s / POS skill / NN . Note . Your Turn : Try to come up with tag patterns to cover these cases . Test them using the graphical interface nltk.app.chunkparser ( ) . Continue to refine your tag patterns with the help of the feedback given by this tool . 2.3 Chunking with Regular Expressions . To find the chunk structure for a given sentence , the RegexpParser chunker begins with a flat structure in which no tokens are chunked . The chunking rules are applied in turn , successively updating the chunk structure . Once all of the rules have been invoked , the resulting chunk structure is returned . 2.3 shows a simple chunk grammar consisting of two rules . The first rule matches an optional determiner or possessive pronoun , zero or more adjectives , then a noun . The second rule matches one or more proper nouns . We also define an example sentence to be chunked , and run the chunker on this input . ( \\\" her \\\" , \\\" PP$ \\\" ) , ( \\\" long \\\" , \\\" JJ \\\" ) , ( \\\" golden \\\" , \\\" JJ \\\" ) , ( \\\" hair \\\" , \\\" NN \\\" ) ] . The $ symbol is a special character in regular expressions , and must be backslash escaped in order to match the tag PP$ . If a tag pattern matches at overlapping locations , the leftmost match takes precedence . For example , if we apply a rule that matches two consecutive nouns to a text containing three consecutive nouns , then only the first two nouns will be chunked : . Once we have created the chunk for money market , we have removed the context that would have permitted fund to be included in a chunk . Note . We have added a comment to each of our chunk rules . These are optional ; when they are present , the chunker prints these comments as part of its tracing output . 2.4 Exploring Text Corpora . In 2 we saw how we could interrogate a tagged corpus to extract phrases matching a particular sequence of part - of - speech tags . We can do the same work more easily with a chunker , as follows : . Note . 2.5 Chinking . Sometimes it is easier to define what we want to exclude from a chunk . We can define a chink to be a sequence of tokens that is not included in a chunk . In the following example , barked / VBD at / IN is a chink : . [ the / DT little / JJ yellow / JJ dog / NN ] barked / VBD at / IN [ the / DT cat / NN ] . Chinking is the process of removing a sequence of tokens from a chunk . If the matching sequence of tokens spans an entire chunk , then the whole chunk is removed ; if the sequence of tokens appears in the middle of the chunk , these tokens are removed , leaving two chunks where there was only one before . If the sequence is at the periphery of the chunk , these tokens are removed , and a smaller chunk remains . These three possibilities are illustrated in 2.1 . 2.6 Representing Chunks : Tags vs Trees . As befits their intermediate status between tagging and parsing ( 8 . ) , chunk structures can be represented using either tags or trees . The most widespread file representation uses IOB tags . In this scheme , each token is tagged with one of three special chunk tags , I ( inside ) , O ( outside ) , or B ( begin ) . A token is tagged as B if it marks the beginning of a chunk . Subsequent tokens within the chunk are tagged I . All other tokens are tagged O . The B and I tags are suffixed with the chunk type , e.g. B - NP , I - NP . Of course , it is not necessary to specify a chunk type for tokens that appear outside a chunk , so these are just labeled O . An example of this scheme is shown in 2.5 . IOB tags have become the standard way to represent chunk structures in files , and we will also be using this format . Here is how the information in 2.5 would appear in a file : . We PRP B - NP saw VBD O the DT B - NP yellow JJ I - NP dog NN I - NP . In this representation there is one token per line , each with its part - of - speech tag and chunk tag . This format permits us to represent more than one chunk type , so long as the chunks do not overlap . As we saw earlier , chunk structures can also be represented using trees . These have the benefit that each chunk is a constituent that can be manipulated directly . An example is shown in 2.6 . NLTK uses trees for its internal representation of chunks , but provides methods for reading and writing such trees to the IOB format . 3 Developing and Evaluating Chunkers . Now you have a taste of what chunking does , but we have n't explained how to evaluate chunkers . As usual , this requires a suitably annotated corpus . We begin by looking at the mechanics of converting IOB format into an NLTK tree , then at how this is done on a larger scale using a chunked corpus . We will see how to score the accuracy of a chunker relative to a corpus , then look at some more data - driven ways to search for NP chunks . Our focus throughout will be on expanding the coverage of a chunker . 3.1 Reading IOB Format and the CoNLL 2000 Corpus . Using the corpus module we can load Wall Street Journal text that has been tagged then chunked using the IOB notation . The chunk categories provided in this corpus are NP , VP and PP . As we have seen , each sentence is represented using multiple lines , as shown below : . he PRP B - NP accepted VBD B - VP the DT B - NP position NN I - NP ... . A conversion function chunk.conllstr2tree ( ) builds a tree representation from one of these multi - line strings . Moreover , it permits us to choose any subset of the three chunk types to use , here just for NP chunks : . draw ( ) . We can use the NLTK corpus module to access a larger amount of chunked text . The CoNLL 2000 corpus contains 270k words of Wall Street Journal text , divided into \\\" train \\\" and \\\" test \\\" portions , annotated with part - of - speech tags and chunk tags in the IOB format . We can access the data using nltk.corpus.conll2000 . Here is an example that reads the 100th sentence of the \\\" train \\\" portion of the corpus : . As you can see , the CoNLL 2000 corpus contains three chunk types : NP chunks , which we have already seen ; VP chunks such as has already delivered ; and PP chunks such as because of . Since we are only interested in the NP chunks right now , we can use the chunk_types argument to select them : . 3.2 Simple Evaluation and Baselines . Now that we can access a chunked corpus , we can evaluate chunkers . We start off by establishing a baseline for the trivial chunk parser cp that creates no chunks : . The IOB tag accuracy indicates that more than a third of the words are tagged with O , i.e. not in an NP chunk . However , since our tagger did not find any chunks , its precision , recall , and f - measure are all zero . Now let 's try a naive regular expression chunker that looks for tags beginning with letters that are characteristic of noun phrase tags ( e.g. CD , DT , and JJ ) . As you can see , this approach achieves decent results . However , we can improve on it by adopting a more data - driven approach , where we use the training corpus to find the chunk tag ( I , O , or B ) that is most likely for each part - of - speech tag . In other words , we can build a chunker using a unigram tagger ( 4 ) . But rather than trying to determine the correct part - of - speech tag for each word , we are trying to determine the correct chunk tag , given each word 's part - of - speech tag . In 3.1 , we define the UnigramChunker class , which uses a unigram tagger to label sentences with chunk tags . Most of the code in this class is simply used to convert back and forth between the chunk tree representation used by NLTK 's ChunkParserI interface , and the IOB representation used by the embedded tagger . The class defines two methods : a constructor which is called when we build a new UnigramChunker ; and the parse method which is used to chunk new sentences . class UnigramChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . return nltk.chunk.conlltags2tree(conlltags ) . The constructor expects a list of training sentences , which will be in the form of chunk trees . It first converts training data to a form that is suitable for training the tagger , using tree2conlltags to map each chunk tree to a list of word , tag , chunk triples . It then uses that converted training data to train a unigram tagger , and stores it in self.tagger for later use . The parse method takes a tagged sentence as its input , and begins by extracting the part - of - speech tags from that sentence . It then tags the part - of - speech tags with IOB chunk tags , using the tagger self.tagger that was trained in the constructor . Next , it extracts the chunk tags , and combines them with the original sentence , to yield conlltags . Finally , it uses conlltags2tree to convert the result back into a chunk tree . Now that we have UnigramChunker , we can train it using the CoNLL 2000 corpus , and test its resulting performance : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 92.9 % Precision : 79.9 % Recall : 86.8 % F - Measure : 83.2 % . This chunker does reasonably well , achieving an overall f - measure score of 83 % . Let 's take a look at what it 's learned , by using its unigram tagger to assign a tag to each of the part - of - speech tags that appear in the corpus : . It has discovered that most punctuation marks occur outside of NP chunks , with the exception of # and $ , both of which are used as currency markers . It has also found that determiners ( DT ) and possessives ( PRP$ and WP$ ) occur at the beginnings of NP chunks , while noun types ( NN , NNP , NNPS , NNS ) mostly occur inside of NP chunks . Having built a unigram chunker , it is quite easy to build a bigram chunker : we simply change the class name to BigramChunker , and modify line in 3.1 to construct a BigramTagger rather than a UnigramTagger . The resulting chunker has slightly higher performance than the unigram chunker : . evaluate(test_sents ) ) ChunkParse score : IOB Accuracy : 93.3 % Precision : 82.3 % Recall : 86.8 % F - Measure : 84.5 % . 3.3 Training Classifier - Based Chunkers . Both the regular - expression based chunkers and the n - gram chunkers decide what chunks to create entirely based on part - of - speech tags . However , sometimes part - of - speech tags are insufficient to determine how a sentence should be chunked . For example , consider the following two statements : . These two sentences have the same part - of - speech tags , yet they are chunked differently . In the first sentence , the farmer and rice are separate chunks , while the corresponding material in the second sentence , the computer monitor , is a single chunk . Clearly , we need to make use of information about the content of the words , in addition to just their part - of - speech tags , if we wish to maximize chunking performance . One way that we can incorporate information about the content of words is to use a classifier - based tagger to chunk the sentence . Like the n - gram chunker considered in the previous section , this classifier - based chunker will work by assigning IOB tags to the words in a sentence , and then converting those tags to chunks . For the classifier - based tagger itself , we will use the same approach that we used in 1 to build a part - of - speech tagger . The basic code for the classifier - based NP chunker is shown in 3.2 . It consists of two classes . The first class is almost identical to the ConsecutivePosTagger class from 1.5 . The only two differences are that it calls a different feature extractor and that it uses a MaxentClassifier rather than a NaiveBayesClassifier . The second class is basically a wrapper around the tagger class that turns it into a chunker . During training , this second class maps the chunk trees in the training corpus into tag sequences ; in the parse ( ) method , it converts the tag sequence provided by the tagger back into a chunk tree . class ConsecutiveNPChunkTagger ( nltk . TaggerI ) : def _ _ init _ _ ( self , train_sents ) : . train_set . append ( ( featureset , tag ) ) . history.append(tag ) . history.append(tag ) . return zip(sentence , history ) class ConsecutiveNPChunker ( nltk . ChunkParserI ) : def _ _ init _ _ ( self , train_sents ) : . nltk.chunk.tree2conlltags(sent ) ] for sent in train_sents ] . return nltk.chunk.conlltags2tree(conlltags ) . The only piece left to fill in is the feature extractor . We begin by defining a simple feature extractor which just provides the part - of - speech tag of the current token . Using this feature extractor , our classifier - based chunker is very similar to the unigram chunker , as is reflected in its performance : . We can also add a feature for the previous part - of - speech tag . Adding this feature allows the classifier to model interactions between adjacent tags , and results in a chunker that is closely related to the bigram chunker . Next , we 'll try adding a feature for the current word , since we hypothesized that word content should be useful for chunking . We find that this feature does indeed improve the chunker 's performance , by about 1.5 percentage points ( which corresponds to about a 10 % reduction in the error rate ) . Finally , we can try extending the feature extractor with a variety of additional features , such as lookahead features , paired features , and complex contextual features . join(sorted(tags ) ) . Note . Your Turn : Try adding different features to the feature extractor function npchunk_features , and see if you can further improve the performance of the NP chunker . 4 Recursion in Linguistic Structure . 4.1 Building Nested Structure with Cascaded Chunkers . So far , our chunk structures have been relatively flat . Trees consist of tagged tokens , optionally grouped under a chunk node such as NP . However , it is possible to build chunk structures of arbitrary depth , simply by creating a multi - stage chunk grammar containing recursive rules . 4.1 has patterns for noun phrases , prepositional phrases , verb phrases , and sentences . This is a four - stage chunk grammar , and can be used to create structures having a depth of at most four . ( \\\" sit \\\" , \\\" VB \\\" ) , ( \\\" on \\\" , \\\" IN \\\" ) , ( \\\" the \\\" , \\\" DT \\\" ) , ( \\\" mat \\\" , \\\" NN \\\" ) ] . Unfortunately this result misses the VP headed by saw . It has other shortcomings too . Let 's see what happens when we apply this chunker to a sentence having deeper nesting . Notice that it fails to identify the VP chunk starting at . The solution to these problems is to get the chunker to loop over its patterns : after trying all of them , it repeats the process . We add an optional second argument loop to specify the number of times the set of patterns should be run : . Note . This cascading process enables us to create deep structures . However , creating and debugging a cascade is difficult , and there comes a point where it is more effective to do full parsing ( see 8 . ) Also , the cascading process can only produce trees of fixed depth ( no deeper than the number of stages in the cascade ) , and this is insufficient for complete syntactic analysis . 4.2 Trees . A tree is a set of connected labeled nodes , each reachable by a unique path from a distinguished root node . Here 's an example of a tree ( note that they are standardly drawn upside - down ) : . We use a ' family ' metaphor to talk about the relationships of nodes in a tree : for example , S is the parent of VP ; conversely VP is a child of S . Also , since NP and VP are both children of S , they are also siblings . For convenience , there is also a text format for specifying trees : . ( S ( NP Alice ) ( VP ( V chased ) ( NP ( Det the ) ( N rabbit ) ) ) ) . Although we will focus on syntactic trees , trees can be used to encode any homogeneous hierarchical structure that spans a sequence of linguistic forms ( e.g. morphological structure , discourse structure ) . In the general case , leaves and node values do not have to be strings . In NLTK , we create a tree by giving a node label and a list of children : . We can incorporate these into successively larger trees as follows : . Here are some of the methods available for tree objects : . The bracketed representation for complex trees can be difficult to read . In these cases , the draw method can be very useful . It opens a new window , containing a graphical representation of the tree . The tree display window allows you to zoom in and out , to collapse and expand subtrees , and to print the graphical representation to a postscript file ( for inclusion in a document ) . 4.3 Tree Traversal . def traverse ( t ) : . try : . ( S ( NP Alice ) ( VP chased ( NP the rabbit ) ) ) . 5 Named Entity Recognition . At the start of this chapter , we briefly introduced named entities ( NEs ) . Named entities are definite noun phrases that refer to specific types of individuals , such as organizations , persons , dates , and so on . 5.1 lists some of the more commonly used types of NEs . These should be self - explanatory , except for \\\" Facility \\\" : human - made artifacts in the domains of architecture and civil engineering ; and \\\" GPE \\\" : geo - political entities such as city , state / province , and country . The goal of a named entity recognition ( NER ) system is to identify all textual mentions of the named entities . This can be broken down into two sub - tasks : identifying the boundaries of the NE , and identifying its type . While named entity recognition is frequently a prelude to identifying relations in Information Extraction , it can also contribute to other tasks . For example , in Question Answering ( QA ) , we try to improve the precision of Information Retrieval by recovering not whole pages , but just those parts which contain an answer to the user 's question . Most QA systems take the documents returned by standard Information Retrieval , and then attempt to isolate the minimal text snippet in the document containing the answer . Now suppose the question was Who was the first President of the US ? , and one of the documents that was retrieved contained the following passage : . The Washington Monument is the most prominent structure in Washington , D.C. and one of the city 's early attractions . It was built in honor of George Washington , who led the country to independence and then became its first President . Analysis of the question leads us to expect that an answer should be of the form X was the first President of the US , where X is not only a noun phrase , but also refers to a named entity of type PERSON . This should allow us to ignore the first sentence in the passage . While it contains two occurrences of Washington , named entity recognition should tell us that neither of them has the correct type . How do we go about identifying named entities ? One option would be to look up each word in an appropriate list of names . For example , in the case of locations , we could use a gazetteer , or geographical dictionary , such as the Alexandria Gazetteer or the Getty Gazetteer . However , doing this blindly runs into problems , as shown in 5.1 . Figure 5.1 : Location Detection by Simple Lookup for a News Story : Looking up every word in a gazetteer is error - prone ; case distinctions may help , but these are not always present . Observe that the gazetteer has good coverage of locations in many countries , and incorrectly finds locations like Sanchez in the Dominican Republic and On in Vietnam . Of course we could omit such locations from the gazetteer , but then we wo n't be able to identify them when they do appear in a document . It gets even harder in the case of names for people or organizations . Any list of such names will probably have poor coverage . New organizations come into existence every day , so if we are trying to deal with contemporary newswire or blog entries , it is unlikely that we will be able to recognize many of the entities using gazetteer lookup . Another major source of difficulty is caused by the fact that many named entity terms are ambiguous . Thus May and North are likely to be parts of named entities for DATE and LOCATION , respectively , but could both be part of a PERSON ; conversely Christian Dior looks like a PERSON but is more likely to be of type ORGANIZATION . A term like Yankee will be ordinary modifier in some contexts , but will be marked as an entity of type ORGANIZATION in the phrase Yankee infielders . Further challenges are posed by multi - word names like Stanford University , and by names that contain other names such as Cecil H. Green Library and Escondido Village Conference Service Center . In named entity recognition , therefore , we need to be able to identify the beginning and end of multi - token sequences . Named entity recognition is a task that is well - suited to the type of classifier - based approach that we saw for noun phrase chunking . In particular , we can build a tagger that labels each word in a sentence using the IOB format , where chunks are labeled by their appropriate type . Here is part of the CONLL 2002 ( conll2002 ) Dutch training data : . Eddy N B - PER Bonte N I - PER is V O woordvoerder N O van Prep O diezelfde Pron O Hogeschool N B - ORG . Punc O . In this representation , there is one token per line , each with its part - of - speech tag and its named entity tag . Based on this training corpus , we can construct a tagger that can be used to label new sentences ; and use the nltk.chunk.conlltags2tree ( ) function to convert the tag sequences into a chunk tree . NLTK provides a classifier that has already been trained to recognize named entities , accessed with the function nltk.ne_chunk ( ) . 6 Relation Extraction . Once named entities have been identified in a text , we then want to extract the relations that exist between them . As indicated earlier , we will typically be looking for relations between specified types of named entity . One way of approaching this task is to initially look for all triples of the form ( X , \\u03b1 , Y ) , where X and Y are named entities of the required types , and \\u03b1 is the string of words that intervenes between X and Y . We can then use regular expressions to pull out just those instances of \\u03b1 that express the relation that we are looking for . The following example searches for strings that contain the word in . The special regular expression ( ? ! \\\\b.+ing\\\\b ) is a negative lookahead assertion that allows us to disregard strings such as success in supervising the transition of , where in is followed by a gerund . As shown above , the conll2002 Dutch corpus contains not just named entity annotation but also part - of - speech tags . This allows us to devise patterns that are sensitive to these tags , as shown in the next example . The method clause ( ) prints out the relations in a clausal form , where the binary relation symbol is specified as the value of parameter relsym . Note . This will show you the actual words that intervene between the two NEs and also their left and right context , within a default 10-word window . With the help of a Dutch dictionary , you might be able to figure out why the result VAN ( ' annie_lennox ' , ' eurythmics ' ) is a false hit . 7 Summary . Information extraction systems search large bodies of unrestricted text for specific types of entities and relations , and use them to populate well - organized databases . These databases can then be used to find answers for specific questions . The typical architecture for an information extraction system begins by segmenting , tokenizing , and part - of - speech tagging the text . The resulting data is then searched for specific types of entity . Finally , the information extraction system looks at entities that are mentioned near one another in the text , and tries to determine whether specific relationships hold between those entities . Entity recognition is often performed using chunkers , which segment multi - token sequences , and label them with the appropriate entity type . Common entity types include ORGANIZATION , PERSON , LOCATION , DATE , TIME , MONEY , and GPE ( geo - political entity ) . Chunkers can be constructed using rule - based systems , such as the RegexpParser class provided by NLTK ; or using machine learning techniques , such as the ConsecutiveNPChunker presented in this chapter . In either case , part - of - speech tags are often a very important feature when searching for chunks . Although chunkers are specialized to create relatively flat data structures , where no two chunks are allowed to overlap , they can be cascaded together to build nested structures . Relation extraction can be performed using either rule - based systems which typically look for specific patterns in the text that connect entities and the intervening words ; or using machine - learning systems which typically attempt to learn such patterns automatically from a training corpus . 8 Further Reading . The popularity of chunking is due in great part to pioneering work by Abney e.g. , ( Church , Young , & Bloothooft , 1996 ) . The IOB format ( or sometimes BIO Format ) was developed for NP chunking by ( Ramshaw & Marcus , 1995 ) , and was used for the shared NP bracketing task run by the Conference on Natural Language Learning ( CoNLL ) in 1999 . The same format was adopted by CoNLL 2000 for annotating a section of Wall Street Journal text as part of a shared task on NP chunking . 9 Exercises . Why are three tags necessary ? What problem would be caused if we used I and O tags exclusively ? Try to do this by generalizing the tag pattern that handled singular noun phrases . Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences that make up this kind of chunk . Develop a simple chunker using the regular expression chunker nltk . RegexpParser . Discuss any tag sequences that are difficult to chunk reliably . Develop a chunker that starts by putting the whole sentence in a single chunk , and then does the rest of its work solely by chinking . Determine which tags ( or tag sequences ) are most likely to make up chinks with the help of your own utility program . Compare the performance and simplicity of this approach relative to a chunker based entirely on chunk rules . Add these patterns to the grammar , one per line . Test your work using some tagged sentences of your own devising . ( Note that most chunking corpora contain some internal inconsistencies , such that any reasonable rule - based approach will produce errors . ) Evaluate your chunker on 100 sentences from a chunked corpus , and report the precision , recall and F - measure . Use the chunkscore.missed ( ) and chunkscore.incorrect ( ) methods to identify the errors made by your chunker . Discuss . Compare the performance of your chunker to the baseline chunker discussed in the evaluation section of this chapter . Use any combination of rules for chunking , chinking , merging or splitting . Instead of requiring manual correction of tagger output , good chunkers are able to work with the erroneous output of taggers . Look for other examples of correctly chunked noun phrases with incorrect tags . Study its errors and try to work out why it does n't get 100 % accuracy . Experiment with trigram chunking . Are you able to improve the performance any more ? Instead of assigning POS tags to words , here we will assign IOB tags to the POS tags . E.g. if the tag DT ( determiner ) often occurs at the start of a chunk , it will be tagged B ( begin ) . Evaluate the performance of these chunking methods relative to the regular expression chunking methods covered in this chapter . that it is possible to establish an upper limit to tagging performance by looking for ambiguous n - grams , n - grams that are tagged in more than one possible way in the training data . Apply the same method to determine an upper bound on the performance of an n - gram chunker . Write functions to do the following tasks for your chosen type : . List all the tag sequences that occur with each instance of this chunk type . Count the frequency of each tag sequence , and produce a ranked list in order of decreasing frequency ; each line should consist of an integer ( the frequency ) and the tag sequence . Inspect the high - frequency tag sequences . Use these as the basis for developing a better chunker . For example , the phrase : [ every / DT time / NN ] [ she / PRP ] sees / VBZ [ a / DT newspaper / NN ] contains two consecutive chunks , and our baseline chunker will incorrectly combine the first two : [ every / DT time / NN she / PRP ] . Write a program that finds which of these chunk - internal tags typically occur at the start of a chunk , then devise one or more rules that will split up these chunks . Combine these with the existing baseline chunker and re - evaluate it , to see if you have discovered an improved baseline . The format uses square brackets , and we have encountered it several times during this chapter . The Treebank corpus can be accessed using : for sent in nltk.corpus.treebank_chunk.chunked_sents(fileid ) . These are flat trees , just as we got using nltk.corpus.conll2000.chunked_sents ( ) . The functions nltk.tree.pprint ( ) and nltk.chunk.tree2conllstr ( ) can be used to create Treebank and IOB strings from a tree . Write functions chunk2brackets ( ) and chunk2iob ( ) that take a single chunk tree as their sole argument , and return the required multi - line string representation . Write command - line conversion utilities bracket2iob.py and iob2bracket.py that take a file in Treebank or CoNLL format ( resp ) and convert it to the other format . ( Obtain some raw Treebank or CoNLL data from the NLTK Corpora , save it to a file , and then use for line in open(filename ) to access it from Python . ) Investigate other models of the context , such as the n-1 previous part - of - speech tags , or some combination of previous chunk tags along with previous and following part - of - speech tags . Now observe how a chunker may re - use this sequence information . For example , both tasks will make use of the information that nouns tend to follow adjectives ( in English ) . It would appear that the same information is being maintained in two places . Is this likely to become a problem as the size of the rule sets grows ? If so , speculate about any ways that this problem might be addressed . \"}",
        "_version_":1692668359994245120,
        "score":23.277803},
      {
        "id":"560e64be-5ac2-453a-8a5f-b32915350091",
        "_src_":"{\"url\": \"http://www.russianmachineneverbreaks.com/2011/04/22/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148558.5/warc/CC-MAIN-20160205193908-00006-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Submissions . Submissions should describe original , unpublished work . Papers presented at EMNLP - CoNLL should mainly contain new material that has not been presented at any other meeting with publicly available proceedings . Papers that have been or will be submitted to other meetings or publications must disclose this information at submission time . Please list all other meetings where the paper has been submitted in the \\\" other submissions \\\" field on the submission site . Each submission consists of a paper of up to ten ( 10 ) pages of content and any number of additional pages containing references only , together with optional supplementary material as described below . The paper must conform to the official ACL 2012 style guidelines . Please use the official ACL 2012 style files for the paper . We reserve the right to reject submissions if the paper does not conform to these styles , including letter size and font size restrictions . Since reviewing will be blind , the submission should not include the authors ' names and affiliations . Furthermore , self - references that reveal the author 's identity , e.g. , \\\" We previously showed ( Smith , 1991 ) ... \\\" , should be avoided . Instead , use citations such as \\\" Smith ( 1991 ) previously showed ... \\\" . Submissions that do not conform to these requirements will be rejected without review . Separate author identification information is required as part of the on - line submission process . The supplementary material should be supplementary ( rather than central ) to the paper . It may include explanations or details of proofs or derivations that do not fit into the paper , lists of features or feature templates , sample inputs and outputs for a system , pseudo - code or source code , and data . Any information that is critical to understanding the paper should be included within the paper itself . While the paper may refer to and cite the supplementary material and the supplementary material will be available to reviewers , they will not be asked to review or even download the supplementary material . Submission and reviewing will be on - line , managed by the START system . The only accepted format for submitted papers is PDF . The supplementary material must be in the form of a single . zip or a . tgz archive file with a maximum size of 10 MB ; otherwise there are no constraints on its format . Submissions , together with all supplementary material , must be uploaded onto the START system by the submission deadlines ; submissions submitted after that time will not be reviewed . To minimize network congestion we request authors upload their submissions as early as possible ( especially if they contain large supplementary material files ) . Authors of accepted submissions are to produce a final paper to be published in the proceedings of the conference . Final papers must follow the same style and length requirements as submitted papers . If an accepted submission has also been submitted elsewhere , the contact author must promptly inform the program co - chairs whether he or she intends the accepted paper to be included in EMNLP - CoNLL , and if so must withdraw the paper from other meetings or publications . At least one author must register for the conference and present the paper . Papers will be presented orally or as a poster presentation , as determined by the program committee . The decisions as to which papers will be presented orally and which as poster presentations will be based on the nature rather than on the quality of the work , and there will be no distinction between them in the proceedings . As in previous CoNLL conferences , EMNLP - CoNLL will include a shared task which is organized by a separate committee . The details of the shared task are included below . Please note the different submission details and deadlines for the shared task . Shared Task . EMNLP - CoNLL 2012 will continue the CoNLL tradition of having the highest profile shared task in the NLP world . This year 's task will be automatic anaphoric mention detection and coreference resolution using the English , Chinese and Arabic language portions of the OntoNotes 5.0 data given predicted information on the other layers . In order to receive future calls and other information about the shared task , participants should register their intent to participate , in either or both of the two tracks , by sending an e - mail to conll-2012-st@cemantix.org . Although the deadline for registration is not until January 22 , 2012 , we recommend participants to register as early as possible , in order not to miss any information . \"}",
        "_version_":1692669749877538818,
        "score":21.542141},
      {
        "id":"e14ac15f-4279-4085-bf16-d6e55eb964e7",
        "_src_":"{\"url\": \"http://newint.org/columns/essays/2009/09/01/why-children-work/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163512.72/warc/CC-MAIN-20160205193923-00210-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"One of those unable to attend was Dr. Jonas Persson of Stockholm University . He was scheduled to speak in the final symposium of the conference , which was on control of executive control , or who controls the \\\" controller \\\" in the brain ( without resorting to a homunculus or an infinite regress of Mini - Me ' s ) . His co - author , Dr. Patricia Reuter - Lorenz of the University of Michigan , gave an interesting presentation about their work on the ups and downs of cognitive training : gains that transfer to other tasks across sessions and fatigue that transfers to other tasks within a session . The abstract is reprinted below . --------------- Symposium Session 5 Tuesday , April 20 , 1:00 - 3:00 pm , Westmount et al Ballroom What Controls Executive Control ? We tested this hypothesis by measuring behavioral interactions between memory tasks presumed to require interference control - a putative executive process that mediates selection from competing representations . Behavioral data show that different training regimens produce either negative or positive transfer from working memory to semantic and episodic memory task performance . We show that eight days of training on high interference versions of three different working memory tasks increased the efficiency of interference control on the training tasks and on untrained tasks in new memory domains . In contrast we have also demonstrated negative transfer and process - specific \\\" fatigue \\\" effects indicating that control efficiency in a second task is diminished by high control demands in a prior task immediately preceding it in time . This suggests that interference control is a finite resource that can be temporarily depleted . Functional magnetic resonance imaging ( fMRI ) was used to elucidate the mechanisms associated with decreasing efficiency or resource depletion of the interference control process . Along with reduced performance , fMRI indicates negative transfer is associated with reduced process - specific activation , and increased homologous activation that may be compensatory . In sum , this suggests that interference control is an executive function that is both resource limited and plastic making it possible for training to alter its efficiency . These findings became extremely relevant ( albeit ignored ) in light of the ultra - high impact paper by Adrian Owen et al . ( 2010 ) published in BBC / Nature on April 20 , informing us there was \\\" No gain from brain training \\\" in a massive group of 11,430 volunteers . The research volunteers participated in online training exercises that tapped ( 1 ) reasoning , planning , problem solving ; ( 2 ) short - term memory , attention , visuospatial processing , mathematics ; or ( 3 ) ability to answer obscure questions using the internet . Numerous other outlets have already summarized these results , so I wo n't attempt to do so here . The main issue was whether the gains obtained through simple practice effects transferred to tasks that were n't in the training set . The short answer is no . I am not a proponent of brain training games that make unsound claims lacking credible scientific evidence to back them up . But in light of the spectacularly negative findings of Owen et al . , the question arises of whether their training regimen ( 10 min 3 times a week for 6 weeks ) was adequate to produce significant effects . Predictably , those with a stake in the matter said no , the training was neither sufficient in duration nor applicable to other commercially available products . A final issue concerns the population under study , a presumably \\\" normal \\\" group of 18 - 60 year olds without cognitive impairments or cognitive decline . Would the same training exercises help an older population at risk for dementia ? On the other hand , seniors might be especially vulnerable to persuasive false claims from unscrupulous brain training vendors . Thus it 's important to have scientifically valid and accessible research on whether cognitive training might help an aging population ( Lustig et al . , 2009 ) . Coincidentally , the NIH is currently trying to reach a consensus on : . Live webcast , also information on archived video and publicly available consensus statement . Let us now return to the studies of Persson and Reuter Lorenz ( 2008 ) . This work examined whether the training of a specific cognitive process ( overcoming interference from competing representations in memory ) in one task will improve the ability to overcome interference in a different task . For example , a set of four letters would be presented in a working memory task , followed by a delay and then a probe letter , which requires a yes / no response ( was this letter a part of the set you just studied ? ) The interference arises when participants must reject a probe letter that was a member of the memory set on the preceding trial , but is not a member of the set on the current trial ( \\\" Recent Negative \\\" , illustrated below ) . Figure 1 ( D'Esposito et al . , 1999 ) . Target items were presented for 950 msec followed by a 7,050-msec delay period . This interval was followed by a probe letter for 1,500 msec that either was recently presented ( Recent trial ) or was not recently presented ( Nonrecent trial ) . Note that , in the Recent Negative trial , the probe letter \\\" P \\\" is not in the target set of that trial but that it is in the target set of the two previous trials . Training consisted of eight 40 min sessions over a two week period . Benchmark tests were administered on the days before and after the training program to evaluate interference resolution performance on non - trained tasks of working memory ( item recognition with concrete nouns ) , semantic memory ( verb generation ) , and episodic memory ( paired associates ) . The experimental design is shown below . Fig . 1 ( Persson & Reuter - Lorenz , 2008 ) . Schematic depiction of the basic experimental design . The illustration shows the tasks performed by each subject group during each phase of the experiment : pretraining ( Session 1 ) , training ( Sessions 2 - 9 ) , and posttraining ( Session 10 ) . All groups performed the same three transfer tasks in the pretraining and posttraining sessions . During training , the interference - training group performed tasks involving working memory ( WM ) and interference ; Control Group 1 performed noninterference versions of the same tasks , and Control Group 2 performed similar tasks that required little WM and involved no interference . Results indicated that Interference Training reduced reaction time ( RT ) measures of interference in each of the three transfer tasks ( left bars ) , but control training did not ( right bars ) . Fig . 3 . ( Persson & Reuter - Lorenz , 2008 ) . Mean interference - resolution scores ( response time in the high - interference condition minus response time in the low - interference condition ) on the transfer tasks as a function of group ( interference - training group vs. control groups ) and time ( pretraining vs. posttraining ) . Results are shown separately for the ( a ) verb - generation task ( semantic memory ) , ( b ) item - recognition task ( working memory ) , and ( c ) paired - associates task ( episodic memory ) . Error bars show standard errors of the means . A previous study demonstrated the opposite sort of process - specific interactions , in the form of within - session fatigue effects ( i.e. , negative transfer ) across a subset of these tasks ( Persson et al . , 2007 ): . The idea that tasks with overlapping neural representations may involve similar executive components was ... critical to our approach . Of particular interest were tasks requiring resolution of interference among competing representations . Within a single experimental session intensive training reduced the ability to resolve interference on a transfer task if the training task placed high demands on interference resolution . Negative transfer was absent when interference resolution was minimally required by the task , or when the training and transfer tasks did not rely on overlapping neural representations . Based on these results , the authors concluded that targeted training -- which identifies and trains a specific cognitive process that is implemented by a known network in the brain -- can produce improvements ( across sessions ) that transfer to tasks other than those in the training set . Conversely , fatigue effects ( within session ) can result in negative transfer to related tasks . What does all this mean for the brain training biz and for the BBC / Nature study ? Adequate training time and a brain - based approach to target highly specific cognitive processes and tasks will yield the best results . ADDENDUM ( April 10 , 2011 ) : The 2008 Psychological Science paper by Persson and Reuter - Lorenz has been retracted : . This article has been retracted by both of the authors . During further extension of this work , it was discovered that a mistake had been made in the programming of the working memory training tasks used in this study . This error resulted in the presentation of repeated trial sequences , which were then practiced repeatedly over the 8-day training period . This practice likely resulted in learning of the sequences , rather than training of interference control as we had originally inferred . The first author takes responsibility for this error , and both authors regret the publication of invalid results . Persson , J. , & Reuter - Lorenz , P. ( 2008 ) . Gaining Control : Training Executive Function and Far Transfer of the Ability to Resolve Interference . Psychological Science 19 : 881 - 888 . DOI : 10.1111/j.1467 - 9280.2008.02172.x . PERSSON , J. , WELSH , K. , JONIDES , J. , & REUTER - LORENZ , P. ( 2007 ) . Cognitive fatigue of executive processes : Interaction between interference resolution tasks . Neuropsychologia 45 : 1571 - 1579 . DOI : 10.1016/j.neuropsychologia.2006.12.007 . 4 Comments : . The Nature / BBC article is very relevant for the specific tasks and training regime they used . There are issues about subject selection etc , but with such a large sample , I would be surprised if these issues were critical . Many companies sell snake oil , let 's face it . What 's new ? However , obviously , the study does not generalize to ALL training types in ALL domains . There is no contradiction , unless someone uses exactly the same tasks and training regime they used and found an effect . I have read dozens of articles about the Brain Test Britain debacle ( personal opinion ) but yours is the first on the new Jonides research . Truly a fascinating study . Jonides was also involved with the dual n - back working memory training that showed transfer to fluid intelligence back in 2008 . As you point out , researchers such as Dr. Jonides are developing a sophisticated understanding of brain function and brain training . Good to see . About Me . Born in West Virginia in 1980 , The Neurocritic embarked upon a roadtrip across America at the age of thirteen with his mother . She abandoned him when they reached San Francisco and The Neurocritic descended into a spiral of drug abuse and prostitution . At fifteen , The Neurocritic 's psychiatrist encouraged him to start writing as a form of therapy . \"}",
        "_version_":1692669076485177344,
        "score":19.511477},
      {
        "id":"2d5eac15-ae1f-4bf3-b390-d537404e7bd6",
        "_src_":"{\"url\": \"http://www.eol.org/data_objects/11389139\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701159654.65/warc/CC-MAIN-20160205193919-00197-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"2012 - 10 - 08 : Proceedings are online in ACM DL 2012 - 09 - 10 : Slides uploaded . 2012 - 09 - 06 : Slides of Kaushik Chakrabarti 's Keynote are online . 2012 - 08 - 24 : Slides of John Shafer 's Keynote are online . 2012 - 08 - 16 : Best Contribution Award goes to the Team of the University of Freiburg for their presentation and paper ! 2012 - 08 - 16 : Workshop is today ! 2012 - 08 - 13 : Papers online ! 2012 - 08 - 01 : Program online ! 2012 - 07 - 30 : List of accepted papers online ! 2012 - 07 - 13 : John Shafer will give a keynote ! 2012 - 07 - 06 : Kaushik Chakrabarti will give a keynote ! 2012 - 07 - 01 : Paper deadline extended to July 9th . 2012 - 05 - 29 : Workshop Web page is ready ! The workshop encompasses various tasks and approaches that go beyond the traditional bag - of - words paradigm and incorporate an explicit representation of the semantics behind information needs and relevant content . This kind of semantic search , based on concepts , entities and relations between them , has attracted attention both from industry and from the research community . The workshop aims to bring people from different communities ( IR , SW , DB , NLP , HCI , etc . ) and backgrounds ( both academics and industry practitioners ) together , to identify and discuss emerging trends , tasks and challenges . This joint workshop is a sequel of the Entity - oriented Search and Semantic Search Workshop series held at different conferences in previous years . The workshop aims to gather all works that discuss entities along three dimensions : tasks , data and interaction . Tasks include entity search ( search for entities or documents representing entities ) , relation search ( search entities related to an entity ) , as well as more complex tasks ( involving multiple entities , spatio - temporal relations inclusive , involving multiple queries ) . In the data dimension , we consider ( web / enterprise ) documents ( possibly annotated with entities / relations ) , Linked Open Data ( LOD ) , as well as user generated content . The interaction dimension gives room for research into user interaction with entities , also considering how to display results , as well as whether to aggregate over multiple entities to construct entity profiles . The workshop especially encourages submissions on the interface of IR and other disciplines , such as the Semantic Web , Databases , Computational Linguistics , Data Mining , Machine Learning , or Human Computer Interaction . Examples of topic of interest include ( but are not limited to ) : . Simple Models , Lots of Data : Mining semantics about entities using Web - Scale Data . Abstract : Many areas in computer science like machine translation , speech recognition and computer vision are becoming more data - driven : statistical techniques that use simple models and use lots of data trump approaches that use complex models , deep algorithms or hand - coded rules . I believe that this is also true for mining semantics about entities . I will give some examples of such tasks like mining alternate names ( aka \\\" synonyms \\\" ) of entities , finding descriptive phrases about entities , extracting semantic mentions of entities in documents and understanding attributes of entities and performing entity augmentation . I will discuss how we have used Web - scale data and simple , unsupervised algorithms to achieve high accuracy in these semantic tasks . This leads to several interesting research questions in statistical semantics and big data management . The Lincoln Project : Building a Web - Scale Semantic Search Engine . Abstract : All too frequently , entity search on the web is dismissed as needing nothing more than a tweaked version of an information - retrieval system , where entities are treated as documents and semantic search means bolting a few filter controls onto the side . To truly bring semantics to entity search requires an end - to - end upheaval of the entire search engine stack , from how rich structured catalogs are built and indexed , to the online query - processing system and user interface . I will speak about our experience in building from the ground up , Lincoln , a web - scale semantic search engine using a data feed of nearly 30 million products . I will present details of the entire system architecture with focus on the inner workings of the query - processing components . I will also touch upon back - end catalog creation , as well as our more recent efforts to extend the work beyond the world of entities and into the Wild Wild Web . All submissions will be reviewed by at least two program committee members , and will be assessed based on their novelty , technical quality , potential impact , and clarity of writing . Selection uses a standard double blind procedure . All accepted papers will be published as part of the SIGIR workshop proceedings and will be indexed in the ACM Digital Library . \"}",
        "_version_":1692669870946123777,
        "score":19.30472},
      {
        "id":"0bcf8e70-461a-4496-905b-2231a7007810",
        "_src_":"{\"url\": \"http://scholar.lib.vt.edu/VA-news/WDBJ-7/script_archives/02/0502/051202/051202.m.htm\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701145578.23/warc/CC-MAIN-20160205193905-00137-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"99 Wikipedia Sources Aiding the Semantic Web . Most Comprehensive Reference List Available Shows Impressive Depth , Breadth . Since about 2005 - and at an accelerating pace - Wikipedia has emerged as the leading online knowledge base for conducting semantic Web and related research . The system is being tapped for both data and structure . Wikipedia has arguably replaced WordNet as the leading lexicon for concepts and relations . Because of its scope and popularity , many argue that Wikipedia is emerging as the de facto structure for classifying and organizing knowledge in the 21st century . Our work on the UMBEL lightweight reference subject concept structure has stated since the project 's announcement in July 2007 that Wikipedia is a key intended resource for identifying subject concepts and entities . For the past few months I have been scouring the globe attempting to find every drop of research I could find on the use of Wikipedia for semantic Web , information extraction , categorization and related issues . Thus , I 'm pleased to offer up herein the most comprehensive such listing available anywhere : more than 99 resources and counting ! ( I say \\\" more than \\\" because some entries below have multiple resources ; I just liked the sound of 99 as a round number ! ) Wikipedia itself maintains a listing of academic studies using Wikipedia as a resource ; fewer than one - third of the listings below are on that list ( which itself may be an indication of the current state of completeness within Wikipedia ) . Some bloggers and other sources around the Web also maintain listings in lesser degrees of completeness . Download access to the full knowledge base has enabled the development of notable core references to the Linked Data aspects of the semantic Web such as DBpedia [ 5,6 ] and YAGO [ 72,73]. Entire research teams , such as Ponzetto and Strube [ 61 - 65 ] ( and others as well ; see below ) are moving toward creating a full - blown ontologies or structured knowledge bases useful for semantic Web purposes based on Wikipedia . So , one of the first and principle uses of Wikipedia to date has been as a data source of concepts , entities and relations . But much broader data mining and text mining and analysis is being conducted against Wikipedia , that is currently defining the state - of - the - art in these areas , too : . Ontology development and categorization . Word sense disambiguation . Named entity recognition . Named entity disambiguation . Semantic relatedness and relations . These objectives , in turn , are mining and extracting these various kinds of structure for these purposes in Wikipedia : . Articles . First paragraph - Definitions . Full text - Description of meaning ; related terms ; translations . Redirects - Synonymy ; spelling variations , misspellings ; abbreviations . Title - Named entities ; domain specific terms or senses . Subject - Category suggestion ( phrase marked in bold or in first paragraph ) . Category - Category suggestion . Contained articles - Semantically related terms ( siblings ) . Hierarchy - Hyponymic and meronymic relations between terms . Disambiguation pages . Article links - Sense inventory . Infobox Templates . Name - . Item - Category suggestion ; entity suggestion . Lists . Hyponyms . These are some of the specific uses that are included in the 99 resources listed below . This is an exciting ( and , for most all of us just a few years back , unanticipated ) use of the Web in socially relevant and contextual knowledge and research . I 'm sure such a listing one year by now will be double in size or larger ! BTW , suggestions for new or overlooked entries are very much welcomed ! S\\u00f6ren Auer , Chris Bizer , Jens Lehmann , Georgi Kobilarov , Richard Cyganiak and Zachary Ives , 2007 . DBpedia : A nucleus for a web of open data , in Proceedings of the 6th International Semantic Web Conference and 2nd Asian Semantic Web Conference ( ISWC / ASWC2007 ) , Busan , South Korea , volume 4825 of LNCS , pages 715 - 728 , November 2007 . S\\u00f6ren Auer and Jens Lehmann , 2007 . What Have Innsbruck and Leipzig in Common ? Extracting Semantics from Wiki Content , in The Semantic Web : Research and Applications , pages 503 - 517 , 2007 . Somnath Banerjee , Krishnan Ramanathan , Ajay Gupta , 2007 . Clustering Short Texts using Wikipedia , poster presented at Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval , Amsterdam , The Netherlands , pp . 787 - 788 . F. Bellomi and R. Bonato , 2005 . Lexical Authorities in an Encyclopedic Corpus : A Case Study with Wikipedia , online reference not found . Maria Ruiz - Casado , Enrique Alfonseca and Pablo Castells , 2007 . Automatising the Learning of Lexical Patterns : an Application to the Enrichment of WordNet by Extracting Semantic Relationships from Wikipedia . Silviu Cucerzan , 2007 . Large - Scale Named Entity Disambiguation Based on Wikipedia Data , in Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) . Cyclopedia , from the Cyc Foundation , an online version of Wikipedia that enables browsing the encyclopedia by concepts . A. Herbelot and Ann Copestake , 2006 . Acquiring Ontological Relationships from Wikipedia Using RMRS , in Proc . International Semantic Web Conference 2006 Workshop , Web Content Mining with Human Language Technologies , Athens , GA , 2006 . Ryuichiro Higashinaka , Kohji Dohsaka and Hideki Isozaki , 2007 . Todd Holloway , Miran Bozicevic , and Katy B\\u00c3\\u00b6rner . Analyzing and Visualizing the Semantic Coverage of Wikipedia and Its Authors . ArXiv Computer Science e - prints , cs/0512085 . Wei Che Huang , Andrew Trotman , and Shlomo Geva , 2007 . Collaborative Knowledge Management : Evaluation of Automated Link Discovery in the Wikipedia , in SIGIR 2007 Workshop on Focused Retrieval , July 27 , 2007 , Amsterdam , The Netherlands . Gjergji Kasneci , Fabian M. Suchanek , Georgiana Ifrim , Maya Ramanath and Gerhard Weikum , 2007 . Jun'ichi Kazama and Kentaro Torisawa , 2007 . Exploiting Wikipedia as External Knowledge for Named Entity Recognition , in Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , pp . 698 - 707 , Prague , June 2007 . A. Krizhanovsky , 2006 . Synonym Search in Wikipedia : Synarcher , in 11th International Conference \\\" Speech and Computer \\\" SPECOM'2006 . Russia , St. Petersburg , June 25 - 29 , 2006 , pp . 474 - 477 . Lev Muchnik , Royi Itzhack , Sorin Solomon and Yoram Louzoun , 2007 . Self - emergence of Knowledge Trees : Extraction of the Wikipedia Hierarchies , in Physical Review E ( Statistical , Nonlinear , and Soft Matter Physics ) , Vol . 76 , No . 1 . Nadeau , D. , Turney , P. , Matwin , S. , 2006 . Unsupervised Named - Entity Recognition : Generating Gazetteers and Resolving Ambiguity , at 19th Canadian Conference on Artificial Intelligence . Qu\\u00e9bec City , Qu\\u00e9bec , Canada . June 7 , 2006 . Does n't specifically use Wikipedia , but techniques are applicable . Kotaro Nakayama , Takahiro Hara and Shojiro Nishio , 2007 . Wikipedia Mining for an Association Web Thesaurus Construction , in Web Information Systems Engineering - WISE 2007 , Vol . 4831 ( 2007 ) , pp . 322 - 334 . Simone Paolo Ponzetto and Michael Strube , 2007c . S. Suh , H. Halpin and E. Klein , 2006 . Extracting Common Sense Knowledge from Wikipedia , in Proc . International Semantic Web Conference 2006 Workshop , Web Content Mining with Human Language Technologies , Athens , GA , 2006 . Anne - Marie Vercoustre , Jovan Pehcevski and James A. Thom , 2007 . Using Wikipedia Categories and Links in Entity Ranking , in Pre - proceedings of the Sixth International Workshop of the Initiative for the Evaluation of XML Retrieval ( INEX 2007 ) , Dec 17 , 2007 . Denny Vrandecic , Markus Kr\\u00f6tzsch and Max V\\u00f6lkel , 2007 . Wikipedia and the Semantic Web , Part II , in Phoebe Ayers and Nicholas Boalch , Proceedings of Wikimania 2006 - The Second International Wikimedia Conference , Wikimedia Foundation , Cambridge , MA , USA , August 2007 . W - Z . Wang Y , Wang H , Zhu H , Yu Y , 2007 . Exploit Semantic Information for Category Annotation Recommendation in Wikipedia , in Natural Language Processing and Information Systems ( 2007 ) , pp . 48 - 60 . Hugo Zaragoza , Henning Rode , Peter Mika , Jordi Atserias , Massimiliano Ciaramita & Giuseppe Attardi , 2007 . Ranking Very Many Typed Entities on Wikipedia , in CIKM ' 07 : Proceedings of the Sixteenth ACM International Conference on Information and Knowledge Management . Schema.org Markup . Most Comprehensive Reference List Available Shows Impressive Depth , Breadth Since about 2005 - and at an accelerating pace - Wikipedia has emerged as the leading online knowledge base for conducting semantic Web and related research . The system is being tapped for both data and structure . Wikipedia has arguably replaced WordNet as the leading lexicon for [ ... ] . 12 thoughts on \\\" 99 Wikipedia Sources Aiding the Semantic Web \\\" . Hi , I would like to suggest one of our projects to make it a three digit number ! Bests , . Sebastian Blohm , Philipp Cimiano , Using the Web to Reduce Data Sparseness in Pattern - based Information Extraction , in Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases ( PKDD ) , pp . 18 - 29 . Springer , Warsaw , Poland , September 2007 . As usual , technical papers quickly become outdated . I would suggest replacing [ 82 ] with the more recent updated journal version : . Markus Kr\\u00c3\\u00b6tzsch , Denny Vrandecic , Max V\\u00c3\\u00b6lkel , Heiko Haller , Rudi Studer . Semantic Wikipedia . Elsevier 2007 . I 'm one of the authors . What you done is quite helpful work for other Wikipedia researchers including me . Thank you very much . By the way , our system named \\\" Wikipedia Thesaurus , \\\" a huge scale association thesaurus constructed by mining Wikipedia , is available on the WWW to prove the capability of Wikipedia Mining . ( The method is described in paper # 57 ) . Thank you for this great bibliography ! I have collected quite a few papers about wikipedia myself ( here ) , for my master 's thesis ( see here ) . Have a look if you like - there 's quite some overlap , and the top - most 20 are the ones i imported from your list just now , so scroll down a bit . By the way : it would be nice to have your list in a machine - readable form , like bibtex , so entries can easily be transferred into places like citeulike or bibsonomy . So you are brightbyte ; I have seen the reference many times before . Thank you for the compliment . I agree with your idea about machine readable form . In fact , I have been playing around at times with things like Zotero , Connotea and the ones you mention . What I am really looking for is an easy online place to put the thousands of references I track ( the Wikipedia is only one example ) , but gives me easy re - formatting , etc . Initial finds I keep in an internal wiki , but structured entry is not the easiest . I probably already have encountered the solution , but have not taken it the last step . Any ideas ? First , let me clarify that my thesis is work in progress . More work than progress , lately ... I probably should n't be posting on the web right now . Anyway , I have myself been frustrated quite a bit about the state of the art of online bibliography systems . After ranting about it , some interresting discussion developed , involving , among others , Jakob Voss ( who is not only one of the authors on your list , but also a professional bibliographer , involved with Wikipedia Deutschalnd and , it seems , with Zotero ) . Have a look and join in ( account available on request but not needed for comments ) . With WikiXMLDB demo you can run predefined or your own XQuery queries via Web interface . \"}",
        "_version_":1692670487553900544,
        "score":18.357418},
      {
        "id":"70fb47ba-960d-44e3-bdd6-c28eb76753de",
        "_src_":"{\"url\": \"http://www.thebrewsite.com/next-session-thanks-to-the-big-boys/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156448.92/warc/CC-MAIN-20160205193916-00286-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"class nltk.corpus.reader.aligned . class nltk.corpus.reader.api . CategorizedCorpusReader ( kwargs ) [ source ] \\u00b6 . Bases : object . A mixin class used to aid in the implementation of corpus readers for categorized corpora . This class defines the method categories ( ) , which returns a list of the categories for the corpus or for a specified set of fileids ; and overrides fileids ( ) to take a categories argument , restricting the set of fileids to be returned . Call _ _ init _ _ ( ) to set up the mapping . Override all view methods to accept a categories parameter , which can be used instead of the fileids parameter , to select which fileids should be included in the returned view . Return a list of file identifiers for the files that make up this corpus , or that make up the given category(s ) if specified . class nltk.corpus.reader.api . Bases : object . A base class for \\\" corpus reader \\\" classes , each of which can be used to read a specific corpus format . Each individual corpus reader instance is used to read a specific corpus , consisting of one or more files under a common root directory . Each file is identified by its file identifier , which is the relative path to the file from the root directory . A separate subclass is be defined for each corpus format . These subclasses define one or more methods that provide ' views ' on the corpus contents , such as words ( ) ( for a list of words ) and parsed_sents ( ) ( for a list of parsed sentences ) . Called with no arguments , these methods will return the contents of the entire corpus . For most corpora , these methods define one or more selection arguments , such as fileids or categories , which can be used to select which portion of the corpus should be returned . fileids ( None or str or list ) - Specifies the set of fileids for which paths should be returned . Can be None , for all fileids ; a list of file identifiers , for a specified set of fileids ; or a single file identifier , for a single file . Note that the return value is always a list of paths , even if fileids is a single file identifier . include_encoding - If true , then return a list of ( path_pointer , encoding ) tuples . Load this corpus ( if it has not already been loaded ) . This is used by LazyCorpusLoader as a simple method that can be used to make sure a corpus is loaded - e.g. , in case a user wants to do help(some_corpus ) . Reader for the Alpino Dutch Treebank . This corpus has a lexical breakdown structure embedded , as read by _ parse Unfortunately this puts punctuation and some other words out of the sentence order in the xml element tree . This is no good for tag _ and word _ _ tag and _ word will be overridden to use a non - default new parameter ' ordered ' to the overridden _ normalize function . The _ parse function can then remain untouched . class nltk.corpus.reader.bracket_parse . Bo Pang and Lillian Lee . \\\" Seeing stars : Exploiting class relationships for . sentiment categorization with respect to rating scales \\\" . Proceedings of the ACL , 2005 . class nltk.corpus.reader.categorized_sents . A reader for corpora in which each row represents a single instance , mainly a sentence . Istances are divided into categories based on their file identifiers ( see CategorizedCorpusReader ) . Since many corpora allow rows that contain more than one sentence , it is possible to specify a sentence tokenizer to retrieve all sentences instead than all rows . Examples using the Subjectivity Dataset : . Examples using the Sentence Polarity Dataset : . sents ( ) [ [ ' simplistic ' , ' , ' , ' silly ' , ' and ' , ' tedious ' , ' . ' ] categories ( ) [ ' neg ' , ' pos ' ] . Corpus reader for the XML version of the CHILDES corpus . Copy the needed parts of the CHILDES XML corpus into the NLTK data directory ( nltk_data / corpora / CHILDES/ ) . For access to the file text use the usual nltk functions , words ( ) , sents ( ) , tagged_words ( ) and tagged_sents ( ) . the given file(s ) as a list of sentences or utterances , each encoded as a list of word strings . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of sentences , each encoded as a list of ( word , tag ) tuples . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of tagged words and punctuation symbols , encoded as tuples ( word , tag ) . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . Map a corpus file to its web version on the CHILDES website , and open it in a web browser . The complete URL to be used is : . If no urlbase is passed , we try to calculate it . This requires that the childes corpus was set up to mirror the folder hierarchy under childes.psy.cmu.edu/data-xml/ , e.g. : nltk_data / corpora / childes / Eng - USA / Cornell/ ? ? ? or nltk_data / corpora / childes / Romance / Spanish / Aguirre/ ? ? ? The function first looks ( as a special case ) if \\\" Eng - USA \\\" is on the path consisting of + fileid ; then if \\\" childes \\\" , possibly followed by \\\" data - xml \\\" , appears . If neither one is found , we use the unmodified fileid and hope for the best . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . A reader for corpora that contain chunked ( and optionally tagged ) documents . class nltk.corpus.reader.chunked . Reader for chunked ( and optionally tagged ) corpora . Paragraphs are split using a block reader . They are then tokenized into sentences using a sentence tokenizer . Finally , these sentences are parsed into chunk trees using a string - to - chunktree conversion function . Each of these steps can be performed using a default function or a custom function . By default , paragraphs are split on blank lines ; sentences are listed one per line ; and sentences are parsed into chunk trees using nltk.chunk.tagstr2tree . the given file(s ) as a list of paragraphs , each encoded as a list of sentences , which are in turn encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of sentences , each encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of tagged words and chunks . Words are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . Chunks are encoded as depth - one trees over ( word , tag ) tuples or word strings . class nltk.corpus.reader.chunked . File Format : Each line consists of an uppercased word , a counter ( for alternative pronunciations ) , and a transcription . E.g. : NATURAL 1 N AE1 CH ER0 AH0 L . The dictionary contains 127069 entries . Of these , 119400 words are assigned a unique pronunciation , 6830 words have two pronunciations , and 839 words have three or more pronunciations . Many of these are fast - speech variants . Phonemes : There are 39 phonemes , as shown below : . class nltk.corpus.reader.cmudict . Nitin Jindal and Bing Liu . \\\" Identifying Comparative Sentences in Text Documents \\\" . Proceedings of the ACM SIGIR International Conference on Information Retrieval ( SIGIR-06 ) , 2006 . Nitin Jindal and Bing Liu . \\\" Mining Comprative Sentences and Relations \\\" . Proceedings of Twenty First National Conference on Artificial Intelligence ( AAAI-2006 ) , 2006 . Murthy Ganapathibhotla and Bing Liu . \\\" Mining Opinions in Comparative Sentences \\\" . Proceedings of the 22nd International Conference on Computational Linguistics ( Coling-2008 ) , Manchester , 18 - 22 August , 2008 . feature , comparison . comparisons ( ) ) 853 . A ConllCorpusReader whose data file contains three columns : words , pos , and chunk . class nltk.corpus.reader.conll . A corpus reader for CoNLL - style files . These files consist of a series of sentences , separated by blank lines . Each sentence is encoded using a table ( or \\\" grid \\\" ) of values , where each line corresponds to a single word , and each column corresponds to an annotation type . The set of columns used by CoNLL - style files can vary from corpus to corpus ; the ConllCorpusReader constructor therefore takes an argument , columntypes , which is used to specify the columns that are used by a given corpus . @todo : Possibly add caching of the grid corpus view ? This would . allow the same grid view to be used by different data access methods ( eg words ( ) and parsed_sents ( ) could both share the same grid corpus view object ) . @todo : Better support for -DOCSTART- . Currently , we just ignore . it , but it could be used to define methods that retrieve a document at a time ( eg parsed_documents ( ) ) . A list of ( argspan , argid ) tuples , specifying the location and type for each of the arguments identified by this instance . argspan is a tuple start , end , indicating that the argument consists of the words[start : end ] . class nltk.corpus.reader.dependency . lu ( 3238 ) . frame . lexUnit [ ' glint.v ' ] is fn . frame_by_name ( ' Replacing ' ) is fn . lus ( ' replace.v ' ) [ 0 ] . lus ( ' prejudice.n ' ) [ 0 ] . frame . frame_relations ( ' Partiality ' ) True . Details for a specific annotated document can be obtained using this class 's annotated_document ( ) function and pass it the value of the ' ID ' field . corpname for x in fn . name ( str ) - A regular expression pattern used to search the file name of each annotated document . The document 's file name contains the name of the corpus that the document is from , followed by two underscores \\\" _ _ \\\" followed by the document name . So , for example , the file name \\\" LUCorpus - v0.3__20000410_nyt - NEW . xml \\\" is from the corpus named \\\" LUCorpus - v0.3 \\\" and the document name is \\\" 20000410_nyt - NEW . xml \\\" . Lists frame element objects . If ' name ' is provided , this is treated as a case - insensitive regular expression to filter by frame name . ( Case - insensitivity is because casing of frame element names is not always consistent across frames . ) frame . name , fe . name ) for fe in fn . name for fe in fn . fes ( ' ^sound$ ' ) ) 2 . Get the details for the specified Frame using the frame 's name or i d number . Usage examples : . frame ( ' Imposing_obligation ' ) frame ( 1494 ) : Imposing_obligation ... . The dict that is returned from this function will contain the following information about the Frame : . ' name ' : the name of the Frame ( e.g. ' Birth ' , ' Apply_heat ' , etc . ) . ' definition ' : textual definition of the Frame . ' ID ' : the internal ID number of the Frame . ' semTypes ' : a list of semantic types for this frame . Each item in the list is a dict containing the following keys : . ' name ' : can be used with the semtype ( ) function . ' ID ' : can be used with the semtype ( ) function . ' lexUnit ' : a dict containing all of the LUs for this frame . The keys in this dict are the names of the LUs and the value for each key is itself a dict containing info about the LU ( see the lu ( ) function for more info . ) FE ' : a dict containing the Frame Elements that are part of this frame . The keys in this dict are the names of the FEs ( e.g. ' Body_system ' ) and the values are dicts containing the following keys . definition \\\" This frame includes words that name ... \\\" . Also see the frame ( ) function for details about what is contained in the dict that is returned . Get the details for the specified Frame using the frame 's name . definition \\\" This frame includes words that name ... \\\" . frame - ( optional ) frame object , name , or ID ; only relations involving . frame_relations ( fn . frames ( r ' ( ? A brief intro to Frames ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A Frame is a script - like conceptual structure that describes a particular type of situation , object , or event along with the participants and props that are needed for that Frame . For example , the \\\" Apply_heat \\\" frame describes a common situation involving a Cook , some Food , and a Heating_Instrument , and is evoked by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . . We call the roles of a Frame \\\" frame elements \\\" ( FEs ) and the frame - evoking words are called \\\" lexical units \\\" ( LUs ) . FrameNet includes relations between Frames . Several types of relations are defined , of which the most important are : . Inheritance : An IS - A relation . The child frame is a subtype of the parent frame , and each FE in the parent is bound to a corresponding FE in the child . An example is the \\\" Revenge \\\" frame which inherits from the \\\" Rewards_and_punishments \\\" frame . Using : The child frame presupposes the parent frame as background , e.g the \\\" Speed \\\" frame \\\" uses \\\" ( or presupposes ) the \\\" Motion \\\" frame ; however , not all parent FEs need to be bound to child FEs . Subframe : The child frame is a subevent of a complex event represented by the parent , e.g. the \\\" Criminal_process \\\" frame has subframes of \\\" Arrest \\\" , \\\" Arraignment \\\" , \\\" Trial \\\" , and \\\" Sentencing \\\" . Perspective_on : The child frame provides a particular perspective on an un - perspectivized parent frame . A pair of examples consists of the \\\" Hiring \\\" and \\\" Get_a_job \\\" frames , which perspectivize the \\\" Employment_start \\\" frame from the Employer 's and the Employee 's point of view , respectively . Returns a list of all frames that contain LUs in which the name attribute of the LU matchs the given regular expression pat . Note that LU names are composed of \\\" lemma . POS \\\" , where the \\\" lemma \\\" part can be made up of either a single lexeme ( e.g. ' run ' ) or multiple lexemes ( e.g. ' a little ' ) . frames_by_lemma ( r ' ( ? Get information about a specific Lexical Unit using the i d number fn_luid . This function reads the LU information from the xml file on disk each time it is called . You may want to cache this info if you plan to call this function with the same i d number multiple times . Usage examples : . lu ( 256 ) . lu ( 256 ) . definition ' COD : be aware of beforehand ; predict . ' lu ( 256 ) . frame . lu ( 256 ) . The dict that is returned from this function will contain most of the following information about the LU . Note that some LUs do not contain all of these pieces of information - particularly ' totalAnnotated ' and ' incorporatedFE ' may be missing in some LUs : . ' _ type ' : ' lu ' . ' status ' : e.g. ' Created ' . ' frame ' : Frame that this LU belongs to . ' POS ' : the part of speech of this LU ( e.g. ' N ' ) . ' totalAnnotated ' : total number of examples annotated with this LU . ' incorporatedFE ' : FE that incorporates this LU ( e.g. ' Ailment ' ) . ' sentenceCount ' : a dict with the following two keys : . ' annotated ' : number of sentences annotated with this LU . ' total ' : total number of sentences with this LU . ' lexemes ' : a list of dicts describing the lemma of this LU . Each dict in the list contains these keys : - ' POS ' : part of speech e.g. ' N ' - ' name ' : either single - lexeme e.g. ' merger ' or . Consider : \\\" take over.v \\\" as in : . Germany took over the Netherlands in 2 days . Germany took the Netherlands over in 2 days . In this case , ' breakBefore ' would be \\\" true \\\" for the lexeme \\\" over \\\" . Contrast this with \\\" take after.v \\\" as in : . Under the hood , this implementation looks up the lexical unit information in the frame definition file . That file does not contain corpus annotations , so the LU files will be accessed on demand if those are needed . In principle , valence patterns could be loaded here too , though these are not currently supported . Returns basic information about the LU whose i d is fn_luid . This is basically just a wrapper around the lu ( ) function with \\\" subCorpus \\\" info excluded . lus ( r ' ( ? A brief intro to Lexical Units ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A lexical unit ( LU ) is a pairing of a word with a meaning . For example , the \\\" Apply_heat \\\" Frame describes a common situation involving a Cook , some Food , and a Heating Instrument , and is _ evoked _ by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . These frame - evoking words are the LUs in the Apply_heat frame . Each sense of a polysemous word is a different LU . We have used the word \\\" word \\\" in talking about LUs . The reality is actually rather complex . When we say that the word \\\" bake \\\" is polysemous , we mean that the lemma \\\" bake.v \\\" ( which has the word - forms \\\" bake \\\" , \\\" bakes \\\" , \\\" baked \\\" , and \\\" baking \\\" ) is linked to three different frames : . Apply_heat : \\\" Michelle baked the potatoes for 45 minutes . \\\" Cooking_creation : \\\" Michelle baked her mother a cake for her birthday . \\\" Absorb_heat : \\\" The potatoes have to bake for more than 30 minutes . \\\" These constitute three different LUs , with different definitions . Multiword expressions such as \\\" given name \\\" and hyphenated words like \\\" shut - eye \\\" can also be LUs . Idiomatic phrases such as \\\" middle of nowhere \\\" and \\\" give the slip ( to ) \\\" are also defined as LUs in the appropriate frames ( \\\" Isolated_places \\\" and \\\" Evading \\\" , respectively ) , and their internal structure is not analyzed . Framenet provides multiple annotated examples of each sense of a word ( i.e. each LU ) . Moreover , the set of examples ( approximately 20 per LU ) illustrates all of the combinatorial possibilities of the lexical unit . Each LU is linked to a Frame , and hence to the other words which evoke that Frame . This makes the FrameNet database similar to a thesaurus , grouping together semantically similar words . In the simplest case , frame - evoking words are verbs such as \\\" fried \\\" in : . \\\" Matilde fried the catfish in a heavy iron skillet . \\\" Sometimes event nouns may evoke a Frame . For example , \\\" reduction \\\" evokes \\\" Cause_change_of_scalar_position \\\" in : . \\\" ... the reduction of debt levels to $ 665 million from $ 2.6 billion . \\\" Adjectives may also evoke a Frame . For example , \\\" asleep \\\" may evoke the \\\" Sleep \\\" frame as in : . \\\" They were asleep for hours . \\\" Many common nouns , such as artifacts like \\\" hat \\\" or \\\" tower \\\" , typically serve as dependents rather than clearly evoking their own frames . A regular expression pattern used to search the LU names . Note that LU names take the form of a dotted string ( e.g. \\\" run.v \\\" or \\\" a little.adv \\\" ) in which a lemma preceeds the \\\" . \\\" and a POS follows the dot . The lemma may be composed of a single lexeme ( e.g. \\\" run \\\" ) or of multiple lexemes ( e.g. \\\" a little \\\" ) . If ' name ' is not given , then all LUs will be returned . The valid POSes are : . v - verb n - noun a - adjective adv - adverb prep - preposition num - numbers intj - interjection art - article c - conjunction scon - subordinating conjunction . Return type : . list of LU objects ( dicts ) . See the lu ( ) function for info about the specifics of LU objects . Apply inference rules to distribute semtypes over relations between FEs . For FrameNet 1.5 , this results in 1011 semtypes being propagated . ( Not done by default because it requires loading all frame files , which takes several seconds . If this needed to be fast , it could be rewritten to traverse the neighboring relations on demand for each FE semtype . ) frames ( ) for fe in f . FE . values ( ) if fe . frames ( ) for fe in f . FE . values ( ) if fe . semType ) 5252 . keys ( ) ) [ ' ID ' , ' _ type ' , ' abbrev ' , ' definition ' , ' name ' , ' rootType ' , ' subTypes ' , ' superType ' ] . This corpus contains the NEWSWIRE development test data for the NIST 1999 IE - ER Evaluation . ref.nwt and filenames were shortened . The corpus contains the following files : APW_19980314 , APW_19980424 , APW_19980429 , NYT_19980315 , NYT_19980403 , and NYT_19980407 . class nltk.corpus.reader.ieer . nltk.corpus.reader.ieer . A list of all documents in this corpus . nltk.corpus.reader.ieer . A dictionary whose keys are the names of documents in this corpus ; and whose values are descriptions of those documents ' contents . Corpus reader designed to work with corpus created by IPI PAN . The corpus includes information about text domain , channel and categories . You can access possible values using domains ( ) , channels ( ) and categories ( ) . The reader supports methods : words , sents , paras and their tagged versions . The IPIPAN Corpus contains tags indicating if there is a space between two tokens . As a result in place where there should be no space between two tokens new pair ( '' , ' no - space ' ) will be inserted ( for tagged data ) and just '' for methods without tags . The corpus reader can also try to append spaces between words . As a result either ' ' or ( ' ' , ' space ' ) will be inserted between tokens . By default , xml entities like \\\" and & are replaced by corresponding characters . Reader for corpora following the TEI - p5 xml scheme , such as MULTEXT - East . MULTEXT - East contains part - of - speech - tagged words with a quite precise tagging scheme . These tags can be converted to the Universal tagset . class nltk.corpus.reader.nkjp . XML_Tool ( root , filename ) [ source ] \\u00b6 . Bases : object . Helper class creating xml file to one without references to nkjp : namespace . That 's needed because the XMLCorpusView assumes that one can find short substrings of XML that are valid XML , which is not true if a namespace is declared at top level . Corpus reader for the nombank corpus , which augments the Penn Treebank with information about the predicate argument structure of every noun instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - noun basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . A list of tuples ( argloc , argid ) , specifying the location and identifier for each of the predicate 's argument in the containing sentence . Argument identifiers are strings such as ' ARG0 ' or ' ARGM - TMP ' . This list does not contain the predicate . Reader for Liu and Hu opinion lexicon . Blank lines and readme are ignored . words ( ) [ ' 2-faced ' , ' 2-faces ' , ' abnormal ' , ' abolish ' , ... ] . The OpinionLexiconCorpusReader provides shortcuts to retrieve positive / negative words : . negative ( ) [ ' 2-faced ' , ' 2-faces ' , ' abnormal ' , ' abolish ' , ... ] . Note that words from words ( ) method are sorted by file i d , not alphabetically : . In the pl196x corpus each category is stored in single file and thus both methods provide identical functionality . In order to accommodate finer granularity , a non - standard textids ( ) method was implemented . All the main functions can be supplied with a list of required chunks - giving much more control to the user . class nltk.corpus.reader.plaintext . Reader for Europarl corpora that consist of plaintext documents . Documents are divided into chapters instead of paragraphs as for regular plaintext documents . Chapters are separated using blank lines . Everything is inherited from PlaintextCorpusReader except that : . Since the corpus is pre - processed and pre - tokenized , the word tokenizer should just split the line at whitespaces . For the same reason , the sentence tokenizer should just split the paragraph at line breaks . There is a new ' chapters ( ) ' method that returns chapters instead instead of paragraphs . The ' paras ( ) ' method inherited from PlaintextCorpusReader is made non - functional to remove any confusion between chapters and paragraphs for Europarl . class nltk.corpus.reader.plaintext . Reader for corpora that consist of plaintext documents . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specificed as parameters to the constructor . This corpus reader can be customized ( e.g. , to skip preface sections of specific document formats ) by creating a subclass and overriding the CorpusView class variable . 42960 gives authority to administration V 46742 gives inventors of microchip N . The PP attachment is to the verb phrase ( V ) or noun phrase ( N ) , i.e. : . ( VP gives ( NP authority ) ( PP to administration ) ) ( VP gives ( NP inventors ( PP of microchip ) ) ) . The corpus contains the following files : . training : training set devset : development test set , used for algorithm development . test : test set , used to report results bitstrings : word classes derived from Mutual Information Clustering for the Wall Street Journal . Corpus reader for the propbank corpus , which augments the Penn Treebank with information about the predicate argument structure of every verb instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - verb basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . A list of tuples ( argloc , argid ) , specifying the location and identifier for each of the predicate 's argument in the containing sentence . Argument identifiers are strings such as ' ARG0 ' or ' ARGM - TMP ' . This list does not contain the predicate . Murthy Ganapathibhotla and Bing Liu . \\\" Mining Opinions in Comparative Sentences \\\" . Proceedings of the 22nd International Conference on Computational Linguistics ( Coling-2008 ) , Manchester , 18 - 22 August , 2008 . Bing Liu , Minqing Hu and Junsheng Cheng . \\\" Opinion Observer : Analyzing and Comparing . Opinions on the Web \\\" . Proceedings of the 14th international World Wide Web conference ( WWW-2005 ) , May 10 - 14 , 2005 , in Chiba , Japan . class nltk.corpus.reader.pros_cons . , ' On ' , ' - ' , ' off ' , ' switch ' , ' too ' , ' easy ' , ' to ' , ' maneuver ' , ' . ' ] words ( ' IntegratedPros.txt ' ) [ ' Easy ' , ' to ' , ' use ' , ' , ' , ' economical ' , ' ! ' , ... ] . Related papers : . Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery & Data Mining ( KDD-04 ) , 2004 . Minqing Hu and Bing Liu . \\\" Mining Opinion Features in Customer Reviews \\\" . Proceedings of Nineteeth National Conference on Artificial Intelligence ( AAAI-2004 ) , 2004 . Xiaowen Ding , Bing Liu and Philip S. Yu . \\\" A Holistic Lexicon - Based Appraoch to . Opinion Mining . \\\" Proceedings of First ACM International Conference on Web Search and Data Mining ( WSDM-2008 ) , Feb 11 - 12 , 2008 , Stanford University , Stanford , California , USA . Symbols used in the annotated reviews : . [ t ] : the title of the review : Each [ t ] tag starts a review . [ + n ] : Positive opinion , n is the opinion strength : 3 strongest , and 1 weakest . Note that the strength is quite subjective . You may want ignore it , but only considering + and - . [ -n ] : Negative opinion # # : start of each sentence . Each line is a sentence . [ u ] : feature not appeared in the sentence . [ p ] : feature not appeared in the sentence . Pronoun resolution is needed . [ s ] : suggestion or recommendation . [ cc ] : comparison with a competing product from a different brand . [ cs ] : comparison with a competing product from the same brand . Note : Some of the files ( e.g. \\\" ipod.txt \\\" , \\\" Canon PowerShot SD500.txt \\\" ) do not . provide separation between different reviews . This is due to the fact that the dataset was specifically designed for aspect / feature - based sentiment analysis , for which sentence - level annotation is sufficient . For document- level classification and analysis , this peculiarity should be taken into consideration . class nltk.corpus.reader.reviews . class nltk.corpus.reader.reviews . Reader for the Customer Review Data dataset by Hu , Liu ( 2004 ) . Note : we are not applying any sentence tokenization at the moment , just word tokenization . reviews ( ' Canon_G3 . We can also reach the same information directly from the stream : . features ( ' Canon_G3 . txt ' ) [ ( ' canon powershot g3 ' , ' +3 ' ) , ( ' use ' , ' +2 ' ) , ... ] . We can compute stats for specific product features : . features ( ' Canon_G3 . features ( ' Canon_G3 . Corpus reader for the Recognizing Textual Entailment ( RTE ) Challenge Corpora . The files were taken from the RTE1 , RTE2 and RTE3 datasets and the files were regularized . The latter are the gold standard annotated files . Each entailment corpus is a list of ' text'/'hypothesis ' pairs . The following example is taken from RTE3 : . The sale was made to pay Yukos ' US$ 27.5 billion tax bill , Yuganskneftegaz was originally sold for US$ 9.4 billion to a little known company Baikalfinansgroup which was later bought by the Russian state - owned oil company Rosneft . Baikalfinansgroup was sold to Rosneft . In order to provide globally unique IDs for each pair , a new attribute challenge has been added to the root element entailment - corpus of each file , taking values 1 , 2 or 3 . The GID is formatted ' m - n ' , where ' m ' is the challenge number and ' n ' is the pair ID . class nltk.corpus.reader.rte . Corpus reader for the SemCor Corpus . For access to the complete XML data structure , use the xml ( ) method . For access to simple word lists and tagged word lists , use words ( ) , sents ( ) , tagged_words ( ) , and tagged_sents ( ) . the given file(s ) as a list of tagged chunks , represented in tree form . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) the given file(s ) as a list of sentences . Each sentence is represented as a list of tagged chunks ( in tree form ) . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) The NLTK version of the Senseval 2 files uses well - formed XML . Each instance of the ambiguous words \\\" hard \\\" , \\\" interest \\\" , \\\" line \\\" , and \\\" serve \\\" is tagged with a sense identifier , and supplied with context . class nltk.corpus.reader.senseval . obj_score ( ) 0.125 . class nltk.corpus.reader.sentiwordnet . SentiSynset ( pos_score , neg_score , synset ) [ source ] \\u00b6 . Feng - Yi Chen , Pi - Fang Tsai , Keh - Jiann Chen , and Chu - Ren Huang ( 1999 ) The Construction of Sinica Treebank . Computational Linguistics and Chinese Language Processing , 4 , pp 87 - 104 . Huang Chu - Ren , Keh - Jiann Chen , Feng - Yi Chen , Keh - Jiann Chen , Zhao - Ming Gao , and Kuang - Yu Chen . Sinica Treebank : Design Criteria , Annotation Guidelines , and On - line Interface . Proceedings of 2nd Chinese Language Processing Workshop , Association for Computational Linguistics . Chen Keh - Jiann and Yu - Ming Hsieh ( 2004 ) Chinese Treebanks and Grammar Extraction , Proceedings of IJCNLP-04 , pp560 - 565 . class nltk.corpus.reader.sinica_treebank . class nltk.corpus.reader.switchboard . SwitchboardTurn ( words , speaker , i d ) [ source ] \\u00b6 . Bases : list . A specialized list object used to encode switchboard utterances . The elements of the list are the words in the utterance ; and two attributes , speaker and i d , are provided to retrieve the spearker identifier and utterance i d . Note that utterance ids are only unique within a given discourse . A corpus reader for the MAC_MORPHO corpus . Each line contains a single tagged word , using ' _ ' as a separator . Sentence boundaries are based on the end - sentence tag ( ' _ . ' ) Paragraph information is not included in the corpus , so each paragraph returned by self.paras ( ) and self.tagged_paras ( ) contains a single sentence . class nltk.corpus.reader.tagged . Reader for simple part - of - speech tagged corpora . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specified as parameters to the constructor . Words are parsed using nltk.tag.str2tuple . By default , ' / ' is used as the separator . I.e. , words should have the form : . word1/tag1 word2/tag2 word3/tag3 ... . But custom separators may be specified as parameters to the constructor . Part of speech tags are case - normalized to upper case . class nltk.corpus.reader.tagged . A specialized corpus view for tagged documents . It can be customized via flags to divide the tagged corpus documents up by sentence or paragraph , and to include or omit part of speech tags . TaggedCorpusView objects are typically created by TaggedCorpusReader ( not directly by nltk users ) . List of utterances in the corpus . There are total 160 utterances , each of which corresponds to a unique utterance of a speaker . Here 's an example of an utterance identifier in the list : . speakers . List of speaker IDs . An example of speaker ID : . dr1 - fvmh0 . Note that if you split an item ID with colon and take the first element of the result , you will get a speaker ID . The second element of the result is a sentence ID . dictionary ( ) . Phonetic dictionary of words contained in this corpus . This is a Python dictionary from words to phoneme lists . spkrinfo ( ) . Speaker information table . It 's a Python dictionary from speaker IDs to records of 10 fields . Speaker IDs the same as the ones in timie.speakers . Each record is a dictionary from field names to values , and the fields are as follows : . : unknown ) comments comments by the recorder . The 4 functions are as follows . Given a list of items , returns an iterator of a list of word lists , each of which corresponds to an item ( sentence ) . If offset is set to True , each element of the word list is a tuple of word(string ) , start offset and end offset , where offset is represented as a number of 16kHz samples . Given a list of items , returns an iterator of a list of phoneme lists , each of which corresponds to an item ( sentence ) . If offset is set to True , each element of the phoneme list is a tuple of word(string ) , start offset and end offset , where offset is represented as a number of 16kHz samples . Given an item , returns a chunk of audio samples formatted into a string . When the fuction is called , if start and end are omitted , the entire samples of the recording will be returned . If only end is omitted , samples from the start offset to the end of the recording will be returned . play(data ) . Play the given audio samples . The audio samples can be obtained from the timit.audiodata function . class nltk.corpus.reader.timit . Times . Lat0117 ' , ' Chinese_Mandarin - HZ ' , ' Marathi - UTF8 ' , ' Azeri_Azerbaijani_Cyrillic - Az . Times . Cyr . Normal0117 ' , ' Hungarian_Magyar - Unicode ' , ' Japanese_Nihongo - JIS ' , ' Vietnamese - VIQR ' , ' Amharic - Afenegus6 . A stream backed corpus view for corpus files that consist of sequences of serialized Python objects ( serialized using pickle.dump ) . One use case for this class is to store the result of running feature detection on a corpus to disk . This can be useful when performing feature detection is expensive ( so we do n't want to repeat it ) ; but the corpus is too large to store in memory . The following example illustrates this technique : . A ' view ' of a corpus file , which acts like a sequence of tokens : it can be accessed by index , iterated over , etc . However , the tokens are only constructed as - needed - the entire corpus is never stored in memory at once . The constructor to StreamBackedCorpusView takes two arguments : a corpus fileid ( specified as a string or as a PathPointer ) ; and a block reader . A \\\" block reader \\\" is a function that reads zero or more tokens from a stream , and returns them as a list . A very simple example of a block reader is : . readline ( ) . split ( ) . This simple block reader reads a single line at a time , and returns a single token ( consisting of a string ) for each whitespace - separated substring on the line . When deciding how to define the block reader for a given corpus , careful consideration should be given to the size of blocks handled by the block reader . Smaller block sizes will increase the memory requirements of the corpus view 's internal data structures ( by 2 integers per block ) . On the other hand , larger block sizes may decrease performance for random access to the corpus . ( But note that larger block sizes will not decrease performance for iteration . ) Internally , CorpusView maintains a partial mapping from token index to file position , with one entry per block . When a token with a given index i is requested , the CorpusView constructs it as follows : . First , it searches the toknum / filepos mapping for the token index closest to ( but less than or equal to ) i . Then , starting at the file position corresponding to that index , it reads one block at a time using the block reader until it reaches the requested token . The toknum / filepos mapping is created lazily : it is initially empty , but every time a new block is read , the block 's initial token is added to the mapping . ( Thus , the toknum / filepos map has one entry per block . ) In order to increase efficiency for random access patterns that have high degrees of locality , the corpus view may cache one or more blocks . Each CorpusView object internally maintains an open file object for its underlying corpus file . This file should be automatically closed when the CorpusView is garbage collected , but if you wish to close it manually , use the close ( ) method . If you access a CorpusView 's items after it has been closed , the file object will be automatically re - opened . Warning : . If the contents of the file are modified during the lifetime of the CorpusView , then the CorpusView 's behavior is undefined . Warning : . Variables : . _ block_reader - The function used to read a single block from the underlying file stream . _ toknum - A list containing the token index of each block that has been processed . In particular , _ toknum[i ] is the token index of the first token in block i . Together with _ filepos , this forms a partial mapping between token indices and file positions . _ filepos - A list containing the file position of each block that has been processed . In particular , _ toknum[i ] is the file position of the first character in block i . Together with _ toknum , this forms a partial mapping between token indices and file positions . _ stream - The stream used to access the underlying corpus file . _ len - The total number of tokens in the corpus , if known ; or None , if the number of tokens is not yet known . _ eofpos - The character position of the last character in the file . This is calculated when the corpus view is initialized , and is used to decide when the end of file has been reached . _ cache - A cache of the most recently read block . Close the file stream associated with this corpus view . This can be useful if you are worried about running out of file handles ( although the stream should automatically be closed upon garbage collection of the corpus view ) . If the corpus view is accessed after it is closed , it will be automatically re - opened . Concatenate together the contents of multiple documents from a single corpus , using an appropriate concatenation function . This utility function is used by corpus readers when the user requests more than one document at a time . nltk.corpus.reader.util . find_corpus_fileids ( root , regexp ) [ source ] \\u00b6 . nltk.corpus.reader.util . Read a sequence of tokens from a stream , where tokens begin with lines that match start_re . If end_re is specified , then tokens end with lines that match end_re ; otherwise , tokens end whenever the next line matching start_re or EOF is found . nltk.corpus.reader.util . Read a sequence of s - expressions from the stream , and leave the stream 's file position at the end the last complete s - expression read . This function will always return at least one s - expression , unless there are no more s - expressions in the file . If the file ends in in the middle of an s - expression , then that incomplete s - expression is returned when the end of the file is reached . From the VerbNet site : \\\" VerbNet ( VN ) ( Kipper - Schuler 2006 ) is the largest on - line verb lexicon currently available for English . It is a hierarchical domain - independent , broad - coverage verb lexicon with mappings to other lexical resources such as WordNet ( Miller , 1990 ; Fellbaum , 1998 ) , Xtag ( XTAG Research Group , 2001 ) , and FrameNet ( Baker et al . , 1998 ) . Return a list of the verbnet class identifiers . If a file identifier is specified , then return only the verbnet class identifiers for classes ( and subclasses ) defined by that file . If a lemma is specified , then return only verbnet class identifiers for classes that contain that lemma as a member . If a wordnetid is specified , then return only identifiers for classes that contain that wordnetid as a member . If a classid is specified , then return only identifiers for subclasses of the specified verbnet class . fileid_or_classid - An identifier specifying which class should be returned . Can be a file identifier ( such as ' put-9.1 . xml ' ) , or a verbnet class identifier ( such as ' put-9.1 ' ) or a short verbnet class identifier ( such as ' 9.1 ' ) . class nltk.corpus.reader.wordnet . Lemma ( wordnet_corpus_reader , synset , name , lexname_index , lex_id , syntactic_marker ) [ source ] \\u00b6 . Bases : nltk.corpus.reader.wordnet . _ WordNetObject . The lexical entry for a single morphological form of a sense - disambiguated word . Create a Lemma from a \\\" ... \\\" string where : is the morphological stem identifying the synset is one of the module attributes ADJ , ADJ_SAT , ADV , NOUN or VERB is the sense number , counting from 0 . is the morphological form of interest . Note that and can be different , e.g. the Synset ' salt.n.03 ' has the Lemmas ' salt.n.03 . salt ' , ' salt.n.03 . saltiness ' and ' salt.n.03 . salinity ' . Lemma attributes , accessible via methods with the same name : . - name : The canonical name of this lemma . - synset : The synset that this lemma belongs to . - syntactic_marker : For adjectives , the WordNet string identifying the . class nltk.corpus.reader.wordnet . Synset ( wordnet_corpus_reader ) [ source ] \\u00b6 . Bases : nltk.corpus.reader.wordnet . _ WordNetObject . Create a Synset from a \\\" . \\\" string where : is the word 's morphological stem is one of the module attributes ADJ , ADJ_SAT , ADV , NOUN or VERB is the sense number , counting from 0 . Synset attributes , accessible via methods with the same name : . name : The canonical name of this synset , formed using the first lemma of this synset . Note that this may be different from the name passed to the constructor if that string used a different lemma to identify the synset . pos : The synset 's part of speech , matching one of the module level attributes ADJ , ADJ_SAT , ADV , NOUN or VERB . Return the transitive closure of source under the rel relationship , breadth - first . closure ( hyp ) ) [ Synset('canine.n.02 ' ) , Synset('domestic_animal . n.01 ' ) , Synset('carnivore.n.01 ' ) , Synset('animal.n.01 ' ) , Synset('placental.n.01 ' ) , Synset('organism.n.01 ' ) , Synset('mammal.n.01 ' ) , Synset('living_thing . n.01 ' ) , Synset('vertebrate.n.01 ' ) , Synset('whole.n.02 ' ) , Synset('chordate.n.01 ' ) , Synset('object.n.01 ' ) , Synset('physical_entity . n.01 ' ) , Synset('entity.n.01 ' ) ] . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . A float score denoting the similarity of the two Synset objects , in the range 0 to 1 . Get a list of lowest synset(s ) that both synsets have as a hypernym . By setting the use_min_depth flag to True , the behavior of NLTK2 can be preserved . This was changed in NLTK3 to give more accurate results in a small set of cases , generally with synsets concerning people . ( eg : ' chef.n.01 ' , ' fireman.n.01 ' , etc . ) . This method is an implementation of Ted Pedersen 's \\\" Lowest Common Subsumer \\\" method from the Perl Wordnet module . It can return either \\\" self \\\" or \\\" other \\\" if they are a hypernym of the other . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( False by default ) creates a fake root that connects all the taxonomies . Set it to True to enable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will need to be added for nouns as well . use_min_depth ( bool ) - This setting mimics older ( v2 ) behavior of NLTK wordnet If True , will use the min_depth function to calculate the lowest common hypernyms . This is known to give strange results for some synset pairs ( eg : ' chef.n.01 ' , ' fireman.n.01 ' ) but is retained for backwards compatibility . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Returns the distance of the shortest path linking the two synsets ( if one exists ) . For each synset , all the ancestor nodes and their distances are recorded and compared . The ancestor node common to both synsets that can be reached with the minimum number of traversals is used . If no ancestor nodes are common , None is returned . If a node is compared with itself 0 is returned . n.01 ' ) , [ Synset('whole.n.02 ' ) , [ Synset('object.n.01 ' ) , [ Synset('physical_entity . n.01 ' ) , [ Synset('entity.n.01 ' ) ] ] ] ] ] ] ] ] ] ] ] ] ] , [ Synset('domestic_animal . n.01 ' ) , [ Synset('animal.n.01 ' ) , [ Synset('organism.n.01 ' ) , [ Synset('living_thing . n.01 ' ) , [ Synset('whole.n.02 ' ) , [ Synset('object.n.01 ' ) , [ Synset('physical_entity . n.01 ' ) , [ Synset('entity.n.01 ' ) ] ] ] ] ] ] ] ] ] . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . nltk.corpus.reader.wordnet . A table of strings that are used to express verb frames . class nltk.corpus.reader.wordnet . WordNetCorpusReader ( root , omw_reader ) [ source ] \\u00b6 . corpus ( CorpusReader ) - The corpus from which we create an information . content dictionary . : type weight_senses_equally : bool : param weight_senses_equally : If this is True , gives all possible senses equal weight rather than dividing by the number of possible senses . ( If a word has 3 synses , each sense gets 0.3333 per appearance when this is False , 1.0 when it is true . ) : param smoothing : How much do we smooth synset counts ( default is 1.0 ) : type smoothing : float : return : An information content dictionary . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Find a possible base form for the given form , with the given part of speech , by checking WordNet 's list of exceptional forms , and by recursively stripping affixes for this part of speech until a form in WordNet is found . morphy ( ' hardrock ' , wn . morphy ( ' book ' , wn . morphy ( ' book ' , wn . ADJ ) . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Load all synsets with a given lemma and part of speech tag . If no pos is specified , all synsets for all parts of speech will be loaded . If lang is specified , all the synsets associated with the lemma name of that language will be returned . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . Load an information content file from the wordnet_ic corpus and return a dictionary . This dictionary has just two keys , NOUN and VERB , whose values are dictionaries that map from synsets to information content values . nltk.corpus.reader.wordnet . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . nltk.corpus.reader.wordnet . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . nltk.corpus.reader.wordnet . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . other ( Synset ) - The Synset that this Synset is being compared to . ic ( dict ) - an information content object ( as returned by nltk.corpus.wordnet_ic.ic ( ) ) . Returns : . A float score denoting the similarity of the two Synset objects , in the range 0 to 1 . nltk.corpus.reader.wordnet . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . nltk.corpus.reader.wordnet . Resnik Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) . nltk.corpus.reader.wordnet . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . A corpus view that selects out specified elements from an XML file , and provides a flat list - like interface for accessing them . ( Note : XMLCorpusView is not used by XMLCorpusReader itself , but may be used by subclasses of XMLCorpusReader . ) Every XML corpus view has a \\\" tag specification \\\" , indicating what XML elements should be included in the view ; and each ( non - nested ) element that matches this specification corresponds to one item in the view . Tag specifications are regular expressions over tag paths , where a tag path is a list of element tag names , separated by ' / ' , indicating the ancestry of the element . Some examples : . ' foo ' : A top - level element whose tag is foo . foo / bar ' : An element whose tag is bar and whose parent is a top - level element whose tag is foo . The view items are generated from the selected XML elements via the method handle_elt ( ) . By default , this method returns the element as - is ( i.e. , as an ElementTree object ) ; but it can be overridden , either via subclassing or via the elt_handler constructor parameter . context ( str ) - A string composed of element tags separated by forward slashes , indicating the XML context of the given element . For example , the string ' foo / bar / baz ' indicates that the element is a baz element whose parent is a bar element and whose grandparent is a top - level foo element . Corpus reader for the York - Toronto - Helsinki Parsed Corpus of Old English Prose ( YCOE ) , a 1.5 million word syntactically - annotated corpus of Old English prose texts . nltk.corpus.reader.ycoe . NLTK corpus readers . The modules in this package provide functions that can be used to read corpus fileids in a variety of formats . These functions can be used to read both the corpus fileids that are distributed in the NLTK corpus package , and corpus fileids that are part of external corpora . Each corpus module defines one or more \\\" corpus reader functions \\\" , which can be used to read documents from that corpus . These functions take an argument , item , which is used to indicate which document should be read from the corpus : . If item is one of the unique identifiers listed in the corpus module 's items variable , then the corresponding document will be loaded from the NLTK corpus package . If item is a fileid , then that file will be read . Additionally , corpus reader functions can be given lists of item names ; in which case , they will return a concatenation of the corresponding documents . Corpus reader functions are named based on the type of information they return . Some common examples , and their return types , are : . sents ( ) : list of ( list of str ) . paras ( ) : list of ( list of ( list of str ) ) . tagged_words ( ) : list of ( str , str ) tuple . tagged_sents ( ) : list of ( list of ( str , str ) ) . tagged_paras ( ) : list of ( list of ( list of ( str , str ) ) ) . chunked_sents ( ) : list of ( Tree w/ ( str , str ) leaves ) . parsed_sents ( ) : list of ( Tree with str leaves ) . parsed_paras ( ) : list of ( list of ( Tree with str leaves ) ) . xml ( ) : A single xml ElementTree . raw ( ) : unprocessed corpus contents . For example , to read a list of the words in the Brown Corpus , use nltk.corpus.brown.words ( ) : . join ( brown . words ( ) ) ) The , Fulton , County , Grand , Jury , said , ... . class nltk.corpus.reader . Bases : object . A base class for \\\" corpus reader \\\" classes , each of which can be used to read a specific corpus format . Each individual corpus reader instance is used to read a specific corpus , consisting of one or more files under a common root directory . Each file is identified by its file identifier , which is the relative path to the file from the root directory . A separate subclass is be defined for each corpus format . These subclasses define one or more methods that provide ' views ' on the corpus contents , such as words ( ) ( for a list of words ) and parsed_sents ( ) ( for a list of parsed sentences ) . Called with no arguments , these methods will return the contents of the entire corpus . For most corpora , these methods define one or more selection arguments , such as fileids or categories , which can be used to select which portion of the corpus should be returned . fileids ( None or str or list ) - Specifies the set of fileids for which paths should be returned . Can be None , for all fileids ; a list of file identifiers , for a specified set of fileids ; or a single file identifier , for a single file . Note that the return value is always a list of paths , even if fileids is a single file identifier . include_encoding - If true , then return a list of ( path_pointer , encoding ) tuples . Load this corpus ( if it has not already been loaded ) . This is used by LazyCorpusLoader as a simple method that can be used to make sure a corpus is loaded - e.g. , in case a user wants to do help(some_corpus ) . class nltk.corpus.reader . CategorizedCorpusReader ( kwargs ) [ source ] \\u00b6 . Bases : object . A mixin class used to aid in the implementation of corpus readers for categorized corpora . This class defines the method categories ( ) , which returns a list of the categories for the corpus or for a specified set of fileids ; and overrides fileids ( ) to take a categories argument , restricting the set of fileids to be returned . Call _ _ init _ _ ( ) to set up the mapping . Override all view methods to accept a categories parameter , which can be used instead of the fileids parameter , to select which fileids should be included in the returned view . Return a list of file identifiers for the files that make up this corpus , or that make up the given category(s ) if specified . class nltk.corpus.reader . Reader for corpora that consist of plaintext documents . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specificed as parameters to the constructor . This corpus reader can be customized ( e.g. , to skip preface sections of specific document formats ) by creating a subclass and overriding the CorpusView class variable . class nltk.corpus.reader . Reader for simple part - of - speech tagged corpora . Paragraphs are assumed to be split using blank lines . Sentences and words can be tokenized using the default tokenizers , or by custom tokenizers specified as parameters to the constructor . Words are parsed using nltk.tag.str2tuple . By default , ' / ' is used as the separator . I.e. , words should have the form : . word1/tag1 word2/tag2 word3/tag3 ... . But custom separators may be specified as parameters to the constructor . Part of speech tags are case - normalized to upper case . class nltk.corpus.reader . Reader for chunked ( and optionally tagged ) corpora . Paragraphs are split using a block reader . They are then tokenized into sentences using a sentence tokenizer . Finally , these sentences are parsed into chunk trees using a string - to - chunktree conversion function . Each of these steps can be performed using a default function or a custom function . By default , paragraphs are split on blank lines ; sentences are listed one per line ; and sentences are parsed into chunk trees using nltk.chunk.tagstr2tree . the given file(s ) as a list of paragraphs , each encoded as a list of sentences , which are in turn encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of sentences , each encoded as a shallow Tree . The leaves of these trees are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . the given file(s ) as a list of tagged words and chunks . Words are encoded as ( word , tag ) tuples ( if the corpus has tags ) or word strings ( if the corpus has no tags ) . Chunks are encoded as depth - one trees over ( word , tag ) tuples or word strings . A corpus reader for the MAC_MORPHO corpus . Each line contains a single tagged word , using ' _ ' as a separator . Sentence boundaries are based on the end - sentence tag ( ' _ . ' ) Paragraph information is not included in the corpus , so each paragraph returned by self.paras ( ) and self.tagged_paras ( ) contains a single sentence . class nltk.corpus.reader . Reader for the Alpino Dutch Treebank . This corpus has a lexical breakdown structure embedded , as read by _ parse Unfortunately this puts punctuation and some other words out of the sentence order in the xml element tree . This is no good for tag _ and word _ _ tag and _ word will be overridden to use a non - default new parameter ' ordered ' to the overridden _ normalize function . The _ parse function can then remain untouched . class nltk.corpus.reader . class nltk.corpus.reader . Reader for Europarl corpora that consist of plaintext documents . Documents are divided into chapters instead of paragraphs as for regular plaintext documents . Chapters are separated using blank lines . Everything is inherited from PlaintextCorpusReader except that : . Since the corpus is pre - processed and pre - tokenized , the word tokenizer should just split the line at whitespaces . For the same reason , the sentence tokenizer should just split the paragraph at line breaks . There is a new ' chapters ( ) ' method that returns chapters instead instead of paragraphs . The ' paras ( ) ' method inherited from PlaintextCorpusReader is made non - functional to remove any confusion between chapters and paragraphs for Europarl . Corpus reader for the propbank corpus , which augments the Penn Treebank with information about the predicate argument structure of every verb instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - verb basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . From the VerbNet site : \\\" VerbNet ( VN ) ( Kipper - Schuler 2006 ) is the largest on - line verb lexicon currently available for English . It is a hierarchical domain - independent , broad - coverage verb lexicon with mappings to other lexical resources such as WordNet ( Miller , 1990 ; Fellbaum , 1998 ) , Xtag ( XTAG Research Group , 2001 ) , and FrameNet ( Baker et al . , 1998 ) . Return a list of the verbnet class identifiers . If a file identifier is specified , then return only the verbnet class identifiers for classes ( and subclasses ) defined by that file . If a lemma is specified , then return only verbnet class identifiers for classes that contain that lemma as a member . If a wordnetid is specified , then return only identifiers for classes that contain that wordnetid as a member . If a classid is specified , then return only identifiers for subclasses of the specified verbnet class . fileid_or_classid - An identifier specifying which class should be returned . Can be a file identifier ( such as ' put-9.1 . xml ' ) , or a verbnet class identifier ( such as ' put-9.1 ' ) or a short verbnet class identifier ( such as ' 9.1 ' ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . stem - If true , then use word stems instead of word strings . class nltk.corpus.reader . A corpus reader for CoNLL - style files . These files consist of a series of sentences , separated by blank lines . Each sentence is encoded using a table ( or \\\" grid \\\" ) of values , where each line corresponds to a single word , and each column corresponds to an annotation type . The set of columns used by CoNLL - style files can vary from corpus to corpus ; the ConllCorpusReader constructor therefore takes an argument , columntypes , which is used to specify the columns that are used by a given corpus . @todo : Possibly add caching of the grid corpus view ? This would . allow the same grid view to be used by different data access methods ( eg words ( ) and parsed_sents ( ) could both share the same grid corpus view object ) . @todo : Better support for -DOCSTART- . Currently , we just ignore . it , but it could be used to define methods that retrieve a document at a time ( eg parsed_documents ( ) ) . corpus ( CorpusReader ) - The corpus from which we create an information . content dictionary . : type weight_senses_equally : bool : param weight_senses_equally : If this is True , gives all possible senses equal weight rather than dividing by the number of possible senses . ( If a word has 3 synses , each sense gets 0.3333 per appearance when this is False , 1.0 when it is true . ) : param smoothing : How much do we smooth synset counts ( default is 1.0 ) : type smoothing : float : return : An information content dictionary . Jiang - Conrath Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Leacock Chodorow Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses ( as above ) and the maximum depth of the taxonomy in which the senses occur . The relationship is given as -log(p/2d ) where p is the shortest path length and d is the taxonomy depth . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally greater than 0 . None is returned if no connecting path could be found . If a Synset is compared with itself , the maximum score is returned , which varies depending on the taxonomy depth . Lin Similarity : Return a score denoting how similar two word senses are , based on the Information Content ( IC ) of the Least Common Subsumer ( most specific ancestor node ) and that of the two input Synsets . Find a possible base form for the given form , with the given part of speech , by checking WordNet 's list of exceptional forms , and by recursively stripping affixes for this part of speech until a form in WordNet is found . morphy ( ' hardrock ' , wn . morphy ( ' book ' , wn . morphy ( ' book ' , wn . ADJ ) . Path Distance Similarity : Return a score denoting how similar two word senses are , based on the shortest path that connects the senses in the is - a ( hypernym / hypnoym ) taxonomy . The score is in the range 0 to 1 , except in those cases where a path can not be found ( will only be true for verbs as there are many distinct verb taxonomies ) , in which case None is returned . A score of 1 represents identity i.e. comparing a sense with itself will return 1 . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A score denoting the similarity of the two Synset objects , normally between 0 and 1 . None is returned if no connecting path could be found . 1 is returned if a Synset is compared with itself . Load all synsets with a given lemma and part of speech tag . If no pos is specified , all synsets for all parts of speech will be loaded . If lang is specified , all the synsets associated with the lemma name of that language will be returned . Wu - Palmer Similarity : Return a score denoting how similar two word senses are , based on the depth of the two senses in the taxonomy and that of their Least Common Subsumer ( most specific ancestor node ) . Previously , the scores computed by this implementation did _ not _ always agree with those given by Pedersen 's Perl implementation of WordNet Similarity . However , with the addition of the simulate_root flag ( see below ) , the score for verbs now almost always agree but not always for nouns . The LCS does not necessarily feature in the shortest path connecting the two senses , as it is by definition the common ancestor deepest in the taxonomy , not closest to the two senses . Typically , however , it will so feature . Where multiple candidates for the LCS exist , that whose shortest path to the root node is the longest will be selected . Where the LCS has multiple paths to the root , the longer path is used for the purposes of the calculation . other ( Synset ) - The Synset that this Synset is being compared to . simulate_root ( bool ) - The various verb taxonomies do not share a single root which disallows this metric from working for synsets that are not connected . This flag ( True by default ) creates a fake root that connects all the taxonomies . Set it to false to disable this behavior . For the noun taxonomy , there is usually a default root except for WordNet version 1.6 . If you are using wordnet 1.6 , a fake root will be added for nouns as well . Returns : . A float score denoting the similarity of the two Synset objects , normally greater than zero . If no connecting path between the two senses can be found , None is returned . class nltk.corpus.reader . WordNetICCorpusReader ( root , fileids ) [ source ] \\u00b6 . Load an information content file from the wordnet_ic corpus and return a dictionary . This dictionary has just two keys , NOUN and VERB , whose values are dictionaries that map from synsets to information content values . class nltk.corpus.reader . Corpus reader for the nombank corpus , which augments the Penn Treebank with information about the predicate argument structure of every noun instance . The corpus consists of two parts : the predicate - argument annotations themselves , and a set of \\\" frameset files \\\" which define the argument labels used by the annotations , on a per - noun basis . Each \\\" frameset file \\\" contains one or more predicates , such as ' turn ' or ' turn_on ' , each of which is divided into coarse - grained word senses called \\\" rolesets \\\" . For each \\\" roleset \\\" , the frameset file provides descriptions of the argument roles , along with examples . Corpus reader designed to work with corpus created by IPI PAN . The corpus includes information about text domain , channel and categories . You can access possible values using domains ( ) , channels ( ) and categories ( ) . The reader supports methods : words , sents , paras and their tagged versions . The IPIPAN Corpus contains tags indicating if there is a space between two tokens . As a result in place where there should be no space between two tokens new pair ( '' , ' no - space ' ) will be inserted ( for tagged data ) and just '' for methods without tags . The corpus reader can also try to append spaces between words . As a result either ' ' or ( ' ' , ' space ' ) will be inserted between tokens . By default , xml entities like \\\" and & are replaced by corresponding characters . In the pl196x corpus each category is stored in single file and thus both methods provide identical functionality . In order to accommodate finer granularity , a non - standard textids ( ) method was implemented . All the main functions can be supplied with a list of required chunks - giving much more control to the user . Corpus reader for the XML version of the CHILDES corpus . Copy the needed parts of the CHILDES XML corpus into the NLTK data directory ( nltk_data / corpora / CHILDES/ ) . For access to the file text use the usual nltk functions , words ( ) , sents ( ) , tagged_words ( ) and tagged_sents ( ) . the given file(s ) as a list of sentences or utterances , each encoded as a list of word strings . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of sentences , each encoded as a list of ( word , tag ) tuples . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( str , pos , relation_list ) . If there is manually - annotated relation info , it will return tuples of ( str , pos , test_relation_list , str , pos , gold_relation_list ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . the given file(s ) as a list of tagged words and punctuation symbols , encoded as tuples ( word , tag ) . Return type : . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . Map a corpus file to its web version on the CHILDES website , and open it in a web browser . The complete URL to be used is : . If no urlbase is passed , we try to calculate it . This requires that the childes corpus was set up to mirror the folder hierarchy under childes.psy.cmu.edu/data-xml/ , e.g. : nltk_data / corpora / childes / Eng - USA / Cornell/ ? ? ? or nltk_data / corpora / childes / Romance / Spanish / Aguirre/ ? ? ? The function first looks ( as a special case ) if \\\" Eng - USA \\\" is on the path consisting of + fileid ; then if \\\" childes \\\" , possibly followed by \\\" data - xml \\\" , appears . If neither one is found , we use the unmodified fileid and hope for the best . speaker - If specified , select specific speaker(s ) defined in the corpus . Default is ' ALL ' ( all participants ) . Common choices are ' CHI ' ( the child ) , ' MOT ' ( mother ) , [ ' CHI','MOT ' ] ( exclude researchers ) . stem - If true , then use word stems instead of word strings . relation - If true , then return tuples of ( stem , index , dependent_index ) . strip_space - If true , then strip trailing spaces from word tokens . Otherwise , leave the spaces on the tokens . replace - If true , then use the replaced ( intended ) word instead of the original word ( e.g. , ' wat ' will be replaced with ' watch ' ) . class nltk.corpus.reader . Corpus reader for the SemCor Corpus . For access to the complete XML data structure , use the xml ( ) method . For access to simple word lists and tagged word lists , use words ( ) , sents ( ) , tagged_words ( ) , and tagged_sents ( ) . the given file(s ) as a list of tagged chunks , represented in tree form . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) the given file(s ) as a list of sentences . Each sentence is represented as a list of tagged chunks ( in tree form ) . Return type : . tag - ' pos ' ( part of speech ) , ' sem ' ( semantic ) , or ' both ' to indicate the kind of tags to include . Semantic tags consist of WordNet lemma IDs , plus an ' NE ' node if the chunk is a named entity without a specific entry in WordNet . ( Named entities of type ' other ' have no lemma . Other chunks not in WordNet have no semantic tag . Punctuation tokens have None for their part of speech tag . ) lu ( 3238 ) . frame . lexUnit [ ' glint.v ' ] is fn . frame_by_name ( ' Replacing ' ) is fn . lus ( ' replace.v ' ) [ 0 ] . lus ( ' prejudice.n ' ) [ 0 ] . frame . frame_relations ( ' Partiality ' ) True . Details for a specific annotated document can be obtained using this class 's annotated_document ( ) function and pass it the value of the ' ID ' field . corpname for x in fn . name ( str ) - A regular expression pattern used to search the file name of each annotated document . The document 's file name contains the name of the corpus that the document is from , followed by two underscores \\\" _ _ \\\" followed by the document name . So , for example , the file name \\\" LUCorpus - v0.3__20000410_nyt - NEW . xml \\\" is from the corpus named \\\" LUCorpus - v0.3 \\\" and the document name is \\\" 20000410_nyt - NEW . xml \\\" . Lists frame element objects . If ' name ' is provided , this is treated as a case - insensitive regular expression to filter by frame name . ( Case - insensitivity is because casing of frame element names is not always consistent across frames . ) frame . name , fe . name ) for fe in fn . name for fe in fn . fes ( ' ^sound$ ' ) ) 2 . Get the details for the specified Frame using the frame 's name or i d number . Usage examples : . frame ( ' Imposing_obligation ' ) frame ( 1494 ) : Imposing_obligation ... . The dict that is returned from this function will contain the following information about the Frame : . ' name ' : the name of the Frame ( e.g. ' Birth ' , ' Apply_heat ' , etc . ) . ' definition ' : textual definition of the Frame . ' ID ' : the internal ID number of the Frame . ' semTypes ' : a list of semantic types for this frame . Each item in the list is a dict containing the following keys : . ' name ' : can be used with the semtype ( ) function . ' ID ' : can be used with the semtype ( ) function . ' lexUnit ' : a dict containing all of the LUs for this frame . The keys in this dict are the names of the LUs and the value for each key is itself a dict containing info about the LU ( see the lu ( ) function for more info . ) FE ' : a dict containing the Frame Elements that are part of this frame . The keys in this dict are the names of the FEs ( e.g. ' Body_system ' ) and the values are dicts containing the following keys . definition \\\" This frame includes words that name ... \\\" . Also see the frame ( ) function for details about what is contained in the dict that is returned . Get the details for the specified Frame using the frame 's name . definition \\\" This frame includes words that name ... \\\" . frame - ( optional ) frame object , name , or ID ; only relations involving . frame_relations ( fn . frames ( r ' ( ? A brief intro to Frames ( excerpted from \\\" FrameNet II : Extended Theory and Practice \\\" by Ruppenhofer et . al . , 2010 ) : . A Frame is a script - like conceptual structure that describes a particular type of situation , object , or event along with the participants and props that are needed for that Frame . For example , the \\\" Apply_heat \\\" frame describes a common situation involving a Cook , some Food , and a Heating_Instrument , and is evoked by words such as bake , blanch , boil , broil , brown , simmer , steam , etc . . We call the roles of a Frame \\\" frame elements \\\" ( FEs ) and the frame - evoking words are called \\\" lexical units \\\" ( LUs ) . FrameNet includes relations between Frames . Several types of relations are defined , of which the most important are : . Inheritance : An IS - A relation . The child frame is a subtype of the parent frame , and each FE in the parent is bound to a corresponding FE in the child . An example is the \\\" Revenge \\\" frame which inherits from the \\\" Rewards_and_punishments \\\" frame . \"}",
        "_version_":1692670479883567104,
        "score":18.200869},
      {
        "id":"8b52ac2f-55e2-4886-ab44-62400e8112e8",
        "_src_":"{\"url\": \"https://www.noisebridge.net/index.php?title=Fermentation_logs&diff=17616&oldid=17044\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162648.4/warc/CC-MAIN-20160205193922-00086-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Last updated : August the fourth 2009 by bosco[at]di.unito.it . The validation of existing NLP models strongly depends on the possibility of generalizing their results on data and languages other than those on which they have been trained and tested , i.e. usually English . A valuable contribute to the validation of existing models and data comes from experiences that allow for consistent comparisons among approaches and representation schemes establishing shared standards , resources , tasks and evaluation practices with reference to various languages . In this perspective , the aim of the EVALITA events is to promote the development of language technologies for the Italian language , by providing a shared framework to evaluate different systems and approaches in a consistent manner . As in its first edition , EVALITA 2007 held in September 2007 , EVALITA 2009 aims at provide a shared framework where participants ' systems were evaluated on different tasks and linguistic resources for Italian . Therefore the task will be articulated into two different tracks , i.e. Dependency and Constituency Parsing . Dependency Parsing . It is articulated into two subtasks that provide the possibility of testing parsers across data differing in size , composition , granularity and annotation schemes : . In this new release , the treebank includes , in particular , a small portion of data shared with Passage , an evaluation campaign for parsing of French language , which are extracted from the JRC - Acquis Multilingual Parallel Corpus . The TANL dependency annotated corpus originates as a revision of the ISST - CoNLL corpus used in the multilingual track of the CONLL-2007 shared task , which was built in its turn starting from the Italian Syntactic - Semantic Treebank , in particular the morpho - syntactic and syntactic dependency annotation levels . All participants are strongly encouraged to perform both the dependency subtasks . Constituency Parsing . All participants are strongly encouraged to perform more subtasks and tracks . The current version of data for constituency task is updated at August the third 2009 ( for civillaw ) and at September the tenth ( for newspaper ) . ( Previous versions can be asked to the organizers ) . MAIN subtask : the development data are the three corpora of the release 2.1 of TUT ; they can be downloaded both in original TUT and CoNLL format from the following links : civillaw , newspaper , JRC - Passage - Evalita . PILOT subtask : the development data are articulated into two different sets : . Training Corpus , containing data annotated according to the TANL specifications to be used for training participating systems . Any further upgraded version of data , if available , will be announced to participants and published in this site . Requests of information and feedbacks about data are welcome and can be addressed to bosco[at]di.unito.it . \"}",
        "_version_":1692670970550026240,
        "score":17.886454},
      {
        "id":"97ef4972-e6b2-4297-a8ea-04cff8817b9f",
        "_src_":"{\"url\": \"http://www.borev.net/2009/05/how_to_read_a_juan_forero_stor.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701160958.15/warc/CC-MAIN-20160205193920-00026-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"SuperTypes ' and Logical Segmentation of Instances . The Message Understanding Conferences ( MUC ) were initiated in 1987 and financed by DARPA to encourage the development of new and better methods of information extraction ( IE ) . It was a seminal series that resulted in basic measures of retrieval and semantic efficacy , recall ( R ) and precision ( P ) and the combined F - measure , and other core terminology and constructs used by IE today . By the sixth version in the series ( MUC-6 ) , in 1995 , the task of recognition of named entities and coreference was added . That initial slate of named entities included the basic building blocks of person ( PER ) , location ( LOC ) , and organization ( ORG ) ; to these were added the numeric building blocks of time , percentage or quantity . The very terminology of named entity was coined for this seminal meeting , as was the idea of inline markup [ 1 ] . What is a ' Nameable Thing ' ? The intuition surrounding \\\" named entity \\\" and nameable \\\" things \\\" was that they were discrete and disjoint . A rock is not a person and is not a chemical or an event . As initially used , all \\\" named entities \\\" were distinct individuals . But , there also emerged the understanding that some classes of things could also be treated as more - or - less distinct nameable \\\" things \\\" : beetles are not the same as frogs and are not the same as rocks . While some of these \\\" things \\\" might be a true individual with a discrete name , such as Kermit the Frog , or The Rock at Northwestern University , most instances of such things are unnamed . The \\\" nameability \\\" ( or logical categorization ) of things is perhaps best kept separate from other epistemological issues of distinguishing sets , collections , or classes from individuals , members or instances . In a closed - world system it is easier to enforce clean distinctions . The Cyc knowledge base , for example , the basis for UMBEL ( Upper Mapping and Binding Exchange Layer ) , makes clear the distinction between individuals and collections . make such distinctions for individual beetles , let alone entire genera or species of beetles . Under precise conditions , these distinctions are important . The fact that Cyc , for example , is assiduous in its application of these distinctions is a major reason for the overall coherence of its knowledge base . This digression sets the background for a natural progression from that first MUC-6 conference . If we could cluster persons or organizations , why not other categories of distinct and disjoint things such as frogs or beetles or rocks ? From the first six entity categories of MUC-6 we begin to see an expansion to broader coverage . Again , the intuition was that real things in the real world could be logically categorized into discrete and disjoint categories . The real , fundamental point is that some \\\" things \\\" ( whether individuals , instances or classes ) are distinct from other \\\" things \\\" . Such disjoint distinctions are a powerful concept that should not be lost sight of by \\\" angels dancing on the head of a pin \\\" epistemological arguments . A frog is not a rock , despite neither are \\\" individuals \\\" , and how can we take advantage of that realilty ? What Works for Entities , Works for Concepts . As we probed the disjoint categories within the Sekine 200 entity types , for example , we began to see significant parallels and overlap . Also gnawing at our sense of order was the rather artificial and arbitrary class of concepts in UMBEL that we termed \\\" Abstract Concepts \\\" . We introduced Abstract Concepts in the first release of UMBEL . When introduced , we defined \\\" Abstract concepts [ as ] representing abstract or ephemeral notions such as truth , beauty , evil or justice , or [ as ] thought constructs useful to organizing or categorizing things but are not readily seen in the experiential world . \\\" In pragmatic terms , Abstract Concepts in UMBEL were often pivotal nodes in the UMBEL subject graph necessary to maintain a high degree of concept interconnectivity . In any world view that attempts to be more - or - less comprehensive , there is a gradation of concepts from the concrete and observable to the abstract and ephemeral . The recognition that some of these concepts may be more abstract , then , was not the issue . The issue was that there was no definable basis for segregating a concrete Subject Concept from the more Abstract Concept . Where was the bright line ? What was the actionable distinction ? Off and on we have probed this question for more than a year , and have looked at what might constitute a more natural and logical ordering and segmentation within UMBEL . After many tests and detailed analysis , we are now releasing the first results of our investigations . For , like nameable entities or things , we can see a logical segmentation of ( mostly ) disjoint concepts within the UMBEL TBox . Here are the summary percentages of these high - level splits : . Disjoint Concepts . Attributes . Classifications . TOTAL . ( Because the analysis is still being refined , exact counts and percentages for the 20,000 concepts in UMBEL are not provided . ) Why a Logical Segmentation ? As we dove deeper into these ideas , not only could we see the basis for a logical segmentation within UMBEL 's concepts , but manifest benefits from doing so as well . Remember that UMBEL 's concept structure performs two main roles . It : 1 ) provides a coherent framework for relating and \\\" mapping \\\" other external ontologies ; and 2 ) provides conceptual binding points for organizing entities and instances [ 4 ] . Via logical segmentation , we get benefits for both roles . Here are some of the broad areas of benefit from a logical UMBEL segmentation that we have identified : . Template - driven - as we discuss elsewhere , Structured Dynamics also uses its ontologies to \\\" drive applications \\\" and the user interfaces ( UI ) that support them . By proper segmentation of UMBEL concepts , we are able to determine to what \\\" cluster \\\" of things ( which we call either dimensions or superTypes ; see below ) a given thing belongs . This identification means we can also determine how best to display information about that \\\" thing \\\" . This determination can include either the attributes or the display templates appropriate for that thing . For example , location - based things or time - based things might invoke map or calendar or timeline type displays . Moreover , because of the logical segmentation of concepts , we can also use the power of the concept graph to infer more generic display templates when specific matches are absent . Computational Efficiency - as the percentages above indicate , once we identify what superType concept to which a given instance belongs , we can eliminate nearly all remaining UMBEL concepts from consideration . This logical winnowing leads to computational efficiencies at all levels in the system . The fastest computational work is not to do it , and when large chunks of data are removed from consideration , many performance advantages accrue . Disambiguation - via this approach we now can assess concept matches in addition to entity matches . This means we can triangulate between the two assessments to aid disambiguation . Because of these logical segmentations , we also have multiple \\\" clusters \\\" ( that is , either the concept , type , superType or dimension ) upon which to do our disambiguation evaluations , either between concepts and entities or within the various concept clusters . We can do so via either multiple semantic vectors ( for statistical - based methods ) or multiple features ( for machine learning methods ) . In other words , because of logical segmentation , we have increased the informational power of our concept graph . Structure and Integrity Testing - the very mindset of looking for logical segmentation has led to much learning about the UMBEL structure and OpenCyc upon which it is based . In the process , missing nodes ( concepts ) , erroneous assignments , and superfluous nodes are all being discovered . Further , many of these tests can be automated using basic logical and inference approaches . The net result is a constant improvement to the scope and completeness of the structure . Lastly , these same approaches can be applied when mapping external ontologies to UMBEL , providing similar consistency benefits . With these benefits in mind , we have undertaken concerted analysis of UMBEL to discern what this \\\" logical segmentation \\\" might be . This investigation has occurred over three concentrated periods over the past year . ( Intervening priorities or other work prevented concentrating solely on this task . ) We are now complete with our first full iteraton of investigation . In this post , and then the subsequent release of UMBEL version 0.80 in the coming weeks , the fruits of this effort should be evident . However , it should also be noted that we are still learning much from this new mindset and approach . UMBEL structure refinement may be likely for some time to come . UMBEL Analysis . Most things and concepts about them are based on real , observable , physical things in the real world . Because most of these things can not occupy both the same moment in time and the same location in physical space , a useful criterion for looking at these things and concepts is disjointedness . In a broad sense , then , we can split our concepts of the world between those ideas that are disjoint because they pertain to separable objects or ideas and those that are cross - cutting or organizational or classificatory . Attributes , such as color ( pink , for example ) , are often cross - cutting in that they can be used to describe quite disparate things . Inherent classification schemes such as academic fields of study or library catalog systems - while useful ways to organize the world - are not themselves in - and - of the world or discrete from other ideas . Thus , classificatory or organizational concepts are inherently not disjoint . With the criterion of disjointedness in hand , then , we began an evaluation process of the UMBEL subject concepts . We looked to organizational schema such as the entity types of Sekine or BBN for some starting guidance . We also kept in mind that we also wanted our categories to inform logical clusterings of possible data presentation , such as media types or locations or time . For terminology , we adopted the term superType to denote the largest cluster designation upon which this disjointedness may occur . As a way to test the basic coherence of these superTypes , we also collected them into larger groups which we termed dimensions . Our analysis process began with branch - by - branch testing of the UMBEL concept graph using automated scripts , attempting to find pivotal nodes where child instance members were disjoint from other superTypes . This we term the \\\" top - down \\\" method . This automated analysis was then supplemented with a complete manual inspection of all unassigned and assigned concepts , with a \\\" bottom up \\\" assignment of concepts or corrections to the automated approach . This inspection then led to new insights and identification of missing concepts that needed to be added into UMBEL . We are still converging between these two methods . Optimally , we should be able to tease out all UMBEL superTypes with a relatively few number of union , intersection , or complement set operations . In its current form , we are close , but there are still some rough spots . Nonetheless , this analysis method has led us to identify some 33 superTypes [ 5 ] , clustered into 9 dimensions . Of these , 29 superTypes and 8 dimensions are mostly disjoint . The one dimension of Classificatory includes the four cross - cutting superTypes of attributes and organizational schema that can apply to any of the 29 disjoint superTypes . UMBEL superTypes . This superType includes natural phenomena and natural processes such as weather , weathering , erosion , fires , lightning , earthquakes , tectonics , etc . Clouds and weather processes are specifically included . Also includes climate cycles , general natural events ( such as hurricanes ) that are not specifically named , and biochemical processes and pathways . Notable inclusions are minerals , compounds , chemicals , or physical objects that are not the outcome of purposeful human effort , but are found naturally occurring . Other natural objects ( such as rock , fossil , etc . ) are also found under this superType . The Earthscape superType consists mostly of the collection of cartographic features that occur on the surface of the Earth . Positive examples include Mountain , Ocean , and Mesa . Artificial features such as canals are excluded . Most instances of these features have a fixed location in space . Underground and underwater are also explicitly contained . This superType is explicitly disjoint with Extraterrestrial ( see below ) . This superType includes all plant types and flora , including flowering plants , algae , non - flowering plants , gymnosperms , cycads , and plant parts and body types . Note that all Plant Parts are also included . This large superType includes all animal types , including specific animal types and vertebrates , invertebrates , insects , crustaceans , fish , reptiles , amphibia , birds , mammals , and animal body parts . Animal parts are specifically included . Also , groupings of such animals are included . Humans , as an animal , are included ( versus as an individual Person ) . Diseases are specifically excluded . Diseases are atypical or unusual or unhealthy conditions for ( mostly human ) living things , generally known as conditions , disorders , infections , diseases or syndromes . Diseases only affect living things and sometimes are caused by living things . This superType also includes impairments , disease vectors , wounds and injuries , and poisoning . The appropriate superType for all named , individual human beings . This superType also includes the assignment of formal , honorific or cultural titles given to specific human individuals . It further includes names given to humans who conduct specific jobs or activities ( the latter case is known as an avocation ) . Examples include steelworker , waitress , lawyer , plumber , artisan . Ethnic groups are specifically included . Human Activities . Organizations . Organization is a broad superType and includes formal collections of humans , sometimes by legal means , charter , agreement or some mode of formal understanding . Examples include geopolitical entities such as nations , municipalities or countries ; or companies , institutes , governments , universities , militaries , political parties , game groups , international organizations , trade associations , etc . All institutions , for example , are organizations . Also included are informal collections of humans . Informal or less defined groupings of humans may result from ethnicity or tribes or nationality or from shared interests ( such as social networks or mailing lists ) or expertise ( \\\" communities of practice \\\" ) . This dimension also includes the notion of identifiable human groups with set members at any given point in time . Examples include music groups , cast members of a play , directors on a corporate Board , TV show members , gangs , mobs , juries , generations , minorities , etc . Finally , Organizations contain the concepts of Industries and Programs and Communities . This superType pertains to all things financial and with respect to the economy , including chartable company performance , stock index entities , money , local currencies , taxes , incomes , accounts and accounting , mortgages and property . This category includes concepts related to political systems , laws , rules or cultural mores governing societal or community behavior , or doctrinal , faith or religious bases or entities ( such as gods , angels , totems ) governing spiritual human matters . Culture , Issues , beliefs and various activisms ( most -isms ) are included . Human Works . Products . This is the largest superType and includes any instance offered for sale or performed as a commercial service . Often physical object made by humans that is not a conceptual work or a facility , such as vehicles , cars , trains , aircraft , spaceships , ships , foods , beverages , clothes , drugs , weapons . Products also include the concept of ' state ' ( e / g/. , on / off ) . All can be geospatially located . Facilities also include animal pens and enclosures and general human \\\" activity \\\" areas ( golf course , archeology sites , etc . ) . Importantly , Facilities include infrastructure systems such as roadways and physical networks . Facilities also include the component parts that go into making them ( such as foundations , doors , windows , roofs , etc . ) . Information . Chemistry ( n.o.c ) . This superType is a residual category ( n.o.c . , not otherwise categorized ) for chemical bonds , chemical composition groupings , and the like . It is formed by what is not a natural substance or living thing ( organic ) substance . Akin to conceptual works , these are codified means of human expression . This superType is for specific time or date or period ( such as eras , or days , weeks , months type intervals ) references in various formats . Descriptive . This general superType category is for descriptive attributes of all kinds . Think of the specific attributes in Wikipedia \\\" infoboxes \\\" to understand the purpose and coverage of this superType . It includes colors , shapes , sizes , or other descriptive characteristics about an object . Classificatory . Abstract - level . This general superType category is largely composed of former AbstractConcepts , and represent some of the more abstract upper - level nodes for connecting the UMBEL structure together . This superType also includes theories or processes or methods for humans to do stuff or any human technology . This largely subject - oriented superType is a means for using controlled vocabularies and classification schemes for characterizing what content \\\" is about \\\" . The key constituents of this category are Types , Classifications , Concepts , Topics , and controlled vocabularies . This superType is a specialized classificatory system for markets and industries . It could be combined with the superType above , but is kept separate in order to provide a separate , economy - oriented system . These may undergo some further refinement prior to release of UMBEL v 0.80 , and some of the definitions will be tightened up . ( Note : It should also be mentioned that some of these superTypes further lend themselves to further splits and analysis . The Product superType , for example , is ripe for such treatment . ) Distribution of superTypes . The following diagram shows the distribution of these 20,000 UMBEL concepts across major area . By far the largest superType is Products , even with further splits into Food and Drinks and Pharmaceuticals . The next largest categories are Person and Places and Events superTypes , with Organizations and Animals not far behind : . Even in its generic state , UMBEL provides a very rich vocabulary for describing things or for tying in more detailed external ontologies . There are nearly 5,000 concepts across products of all types , for example . Possible Overlaps ( non - disjoint ) between superTypes . You may recall that our analysis showed 29 of the superTypes to be \\\" mostly disjoint . \\\" This is because there are some concepts - say , MusicPerformingAgent - that can apply to either a person or a group ( band or orchestra , for example ) . Thus , for this concept alone , we have a bit of overlap between the normally disjoint Person and Organization superTypes . The following shows the resulting interaction matrix where there may be some overlap between superTypes : . This kind of interaction diagram is also useful for further analyzing the concept graph structure , as well . Even Where Overlaps Occur , They are Minor . Of the 29 \\\" mostly \\\" disjoint superTypes , only a relatively few show potential interactions , and then only in minor ways . We can illustrate this ( drawn to scale ) for the interaction between the Product , Food & Drink and Drug ( Pharmaceuticals ) superTypes , with the fully disjoint Organization superType thrown in for comparison : . Across all 20,000 concepts , then , fully 85 % are disjoint from one another ( 5 % is lost due to overlaps between \\\" mostly \\\" disjoint superTypes ) . This is a surprising high percentage , with even better likelihood to deliver the benefits previously noted . Interim Conclusions and Observations . These are exciting findings that bode well for UMBEL 's ongoing role and usefulness . Also , the very detailed analysis that has led to these interim findings very much reaffirms the wisdom of basing UMBEL on Cyc . Cyc showed itself to be admirably coherent and remarkably complete . ( It also appears that the first versions of UMBEL were also extracted well in terms of good coverage . ) This approach now gives us an understandable and defensible basis for logical segementation of UMBEL . It also provides a much - desired alternative to the earlier Abstract Concepts , which will now be dropped entirely as a schema concept . One area deserving further attention is in the Attribute superType . We are in the process , for example , of analyzing attributes across Wikipedia and need to look through a slightly different lens at this superType [ 6 ] . This area is further important in its strong interaction with the Instance Record Vocabulary that is accompanying this effort on the entity side . Another lesson for us has been to back away from the terminology of named entity , introduced at MUC-6 . The expansions of that idea into other \\\" nameable \\\" things has caused us to embrace the \\\" instance \\\" nomenclature , as evidenced by our emerging IRV . It is rewarding to prepare this next iteration release of UMBEL with its new mindset of logical segmentation and disjointedness . But - what is also clear - there are many treasures left to mine still hidden in the inherent structure of UMBEL and its Cyc parent . [ 1 ] The original labels were ENAMEX for entity named expression and NUMEX for numeric expression . The markup format specified was also SGML . For an interesting history of this MUC-6 watershed , see Ralph Grishman and Beth Sundheim , 1996 . Message Understanding Conference - 6 : A Brief History , in Proceedings of the 16th International Conference on Computational Linguistics ( COLING ) , I , Kopenhagen , 1996 , 466 - 471 . [ 2 ] In a named entity , the word named applies to entities that have a \\\" rigid designators \\\" as defined by Kripke for the referent . For instance , the automotive company created by Henry Ford in 1903 is referred to as Ford or Ford Motor Company . Rigid designators include proper names as well as certain natural kind of terms like biological species and substances . Sekine 's extended hierarchy proposed in 2002 is made up of 200 subtypes , with 32 larger clusters within that . Here is the top level of the Sekine type system : . Point . Measurement . Event . Color . Percent . Countx . Natural Object . Time - Other . Multiplication . Ordinal Number . Though developed separately and for different purposes , BBN categories also proposed in 2002 consists of 29 types and 64 subtypes . Here are the BBN types ( Note : BBN claims 29 types because there are double entries or considerations for the first five entries ) : . Law . Location . Cardinal . Language . Product . Events . Contact Info . Date . Plant . Game . Of course , other entity extraction systems have similar clusterings and approaches . Though less formal in the sense of a hierarchy or purported complete entity coverage , here for example is the listing of entity types within Calais : . \\\" Description logics and their semantics traditionally split concepts and their relationships from the different treatment of instances and their attributes and roles , expressed as fact assertions . The concept split is known as the TBox ( for terminological knowledge , the basis for T in TBox ) and represents the schema or taxonomy of the domain at hand . The TBox is the structural and intensional component of conceptual relationships . [ 4 ] UMBEL also provides a SKOS -based vocabulary extension for describing other domains and mappings between classes and instances . This purpose , however , is outside of the scope of this current article . [ 5 ] As a reference roadmap , UMBEL was specifically designed not to include meronymous ( part of ) relationships ( see further this reference ) . Thus , all \\\" part of \\\" type concepts were assigned to the whole superType category for which they are a part . Thus , \\\" animal parts \\\" are assigned to the superType Animal ; \\\" car parts \\\" to the superType Product . [ 6 ] For a general discussion of attributes and their relation to entities , see Satoshi Sekine , 2008 . Extended Named Entity Ontology with Attribute Information , in Proceedings of the 6th edition of the Language Resources and Evaluation Conference ( LREC 2008 ) . Marrakech , Morocco . This analysis of UMBEL has led us to identify some 33 superTypes clustered into 9 dimensions . Of these , 29 superTypes and 8 dimensions are mostly disjoint . Via logical segmentation , we get benefits in assigning reporting templates , computational efficiency , disambiguation and structure and integrity testing . \"}",
        "_version_":1692669405039689728,
        "score":17.508963}]
  }}
