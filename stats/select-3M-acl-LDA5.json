{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"translation structure representation number terms semantic proposed machine work annotation head problem context sato theory language galley represent kilgarriff process",
      "fl":"*,score"}},
  "response":{"numFound":1689389,"start":0,"maxScore":26.603703,"numFoundExact":true,"docs":[
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":26.603703},
      {
        "id":"e507deec-5e9a-4ecf-a357-26a34f9ddc97",
        "_src_":"{\"url\": \"http://boingboing.net/tag/walmart\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701145751.1/warc/CC-MAIN-20160205193905-00103-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Glossary of Semantic Technology Terms . There are many semantic technology terms relevant to the context of a semantic technology installation [ 1 ] . Some of these are general terms related to language standards , as well as to ontologies or the dataset concept . An ABox ( for assertions , the basis for A in ABox ) is an \\\" assertion component \\\" ; that is , a fact associated with a terminological vocabulary within a knowledge base . ABox are TBox -compliant statements about instances belonging to the concept of an ontology . An annotation , specifically as an annotation property , is a way to provide metadata or to describe vocabularies and properties used within an ontology . Annotations do not participate in reasoning or coherency testing for ontologies . The name Atom applies to a pair of related standards . The Atom Syndication Format is an XML language used for web feeds , while the Atom Publishing Protocol ( APP for short ) is a simple HTTP - based protocol for creating and updating Web resources . These are the aspects , properties , features , characteristics , or parameters that objects ( and classes ) may have . They are the descriptive characteristics of a thing . Key - value pairs match an attribute with a value ; the value may be a reference to another object , an actual value or a descriptive label or string . In an RDF statement , an attribute is expressed as a property ( or predicate or relation ) . In intensional logic , all attributes or characteristics of similarly classifiable items define the membership in that set . A class is a collection of sets or instances ( or sometimes other mathematical objects ) which can be unambiguously defined by a property that all of its members share . In ontologies , classes may also be known as sets , collections , concepts , types of objects , or kinds of things . CWA is the presumption that what is not currently known to be true , is false . CWA also has a logical formalization . CWA is the most common logic applied to relational database systems , and is particularly useful for transaction - type systems . See contrast to the open world assumption . Data Space . A data space may be personal , collective or topical , and is a virtual \\\" container \\\" for related information irrespective of storage location , schema or structure . Dataset . An aggregation of similar kinds of things or items , mostly comprised of instance records . A project that extracts structured content from Wikipedia , and then makes that data available as linked data . There are millions of entities characterized by DBpedia in this way . As such , DBpedia is one of the largest - and most central - hubs for linked data on the Web . Description logics and their semantics traditionally split concepts and their relationships from the different treatment of instances and their attributes and roles , expressed as fact assertions . The concept split is known as the TBox and represents the schema or taxonomy of the domain at hand . The TBox is the structural and intensional component of conceptual relationships . The second split of instances is known as the ABox and describes the attributes of instances ( and individuals ) , the roles between instances , and other assertions about instances regarding their class membership with the TBox concepts . Domain ( or content ) ontologies embody more of the traditional ontology functions such as information interoperability , inferencing , reasoning and conceptual and knowledge capture of the applicable domain . Entity . An individual object or member of a class ; when affixed with a proper name or label is also known as a named entity ( thus , named entities are a subset of all entities ) . EAV is a data model to describe entities where the number of attributes ( properties , parameters ) that can be used to describe them is potentially vast , but the number that will actually apply to a given entity is relatively modest . In the EAV data model , each attribute - value pair is a fact describing an entity . EAV systems trade off simplicity in the physical and logical structure of the data for complexity in their metadata , which , among other things , plays the role that database constraints and referential integrity do in standard database designs . The extension of a class , concept , idea , or sign consists of the things to which it applies , in contrast with its intension . For example , the extension of the word \\\" dog \\\" is the set of all ( past , present and future ) dogs in the world . The extension is most akin to the attributes or characteristics of the instances in a set defining its class membership . GRDDL is a markup format for Gleaning Resource Descriptions from Dialects of Languages ; that is , for getting RDF data out of XML and XHTML documents using explicitly associated transformation algorithms , typically represented in XSLT . High - level Subject . A high - level subject is both a subject proxy and category label used in a hierarchical subject classification scheme ( taxonomy ) . Higher - level subjects are classes for more atomic subjects , with the height of the level representing broader or more aggregate classes . Inference is the act or process of deriving logical conclusions from premises known or assumed to be true . The logic within and between statements in an ontology is the basis for inferring new conclusions from it , using software applications known as inference engines or reasoners . Instances are the basic , \\\" ground level \\\" components of an ontology . An instance is individual member of a class , also used synonomously with entity . The instances in an ontology may include concrete objects such as people , animals , tables , automobiles , molecules , and planets , as well as abstract instances such as numbers and words . An instance is also known as an individual , with member and entity also used somewhat interchangeably . irON ( instance record and Object Notation ) is a abstract notation and associated vocabulary for specifying RDF ( Resource Description Framework ) triples and schema in non - RDF forms . Its purpose is to allow users and tools in non - RDF formats to stage interoperable datasets using RDF . The intension of a class is what is intended as a definition of what characteristics its members should have ; it is akin to a definition of a concept and what is intended for a class to contain . It is therefore like the schema aspects ( or TBox ) in an ontology . Key - value pair . Also known as a name - value pair or attribute - value pair , a key - value pair is a fundamental , open - ended data representation . The key is the defined attribute and the value may be a reference to another object or a literal string or value . In RDF triple terms , the subject is implied in a key - value pair by nature of the instance record at hand . A knowledge base ( abbreviated KB or kb ) is a special kind of database for knowledge management . A knowledge base provides a means for information to be collected , organized , shared , searched and utilized . Formally , the combination of a TBox and ABox is a knowledge base . Linkage . A specification that relates an object or attribute name to its full URI ( as required in the RDF language ) . Linked data is a set of best practices for publishing and deploying instance and class data using the RDF data model , and uses uniform resource identifiers ( URIs ) to name the data objects . The approach exposes the data for access via the HTTP protocol , while emphasizing data interconnections , interrelationships and context useful to both humans and machine agents . Mapping . A considered correlation of objects in two different sources to one another , with the relation between the objects defined via a specific property . Linkage is a subset of possible mappings . It is \\\" data about data \\\" , or the means by which data objects or aggregations can be described . Contrasted to an attribute , which is an individual characteristic intrinsic to a data object or instance , metadata is a description about that data , such as how or when created or by whom . Microdata is a proposed specification used to nest semantics within existing content on web pages . Microdata is an attempt to provide a simpler way of annotating HTML elements with machine - readable tags than the similar approaches of using RDFa or microformats . A microformat ( sometimes abbreviated \\u03bcF or uF ) is a piece of mark up that allows expression of semantics in an HTML ( or XHTML ) web page . Programs can extract meaning from a web page that is marked up with one or more microformats . NLP is the process of a computer extracting meaningful information from natural language input and/or producing natural language output . NLP is one method for assigning structured data characterizations to text content for use in semantic technologies . ( Hand assignment is another method . ) OBIE . Information extraction ( IE ) is the task of automatically extracting structured information from unstructured and/or semi - structured machine - readable documents . Ontology - based information extraction ( OBIE ) is the use of an ontology to inform a \\\" tagger \\\" or information extraction program when doing natural language processing . Input ontologies thus become the basis for generating metadata tags when tagging text or documents . An ontology is a data model that represents a set of concepts within a domain and the relationships between those concepts . Loosely defined , ontologies on the Web can have a broad range of formalism , or expressiveness or reasoning power . Ontology - driven applications ( or ODapps ) are modular , generic software applications designed to operate in accordance with the specifications contained in one or more ontologies . The relationships and structure of the information driving these applications are based on the standard functions and roles of ontologies ( namely as domain ontologies ) , as supplemented by UI and instruction sets and validations and rules . The open semantic framework , or OSF , is a combination of a layered architecture and an open - source , modular software stack . The stack combines many leading third - party software packages with open source semantic technology developments from Structured Dynamics . OWA is a formal logic assumption that the truth - value of a statement is independent of whether or not it is known by any single observer or agent to be true . OWA is used in knowledge representation to codify the informal notion that in general no single agent or observer has complete knowledge , and therefore can not make the closed world assumption . The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true . OWA is useful when we represent knowledge within a system as we discover it , and where we can not guarantee that we have discovered or will discover complete information . In the OWA , statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown , rather than wrong or false . Semantic Web languages such as OWL make the open world assumption . See contrast to the closed world assumption . The Web Ontology Language ( OWL ) is designed for defining and instantiating formal Web ontologies . An OWL ontology may include descriptions of classes , along with their related properties and instances . There are also a variety of OWL dialects . Properties are the ways in which classes and instances can be related to one another . Properties are thus a relationship , and are also known as predicates . Properties are used to define an attribute relation for an instance . In computer science , punning refers to a programming technique that subverts or circumvents the type system of a programming language , by allowing a value of a certain type to be manipulated as a value of a different type . When used for ontologies , it means to treat a thing as both a class and an instance , with the use depending on context . Resource Description Framework ( RDF ) is a family of World Wide Web Consortium ( W3C ) specifications originally designed as a metadata model but which has come to be used as a general method of modeling information , through a variety of syntax formats . The RDF metadata model is based upon the idea of making statements about resources in the form of subject - predicate - object expressions , called triples in RDF terminology . The subject denotes the resource , and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object . RDFa 1.0 is a set of extensions to XHTML that is a W3C Recommendation . RDFa uses attributes from meta and link elements , and generalizes them so that they are usable on all elements allowing annotation markup with semantics . A W3C Working draft is presently underway that expands RDFa into version 1.1 with HTML5 and SVG support , among other changes . RDFS or RDF Schema is an extensible knowledge representation language , providing basic elements for the description of ontologies , otherwise called RDF vocabularies , intended to structure RDF resources . A semantic reasoner , reasoning engine , rules engine , or simply a reasoner , is a piece of software able to infer logical consequences from a set of asserted facts or axioms . The notion of a semantic reasoner generalizes that of an inference engine , by providing a richer set of mechanisms . Reasoning . Reasoning is one of many logical tests using inference rules as commonly specified by means of an ontology language , and often a description language . Many reasoners use first - order predicate logic to perform reasoning ; inference commonly proceeds by forward chaining or backward chaining . Schema.org is an initiative launched by the major search engines of Bing , Google and Yahoo ! , and later jointed by Yandex , in order to create and support a common set of schemas for structured data markup on web pages . schema.org provided a starter set of schema and extension mechanisms for adding to them . schema.org supports markup in microdata , microformat and RDFa formats . Semantic technologies are a combination of software and semantic specifications that encodes meanings separately from data and content files and separately from application code . This approach enables machines as well as people to understand , share and reason with data and specifications separately . With semantic technologies , adding , changing and implementing new relationships or interconnecting programs in a different way can be as simple as changing the external model that these programs share . New data can also be brought into the system and visualized or worked upon based on the existing schema . Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data , content , and processes . The Semantic Web is a collaborative movement led by the World Wide Web Consortium ( W3C ) that promotes common formats for data on the World Wide Web . By encouraging the inclusion of semantic content in web pages , the Semantic Web aims at converting the current web of unstructured documents into a \\\" web of data \\\" . It builds on the W3C 's Resource Description Framework ( RDF ) . A semset is the use of a series of alternate labels and terms to describe a concept or entity . These alternatives include true synonyms , but may also be more expansive and include jargon , slang , acronyms or alternative terms that usage suggests refers to the same concept . Semantically - Interlinked Online Communities Project ( SIOC ) is based on RDF and is an ontology defined using RDFS for interconnecting discussion methods such as blogs , forums and mailing lists to each other . SKOS or Simple Knowledge Organisation System is a family of formal languages designed for representation of thesauri , classification schemes , taxonomies , subject - heading systems , or any other type of structured controlled vocabulary ; it is built upon RDF and RDFS . A statement is a \\\" triple \\\" in an ontology , which consists of a subject - predicate - object ( S - P - O ) assertion . By definition , each statement is a \\\" fact \\\" or axiom within an ontology . Subject . A subject is always a noun or compound noun and is a reference or definition to a particular object , thing or topic , or groups of such items . Subjects are also often referred to as concepts or topics . Subject extraction . Subject extraction is an automatic process for retrieving and selecting subject names from existing knowledge bases or data sets . Extraction methods involve parsing and tokenization , and then generally the application of one or more information extraction techniques or algorithms . Subject proxy . A subject proxy as a canonical name or label for a particular object ; other terms or controlled vocabularies may be mapped to this label to assist disambiguation . A subject proxy is always representative of its object but is not the object itself . A tag is a keyword or term associated with or assigned to a piece of information ( e.g. , a picture , article , or video clip ) , thus describing the item and enabling keyword - based classification of information . Tags are usually chosen informally by either the creator or consumer of the item . A TBox ( for terminological knowledge , the basis for T in TBox ) is a \\\" terminological component \\\" ; that is , a conceptualization associated with a set of facts . TBox statements describe a conceptualization , a set of concepts and properties for these concepts . The TBox is sufficient to describe an ontology ( best practice often suggests keeping a split between instance records - and ABox - and the TBox schema ) . In the context of knowledge systems , taxonomy is the hierarchical classification of entities of interest of an enterprise , organization or administration , used to classify documents , digital assets and other information . Taxonomies can cover virtually any type of physical or conceptual entities ( products , processes , knowledge fields , human groups , etc . ) at any level of granularity . The topic ( or theme ) is the part of the proposition that is being talked about ( predicated ) . In topic maps , the topic may represent any concept , from people , countries , and organizations to software modules , individual files , and events . Topics and subjects are closely related . Topic maps are an ISO standard for the representation and interchange of knowledge . A topic map represents information using topics , associations ( similar to a predicate relationship ) , and occurrences ( which represent relationships between topics and information resources relevant to them ) , quite similar in concept to the RDF triple . Triple . A basic statement in the RDF language , which is comprised of a subject - property - object construct , with the subject and property ( and object optionally ) referenced by URIs . This vocabulary is also designed for interoperable domain ontologies . An upper ontology ( also known as a top - level ontology or foundation ontology ) is an ontology that describes very general concepts that are the same across all knowledge domains . An important function of an upper ontology is to support very broad semantic interoperability between a large number of ontologies that are accessible ranking \\\" under \\\" this upper ontology . A vocabulary in the sense of knowledge systems or ontologies are controlled vocabularies . They provide a way to organize knowledge for subsequent retrieval . They are used in subject indexing schemes , subject headings , thesauri , taxonomies and other form of knowledge organization systems . WordNet is a lexical database for the English language . It groups English words into sets of synonyms called synsets , provides short , general definitions , and records the various semantic relations between these synonym sets . The purpose is twofold : to produce a combination of dictionary and thesaurus that is more intuitively usable , and to support automatic text analysis and artificial intelligence applications . The database and software tools can be downloaded and used freely . Multiple language versions exist , and WordNet is a frequent reference structure for semantic applications . Schema.org Markup . There are many semantic technology terms relevant to the context of a semantic technology installation [ 1]. Some of these are general terms related to language standards , as well as to ontologies or the dataset concept . ABox An ABox ( for assertions , the basis for A in ABox ) is an \\\" assertion component \\\" ; that is , a fact associated [ ... ] . 5 thoughts on \\\" Glossary of Semantic Technology Terms \\\" . Great resource , but there are some inaccuracies : RDFa is a W3C Recommendation , not a proposal , and it is for HTML , SVG and many others in addition to XHTML . HTML+RDFa is still a Working draft , as is microdata and other HTML5 related specs . \"}",
        "_version_":1692670548813807617,
        "score":24.248074},
      {
        "id":"08b29993-1bbb-4d31-a693-ec313b2adbb6",
        "_src_":"{\"url\": \"http://alaninbelfast.blogspot.com/2006/11/death-and-penguin-andrey-kurkov.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701962902.70/warc/CC-MAIN-20160205195242-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In natural language processing , word sense disambiguation ( WSD ) is the problem of determining which \\\" sense \\\" ( meaning ) of a word is activated by the use of the word in a particular context , a process which appears to be largely unconscious in people . WSD is a natural classification problem : Given a word and its possible senses , as defined by a dictionary , classify an occurrence of the word in context into one or more of its sense classes . The features of the context ( such as neighboring words ) provide the evidence for classification . A famous example is to determine the sense of pen in the following passage ( Bar - Hillel 1960 ) : . Little John was looking for his toy box . Finally he found it . The box was in the pen . John was very happy . playpen , pen - a portable enclosure in which babies may be left to play . penitentiary , pen - a correctional institution for those convicted of major crimes . pen - female swan . Research has progressed steadily to the point where WSD systems achieve consistent levels of accuracy on a variety of word types and ambiguities . Among these , supervised learning approaches have been the most successful algorithms to date . Current accuracy is difficult to state without a host of caveats . On English , accuracy at the coarse - grained ( homograph ) level is routinely above 90 % , with some methods on particular homographs achieving over 96 % . WSD was first formulated as a distinct computational task during the early days of machine translation in the 1940s , making it one of the oldest problems in computational linguistics . Warren Weaver , in his famous 1949 memorandum on translation , first introduced the problem in a computational context . Early researchers understood well the significance and difficulty of WSD . In fact , Bar - Hillel ( 1960 ) used the above example to argue that WSD could not be solved by \\\" electronic computer \\\" because of the need in general to model all world knowledge . In the 1970s , WSD was a subtask of semantic interpretation systems developed within the field of artificial intelligence , but since WSD systems were largely rule - based and hand - coded they were prone to a knowledge acquisition bottleneck . By the 1980s large - scale lexical resources , such as the Oxford Advanced Learner 's Dictionary of Current English ( OALD ) , became available : hand - coding was replaced with knowledge automatically extracted from these resources , but disambiguation was still knowledge - based or dictionary - based . In the 1990s , the statistical revolution swept through computational linguistics , and WSD became a paradigm problem on which to apply supervised machine learning techniques . The 2000s saw supervised techniques reach a plateau in accuracy , and so attention has shifted to coarser - grained senses , domain adaptation , semi - supervised and unsupervised corpus - based systems , combinations of different methods , and the return of knowledge - based systems via graph - based methods . Still , supervised systems continue to perform best . Applications . The utility of WSD . There is no doubt that the above applications require and use word sense disambiguation in one form or another . However , WSD as a separate module has not yet been shown to make a decisive difference in any application . There are a few recent results that show small positive effects in , for example , machine translation , but WSD has also been shown to hurt performance , as is the case in well - known experiments in information retrieval . There are several possible reasons for this . First , the domain of an application often constrains the number of senses a word can have ( e.g. , one would not expect to see the ' river side ' sense of bank in a financial application ) , and so lexicons can and have been constructed accordingly . Second , WSD might not be accurate enough yet to show an effect and moreover the sense inventory used is unlikely to match the specific sense distinctions required by the application . Third , treating WSD as a separate component or module may be misguided , as it might have to be more tightly integrated as an implicit process ( i.e. , as mutual disambiguation , below ) . Machine translation . WSD is required for lexical choice in MT for words that have different translations for different senses . For example , in an English - French financial news translator , the English noun change could translate to either changement ( ' transformation ' ) or monnaie ( ' pocket money ' ) . However , most translation systems do not use a separate WSD module . The lexicon is often pre - disambiguated for a given domain , or hand - crafted rules are devised , or WSD is folded into a statistical translation model , where words are translated within phrases which thereby provide context . Information retrieval . Ambiguity has to be resolved in some queries . For instance , given the query \\\" depression \\\" should the system return documents about illness , weather systems , or economics ? Current IR systems ( such as Web search engines ) , like MT , do not use a WSD module ; they rely on the user typing enough context in the query to only retrieve documents relevant to the intended sense ( e.g. , \\\" tropical depression \\\" ) . In a process called mutual disambiguation , reminiscent of the Lesk method ( below ) , all the ambiguous words are disambiguated by virtue of the intended senses co - occurring in the same document . Information extraction and knowledge acquisition . In information extraction and text mining , WSD is required for the accurate analysis of text in many applications . For instance , an intelligence gathering system might need to flag up references to , say , illegal drugs , rather than medical drugs . Bioinformatics research requires the relationships between genes and gene products to be catalogued from the vast scientific literature ; however , genes and their proteins often have the same name . More generally , the Semantic Web requires automatic annotation of documents according to a reference ontology . WSD is only beginning to be applied in these areas . Methods . There are four conventional approaches to WSD : . Dictionary- and knowledge - based methods : These rely primarily on dictionaries , thesauri , and lexical knowledge bases , without using any corpus evidence . Supervised methods : These make use of sense - annotated corpora to train from . Semi - supervised or minimally - supervised methods : These make use of a secondary source of knowledge such as a small annotated corpus as seed data in a bootstrapping process , or a word - aligned bilingual corpus . Unsupervised methods : These eschew ( almost ) completely external information and work directly from raw unannotated corpora . These methods are also known under the name of word sense discrimination . Dictionary- and knowledge - based methods . The Lesk method ( Lesk 1986 ) is the seminal dictionary - based method . It is based on the hypothesis that words used together in text are related to each other and that the relation can be observed in the definitions of the words and their senses . Two ( or more ) words are disambiguated by finding the pair of dictionary senses with the greatest word overlap in their dictionary definitions . For example , when disambiguating the words in pine cone , the definitions of the appropriate senses both include the words evergreen and tree ( at least in one dictionary ) . An alternative to the use of the definitions is to consider general word - sense relatedness and to compute the semantic similarity of each pair of word senses based on a given lexical knowledge - base such as WordNet . Graph - based methods reminiscent of spreading - activation research of the early days of AI research have been applied with some success . The use of selectional preferences ( or selectional restrictions ) are also useful . For example , knowing that one typically cooks food , one can disambiguate the word bass in I am cooking bass ( i.e. , it 's not a musical instrument ) . Supervised methods . Supervised methods are based on the assumption that the context can provide enough evidence on its own to disambiguate words ( hence , world knowledge and reasoning are deemed unnecessary ) . Probably every machine learning algorithm going has been applied to WSD , including associated techniques such as feature selection , parameter optimization , and ensemble learning . Support vector machines and memory - based learning have been shown to be the most successful approaches , to date , probably because they can cope with the high - dimensionality of the feature space . However , these supervised methods are subject to a new knowledge acquisition bottleneck since they rely on substantial amounts of manually sense - tagged corpora for training , which are laborious and expensive to create . Semi - supervised methods . The bootstrapping approach starts from a small amount of seed data for each word : either manually - tagged training examples or a small number of surefire decision rules ( e.g. , play in the context of bass almost always indicates the musical instrument ) . The seeds are used to train an initial classifier , using any supervised method . This classifier is then used on the untagged portion of the corpus to extract a larger training set , in which only the most confident classifications are included . The process repeats , each new classifier being trained on a successively larger training corpus , until the whole corpus is consumed , or until a given maximum number of iterations is reached . Other semi - supervised techniques use large quantities of untagged corpora to provide co - occurrence information that supplements the tagged corpora . These techniques have the potential to help in the adaptation of supervised models to different domains . Also , an ambiguous word in one language is often translated into different words in a second language depending on the sense of the word . Word - aligned bilingual corpora have been used to infer cross - lingual sense distinctions , a kind of semi - supervised system . Unsupervised methods . Unsupervised learning is the greatest challenge for WSD researchers . The underlying assumption is that similar senses occur in similar contexts , and thus senses can be induced from text by clustering word occurrences using some measure of similarity of context . Then , new occurrences of the word can be classified into the closest induced clusters / senses . Performance has been lower than other methods , above , but comparisons are difficult since senses induced must be mapped to a known dictionary of word senses . Alternatively , if a mapping to a set of dictionary senses is not desired , cluster - based evaluations ( including measures of entropy and purity ) can be performed . It is hoped that unsupervised learning will overcome the knowledge acquisition bottleneck because they are not dependent on manual effort . Evaluation . The evaluation of WSD systems requires a test corpus hand - annotated with the target or correct senses , and assumes that such a corpus can be constructed . Two main performance measures are used : . Precision : the fraction of system assignments made that are correct . Recall : the fraction of total word instances correctly assigned by a system . If a system makes an assignment for every word , then precision and recall are the same , and can be called accuracy . This model has been extended to take into account systems that return a set of senses with weights for each occurrence . There are two kinds of test corpora : . Lexical sample : the occurrences of a small sample of target words need to be disambiguated , and . All - words : all the words in a piece of running text need to be disambiguated . In order to define common evaluation datasets and procedures , public evaluation campaigns have been organized . Senseval has been run three times : Senseval-1 ( 1998 ) , Senseval-2 ( 2001 ) , Senseval-3 ( 2004 ) , and its successor , SemEval ( 2007 ) , once . Why is WSD hard ? This article discusses the common and traditional characterization of WSD as an explicit and separate process of disambiguation with respect to a fixed inventory of word senses . Words are typically assumed to have a finite and discrete set of senses , a gross simplification of the complexity of word meaning , as studied in lexical semantics . While this characterization has been fruitful for research into WSD per se , it is somewhat at odds with what seems to be needed in real applications , as discussed above . WSD is hard for many reasons , three of which are discussed here . A sense inventory can not be task - independent . A task - independent sense inventory is not a coherent concept : each task requires its own division of word meaning into senses relevant to the task . For example , the ambiguity of mouse ( animal or device ) is not relevant in English - French machine translation , but is relevant in information retrieval . The opposite is true of river , which requires a choice in French ( fleuve ' flows into the sea ' , or rivi\\u00e8re ' flows into a river ' ) . Different algorithms for different applications . Completely different algorithms might be required by different applications . In machine translation , the problem takes the form of target word selection . Here the \\\" senses \\\" are words in the target language , which often correspond to significant meaning distinctions in the source language ( bank could translate to French banque ' financial bank ' or rive ' edge of river ' ) . In information retrieval , a sense inventory is not necessarily required , because it is enough to know that a word is used in the same sense in the query and a retrieved document ; what sense that is , is unimportant . Word meaning does not divide up into discrete senses . Finally , the very notion of \\\" word sense \\\" is slippery and controversial . Most people can agree in distinctions at the coarse - grained homograph level ( e.g. , pen as writing instrument or enclosure ) , but go down one level to fine - grained polysemy , and disagreements arise . For example , in Senseval-2 , which used fine - grained sense distinctions , human annotators agreed in only 85 % of word occurrences . Word meaning is in principle infinitely variable and context sensitive . It does not divide up easily into distinct or discrete sub - meanings . Lexicographers frequently discover in corpora loose and overlapping word meanings , and standard or conventional meanings extended , modulated , and exploited in a bewildering variety of ways . The art of lexicography is to generalize from the corpus to definitions that evoke and explain the full range of meaning of a word , making it seem like words are well - behaved semantically . However , it is not at all clear if these same meaning distinctions are applicable in computational applications , as the decisions of lexicographers are usually driven by other considerations . References . Suggested reading . Agirre , Eneko & Philip Edmonds ( eds . ) Word Sense Disambiguation : Algorithms and Applications . Dordrecht : Springer . Bar - Hillel , Yehoshua . Language and Information . New York : Addison - Wesley . Edmonds , Philip & Adam Kilgarriff . Introduction to the special issue on evaluating word sense disambiguation systems . Journal of Natural Language Engineering , 8(4):279 - 291 . Edmonds , Philip . Lexical disambiguation . The Elsevier Encyclopedia of Language and Linguistics , 2nd Ed . , ed . by Keith Brown , 607 - 23 . Oxford : Elsevier . Ide , Nancy & Jean V\\u00e9ronis . Word sense disambiguation : The state of the art . Computational Linguistics , 24(1):1 - 40 . Jurafsky , Daniel & James H. Martin . Speech and Language Processing . New Jersey , USA : Prentice Hall . Lesk , Michael . Automatic sense disambiguation using machine readable dictionaries : How to tell a pine cone from an ice cream cone . Proceedings of SIGDOC-86 : 5th International Conference on Systems Documentation , Toronto , Canada , 24 - 26 . Manning , Christopher D. & Hinrich Sch\\u00fctze . Foundations of Statistical Natural Language Processing . Cambridge , MA : MIT Press . Mihalcea , Rada . Word sense disambiguation . Encyclopedia of Machine Learning . Springer - Verlag . Resnik , Philip and David Yarowsky . Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation , Natural Language Engineering , 5(2):113 - 133 . Sch\\u00fctze , Hinrich . Automatic word sense discrimination . Computational Linguistics , 24(1):97 - 123 . Weaver , Warren . Translation . In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press . Yarowsky , David . Unsupervised word sense disambiguation rivaling supervised methods . Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics , 189 - 196 . Yarowsky , David . Word sense disambiguation . Handbook of Natural Language Processing , ed . by Dale et al . , 629 - 654 . New York : Marcel Dekker . \"}",
        "_version_":1692669505775337472,
        "score":23.16445},
      {
        "id":"046ea5e9-6431-475e-9e48-aa76ffc1250a",
        "_src_":"{\"url\": \"http://www.davidbrewster.com/2007/05/07/in-fact-web-20-is-simplicity-defined/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168076.20/warc/CC-MAIN-20160205193928-00332-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Acronyms & Glossary . Glossary . There are terms relevant to artificial intelligence , knowledge bases and semantic technologies . The \\\" official \\\" ones used by Structured Dynamics in its various projects and products are provided in alphabetical order below . Most definitions are from Wikipedia or standards groups , except in those cases where they are terms of art of SD initiatives . Terms in bold are found elsewhere in the glossary . Acronyms . Listed at the bottom part of this page are acronyms and definitions related to artificial intelligence , knowledge bases and semantic technologies . Most definitions are from Wikipedia , with the remaining from the appropriate standards group . Glossary Listings . An ABox ( for assertions , the basis for A in ABox ) is an \\\" assertion component \\\" ; that is , a fact associated with a terminological vocabulary within a knowledge base . ABox are TBox -compliant statements about instances belonging to the concept of an ontology . Instances and instance records reside within the ABox . A statistical measure of how well a binary classification test correctly identifies or excludes a condition . It is calculated as the sum of true positives and true negatives divided by the total population . An annotation , specifically as an annotation property , is a way to provide metadata or to describe vocabularies and properties used within an ontology . Annotations do not participate in reasoning or coherency testing for ontologies . These are the aspects , features , characteristics , or descriptors that qualify individual entities . Attributes are the way we describe and characterize individual things . Key - value pairs match an attribute with a value ; the value may be a reference to another object , an actual value or a descriptive label or string . In an RDF statement , an attribute is expressed as a property ( or predicate or relation ) . In intensional logic , all attributes or characteristics of similarly classifiable items define the membership in that set . Attribute type . An aggregation ( or class ) of multiple attributes that have similar characteristics amongst themselves . As with other types , shared characteristics are subsumed over some essence ( s ) that give the type its unique character . Also called a bnode , a blank node in RDF is a resource for which a URI or literal is not given . A blank node indicates the existence of a thing , implied by the structure of the knowledge graph , but which was never explicitly identified by giving it a URI . Blank nodes have no meaning outside of their current graph and therefore can not be mapped to other resources or graphs . A class is a collection of sets or instances ( or sometimes other mathematical objects ) which can be unambiguously defined by a property that all of its members share . In ontologies , classes may also be known as sets , collections , concepts , types of objects , or kinds of things . CWA is the presumption that what is not currently known to be true , is false . CWA also has a logical formalization . CWA is the most common logic applied to relational database systems , and is particularly useful for transaction - type systems . See contrast to the open world assumption . A common - sense knowledge base that has been under development for over 20 years backed by 1000 person - years of effort . The smaller OpenCyc version is available in OWL as open source ; a ResearchCyc version of the entire system is available to researchers . The Cyc platform contains its own logic language , CycL , and has many buillt - in functions in areas such as natural language processing , search , inferencing and the like . UMBEL is based on a subset of Cyc . A project that extracts structured content from Wikipedia , and then makes that data available as linked data . There are millions of entities characterized by DBpedia in this way . As such , DBpedia is one of the largest - and most central - hubs for linked data on the Web . Description logics and their semantics traditionally split concepts and their relationship s from the different treatment of instances and their attributes and roles , expressed as fact assertions . The concept split is known as the TBox and represents the schema or taxonomy of the domain at hand . The TBox is the structural and intensional component of conceptual relationships . The second split of instances is known as the ABox and describes the attributes of instances ( and individuals ) , the roles between instances , and other assertions about instances regarding their class membership with the TBox concepts . Distant supervision . A method to use knowledge bases to label entities automatically in text through machine learning , which is then used to extract features and train a machine learning classifier . The knowledge bases provide coherent positive training examples and avoid the high cost and effort of manual labeling . The collection of objects and their relationships germane to a particular discourse or scope of inquiry . The domain bounds the scope of a given knowledge representation project . Scoping the domain is one of the first activities undertaken in a new KR project . Domain ( or content ) ontologies embody more of the traditional ontology functions such as information interoperability , inferencing , reasoning and conceptual and knowledge capture of the applicable domain . The basic , real things in our domain of interest . An entity is an individual object or member of a class ; when affixed with a proper name or label is also known as a named entity ( thus , named entities are a subset of all entities ) . Entities are described and characterized by attributes . Entities are connected or related to one another through relations . EAV is a data model to describe entities where the number of attributes ( properties , parameters ) that can be used to describe them is potentially vast , but the number that will actually apply to a given entity is relatively modest . In the EAV data model , each attribute - value pair is a fact describing an entity . EAV systems trade off simplicity in the physical and logical structure of the data for complexity in their metadata , which , among other things , plays the role that database constraints and referential integrity do in standard database designs . The extension of a class , concept , idea , or sign consists of the things to which it applies , in contrast with its intension . For example , the extension of the word \\\" dog \\\" is the set of all ( past , present and future ) dogs in the world . The extension is most akin to the attributes or characteristics of the instances in a set defining its class membership . An error where a test result indicates that a condition failed , while it actually was successful . That is , the test result indicates a negative , when the correct result should have been positive . Also known as a false negative error or Type II error in statistics . It is abbreviated FN . An error where a test result indicates that a condition was met or achieved , while it actually should have failed . That is , the test result indicates a positive , when the correct result should have been negative . Also known as a false positive error or Type I error in statistics . It is abbreviated FP . GRDDL is a markup format for Gleaning Resource Descriptions from Dialects of Languages ; that is , for getting RDF data out of XML and XHTML documents using explicitly associated transformation algorithms , typically represented in XSLT . A high - level subject is both a subject proxy and category label used in a hierarchical subject classification scheme ( taxonomy ) . Higher - level subjects are classes for more atomic subjects , with the height of the level representing broader or more aggregate classes . Inference is the act or process of deriving logical conclusions from premises known or assumed to be true . The logic within and between statements in an ontology is the basis for inferring new conclusions from it , using software applications known as inference engines or reasoners . Instances are the basic , \\\" ground level \\\" components of an ontology . An instance is an individual member of a class , also used synonomously with entity . The instances in an ontology may include concrete objects such as people , animals , tables , automobiles , molecules , and planets , as well as abstract instances such as numbers and words . An instance is also known as an individual , with member and entity also used somewhat interchangeably . irON ( instance record and Object Notation ) is a abstract notation and associated vocabulary for specifying RDF ( Resource Description Framework ) triples and schema in non - RDF forms . Its purpose is to allow users and tools in non - RDF formats to stage interoperable datasets using RDF . The intension of a class is what is intended as a definition of what characteristics its members should have ; it is akin to a definition of a concept and what is intended for a class to contain . It is therefore like the schema aspects ( or TBox ) in an ontology . Also known as a name - value pair or attribute - value pair , a key - value pair is a fundamental , open - ended data representation . The key is the defined attribute and the value may be a reference to another object or a literal string or value . In RDF triple terms , the subject is implied in a key - value pair by nature of the instance record at hand . A knowledge base ( abbreviated KB or kb ) is a special kind of database for knowledge management . A knowledge base provides a means for information to be collected , organized , shared , searched and utilized . Formally , the combination of a TBox and ABox is a knowledge base . A field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks . Knowledge supervision . A method of machine learning to use knowledge bases in a purposeful way to create features , and negative and positive training sets in order to train the classifiers or extractors . Distant supervision also uses knowledge bases , but not is such a purposeful , directed manner across multiple machine learning problems . Linked data is a set of best practices for publishing and deploying instance and class data using the RDF data model , and uses uniform resource identifiers ( URIs ) to name the data objects . The approach exposes the data for access via the HTTP protocol , while emphasizing data interconnections , interrelationships and context useful to both humans and machine agents . The construction of algorithms that can learn from and make predictions on data by building a model from example inputs . A wide variety of techniques and algorithms ranging from supervised to unsupervised may be employed . It is \\\" data about data \\\" , or the means by which data objects or aggregations can be described . Contrasted to an attribute , which is an individual characteristic intrinsic to a data object or instance , metadata is a description about that data , such as how or when created or by whom . Microdata is a proposed specification used to nest semantics within existing content on web pages . Microdata is an attempt to provide a simpler way of annotating HTML elements with machine - readable tags than the similar approaches of using RDFa or microformats . A microformat ( sometimes abbreviated \\u03bcF or uF ) is a piece of mark up that allows expression of semantics in an HTML ( or XHTML ) web page . Programs can extract meaning from a web page that is marked up with one or more microformats . NLP is the process of a computer extracting meaningful information from natural language input and/or producing natural language output . NLP is one method for assigning structured data characterizations to text content for use in semantic technologies . ( Hand assignment is another method . ) Information extraction ( IE ) is the task of automatically extracting structured information from unstructured and/or semi - structured machine - readable documents . Ontology - based information extraction ( OBIE ) is the use of an ontology to inform a \\\" tagger \\\" or information extraction program when doing natural language processing . Input ontologies thus become the basis for generating metadata tags when tagging text or documents . An object is anything we can think about or talk about . In their use in semantic technologies , objects are nouns and always given a URI ( bnodes can also act in the object position but they lack a persistent URI ) . An ontology is a data model that represents a set of concepts within a domain and the relationships between those concepts . Loosely defined , ontologies on the Web can have a broad range of formalism , or expressiveness or reasoning power . Ontology - driven applications ( or ODapps ) are modular , generic software applications designed to operate in accordance with the specifications contained in one or more ontologies . The relationships and structure of the information driving these applications are based on the standard functions and roles of ontologies ( namely as domain ontologies ) , as supplemented by UI and instruction sets and validations and rules . The open semantic framework , or OSF , is a combination of a layered architecture and an open - source , modular software stack . The stack combines many leading third - party software packages with open source semantic technology developments from Structured Dynamics . OWA is a formal logic assumption that the truth - value of a statement is independent of whether or not it is known by any single observer or agent to be true . OWA is used in knowledge representation to codify the informal notion that in general no single agent or observer has complete knowledge , and therefore can not make the closed world assumption . The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true . OWA is useful when we represent knowledge within a system as we discover it , and where we can not guarantee that we have discovered or will discover complete information . In the OWA , statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown , rather than wrong or false . Semantic Web languages such as OWL make the open world assumption . See contrast to the closed world assumption . The Web Ontology Language ( OWL ) is designed for defining and instantiating formal Web ontologies . An OWL ontology may include descriptions of classes , along with their related properties and instances . There are also a variety of OWL dialects . The fraction of retrieved documents that are relevant to the query . It is measured as true positives divided by all measured positives ( true and false ) . High precision indicates a high percentage of true positives in relation to all positive results . Properties are the ways in which classes and instances can be related to one another . Between objects , properties are thus a relationship , and are also known as predicates . Properties are used to define an attribute or relation for an instance . In computer science , punning refers to a programming technique that subverts or circumvents the type system of a programming language , by allowing a value of a certain type to be manipulated as a value of a different type . When used for ontologies , it means to treat a thing as both a class and an instance , with the use depending on context . Resource Description Framework ( RDF ) is a family of World Wide Web Consortium ( W3C ) specifications originally designed as a metadata model but which has come to be used as a general method of modeling information , through a variety of syntax formats . The RDF metadata model is based upon the idea of making statements about resources in the form of subject - predicate - object expressions , called triples in RDF terminology . The subject denotes the resource , and the predicate denotes traits or aspects of the resource and expresses a relationship between the subject and the object . RDFa uses attributes from meta and link elements , and generalizes them so that they are usable on all elements allowing annotation markup with semantics . RDFa 1.1 is a W3C Recommendation that removes prior dependence on the XML namespace and expands HTML5 and SVG support , among other changes . RDFS or RDF Schema is an extensible knowledge representation language , providing basic elements for the description of ontologies , otherwise called RDF vocabularies , intended to structure RDF resources . A semantic reasoner , reasoning engine , rules engine , or simply a reasoner , is a piece of software able to infer logical consequences from a set of asserted facts or axioms . The notion of a semantic reasoner generalizes that of an inference engine , by providing a richer set of mechanisms . Reasoning . Reasoning is one of many logical tests using inference rules as commonly specified by means of an ontology language , and often a description language . Many reasoners use first - order predicate logic to perform reasoning ; inference commonly proceeds by forward chaining or backward chaining . The fraction of the documents that are relevant to the query that are successfully retrieved . It is measured as true positives divided by all potential positives that could be returned from the corpus . High recall indicates a high yield in obtaining relevant results . Any of the noun objects within UMBEL , and abbreviated as RC . An RC may be either an entity , entity type , attribute , attribute type , relation , relation type , topic or abstract concept . There are presently about 35 K RCs in UMBEL . All RCs are objects . An aggregation ( or class ) of multiple relations that have similar characteristics amongst themselves . As with other types , shared characteristics are subsumed over some essence ( s ) that give the type its unique character . Schema.org is an initiative launched by the major search engines of Bing , Google and Yahoo ! , and later jointed by Yandex , in order to create and support a common set of schema for structured data markup on web pages . schema.org provided a starter set of schema and extension mechanisms for adding to them . schema.org supports markup in microdata , microformat and RDFa formats . Semantic technologies are a combination of software and semantic specifications that encode meanings separately from data and content files and separately from application code . This approach enables machines as well as people to understand , share and reason with data and specifications separately . With semantic technologies , adding , changing and implementing new relationships or interconnecting programs in a different way can be as simple as changing the external model that these programs share . New data can also be brought into the system and visualized or worked upon based on the existing schema . Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data , content , and processes . The Semantic Web is a collaborative movement led by the World Wide Web Consortium ( W3C ) that promotes common formats for data on the World Wide Web . By encouraging the inclusion of semantic content in web pages , the Semantic Web aims at converting the current web of unstructured documents into a \\\" web of data \\\" . It builds on the W3C 's Resource Description Framework ( RDF ) . A semset is the use of a series of alternate labels and terms to describe a concept or entity . These alternatives include true synonyms , but may also be more expansive and include jargon , slang , acronyms or alternative terms that usage suggests refers to the same concept . Semantically - Interlinked Online Communities Project ( SIOC ) is based on RDF and is an ontology defined using RDFS for interconnecting discussion methods such as blogs , forums and mailing lists to each other . SKOS or Simple Knowledge Organisation System is a family of formal languages designed for representation of thesauri , classification schemes , taxonomies , subject - heading systems , or any other type of structured controlled vocabulary ; it is built upon RDF and RDFS . A statement is a \\\" triple \\\" in an ontology , which consists of a subject - predicate - object ( S - P - O ) assertion . By definition , each statement is a \\\" fact \\\" or axiom within an ontology . Subject . A subject is always a noun or compound noun and is a reference or definition to a particular object , thing or topic , or groups of such items . Subjects are also often referred to as concepts or topics . Subject extraction . Subject extraction is an automatic process for retrieving and selecting subject names from existing knowledge bases or data sets . Extraction methods involve parsing and tokenization , and then generally the application of one or more information extraction techniques or algorithms . Subject proxy . A subject proxy as a canonical name or label for a particular object ; other terms or controlled vocabularies may be mapped to this label to assist disambiguation . A subject proxy is always representative of its object but is not the object itself . SuperType . One of about 30 segregated splits within UMBEL that are mostly disjoint from one another and mostly conform to broad groupings of entities . SuperTypes are a major organizational dimension of UMBEL . A machine learning task of inferring a function from labeled training data , which optimally consists of positive and negative training sets . The supervised learning algorithm analyzes the training data and produces an inferred function to correctly determine the class labels for unseen instances . A tag is a keyword or term associated with or assigned to a piece of information ( e.g. , a picture , article , or video clip ) , thus describing the item and enabling keyword - based classification of information . Tags are usually chosen informally by either the creator or consumer of the item . A TBox ( for terminological knowledge , the basis for T in TBox ) is a \\\" terminological component \\\" ; that is , a conceptualization associated with a set of facts . TBox statements describe a conceptualization , a set of concepts and properties for these concepts . The TBox is sufficient to describe an ontology ( best practice often suggests keeping a split between instance records - and ABox - and the TBox schema ) . In the context of knowledge systems , taxonomy is the hierarchical classification of entities of interest of an enterprise , organization or administration , used to classify documents , digital assets and other information . Taxonomies can cover virtually any type of physical or conceptual entities ( products , processes , knowledge fields , human groups , etc . ) at any level of granularity . The topic ( or theme ) is the part of the proposition that is being talked about ( predicated ) . In topic maps , the topic may represent any concept , from people , countries , and organizations to software modules , individual files , and events . Topics and subjects are closely related . Topic maps are an ISO standard for the representation and interchange of knowledge . A topic map represents information using topics , associations ( similar to a predicate relationship ) , and occurrences ( which represent relationships between topics and information resources relevant to them ) , quite similar in concept to the RDF triple . A set of data used to discover potentially predictive relationships . In supervised learning , a positive training set provides data that meets the training objectives ; a negative training set fails to meet the objectives . Triple . A basic statement in the RDF language , which is comprised of a subject - property - object construct , with the subject and property ( and object optionally ) referenced by URIs . Is a flat , hierarchical taxonomy comprised of related entity types within the context of a given UMBEL SuperType ( ST ) . Typologies are a critical connection point between the TBox and ABox . The link shown here uses an archaeology example . This vocabulary is also designed for interoperable domain ontologies . An upper ontology ( also known as a top - level ontology or foundation ontology ) is an ontology that describes very general concepts that are the same across all knowledge domains . An important function of an upper ontology is to support very broad semantic interoperability between a large number of ontologies that are accessible ranking \\\" under \\\" this upper ontology . A vocabulary in the sense of knowledge systems or ontologies are controlled vocabularies . They provide a way to organize knowledge for subsequent retrieval . They are used in subject indexing schemes , subject headings , thesauri , taxonomies and other form of knowledge organization systems . This is a crowdsourced , open knowledge base of ( currently ) about 18 million structured entity records . Each record consists of attributes and values with robust cross - links to multiple languages . Wikidata is a key entities source . Wikipedia is a crowdsourced , free - access and free - content knowledge base of human knowledge . It has nearly 5 million articles in its English version . Across all Wikipedias there are nearly 35 million articles in 288 different language versions . WordNet is a lexical database for the English language . It groups English words into sets of synonyms called synsets , provides short , general definitions , and records the various semantic relations between these synonym sets . The purpose is twofold : to produce a combination of dictionary and thesaurus that is more intuitively usable , and to support automatic text analysis and artificial intelligence applications . The database and software tools can be downloaded and used freely . Multiple language versions exist , and WordNet is a frequent reference structure for semantic applications . \"}",
        "_version_":1692670552485920769,
        "score":22.79788},
      {
        "id":"dadaaffe-a016-4da6-b5f7-ac3beaed8ea3",
        "_src_":"{\"url\": \"http://strolen.com/viewing/Mugama_The_Gold_That_Is_Not\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146550.16/warc/CC-MAIN-20160205193906-00111-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"As everyday real life contact becomes less necessary to conduct business , we will soon start seeing the genesis of ' virtual ' Silicon Valleys leveraging the power of the Internet . Knowledge representation . From Wikipedia , the free encyclopedia . When we design a knowledge representation ( and a knowledge representation system to interpret sentences in the logic in order to derive inferences from them ) we have to make trades across a number of design spaces , described in the following sections . The single most important decision to be made , however is the expressivity of the KR . The more expressive , the easier ( and more compact ) it is to \\\" say something \\\" . However , more expressive languages are harder to automatically derive inferences from . An example of a less expressive KR would be propositional logic . An example of a more expressive KR would be autoepistemic temporal modal logic . Less expressive KRs may be both complete and consistent ( formally less expressive than set theory ) . More expressive KRs may be neither complete nor consistent . The key problem is to find a KR ( and a supporting reasoning system ) that can make the inferences your application needs in time , that is , within the resource constraints appropriate to the problem at hand . This tension between the kinds of inferences an application \\\" needs \\\" and what counts as \\\" in time \\\" along with the cost to generate the representation itself makes knowledge representation engineering interesting . There are representation techniques such as frames , rules and semantic networks which have originated from theories of human information processing . Since knowledge is used to achieve intelligent behavior , the fundamental goal of knowledge representation is to represent knowledge in a manner as to facilitate inferencing ( i.e. drawing conclusions ) from knowledge . Some issues that arise in knowledge representation from an AI perspective are : . What is the nature of knowledge and how do we represent it ? Should a representation scheme deal with a particular domain or should it be general purpose ? There has been very little top - down discussion of the knowledge representation ( KR ) issues and research in this area is a well aged quiltwork . For example a tomato could be classified both as a fruit and a vegetable . In the field of artificial intelligence , problem solving can be simplified by an appropriate choice of knowledge representation . Representing knowledge in some ways makes certain problems easier to solve . For example , it is easier to divide numbers represented in Hindu - Arabic numerals than numbers represented as Roman numerals . Representing knowledge in such explicit form enables computers to draw conclusions from knowledge already stored ( ' Clyde is grey ' ) . Many KR methods were tried in the 1970s and early 1980s , such as heuristic question - answering , neural networks , theorem proving , and expert systems , with varying success . Medical diagnosis ( e.g. , Mycin ) was a major application area , as were games such as chess . In the 1980s formal computer knowledge representation languages and systems arose . Through such work , the difficulty of KR came to be better appreciated . In computational linguistics , meanwhile , much larger databases of language information were being built , and these , along with great increases in computer speed and capacity , made deeper KR more feasible . Several programming languages have been developed that are oriented to KR . Prolog developed in 1972 , [ 1 ] but popularized much later , represents propositions and basic logic , and can derive conclusions from known premises . KL - ONE ( 1980s ) is more specifically aimed at knowledge representation itself . In 1995 , the Dublin Core standard of metadata was conceived . In the electronic document world , languages were being developed to represent the structure of documents , such as SGML ( from which HTML descended ) and later XML . These facilitated information retrieval and data mining efforts , which have in recent years begun to relate to knowledge representation . Some people think it would be best to represent knowledge in the same way that it is represented in the human mind , or to represent knowledge in the form of human language . Psycholinguistics is investigating how the human mind stores and manipulates language . Other branches of cognitive science examine how human memory stores sounds , sights , smells , emotions , procedures , and abstract ideas . Science has not yet completely described the internal mechanisms of the brain to the point where they can simply be replicated by computer programmers . Various [ which ? ] artificial languages and notations have been proposed for representing knowledge . They are typically based on logic and mathematics , and have easily parsed grammars to ease machine processing . They usually fall into the broad domain of ontologies . After CycL , a number of ontology languages have been developed . Most are declarative languages , and are either frame languages , or are based on first - order logic . Most of these languages only define an upper ontology with generic concepts , whereas the domain concepts are not part of the language definition . Gellish English is an example of an ontological language that includes a full engineering English Dictionary . While hyperlinks have come into widespread use , the closely related semantic link is not yet widely used . The mathematical table has been used since Babylonian times . More recently , these tables have been used to represent the outcomes of logic operations , such as truth tables , which were used to study and model Boolean logic , for example . Spreadsheets are yet another tabular representation of knowledge . Other knowledge representations are trees , by means of which the connections among fundamental concepts and derivative concepts can be shown . Visual representations are relatively new in the field of knowledge management but give the user a way to visualise how one thought or idea is connected to other ideas enabling the possibility of moving from one thought to another in order to locate required information . The approach is not without its competitors . [ 2 ] . The recent fashion in knowledge representation languages is to use XML as the low - level syntax . This tends to make the output of these KR languages easy for machines to parse , at the expense of human readability and often space - efficiency . First - order predicate calculus is commonly used as a mathematical basis for these systems , to avoid excessive complexity . However , even simple systems based on this simple logic can be used to represent data that is well beyond the processing capability of current computer systems : see computability for reasons . One problem in knowledge representation consists of how to store and manipulate knowledge in an information system in a formal way so that it may be used by mechanisms to accomplish a given task . Examples of applications are expert systems , machine translation systems , computer - aided maintenance systems and information retrieval systems ( including database front - ends ) . Semantic networks may be used to represent knowledge . Each node represents a concept and arcs are used to define relations between the concepts . One of the most expressive and comprehensively described knowledge representation paradigms along the lines of semantic networks is MultiNet ( an acronym for Multilayered Extended Semantic Networks ) . From the 1960s , the knowledge frame or just frame has been used . Each frame has its own name and a set of attributes , or slots which contain values ; for instance , the frame for house might contain a color slot , number of floors slot , etc . . Using frames for expert systems is an application of object - oriented programming , with inheritance of features described by the \\\" is - a \\\" link . Other links include the \\\" has - part \\\" link . Frame structures are well - suited for the representation of schematic knowledge and stereotypical cognitive patterns . The elements of such schematic patterns are weighted unequally , attributing higher weights to the more typical elements of a schema . Frame representations are object - centered in the same sense as semantic networks are : All the facts and properties connected with a concept are located in one place - there is no need for costly search processes in the database . A behavioral script is a type of frame that describes what happens temporally ; the usual example given is that of describing going to a restaurant . The steps include waiting to be seated , receiving a menu , ordering , etc . The different solutions can be arranged in a so - called semantic spectrum with respect to their semantic expressivity . Francisco Antonio Cer\\u00f3n Garc\\u00eda . The main question : What would be your next strategy step to continue developing Internet in a new radical way ? It is a way in the sense of \\\" meta \\\" , like Google is a \\\" meta internet \\\" . Do we know how to do it ? A silicon valley is essentially 90 % about the people and 10 % about the place . Places close to financial centres and developed cities are more likely to host the next silicon valley , but smart people can turn any place into a silicon valley if that 's what they want , even if it 's in the middle of nowhere . However , now with the Internet I believe less in silicon valleys . I mean , what 's the point of having silicon valleys when entrepreneurs and techies can network through the Net and telecommute ? As everyday real life contact becomes less necessary to conduct business , we will soon start seeing the genesis of ' virtual ' silicon valleys leveraging the power of the Internet . If Ihad to build the next silicon valley , I would start by recruiting smart people on the Internet and creating incentives for like - minded individuals and companies to participate in some sort of hub website virtual marketplace . It makes me think carefully about the next big revolutionary step on internet development . Eventually , I think that the issue that is being treated here is a key issue and it deserves a new blog to be opened for it . This is an open question , and I want that it would be the spirit of this simple blog ! You are all invited to build the meta internet ! Then you could start thinking a lot about this issue ! Meta . Bookmark / Share . Post a comment . Log in to comment , or comment anonymously . Warning : Anonymous messages are held for moderation . This could take a ( long ) while . Or your comment may not be posted at all . Please consider creating an account and logging in . It 's fast , free , and we do n't spam , ever . \"}",
        "_version_":1692670754351480832,
        "score":22.705755},
      {
        "id":"196d7ee8-6c21-4795-accc-22c761580d50",
        "_src_":"{\"url\": \"https://interchangeableparts.wordpress.com/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701154221.36/warc/CC-MAIN-20160205193914-00035-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Software documentation is like sex : when it is good , it is very , very good ; and when it is bad , it is better than nothing . ( Anonymous . ) There are two ways of constructing a software design : one way is to make it so simple that there are obviously no deficiencies ; the other way is to make it so complicated that there are no obvious deficiencies . ( C.A.R. Hoare ) . A computer language is not just a way of getting a computer to perform operations but rather that it is a novel formal medium for expressing ideas about methodology . Thus , programs must be written for people to read , and only incidentally for machines to execute . ( The Structure and Interpretation of Computer Programs , H. Abelson , G. Sussman and J. Sussman , 1985 . ) If you try to make something beautiful , it is often ugly . If you try to make something useful , it is often beautiful . ( Oscar Wilde ) 1 . GATE 2 is an infrastructure for developing and deploying software components that process human language . It is nearly 15 years old and is in active use for all types of computational task involving human language . GATE excels at text analysis of all shapes and sizes . From large corporations to small startups , from \\u20ac multi - million research consortia to undergraduate projects , our user community is the largest and most diverse of any system of this type , and is spread across all but one of the continents 3 . GATE is open source free software ; users can obtain free support from the user and developer community via GATE.ac.uk or on a commercial basis from our industrial partners . We are the biggest open source language processing project with a development team more than double the size of the largest comparable projects ( many of which are integrated with GATE 4 ) . More than \\u20ac 5 million has been invested in GATE development 5 ; our objective is to make sure that this continues to be money well spent for all GATE 's users . GATE has grown over the years to include a desktop client for developers , a workflow - based web application , a Java library , an architecture and a process . GATE is : . One of our original motivations was to remove the necessity for solving common engineering problems before doing useful research , or re - engineering before deploying research results into applications . Core functions of GATE take care of the lion 's share of the engineering : . modelling and persistence of specialised data structures . measurement , evaluation , benchmarking ( never believe a computing researcher who has n't measured their results in a repeatable and open setting ! ) visualisation and editing of annotations , ontologies , parse trees , etc . . a finite state transduction language for rapid prototyping and efficient implementation of shallow analysis methods ( JAPE ) . extraction of training instances for machine learning . pluggable machine learning implementations ( Weka , SVM Light , ... ) . On top of the core functions GATE includes components for diverse language processing tasks , e.g. parsers , morphology , tagging , Information Retrieval tools , Information Extraction components for various languages , and many others . GATE Developer and Embedded are supplied with an Information Extraction system ( ANNIE ) which has been adapted and evaluated very widely ( numerous industrial systems , research systems evaluated in MUC , TREC , ACE , DUC , Pascal , NTCIR , etc . ) . ANNIE is often used to create RDF or OWL ( metadata ) for unstructured content ( semantic annotation ) . GATE version 1 was written in the mid-1990s ; at the turn of the new millenium we completely rewrote the system in Java ; version 5 was released in June 2009 . We invite you to give it a try , to get involved with the GATE community , and to contribute to human language science , engineering and development . This book describes how to use GATE to develop language processing components , test their performance and deploy them as parts of other applications . In the rest of this chapter : . ( Often the process of getting a new component is as simple as typing the URL into GATE Developer ; the system will do the rest . ) The material presented in this book ranges from the conceptual ( e.g. ' what is software architecture ? ' ) to practical instructions for programmers ( e.g. how to deal with GATE exceptions ) and linguists ( e.g. how to write a pattern grammar ) . Furthermore , GATE 's highly extensible nature means that new functionality is constantly being added in the form of new plugins . Important functionality is as likely to be located in a plugin as it is to be integrated into the GATE core . This presents something of an organisational challenge . Our ( no doubt imperfect ) solution is to divide this book into three parts . Part I covers installation , using the GATE Developer GUI and using ANNIE , as well as providing some background and theory . We recommend the new user to begin with Part I . Part II covers the more advanced of the core GATE functionality ; the GATE Embedded API and JAPE pattern language among other things . Part III provides a reference for the numerous plugins that have been created for GATE . Although ANNIE provides a good starting point , the user will soon wish to explore other resources , and so will need to consult this part of the text . We recommend that Part III be used as a reference , to be dipped into as necessary . In Part III , plugins are grouped into broad areas of functionality . Software Architecture ' is used rather loosely here to mean computer infrastructure for software development , including development environments and frameworks , as well as the more usual use of the term to denote a macro - level organisational structure for software systems [ Shaw & Garlan 96 ] . Language Engineering ( LE ) may be defined as : . ... the discipline or act of engineering software systems that perform tasks involving processing human language . Both the construction process and its outputs are measurable and predictable . The literature of the field relates to both application of relevant scientific results and a body of practice . [ Cunningham 99a ] . The relevant scientific results in this case are the outputs of Computational Linguistics , Natural Language Processing and Artificial Intelligence in general . Unlike these other disciplines , LE , as an engineering discipline , entails predictability , both of the process of constructing LE - based software and of the performance of that software after its completion and deployment in applications . Some working definitions : . Computational Linguistics ( CL ) : science of language that uses computation as an investigative tool . Natural Language Processing ( NLP ) : science of computation whose subject matter is data structures and algorithms for computer processing of human language . Language Engineering ( LE ) : building NLP systems whose cost and outputs are measurable and predictable . Software Architecture : macro - level organisational principles for families of systems . In this context is also used as infrastructure . Software Architecture for Language Engineering ( SALE ) : software infrastructure , architecture and development tools for applied CL , NLP and LE . ( Of course the practice of these fields is broader and more complex than these definitions . ) In the scientific endeavours of NLP and CL , GATE 's role is to support experimentation . In this context GATE 's significant features include support for automated measurement ( see Chapter 10 ) , providing a ' level playing field ' where results can easily be repeated across different sites and environments , and reducing research overheads in various ways . GATE as an architecture suggests that the elements of software systems that process natural language can usefully be broken down into various types of component , known as resources 8 . Components are reusable software chunks with well - defined interfaces , and are a popular architectural form , used in Sun 's Java Beans and Microsoft 's . Net , for example . GATE components are specialised types of Java Bean , and come in three flavours : . LanguageResources ( LRs ) represent entities such as lexicons , corpora or ontologies ; . ProcessingResources ( PRs ) represent entities that are primarily algorithmic , such as parsers , generators or ngram modellers ; . VisualResources ( VRs ) represent visualisation and editing components that participate in GUIs . These definitions can be blurred in practice as necessary . Collectively , the set of resources integrated with GATE is known as CREOLE : a Collection of REusable Objects for Language Engineering . All the resources are packaged as Java Archive ( or ' JAR ' ) files , plus some XML configuration data . The JAR and XML files are made available to GATE by putting them on a web server , or simply placing them in the local file space . Section 1.3.2 introduces GATE 's built - in resource set . When using GATE to develop language processing functionality for an application , the developer uses GATE Developer and GATE Embedded to construct resources of the three types . This may involve programming , or the development of Language Resources such as grammars that are used by existing Processing Resources , or a mixture of both . GATE Developer is used for visualisation of the data structures produced and consumed during processing , and for debugging , performance measurement and so on . For example , figure 1.1 is a screenshot of one of the visualisation tools . GATE Developer is analogous to systems like Mathematica for Mathematicians , or JBuilder for Java programmers : it provides a convenient graphical environment for research and development of language processing software . When an appropriate set of resources have been developed , they can then be embedded in the target client application using GATE Embedded . GATE Embedded is supplied as a series of JAR files . 9 To embed GATE - based language processing facilities in an application , these JAR files are all that is needed , along with JAR files and XML configuration files for the various resources that make up the new facilities . GATE includes resources for common LE data structures and algorithms , including documents , corpora and various annotation types , a set of language analysis components for Information Extraction and a range of data visualisation and editing components . GATE supports documents in a variety of formats including XML , RTF , email , HTML , SGML and plain text . In all cases the format is analysed and converted into a single unified model of annotation . The annotation format is a modified form the TIPSTER format [ Grishman 97 ] which has been made largely compatible with the Atlas format [ Bird & Liberman 99 ] , and uses the now standard mechanism of ' stand - off markup ' . GATE documents , corpora and annotations are stored in databases of various sorts , visualised via the development environment , and accessed at code level via the framework . See Chapter 5 for more details of corpora etc . . A family of Processing Resources for language analysis is included in the shape of ANNIE , A Nearly - New Information Extraction system . These components use finite state techniques to implement various tasks from tokenisation to semantic tagging or verb phrase chunking . All ANNIE components communicate exclusively via GATE 's document and annotation resources . See Chapter 6 for more details . Other CREOLE resources are described in Part III . JAPE , a Java Annotation Patterns Engine , provides regular - expression based pattern / action rules over annotations - see Chapter 8 . The ' annotation diff ' tool in the development environment implements performance metrics such as precision and recall for comparing annotations . Typically a language analysis component developer will mark up some documents by hand and then use these along with the diff tool to automatically measure the performance of the components . See Chapter 10 . GUK , the GATE Unicode Kit , fills in some of the gaps in the JDK 's 10 support for Unicode , e.g. by adding input methods for various languages from Urdu to Chinese . See Section 3.10.2 for more details . This section gives a very brief example of a typical use of GATE to develop and deploy language processing capabilities in an application , and to generate quantitative results for scientific publication . Let 's imagine that a developer called Fatima is building an email client 11 for Cyberdyne Systems ' large corporate Intranet . In this application she would like to have a language processing system that automatically spots the names of people in the corporation and transforms them into mailto hyperlinks . A little investigation shows that GATE 's existing components can be tailored to this purpose . Fatima starts up GATE Developer , and creates a new document containing some example emails . She then loads some processing resources that will do named - entity recognition ( a tokeniser , gazetteer and semantic tagger ) , and creates an application to run these components on the document in sequence . Having processed the emails , she can see the results in one of several viewers for annotations . The GATE components are a decent start , but they need to be altered to deal specially with people from Cyberdyne 's personnel database . Therefore Fatima creates new ' cyber- ' versions of the gazetteer and semantic tagger resources , using the ' bootstrap ' tool . This tool creates a directory structure on disk that has some Java stub code , a Makefile and an XML configuration file . After several hours struggling with badly written documentation , Fatima manages to compile the stubs and create a JAR file containing the new resources . She tells GATE Developer the URL of these files 12 , and the system then allows her to load them in the same way that she loaded the built - in resources earlier on . Fatima then creates a second copy of the email document , and uses the annotation editing facilities to mark up the results that she would like to see her system producing . She saves this and the version that she ran GATE on into her serial datastore . From now on she can follow this routine : . Run her application on the email test corpus . Check the performance of the system by running the ' annotation diff ' tool to compare her manual results with the system 's results . This gives her both percentage accuracy figures and a graphical display of the differences between the machine and human outputs . Make edits to the code , pattern grammars or gazetteer lists in her resources , and recompile where necessary . To make the alterations that she requires , Fatima re - implements the ANNIE gazetteer so that it regenerates itself from the local personnel data . She then alters the pattern grammar in the semantic tagger to prioritise recognition of names from that source . This latter job involves learning the JAPE language ( see Chapter 8 ) , but as this is based on regular expressions it is n't too difficult . Eventually the system is running nicely , and her accuracy is 93 % ( there are still some problem cases , e.g. when people use nicknames , but the performance is good enough for production use ) . Now Fatima stops using GATE Developer and works instead on embedding the new components in her email application using GATE Embeddded . She takes the accuracy measures that she has attained for her system and writes a paper for the Journal of Nasturtium Logarithm Encitement describing the approach used and the results obtained . Because she used GATE for development , she can cite the repeatability of her experiments and offer access to example binary versions of her software by putting them on an external web server . This section contains an incomplete list of publications describing systems that used GATE in competitive quantitative evaluation programmes . These programmes have had a significant impact on the language processing field and the widespread presence of GATE is some measure of the maturity of the system and of our understanding of its likely performance on diverse text processing tasks . describes the performance of an SVM - based learning system in the NTCIR-6 Patent Retrieval Task . The system achieved the best result on two of three measures used in the task evaluation , namely the R - Precision and F - measure . The system obtained close to the best result on the remaining measure ( A - Precision ) . describes a cross - source coreference resolution system based on semantic clustering . It uses GATE for information extraction and the SUMMA system to create summaries and semantic representations of documents . One system configuration ranked 4th in the Web People Search 2007 evaluation . describes a cross - lingual summarization system which uses SUMMA components and the Arabic plugin available in GATE to produce summaries in English from a mixture of English and Arabic documents . Open - Domain Question Answering : . The University of Sheffield has a long history of research into open - domain question answering . GATE has formed the basis of much of this research resulting in systems which have ranked highly during independent evaluations since 1999 . The first successful question answering system developed at the University of Sheffield was evaluated as part of TREC 8 and used the LaSIE information extraction system ( the forerunner of ANNIE ) which was distributed with GATE [ Humphreys et al . 99 ] . Further research was reported in [ Scott & Gaizauskas . 00 ] , [ Greenwood et al . 02 ] , [ Gaizauskas et al . 03 ] , [ Gaizauskas et al . 04 ] and [ Gaizauskas et al . 05 ] . In 2004 the system was ranked 9th out of 28 participating groups . describes techniques for answering definition questions . The system uses definition patterns manually implemented in GATE as well as learned JAPE patterns induced from a corpus . In 2004 , the system was ranked 4th in the TREC / QA evaluations . describes a multidocument summarization system implemented using summarization components compatible with GATE ( the SUMMA system ) . The system was ranked 2nd in the Document Understanding Evaluation programmes . describe participation in the TIDES surprise language program . ANNIE was adapted to Cebuano with four person days of effort , and achieved an F - measure of 77.5 % . Unfortunately , ours was the only system participating ! describe results obtained on systems designed for the ACE task ( Automatic Content Extraction ) . Although a comparison to other participating systems can not be revealed due to the stipulations of ACE , results show 82%-86 % precision and recall . To get HTML reports from profiled processing resources , there is a new menu item in the ' Tools ' menu called ' Profiling reports ' , see chapter 11 . To deal with quality assurance of annotations , one component has been updated and two new components have been added . The annotation diff tool has a new mode to copy annotations to a consensus set , see section 10.2.1 . An annotation stack view has been added in the document editor and it allows to copy annotations to a consensus set , see section 3.4.3 . A corpus view has been added for all corpus to get statistics like precision , recall and F - measure , see section 10.3 . An annotation stack view has been added in the document editor to make easier to see overlapping annotations , see section 3.4.3 . Added an isInitialised ( ) method to gate . Gate ( ) . The ontology API ( package gate.creole.ontology has been changed , the existing ontology implementation based on Sesame1 and OWLIM2 ( package gate.creole.ontology.owlim ) has been moved into the plugin Ontology_OWLIM2 . An upgraded implementation based on Sesame2 and OWLIM3 that also provides a number of new features has been added as plugin Ontology . See Section 14.12 for a detailed description of all changes . The new Imports : statement at the beginning of a JAPE grammar file can now be used to make additional Java import statements available to the Java RHS code , see 8.6.5 . The User Guide has been amalgamated with the Programmer 's Guide ; all material can now be found in the User Guide . The ' How - To ' chapter has been converted into separate chapters for installation , GATE Developer and GATE Embedded . Other material has been relocated to the appropriate specialist chapter . Plugin names have been rationalised . Mappings exist so that existing applications will continue to work , but the new names should be used in the future . Plugin name mappings are given in Appendix B . The Montreal Transducer has been made obsolete . The UIMA integration layer ( Chapter 18 ) has been upgraded to work with Apache UIMA 2.2.2 . The JAPE debugger has been removed . Debugging of JAPE has been made easier as stack traces now refer to the JAPE source file and line numbers instead of the generated Java source code . Oracle and PostGreSQL are no longer supported . The MIAKT Natural Language Generation plugin has been removed . The Minorthird plugin has been removed . Minorthird has changed significantly since this plugin was written . We will consider writing an up - to - date Minorthird plugin in the future . A new gazetteer , Large KB Gazetteer ( in the plugin ' Gazetteer_LKB ' ) has been added , see Section 13.9 for details . gate.creole.tokeniser.chinesetokeniser.ChineseTokeniser and related resources under the plugins / ANNIE / tokeniser / chinesetokeniser folder have been removed . Please refer to the Lang_Chinese plugin for resources related to the Chinese language in GATE . A number of improvements to the benchmarking support in GATE . JAPE transducers now log the time spent in individual phases of a multi - phase grammar and by individual rules within each phase . Other PRs that use JAPE grammars internally ( the pronominal coreferencer , English tokeniser ) log the time taken by their internal transducers . A reporting tool , called ' Profiling reports ' under the ' Tools ' menu makes summary information easily available . For more details , see chapter 11 . We have added a new PR called ' Segment Processing PR ' . As the name suggests this PR allows processing individual segments of a document independently of one other . For more details , please look at the section 16.2.8 . The gate . Controller implementations provided with the main GATE distribution now also implement the gate . ProcessingResource interface . This means that an application can now contain another application as one of its components . LingPipe is a suite of Java libraries for the linguistic analysis of human language . We have provided a plugin called ' LingPipe ' with wrappers for some of the resources available in the LingPipe library . For more details , see the section 19.15 . OpenNLP provides tools for sentence detection , tokenization , pos - tagging , chunking and parsing , named - entity detection , and coreference . The tools use Maximum Entropy modelling . We have provided a plugin called ' OpenNLP ' with wrappers for some of the resources available in the OpenNLP Tools library . For more details , see section 19.16 . A new plugin has been added to provide an easy route to integrate taggers with GATE . The Tagger_Framework plugin provides examples of incorporating a number of external taggers which should serve as a starting point for using other taggers . See Section 17.4 for more details . reviews the current state of the art in email processing and communication research , focusing on the roles played by email in information management , and commercial and research efforts to integrate a semantic - based approach to email . investigates two techniques for making SVMs more suitable for language learning tasks . Firstly , an SVM with uneven margins ( SVMUM ) is proposed to deal with the problem of imbalanced training data . Secondly , SVM active learning is employed in order to alleviate the difficulty in obtaining labelled training data . The algorithms are presented and evaluated on several Information Extraction ( IE ) tasks . presents a semantic - based prototype that is made for an open - source software engineering project with the goal of exploring methods for assisting open - source developers and software users to learn and maintain the system without major effort . discusses methods of measuring the performance of ontology - based information extraction systems , focusing particularly on the Balanced Distance Metric ( BDM ) , a new metric we have proposed which aims to take into account the more flexible nature of ontologically - based applications . describes the development of a system for content mining using domain ontologies , which enables the extraction of relevant information to be fed into models for analysis of financial and operational risk and other business intelligence applications such as company intelligence , by means of the XBRL standard . describes experiments for the cross - document coreference task in SemEval 2007 . Our cross - document coreference system uses an in - house agglomerative clustering implementation to group documents referring to the same entity . describes the application of ontology - based extraction and merging in the context of a practical e - business application for the EU MUSING Project where the goal is to gather international company intelligence and country / region information . studies Japanese - English cross - language patent retrieval using Kernel Canonical Correlation Analysis ( KCCA ) , a method of correlating linear relationships between two variables in kernel defined feature spaces . ( Proceedings of the 5th International Semantic Web Conference ( ISWC2006 ) ) In this paper the problem of disambiguating author instances in ontology is addressed . We describe a web - based approach that uses various features such as publication titles , abstract , initials and co - authorship information . describes work in progress concerning the application of Controlled Language Information Extraction - CLIE to a Personal Semantic Wiki - Semper- Wiki , the goal being to permit users who have no specialist knowledge in ontology tools or languages to semi - automatically annotate their respective personal Wiki pages . discusses existing evaluation metrics , and proposes a new method for evaluating the ontology population task , which is general enough to be used in a variety of situation , yet more precise than many current metrics . describes an approach that allows users to create and edit ontologies simply by using a restricted version of the English language . The controlled language described is based on an open vocabulary and a restricted set of grammatical constructs . ( Proceedings of Fifth International Conference on Recent Advances in Natural Language Processing ( RANLP2005 ) ) It is a full - featured annotation indexing and search engine , developed as a part of the GATE . It is powered with Apache Lucene technology and indexes a variety of documents supported by the GATE . ( Proceedings of Ninth Conference on Computational Natural Language Learning ( CoNLL-2005 ) ) uses the uneven margins versions of two popular learning algorithms SVM and Perceptron for IE to deal with the imbalanced classification problems derived from IE . ( Proceedings of the 2nd European Workshop on the Integration of Knowledge , Semantic and Digital Media Technologies ( EWIMT 2005))Digital Media Preservation and Access through Semantically Enhanced Web - Annotation . describes a sentence extraction system that produces two sorts of multi - document summaries ; a general - purpose summary of a cluster of related documents and an entity - based summary of documents related to a particular person . 8 The terms ' resource ' and ' component ' are synonymous in this context . ' Resource ' is used instead of just ' component ' because it is a common term in the literature of the field : cf . the Language Resources and Evaluation conference series [ LREC-1 98 , LREC-2 00 ] . 9 The main JAR file ( gate.jar ) supplies the framework . Built - in resources and various 3rd - party libraries are supplied as separate JARs ; for example ( guk.jar , the GATE Unicode Kit . ) contains Unicode support ( e.g. additional input methods for languages not currently supported by the JDK ) . They are separate because the latter has to be a Java extension with a privileged security profile . 10 JDK : Java Development Kit , Sun Microsystem 's Java implementation . Unicode support is being actively improved by Sun , but at the time of writing many languages are still unsupported . In fact , Unicode itself does n't support all languages , e.g. Sylheti ; hopefully this will change in time . 11 Perhaps because Outlook Express trashed her mail folder again , or because she got tired of Microsoft - specific viruses and had n't heard of Netscape or Emacs . 12 While developing , she uses a file:/ ... URL ; for deployment she can put them on a web server . 13 Languages other than Java require an additional interface layer , such as JNI , the Java Native Interface , which is in C. \"}",
        "_version_":1692670548559003648,
        "score":22.547695},
      {
        "id":"a6ad67e8-014e-4063-b305-adc7b3b27972",
        "_src_":"{\"url\": \"http://encyclobeamia.solarbotics.net/articles/otu.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701163421.31/warc/CC-MAIN-20160205193923-00174-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"In the Midst of an Evolutionary Explosion . A Decade of Remarkable Advances in Ten Grand IT Challenges . I 've been in the information theory and technology game for quite some time , but believe nothing has matched the pace of advances of the past ten years . As one example , it was a mere eight years ago that I was sitting in a room with language translation vendors contemplating automated translation techniques for US intelligence agencies . The prospects finally looked doable , but the success of large - scale translation was not assured . At about that same time , and the years until just recently , a whole slew of Grand Challenges [ 1 ] in computing hung out there : tantalizing yet not proven . These areas ranged from information extraction and natural language understanding to speech recognition and automated reasoning . But things have been changing fast , and with a subtle steadiness that has caused it to go largely unremarked . Sure , all of us have been aware of the huge changes on the Web and search engine ubiquity and social networking . But some of the fundamentally hard problems in computing have also gone through some remarkable ( but largely unremarked ) advances . We now have smart phones that speak instructions to us while we instruct them by voice in turn . Virtually all information conceivable is now indexed and made available through the Web ; structure is now rapidly characterizing that information , making it even more useful to discover and organize . We can translate documents online with acceptable accuracy into more than 60 languages [ 2 ] . We can get directions to or see satellite views of virtually any place on earth . We have in fact become accustomed to new technology magic on a nearly daily basis , so much so that the pace of these advances seems to be a constant , blunting our perspective of just how rapid these advances have been progressing . These advances are perhaps not the realization of artificial intelligence as articulated in the 1950s to 1980s , but are contributing to a machine - based ability to do tasks useful to humans heretofore impossible and at scales unimaginable . As Google and IBM 's Watson are showing , statistics ( among other techniques ) applied to massive knowledge bases or text corpora are breaking down all of the Grand Challenges of symbolic computing . The image that is emerging is less one of intelligent machines working autonomously than it is of computers working interactively or semi - automatically with humans to address previously unsolvable problems . By using a perspective of the decade past , we also demark the seminal paper on the semantic Web by Berners - Lee , Hendler and Lassila from May 2001 [ 3 ] . Yet , while this semantic Web vision has been a contributor to the success of the Grand Challenge advances of the past ten years , I think we can also say that it has not been the key or even a primary driver . That day may still yet come . Rather , I think we have to look to natural language and statistics surrounding large - scale corpora as the more telling drivers . Ten Grand Challenge Advances . Over the past ten years there have been significant advances on at least ten Grand Challenges in symbolic computation . As the concluding section notes , these advances can be traced in most part to broader advances in natural language processing , the logical and semiotic bases for interoperability , and standards ( nominally in the semantic Web ) for embracing them . Here are these ten areas of advance , all achieved over the past ten years : . # 1 Information Extraction . Information extraction ( IE ) uses various forms of natural language processing ( NLP ) to identify structured information within unstructured or semi - structured documents . These documents are presented in machine - readable form ( including straight text , various document formats or HTML ) with the various types of information \\\" tagged \\\" or prompted for inclusion . Information types that can be extracted with one of the various techniques include entities , relations , topics , categories , and so forth . Once tagged or extracted , the information in the documents can now be included and linked to standard structured information ( as might come from conventional databases ) or to structure in other documents . # 2 Machine Translation . Machine translation is the automatic translation of machine - readable text from one human language to another . Accurate and acceptable machine translation requires applying different types of knowledge including grammar , semantics , facts about the real world , etc . Various approaches have been developed and refined over time . While it is true none of these systems have 100 % accuracy ( even human translators show much variation ) , the more advanced ones are truly impressive with remaining ambiguities flagged for resolution by semi - automatic means . # 3 Sentiment Analysis . Though sentiment analysis is strictly speaking a subset of information extraction , it has the more demanding and useful task of extracting subjective information , often across a group of documents or texts . Sentiment analysis can be applied to online reviews to determine the \\\" polarity \\\" about specific objects , and it is especially useful for identifying public opinion trends or evaluating social media for ranking , polling or marketing purposes . Because of its greater difficulty and potential high value , many of the leading sentiment analysis capabilities remain proprietary . Some capable open source versions are available nonetheleless . There is also an interesting online application using Twitter feeds . # 4 Disambiguation . Many words have more than one meaning . Word sense disambiguation uses either machine learning , dictionaries ( gazetteers ) of known entities and concepts , ontologies or linguistic databases such as WordNet , or combinations thereof to evaluate ambiguous terms or phrases and resolve them based on context . Some systems need to be \\\" trained \\\" or some work automatically or others are based on evaulation and prompting ( semi - automatic ) to complete the disambiguation process . State - of - the - art systems have greater than 90 % precision [ 5 ] . Most of the leading open source NLP toolkits have quite capable disambiguation modules , and even better proprietary systems exist . # 5 Speech Synthesis and Recognition . Speech synthesis is the conversion of text to spoken speech and has been around for quite some time . Speech recognition is a far more difficult task in that a given sound clip or real - time spoken speech of a person must be converted to a textual representation , which itself can then be acted upon such as navigating or making selections . Speech recognition is made difficult because of individual voice differences , the variations of human languages and speech patterns , and the need to segment speech into a sequence of words . ( In most spoken languages , the sounds representing successive letters blend into each other , so the conversion of the modulated wave form to discrete characters or tokens can be a very difficult process . ) Crude systems of a decade ago required much training with a specific speaker 's voice to show much effectiveness . Today , the range and ability to use these systems without training has markedly improved . Until recently , improvements largely were driven by military and intelligence requirements . Today , however , with the ubiquity of smart phones and speech interfaces , the consumer market is greatly accelerating progress . # 6 Image Recognition . Image recognition is the ability to determine whether or not an electronic image contains some specific object , feature , or activity , and then to extract the image data associated with it . Today , under specific circumstances and for specific tasks , this can be done by computer . However , for the general case of arbitrary objects in arbitrary situations this challenge has not yet been fully met . The systems of today work best for simple geometric objects ( e.g . , polyhedra ) , human faces , printed or hand - written characters , or vehicles , and in specific situations , typically described in terms of well - defined illumination , background , and orientation of the object relative to the camera . Auto license recognition at intersections , face recognition by security cameras , and greatly expanded and improved character recognition systems ( machine vision ) represent some of the current state - of - the - art . Again , smart phone apps are helping to drive advances . # 7 Interoperability Standards and Methods . Rapid Progress in Climbing the Data Federation Pyramid . Most of the previous advances are related to extracting structured information or mapping or deriving additional structured information . Once obtained , of course , the next challenge is in how to relate that information together ; that is , how to make it interoperate . We have been steadily climbing a data federation pyramid [ 6 ] - and at an impressively accelerating rate since the adoption of the Internet and Web . These network innovations gave us a common basis and protocols for connecting distributed devices . That , in turn , has freed us to concentrate on the standards for data representation and interoperability . XML first provided a means for a common data serialization that encouraged various communities and industries to devise exchange vocabularies . RDF provided a means for a common data model , one that was both simple and extensible at the same time [ 7 ] . OWL built upon that basis to enable us to build common domain models ( see next ) . There are alternatives to the semantic Web standards of RDF and OWL such as common logic and there are many competing data exchange formats to XML . None of these standards is essential on its own and all have their communities and advocates . However , because they are standards and they share common network bases , it has also been relatively easy to convert amongst the various available protocols . We are nearly at a global level where everything is connected , machine - readable , and in structured form . # 7 Common Domain Models . Semantics in machine - readable form means that we can more confidently link and combine available information . We are seeing a veritable explosion of domain models to represent various domains and viewpoints in consensual , interoperable form . What this means is that we are now gaining the computing vocabularies and grammars - along with shared community models ( world views ) - to get this stuff to work together . Five years ago we called this phenomena mashups , but no one uses that term any longer because these information brewpots are everywhere , including in our very hands when we interact with the apps on our smart phones . This glue of domain models is generally as invisible to us as is the glue in laminates or the resin in plastics . But they are the strength and foundations nonetheless that enable much of the computing magic unfolding around us . # 9 Virtual Apps ( Cloud Computing ) . Once the tyranny of physical separation was shattered between data and machine by the network , the rationale for keeping the data with the app or even the user with the app disappeared . Cloud computing may seem mysterious or sound to have some high - octave hum , but it really is nothing more than saying that the Web enables us to treat all of our computing resources as virtual . Data can be anywhere ; machines and hard drives can be anywhere ; and applications can be anywhere . And , virtualness brings benefits in and of itself . Whole computing environments can be installed or removed nearly instantaneously . Peak computing demands can be met with virtual headrooms . Backup and rollover and redundancy practices and strategies can change . Web services mean tailored capabilities can be invoked from anywhere and integrated for local needs . Massive computing resources and server farms can be as accessible to the individual as they are to prior computing behemoths . Combined with continued advances in underlying computing hardware and chips , the computing power available to any user is rising exponentially . There is now even more power in the power curve . # 10 Big Data . One hears stories of Google or the National Security Agency having access and managing servers measured in the hundreds of thousands . Entirely new operating systems and computing environments - many with roots in open source - such as virtual operating systems and MapReduce approaches like Hadoop have been innovated to deal with the current era of \\\" big data \\\" . MapReduce is a framework for processing huge datasets using a large number of servers . The \\\" map \\\" step partitions the problem into tractable sub - problems , organized in a tree structure . The \\\" reduce \\\" step then takes the answers to all the sub - problems and combines them to produce the final output . Such techniques enable analysis of datasets of a size impossible before . This has enabled the development of statistics and analytical techniques that have been able to make correlations and find patterns for some of the Grand Challenge tasks noted before that simply could not be addressed within previous limits . The \\\" big data \\\" approach is providing a brute force alternative to previously intractable problems . Why Such Progress ? Declining hardware costs and increasing performance ( such as from Moore 's Law ) , combined with the adoption of the Internet + Web network , set the fertile conditions for these unprecedented advances in computing 's Grand Challenges . But the adaptive radiation in innovations now occurring has its own dynamics . In computing terms , we are seeing the equivalent of the Cambrian explosion in evolutionary history . The dynamics driving this computing explosion are based largely , I believe , on the statistics of information retrieval and extraction needed to cope with the scale of documents on the Web . That , in turn , has impelled innovations in big data and distributed architectures and designs that have pried open previously closed computing lockboxes . As data from everywhere and from every provenance pours into the system , means for handling and interoperating with it have become imperatives . These forces , in turn , have been channeled and are being met through the open and standards - based approaches that helped lead to the development of the Internet and its infrastructure in the first place . These powerful evolutionary forces in computing are clearly evident in the ten Grand Challenge advances above . But the challenges above are also silent on another factor , underpinning the interoperability initiatives , that is only now just becoming evident and exerting its own powerful force . That is the workable , intellectual foundations for interoperability itself . Clearly , as the advances in the Grand Challenges show , we are seeing immense exposures of new structured information and impressive means for accessing and managing it on a global , distributed scale . Yet all of this data and this structure begs the question of how to get the information to work together . Further , the sources and viewpoints and methods by which all of this data has been created also puts a huge premium on means to deal with the diversity . Though not evident , and perhaps not even known to many of the innovators and practitioners , there has been a growing intellectual force shaping our foundational views about the nature of things and their representations . This force has been , I believe , one of those root cause drivers helping to show the way to interoperability . John Sowa , despite his unending criticism of the semantic Web in favor of common logic , has nonetheless been a very positive evangelist for the 19th century American logician and philosopher , Charles Sanders Peirce . Sowa points out that the entire 20th century largely neglected Peirce 's significant contributions in many areas and some philosophers appropriated Peircean insights without proper attribution [ 8 ] . Indeed , Peirce has only come to wider attention within the past decade or so . Much of his voluminous lifetime writings have still not yet been committed to publication . Among many notable contributions , Peirce was passionate about signs and their triadic representations , in a field known as semiotics . The philosophical and logical basis of his triangle of signs deserves your attention , which can not be adequately treated here [ 9 ] . However , as summarized by Sowa [ 8 ] , \\\" A semiotic view of language and logic gets to the heart of the philosophical controversies and their practical implications for linguistics , artificial intelligence , and related subjects . \\\" In essence , Peirce 's triadic logic of semiotics helps clarify philosophical questions about things , how they are perceived and how they are named that has vexed philosophers at least since the time of Aristotle . What Peirce was able to put forward was a testable logic for how things and the names of things can be understood and related to one another , via logical statements or structures . These , in turn , can be symbolized and formalized into logical constructs that can capture the structure of natural language as well as more structured data . The clarity of Peirce 's logic of signs is an underlying factor , I believe , for why we are finally seeing our way clear to how to capture , represent and relate information from a diversity of sources and viewpoints that is defensible and interoperable [ 10 ] . As we plumb Peircean logics further , I believe we will continue to gain additional insights and methods for combining and relating information . The next phase of our advances on these Grand Challenges is likely to be fueled more by connections and interoperability than in basic extraction or representation . The Widening Explosion . We are not seeing the vision of artificial intelligence unfold as posed three decades ago . Nor are we seeing the AI - complete type of problems being solved in their entirety [ 11 ] . Rather , we are seeing impressive but incomplete approaches . Full automation and autonomy are not yet at hand , and may be so far in the future as to never be . But we are nevertheless seeing advances across the board in all Grand Challenge areas . What is emerging is a practical achievement of the Grand Challenges , the scale and scope of which is unprecedented in symbolic computing . As we see Peircean logic continue to take hold and interoperability grow in usefulness and stature , I think it fair to say we can look back in ten years to describe where we stand today as having been in the midst of an evolutionary explosion . [ 1 ] Grand Challenges were United States policy objectives for high - performance computing and communications research set in the late 1980s . [ 2 ] For example , as of July 17 , 2011 , Google offered 63 different source or target languages for translation . [ 8 ] John Sowa , 2006 . \\\" Peirce 's Contributions to the 21st Century \\\" , in H. Sch\\u00e4rfe , P. Hitzler , & P. \\u00d8hrstr\\u00f8m , eds . , Conceptual Structures : Inspiration and Application , LNAI 4068 , Springer , Berlin , 2006 , pp . 54 - 69 . [ 9 ] See , as a start , the Wikipedia article on Charles Sanders Peirce ( pronounced \\\" purse \\\" ) , as well as the Arisbe collection of his assembled papers ( to date ) . Also see John Sowa , 2010 . \\\" The Role of Logic and Ontology in Language and Reasoning , \\\" from Chapter 11 of Theory and Applications of Ontology : Philosophical Perspectives , edited by R. Poli & J. Seibt , Berlin : Springer , 2010 , pp . 231 - 263 . Sowa also says , \\\" Although formal logic can be studied independently of natural language semantics , no formal ontology that has any practical application can ever be developed and used without acknowledging its intimate connection with NL semantics . \\\" [ 10 ] While Peirce 's logic and clarity of conceptual relationships is compelling , I find reading his writings quite demanding . [ 11 ] In the field of artificial intelligence , the most difficult problems are informally known as AI - complete or AI - hard , meaning that the difficulty of these computational problems is equivalent to solving the central artificial intelligence problem of making computers as intelligent as people . Computer vision , autonomous robots and understanding natural language are amongst challenges recognized by consensus as being AI - complete . However , practical advances on the Grand Challenges were never defined as needing to meet the AI - complete criterion . Indeed , it is even questionable whether such a hurdle is even worthwhile or meaningful on its own . Over the past ten years there have been significant advances on at least ten Grand Challenges in symbolic computation . These advances can be traced in most part to broader advances in natural language processing , the logical and semiotic bases for interoperability , and standards ( nominally in the semantic Web ) for embracing them . In this work , Kuhn challenged the then prevailing view of progress in \\\" normal science . \\\" Scientific progress had been seen primarily as a continuous increase in a set of accepted facts and theories . Kuhn argued for an episodic model in which periods of such conceptual continuity in normal science were interrupted by periods of revolutionary science . Finally I agree with your observations on AI but these some interesting new development in this area \"}",
        "_version_":1692670421647753217,
        "score":22.313164},
      {
        "id":"9476cc07-77f3-444d-b10c-18f16f915e29",
        "_src_":"{\"url\": \"http://lippard.blogspot.com/2008/10/republicans-kicked-out-of-mccain-event.html\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701999715.75/warc/CC-MAIN-20160205195319-00328-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Edward Kilgarriff , Brahmananda Sapkota , Laurentiu Vasiliu , David Aiken \\\" Christoph Bussler , Dieter Fensel , Uwe Keller , Brahmanada Sapkota ( editors ) \\\" XML to WSML adapter Implementation \\\" , Proceedings of the 2nd WSMO Implementation Workshop , 2005 . en . dc.identifier.uri . en . dc.description.abstract . This paper describes an implementation of an Adapter that converts XML to a Web Service Modeling Language ( WSML ) . WSML is the language used to describe Web Service Modeling Ontology ( WSMO ) concepts , related to Semantic Web services ( SWS ) . SWS are web services that are semantically annotated . The semantic annotation is necessary to address various business logics in an appropriate manner , thus allowing complex business applications to be built and executed . The Web Service Execution Environment ( WSMX ) is an execution environment for dynamic discovery , selection , mediation and invocation of semantic web services . WSMX is a reference implementation for WSMO . Files in this item . This item is available under the Attribution - NonCommercial - NoDerivs 3.0 Ireland . No item may be reproduced for commercial purposes . Please refer to the publisher 's URL where this is made available , or to notes contained in the item itself . Other terms may apply . \"}",
        "_version_":1692668743273938944,
        "score":22.069859},
      {
        "id":"43051447-bb2a-4a77-ba5a-72be3ae97719",
        "_src_":"{\"url\": \"http://nopr.niscair.res.in/handle/123456789/4806\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701168998.49/warc/CC-MAIN-20160205193928-00225-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"View/ Open . Date . Author . Metadata . Usage . Recommended Citation . Edward Kilgarriff , Brahmananda Sapkota , Laurentiu Vasiliu , David Aiken \\\" Christoph Bussler , Dieter Fensel , Uwe Keller , Brahmanada Sapkota ( editors ) \\\" XML to WSML adapter Implementation \\\" , Proceedings of the 2nd WSMO Implementation Workshop , 2005 . Abstract . This paper describes an implementation of an Adapter that converts XML to a Web Service Modeling Language ( WSML ) . WSML is the language used to describe Web Service Modeling Ontology ( WSMO ) concepts , related to Semantic Web services ( SWS ) . SWS are web services that are semantically annotated . The semantic annotation is necessary to address various business logics in an appropriate manner , thus allowing complex business applications to be built and executed . The Web Service Execution Environment ( WSMX ) is an execution environment for dynamic discovery , selection , mediation and invocation of semantic web services . WSMX is a reference implementation for WSMO . URI . Collections . This item is available under the Attribution - NonCommercial - NoDerivs 3.0 Ireland . No item may be reproduced for commercial purposes . Please refer to the publisher 's URL where this is made available , or to notes contained in the item itself . Other terms may apply . \"}",
        "_version_":1692668661790146561,
        "score":22.069859},
      {
        "id":"e702295d-151d-4fc9-8e16-ee8a8afc7fa7",
        "_src_":"{\"url\": \"https://www.ag.ndsu.edu/burleighcountyextension/food-nutrition-and-health\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701162035.80/warc/CC-MAIN-20160205193922-00275-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Alternative locations . Abstract . OntoTag - A Linguistic and Ontological Annotation Model Suitable for the Semantic Web 1 . INTRODUCTION . LINGUISTIC TOOLS AND ANNOTATIONS : THEIR LIGHTS AND SHADOWS Computational Linguistics is already a consolidated research area . It builds upon the results of other two major ones , namely Linguistics and Computer Science and Engineering , and it aims at developing computational models of human language ( or natural language , as it is termed in this area ) . Possibly , its most well - known applications are the different tools developed so far for processing human language , such as machine translation systems and speech recognizers or dictation programs . These tools for processing human language are commonly referred to as linguistic tools . Apart from the examples mentioned above , there are also other types of linguistic tools that perhaps are not so well - known , but on which most of the other applications of Computational Linguistics are built . These other types of linguistic tools comprise POS taggers , natural language parsers and semantic taggers , amongst others . All of them can be termed linguistic annotation tools . Linguistic annotation tools are important assets . In fact , POS and semantic taggers ( and , to a lesser extent , also natural language parsers ) have become critical resources for the computer applications that process natural language . Hence , any computer application that has to analyse a text automatically and ' intelligently ' will include at least a module for POS tagging . The more an application needs to ' understand ' the meaning of the text it processes , the more linguistic tools and/or modules it will incorporate and integrate . However , linguistic annotation tools have still some limitations , which can be summarised as follows : 1 . Normally , they perform annotations only at a certain linguistic level ( that is , Morphology , Syntax , Semantics , etc . ) . They usually introduce a certain rate of errors and ambiguities when tagging . This error rate ranges from 10 percent up to 50 percent of the units annotated for unrestricted , general texts . Their annotations are most frequently formulated in terms of an annotation schema designed and implemented ad hoc . A priori , it seems that the interoperation and the integration of several linguistic tools into an appropriate software architecture could most likely solve the limitations stated in ( 1 ) . Besides , integrating several linguistic annotation tools and making them interoperate could also minimise the limitation stated in ( 2 ) . Nevertheless , in the latter case , all these tools should produce annotations for a common level , which would have to be combined in order to correct their corresponding errors and inaccuracies . Yet , the limitation stated in ( 3 ) prevents both types of integration and interoperation from being easily achieved . In addition , most high - level annotation tools rely on other lower - level annotation tools and their outputs to generate their own ones . For example , sense - tagging tools ( operating at the semantic level ) often use POS taggers ( operating at a lower level , i.e. , the morphosyntactic ) to identify the grammatical category of the word or lexical unit they are annotating . Accordingly , if a faulty or inaccurate low - level annotation tool is to be used by other higher - level one in its process , the errors and inaccuracies of the former should be minimised in advance . Otherwise , these errors and inaccuracies would be transferred to ( and even magnified in ) the annotations of the high - level annotation tool . Clearly , solving ( i ) and ( ii ) should ease the automatic annotation of web pages by means of linguistic tools , and their transformation into Semantic Web pages ( Berners - Lee , Hendler and Lassila , 2001 ) . Yet , as stated above , ( ii ) is a type of interoperability problem . There again , ontologies ( Gruber , 1993 ; Borst , 1997 ) have been successfully applied thus far to solve several interoperability problems . Hence , ontologies should help solve also the problems and limitations of linguistic annotation tools aforementioned . The concrete goals that helped attain this aim are presented in the following section . Besides , to be useful for the Semantic Web , this model should provide a way to automate the annotation of web pages . Therefore , this model had to minimise these limitations by means of the integration of several linguistic annotation tools into a common architecture . Since this integration required the interoperation of tools and their annotations , ontologies were proposed as the main technological component to make them effectively interoperate . From the very beginning , it seemed that the formalisation of the elements and the knowledge underlying linguistic annotations within an appropriate set of ontologies would be a great step forward towards the formulation of such a model ( henceforth referred to as OntoTag ) . Obviously , first , to combine the results of the linguistic annotation tools that operated at the same level , their annotation schemas had to be unified ( or , preferably , standardised ) in advance . This entailed the unification ( i d . standardisation ) of their tags ( both their representation and their meaning ) , and their format or syntax . Second , to merge the results of the linguistic annotation tools operating at different levels , their respective annotation schemas had to be ( a ) made interoperable and ( b ) integrated . And third , in order for the resulting annotations to suit the Semantic Web , they had to be specified by means of an ontology - based vocabulary , and structured by means of ontology - based triples , as hinted above . Therefore , a new annotation scheme had to be devised , based both on ontologies and on this type of triples , which allowed for the combination and the integration of the annotations of any set of linguistic annotation tools . This annotation scheme was considered a fundamental part of the model proposed here , and its development was , accordingly , another major objective of the present work . All these goals , aims and objectives could be re - stated more clearly as follows : Goal 1 : Development of a set of ontologies for the formalisation of the linguistic knowledge relating linguistic annotation . Sub - goal 1.2 : Incorporation into this preliminary ontological formalisation of other existing standards and standard proposals relating the levels mentioned above , such as those currently under development within ISO / TC 37 ( the ISO Technical Committee dealing with Terminology , which deals also with linguistic resources and annotations ) . Sub - goal 1.3 : Generalisation and extension of the recommendations in EAGLES ( 1996a ; 1996b ) and ISO / TC 37 to the semantic level , for which no ISO / TC 37 standards have been developed yet . Sub - goal 1.4 : Ontological formalisation of the generalisations and/or extensions obtained in the previous sub - goal as generalisations and/or extensions of the corresponding ontology ( or ontologies ) . Sub - goal 1.5 : Ontological formalisation of the knowledge required to link , combine and unite the knowledge represented in the previously developed ontology ( or ontologies ) . Goal 2 : Development of OntoTag 's annotation scheme , a standard - based abstract scheme for the hybrid ( linguistically - motivated and ontological - based ) annotation of texts . Sub - goal 2.1 : Development of the standard - based morphosyntactic annotation level of OntoTag 's scheme . This level should include , and possibly extend , the recommendations of EAGLES ( 1996a ) and also the recommendations included in the ISO / MAF ( 2008 ) standard draft . Sub - goal 2.2 : Development of the standard - based syntactic annotation level of the hybrid abstract scheme . This level should include , and possibly extend , the recommendations of EAGLES ( 1996b ) and the ISO / SynAF ( 2010 ) standard draft . Sub - goal 2.3 : Development of the standard - based semantic annotation level of OntoTag 's ( abstract ) scheme . Sub - goal 2.4 : Development of the mechanisms for a convenient integration of the three annotation levels already mentioned . These mechanisms should take into account the recommendations included in the ISO / LAF ( 2009 ) standard draft . Sub - goal 3.1 : Specification of the decanting processes that allow for the classification and separation , according to their corresponding levels , of the results of the linguistic tools annotating at several different levels . Sub - goal 3.2 : Specification of the standardisation processes that allow ( a ) complying with the standardisation requirements of OntoTag 's annotation scheme , as well as ( b ) combining the results of those linguistic tools that share some level of annotation . Sub - goal 3.3 : Specification of the merging processes that allow for the combination of the output annotations and the interoperation of those linguistic tools that share some level of annotation . Sub - goal 3.4 : Specification of the merge processes that allow for the integration of the results and the interoperation of those tools performing their annotations at different levels . Goal 4 : Generation of OntoTagger 's schema , a concrete instance of OntoTag 's abstract scheme for a concrete set of linguistic annotations . This schema should help evaluate OntoTag 's underlying hypotheses , stated below . Consequently , it should implement , at least , those levels of the abstract scheme dealing with the annotations of the set of tools considered in this implementation . This includes the morphosyntactic , the syntactic and the semantic levels . Goal 5 : Implementation of OntoTagger 's configuration , a concrete instance of OntoTag 's abstract architecture for this set of linguistic tools and annotations . This configuration ( 1 ) had to use the schema generated in the previous goal ; and ( 2 ) should help support or refute the hypotheses of this work as well ( see the next section ) . Sub - goal 5.2 : Implementation of the standardisation processes that allow ( i ) specifying the results of those linguistic tools that share some level of annotation according to the requirements of OntoTagger 's schema , as well as ( ii ) combining these shared level results . In particular , all the tools selected perform morphosyntactic annotations and they had to be conveniently combined by means of these processes . Sub - goal 5.4 : Implementation of the merging processes that allow for the integration of the different standardised and combined annotations aforementioned , relating all the levels considered . Accordingly , several hypotheses had to be supported ( or rejected ) by the development of the OntoTag model and OntoTagger ( its implementation ) . The hypotheses underlying OntoTag are surveyed below . Only one of the hypotheses ( H.6 ) was rejected ; the other five could be confirmed . H.1 The annotations of different levels ( or layers ) can be integrated into a sort of overall , comprehensive , multilayer and multilevel annotation , so that their elements can complement and refer to each other . H.2 Tool - dependent annotations can be mapped onto a sort of tool - independent annotations and , thus , can be standardised . H.3 Standardisation should ease : H.3.1 : The interoperation of linguistic tools . H.3.2 : The comparison , combination ( at the same level and layer ) and integration ( at different levels or layers ) of annotations . o Integration of morphosyntactic , syntactic and semantic annotations . H.4 Ontologies and Semantic Web technologies ( can ) play a crucial role in the standardisation of linguistic annotations , by providing consensual vocabularies and standardised formats for annotation ( e.g. , RDF triples ) . H.5 The rate of errors introduced by a linguistic tool at a given level , when annotating , can be reduced automatically by contrasting and combining its results with the ones coming from other tools , operating at the same level . However , these other tools might be built following a different technological ( stochastic vs. rule - based , for example ) or theoretical ( dependency vs. HPS - grammar - based , for instance ) approach . H.6 Each linguistic level can be managed and annotated independently . In fact , Hypothesis H.6 was already rejected when OntoTag 's ontologies were developed . We observed then that several linguistic units stand on an interface between levels , belonging thereby to both of them ( such as morphosyntactic units , which belong to both the morphological level and the syntactic level ) . Therefore , the annotations of these levels overlap and can not be handled independently when merged into a unique multileveled annotation . OTHER MAIN RESULTS AND CONTRIBUTIONS First , interoperability is a hot topic for both the linguistic annotation community and the whole Computer Science field . The specification ( and implementation ) of OntoTag 's architecture for the combination and integration of linguistic ( annotation ) tools and annotations by means of ontologies shows a way to make these different linguistic annotation tools and annotations interoperate in practice . Second , as mentioned above , the elements involved in linguistic annotation were formalised in a set ( or network ) of ontologies ( OntoTag 's linguistic ontologies ) . Third , we showed that the combination of the results of tools annotating at the same level can yield better results ( both in precision and in recall ) than each tool separately . In particular , 1 . OntoTagger clearly outperformed two of the tools integrated into its configuration , namely DataLexica and FDG in all the combination sub - phases in which they overlapped ( i.e. POS tagging , lemma annotation and morphological feature annotation ) . As far as the remaining tool is concerned , i.e. LACELL 's tagger , it was also outperformed by OntoTagger in POS tagging and lemma annotation , and it did not behave better than OntoTagger in the morphological feature annotation layer . Fourth , Semantic Web annotations are usually performed by humans or else by machine learning systems . Both of them leave much to be desired : the former , with respect to their annotation rate ; the latter , with respect to their ( average ) precision and recall . In this work , we showed how linguistic tools can be wrapped in order to annotate automatically Semantic Web pages using ontologies . This entails their fast , robust and accurate semantic annotation . As a way of example , as mentioned in Sub - goal 5.5 , we developed a particular OntoTagger module for the recognition , classification and labelling of named entities , according to the MUC and ACE tagsets ( Chinchor , 1997 ; Doddington et al . , 2004 ) . These tagsets were further specified by means of a domain ontology , namely the Cinema Named Entities Ontology ( CNEO ) . This module was applied to the automatic annotation of ten different web pages containing cinema reviews ( that is , around 5000 words ) . In addition , the named entities annotated with this module were also labelled as instances ( or individuals ) of the classes included in the CNEO and , then , were used to populate this domain ontology . They can be explained on the basis of the high accuracy of the annotations provided by OntoTagger at the lower levels ( mainly at the morphosyntactic level ) . However , they should be conveniently qualified , since they might be too domain- and/or language - dependent . It should be further experimented how our approach works in a different domain or a different language , such as French , English , or German . Fifth , as shown in the State of the Art of this work , there are different approaches and models for the semantic annotation of texts , but all of them focus on a particular view of the semantic level . Clearly , all these approaches and models should be integrated in order to bear a coherent and joint semantic annotation level . OntoTag shows how ( i ) these semantic annotation layers could be integrated together ; and ( ii ) they could be integrated with the annotations associated to other annotation levels . Sixth , we identified some recommendations , best practices and lessons learned for annotation standardisation , interoperation and merge . They show how standardisation ( via ontologies , in this case ) enables the combination , integration and interoperation of different linguistic tools and their annotations into a multilayered ( or multileveled ) linguistic annotation , which is one of the hot topics in the area of Linguistic Annotation . And last but not least , OntoTag 's annotation scheme and OntoTagger 's annotation schemas show a way to formalise and annotate coherently and uniformly the different units and features associated to the different levels and layers of linguistic annotation . This is a great scientific step ahead towards the global standardisation of this area , which is the aim of ISO / TC 37 ( in particular , Subcommittee 4 , dealing with the standardisation of linguistic annotations and resources ) . \"}",
        "_version_":1692670202946256896,
        "score":21.743874}]
  }}
