{
  "responseHeader":{
    "status":0,
    "QTime":1,
    "params":{
      "q":"previous work algorithm similar parser features training set data experiments approach results feature grammar parsing method nivre juola obtained technique",
      "fl":"*,score"}},
  "response":{"numFound":2979189,"start":0,"maxScore":24.396542,"numFoundExact":true,"docs":[
      {
        "id":"3339a7c3-a62e-46d8-b26c-bb1d3c5ec4d4",
        "_src_":"{\"url\": \"http://cyberinsecure.com/wordpress-doorway-spam-attacks/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454702039825.90/warc/CC-MAIN-20160205195359-00135-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Accepted papers . Conversational Speech Transcription Using Context - Dependent Deep Neural Networks . Dong Yu , Frank Seide , Gang Li . Abstract : Context - Dependent Deep - Neural - Network HMMs , or CD - DNN - HMMs , combine the classic artificial - neural - network HMMs with traditional context - dependent acoustic modeling and deep - belief - network pre - training . CD - DNN - HMMs greatly outperform conventional CD - GMM ( Gaussian mixture model ) HMMs : The word error rate is reduced by up to one third on the difficult benchmarking task of speaker - independent single - pass transcription of telephone conversations . Data - driven Web Design . Ranjitha Kumar , Jerry Talton , Salman Ahmad , Scott Klemmer . Abstract : This short paper summarizes challenges and opportunities of applying machine learning methods to Web design problems , and describes how structured prediction , deep learning , and probabilistic program induction can enable useful interactions for designers . We intend for these techniques to foster new work in data - driven Web design . Learning the Central Events and Participants in Unlabeled Text . Nathanael Chambers , Dan Jurafsky . Abstract : The majority of information on the Internet is expressed in written text . Understanding and extracting this information is crucial to building intelligent systems that can organize this knowledge . Today , most algorithms focus on learning atomic facts and relations . For instance , we can reliably extract facts like ' Annapolis is a City ' by observing redundant word patterns across a corpus . However , these facts do not capture richer knowledge like the way detonating a bomb is related to destroying a building , or that the perpetrator who was convicted must have been arrested . A structured model of these events and entities is needed for a deeper understanding of language . This talk describes unsupervised approaches to learning such rich knowledge . Exemplar - SVMs for Visual Ob ject Detection , Label Transfer and Image Retrieval . Tomasz Malisiewicz , Abhinav Shrivastava , Abhinav Gupta , Alexei Efros . While conventional wisdom tends to attribute the success of such methods to the ability of the classifier to generalize across the positive class instances , here we report on empirical findings suggesting that this might not necessarily be the case . We have experimented with a very simple idea : to learn a separate classifier for each positive object instance in the dataset . In this setup , no generalization across the positive instances is possible by definition , and yet , surprisingly , we did not observe any drastic drop in performance compared to the standard , category - based approaches . Capturing topical content with frequency and exclusivity . Jonathan Bischof , Edoardo Airoldi . HPC uses known hierarchical structure on human labeled topics to make focused comparisons of differential usage within each branch of the tree . We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation . TrueLabel + Confusions : A Spectrum of Probabilistic Models in Analyzing Multiple Ratings . Chao Liu , Yi - Min Wang . Abstract : This paper revisits the problem of analyzing multiple ratings given by different judges . Different from previous work that focuses on distilling the true labels from noisy crowdsourcing ratings , we emphasize gaining diagnostic insights into our in - house well - trained judges . Robust Multiple Manifold Structure Learning . Dian Gong , Xuemei Zhao , Gerard Medioni . Abstract : We present a robust multiple manifold structure learning ( RMMSL ) scheme to robustly estimate data structures under the multiple low intrinsic dimensional manifolds assumption . In the local learning stage , RMMSL efficiently estimates local tangent space by weighted low - rank matrix factorization . In the global learning stage , we propose a robust manifold clustering method based on local structure learning results . The proposed clustering method is designed to get the flattest manifolds clusters by introducing a novel curved - level similarity function . Our approach is evaluated and compared to state - of - the - art methods on synthetic data , handwritten digit images , human motion capture data and motorbike videos . We demonstrate the effectiveness of the proposed approach , which yields higher clustering accuracy , and produces promising results for challenging tasks of human motion segmentation and motion flow learning from videos . Two Manifold Problems with Applications to Nonlinear System Identification . Byron Boots , Geoff Gordon . Abstract : Recently , there has been much interest in spectral approaches to learning manifolds - so - called kernel eigenmap methods . These methods have had some successes , but their applicability is limited because they are not robust to noise . To address this limitation , we look at two - manifold problems , in which we simultaneously reconstruct two related manifolds , each representing a different view of the same data . By solving these interconnected learning problems together , two - manifold algorithms are able to succeed where a non - integrated approach would fail : each view allows us to suppress noise in the other , reducing bias . We propose a class of algorithms for two - manifold problems , based on spectral decomposition of cross - covariance operators in Hilbert space and discuss when two - manifold problems are useful . Finally , we demonstrate that solving a two - manifold problem can aid in learning a nonlinear dynamical system from limited data . On the Difficulty of Nearest Neighbor Search . Junfeng He , Sanjiv Kumar , Shih - Fu Chang . Abstract : Fast approximate nearest neighbor search in large databases is becoming popular . Several powerful learning - based formulations have been proposed recently . However , not much attention has been paid to a more fundamental question : how difficult is ( approximate ) nearest neighbor search in a given data set ? More broadly , which data properties affect the nearest neighbor search and how ? This paper introduces the first concrete measure called Relative Contrast that can be used to evaluate the influence of several crucial data characteristics such as dimensionality , sparsity , and database size simultaneously in arbitrary normed metric spaces . To further justify why relative contrast is an important and effective measure , we present a theoretical analysis to prove how relative contrast determines / affects the performance / complexity of Locality Sensitive Hashing , a popular hashing based approximate nearest neighbor search method . Finally , relative contrast also provides an explanation for a family of heuristic hashing algorithms based on PCA with good practical performance . Learning Force Control Policies for Compliant Robotic Manipulation . Mrinal Kalakrishnan , Ludovic Righetti , Peter Pastor , Stefan Schaal . Abstract : Developing robots capable of fine manipulation skills is of major importance in order to build truly assistive robots . These robots need to be compliant in their actuation and control in order to operate safely in human environments . Manipulation tasks imply complex contact interactions with the external world , and involve reasoning about the forces and torques to be applied . Planning under contact conditions is usually impractical due to computational complexity , and a lack of precise dynamics models of the environment . We present an approach to acquiring manipulation skills on compliant robots through reinforcement learning . The initial position control policy for manipulation is initialized through kinesthetic demonstration . This policy is augmented with a force / torque profile to be controlled in combination with the position trajectories . The Policy Improvement with Path Integrals ( PI^2 ) algorithm is used to learn these force / torque profiles by optimizing a cost function that measures task success . We introduce a policy representation that ensures trajectory smoothness during exploration and learning . Our approach is demonstrated on the Barrett WAM robot arm equipped with a 6-DOF force / torque sensor on two different manipulation tasks : opening a door with a lever door handle , and picking up a pen off the table . We show that the learnt force control policies allow successful , robust execution of the tasks . Estimation of Simultaneously Sparse and Low Rank Matrices . Pierre - Andr\\u00e9 Savalle , Emile Richard , Nicolas Vayatis . Abstract : The paper introduces a penalized matrix estimation procedure aiming at solutions which are sparse and low - rank at the same time . Such structures arise in the context of social networks or protein interactions where underlying graphs have adjacency matrices which are block - diagonal in the appropriate basis . We obtain an oracle inequality which indicates how the two effects interact according to the nature of the target matrix . We bound generalization error in the link prediction problem . We also develop proximal descent strategies to solve the the optimization problem efficiently and evaluate performance on synthetic and real data sets . Online Structured Prediction via Coactive Learning . Pannaga Shivaswamy , Thorsten Joachims . Abstract : We propose Coactive Learning as a model of interaction between a learning system and a human user , where both have the common goal of providing results of maximum utility to the user . At each step , the system ( e.g. search engine ) receives a context ( e.g. query ) and predicts an object ( e.g. ranking ) . The user responds by correcting the system if necessary , providing a slightly improved - but not necessarily optimal - object as feedback . We argue that such feedback can be inferred from observable user behavior , specifically clicks in web search . We demonstrate the applicability of our model and learning algorithms on a movie recommendation task , as well as ranking for web search . Using CCA to improve CCA : A new spectral method for estimating vector models of words . Paramveer Dhillon , Jordan Rodu , Dean Foster , Lyle Ungar . Abstract : Unlabeled data is often used to learn representations which can be used to supplement baseline features in a supervised learner . For example , for text applications where the words lie in a very high dimensional space ( the size of the vocabulary ) , one can learn a low rank \\\" dictionary \\\" by an eigen - decomposition of the word co - occurrence matrix ( e.g. using PCA or CCA ) . In this paper , we present a new spectral method based on CCA to learn an eigenword dictionary . Our improved procedure computes two set of CCAs , the first one between the left and right contexts of the given word and the second one between the projections resulting from this CCA and the word itself . A Discrete Optimization Approach for Supervised Ranking with an Application to Reverse - Engineering Quality Ratings . Allison Chang , Cynthia Rudin , Dimitris Bertsimas , Michael Cavaretta , Robert Thomas , Gloria Chou . - Not for proceedings . Abstract : We present a new methodology based on mixed integer optimization ( MIO ) for supervised ranking tasks . Other methods for supervised ranking approximate ranking quality measures by convex functions in order to accommodate extremely large problems , at the expense of exact solutions . As our MIO approach provides exact modeling for ranking problems , our solutions are benchmarks for the other non - exact methods . We report computational results that demonstrate significant advantages for MIO methods over current state - of - the - art . We also use our technique for a new application : reverse - engineering quality rankings . A good or bad product quality rating can make or break an organization , and in order to invest wisely in product development , organizations are starting to use intelligent approaches to reverse - engineer the rating models . We present experiments on data from a major quality rating company , and provide new methods for evaluating the solution . In addition , we provide an approach to use the reverse - engineered model to achieve a top ranked product in a cost - effective way . Bounded Planning in Passive POMDPs . Roy Fox , Naftali Tishby . Abstract : In Passive POMDPs actions do not affect the world state , but still incur costs . When the agent is bounded by information - processing constraints , it can only keep an approximation of the belief . We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost , and introduce an efficient and simple algorithm for finding an optimum . Minimizing The Misclassification Error Rate Using a Surrogate Convex Loss . Shai Ben - David , David Loker , Nathan Srebro , Karthik Sridharan . Abstract : We carefully study how well minimizing convex surrogate loss functions , corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors . In particular , we show that amongst all convex surrogate losses , the hinge loss gives essentially the best possible bound , of all convex loss functions , for the misclassification error rate of the resulting linear predictor in terms of the best possible margin error rate . We also provide lower bounds for specific convex surrogates that show how different commonly used losses qualitatively differ from each other . Bayesian Efficient Multiple Kernel Learning . Mehmet G\\u00f6nen . Abstract : Multiple kernel learning algorithms are proposed to combine kernels in order to obtain a better similarity measure or to integrate feature representations coming from different data sources . Most of the previous research on such methods is focused on the computational efficiency issue . However , it is still not feasible to combine many kernels using existing Bayesian approaches due to their high time complexity . We propose a fully conjugate Bayesian formulation and derive a deterministic variational approximation , which allows us to combine hundreds or thousands of kernels very efficiently . We briefly explain how the proposed method can be extended for multiclass learning and semi - supervised learning . Experiments with large numbers of kernels on benchmark data sets show that our inference method is quite fast , requiring less than a minute . On one bioinformatics and three image recognition data sets , our method outperforms previously reported results with better generalization performance . Bayesian Nonexhaustive Learning for Online Discovery and Modeling of Emerging Classes . Murat Dundar , Ferit Akova , Alan Qi , Bartek Rajwa . Abstract : In this study we present a framework for online inference in the presence of a nonexhaustively defined set of classes that incorporates supervised classification with class discovery and modeling . A Dirichlet process prior ( DPP ) model defined over class distributions ensures that both known and unknown class distributions originate according to a common base distribution . In an attempt to automatically discover potentially interesting class formations , the prior model is coupled with a suitably chosen data model , and sequential Monte Carlo sampling is used to perform online inference . Exact Soft Confidence - Weighted Learning . Steven C.H. Hoi , Jialei Wang , Peilin Zhao . Abstract : In this paper , we propose a new Soft Confidence - Weighted ( SCW ) online learning scheme , which enables the conventional confidence - weighted learning method to handle non - separable cases . Unlike the previous confidence - weighted learning algorithms , the proposed soft confidence - weighted learning method enjoys all the four salient properties : ( i ) large margin training , ( ii ) confidence weighting , ( iii ) capability to handle non - separable data , and ( iv ) adaptive margin . Our experimental results show that SCW performs significantly better than the original CW algorithm . When comparing with the state - of - the - art AROW algorithm , we found that SCW in general achieves better or at least comparable predictive performance , but enjoys considerably better efficiency performance ( i.e. , producing less number of updates and spending less time cost ) . Distributed Tree Kernels . Fabio Massimo Zanzotto , Lorenzo Dell'Arciprete . Abstract : In this paper , we propose the distributed tree kernels ( DTK ) as a novel method to reduce time and space complexity of tree kernels . Using a linear complexity algorithm to compute vectors for trees , we embed feature spaces of tree fragments in low - dimensional spaces where the kernel computation is directly done with dot product . We show that DTKs are faster , correlate with tree kernels , and obtain a statistically similar performance in two natural language processing tasks . Multiple Kernel Learning from Noisy Labels by Stochastic Programming . Tianbao Yang , Mehrdad Mahdavi , Rong Jin , Lijun Zhang , Yang Zhou , . Abstract : We study the problem of multiple kernel learning from noisy labels . This is in contrast to most of the previous studies on multiple kernel learning that mainly focus on developing efficient algorithms and assume perfectly labeled training examples . Directly applying the existing multiple kernel learning algorithms to noisily labeled examples often leads to suboptimal performance due to the incorrect class assignments . We address this challenge by casting multiple kernel learning from noisy labels into a stochastic programming problem , and presenting a minimax formulation . We develop an efficient algorithm for solving the related convex - concave optimization problem with a fast convergence rate of O(1/T ) where T is the number of iterations . Empirical studies on UCI data sets verify both the effectiveness of the proposed framework and the efficiency of the proposed optimization algorithm . Improved Nystrom Low - rank Decomposition with Priors . Kai Zhang , Liang Lan , Jun Liu , andreas Rauber . Abstract : Low - rank matrix decomposition has gained great popularity recently in scaling up kernel methods to large amount of data . However , some limitations could prevent them from working effectively in certain domains . To solve these problems , in this paper we propose an \\\" inductive\\\"-flavored method for low - rank kernel decomposition with priors or side information . We achieve this by generalizing the Nystr\\u00f3m method in a novel way . Empirical results demonstrate the efficacy and efficiency of the proposed method . Active Learning for Matching Problems . Laurent Charlin , Rich Zemel , Craig Boutilier . Abstract : Effective learning of user preferences is critical to easing user burden in various types of matching problems . Equally important is active query selection to further reduce the amount of preference information users must provide . We address the problem of active learning of user preferences for matching problems , introducing a novel method for determining probabilistic matchings , and developing several new active learning strategies that are sensitive to the specific matching objective . Experiments with real - world data sets spanning diverse domains demonstrate that matching - sensitive active learning outperforms standard techniques . Ensemble Methods for Convex Regression with Applications to Geometric Programming Based Circuit Design . Lauren Hannah , David Dunson . Abstract : Convex regression is a promising area for bridging statistical estimation and deterministic convex optimization . We develop a new piecewise linear convex regression method that uses the Convex Adaptive Partitioning ( CAP ) estimator in an ensemble setting , Ensemble Convex Adaptive Partitioning ( E - CAP ) . The ensembles alleviate some problems associated with convex piecewise linear estimators , such as instability when used to approximate constraints or objective functions for optimization , while maintaining desirable properties , such as consistency and O(n log(n)^2 ) computational complexity . We empirically demonstrate that E - CAP outperforms existing convex regression methods both when used for prediction and optimization . We then apply E - CAP to device modeling and constraint approximation for geometric programming based circuit design . Groupwise Constrained Reconstruction for Subspace Clustering . Ruijiang Li , Bin Li , Cheng Jin , Xiangyang Xue . Abstract : Recent proposed subspace clustering methods first compute a self reconstruction matrix for dataset , then converted it to an affinity matrix , before input to a spectral clustering method to obtain the final clustering result . Their success is largely based on the subspace independence assumption , which , however , does not always hold for the applications with increasing number of clusters such as face clustering . In this paper , we proposes a novel reconstruction based subspace clustering method without making the subspace independence assumption . In our model , certain properties of the reconstruction matrix are explicitly characterized using the latent cluster indicators , and the affinity matrix input to the spectral clustering is built from the posterior of the cluster indicators . Evaluation on both synthetic and real - world datasets show that our method can outperform the state - of - the - arts . Stability of matrix factorization for collaborative filtering . Yu - Xiang Wang , Huan Xu . Abstract : We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion . We apply these results to the problem of collaborative filtering under manipulator attack , which leads to useful insights and guidelines for collaborative filtering system design . Adaptive Regularization for Similarity Measures . Koby Crammer , Gal Chechik . Abstract : Algorithms for learning distributions over weight - vectors , such as AROW were recently shown empirically to achieve state - of - the - art performance at various problems , with strong theoretical guaranties . Extending these algorithms to matrix models poses a challenge since the number of free parameters in the covariance of the distribution scales as n^4 with the dimension n of the matrix . We describe , analyze and experiment with two new algorithms for learning distribution of matrix models . Our first algorithm maintains a diagonal covariance over the parameters and is able to handle large covariance matrices . The second algorithm factores the covariance capturing some inter - features correlation while keeping the number of parameters linear in the size of the original matrix . We analyze the diagonal algorithm in the mistake bound model and show the superior precision of our approach over other algorithms in two tasks : retrieving similar images , and ranking similar documents . The second algorithms is shown to attain faster convergence rate . Linear Off - Policy Actor - Critic . Thomas Degris , Martha White , Richard Sutton . Abstract : This paper presents the first off - policy actor - critic reinforcement learning algorithm with a per - time - step complexity that scales linearly with the number of learned parameters . Previous work on actor - critic algorithms is limited to the on - policy setting and does not take advantage of the recent advances in off - policy gradient temporal - difference learning . Off - policy techniques , such as Greedy - GQ , enable a target policy to be learned while following and obtaining data from another ( behavior ) policy . For many problems , however , actor - critic methods are more practical than action value methods ( like Greedy - GQ ) because they explicitly represent the policy ; consequently , the policy can be stochastic and utilize a large action space . In this paper , we illustrate how to practically combine the generality and learning potential of off - policy learning with the flexibility in action selection given by actor - critic methods . We derive an incremental , linear time and space complexity algorithm that includes eligibility traces , prove convergence under assumptions similar to previous off - policy algorithms , and empirically show better or comparable performance to existing algorithms on standard reinforcement - learning benchmark problems . Modeling Latent Variable Uncertainty for Loss - based Learning . M. Pawan Kumar , Ben Packer , Daphne Koller . Abstract : We consider the problem of parameter estimation using weakly supervised datasets , where a training sample consists of the input and a partially specified annotation ( called the output ) . In addition , the missing information in the annotation is modeled using latent variables . Traditional methods , such as expectation - maximization , overburden a single distribution with two separate tasks : ( i ) modeling the uncertainty in the latent variables during training ; and ( ii ) making accurate predictions for the output and the latent variables during testing . During learning , we encourage agreement between the two distributions by minimizing a loss - based dissimilarity coefficient . We demonstrate the efficacy of our approach on two challenging problems - object detection and action detection - using publicly available datasets . Dimensionality Reduction by Local Discriminative Gaussians . Nathan Parrish , Maya Gupta . Abstract : We present local discriminative Gaussian ( LDG ) dimensionality reduction , a supervised linear dimensionality reduction technique that acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data . We focus on the classification setting ; however , our algorithm can be applied whenever training data is accompanied with similarity or dissimilarity constraints . Our experiments show that LDG is superior to other state - of - the - art linear dimensionality reduction techniques when the number of features in the original data is large . We also adapt LDG to the transfer learning setting , and show that it achieves good performance when the test data distribution differs from that of the training data . Learning to Label Aerial Images from Noisy Data . Volodymyr Mnih , Geoffrey Hinton . Abstract : When training a system to label images , the amount of labeled training data tends to be a limiting factor . We consider the task of learning to label aerial images from existing maps . These provide abundant labels , but the labels are often incomplete and sometimes poorly registered . We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets . The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider . The Most Persistent Soft - Clique in a Set of Sampled Graphs . Novi Quadrianto , Chao Chen , Christoph Lampert . Abstract : When searching for characteristic subpatterns in potentially noisy graph data , it appears self - evident that having multiple observations would be better than having just one . However , it turns out that the inconsistencies introduced when different graph instances have different edge sets pose a serious challenge . In this work we address this challenge for the problem of finding maximum weighted cliques . We introduce the concept of most persistent soft - clique . This is subset of vertices , that 1 ) is almost fully or at least densely connected , 2 ) occurs in all or almost all graph instances , and 3 ) has the maximum weight . We present a measure of clique - ness , that essentially counts the number of edge missing to make a subset of vertices into a clique . With this measure , we show that the problem of finding the most persistent soft - clique problem can be cast either as : a ) a max - min two person game optimization problem , or b ) a min - min soft margin optimization problem . Both formulations lead to the same solution when using a partial Lagrangian method to solve the optimization problems . By experiments on synthetic data and on real social network data , we show that the proposed method is able to reliably find soft cliques in graph data , even if that is distorted by random noise or unreliable observations . Learning Efficient Structured Sparse Models . Alex Bronstein , Pablo Sprechmann , Guillermo Sapiro . Abstract : We present a comprehensive framework for structured sparse coding and modeling extending the recent ideas of using learnable fast regressors to approximate exact sparse codes . For this purpose , we develop a novel block - coordinate proximal splitting method for the iterative solution of hierarchical sparse coding problems , and show an efficient feed forward architecture derived from its iteration . This architecture faithfully approximates the exact structured sparse codes with a fraction of the complexity of the standard optimization methods . We also show that by using different training objective functions , learnable sparse encoders are no longer restricted to be mere approximants of the exact sparse code for a pre - given dictionary , as in earlier formulations , but can be rather used as full - featured sparse encoders or even modelers . A simple implementation shows several orders of magnitude speedup compared to the state - of - the - art at minimal performance degradation , making the proposed framework suitable for real time and large - scale applications . PAC Subset Selection in Stochastic Multi - armed Bandits . Shivaram Kalyanakrishnan , Ambuj Tewari , Peter Auer , Peter Stone . Abstract : We consider the problem of selecting , from among the arms of a stochastic n - armed bandit , a subset of size m of those arms with the highest expected rewards , based on efficiently sampling the arms . This \\\" subset selection \\\" problem finds application in a variety of areas . Kalyanakrishnan & Stone ( 2010 ) frame this problem under a PAC setting ( denoting it \\\" Explore - m \\\" ) and analyze corresponding sampling algorithms both formally and experimentally . Whereas their formal analysis is restricted to the worst case sample complexity of algorithms , in this paper , we design and analyze an algorithm ( \\\" LUCB \\\" ) with improved expected sample complexity . Interestingly LUCB bears a close resemblance to the well - known UCB algorithm for regret minimization . We also provide a lower bound on the worst case sample complexity of PAC algorithms for Explore - m . Nonparametric variational inference . Samuel Gershman , Matt Hoffman , David Blei . Abstract : Variational methods are widely used for approximate posterior inference . However , their use is typically limited to families of distributions that enjoy particular conjugacy properties . To circumvent this limitation , we propose a family of variational approximations inspired by nonparametric kernel density estimation . The locations of these kernels and their bandwidth are treated as variational parameters and optimized to improve an approximate lower bound on the marginal likelihood of the data . Using multiple kernels allows the approximation to capture multiple modes of the posterior , unlike most other variational approximations . We demonstrate the efficacy of the nonparametric approximation with a hierarchical logistic regression model and a nonlinear matrix factorization model . We obtain predictive performance as good as or better than more specialized variational methods and sample - based approximations . The method is easy to apply to more general graphical models for which standard variational methods are difficult to derive . The Convexity and Design of Composite Multiclass Losses . Mark Reid , Robert Williamson , Peng Sun . Abstract : We consider composite loss functions for multiclass prediction comprising a proper ( i.e. , Fisher - consistent ) loss over probability distributions and an inverse link function . We establish conditions for their ( strong ) convexity and explore their implications . We also show how the separation of concerns afforded by using this composite representation allows for the design of families of losses with the same Bayes risk . Finding Botnets Using Minimal Graph Clusterings . Peter Haider , Tobias Scheffer . Abstract : We study the problem of identifying botnets and the IP addresses which they comprise , based on the observation of a fraction of the global email spam traffic . Observed mailing campaigns constitute evidence for joint botnet membership , they are represented by cliques in the graph of all messages . No evidence against an association of nodes is ever available . We reduce the problem of identifying botnets to a problem of finding a minimal clustering of the graph of messages . We directly model the distribution of clusterings given the input graph ; this avoids potential errors caused by distributional assumptions of a generative model . We report on a case study in which we evaluate the model by its ability to predict the spam campaign that a given IP address is going to participate in . Learning the Experts for Online Sequence Prediction . Elad Eban , Aharon Birnbaum , Shai Shalev - Shwartz , Amir Globerson . Abstract : Online sequence prediction is the problem of predicting the next element of a sequence given previous elements . This problem has been extensively studied in the context of individual sequence prediction , where no prior assumptions are made on the origin of the sequence . Individual sequence prediction algorithms work quite well for long sequences , where the algorithm has enough time to learn the temporal structure of the sequence . However , they might give poor predictions for short sequences . A possible remedy is to rely on the general model of prediction with expert advice , where the learner has access to a set of r experts , each of which makes its own predictions on the sequence . It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of log ( r ) . But , without firm prior knowledge on the problem , it is not clear how to choose a small set of good experts . In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences . We demonstrate the merits of our approach by experimenting with the task of click prediction on the web . Efficient Active Algorithms for Hierarchical Clustering . Akshay Krishnamurthy , Sivaraman Balakrishnan , Min Xu , Aarti Singh . Abstract : Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets , while storage and processing power continue to lag behind . This motivates the need for algorithms that are efficient , both in terms of the number of measurements needed and running time . To combat the challenges associated with large datasets , we propose a general framework for active hierarchical clustering that repeatedly runs an off - the - shelf clustering algorithm on small subsets of the data and comes with guarantees on performance , measurement complexity and runtime complexity . Through extensive experimentation we also demonstrate that this framework is practically alluring . Copula Mixture Model for Dependency - seeking Clustering . Melanie Rey , Volker Roth . Abstract : We introduce a copula mixture model to perform dependency - seeking clustering when co - occurring samples from different data sources are available . The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities . We formulate our model as a non - parametric Bayesian mixture , while providing efficient MCMC inference . Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results . The Landmark Selection Method for Multiple Output Prediction . Krishnakumar Balasubramanian , Guy Lebanon . A substantial research effort is devoted to such modeling when x is high dimensional . We consider , instead , the case of a high dimensional y , where x is either low dimensional or high dimensional . Classification and regression experiments on multiple datasets show that this model outperforms the one vs. all approach as well as several sophisticated multiple output prediction methods . Subgraph Matching Kernels for Attributed Graphs . Nils Kriege , Petra Mutzel . Abstract : We propose graph kernels based on subgraph matchings , i.e. structure - preserving bijections between subgraphs . We show that subgraph matching kernels generalize several known kernels . To compute the kernel we propose a graph - theoretical algorithm inspired by a classical relation between common subgraphs of two graphs and cliques in their product graph observed by Levi ( 1973 ) . Adaptive Canonical Correlation Analysis Based On Matrix Manifolds . Florian Yger , Maxime Berar , Gilles Gasso , Alain Rakotomamonjy . Abstract : In this paper , we formulate the Canonical Correlation Analysis ( CCA ) problem on matrix manifolds . This framework provides a natural way for dealing with matrix constraints and tools for building efficient algorithms even in an adaptive setting . Finally , an adaptive CCA algorithm is proposed and applied to a change detection problem in EEG signals . Batch Active Learning via Coordinated Matching . Javad Azimi , Alan Fern , Xiaoli Zhang - Fern , Glencora Borradaile , Brent Heeringa . - Accepted . Abstract : Most prior work on active learning of classifiers has focused on sequentially selecting one unlabeled example at a time to be labeled in order to reduce the overall labeling effort . In many scenarios , however , it is desirable to label an entire batch of examples at once , for example , when labels can be acquired in parallel . We propose a novel batch active learning method that leverages the availability of high - quality and efficient sequential active - learning policies by attempting to approximate their behavior when applied for k steps . Specifically , our algorithm first uses Monte - Carlo simulation to estimate the distribution of unlabeled examples selected by a sequential policy over k step executions . The algorithm then attempts to select a set of k examples that best matches this distribution , leading to a combinatorial optimization problem that we term \\\" bounded coordinated matching ' . While we show this problem is NP - hard in general , we give an efficient greedy solution , which inherits approximation bounds from supermodular minimization theory . Our experimental results on eight benchmark datasets show that the proposed approach is highly effective . Hybrid Batch Bayesian Optimization . Javad Azimi , Ali Jalali , Xiaoli Zhang - Fern . Abstract : Bayesian Optimization aims at optimizing an unknown non - convex / concave function that is costly to evaluate . We are interested in application scenarios where concurrent function evaluations are possible . Under such a setting , BO could choose to either sequentially evaluate the function , one input at a time and wait for the output of the function before making the next selection , or evaluate the function at a batch of multiple inputs at once . These two different settings are commonly referred to as the sequential and batch settings of Bayesian Optimization . In general , the sequential setting leads to better optimization performance as each function evaluation is selected with more information , whereas the batch setting has an advantage in terms of the total experimental time ( the number of iterations ) . In this work , our goal is to combine the strength of both settings . Specifically , we systematically analyze Bayesian optimization using Gaussian process as the posterior estimator and provide a hybrid algorithm that , based on the current state , dynamically switches between a sequential policy and a batch policy with variable batch sizes . We provide theoretical justification for our algorithm and present experimental results on eight benchmark BO problems . The results show that our method achieves substantial speedup ( up to % 78 ) compared to a pure sequential policy , without suffering any significant performance loss . Efficient and Practical Stochastic Subgradient Descent for Nuclear Norm Regularization . Haim Avron , Satyen Kale , Shiva Kasiviswanathan , Vikas Sindhwani . Abstract : We describe novel subgradient methods for a broad class of matrix optimization problems involving nuclear norm regularization . Unlike existing approaches , our method executes very cheap iterations by combining low - rank stochastic subgradients with efficient incremental SVD updates , made possible by highly optimized and parallelizable dense linear algebra operations on small matrices . Our practical algorithms always maintain a low - rank factorization of iterates that can be conveniently held in memory and efficiently multiplied to generate predictions in matrix completion settings . Empirical comparisons confirm that our approach is highly competitive with several recently proposed state - of - the - art solvers for such problems . Gap Filling in the Plant Kingdom - Trait Prediction Using Hierarchical Probabilistic Matrix Factorization . Hanhuai Shan , Jens Kattge , Peter Reich , Arindam Banerjee , Franziska Schrodt , Markus Reichstein . - Accepted . Abstract : Plant traits are a key to understand and predict the adaptation of ecosystems to environmental changes , which motivates the TRY project aiming at constructing a global database for plant traits and becoming a standard resource for the ecological community . Despite its unprecedented coverage , a large percentage of missing data substantially constrains joint trait analysis . Meanwhile , the trait data are characterized by the hierarchical phylogenetic structure of the plant kingdom . While factorization based matrix completion techniques have been widely used to address the missing data problem , traditional matrix factorization methods are unable to leverage the phylogenetic structure . We propose hierarchical probabilistic matrix factorization ( HPMF ) , which effectively uses hierarchical phylogenetic information for trait prediction . We demonstrate HPMF 's high accuracy , effectiveness of incorporating hierarchical structure and ability to capture trait correlation through experiments . Sparse Support Vector Infinite Push . Alain Rakotomamonjy . Abstract : In this paper , we address the problem of embedded feature selection for ranking on top of the list problems . We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer . Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator . Experimental results on toy , DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function . A Dantzig Selector Approach to Temporal Difference Learning . Matthieu Geist , Bruno Scherrer , Alessandro Lazaric , Mohammad Ghavamzadeh . - Accepted . Abstract : LSTD is one of the most popular reinforcement learning algorithms for value function approximation . Whenever the number of samples is larger than the number of features , LSTD must be paired with some form of regularization . In particular , L1-regularization methods tends to perform feature selection by promoting sparsity and thus they are particularly suited in high - dimensional problems . Nonetheless , since LSTD is not a simple regression algorithm but it solves a fixed - point problem , the integration with L1-regularization is not straightforward and it might come with some drawbacks ( see e.g. , the P - matrix assumption for LASSO - TD ) . In this paper we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector . In particular , we investigate the performance of the algorithm and its relationship with existing regularized approaches , showing how it overcomes some of the drawbacks of existing solutions . Chad Scherrer , Mahantesh Halappanavar , Ambuj Tewari , David Haglin . Abstract : We present a generic framework for parallel coordinate descent ( CD ) algorithms that has as special cases the original sequential algorithms of Cyclic CD and Stochastic CD , as well as the recent parallel Shotgun algorithm of Bradley et al . We introduce two novel parallel algorithms that are also special cases - Thread - Greedy CD and Coloring - Based CD - and give performance measurements for an OpenMP implementation of these . Cross - Domain Multitask Learning with Latent Probit Models . Shaobo Han , Xuejun Liao , Lawrence Carin . Abstract : Learning multiple tasks across heterogeneous domains is a challenging problem since the feature space may not be the same for different tasks . We assume the data in multiple tasks are generated from a latent common domain via sparse domain transforms and propose a latent probit model ( LPM ) to jointly learn the domain transforms , and the shared probit classifier in the common domain . To learn meaningful task relatedness and avoid over - fitting in classification , we introduce sparsity in the domain transforms matrices , as well as in the common classifier . We derive theoretical bounds for the estimation error of the classifier in terms of the sparsity of domain transforms . An expectation - maximization algorithm is derived for learning the LPM . The effectiveness of the approach is demonstrated on several real datasets . Structured Learning from Partial Annotations . Xinghua Lou , Fred Hamprecht . Abstract : Structured learning is appropriate when predicting structured outputs such as trees , graphs , or sequences . Most prior work requires the training set to consist of complete trees , graphs or sequences . Specifying such detailed ground truth can be tedious or infeasible for large outputs . Our main contribution is a large margin formulation that makes structured learning from only partially annotated data possible . The resulting optimization problem is non - convex , yet can be efficiently solve by concave - convex procedure ( CCCP ) with novel speedup strategies . We apply our method to a challenging tracking - by - assignment problem of a variable number of divisible objects . On this benchmark , using only 25 % of a full annotation we achieve a performance comparable to a model learned with a full annotation . Finally , we offer a unifying perspective of previous work using the hinge , ramp , or max loss for structured learning , followed by an empirical comparison on their practical performance . Maximum Margin Output Coding . Yi Zhang , Jeff Schneider . Abstract : In this paper we study output coding for multi - label prediction . For a multi - label output coding to be discriminative , it is important that codewords for different label vectors are significantly different from each other . In the meantime , unlike in traditional coding theory , codewords in output coding are to be predicted from the input , so it is also critical to have a predictable label encoding . To find output codes that are both discriminative and predictable , we first propose a max - margin formulation that naturally captures these two properties . We then convert it to a metric learning formulation , but with an exponentially large number of constraints as commonly encountered in structured prediction problems . Without a label structure for tractable inference , we use overgenerating ( i.e. , relaxation ) techniques combined with the cutting plane method for optimization . In our empirical study , the proposed output coding scheme outperforms a variety of existing multi - label prediction methods for image , text and music classification . Sequential Nonparametric Regression . Haijie Gu , John Lafferty . Abstract : We present algorithms for nonparametric regression in settings where the data are obtained sequentially . While traditional estimators select bandwidths that depend upon the sample size , for sequential data the effective sample size is dynamically changing . We propose a linear time algorithm that adjusts the bandwidth for each new data point , and show that the estimator achieves the optimal minimax rate of convergence . We also propose the use of online expert mixing algorithms to adapt to unknown smoothness of the regression function . We provide simulations that confirm the theoretical results , and demonstrate the effectiveness of the methods . An Infinite Latent Attribute Model for Network Data . Konstantina Palla , David A. Knowles , Zoubin Ghahramani . Abstract : Latent variable models for network data extract a summary of the relational structure underlying an observed network . The simplest possible models subdivide nodes of the network into clusters ; the probability of a link between any two nodes then depends only on their cluster assignment . Currently available models can be classified by whether clusters are disjoint or are allowed to overlap . These models can explain a \\\" flat \\\" clustering structure . Hierarchical Bayesian models provide a natural approach to capture more complex dependencies . We propose a model in which objects are characterised by a latent feature vector . Each feature is itself partitioned into disjoint groups ( subclusters ) , corresponding to a second layer of hierarchy . In experimental comparisons , the model achieves significantly improved predictive performance on social and biological link prediction tasks . The results indicate that models with a single layer hierarchy over - simplify real networks . On Local Regret . Michael Bowling , Martin Zinkevich . Abstract : Online learning typically aims to perform nearly as well as the best hypothesis in hindsight . For some hypothesis classes , though , even finding the best hypothesis offline is challenging . In such offline cases , local search techniques are often employed and only local optimality guaranteed . For online decision - making with such hypothesis classes , we introduce local regret , a generalization of regret that aims to perform nearly as well as only nearby hypotheses . We then present a general algorithm that can minimize local regret for arbitrary locality graphs . We also show that certain forms of structure in the graph can be exploited to drastically simplify learning . These algorithms are then demonstrated on a diverse set of online problems ( some previously unexplored ) : online disjunct learning , online Max - SAT , and online decision tree learning . Smoothness and Structure Learning by Proxy . Benjamin Yackley , Terran Lane . Abstract : As data sets grow in size , the ability of learning methods to find structure in them is increasingly hampered by the time needed to search the large spaces of possibilities and generate a score for each that takes all of the observed data into account . For instance , Bayesian networks , the model chosen in this paper , have a super - exponentially large search space for a fixed number of variables . One possible method to alleviate this problem is to use a proxy , such as a Gaussian Process regressor , in place of the true scoring function , training it on a selection of sampled networks . We prove here that the use of such a proxy is well - founded , as we can bound the smoothness of a commonly - used scoring function for Bayesian network structure learning . We show here that , compared to an identical search strategy using the network 's exact scores , our proxy - based search is able to get equivalent or better scores on a number of data sets in a fraction of the time . A fast and simple algorithm for training neural probabilistic language models . Andriy Mnih , Yee Whye Teh . Abstract : Neural probabilistic language models ( NPLMs ) have recently superseded smoothed n - gram models as the best - performing model class for language modelling . Unfortunately , the adoption of NPLMs is held back by their notoriously long training times , which can be measured in weeks even for moderately - sized datasets . These are a consequence of the models being explicitly normalized , which leads to having to consider all words in the vocabulary when computing the log - likelihood gradients . We propose a fast and simple algorithm for training NPLMs based on noise - contrastive estimation , a newly introduced procedure for estimating unnormalized continuous distributions . We investigate the behaviour of the algorithm on the Penn Treebank corpus and show that it reduces the training times by more than an order of magnitude without affecting the quality of the resulting models . The algorithm is also more efficient and much more stable than importance sampling because it requires far fewer noise samples to perform well . We demonstrate the scalability of the proposed approach by training several neural language models on a 47M - word corpus with a 80K - word vocabulary , obtaining state - of - the - art results in the Microsoft Research Sentence Completion Challenge . Incorporating Causal Prior Knowledge as Path - Constraints in Bayesian Networks and Maximal Ancestral Graphs . Giorgos Borboudakis , Ioannis Tsamardinos . Abstract : We consider the incorporation of causal knowledge about the presence or absence of ( possibly indirect ) causal relations into a causal model . Such causal relations correspond to directed paths in a causal model . This type of knowledge naturally arises from experimental data , among others . Specifically , we consider the formalisms of Causal Bayesian Networks and Maximal Ancestral Graphs and their Markov equivalence classes : Partially Directed Acyclic Graphs and Partially Oriented Ancestral Graphs . We introduce sound and complete procedures which are able to incorporate causal prior knowledge in such models . In simulated experiments , we show that often considering even a few causal facts leads to a significant number of new inferences . In a case study , we also show how to use real experimental data to infer causal knowledge and incorporate it into a real biological causal network . High - Dimensional Covariance Decomposition into Sparse Markov and Independence Domains . Majid Janzamin , Animashree Anandkumar . Abstract : In this paper , we present a novel framework incorporating a combination of sparse models in different domains . We posit the observed data as generated from a linear combination of a sparse Gaussian Markov model ( with a sparse precision matrix ) and a sparse Gaussian independence model ( with a sparse covariance matrix ) . We provide efficient methods for decomposition of the data into two domains , viz Markov and independence domains . We characterize a set of sufficient conditions for identifiability and model consistency . Latent Collaborative Retrieval . Jason Weston , Chong Wang , Ron Weiss , Adam Berenzweig . Abstract : Retrieval tasks typically require a ranking of items given a query . Collaborative filtering tasks , on the other hand , learn models comparing users with items . In this paper we study the joint problem of recommending items to a user with respect to a given query , which is a surprisingly common task . This setup differs from the standard collaborative filtering one in that we are given a query \\u00d7 user \\u00d7 item tensor for training instead of the more traditional user \\u00d7 item matrix . Compared to document retrieval we do have a query , but we may or may not have content features ( we will consider both cases ) and we can also take account of the user 's profile . We introduce a factorized model for this new task that optimizes the top ranked items returned for the given query and user . We report empirical results where it outperforms several baselines . Lightning Does Not Strike Twice : Robust MDPs with Coupled Uncertainty . Shie Mannor , Ofir Mebel , Huan Xu . Abstract : We consider Markov decision processes under parameter uncertainty . Previous studies all restrict to the case that uncertainties among different states are uncoupled , which leads to conservative solutions . In contrast , we introduce an intuitive concept , termed ' Lightning Does not Strike Twice , ' to model coupled uncertain parameters . Specifically , we require that the system can deviate from its nominal parameters only a bounded number of times . We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies using this concept . On causal and anticausal learning . Bernhard Schoelkopf , Dominik Janzing , Jonas Peters , Eleni Sgouritsa , Kun Zhang , Joris Mooij . - Accepted . Abstract : We consider the problem of function estimation in the case where an underlying causal model can be identified . This has implications for popular scenarios such as covariate shift , concept drift , transfer learning and semi - supervised learning . We argue that causal knowledge may facilitate some approaches for a given problem , and rule out others . In particular , we formulate a hypothesis for when semi - supervised learning can help , and corroborate it with empirical results . Compact Hyperplane Hashing with Bilinear Functions . Wei Liu , Jun Wang , Yadong Mu , Sanjiv Kumar , Shih - Fu Chang . Abstract : Hyperplane hashing aims at rapidly searching nearest points to a hyperplane , and has shown practical impact in scaling up active learning with SVMs . Unfortunately , the existing randomized methods need long hash codes and a number of hash tables to achieve reasonable search accuracy . Thus , they suffer from reduced search speed and large memory overhead . To this end , this paper proposes a novel hyperplane hashing technique which yields compact hash codes . The key idea is the bilinear form of the proposed hash functions , which leads to higher collision probability than the existing hyperplane hash functions when using random projections . To further increase the performance , we propose a learning based framework in which bilinear functions are directly learned from the data . This yields compact yet discriminative codes , and also increases the search performance over the random projection based solutions . Large - scale active learning experiments carried out on two datasets with up to one million samples demonstrate the overall superiority of the proposed approach . Continuous Inverse Optimal Control with Locally Optimal Examples . Sergey Levine , Vladlen Koltun . Abstract : Inverse optimal control , also known as inverse reinforcement learning , is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy . We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality , and is suitable for large , continuous domains where even computing a full policy is impractical . By using a local approximation of the reward function , our method can also drop the assumption that the demonstrations are globally optimal , requiring only local optimality . This allows it to learn from examples that are unsuitable for prior methods . Convex Multitask Learning with Flexible Task Clusters . Wenliang Zhong , James Kwok . Abstract : Traditionally , multitask learning ( MTL ) assumes that all the tasks are related . This can lead to negative transfer when tasks are indeed incoherent . Recently , a number of approaches have been proposed that alleviate this problem by discovering the underlying task clusters or relationships . However , they are limited to modeling these relationships at the task level , which may be restrictive in some applications . In this paper , we propose a novel MTL formulation that captures task relationships at the feature - level . Depending on the interactions among tasks and features , the proposed method construct different task clusters for different features , without even the need of pre - specifying the number of clusters . Computationally , the proposed formulation is strongly convex , and can be efficiently solved by accelerated proximal methods . Experiments are performed on a number of synthetic and real - world data sets . Under various degrees of task relationships , the accuracy of the proposed method is consistently among the best . Moreover , the feature - specific task clusters obtained agree with the known / plausible task structures of the data . A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling . Drausin Wulsin , Shane Jensen , Brian Litt . Abstract : Driven by the multi - level structure of human intracranial electroencephalogram ( iEEG ) recordings of epileptic seizures , we introduce a new variant of a hierarchical Dirichlet Process - the multi - level clustering hierarchical Dirichlet Process ( MLC - HDP)-that simultaneously clusters datasets on multiple levels . Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient . The MLC - HDP model clusters over channels - types , seizure - types , and patient - types simultaneously . We describe this model and its implementation in detail . We also present the results of a simulation study comparing the MLC - HDP to a similar model , the Nested Dirichlet Process and finally demonstrate the MLC - HDP 's use in modeling seizures across multiple patients . We find the MLC - HDP 's clustering to be comparable to independent human physician clusterings . To our knowledge , the MLC - HDP model is the first in the epilepsy literature capable of clustering seizures within and between patients . Levy Measure Decompositions for the Beta and Gamma Processes . Yingjian Wang , Lawrence Carin . Abstract : We develop new representations for the Levy measures of the beta and gamma processes . These representations are manifested in terms of an infinite sum of well - behaved ( proper ) beta and gamma distributions . Further , we demonstrate how these infinite sums may be truncated in practice , and explicitly characterize truncation errors . We also perform an analysis of the characteristics of posterior distributions , based on the proposed decompositions . The decompositions provide new insights into the beta and gamma processes , and we demonstrate how the proposed representation unifies some properties of the two . This paper is meant to provide a rigorous foundation for and new perspectives on L\\u00b4evy processes , as these are of increasing importance in machine learning . Building high - level features using large scale unsupervised learning . Quoc Le , Marc'Aurelio Ranzato , Rajat Monga , Matthieu Devin , Greg Corrado , Kai Chen , Jeff Dean , Andrew Ng . - Accepted . Abstract : We consider the challenge of building feature detectors for high - level concepts from only unlabeled data . For example , we would like to understand if it is possible to learn a face detector using only unlabeled images downloaded from the Internet . To answer this question , we trained a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images ( which has 10 million images , each image has 200x200 pixels ) . On contrary to what appears to be a widely - held negative belief , our experimental results reveal that it is possible to achieve a face detector via only unlabeled data . Control experiments show that the feature detector is robust not only to translation but also to scaling and 3D rotation . Also via recognition and visualization , we find that the same network is sensitive to other high - level concepts such as cat faces and human bodies . Near - Optimal BRL using Optimistic Local Transitions . Mauricio Araya , Olivier Buffet , Vincent Thomas . Abstract : Model - based Bayesian Reinforcement Learning ( BRL ) allows a found formalization of the problem of acting optimally while facing an unknown environment , i.e. , avoiding the exploration - exploitation dilemma . However , algorithms explicitly addressing BRL suffer from such a combinatorial explosion that a large body of work relies on heuristic algorithms . This paper introduces BOLT , a simple and ( almost ) deterministic heuristic algorithm for BRL which is optimistic about the transition function . We analyze BOLT 's sample complexity , and show that under certain parameters , the algorithm is near - optimal in the Bayesian sense with high probability . Then , experimental results highlight the key differences of this method compared to previous work . A Unified Robust Classification Model . Akiko Takeda , Hiroyuki Mitsugi , Takafumi Kanamori . Abstract : A wide variety of machine learning algorithms such as support vector machine ( SVM ) , minimax probability machine ( MPM ) , and Fisher discriminant analysis ( FDA ) , exist for binary classification . The purpose of this paper is to provide a unified classification model that includes the above models through a robust optimization approach . This unified model has several benefits . One is that the extensions and improvements intended for SVM become applicable to MPM and FDA , and vice versa . Another benefit is to provide theoretical results to above learning methods at once by dealing with the unified model . We give a statistical interpretation of the unified classification model and propose a non - convex optimization algorithm that can be applied to non - convex variants of existing learning methods . Manifold Relevance Determination . Andreas Damianou , Carl Ek , Michalis Titsias , Neil Lawrence . Abstract : In this paper we present a fully Bayesian latent variable model which exploits conditional non- linear ( in)-dependence structures to learn an efficient latent representation . The model is capable of learning from extremely high - dimensional data such as directly modelling high resolution images . The latent representation is factorized to represent shared and private information from multiple views of the data . Bayesian techniques allow us to automatically estimate the dimensionality of the latent spaces . We demonstrate the model by prediction of human pose in an ambiguous setting . Our Bayesian representation allows us to perform disambiguation in a principled manner by including priors which incorporate the dynamics structure of the data . We demonstrate the ability of the model to capture structure underlying extremely high dimensional spaces by learning a low - dimensional representation of a set of facial images under different illumination conditions . The model correctly automatically creates a factorized representation where the lighting variance is represented in a separate latent space from the variance associated with different faces . We show that the model is capable of generating morphed faces and images from novel light directions . Residual Components Analysis . Alfredo Kalaitzis , Neil Lawrence . The maximum likelihood solution for the model is an eigenvalue problem on the sample covariance matrix . In this paper we consider the situation where the data variance is already partially explained by other factors , e.g. conditional dependencies between the covariates , or temporal correlations leaving some residual variance . We decompose the residual variance into its components through a generalised eigenvalue problem , which we call residual component analysis ( RCA ) . We explore a range of new algorithms that arise from the framework , including one that factorises the covariance of a Gaussian density into a low - rank and a sparse - inverse component . We illustrate the ideas on the recovery of a protein - signaling network , a gene expression time - series data set and the recovery of the human skeleton from motion capture 3-D cloud data . Clustering to Maximize the Ratio of Split to Diameter . Jiabing Wang , Jiaye Chen . The diameter of a cluster is the maximum dissimilarity between pairs of objects in the cluster , and the split of a cluster is the minimum dissimilarity between objects within the cluster and objects outside the cluster . In this paper , we propose a new criterion for measuring the goodness of clusters : ? the ratio of the minimum split to the maximum diameter , and the objective is to maximize the ratio . The worst - case runtime of both algorithms is O(n3 ) . We compare the proposed algorithms with the Normalized Cut by applying them to image segmentation . The experimental results on both natural and synthetic images demonstrate the effectiveness of the proposed algorithms . A Graphical Model Formulation of Collaborative Filtering Neighbourhood Methods with Fast Maximum Entropy Training . Aaron Defazio , Tiberio Caetano . Abstract : Item neighbourhood methods for collaborative filtering learn a weighted graph over the set of items , where each item is connected to those it is most similar to . The prediction of a user 's rating on an item is then given by that rating of neighbouring items , weighted by their similarity . This paper presents a new neighbourhood approach which we call item fields , whereby an undirected graphical model is formed over the item graph . The resulting prediction rule is a simple generalization of the classical approaches , which takes into account non - local information in the graph , allowing its best results to be obtained when using drastically fewer edges than other neighbourhood approaches . A fast approximate maximum entropy training method based on the Bethe approximation is presented which utilizes a novel decomposition into tractable sub - problems . When using precomputed sufficient statistics on the Movielens dataset , our method outperforms maximum likelihood approaches by two orders of magnitude . On - Line Portfolio Selection with Moving Average Reversion . Bin Li , Steven C.H. Hoi . Abstract : On - line portfolio selection has attracted increasing interests in machine learning and AI communities recently . Empirical evidences show that stock 's high and low prices are temporary and stock price relatives are likely to follow the mean reversion phenomenon . While the existing mean reversion strategies are shown to achieve good empirical performance on many real datasets , they often make the single - period mean reversion assumption , which is not always satisfied in some real datasets , leading to poor performance when the assumption does not hold . From our empirical results , we found that OLMAR can overcome the drawback of existing mean reversion algorithms and achieve significantly better results , especially on the datasets where the existing mean reversion algorithms failed . In addition to superior trading performance , OLMAR also runs extremely fast , further supporting its practical applicability to a wide range of applications . Improved Information Gain Estimates for Decision Tree Induction . Sebastian Nowozin . Abstract : Ensembles of classification and regression trees remain popular machine learning methods because they define flexible non - parametric models that predict well and are computationally efficient both during training and testing . During induction of decision trees one aims to find predicates that are maximally informative about the prediction target . To select good predicates most approaches estimate an information - theoretic scoring function , the information gain , both for classification and regression problems . We point out that the common estimation procedures are biased and show that by replacing them with improved estimators of the discrete and the differential entropy we can obtain better decision trees . In effect our modifications yield improved predictive performance and are simple to implement in any decision tree code . Influence Maximization in Continuous Time Diffusion Networks . Manuel Gomez Rodriguez , Bernhard Sch\\u00f6lkopf . Abstract : The problem of finding the optimal set of source nodes in a diffusion network that maximizes the spread of information , influence , and diseases in a limited amount of time depends dramatically on the underlying temporal dynamics of the network . However , this still remains largely unexplored to date . To this end , given a network and its temporal dynamics , we first describe how continuous time Markov chains allow us to analytically compute the average total number of nodes reached by a diffusion process starting in a set of source nodes . We then show that selecting the set of most influential source nodes in the continuous time influence maximization problem is NP - hard and develop an efficient approximation algorithm with provable near - optimal performance . Experiments on synthetic and real diffusion networks show that our algorithm outperforms other state of the art algorithms by at least 20 % and is robust across different network topologies . On the Size of the Online Kernel Sparsification Dictionary . Yi Sun , Faustino Gomez , Juergen Schmidhuber . Abstract : We analyze the size of the dictionary constructed from online kernel sparsification , using a novel formula that expresses the expected determinant of the kernel Gram matrix in terms of the eigenvalues of the covariance operator . Using this formula , we are able to connect the cardinality of the dictionary with the eigen - decay of the covariance operator . In particular , we show that for bounded kernels , the size of the dictionary always grows sub - linearly in the number of data points , and , as a consequence , the kernel linear regressor constructed from the resulting dictionary is consistent . Multi - level Lasso for Sparse Multi - task Regression . Aurelie Lozano , Grzegorz Swirszcz . Abstract : We present a flexible formulation for variable selection in multi - task regression to allow for discrepancies in the estimated sparsity patterns accross the multiple tasks , while leveraging the common structure among them . Our approach is based on an intuitive decomposition of the regression coefficients into a product between a component that is common to all tasks and another component that captures task - specificity . This decomposition yields the Multi - level Lasso objective that can be solved efficiently via alternating optimization . The analysis of the \\\" orthonormal design \\\" case reveals some interesting insights on the nature of the shrinkage performed by our method , compared to that of related work . Theoretical guarantees are provided on the consistency of Multi - level Lasso . Simulations and empirical study of micro - array data further demonstrate the value of our framework . Fast Computation of Subpath Kernel for Trees . Daisuke Kimura , Hisashi Kashima . Abstract : The kernel method is a potential approach to analyzing structured data such as sequences , trees , and graphs ; however , unordered trees have not been investigated extensively . Kimura et al . ( 2011 ) proposed a kernel function for unordered trees on the basis of their subpaths , which are vertical substructures of trees responsible for hierarchical information in them . Their kernel exhibits practically good performance in terms of accuracy and speed ; however , linear - time computation is not guaranteed theoretically , unlike the case of the other unordered tree kernel proposed by Vishwanathan and Smola ( 2003 ) . In this paper , we propose a theoretically guaranteed linear - time kernel computation algorithm that is practically fast , and we present an efficient prediction algorithm whose running time depends only on the size of the input tree . Experimental results show that the proposed algorithms are quite efficient in practice . Total Variation and Euler 's Elastica for Supervised Learning . Tong Lin , Hanlin Xue , Ling Wang , Hongbin Zha . Abstract : In recent years , total variation ( TV ) and Euler 's elastica ( EE ) have been successfully applied to image processing tasks such as denoising and inpainting . This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data . The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme , where the energy is composed of a squared loss and a total variation smoothing ( or Euler 's elastica smoothing ) . Its solution via variational principles leads to an Euler - Lagrange PDE . However , the PDE is always high - dimensional and can not be directly solved by common methods . Instead , radial basis functions are utilized to approximate the target function , reducing the problem to finding the linear coefficients of basis functions . We apply the proposed methods to supervised learning tasks ( including binary classification , multi - class classification , and regression ) on benchmark data sets . Extensive experiments have demonstrated promising results of the proposed methods . Learning the Dependence Graph of Time Series with Latent Factors . Ali Jalali , Sujay Sanghavi . Abstract : This paper considers the problem of learning , from samples , the dependency structure of a system of linear stochastic differential equations , when some of the variables are latent . We observe the time evolution of some variables , and never observe other variables ; from this , we would like to find the dependency structure of the observed variables - separating out the spurious interactions caused by the latent variables ' time series . We develop a new convex optimization based method to do so in the case when the number of latent variables is smaller than the number of observed ones . For the case when the dependency structure between the observed variables is sparse , we theoretically establish a high - dimensional scaling result for structure recovery . We verify our theoretical result with both synthetic and real data ( from the stock market ) . A Generalized Loop Correction Method for Approximate Inference in Graphical Models . Siamak Ravanbakhsh , Chun - Nam Yu , Russell Greiner . Abstract : Belief Propagation ( BP ) is one of the most popular methods for inference in probabilistic graphical models . BP is guaranteed to return the correct answer for tree structures , but can be incorrect or non - convergent for loopy graphical models . Recently , several new approximate inference algorithms based on cavity distribution have been proposed . These methods can account for the effect of loops by incorporating the dependency between BP messages . Alternatively , region - based approximations ( that lead to methods such as Generalized Belief Propagation ) improve upon BP by considering interactions within small clusters of variables , thus taking small loops within these clusters into account . This paper introduces an approach , Generalized Loop Correction ( GLC ) , that benefits from both of these types of loop correction . We show how GLC relates to these two families of inference methods , then provide empirical evidence that GLC works effectively in general , and can be significantly more accurate than both correction schemes . Consistent Covariance Selection From Data With Missing Values . Mladen Kolar , eric xing . Abstract : Data sets with missing values arise in many practical problems and domains . However , correct statistical analysis of these data sets is difficult . A popular likelihood approach to statistical inference from partially observed data is the expectation maximization ( EM ) algorithm , which leads to non - convex optimization and estimates that are difficult to analyze theoretically . We study a simple two step procedure for covariance selection , which is tractable in high - dimensions and does not require imputation of the missing values . Simulation studies show that this estimator compares favorably with the EM algorithm . Our results have important practical consequences as they show that standard tools for covariance selection can be used when data contains missing values , without resorting to the iterative EM algorithm that can be slow to converge in practice for large problems . Is margin preserved after random projection ? Qinfeng Shi , Chunhua Shen , Rhys Hill , Anton van den Hengel . Abstract : Random projections have been applied in many machine learning algorithms . However , whether margin is preserved after random projection is non - trivial and not well studied . In this paper we analyse margin distortion after random projection , and give the conditions of margin preservation for binary classification problems . We also extend our analysis to margin for multiclass problems , and provide theoretical bounds on multiclass margin on the projected data . A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices . Mingjun Zhong , Mark Girolami . Abstract : We present a fully Bayesian approach to simultaneously approximate diagonalization of several square matrices which are not necessary symmetric . A Gibbs sampler has been derived for simulating the common eigenvectors and the eigenvalues for these matrices . Several data are used to demonstrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms , which shows that the Gibbs sampler achieves the state - of - the - art performance . As a byproduct , the output of the Gibbs sampler could be used to estimate the log marginal likelihood by the Bayesian information criterion ( BIC ) which correctly located the number of common eigenvectors . We then applied the Gibbs sampler to the blind sources separation problem , the common principal component analysis and the common spatial pattern analysis . Predicting accurate probabilities with a ranking loss . Aditya Menon , Xiaoqian Jiang , Shankar Vembu , Charles Elkan , Lucila Ohno - Machado . - Accepted . Abstract : In many real - world applications of machine learning classifiers , it is essential to predict the probability of an example belonging to a particular class . This paper proposes a simple technique for predicting probabilities based on optimizing a ranking loss , followed by isotonic regression . This semi - parametric technique offers both good ranking and regression performance , and models a richer set of probability distributions than statistical workhorses such as logistic regression . We provide experimental results that show the effectiveness of this technique on real - world applications of probability prediction . Learning with Augmented Features for Heterogeneous Domain Adaptation . Lixin Duan , Dong Xu , Ivor Tsang . Abstract : We propose a new learning method for heterogeneous domain adaptation ( HDA ) , in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions . Using two different projection matrices , we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains . We then propose two new feature mapping functions to augment the transformed data with their original features and zeros . The existing learning methods ( e.g. , SVM and SVR ) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA . Using the hinge loss function in SVM as an example , we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation ( HFA ) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions . Moreover , we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method . Comprehensive experiments on two benchmark datasets clearly demonstrate that our HFA outperforms the existing HDA methods . Dirichlet Process with Mixed Random Measures : A Nonparametric Topic Model for Labeled Data . Dongwoo Kim , Suin Kim , Alice Oh . Abstract : We describe a nonparametric topic model for labeled data . The model uses a mixture of random measures ( MRM ) as a base distribution of the Dirichlet process ( DP ) of the HDP framework , so we call it the DP - MRM . To model labeled data , we define a DP distributed random measure for each label , and the resulting model generates an unbounded number of topics for each label . We apply DP - MRM on single - labeled and multi - labeled corpora of documents and compare the performance on label prediction with LDA - SVM and Labeled - LDA . We further enhance the model by incorporating ddCRP and modeling multi - labeled images for image segmentation and object labeling , comparing the performance with nCuts and rddCRP . Evaluating Bayesian and L1 Approaches for Sparse Unsupervised Learning . Shakir Mohamed , Katherine Heller , Zoubin Ghahramani . Abstract : The use of L_1 regularisation for sparse learning has generated immense research interest , with many successful applications in diverse areas such as signal acquisition , image coding , genomics and collaborative filtering . While existing work highlights the many advantages of L_1 methods , in this paper we find that L_1 regularisation often dramatically under - performs in terms of predictive performance when compared with other methods for inferring sparsity . We focus on unsupervised latent variable models , and develop L_1 minimising factor models , Bayesian variants of \\\" L_1 \\\" , and Bayesian models with a stronger L_0 -like sparsity induced through spike - and - slab distributions . These spike - and - slab Bayesian factor models encourage sparsity while accounting for uncertainty in a principled manner , and avoid unnecessary shrinkage of non - zero values . We demonstrate on a number of data sets that in practice spike - and - slab Bayesian methods outperform L_1 minimisation , even on a computational budget . We thus highlight the need to re - assess the wide use of L_1 methods in sparsity - reliant applications , particularly when we care about generalising to previously unseen data , and provide an alternative that , over many varying conditions , provides improved generalisation performance . Collaborative Topic Regression with Social Matrix Factorization for Recommendation Systems . Sanjay Purushotham , Yan Liu . Abstract : Social network websites , such as Facebook , YouTube , Lastfm etc , have become a popular platform for users to connect with each other and share content or opinions . They provide rich information to study the influence of user 's social circle in their decision process . In this paper , we are interested in examining the effectiveness of social network information to predict the user 's ratings of items . We propose a novel hierarchical Bayesian model which jointly incorporates topic modeling and probabilistic matrix factorization of social networks . A major advantage of our model is to automatically infer useful latent topics and social information as well as their importance to collaborative filtering from the training data . Empirical experiments on two large - scale datasets show that our algorithm provides a more effective recommendation system than the state - of - the art approaches . Our results also reveal interesting insight that the social circles have more influence on people 's decisions about the usefulness of information ( e.g. , bookmarking preference on Delicious ) than personal taste ( e.g. , music preference on Lastfm ) . LPQP for MAP : Putting LP Solvers to Better Use . Patrick Pletscher , Sharon Wulff . Abstract : MAP inference for general energy functions remains a challenging problem . While most efforts are channeled towards improving the linear programming ( LP ) based relaxation , this work is motivated by the quadratic programming ( QP ) relaxation . We propose a novel MAP relaxation that penalizes the Kullback - Leibler divergence between the LP pairwise auxiliary variables , and QP equivalent terms given by the product of the unaries . We develop two efficient algorithms based on variants of this relaxation . The algorithms minimize the non - convex objective using belief propagation and dual decomposition as building blocks . Experiments on synthetic and real - world data show that the solutions returned by our algorithms substantially improve over the LP relaxation . Clustering by Low - Rank Doubly Stochastic Matrix Decomposition . Zhirong Yang , Erkki Oja . Abstract : Clustering analysis by nonnegative low - rank approximations has achieved remarkable progress in the past decade . However , most approximation approaches in this direction are still restricted to matrix factorization . We propose a new low - rank learning method to improve the clustering performance , which is beyond matrix factorization . The approximation is based on a two - step bipartite random walk through virtual cluster nodes , where the approximation is formed by only cluster assigning probabilities . Minimizing the approximation error measured by Kullback - Leibler divergence is equivalent to maximizing the likelihood of a discriminative model , which endows our method with a solid probabilistic interpretation . The optimization is implemented by a relaxed Majorization - Minimization algorithm that is advantageous in finding good local minima . Furthermore , we point out that the regularized algorithm with Dirichlet prior only serves as initialization . Experimental results show that the new method has strong performance in clustering purity for various datasets , especially for large - scale manifold data . Dependent Hierarchical Normalized Random Measures for Dynamic Topic Modeling . Changyou Chen , Nan Ding , Wray Buntine . Abstract : We develop dependent hierarchical normalized random measures and apply them to dynamic topic modeling . The dependency arises via superposition , subsampling and point transition on the underlying Poisson processes of these measures . The measures used include normalised generalised Gamma processes that demonstrate power law properties , unlike Dirichlet processes used previously in dynamic topic modeling . Inference for the model includes adapting a recently developed slice sampler to directly manipulate the underlying Poisson process . Experiments performed on news , blogs , academic and Twitter collections demonstrate the technique gives superior perplexity over a number of previous models . State - Space Inference for Non - Linear Latent Force Models with Application to Satellite Orbit Prediction . Jouni Hartikainen , Mari Sepp\\u00e4nen , Simo S\\u00e4rkk\\u00e4 . Abstract : Latent force models ( LFMs ) are flexible models that combine mechanistic modelling principles ( i.e. , physical models ) with non - parametric data - driven components . Several key applications of LFMs need non - linearities , which results in analytically intractable inference . In this work we show how non - linear LFMs can be represented as non - linear white noise driven state - space models and present an efficient non - linear Kalman filtering and smoothing based method for approximate state and parameter inference . We illustrate the performance of the proposed methodology via two simulated examples , and apply it to a real - world problem of long - term prediction of GPS satellite orbits . Sparse Additive Functional and Kernel CCA . Sivaraman Balakrishnan , Kriti Puniyani , John Lafferty . Abstract : Canonical Correlation Analysis ( CCA ) is a classical tool for finding correlations among the components of two random vectors . In recent years , CCA has been widely applied to the analysis of genomic data , where it is common for researchers to perform multiple assays on a single set of patient samples . Recent work has proposed sparse variants of CCA to address the high dimensionality of such data . However , classical and sparse CCA are based on linear models , and are thus limited in their ability to find general correlations . In this paper , we present two approaches to high - dimensional nonparametric CCA , building on recent developments in high - dimensional nonparametric regression . We present estimation procedures for both approaches , and analyze their theoretical properties in the high - dimensional setting . We demonstrate the effectiveness of these procedures in discovering nonlinear correlations via extensive simulations , as well as through experiments with genomic data . The Kernelized Stochastic Batch Perceptron . Andrew Cotter , Shai Shalev - Shwartz , Nathan Srebro . Abstract : We present a novel approach for training kernel Support Vector Machines , establish learning runtime guarantees for our method that are better then those of any other known kernelized SVM optimization approach , and show that our method works well in practice compared to existing alternatives . Fast classification using sparse decision DAGs . Robert Busa - Fekete , Djalel Benbouzid , Balazs Kegl . Abstract : In this paper we propose an algorithm that builds sparse decision DAGs ( directed acyclic graphs ) out of a list of base classifiers provided by an external learning method such as AdaBoost . The basic idea is to cast the DAG design task as a Markov decision process . Each instance can decide to use or to skip each base classifier , based on the current state of the classifier being built . The result is a sparse decision DAG where the base classifiers are selected in a data - dependent way . The method has a single hyperparameter with a clear semantics of controlling the accuracy / speed trade - off . The algorithm is competitive with state - of - the - art cascade detectors on three object - detection benchmarks , and it clearly outperforms them in the regime of low number of base classifiers . Unlike cascades , it is also readily applicable for multi - class classification . Using the multi - class setup , we show on a benchmark web page ranking data set that we can significantly improve the decision speed without harming the performance of the ranker . A Combinatorial Algebraic Approach for the Identifiability of Low - Rank Matrix Completion . Franz Kir\\u00e1ly , Ryota Tomioka . Abstract : In this paper , we review the problem of matrix completion and expose its intimate relations with algebraic geometry and combinatorial graph theory . We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries , yielding theoretical constraints and new algorithms for the problem of matrix completion . We conclude with algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes , showing that the algebraic - combinatoric approach can lead to improvements over state - of - the - art matrix completion methods . Rethinking Collapsed Variational Bayes Inference for LDA . Issei Sato , Hiroshi Nakagawa . Abstract : We provide a unified view for existing inference algorithms of latent Dirichlet allocation by using the alpha - divergence projection . In particular , we explain the collapsed variational Bayes inference with zero - order information , called the CVB0 inference , in terms of the local alpha - divergence projection . Exact Maximum Margin Structure Learning of Bayesian Networks . Robert Peharz , Franz Pernkopf . Abstract : Recently , there has been much interest in finding globally optimal Bayesian network structures . % , which is NP - hard in general . These techniques were developed for generative scores and can not be directly extended to discriminative scores , as desired for classification . In this paper , we propose an exact method for finding network structures maximizing the probabilistic soft margin , a successfully applied discriminative score . Our method is based on branch - and - bound techniques within a linear programming framework and maintains an any - time solution , together with worst - case suboptimality bounds . We introduce a set of order constraints for enforcing the network structure to be acyclic , which allows a compact problem representation and the use of general - purpose optimization techniques . In classification experiments , our methods clearly outperform generatively trained network structures and compete with support vector machines . AOSO - LogitBoost : Adaptive One - Vs - One LogitBoost for Multi - Class Problem . Peng Sun , Mark Reid , Jie Zhou . Abstract : This paper is dedicated to the improvement of model learning in multi - class LogitBoost for classification . Motivated by statistical view , LogitBoost can be seen as additive tree regression . Important facts in such a setting are 1 ) coupled classifier output as sum - to - zero constraint and 2 ) dense Hessian matrix arising in tree node split gain and node values fitting . On the one hand , the setting is too complicated for a tractable model learning algorithm ; On the other hand , too aggressive simplification of the setting may lead to degraded performance . For example , the original LogitBoost is outperformed by ABC - LogitBoost due to the later 's more careful treatment for the above two key points in problem settings . In this paper we propose improved methods to address the challenge : we adopt 1 ) vector tree ( i.e. node value is vector ) that enforces sum - to - zero constraint and 2 ) adaptive block coordinate descent exploiting dense Hessian when computing tree split gain and node values . Higher classification accuracy and faster convergence rate are observed for a range of public data sets when comparing to both original and ABC LogitBoost . Hypothesis testing using pairwise distances and associated kernels . Dino Sejdinovic , Arthur Gretton , Bharath Sriperumbudur , Kenji Fukumizu . - Accepted . The equivalence holds when energy distances are computed with semimetrics of negative type , in which case a kernel may be defined such that the RKHS distance between distributions corresponds exactly to the energy distance . We determine the class of probability distributions for which kernels induced by semimetrics are characteristic ( that is , for which embeddings of the distributions to an RKHS are injective ) . Monte Carlo Bayesian Reinforcement Learning . Yi Wang , Kok Sung Won , David Hsu , Wee Sun Lee . Abstract : Bayesian reinforcement learning ( BRL ) encodes prior knowledge of the world in a model and represents uncertainty in model parameters by maintaining a posterior distribution over them . This paper proposes a simple and general approach to BRL . The POMDP does not require conjugate distributions for belief representation , as earlier works do , and can be solved relatively easily with point - based approximation algorithms . Our approach naturally handles both fully and partially observable worlds . Theoretical and experimental results show that the discrete POMDP approximates the underlying BRL problem well with guaranteed performance . A Topic Model for Melodic Sequences . Athina Spiliopoulou , Amos Storkey . Abstract : We examine the problem of learning a probabilistic model for melody directly from musical sequences belonging to the same genre . This is a challenging task as one needs to tackle not only the complex statistical dependencies that characterize a music genre , but also the element of novelty , as each music piece is a unique realization of a musical form . To address this problem we introduce the Variable - gram Topic Model , which couples the latent topic formalism with a systematic model for contextual information . We evaluate the model using a next - step prediction task . Additionally , we present a novel way of model evaluation , where we directly compare model samples with data sequences using the Maximum Mean Discrepancy ( MMD ) of string kernels , to assess how close is the model distribution to the data distribution . We show that the model has the highest performance under both evaluation measures when compared to LDA , the Topic Bigram and related non - topic models . Output Space Search for Structured Prediction . Janardhan Rao Doppa , Alan Fern , Prasad Tadepalli . Abstract : We consider a framework for structured prediction based on search in the space of complete structured outputs . Given a structured input , an output is produced by running a time - bounded search procedure , guided by a learned cost function , and then returning the least cost output uncovered during the search . This framework can be instantiated for a wide range of search spaces and search procedures , and easily incorporates arbitrary structured - prediction loss functions . In this paper , we make two main technical contributions within this framework . First , we describe a novel approach to automatically defining an effective search space over structured outputs , which is able to leverage the availability of powerful classification learning algorithms . In particular , we define the limited - discrepancy search space and relate the quality of that space to the quality of learned classifiers . Second , we give a generic cost function learning approach that can be instantiated for a wide range of search procedures . The key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function . Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state - of - the - art structured - prediction performance . How To Grade a Test Without Knowing the Answers - A Bayesian Graphical Model for Adaptive Crowdsourcing and Aptitude Testing . Yoram Bachrach , Thore Graepel , Tom Minka , John Guiver . Abstract : We propose a new probabilistic graphical model that jointly models the difficulties of questions , the abilities of participants and the correct answers to questions in aptitude testing and crowdsourcing settings . We devise an active learning / adaptive testing scheme based on a greedy minimization of expected model entropy , which allows a more efficient resource allocation by dynamically choosing the next question to be asked based on the previous responses . We present experimental results that confirm the ability of our model to infer the required parameters and demonstrate that the adaptive testing scheme requires fewer questions to obtain the same accuracy as a static test scenario . Parallelizing Exploration - Exploitation Tradeoffs with Gaussian Process Bandit Optimization . Thomas Desautels , Andreas Krause , Joel Burdick . Abstract : Can one parallelize complex exploration - exploitation tradeoffs ? As an example , consider the problem of optimal high - throughput experimental design , where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response , as well as identifying the maximum of the function . We formalize the task as a multi - armed bandit problem , where the unknown payoff function is sampled from a Gaussian process ( GP ) , and instead of a single arm , in each round we pull a batch of several arms in parallel . We develop GP - BUCB , a principled algorithm for choosing batches , based on the GP - UCB algorithm for sequential GP optimization . We prove that , perhaps surprisingly , the cumulative regret of the parallel algorithm , compared to the sequential approach , only increases by a constant factor independent of the batch size B , for any fixed B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization . We demonstrate the effectiveness of our approach on two real - world applications . A Simple Algorithm for Semi - supervised Learning with Improved Generalization Error Bound . Ming Ji , Tianbao Yang , Binbin Lin , Rong Jin , Jiawei Han . Abstract : In this work , we develop a simple algorithm for semi - supervised regression . The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression . We show that under appropriate assumptions about the integral operator , this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning . We also verify the effectiveness of the proposed algorithm by an empirical study . Bayesian Optimal Active Search and Surveying . Roman Garnett , Yamuna Krishnamurthy , Xuehan Xiong , Jeff Schneider , Richard Mann . - Accepted . Abstract : We consider two active binary - classification problems with atypical objectives . In the first , active search , our goal is to actively uncover as many members of a given class as possible . In the second , active surveying , our goal is to actively query points to ultimately predict the class proportion of a given class . Numerous real - world problems can be framed in these terms , and in either case typical model - based concerns such as generalization error are only of secondary importance . We approach these problems via Bayesian decision theory ; after choosing natural utility functions , we derive the optimal policies . We provide three contributions . In addition to introducing the active surveying problem , we extend previous work on active search in two ways . First , we prove a novel theoretical result , that less - myopic approximations to the optimal policy can outperform more - myopic approximations by any arbitrary degree . We then derive bounds that for certain models allow us to reduce ( in practice dramatically ) the exponential search space required by a na\\u00efve implementation of the optimal policy , enabling further lookahead while still ensuring the optimal decisions are always made . Incorporating Domain Knowledge in Matching Problems via Harmonic Analysis . Deepti Pachauri , Maxwell Collins , Vikas SIngh . Abstract : Matching one set of objects to another is a ubiquitous task in machine learning and computer vision that often reduces to some form of the quadratic assignment problem ( QAP ) . The QAP is known to be notoriously hard , both in theory and in practice . We propose a new approach to accelerating the solution of QAPs based on learning parameters for a modified objective function from prior QAP instances . Experiments show that in practical domains the new method can significantly outperform existing approaches . Lognormal and Gamma Mixed Negative Binomial Regression . Mingyuan Zhou , Lingbo Li , David Dunson , Lawrence Carin . Abstract : In regression analysis of counts , a lack of simple and efficient algorithms for posterior computation has made Bayesian approaches appear unattractive and thus underdeveloped . By placing a gamma distribution prior on the NB dispersion parameter r , and connecting a lognormal distribution prior with the logit of the NB probability parameter p , efficient Gibbs sampling and variational Bayes inference are both developed . The closed - form updates are obtained by exploiting conditional conjugacy via both a compound Poisson representation and a Polya - Gamma distribution based data augmentation approach . The proposed Bayesian inference can be implemented routinely , while being easily generalizable to more complex settings involving multivariate dependence structures . The algorithms are illustrated using real examples . Greedy Algorithms for Sparse Reinforcement Learning . Christopher Painter - Wakefield , Ronald Parr . Abstract : Feature selection and regularization are becoming increasingly prominent tools in the efforts of the reinforcement learning ( RL ) community to expand the reach and applicability of RL . One approach to the problem of feature selection is to impose a sparsity - inducing form of regularization on the learning method . Recent work on L_1 regularization has adapted techniques from the supervised learning literature for use with RL . Another approach that has received renewed attention in the supervised learning community is that of using a simple algorithm that greedily adds new features . Such algorithms have many of the good properties of the L_1 regularization methods , while also being extremely efficient and , in some cases , allowing theoretical guarantees on recovery of the true form of a sparse target function from sampled data . This paper considers variants of orthogonal matching pursuit ( OMP ) applied to reinforcement learning . The resulting algorithms are analyzed and compared experimentally with existing L_1 regularized approaches . We demonstrate that perhaps the most natural scenario in which one might hope to achieve sparse recovery fails ; however , one variant , OMP - BRM , provides promising theoretical guarantees under certain assumptions on the feature dictionary . Another variant , OMP - TD , empirically outperforms prior methods both in approximation accuracy and efficiency on several benchmark problems . Variance Function Estimation in High - dimensions . Mladen Kolar , James Sharpnack . Abstract : We consider the high - dimensional heteroscedastic regression model , where the mean and the log variance are modeled as a linear combination of input variables . Existing literature on high - dimensional linear regression models has largely ignored non - constant error variances , even though they commonly occur in a variety of applications ranging from biostatistics to finance . In this paper we study a class of non - convex penalized pseudolikelihood estimators for both the mean and variance parameters . We show that the Heteroscedastic Iterative Penalized Pseudolikelihood Optimizer ( HIPPO ) achieves the oracle property , that is , we prove that the rates of convergence are the same as if the true model was known . We demonstrate numerical properties of the procedure on a simulation study and real world data . Conditional Sparse Coding and Grouped Multivariate Regression . Min Xu , John Lafferty . Abstract : We study the problem of multivariate regression where the data are naturally grouped , and a regression matrix is to be estimated for each group . We propose an approach in which a dictionary of low rank parameter matrices is estimated across groups , and a sparse linear combination of the dictionary elements is estimated to form a model within each group . We refer to the method as conditional sparse coding since it is a coding procedure for the response vectors Y conditioned on the covariate vectors X. This approach captures the shared information across the groups while adapting to the structure within each group . It exploits the same intuition behind sparse coding that has been successfully developed in computer vision and computational neuroscience . We propose an algorithm for conditional sparse coding , analyze its theoretical properties in terms of predictive accuracy , and present the results of simulation experiments that compare the new technique to reduced rank regression . Apprenticeship Learning for Model Parameters of Partially Observable Environments . Takaki Makino , Johane Takeuchi . Abstract : We consider apprentice learning - i.e. , having an agent learn a task by observing an expert demonstrating the task - in a partially observable environment when the model of the environment is uncertain . This setting is useful in applications where the explicit modeling of the environment is difficult , such as a dialogue system . We show that we can extract information about the environment model by inferring action selection process behind the demonstration , under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment . We show that our proposed algorithms can estimate the parameters of the environment model with much shorter demonstration compared to learning the model only from the reaction from the environment . Modeling Images using Transformed Indian Buffet Processes . KE ZHAI , Yuening Hu , Jordan Boyd - Graber , Sinead Williamson . Abstract : Latent feature models are attractive for image modeling ; images generally contain multiple objects . However , many latent feature models ignore that objects can appear at different locations , or require pre - segmentation of images . While the transformed Indian buffet process ( tIBP ) provides a method for modeling transformation - invariant features in simple , unsegmented binary images , in its current form it is inappropriate for real images because of computational constraints and modeling assumptions . We combine the tIBP with likelihoods appropriate for real images . We also develop an efficient inference scheme using the cross - correlation between images and features that is both theoretically and empirically faster than existing inference techniques . We demonstrate that , using our method , we are able to discover reasonable components and achieve effective image reconstruction in natural images . Learning Object Arrangements in 3D Scenes using Human Context . Yun Jiang , Marcus Lim , Ashutosh Saxena . Abstract : We consider the problem of learning object arrangements in a 3D scene . The key idea here is to learn how objects relate to human skeletons based their affordances , ease of use and reachability . We design appropriate density functions based on 3D spatial features to capture this . We then learn the distribution of human poses in a scene using Dirichlet processes . This allows our algorithm to reason about arrangement of the objects in the room . This is in contrast to previous approaches that learn context by learning object - object context such as co - occurrence relationships . We tested our algorithm extensively on 20 different rooms with a total of 47 objects . Our algorithm predicted correct placements with an error of 1.6 meters from the ground truth and received a score of 4.3/5 in arranging five real scenes compared to 3.7 of the best baseline . Cross Language Text Classification via Subspace Co - regularized Multi - view Learning . Yuhong Guo , Min Xiao . Abstract : In many multilingual text classification problems , the documents in different languages often share the same set of categories . To reduce the labeling cost of training a classification model for each individual language , it is important to transfer knowledge gained from the labeled data in one language to another language by conducting cross language classification . In this paper we develop a novel subspace co - regularized multi - view learning method for cross language text classification . This method is built on parallel corpora produced by machine translation . It jointly minimizes the training error of each classifier in each language while penalizing the distance between the subspace representations of parallel documents . Our empirical study on a large set of cross language text classification tasks shows the proposed method consistently outperforms the alternative inductive methods , domain adaptation methods , and multi - view learning methods . Plug - in martingales for testing exchangeability on - line . Valentina Fedorova , Alex Gammerman , Ilia Nouretdinov , Volodya Vovk . Abstract : A standard assumption in machine learning is the exchangeability of data ; in other words , the examples are assumed to be generated from the same probability distribution independently . This paper is devoted to testing the assumption of exchangeability on - line : the examples arrive one by one , and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified . Such measure are provided by exchangeability martingales . We extend known techniques for constructing exchangeability martingales and show that our new method is competitive to martingales introduced before . Finally we investigate the performance of our testing method on two benchmark datasets , USPS and Statlog Satellite data ; for the former , the known techniques give satisfactory results , but for the latter our new more flexible method becomes necessary . An Iterative Locally Linear Embedding Algorithm . Deguang Kong , Chris H.Q. Ding . Abstract : Local Linear embedding(LLE ) is a popular dimension reduction method . In this paper , we first show LLE with nonnegative constraint is equivalent to the widely used Laplacian embedding . We further propose to iterate the two steps in LLE repeatedly to improve the results . Thirdly , we relax the kNN constraint of LLE and present a sparse similarity learning algorithm . The final Iterative LLE combines these three improvements . Extensive experiment results show that iterative LLE algorithm significantly improve both classification and clustering results . Gaussian Process Quantile Regression using Expectation Propagation . Alexis Boukouvalas , Remi Barillec , Dan Cornford . Abstract : Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables . We present a new framework for direct quantile regression where a Gaussian process model is learned , minimising the expected tilted loss function . The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm . We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets . Latent Multi - group Membership Graph Model . Myunghwan Kim , Jure Leskovec . Abstract : We develop the Latent Multi - group Membership Graph ( LMMG ) model , a model of networks with rich node feature structure . In the LMMG model , each node belongs to multiple groups and each latent group models the occurrence of links as well as the node feature structure . The LMMG can be used to summarize the network structure , to predict links between the nodes , and to predict missing features of a node . We derive efficient inference and learning algorithms and evaluate the predictive performance of the LMMG on several social and document network datasets . Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations . Nando de Freitas , Alex Smola , Masrour Zoghi . Abstract : This paper analyzes the problem of Gaussian process ( GP ) bandits with deterministic observations . The analysis uses a branch and bound algorithm that is related to the UCB algorithm of ( Srinivas et al , 2010 ) . To complement their result , we attack the deterministic case and attain a much faster exponential convergence rate . Here , d is the dimension of the search space and tau is a constant that depends on the behaviour of the objective function near its global maximum . Estimating the Hessian by Back - propagating Curvature . James Martens , Ilya Sutskever , Kevin Swersky . Abstract : In this work we develop Curvature Propagation ( CP ) , a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph . At the cost of roughly two gradient evaluations , CP can give a rank-1 approximation of the whole Hessian , and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian . Of particular interest is the diagonal of the Hessian , for which no general approach is known to exist that is both efficient and accurate . We show in experiments that CP turns out to work well in practice , giving very accurate estimates of the Hessian of neural networks , for example , with a relatively small amount of work . We also apply CP to Score Matching , where a diagonal of a Hessian plays an integral role in the Score Matching objective , and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models . Feature Selection via Probabilistic Outputs . Andrea Danyluk , Nicholas Arnosti . Abstract : This paper investigates two feature - scoring criteria that make use of estimated class probabilities : one method proposed by shen and a complementary approach proposed below . We develop a theoretical framework to analyze each criterion and show that both estimate the spread ( across all values of a given feature ) of the probability that an example belongs to the positive class . Based on our analysis , we predict when each scoring technique will be advantageous over the other and give empirical results validating those predictions . Hierarchical Exploration for Accelerating Contextual Bandits . Yisong Yue , Sue Ann Hong , Carlos Guestrin . Abstract : Contextual bandit learning is a popular approach to optimizing recommender systems , but can be slow to converge in practice due to the need for explorating a large feature space . In this paper , we propose a coarse - to - fine hierarchical approach for encoding prior knowl- 016 edge which drastically reduces the amount of exploration required . Intuitively , user preferences can be reasonably embedded in a coarse low - dimensional feature space that can be explored efficiently , requiring exploration in the high - dimensional space only as necessary . We introduce a bandit algorithm for exploring within this coarse - to - fine spectrum , and prove performance guarantees that depend on how well the coarser feature space captures the user 's preferences . We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation . Fast Training of Nonlinear Embedding Algorithms . Max Vladymyrov , Miguel Carreira - Perpinan . Abstract : Stochastic neighbor embedding and related nonlinear manifold learning algorithms achieve high - quality low - dimensional representations of similarity data , but are notoriously slow to train . We propose a generic formulation of embedding algorithms that includes SNE and other existing algorithms , and study their relation with spectral methods and graph Laplacians . This allows us to define several partial - Hessian optimization strategies , characterize their global and local convergence , and evaluate them empirically . We achieve up to two orders of magnitude speedup over existing training methods with a strategy that adds nearly no overhead to the gradient and yet is simple , scalable and applicable to several existing and future embedding algorithms . Fast Prediction of New Feature Utility . Hoyt Koepke , Mikhail Bilenko . Abstract : We study the new feature utility prediction problem : statistically testing whether adding a new feature to the data representation can improve predictive accuracy on a supervised learning task . In many applications , identifying new informative features is the primary pathway for improving performance . However , evaluating every potential feature by re - training the predictor with it can be costly computationally , logistically and financially . The paper describes an efficient , learner - independent technique for estimating new feature utility without re - training based on the current predictor 's outputs . The method is obtained by deriving a connection between loss reduction potential and the new feature 's correlation with the loss gradient of the current predictor . This leads to a simple yet powerful hypothesis testing procedure , for which we prove consistency . Our theoretical analysis is accompanied by empirical evaluation on standard benchmarks and a large - scale industrial dataset . Robust Classification with Adiabatic Quantum Optimization . Vasil Denchev , Nan Ding , SVN Vishwanathan , Hartmut Neven . Abstract : We propose a non - convex training objective for robust binary classification of data sets in which label noise is present . The design is guided by the intention of solving the resulting problem by adiabatic quantum optimization . Two choices are prompted by the engineering constraints of existing quantum hardware : training problems are formulated as quadratic unconstrained binary optimization ; and model parameters are represented as binary expansions of low bit - depth . In the present work we validate this approach by using a heuristic classical solver as a stand - in for quantum hardware . Testing on several popular data sets and comparing with a number of existing convex and non - convex losses solved by convex optimization , we find substantial advantages in robustness as measured by test error under increasing label noise . Robustness is enabled by the non - convexity of our hardware - compatible loss function , which we name q -loss . We emphasize that we do not claim any theoretical superiority of q -loss over other non - convex losses . In fact if solving them to optimality was possible , they could be just as good or better , but q -loss has one important property that all others are lacking - namely that it is compatible with quantum hardware . Agglomerative Bregman Clustering . Matus Telgarsky , Sanjoy Dasgupta . Abstract : This manuscript develops the theory of agglomerative clustering with Bregman divergences . Geometric smoothing techniques are developed to deal with degenerate clusters . To allow for cluster models based on exponential families with overcomplete representations , Bregman divergences are developed for nondifferentiable convex functions . Isoelastic Agents and Wealth Updates in Machine Learning Markets . Amos Storkey , Jono Millin , Krzysztof Geras . Abstract : Recently , prediction markets have shown considerable promise for developing flexible mechanisms for machine learning . In this paper agents with isoelastic utilities are considered , and it is shown that the costs associated with homogeneous markets of agents with isoelastic utilities produce equilibrium prices corresponding to alpha - mixtures , with a particular form of mixing component relating to each agent 's wealth . We also demonstrate that wealth accumulation for logarithmic and other isoelastic agents ( through payoffs on prediction of training targets ) can implement both Bayesian model updates and mixture weight updates by imposing different market payoff structures . An efficient variational algorithm is given for market equilibrium computation . Quasi - Newton Methods : A New Direction . Philipp Hennig , Martin Kiefel . Abstract : Four decades after their invention , quasi - Newton methods are still state of the art in unconstrained numerical optimization . Although not usually interpreted thus , these are learning algorithms that fit a local quadratic approximation to the objective function . We show that many , including the most popular , quasi - Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions . This new notion elucidates some shortcomings of classical algorithms , and lights the way to a novel nonparametric quasi - Newton method , which is able to make more efficient use of available information at computational cost similar to its predecessors . No - Regret Learning in Extensive - Form Games with Imperfect Recall . Marc Lanctot , Richard Gibson , Neil Burch , Michael Bowling . Abstract : Counterfactual Regret Minimization ( CFR ) is an efficient no - regret learning algorithm for decision problems modeled as extensive games . CFR 's regret bounds depend on the requirement of perfect recall : players always remember information that was revealed to them and the order in which it was revealed . In games without perfect recall , however , CFR 's guarantees do not apply . In this paper , we present the first regret bound for CFR when applied to a general class of games with imperfect recall . In addition , we show that CFR applied to any abstraction belonging to our general class results in a regret bound not just for the abstract game , but for the full game as well . We verify our theory and show how imperfect recall can be used to trade a small increase in regret for a significant reduction in memory in three domains : die - roll poker , phantom tic - tac - toe , and Bluff . Information - theoretic Semi - supervised Metric Learning via Entropy Regularization . Gang Niu , Bo Dai , Makoto Yamada , Masashi Sugiyama . Abstract : We propose a general information - theoretic approach called Seraph for semi - supervised metric learning that does not rely upon the manifold assumption . It maximizes / minimizes entropies of labeled / unlabeled data pairs in the supervised / unsupervised part , which allows these two parts to be integrated in a natural and meaningful way . Furthermore , it is equipped with the hyper - sparsity : Given a certain probabilistic model parameterized by the learned metric , the posterior distribution and the resultant projection matrix are both ' sparse ' . Consequently , the metric learned by Seraph possesses high discriminability even under a noisy environment . The optimization problem of Seraph can be solved efficiently and stably by an EM - like scheme , where the E - Step is analytical and the M - Step is convex and ' smooth ' . Experiments demonstrate that Seraph compares favorably with many well - known metric learning methods . Fast approximation of matrix coherence and statistical leverage . Michael Mahoney , Petros Drineas , Malik Magdon - Ismail , David Woodruff . Abstract : The statistical leverage scores of a matrix A are the squared row - norms of the matrix containing its ( top ) left singular vectors and the coherence is the largest leverage score . These quantities have been of interest in recently - popular problems such as matrix completion and Nystrom - based low - rank matrix approximation ; in large - scale statistical data analysis applications more generally ; and since they define the key structural nonuniformity that must be dealt with in developing fast randomized matrix algorithms . The proposed algorithm runs in O(nd log n ) time , as opposed to the O(nd2 ) time required by the na\\u0131ve algorithm that involves computing an orthogonal basis for the range of A. This resolves an open question from ( Drineas et al . , 2006b ) and ( Mohri & Talwalkar , 2011 ) ; and our result leads to immediate improvements in coreset - based L2-regression , the estimation of the coherence of a matrix , and several related low - rank matrix problems . Interestingly , to achieve our result we judiciously apply random projections on both sides of A. . Predicting Manhole Events in New York City . Cynthia Rudin , Rebecca Passonneau , Axinia Radeva , Steve Ierome , Delfina Isaac . - Not for proceedings . Abstract : We present a knowledge discovery and data mining process developed as part of the Columbia / Con Edison project on manhole event prediction . This process can assist with real - world prioritization problems that involve raw data in the form of noisy documents requiring significant amounts of pre - processing . The documents are linked to a set of instances to be ranked according to prediction criteria . Our ranking results are currently used to help prioritize repair work on the Manhattan , Brooklyn , and Bronx electrical grids . Artist Agent : A Reinforcement Learning Approach to Automatic Stroke Generation in Oriental Ink Painting . Ning Xie , Hirotaka Hachiya , Masashi Sugiyama . Abstract : Oriental ink painting , called Sumi - e , is one of the most appealing painting styles that has attracted artists around the world . Major challenges in computer - based Sumi - e simulation are to abstract complex scene information and draw smooth and natural brush strokes . To automatically find such strokes , we propose to model the brush as a reinforcement learning agent , and learn desired brush - trajectories by maximizing the sum of rewards in the policy search framework . We also provide elaborate design of actions , states , and rewards tailored for a Sumi - e agent . The effectiveness of our proposed approach is demonstrated through simulated Sumi - e experiments . On multi - view feature learning . Roland Memisevic . Abstract : Sparse coding is a common approach to learning local features for object recognition . Recently , there has been an increasing interest in learning features from spatio - temporal , binocular , or other multi - observation data , where the goal is to encode the relationship between images rather than the content of a single image . We discuss the role of multiplicative interactions and of squaring non - linearities in learning such relations . In particular , we show that training a sparse coding model whose filter responses are multiplied or squared amounts to jointly diagonalizing a set of matrices that encode image transformations . Inference amounts to detecting rotations in the shared eigenspaces . Our analysis helps explain recent experimental results showing that Fourier features and circular Fourier features emerge when training complex cell models on translating or rotating images . It also shows how learning about transformations makes it possible to learn invariant features . Fast Bounded Online Gradient Descent Algorithms for Scalable Kernel - Based Online Learning . Steven C.H. Hoi , Jialei Wang , Peilin Zhao , Rong Jin , Pengcheng Wu . Abstract : Although kernel - based online learning has shown promising performance in a number of studies , its main shortcoming is the unbounded number of support vectors , making it non - scalable and unsuitable for large - scale datasets . In this work , we study the problem of bounded kernel - based online learning that aims to constrain the number of support vectors by a predefined budget . Although several algorithms have been proposed , they are mostly based on the Perceptron algorithm and only provide bounds for the number of classification mistakes . To address this limitation , we propose a framework for bounded kernel - based online learning based on online gradient descent approach . We propose two efficient algorithms of bounded online gradient descent ( BOGD ) for bounded kernel - based online learning : ( i ) BOGD using uniform sampling and ( ii ) BOGD++ using non - uniform sampling . We present theoretical analysis of regret bound for both algorithms , show the promising empirical performance of the proposed algorithms by comparing them to the state - of - the - art algorithms for bounded kernel - based online learning . A Hybrid Algorithm for Convex Semidefinite Optimization . Soeren Laue . Abstract : We present a hybrid algorithm for optimizing a convex , smooth function over the cone of positive semidefinite matrices . Our algorithm converges to the global optimal solution and can be used to solve general large - scale semidefinite programs and hence can be readily applied to a variety of machine learning problems . We show experimental results on three machine learning problems ( matrix completion , metric learning , and sparse PCA ) . Our approach outperforms state - of - the - art algorithms . A Complete Analysis of the l_1,p Group - Lasso . Julia Vogt , Volker Roth . Abstract : The Group - Lasso is a well - known tool for joint regularization in machine learning methods . For all p - norms , a highly efficient projected gradient algorithm is presented . This new algorithm enables us to compare the prediction performance of many variants of the Group - Lasso in a multi - task learning setting , where the aim is to solve many learning problems in parallel which are coupled via the Group - Lasso constraint . We conduct large - scale experiments on synthetic data and on two real - world data sets . Convergence Rates of Biased Stochastic Optimization for Learning Sparse Ising Models . Jean Honorio . Abstract : We study the convergence rate of stochastic optimization of exact ( NP - hard ) objectives , for which only biased estimates of the gradient are available . We motivate this problem in the context of learning the structure and parameters of Ising models . We first provide a convergence - rate analysis of deterministic errors for forward - backward splitting ( FBS ) . We then extend our analysis to biased stochastic errors , by first characterizing a family of samplers and providing a high probability bound that allows understanding not only FBS , but also proximal gradient ( PG ) methods . Decoupling Exploration and Exploitation in Multi - Armed Bandits . Orly Avner , Shie Mannor , Ohad Shamir . Abstract : We consider a multi - armed bandit problem where the decision maker can explore and exploit different arms at every round . The exploited arm adds to the decision maker 's cumulative reward ( without necessarily observing the reward ) while the explored arm reveals its value . We devise algorithms for this setup and show that the dependence on the number of arms can be much better than the standard square root of the number of arms dependence , according to the behavior of the arms ' reward sequences . For the important case of piecewise stationary stochastic bandits , we show a significant improvement over existing algorithms . Our algorithms are based on a non - uniform sampling policy , which we show is essential to the success of any algorithm in the adversarial setup . We finally show some simulation results on an ultra - wide band channel selection inspired setting indicating the applicability of our algorithms . Learning to Identify Regular Expressions that Describe Email Campaigns . Paul Prasse , Christoph Sawade , Niels Landwehr , Tobias Scheffer . Abstract : This paper addresses the problem of inferring a regular expression from a given set of strings that resembles , as closely as possible , the regular expression that a human expert would have written to identify the language . This is motivated by our goal of automating the task of postmasters of an email service who use regular expressions to describe and blacklist email spam campaigns . Training data contains batches of messages and corresponding regular expressions that an expert postmaster feels confident to blacklist . We model this task as a learning problem with structured output spaces and an appropriate loss function , derive a decoder and the resulting optimization problem , and a report on a case study conducted with an email service . Deep Mixtures of Factor Analysers . Yichuan Tang , Ruslan Salakhutdinov , Geoffrey Hinton . Abstract : An efficient way to learn deep density models that have many layers of latent variables is to learn one layer at a time using a model that has only one layer of latent variables . After learning each layer , samples from the posterior distributions for that layer are used as training data for learning the next layer . This approach is commonly used with Restricted Boltzmann Machines , which are undirected graphical models with a single hidden layer , but it can also be used with Mixtures of Factor Analysers ( MFAs ) which are directed graphical models . In this paper , we present a greedy layer - wise learning algorithm for Deep Mixtures of Factor Analysers ( DMFAs ) . We demonstrate empirically that DMFAs learn better density models than both MFAs and two types of Restricted Boltzmann Machines on a wide variety of datasets . Revisiting k - means : New Algorithms via Bayesian Nonparametrics . Brian Kulis , Michael Jordan . Abstract : Bayesian models offer great flexibility for clustering applications - Bayesian nonparametrics can be used for modeling infinite mixtures , and hierarchical Bayesian models can be utilized for shared clusters across multiple data sets . For the most part , such flexibility is lacking in classical clustering methods such as k - means . In this paper , we revisit the k - means clustering algorithm from a Bayesian nonparametric viewpoint . We generalize this analysis to the case of clustering multiple data sets through a similar asymptotic argument with the hierarchical Dirichlet process . We also discuss further extensions that highlight the benefits of our analysis : i ) a spectral relaxation involving thresholded eigenvectors , and ii ) a normalized cut graph clustering algorithm that does not fix the number of clusters in the graph . Gaussian Process Regression Networks . Andrew Wilson , David A. Knowles , Zoubin Ghahramani . Abstract : We introduce a new regression framework , Gaussian process regression networks ( GPRN ) , which combines the structural properties of Bayesian neural networks with the nonparametric flexibility of Gaussian processes . GPRN accommodates input ( predictor ) dependent signal and noise correlations between multiple output ( response ) variables , input dependent length - scales and amplitudes , and heavy - tailed predictive distributions . We derive both elliptical slice sampling and variational Bayes inference procedures for GPRN . We apply GPRN as a multiple output regression and multivariate volatility model , demonstrating substantially improved performance over eight popular multiple output ( multi - task ) Gaussian process models and three multivariate volatility models on real datasets , including a 1000 dimensional gene expression dataset . Analysis of Kernel Mean Matching under Covariate Shift . Yaoliang Yu , Csaba Szepesvari . Abstract : In real supervised learning scenarios , it is not uncommon that the training and test sample follow different probability distributions , thus rendering the necessity to correct the sampling bias . Focusing on a particular covariate shift problem , we derive high probability confidence bounds for the kernel mean matching ( KMM ) estimator , whose convergence rate turns out to depend on some regularity measure of the regression function and also on some capacity measure of the kernel . By comparing KMM with the natural plug - in estimator , we establish the superiority of the former hence provide concrete evidence / understanding to the effectiveness of KMM under covariate shift . Tighter Variational Representations of f - Divergences via Restriction to Probability Measures . Avraham Ruderman , Mark Reid , Dar\\u00edo Garc\\u00eda - Garc\\u00eda , James Petterson . Abstract : We show that the variational representations for f - divergences currently used in the literature can be tightened . This has implications to a number of methods recently proposed based on this representation . As an example application we use our tighter representation to derive a general f - divergence estimator based on two i.i.d . samples and derive the dual program for this estimator that performs well empirically . We also point out a connection between our estimator and MMD . Training Restricted Boltzmann Machines on Word Observations . George Dahl , Ryan Adams , Hugo Larochelle . Abstract : The restricted Boltzmann machine ( RBM ) is a flexible tool for modeling complex data , however there have been significant computational difficulties in using RBMs to model high - dimensional multinomial observations . In natural language processing applications , words are naturally modeled by K - ary discrete distributions , where K is determined by the vocabulary size and can easily be in the hundreds of thousands . The conventional approach to training RBMs on word observations is limited because it requires sampling the states of K - way softmax visible units during block Gibbs updates , an operation that takes time linear in K. In this work , we address this issue by employing a more general class of Markov chain Monte Carlo operators on the visible units , yielding updates with computational complexity independent of K. We demonstrate the success of our approach by training RBMs on hundreds of millions of word n - grams using larger vocabularies than previously feasible and using the learned features to improve performance on chunking and sentiment classification tasks , achieving state - of - the - art results on the latter . Bayesian Watermark Attacks . Ivo Shterev , David Dunson . Abstract : This paper presents an application of statistical machine learning to the field of watermarking . We develop a new attack model on spread - spectrum watermarking systems , using Bayesian statistics . Our model jointly infers the watermark signal and embedded message bitstream , directly from the watermarked signal . No access to the watermark decoder is required . We develop an efficient Markov chain Monte Carlo sampler for updating the model parameters from their conjugate full conditional posteriors . We also provide a variational Bayesian solution , which further increases the convergence speed of the algorithm . Experiments with synthetic and real image signals demonstrate that the attack model is able to correctly infer a large part of the message bitstream , while at the same time obtaining a very accurate estimate of the watermark signal . Max - Margin Nonparametric Latent Feature Models for Link Prediction . Jun Zhu . Abstract : Recent work on probabilistic latent feature models have shown great promise in predicting unseen links in social network and relational data . We present a max - margin nonparametric latent feature model , which unites the ideas of max - margin learning and Bayesian nonparametrics to discover discriminative latent features for link prediction and automatically infer the unknown latent social dimension . By minimizing a hinge - loss using the linear expectation operator , we can perform posterior inference efficiently without dealing with a highly nonlinear link likelihood function ; by using a fully - Bayesian formulation , we can avoid tuning regularization constants . Experimental results on real datasets appear to demonstrate the benefits inherited from max - margin learning and fully - Bayesian nonparametric inference . Discriminative Probabilistic Prototype Learning . Edwin Bonilla , Antonio Robles - Kelly . Abstract : In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where each original input datapoint is described by a set of vectors and their associated outputs may be given by soft labels indicating , for example , class probabilities . We represent an input datapoint as a mixture of probabilities over the corresponding set of feature vectors where each probability indicates how likely each vector is to belong to an unknown prototype pattern . We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization . More importantly , both the model parameters and the prototype patterns can be learned from data in a discriminative way . We show that our model can be seen as a probabilistic generalization of learning vector quantization ( LVQ ) . We apply our method to the problems of shape classification , hyperspectral imaging classification and people 's work class categorization , showing the superior performance of our method compared to the standard prototype - based classification approach and other competitive benchmark methods . Sparse - GEV : Sparse Latent Space Model for Multivariate Extreme Value Time Serie Modeling . Yan Liu , Taha Bahadori , Hongfei Li . Abstract : In many applications of time series models , such as climate analysis or social media analysis , we are often interested in extreme events , such as heatwave , wind gust , and burst of topics . These time series data usually exhibit a heavy - tailed distribution rather than a normal distribution . This poses great challenges to existing approaches due to the significantly different assumptions on the data distributions and the lack of sufficient past data on extreme events . In this paper , we propose the sparse - GEV model , a latent state model based on the theory of extreme value modeling to automatically learn sparse temporal dependence and make predictions . Our model is theoretically significant because it is among the first models to learn sparse temporal dependencies between multivariate extreme value time series . We demonstrate the superior performance of our algorithm compared with state - of - art methods , including Granger causality , copula approach , and transfer entropy , on one synthetic dataset , one climate dataset and one Twitter dataset . Factorized Asymptotic Bayesian Hidden Markov Models . Ryohei Fujimaki , Kohei Hayashi . Abstract : This paper addresses the issue of model selection for hidden Markov models ( HMMs ) . We generalize factorized asymptotic Bayesian inference ( FAB ) , which has been recently developed for model selection on independent hidden variables ( i.e. , mixture models ) , for time - dependent hidden variables . As with FAB in mixture models , FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion ( FIC ) . It inherits , from FAB for mixture models , several desirable properties for learning HMMs , such as asymptotic consistency of FIC with marginal log - likelihood , a shrinkage effect for hidden state selection , monotonic increase of the lower FIC bound through the iterative optimization . Further , it does not have a tunable hyper - parameter , and thus its model selection process can be fully automated . Experimental results shows that FAB outperforms states - of - the - art variational Bayesian HMM and non - parametric Bayesian HMM in terms of model selection accuracy and computational efficiency . PAC - Bayesian Generalization Bound on Confusion Matrix for Multi - Class Classification . Emilie Morvant , Sokol Ko\\u00e7o , Liva Ralaivola . Abstract : In this paper , we propose a PAC - Bayes bound for the generalization risk of the Gibbs classifier in the multi - class classification framework . The novelty of our work is the critical use of the confusion matrix of a classifier as an error measure ; this puts our contribution in the line of work aiming at dealing with performance measure that are richer than mere scalar criterion such as the misclassification rate . Thanks to very recent and beautiful results on matrix concentration inequalities , we derive two bounds showing that the true confusion risk of the Gibbs classifier is upper - bounded by its empirical risk plus a term depending on the number of training examples in each class . To the best of our knowledge , this is the first PAC - Bayes bounds based on confusion matrices . Semi - Supervised Learning of Class Balance under Class - Prior Change by Distribution Matching . Marthinus Du Plessis , Masashi Sugiyama . Abstract : In real - world classification problems , the class balance in the training dataset does not necessarily reflect that of the test dataset , which can cause significant estimation bias . If the class ratio of the test dataset is known , instance re - weighting or resampling allows systematical bias correction . However , learning the class ratio of the test dataset is challenging when no labeled data is available from the test domain . In this paper , we propose to estimate the class ratio in the test dataset by matching probability distributions of training and test input data . We demonstrate the utility of the proposed approach through experiments . A Proximal - Gradient Homotopy Method for the L1-Regularized Least - Squares Problem . Lin Xiao , Tong Zhang . Since the objective function is not strongly convex , standard proximal gradient methods only achieve sublinear convergence . We propose a homotopy continuation strategy , which employs a proximal gradient method to solve the problem with a sequence of decreasing regularization parameters . It is shown that under common assumptions in compressed sensing , the proposed method ensures that all iterates along the homotopy solution path are sparse , and the objective function is effectively strongly convex along the solution path . This observation allows us to obtain a global geometric convergence rate for the procedure . Empirical results are presented to support our theoretical analysis . Comparison - Based Learning with Rank Nets . Amin Karbasi , Stratis Ioannidis , laurent Massoulie . Abstract : We consider the problem of search through comparisons , where a user is presented with two candidate objects and reveals which is closer to her intended target . We study adaptive strategies for finding the target , that require knowledge of rank relationships but not actual distances between objects . We propose a new strategy based on rank nets , and show that for target distributions with a bounded doubling constant , it finds the target in a number of comparisons close to the entropy of the target distribution and , hence , of the optimum . We extend these results to the case of noisy oracles , and compare this strategy to prior art over multiple datasets . Semi - Supervised Collective Classification via Hybrid Label Regularization . Luke McDowell , David Aha . Abstract : Many classification problems involve data instances that are interlinked with each other , such as webpages connected by hyperlinks . Techniques for ' collective classification ' ( CC ) often increase accuracy for such data graphs , but usually require a fully - labeled training graph . In contrast , we examine how to improve the semi - supervised learning of CC models when given only a sparsely - labeled graph , a common situation . We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non - relational features . We also extend the ideas of ' label regularization ' to such hybrid classifiers , enabling them to leverage the unlabeled data to bias the learning process . We find that these techniques , which are efficient and easy to implement , significantly increase accuracy on three real datasets . In addition , our results explain conflicting findings from prior related studies . Shortest path distance in random k - nearest neighbor graphs . Morteza Alamgir , Ulrike von Luxburg . Abstract : Consider a weighted or unweighted k - nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R^d . We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity . We prove that for unweighted kNN graphs , this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning . We also study the behavior of the shortest path distance in weighted kNN graphs . A Split - Merge Framework for Comparing Clusterings . Qiaoliang Xiang , Qi Mao , Kian Ming Chai , Hai Leong Chieu , Ivor Tsang , Zhenddong Zhao . - Accepted . Abstract : Clustering evaluation measures are frequently used to evaluate the performance of algorithms . However , most measures are not suitable for this task mainly because they are not normalized properly and ignore some information in the inherent structure of clusterings such as dependency and consistency . We model two clusterings as a bipartite graph and propose a general component - based decomposition formula based on the components of the graph . Under this formula , most existing measures can be reinterpreted . In order to capture dependency and satisfy consistency in the component , we further propose a split - merge framework for comparing clusterings of different data sets . The proposed framework is conditionally normalized , flexible to utilize previous measures , and extensible to take into account data point information , such as feature vectors and pairwise distances . Moreover , experimental results on a real world data set demonstrate that one instantiated measure of the framework outperforms other measures . Compositional Planning Using Optimal Option Models . David Silver , Kamil Ciosek . Abstract : In this paper we introduce a framework for option model composition . Option models are temporal abstractions that , like macro - operators in classical planning , jump directly from a start state to an end state . Prior work has focused on constructing option models from primitive actions , by intra - option model learning ; or on using option models to construct a value function , by inter - option planning . We present a unified view of intra- and inter - option model learning , based on a major generalisation of the Bellman equation . Our fundamental operation is the recursive composition of option models into other option models . This key idea enables compositional planning over many levels of abstraction . We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals , and also searches over those option models to provide rapid progress towards other subgoals . Information - Theoretical Learning of Discriminative Clusters for Unsupervised Domain Adaptation . Yuan Shi , Fei Sha . Abstract : We study the problem of unsupervised domain adaptation , which aims to adapt classifiers trained on a labeled source domain to an unlabeled target domain . Many existing approaches first learn domain - invariant features and then construct classifiers with them . We propose a novel approach that jointly learn the both . Specifically , the proposed method identifies a feature space where data in the source and the target domains are similarly distributed . More importantly , our method learns a discriminative feature space , optimizing an information - theoretic metric as an proxy to the expected misclassification error on the target domain . We show how this optimization can be effectively carried out with simple gradient - based methods and how hyperparameters can be cross validated without demanding any labeled data from the target domain . Empirical studies on the benchmark tasks of visual object recognition and sentiment analysis validate our modeling assumptions and demonstrate significant improvement of our method over competing ones in classification accuracies . Flexible Modeling of Latent Task Structures in Multitask Learning . Alexandre Passos , Piyush Rai , Jacques Wainer , Hal Daume III . Abstract : When learning multiple related tasks with limited training data per task , it is natural to share parameters across tasks to improve generalization performance . Imposing the correct notion of task relatedness is critical to achieve this goal . However , one rarely knows the correct relatedness notion a priori . We present a generative model that is based on the weight vector of each task being drawn from a nonparametric mixture of nonparametric factor analyzers . The nonparametric aspect of the model makes it broad enough to subsume many types of latent task structures previously considered in the literature as special cases , and also allows it to learn more general task structures , addressing their individual shortcomings . This brings in considerable flexibility as compared to the commonly used multitask learning models that are based on some a priori fixed notion of task relatedness . We also present an efficient variational inference algorithm for our model . Experimental results on synthetic and real - world datasets , on both regression and classification problems , establish the efficacy of the proposed method . An Efficient Approach to Sparse Linear Discriminant Analysis . Luis Francisco S\\u00e1nchez Merchante , Yves Grandvalet , G\\u00e9rrad Govaert . Abstract : We present the Group - Lasso Optimal Scoring Solver ( GLOSS ) , a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis ( LDA ) . Our formulation , which is based on penalized Optimal Scoring , preserves an exact equivalence with sparse LDA , contrary to the multi - class approaches based on the regression of class indicator that have been proposed so far . Additionally , the versatility of the implementation allows to impose some particular structure on the within - class covariance matrix . Computationally , the optimization algorithm considers two nested convex problems , the main one being a linear regression regularized by a quadratic penalty implementing the group - lasso penalty . As group - Lasso selects the same features in all discriminant directions , it generates extremely parsimonious models without compromising the prediction performances . Moreover , the resulting sparse discriminant directions are amenable to low - dimensional representations . Our algorithm is highly efficient for medium to large number of variables , and is thus particularly well suited to the analysis of gene expression data . The Greedy Miser : Learning under Test - time Budgets . Zhixiang Xu , Kilian Weinberger , Olivier Chapelle . Abstract : As machine learning algorithms enter applications in industrial settings , there is increased interest in controlling their cpu - time during testing . The cpu - time consists of the running time of the algorithm and the extraction time of the features . The latter can vary drastically when the feature set is diverse . In this paper , we propose an algorithm , the Greedy Miser , that incorporates the feature extraction cost during training to explicitly minimize the cpu - time during testing . The algorithm is a straightforward extension of stage - wise regression and is equally suitable for regression or multi - class classification . Compared to prior work , it is significantly more cost - effective and scales to larger data sets . Canonical Trends : Detecting Trend Setters in Web Data . Felix Biessmann , Jens - Michalis Papaioannou , Mikio Braun , Andreas Harth . - Accepted . Abstract : The web offers large amounts of information most of which is just copied or rephrased , a phenomenon that is often called trend . A central problem in the context of web data mining is to detect those web sources that publish information which will give rise to a trend first . Here we present a simple and efficient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to this trend before other websites do so . We validate our approach on real data collected from the most influential technology news feeds . A convex relaxation for weakly supervised classifiers . Armand Joulin , Francis Bach . Abstract : This paper introduces a general multi - class approach to weakly supervised classification . Inferring the labels and learning the parameters of the model is usually done jointly through a block - coordinate descent algorithm such as expectation - maximization ( EM ) , which may lead to local minima . To avoid this problem , we propose a cost function based on a convex relaxation of the soft - max loss . We then propose an algorithm specifically designed to efficiently solve the corresponding semidefinite program ( SDP ) . Empirically , our method compares favorably to standard ones on different datasets for multiple instance learning and semi - supervised learning as well as on clustering tasks . Demand - Driven Clustering in Relational Domains for Predicting Adverse Drug Events . Jesse Davis , Vitor Santos Costa , Elizabeth Berg , David Page , Peggy Peissig , Michael Caldwell . - Accepted . Abstract : Learning from electronic medical records ( EMR ) is challenging due to their relational nature and the need to capture the uncertain dependence between a patient 's past and future health status . Statistical relational learning ( SRL ) , which combines relational and probabilistic representations , is a natural fit for analyzing EMRs . SRL is less adept at handling the inherent latent structure , such as connections between related medications or diseases , in EMRs . One solution is to capture the latent structure via a relational clustering of objects . We propose a novel approach that , instead of pre - clustering the objects , performs a demand - driven clustering during the learning process . We evaluate our algorithm on three real - world tasks where the goal is to use EMRs to predict whether a patient will have an adverse reaction to a medication . We find that the proposed approach is more accurate than performing no clustering , pre - clustering and using expert - constructed medical heterarchies . A Binary Classification Framework for Two - Stage Multiple Kernel Learning . Abhishek Kumar , Alexandru Niculescu - Mizil , Koray Kavukcuoglu , Hal Daume III . - Accepted . Abstract : With the advent of kernel methods , automating the task of specifying a suitable kernel has become increasingly important . In this context , the Multiple Kernel Learning ( MKL ) problem of finding a combination of pre - specified base kernels that is suitable for the task at hand has received significant attention from researchers . In this paper we show that Multiple Kernel Learning can be framed as a standard binary classification problem with additional constraints that ensure the positive definiteness of the learned kernel . Framing MKL in this way has the distinct advantage that it makes it easy to leverage the extensive research in binary classification to develop better performing and more scalable MKL algorithms that are conceptually simpler , and , arguably , more accessible to practitioners . Experiments on nine data sets from different domains show that , despite their simplicity , the proposed techniques can significantly outperform current leading MKL approaches . Learning Invariant Representations with Local Transformations . Kihyuk Sohn , Honglak Lee . Abstract : The difficulty of developing feature learning algorithms that are robust to the novel transformations ( e.g. , scale , rotation , or translation ) has been a challenge in many applications ( e.g. , object recognition problems ) . In this paper , we address this important problem of transformation invariant feature learning by introducing the transformation matrices into the energy function of the restricted Boltzmann machines . Consistent Multilabel Ranking through Univariate Losses . Krzysztof Dembczy\\u0144ski , Wojciech Kot\\u0142owski , Eyke Huellermeier . Abstract : We consider the problem of rank loss minimization in the setting of multilabel classification , which is commonly tackled by means of convex surrogate losses defined on pairs of labels . Very recently , this approach was put into question by the negative result showing that any such loss function is necessarily inconsistent . In this paper , we show a positive result which is arguably surprising in light of the previous one : despite being simpler , common convex surrogates for binary classification , defined on single labels instead of label pairs , are consistent for rank loss minimization . Instead of directly proving convergence , we give a much stronger result by deriving regret bounds and convergence rates . The proposed losses suggest efficient and scalable algorithms , which are tested experimentally . On the Equivalence between Herding and Conditional Gradient Algorithms . Francis Bach , Simon Lacoste - Julien , Guillaume Obozinski . Abstract : We show the equivalence of the herding procedure of Welling ( 2009 ) with a standard convex optimization algorithm - namely a conditional gradient algorithm minimizing a quadratic moment discrepancy . This link enables us to harness convergence results from convex optimization as well as suggest faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space . We study the behavior of the different variants with numerical simulations . The experiments indicate that while we can improve on the task of approximating integrals , the original herding algorithm tends to approach more often the maximum entropy distribution , shedding more light on the learning bias behind herding . Variational Bayesian Inference with Stochastic Search . John Paisley , David Blei , Michael Jordan . Abstract : Mean - field variational inference is an approximate posterior inference method for Bayesian models . It approximates the full posterior distribution of a model 's variables with a factorized set of distributions by maximizing a lower bound on the marginal likelihood . This requires the ability to integrate the log joint likelihood of the model with respect to the factorized approximation . Often not all integrals are closed - form , which is traditionally handled using lower bound approximations . We present an algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound in all models . This method uses control variates for variance reduction of the stochastic search gradient , in which existing lower bounds can play an important role . We demonstrate the approach on logistic regression and the hierarchical Dirichlet process . Small - sample brain mapping : sparse recovery on spatially correlated designs with randomization and clustering . Gael Varoquaux , Alexandre Gramfort , Bertrand Thirion . Abstract : Functional neuroimaging can measure the brain 's response to an external stimulus . It is used to perform brain mapping : identifying from these observations the brain regions involved . This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus . Brain mapping is then seen as a support recovery problem . On functional MRI ( fMRI ) data , this problem is particularly challenging as i ) the number of samples is small due to limited acquisition time and ii ) the variables are strongly correlated . We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables . The use of randomization techniques , e.g. bootstrap samples , and hierarchical clustering of the variables improves the recovery properties of sparse methods . We demonstrate the benefit of our approach on an extensive simulation study as well as two publicly available fMRI datasets . Joint Optimization and Variable Selection of High - dimensional Gaussian Processes . Bo Chen , Rui Castro , Andreas Krause . Abstract : Maximizing high - dimensional , non - convex functions through noisy observations is a notoriously hard problem , but one that arises in many applications . In this paper , we tackle this challenge by modeling the unknown function as a sample from a high - dimensional Gaussian process ( GP ) distribution . Assuming that the unknown function only depends on few relevant variables , we show that it is possible to perform joint variable selection and GP optimization . We provide strong performance guarantees for our algorithm , bounding the sample complexity of variable selection , and as well as providing cumulative regret bounds . We further provide empirical evidence on the effectiveness of our algorithm on several benchmark optimization problems . Large - Scale Feature Learning With Spike - and - Slab Sparse Coding . Ian Goodfellow , Aaron Courville , Yoshua Bengio . Abstract : We consider the problem of object recogni- tion with a large number of classes . In or- der to scale existing feature learning algo- rithms to this setting , we introduce a new feature learning and extraction procedure based on a factor model we call spike - and- slab sparse coding ( S3C ) . Prior work on this model has not prioritized the ability to ex- ploit parallel architectures and scale to the enormous problem sizes needed for object recognition . We present an inference proce- dure appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors . We demonstrate that this approach improves upon the supervised learning ca- pabilities of both sparse coding and the ss- RBM on the CIFAR-10 dataset . We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes bet- ter than previous methods . Finally , we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Mod- els ' Transfer Learning Challenge . Anytime Marginal MAP Inference . Denis Maua , Cassio De Campos . Abstract : This paper presents a new anytime algorithm for the marginal MAP problem in graphical models . The algorithm is described in detail , its complexity and convergence rate are studied , and relations to previous theoretical results for the problem are discussed . It is shown that the algorithm runs in polynomial - time if the underlying graph of the model has bounded tree - width , and that it provides guarantees to the lower and upper bounds obtained within a fixed amount of computational resources . Experiments with both real and synthetic generated models highlight its main characteristics and show that it compares favorably against Park and Darwiche 's systematic search , particularly in the case of problems with many MAP variables and moderate tree - width . Distributed Parameter Estimation via Pseudo - likelihood . Qiang Liu , Alexander Ihler . Abstract : Estimating statistical models within sensor networks requires distributed algorithms , in which both data and computation are distributed across the nodes of the network . We propose a general approach for distributed learning based on combining local estimators defined by pseudo - likelihood components , encompassing a number of combination methods , and provide both theoretical and experimental analysis . We show that simple linear combination or max - voting methods , when combined with second - order information , are statistically competitive with more advanced and costly joint optimization . Our algorithms have many attractive properties including low communication and computational cost and \\\" any - time \\\" behavior . The Nonparametric Metadata Dependent Relational Model . Dae Il Kim , Michael Hughes , Erik Sudderth . Abstract : We introduce the nonparametric metadata dependent relational ( NMDR ) model , a Bayesian nonparametric extension of previous stochastic block models . The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities . Learned regression models allow these memberships to depend on , and be predicted from , arbitrary node metadata . We develop efficient MCMC algorithms for learning NMDR models from ( partially observed ) node relationships . Retrospective MCMC methods allow our sampler to work directly with the infinite stick - breaking representation of the NMDR , avoiding the need for finite truncations . Our results demonstrate recovery of useful latent communities from real - world social and ecological networks , and the usefulness of metadata in link prediction tasks . Deep Lambertian Networks . Yichuan Tang , Ruslan Salakhutdinov , Geoffrey Hinton . Abstract : Visual perception is a challenging problem in part due to illumination variations . A possible solution is to first estimate an illumination invariant representation before using it for recognition . The object albedo and surface normals are examples of such representation . In this paper , we introduce a multilayer generative model where the latent variables include the albedo , surface normals , and the light source . Combining Deep Belief Nets with the Lambertian reflectance assumption , our model can learn good priors over the albedo from 2D images . Illumination variations can be explained by changing only the lighting latent variable in our model . By transferring learned knowledge from similar objects , albedo and surface normals estimation from a single image is possible in our model . Experiments demonstrate that our model is able to generalize as well as improve over standard baselines in one - shot face recognition . Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients . Iftekhar Naim , Daniel Gildea . Abstract : The speed of convergence of the Expectation Maximization ( EM ) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components . In this paper , we study the impact of mixing coefficients on the convergence of EM . We show that when the mixture components exhibit some overlap , the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases . We propose a deterministic anti - annealing algorithm , that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients . The proposed algorithm is compared against other standard optimization techniques like LBFGS , Conjugate Gradient , and the traditional EM algorithm . Finally , we propose a similar deterministic anti - annealing based algorithm for the Dirichlet process mixture model fitting and demonstrate its advantages over the conventional variational Bayesian approach . A Joint Model of Language and Perception for Grounded Attribute Learning . Cynthia Matuszek , Nicholas FitzGerald , Luke Zettlemoyer , Liefeng Bo , Dieter Fox . - Accepted . Abstract : As robots become more ubiquitous and capable , it becomes ever more important to enable untrained users to easily interact with them . Recently , this has led to study of the language grounding problem , where the goal is to extract representations of the meanings of natural language tied to perception and actuation in the physical world . In this paper , we present an approach for joint learning of language and perception models for grounded attribute induction . Our perception model includes attribute classifiers , for example to detect object color and shape , and the language model is based on a probabilistic categorial grammar that enables the construction of rich , compositional meaning representations . The approach is evaluated on the task of interpreting sentences that describe sets of objects in a physical workspace . We demonstrate accurate task performance and effective latent - variable concept induction in physical grounded scenes . Learning Parameterized Skills . Bruno Da Silva , George Konidaris , Andrew Barto . Abstract : We introduce a method for constructing a single skill capable of solving any tasks drawn from a distribution of parameterized reinforcement learning problems . The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the geometry of the lower - dimensional manifold on which the skill policies lie . This manifold models how policy parameters change as the skill parameters are varied . We then apply non - linear regression to construct a parameterized skill by predicting policy parameters from task parameters . We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at any target location around it . Safe Exploration in Markov Decision Processes . Teodor Mihai Moldovan , Pieter Abbeel . Abstract : In unknown or partially unknown environments exploration is necessary to learn how to perform well . Existing reinforcement learning algorithms provide strong exploration guarantees , but they tend to rely on an ergodicity assumption . While ergodicity is a more generally applicable property , it is simplest to state for discrete state spaces , and its essence is that any state is reachable from any other state within some finite amount of time with non - zero probability by following a suitable policy . This assumption allows for exploration algorithms that operate by favoring visiting previously unvisited ( or rarely visited ) states . For most physical systems this assumption is completely impractical as the systems would break before any reasonable exploration has taken place , i.e. , most physical systems do n't satisfy the ergodicity assumption . In this paper we address the need for safe exploration methods in Markov decision processes . We first propose a general formulation of safety that can be thought of as forcing ergodicity . We show that imposing the safety constraint exactly is NP - hard . We present an efficient approximation algorithm for guaranteed safe , but potentially suboptimal , exploration . At the core of our approach is an optimization formulation in which the constraints restrict attention to guaranteed safe policies . The objective favors exploration policies . Our framework is compatible with the majority of previously proposed exploration methods , which rely on an exploration bonus , which we can replicate in the objective . Our experiments show that our method is able to safely explore state - spaces in which classical exploration methods get stuck . Improved Estimation in Time Varying Models . Doina Precup , Philip Bachman . Abstract : Locally adapted parametrizations of a model ( such as locally weighted regression ) are expressive but often suffer from high variance . We describe an approach for reducing the variance , based on the idea of estimating simultaneously a transformed space for the model , as well as locally adapted parameterizations in this new space . We present a new problem formulation that captures this idea and illustrate it in the important context of time - varying models . We develop an algorithm for learning a set of bases for approximating a time - varying sparse network ; each learned basis constitutes an archetypal sparse network structure . We also provide an extension for learning task - specific bases . We present empirical results on synthetic data sets , as well as on a BCI EEG classification task . Poisoning Attacks against Support Vector Machines . Battista Biggio , Blaine Nelson , Pavel Laskov . Abstract : We investigate a family of poisoning attacks against Support Vector Machines ( SVM ) . Such attacks amount to injecting specially crafted training data so as to increase the test error of the trained SVM . Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well - behaved distribution . However , this iassumption does not generally hold in security - sensitive settings . As we demonstrate in this contribution , an intelligent adversary can to some extent predict the change of the SVM decision function in response to malicious input and use this ability to construct malicious data points . The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM 's optimal solution . The gradient ascent method can be easily kernelized and enables the attack to be constructed in the input space even for non - linear kernels . We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non - convex validation error surface and inflicts a significant damage on the test error of the trained classifier . Regularizers versus Losses for Nonlinear Dimensionality Reduction : A Factored View with New Convex Relaxations . James Neufeld , Yaoliang Yu , Xinhua Zhang , Ryan Kiros , Dale Schuurmans . Abstract : We demonstrate that almost all non - parametric dimensionality reduction methods can be expressed by a simple procedure : regularized loss minimization plus singular value truncation . By distinguishing the role of the loss and regularizer in such a process , we recover a factored perspective that reveals some gaps in the current literature . Beyond identifying a useful new loss for manifold unfolding , a key contribution is to derive new convex regularizers that combine distance maximization with rank reduction . These regularizers can be applied to any loss . Utilizing Static Analysis and Code Generation to Accelerate Neural Networks . Lawrence McAfee , Kunle Olukotun . Abstract : As datasets continue to grow , neural network ( NN ) applications are becoming increasingly limited by both the amount of available computational power and the ease of developing high - performance applications . Researchers often must have expert systems knowledge to make their algorithms run efficiently . Although available computing power approximately doubles every two years , algorithm efficiency is not able to keep pace due to the use of general purpose compilers , which are not able to fully optimize specialized application domains . Within the domain of NNs , we have the added knowledge that network architecture remains constant during training , meaning the architecture 's data structure can be statically optimized by a compiler . In this paper , we present SONNC , a compiler for NNs that utilizes static analysis to generate optimized parallel code . We show that SONNC 's use of static optimizations make it able to outperform optimized MATLAB code by up to 242X. Additionally , we show that use of SONNC significantly reduces code complexity when using structurally sparse networks . Similarity Learning for Provably Accurate Sparse Linear Classification . Aur\\u00e9lien Bellet , Amaury Habrard , Marc Sebban . Abstract : In recent years , the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions . Most of the state of the art focus on learning Mahalanobis distances ( requiring to fulfill a constraint of positive semi - definiteness ) for use in a local k - NN algorithm . However , no theoretical link is established between the learned metrics and their performance in classification . In this paper , we make use of the formal framework of good similarities introduced by Balcan et al . to propose an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space , tailored to a use in a global linear classifier . We show that our approach has the property of stability , allowing us to derive a generalization bound on the classification error . Experiments performed on various datasets confirm the effectiveness of our approach compared to state - of - the - art methods and provide evidence that ( i ) it is fast , ( ii ) robust to overfitting and ( iii ) produces very sparse models . Variational Inference in Non - negative Factorial Hidden Markov Models for Efficient Audio Source Separation . Gautham Mysore , Maneesh Sahani . Abstract : The last decade has seen substantial work on the use of non - negative matrix factorization ( NMF ) and its probabilistic counterparts for audio source separation . Although able to capture audio spectral structure well , these models neglect the non - stationarity and temporal dynamics that are important properties of audio . The recently proposed non - negative factorial hidden Markov model ( N - FHMM ) introduces a temporal dimension and improves source separation performance . However , the factorial nature of this model makes the complexity of inference exponential in the number of sound sources . Here , we present a Bayesian variant of the N - FHMM suited to an efficient variational inference algorithm , whose complexity is linear in the number of sound sources . Our algorithm performs comparably to exact inference in the original NFHMM but is significantly faster . In typical configurations of the N - FHMM , our method achieves around a 30x increase in speed . Conditional Likelihood Maximization : A Unifying Framework for Information Theoretic Feature Selection . Gavin Brown , Adam Pocock , Ming - Jie Zhao , Mikel Lujan . Abstract : We present a unifying framework for information theoretic feature selection , bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation . This is in response to the question : \\\" what are the implicit statistical assumptions of feature selection criteria based on mutual information ? \\\" While many hand - designed heuristic criteria try to optimize a definition of feature ' relevancy ' and ' redundancy ' , our approach leads to a probabilistic framework which naturally incorporates these concepts . As a result we unify the numerous criteria published over the last two decades , and show them to be low - order approximations to the exact ( but intractable ) optimization problem . The primary contribution is to show that common heuristics for information based feature selection ( including Markov Blanket algorithms as a special case ) are approximate iterative maximizers of the conditional likelihood . A large empirical study provides strong evidence to favor certain classes of criteria , in particular those that balance the relative size of the relevancy / redundancy terms . Overall we conclude that the JMI criterion ( Yang and Moody , 1999 ; Meyer et al . , 2008 ) provides the best tradeoff in terms of accuracy , stability , and flexibility with small data samples . Communications Inspired Linear Discriminant Analysis . Minhua Chen , William Carson , Miguel Rodrigues , Lawrence Carin , Robert Calderbank . - Accepted . Abstract : We study the problem of supervised linear dimensionality reduction , taking an information - theoretic viewpoint . The linear projection matrix is designed by maximizing the mutual information between the projected signal and the class label ( based on a Shannon entropy measure ) . By harnessing a recent theoretical result on the gradient of mutual information , the above optimization problem can be solved directly using gradient descent , without requiring simplification of the objective function . Relative to these alternative approaches , the proposed method achieves promising results on real datasets . Sparse stochastic inference for latent Dirichlet allocation . David Mimno , Matt Hoffman , David Blei . Abstract : We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference . The resulting method scales to a corpus of 1.2 million books comprising 33 billion words with thousands of topics on one CPU . This approach reduces the bias of variational inference and generalizes to many Bayesian hidden - variable models . Stochastic Smoothing for Nonsmooth Minimizations : Accelerating SGD by Exploiting Structure . Hua Ouyang , Alexander Gray . Abstract : In this work we consider the stochastic minimization of nonsmooth convex loss functions , a central problem in machine learning . We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent ( ANSGD ) , which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs . It is the first stochastic algorithm that can achieve the optimal O(1/t ) rate for minimizing nonsmooth loss functions . The fast rates are confirmed by empirical comparisons , in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD . A Convex Feature Learning Formulation for Latent Task Structure Discovery . Pratik Jawanpuria , J. Saketha Nath . Abstract : This paper considers the multi - task learning problem and in the setting where some relevant features could be shared across few related tasks . Most of the existing methods assume the extent to which the given tasks are related or share a common feature space to be known apriori . In real - world applications however , it is desirable to automatically discover the groups of related tasks that share a feature space . In this paper we aim at searching the exponentially large space of all possible groups of tasks that may share a feature space . The main contribution is a convex formulation that employs a graph - based regularizer and simultaneously discovers few groups of related tasks , having close - by task parameters , as well as the feature space shared within each group . An efficient active set algorithm that exploits this simplification and performs a clever search in the exponentially large space is presented . The algorithm is guaranteed to solve the proposed formulation ( within some precision ) in a time polynomial in the number of groups of related tasks discovered . Empirical results on benchmark datasets show that the proposed formulation achieves good generalization and outperforms state - of - the - art multi - task learning algorithms in some cases . Efficient Decomposed Learning for Structured Prediction . Rajhans Samdani , Dan Roth . Abstract : Structured prediction is the cornerstone of several machine learning applications . Unfortunately , in structured prediction settings with expressive inter - variable interactions , inference and hence exact learning is often intractable . We present a new way , Decomposed Learning ( DecL ) , for performing efficient learning over structured output spaces . In DecL , we restrict the inference step to a limited part of the output space . We use characterizations based on the structure , target parameters , and gold labels to guarantee that DecL with limited inference is equivalent to exact learning . We also show that in real world settings , where our theoretical assumptions may not hold exactly , DecL - based algorithms are significantly more efficient and provide accuracies close to exact learning . Path Integral Policy Improvement with Covariance Matrix Adaptation . Freek Stulp , Olivier Sigaud . Abstract : There has been a recent focus in reinforcement learning ( RL ) on addressing continuous state and action problems by optimizing parameterized policies . PI2 is a recent example of this approach . It combines a derivation from first principles of stochastic optimal control with tools from statistical estimation theory . In this paper , we consider PI2 as a member of the wider family of methods which share the concept of probability - weighted averaging to iteratively update parameters to optimize a cost function . We compare PI2 to other members of the same family - Cross - Entropy Methods and CMA - ES - at the conceptual level and in terms of performance . The comparison suggests the derivation of a novel algorithm which we call PI2-CMA for \\\" Path Integral Policy Improvement with Covariance Matrix Adaptation \\\" . PI2-CMA 's main advantage is that it determines the magnitude of the exploration noise automatically . Optimizing F - measure : A Tale of Two Approaches . Ye Nan , Kian Ming Chai , Wee Sun Lee , Hai Leong Chieu . Abstract : F - measures are popular performance metrics , particularly for tasks with imbalanced data sets . Algorithms for learning to maximize F - measures follow two approaches : the empirical utility maximization ( EUM ) approach learns a classifier having optimal performance on training data , while the decision - theoretic approach learns a probabilistic model and then predict labels with maximum expected F - measure . In this paper , we investigate the theoretical justifications and connections for these two approaches , and we study the conditions under which one approach is preferable to the other using synthetic and real datasets . Given accurate models , our results suggest that the two approaches are asymptotically equivalent given large training and test sets . Nevertheless , the EUM approach appears to be more robust against model misspecification , and given a good model , the decision - theoretic approach appears to be better for handling rare classes and for a common domain adaptation scenario . Efficient Euclidean Projections onto the Intersection of Norm Balls . Adams Wei Yu , Hao Su , Li Fei - Fei . Abstract : Using sparse - inducing norms to learn robust models has received increasing attention from many fields for its attractive properties . Projection - based methods have been widely applied to learning tasks constrained by such norms . We prove that the projection can be reduced to finding the root of an auxiliary function which is piecewise smooth and monotonic . Hence , a bisection algorithm is sufficient to solve the problem . Empirical study reveals that our method achieves significantly better performance than classical methods in terms of running time and memory usage . We further show that embedded with our efficient projection operator , projection - based algorithms can solve regression problems with composite norm constraints more efficiently than other methods and give superior accuracy . Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization . Alexander Rakhlin , Ohad Shamir , Karthik Sridharan . Abstract : Stochastic gradient descent ( SGD ) is a simple and popular method to solve stochastic optimization problems which arise in machine learning . For strongly convex problems , its convergence rate was known to be O(log ( T)/T ) , by running SGD for T iterations and returning the average point . However , recent results showed that using a different algorithm , one can get an optimal O1/T ) rate . This might lead one to believe that standard SGD is suboptimal , and maybe should even be replaced as a method of choice . In this paper , we investigate the optimality of SGD in a stochastic setting . We show that for smooth problems , the algorithm attains the optimal O(1/T ) rate . However , for non - smooth problems , the convergence rate with averaging might really be \\u03a9(log ( T)/T ) , and this is not just an artifact of the analysis . On the flip side , we show that a simple modification of the averaging step suffices to recover the O(1/T ) rate , and no other change of the algorithm is necessary . We also present experimental results which support our findings , and point out open problems . Clustering using Max - norm Constrained Optimization . Ali Jalali , Nathan Srebro . Abstract : We suggest using the max - norm as a convex surrogate constraint for clustering . We show how this yields a better exact cluster recovery guarantee than previously suggested nuclear - norm relaxation , and study the effectiveness of our method , and other related convex relaxations , compared to other approaches . Submodular Inference of Diffusion Networks from Multiple Trees . Manuel Gomez Rodriguez , Bernhard Sch\\u00f6lkopf . Abstract : Diffusion and propagation of information , influence and diseases take place over increasingly larger networks . We observe when a node copies information , makes a decision or becomes infected but networks are often hidden or unobserved . Since networks are highly dynamic , changing and growing rapidly , we only observe a relatively small set of cascades before a network changes significantly . Scalable network inference based on a small cascade set is then necessary for understanding the rapidly evolving dynamics that govern diffusion . In this article , we develop a scalable approximation algorithm with provable near - optimal performance which achieves a high accuracy in such scenario , solving an open problem first introduced by Gomez - Rodriguez et al ( 2010 ) . Experiments on synthetic and real diffusion data show that our algorithm in practice achieves an optimal trade - off between accuracy and running time . Approximate Dynamic Programming By Minimizing Distributionally Robust Bounds . Marek Petrik . Abstract : Approximate dynamic programming is a popular method for solving large Markov decision processes . This paper describes a new class of approximate dynamic programming ( ADP ) methods - distributionally robust ADP - that address the curse of dimensionality by minimizing a pessimistic bound on the policy loss . This approach turns ADP into an optimization problem , for which we derive new mathematical program formulations and analyze itsp properties . DRADP improves on the theoretical guarantees of existing ADP methods - it guarantees convergence and L_1 norm - based error bounds . The empirical evaluation of DRADP shows that the theoretical guarantees translate well into good performance on benchmark problems . Modelling transition dynamics in MDPs with RKHS embeddings . Steffen Grunewalder , Guy Lever , Luca Baldassarre , Massi Pontil , Arthur Gretton . - Accepted . Abstract : We propose a new , nonparametric approach to learning and representing transition dynamics in Markov decision processes ( MDPs ) , which can be combined easily with dynamic programming methods for policy optimisation and value estimation . This approach makes use of a recently developed representation of conditional distributions as embeddings in a reproducing kernel Hilbert space ( RKHS ) . Such representations bypass the need for estimating transition probabilities or densities , and apply to any domain on which kernels can be defined . This avoids the need to calculate intractable integrals , since expectations are represented as RKHS inner products whose computation has linear complexity in the number of points used to represent the embedding . In experiments , we investigate a learning task in a typical classical control setting ( the under - actuated pendulum ) , and on a navigation problem where only images from a sensor are observed . For policy optimisation we compare with least - squares policy iteration where a Gaussian process is used for value function estimation . For value estimation we also compare to the recent NPDP method . Our approach achieves better performance in all experiments . Approximate Principal Direction Trees . Mark McCartin - Lim , Andrew McGregor , Rui Wang . Abstract : We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree ( APD tree ) that adapts to the intrinsic dimension of the data . Our algorithm ensures vector - quantization accuracy similar to that of computationally - expensive PCA trees with similar time - complexity to that of lower - accuracy RP trees . APD trees use a small number of power - method iterations to find splitting planes for recursively partitioning the data . As such they provide a natural trade - off between the running - time and accuracy achieved by RP and PCA trees . Our theoretical results establish a ) strong performance guarantees regardless of the convergence rate of the power - method and b ) that O(log d ) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d . We demonstrate this trade - off and the efficacy of our data structure on both the CPU and GPU . Unachievable Region in Precision - Recall Space and Its Effect on Empirical Evaluation . Kendrick Boyd , Jesse Davis , David Page , Vitor Santos Costa . Abstract : Precision - recall ( PR ) curves and the areas under them are widely used to summarize machine learning results , especially for data sets exhibiting class skew . They are often used analogously to ROC curves and the area under ROC curves . It is already known that PR curves vary as class skew varies . What was not recognized before this paper is that there is a region of PR space that is completely unachievable , and the size of this region varies only with the skew . This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning . Randomized Smoothing for ( Parallel ) Stochastic Optimization . John Duchi , Martin Wainwright , Peter Bartlett . Abstract : By combining randomized smoothing techniques with accelerated gradient methods , we obtain convergence rates for stochastic optimization procedures , both in expectation and with high probability , that have optimal dependence on the variance of the gradient estimates . To the best of our knowledge , these are the first variance - based convergence guarantees for non - smooth optimization . A combination of our techniques with recent work on decentralized optimization yields order - optimal parallel stochastic optimization algorithms . We give applications of our results to several statistical machine learning problems , providing experimental results demonstrating the effectiveness of our algorithms . Marginalized Denoising Autoencoders for Domain Adaptation . Minmin Chen , Zhixiang Xu , Kilian Weinberger , Fei Sha . Abstract : Glorot et al . ( 2011 ) have successfully used Stacked Denoising Autoencoders ( SDAs ) ( Vincent et al . , 2008 ) to learn new representations for domain adaptation , resulting in record accuracy levels on well - known benchmark datasets for sentiment analysis . The representations are learned by reconstructing input data from partial corruption . In this paper , we introduce a variation , marginalized SDA ( mSDA ) . In contrast to the original SDA , mSDA requires no training through back - propagation as we explicitly marginalize out the feature corruption and solve for the parameters in closed form . mSDA learns representations that lead to comparable accuracy levels , but can be implemented in only 20 lines of MATLAB and reduces the computation time on large data sets from several days to mere minutes . Copula - based Kernel Dependency Measures . Barnabas Poczos , Zoubin Ghahramani , Jeff Schneider . Abstract : The paper presents a new copula based method for measuring dependence between random variables . Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution . We prove that this approach has several advantageous properties . Similarly to Shannon mutual information , the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables . This is important in many applications , for example in feature selection . The estimator is consistent , robust to outliers , and uses rank statistics only . We derive upper bounds on the convergence rate and propose independence tests too . We illustrate the theoretical contributions through a series of experiments in feature selection and low - dimensional embedding of distributions using real and toy datasets . Group Sparse Additive Models . Junming Yin , Xi Chen , eric xing . Abstract : We consider the problem of sparse variable selection in nonparametric additive models , with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly . Previous works either study the group sparsity in the parametric setting ( e.g. , group lasso ) , or address the variable selection problem in the nonparametric setting without exploiting the structural information ( e.g. , sparse additive models ( SpAM ) ) . In this paper , we present a new method , called group sparse additive models ( GroupSpAM ) , which can handle group sparsity in nonparametric additive models . We generalize the l1/l2 norm to Hilbert spaces as the sparsity - inducing penalty in GroupSpAM . Moreover , we derive a novel thresholding condition for identifying the functional sparsity at the group level , and propose an efficient block coordinate descent algorithm for constructing the estimate . We demonstrate by simulation that GroupSpAM substantially outperforms competing methods in terms of support recovery and prediction accuracy in additive models , and also conduct a comparative experiment on a real breast cancer dataset . Policy Gradients with Variance Related Risk Criteria . Dotan Di Castro , Aviv Tamar , Shie Mannor . Abstract : Managing risk in dynamic decision problems is of cardinal importance in many fields such as finance and process control . The most common approach to defining risk is through various variance related criteria such as the Sharpe Ratio or the standard deviation adjusted reward . It is known that optimizing many of the variance related risk criteria is NP - hard . In this paper we devise a framework for local policy gradient style algorithms for reinforcement learning for variance related criteria . Our starting point is a new formula for the variance of the cost - to - go in episodic tasks . Using this formula we develop policy gradient algorithms for criteria that involve both the expected cost and the variance of the cost . We prove the convergence of these algorithms to local minima and demonstrate their applicability in a portfolio planning problem . Efficient Structured Prediction with Latent Variables for General Graphical Models . Alexander Schwing , Tamir Hazan , Marc Pollefeys , Raquel Urtasun . Abstract : In this paper we propose a unified framework for structured prediction with hidden variables which includes hidden conditional random fields and latent structural support vector machines as special cases . We describe an approximation for this general structured prediction formulation using duality , which is based on a local entropy approximation . On the Partition Function and Random Maximum A - Posteriori Perturbations . Tamir Hazan , Tommi Jaakkola . Abstract : n this paper we relate the partition function to the max - statistics of random variables . In particular , we provide a novel framework for approximating and bounding the partition function using MAP inference on randomly perturbed models . As a result , we can directly use efficient MAP solvers such as graph - cuts to evaluate the corresponding partition function . We show that our method excels in the typical \\\" high signal - high coupling \\\" regime that results in ragged energy landscapes difficult for alternative approaches . Infinite Tucker Decomposition : Nonparametric Bayesian Models for Multiway Data Analysis . Zenglin Xu , Feng Yan , Alan Qi . Abstract : Tensor decomposition is a powerful computational tool for multiway data analysis . Many popular tensor decomposition approaches - such as the Tucker decomposition and CANDECOMP / PARAFAC ( CP)-amount to multi - linear factorization . They are insufficient to model ( i ) complex interactions between data entities , ( ii ) various data types ( eg missing data and binary data ) , and ( iii ) noisy observations and outliers . To address these issues , we propose tensor - variate latent nonparametric Bayesian models , coupled with efficient inference methods , for multiway data analysis . We name these models InfTucker . Using these InfTucker , we conduct Tucker decomposition in an infinite feature space . Unlike classical tensor decomposition models , our new approaches handle both continuous and binary data in a probabilistic framework . Unlike previous Bayesian models on matrices and tensors , our models are based on latent Gaussian or t processes with nonlinear covariance functions . To efficiently learn the InfTucker from data , we develop a variational inference technique on tensors . Compared with classical implementation , the new technique reduces both time and space complexities by several orders of magnitude . Our experimental results on chemometrics and social network datasets demonstrate that our new models achieved significantly higher prediction accuracy than the most state - of - art tensor decomposition approaches . Inferring Latent Structure From Mixed Real and Categorical Relational Data . Esther Salazar , Lawrence Carin . Abstract : We consider analysis of relational data ( a matrix ) , in which the rows correspond to subjects ( e.g. , people ) and the columns correspond to attributes . The elements of the matrix may be a mix of real and categorical . Each subject and attribute is characterized by a latent binary feature vector , and an inferred matrix maps each row - column pair of binary feature vectors to an observed matrix element . The latent binary features of the rows are modeled via a multivariate Gaussian distribution with low - rank covariance matrix , and the Gaussian random variables are mapped to latent binary features via a probit link . The same type construction is applied jointly to the columns . The model infers latent , low - dimensional binary features associated with each row and each column , as well correlation structure between all rows and between all columns . The Bayesian construction is successfully applied to real - world data , demonstrating an ability to infer meaningful low - dimensional structure from high - dimensional relational data . Bayesian Conditional Cointegration . Chris Bracegirdle , David Barber . Abstract : Cointegration is an important topic for time - series , and describes a relationship between two series in which a linear combination is stationary . Classically , the test for cointegration is based on a two stage process in which first the linear relation between the series is estimated by Ordinary Least Squares . Subsequently a unit root test is performed on the residuals . A well - known deficiency of this classical approach is that is can lead to erroneous conclusions about the presence of cointegration . As an alternative , we present a coherent framework for estimating whether cointegration exists using Bayesian inference which is empirically superior to the classical approach . Finally , we apply our technique to model segmented cointegration in which cointegration may exist only for limited time . In contrast to previous approaches our model makes no restriction on the number of possible cointegration segments . Online Alternating Direction Method . Huahua Wang , Arindam Banerjee . Abstract : Online optimization has emerged as powerful tool in large scale optimization . In this paper , we introduce efficient online algorithms based on the alternating directions method ( ADM ) . We introduce a new proof technique for ADM in the batch setting , which yields a linear rate of convergence of ADM and forms the basis of regret analysis in the online setting . We consider two scenarios in the online setting , based on whether the solution needs to lie in the feasible set or not . In both settings , we establish regret bounds for both the objective function as well as constraint violation for general and strongly convex functions . Preliminary results are presented to illustrate the performance of the proposed algorithms . On the Sample Complexity of Reinforcement Learning with a Generative Model . Mohammad Gheshlaghi Azar , Remi Munos , Bert Kappen . Abstract : We consider the problem of learning the optimal action - value function in the discounted - reward Markov decision processes ( MDPs ) . We also prove a matching lower bound of \\u0398 ( N log ( N / \\u03b4)/ ( ( 1-\\u03b3)^3\\u03b5^2 ) ) on the sample complexity of estimating the optimal action - value function by every RL algorithm . To the best of our knowledge , this is the first matching result on the sample complexity of estimating the optimal ( action-)value function in which the upper bound matches the lower bound of RL in terms of N , \\u03b5 , \\u03b4 and 1-\\u03b3 . Also , both our lower bound and our upper bound significantly improve on the state - of - the - art in terms of 1/(1-\\u03b3 ) . Convergence Rates for Differentially Private Statistical Estimation . Kamalika Chaudhuri , Daniel Hsu . Abstract : Differential privacy is a cryptographically - motivated privacy definition which has gained significant attention over the past few years . Differentially private solutions enforce privacy by adding random noise to the data or a function computed on the data , and the challenge in designing such algorithms is to optimize the privacy - accuracy - sample size tradeoff . This work studies differentially - private statistical estimation , and shows upper and lower bounds on the convergence rates of differentially private approximations to statistical estimators . Our results reveal a connection between differential privacy and the notion of B - robustness in robust statistics , by showing that unless an estimator is B - robust , we can not approximate it well with differential privacy over a large class of distributions . We then provide an upper bound on the convergence rate of a differentially private approximation to a B - robust estimator with a bounded range . We show that the bounded range condition is necessary if we wish to ensure a strict form of differential privacy . Learning Task Grouping and Overlap in Multi - task Learning . Abhishek Kumar , Hal Daume III . Abstract : In the paradigm of multi - task learning , mul- tiple related prediction tasks are learned jointly , sharing information across the tasks . We propose a framework for multi - task learn- ing that enables one to selectively share the information across the tasks . We assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks . The coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these . Our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases . Experimental results on four datasets show that our approach outperforms competing methods . High Dimensional Semiparametric Gaussian Copula Graphical Models . Han Liu , Fang Han , Ming Yuan , John Lafferty , Larry Wasserman . Abstract : In this paper , we propose a semiparametric approach named nonparanormal SKEPTIC for efficiently and robustly estimating high dimensional undirected graphical models . To achieve modeling flexibility , we consider Gaussian Copula graphical models ( or the nonparanormal ) as proposed by Liu et al . ( 2009 ) . To achieve estimation robustness , we exploit nonparametric rank - based correlation coefficient estimators , including Spearman 's rho and Kendall 's tau . In high dimensional settings , we prove that the nonparanormal SKEPTIC achieves the optimal parametric rate of convergence in both graph and parameter estimation . This celebrating result suggests that the Gaussian copula graphical models can be used as a safe replacement of the popular Gaussian graphical models , even when the data are truly Gaussian . Besides theoretical analysis , we also conduct thorough numerical simulations to compare different estimators for their graph recovery performance under both ideal and noisy settings . The proposed methods are then applied on a large - scale genomic dataset to illustrate their empirical usefulness . Discovering Support and Affiliated Features from Very High Dimensions . Yiteng Zhai , Mingkui Tan , Ivor Tsang , Yew Soon Ong . Abstract : In this paper , a novel learning paradigm is presented to automatically identify groups of informative and correlated features from very high dimensions . Specifically , we explicitly incorporate correlation measures as constraints and then propose an efficient embedded feature selection method using recently developed cutting plane strategy . The benefits of the proposed algorithm are two - folds . First , it can identify the optimal discriminative and uncorrelated feature subset to the output labels , denoted here as Support Features , which brings about significant improvements in prediction performance over other state of the art feature selection methods considered in the paper . Second , during the learning process , the underlying group structures of correlated features associated with each support feature , denoted as Affiliated Features , can also be discovered without any additional cost . These affiliated features serve to improve the interpretations on the learning tasks . Extensive empirical studies on both synthetic and very high dimensional real - world datasets verify the validity and efficiency of the proposed method . Online Bandit Learning against an Adaptive Adversary : from Regret to Policy Regret . Ofer Dekel , Ambuj Tewari , Raman Arora . Abstract : Online learning algorithms are designed to learn even when their input is generated by an adversary . The widely - accepted formal definition of an online algorithm 's ability to learn is the game - theoretic notion of regret . We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm 's actions . We define the alternative notion of pol- icy regret , which attempts to provide a more meaningful way to measure an online algorithm 's performance against adaptive adversaries . Focusing on the online bandit setting , we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory . On the other hand , if the adversary 's memory is unbounded , we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound . We extend this result to other variants of regret , such as switching regret , internal regret , and swap regret . Statistical linear estimation with penalized estimators : an application to reinforcement learning . Bernardo Avila Pires , Csaba Szepesvari . Abstract : Motivated by value function estimation in reinforcement learning , we study statistical linear inverse problems , i.e. , problems where the coefficients of a linear system to be solved are observed in noise . We consider penalized estimators , where performance is evaluated using a matrix - weighted two - norm of the defect of the estimator measured with respect to the true , unknown coefficients . Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared . We propose simple , yet novel and theoretically well - founded data - dependent choices for the regularization parameters for both cases that avoid data - splitting . A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients , thus allowing the complete separation of the analysis of the stochastic properties of these errors . We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning . Large Scale Variational Bayesian Inference for Structured Scale Mixture Models . Young Jun Ko , Matthias Seeger . Abstract : Natural image statistics exhibit hierarchical dependencies across multiple scales . Representing such prior knowledge in non - factorial latent tree models can boost performance of image denoising , inpainting , deconvolution or reconstruction substantially , beyond standard factorial \\\" sparse \\\" methodology . We derive a large scale approximate Bayesian inference algorithm for linear models with non - factorial ( latent tree - structured ) scale mixture priors . Experimental results on a range of denoising and inpainting problems demonstrate substantially improved performance compared to MAP estimation or to inference with factorial priors . Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring . Sungjin Ahn , Anoop Korattikara , Max Welling . Abstract : In this paper we address the following question : \\\" Can we approximately sample from a Bayesian posterior distribution if we are only allowed to touch a small mini - batch of data - items for every sample we generate ? \\\" An algorithm based on the Langevin equation with stochastic gradients ( SGLD ) was previously proposed to solve this , but its mixing rate was slow . By leveraging the Bayesian Central Limit Theorem , we extend the SGLD algorithm so that at high mixing rates it will sample from a normal approximation of the posterior , while for slow mixing rates it will mimic the behavior of SGLD with a pre - conditioner matrix . As a bonus , the proposed algorithm is reminiscent of Fisher scoring ( with stochastic gradients ) and as such an efficient optimizer during burn - in . An adaptive algorithm for finite stochastic partial monitoring . Gabor Bartok , Navid Zolghadr , Csaba Szepesvari . Abstract : We present a new anytime algorithm that achieves near - optimal regret for any instance of finite stochastic partial monitoring . In particular , the new algorithm achieves the minimax regret , within logarithmic factors , for both \\\" easy \\\" and \\\" hard \\\" problems . For easy problems , it additionally achieves logarithmic individual regret . Most importantly , the algorithm is adaptive in the sense that if the opponent strategy is in an \\\" easy region \\\" of the strategy space then the regret grows as if the problem was easy . The Big Data Bootstrap . Ariel Kleiner , Ameet Talwalkar , Purnamrita Sarkar , Michael Jordan . Abstract : The bootstrap provides a simple and powerful means of assessing the quality of estimators . However , in settings involving large datasets , the computation of bootstrap - based quantities can be prohibitively demanding . As an alternative , we present BLB , a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust , computationally efficient means of assessing the quality of estimators . BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability , statistical efficiency , and favorable theoretical properties of the bootstrap . We provide a theoretical analysis elucidating the properties of BLB , as well as an extensive empirical investigation , including a study of its statistical correctness , its large - scale implementation and performance , selection of hyperparameters , and performance on real data . Predicting Consumer Behavior in Commerce Search . Or Sheffet , Nina Mishra , Samuel Ieong . Abstract : Traditional approaches to ranking in web search follow the paradigm of rank - by - score : a learned function gives each query - URL combination an absolute score and URLs are ranked according to this score . This paradigm ensures that if the score of one URL is better than another then one will always be ranked higher than the other . Scoring contradicts prior work in behavioral economics that showed that users ' preferences between two items depend not only on the items but also on the presented alternatives . Thus , for the same query , users ' preference between items A and B depends on the presence / absence of item C. We propose a new model of ranking , the Random Shopper Model , that allows and explains such behavior . In this model , each feature is viewed as a Markov chain over the items to be ranked , and the goal is to find a weighting of the features that best reflects their importance . We show that our model can be learned under the empirical risk minimization framework , and give an efficient learning algorithm . Experiments on commerce search logs demonstrate that our algorithm outperforms scoring - based approaches including regression and listwise ranking . Conditional mean embeddings as regressors . Steffen Grunewalder , Guy Lever , Arthur Gretton , Luca Baldassarre , Sam Patterson , Massi Pontil . - Accepted . Abstract : We demonstrate an equivalence between reproducing kernel Hilbert space ( RKHS ) embeddings of conditional distributions and vector - valued regressors . This connection introduces a natural regularized loss function which the RKHS embeddings minimise , providing an intuitive understanding of the embeddings and a solid justification for their use . Furthermore , the equivalence allows the application of vector - valued regression methods and results to the problem of learning conditional distributions . Using this link we derive a sparse version of the embedding by considering alternative formulations . These minimax lower rates coincide with upper rates up to a logarithmic factor and show that the embedding method achieves nearly optimal rates . We study our sparse embedding algorithm in a reinforcement learning task where the algorithm shows significant improvement in sparsity over a Cholesky decomposition . A Generative Process for Contractive Auto - Encoders . Salah Rifai , Yann Dauphin , Pascal Vincent , Yoshua Bengio . Abstract : The contractive auto - encoder learns a representation of the input data that captures the local manifold structure around each data point , through the leading singular vectors of the Jacobian of the transformation from input to representation . The corresponding singular values specify how much local variation is plausible in directions associated with the corresponding singular vectors , while remaining in a high - density region of the input space . This paper proposes a procedure for generating samples that are consistent with the local structure captured by a contractive auto - encoder . The associated stochastic process defines a distribution from which one can sample , and which experimentally appears to converge quickly and mix well between modes , compared to Restricted Boltzmann Machines and Deep Belief Networks . The intuitions behind this procedure can also be used to train the second layer of contraction that pools lower - level features and learns to be invariant to the local directions of variation discovered in the first layer . We show that this can help learn and represent invariances present in the data and improve classification error . Local Loss Optimization in Operator Models : A New Insight into Spectral Learning . Borja Balle , Ariadna Quattoni , Xavier Carreras . Abstract : This paper re - visits the spectral method for learning latent variable models defined in terms of observable operators . We give a new perspective on the method , showing that operators can be recovered by minimizing a loss defined on a finite subspace of the domain . A non - convex optimization similar to the spectral method is derived . We also propose a regularized convex relaxation of this optimization . We show that in practice the availabilty of a continuous regularization parameter ( in contrast with the discrete number of states in the original method ) allows a better trade - off between accuracy and model complexity . We also prove that in general , a randomized strategy for choosing the local loss will succeed with high probability . We also prove that in general , a randomized strategy for choosing the local loss will succeed with high probability . Robust PCA in High - dimension : A Deterministic Approach . Jiashi Feng , Huan Xu , Shuicheng Yan . Abstract : We consider principal component analysis for contaminated data - set in the high dimensional regime , where the number of observations is comparable or more than the dimensionality of each observation . More importantly , the proposed method exhibits significantly better computational efficiency , which makes it suitable for large - scale real applications . Complexity Analysis of the Lasso Regularization Path . Julien Mairal , Bin Yu . Abstract : The regularization path of the Lasso can be shown to be piecewise linear , making it possible to \\\" follow \\\" and explicitly compute the entire path . We analyze in this paper this popular strategy , and prove that its worst case complexity is exponential in the number of variables . We complete our theoretical analysis with a practical algorithm to compute these approximate paths . Projection - free Online Learning . Elad Hazan , Satyen Kale . Abstract : We present efficient online learning algorithms that eschew projections in favor of linear optimizations using the Frank - Wolfe technique . Besides the computational advantage , other desirable features of our algorithms are that they are parameter - free in the stochastic case and produce sparse decisions . Machine Learning that Matters . Kiri Wagstaff . Abstract : Much of current machine learning ( ML ) research has lost its connection to problems of import to the larger world of science and society . From this perspective , there exist glaring limitations in the data sets we investigate , the metrics we employ for evaluation , and the degree to which results are communicated back to their originating domains . What changes are needed to how we conduct research to increase the impact that ML has ? We present six Impact Challenges to explicitly focus the field 's energy and attention , and we discuss existing obstacles that must be addressed . We aim to inspire ongoing discussion and focus on ML that matters . Scene parsing with Multiscale Feature Learning , Purity Trees , and Optimal Covers . Cl\\u00e9ment Farabet , Camille Couprie , Laurent Najman , Yann LeCun . Abstract : Scene parsing consists in labeling each pixel in an image with the category of the object it belongs to . We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel . The method alleviates the need for engineered features . In parallel to feature extraction , a tree of segments is computed from a graph of pixel dissimilarities . The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment . A subset of tree nodes that cover the image are then selected so as to maximize the average ' purity ' of the class distributions , hence maximizing the overall likelihood that each segment will contain a single object . Linear Regression with Limited Observation . Elad Hazan , Tomer Koren . Abstract : We consider the most common variants of linear regression , including Ridge , Lasso and Support - vector regression , in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time . We present simple and efficient algorithms for these problems : for Lasso and Ridge regression they need the same number of attributes ( up to constants ) as do full - information algorithms , for reaching a certain accuracy . For Support - vector regression , we require exponentially less attributes compared to the state of the art . By that , we resolve an open problem recently posed by ( Cesa - Bianchi et al . , 2010 ) . Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art . An Online Boosting Algorithm with Theoretical Justifications . Shang - Tse Chen , Hsuan - Tien Lin , Chi - Jen Lu . Abstract : We study the task of online boosting , which aims to combine online weak learners into an online strong learner . While boosting in the batch setting has a sound theoretical foundation , not much theory is known for the online setting . In this paper , we propose an online boosting algorithm and we provide a theoretical guarantee . In addition , we also perform experiments on real - world datasets and the results show that our algorithm compares favorably with existing algorithms . Modeling Temporal Dependencies in High - Dimensional Sequences : Application to Polyphonic Music Generation and Transcription . Nicolas Boulanger - Lewandowski , Yoshua Bengio , Pascal Vincent . Abstract : We investigate the problem of modeling symbolic sequences of polyphonic music in a completely general piano - roll representation . We introduce a probabilistic model based on distribution estimators conditioned on a recurrent neural network that is able to discover temporal dependencies in high - dimensional sequences . Our approach outperforms many traditional models of polyphonic music on a variety of realistic datasets . We show how our musical language model can serve as a symbolic prior to improve the accuracy of polyphonic transcription . Approximate Modified Policy Iteration . Bruno Scherrer , Victor Gabillon , Mohammad Ghavamzadeh , Matthieu Geist . Abstract : Modified policy iteration ( MPI ) is a dynamic programming ( DP ) algorithm that contains the two celebrated policy and value iteration methods . Despite its generality , MPI has not been thoroughly studied , especially its approximation form which is used when the state and/or action spaces are large or infinite . In this paper , we propose three approximate MPI ( AMPI ) algorithms that are extensions of the well - known approximate DP algorithms : fitted - value iteration , fitted - Q iteration , and classification - based policy iteration . We provide an error propagation analysis for AMPI that unifies those for approximate policy and value iteration . We also provide a finite - sample analysis for the classification - based implementation of AMPI ( CBMPI ) , which is more general ( and somehow contains ) than the analysis of the other presented AMPI algorithms . An interesting observation is that the MPI 's parameter allows us to control the balance of errors ( in value function approximation and in estimating the greedy policy ) in the final performance of the CBMPI algorithm . Nonparametric Link Prediction in Dynamic Networks . Purnamrita Sarkar , Deepayan Chakrabarti , Michael Jordan . Abstract : We propose a non - parametric link prediction algorithm for a sequence of graph snapshots over time . The model predicts links based on the features of its endpoints , as well as those of the local neighborhood around the endpoints . This allows for different types of neighborhoods in a graph , each with its own dynamics ( e.g , growing or shrinking communities ) . We prove the consistency of our estimator , and give a fast implementation based on locality - sensitive hashing . Experiments with simulated as well as five real - world dynamic graphs show that we outperform the state of the art , especially when sharp fluctuations or non - linearities are present . Agnostic System Identification for Model - Based Reinforcement Learning . Stephane Ross , Drew Bagnell . Abstract : A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis . To provide good performance guarantees , existing methods must assume that the real system is in the class of models considered during learning . We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class . In particular , we show that any no - regret online learning algorithm can be used to obtain a near - optimal policy , provided some model achieves low training error and access to a good exploration distribution . Our approach applies to both discrete and continuous domains . We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature . \"}",
        "_version_":1692670882082717696,
        "score":24.396542},
      {
        "id":"a2c0d7cd-1251-43c1-94e4-5b0f0dcde216",
        "_src_":"{\"url\": \"http://technokoopa.deviantart.com/art/Dragoon-class-Destroyer-448332152\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701156520.89/warc/CC-MAIN-20160205193916-00243-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Computational Linguistics and Classical Lexicography . Abstract . Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials . As we build a cyberinfrastructure for Classics in the future , we must explore the role that automatic methods can play within it . Great advances have been made in the sciences on which lexicography depends . Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography . Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown . Syntax has been subjected to a profounder analysis . The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information . Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish . ( Advertisement for the Lewis & Short Latin Dictionary , March 1 , 1879 . ) The \\\" scholarship of thirty years ago \\\" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\\u00f6rterbuch published only a decade before . Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide . It has , along with the Thesaurus Linguae Latinae , provided extremely thorough coverage for the texts of the Golden and Silver Age in Latin literature and has driven modern scholarship for the past thirty years . Digital methods also let us deal well with scale . For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond . The Index Thomisticus [ Busa 1974 - 1980 ] alone contains 10.6 million words attributed to Thomas Aquinas and related authors , which is by itself larger than the entire corpus of extant classical Latin . In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \\\" where are we now ? \\\" and \\\" where do we want to be ? \\\" : where are our colleagues already ? Many of the tools we would want in the future are founded on technologies that already exist for English and other languages ; our task in designing a cyberinfrastructure may simply be to transfer and customize them for Classical Studies . Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique . In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways . Where are we now ? In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching . This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) . Here the act of lexicography is still very manual , as each dictionary sense is still heavily curated , but the tedious job of citation collection is not . We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \\\" computational lexicography \\\" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s . This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with . By keeping the underlying corpus up to date , the editors are also able to add new headwords as they appear in the language , and common multi - word expressions and idioms ( such as bear fruit ) can also be uncovered as well . This corpus - based approach has since been augmented in two dimensions . [ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them . While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al . 2004 ] also enable lexicographers to induce information about a word 's grammatical behavior as well . In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years . Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica . Like the OED , Classical lexica generally include a list of citations under each headword , providing testimony by real authors for each sense . Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata . These citations essentially function as an index into the textual collection . If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic . Ver . 1.72 , Plin . Nat . 6.90 , etc . For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL . This is what we might consider a manual form of \\\" lemmatized searching . \\\" The search results are thus significantly diluted by a large number of false positives . The advantage of the Perseus and TLG lemmatized search is that it gives scholars the opportunity to find all the instances of a given word form or lemma in the textual collections they each contain . The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty . What we need is a resource to combine the best of both . Where do we want to be ? The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated . Heavily curated reference works provide great detail for a small set of texts ; our complement is to provide lesser detail for all texts . In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure . [ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research . This will be true of a cyberinfrastructure for any historical culture , and for any future structure that develops for modern scholarly corpora as well . We therefore must concentrate on two problems . First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ? And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ? What we can learn from such a corpus is actually quite significant . With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts . In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections . We can thus not only identify patterns in one hundred and fifty million words of later Latin but also compare which senses of which words appear in the one hundred and fifty thousand words of Thucydides . Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work . The first section ( \\\" Translation equivalents \\\" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use . The main difference between a dynamic lexicon and those print lexica , however , lies in the scope of the examples : while print lexica select one or several highly illustrative examples of usage from a source text , we are in a position to present far more . How do we get there ? We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] . Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and . Each of these technologies has a long history of development both within the Perseus Project and in the natural language processing community at large . In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text . Word Sense Induction . So , for example , the Greek word arch\\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 . Finding all of the translation equivalents for any given word then becomes a task of aligning the source text with its translations , at the level of individual words . The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts . Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated . The workflow for this process is shown in Figure 2 . Since the XML files of both the source text and its translations are marked up with the same reference points , \\\" chapter 1 , section 1 \\\" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) . This results ( for Latin at least ) in aligned chunks of text that are 217 words long . In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] . This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language . Figure 3 shows the result of this word alignment ( here with English as the source language ) . The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) . The original English is you wished me to be safe . As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection . For instance , during lemmatization , the Latin word esse is replaced with the two lemmas from which it can be derived - sum1 ( to be ) and edo1 ( to eat ) . If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them . The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work . In the example above , we have induced from our collection of parallel texts that the headword oratio is primarily used with two senses : speech and prayer . Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from . And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts . Second , our word alignment also maps multi - word expressions , so we can include significant collocations in our lexicon as well . This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) . Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task . [ 8 ] Manually annotated corpora , however , are generally cost - prohibitive to create , and this is especially exacerbated with sense - tagged corpora , for which the human inter - annotator agreement is often low . Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation . This method uses the same techniques we used to create a sense inventory to disambiguate words in context . After we have a list of possible translation equivalents for a word , we can use the surrounding Latin or Greek context as an indicator for which sense is meant in texts where we have no corresponding translation . There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \\\" context \\\" is most appropriate itself . One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al . 1992 ) , with context defined as a sentence - level bag of words ( all of the words in the sentence containing the word to be disambiguated contribute equally to its disambiguation ) . Bayesian classification is most commonly found in spam filtering . By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other . We can also use this principle to disambiguate word senses by building a classifier for every sense and training it on sentences where we do know the correct sense for a word . Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation . For instance , the Latin word spiritus has several senses , including spirit and wind . In our texts , when spiritus is translated as wind , it is accompanied by words like mons ( mountain ) , ala ( wing ) or ventus ( wind ) . When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) . If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin . Word sense disambiguation will be most helpful for the construction of a lexicon when we are attempting to determine the sense for words in context for the large body of later Latin literature for which there exists no English translation . This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well . Parsing . Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference . A verb 's subcategorization frame is the set of possible combinations of surface syntactic arguments it can appear with . In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) . A predicate 's selectional preference specifies the type of argument it generally appears with . The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically . Selectional preference , however , can also be much more detailed , reflecting not only a word class ( such as animate or human ) , but also individual words themselves . [ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure . In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it . A second , more practical option is to assign syntactic structure to a sentence using automatic methods . Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis . We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek . Now in version 1.5 , the Latin Dependency Treebank [ 10 ] is composed of excerpts from eight texts , including Caesar , Cicero , Jerome , Ovid , Petronius , Propertius , Sallust and Vergil . Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\\u010duk 1988 ] . Dependency grammars differ from phrase - structure grammars in that they forego non - terminal phrasal categories and link words themselves to their immediate heads . This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents . A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \\\" that glory would know my old age \\\" - would look like the following : . While this treebank is still in its infancy , we can still use it to train a parser to parse the volumes of unstructured Latin in our collection . Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] . The key to improving this parsing accuracy is to increase the size of the annotated treebank : the better the parser , the more accurate the syntactic information we can extract from our corpus . Beyond the lexicon . These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage . This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry . The technologies that will build this reference work , however , do so by processing a large Greek and Latin textual corpus . The results of this automatic processing go far beyond the construction of a single lexicon . I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use . As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text . Searching by word sense . The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval . By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies . Figure 7 presents a mock - up of what such a service could look like . Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same . So , for example , the diphthong ae is often reduced to e , and prevocalic ti is changed to ci . Even within a given time frame , spelling can vary , especially from poetry to prose . By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era . Additionally , we can automate this process to discover common words with multiple orthographic variations , and include these in our dynamic lexicon as well . Searching by selectional preference . The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \\\" means . \\\" In building the lexicon , we automatically assign an argument structure to all of the verbs . Once this structure is in place , it can stay attached to our texts and thereby be searchable in the future , allowing us to search a text for the subjects and direct objects of any verb . This is a powerful resource that can give us much more information about a text than simple search engines currently allow . Conclusion . In this a dynamic lexicon fills a gap left by traditional reference works . By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not . Even driven by the scholarship of the past thirty years , however , a dynamic lexicon can not yet compete with the fine sense distinctions that traditional dictionaries make , and in this the two works are complementary . Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own . Notes . [ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE . For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) . Works Cited . Andrews 1850 Andrews , E. A. ( ed . ) A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . . New York : Harper & Bros. , 1850 . Bamman and Crane 2007 Bamman , David and Gregory Crane . \\\" The Latin Dependency Treebank in a Cultural Heritage Digital Library \\\" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) . Bamman and Crane 2008 Bamman , David and Gregory Crane . \\\" Building a Dynamic Lexicon from a Digital Library \\\" , Proceedings of the 8th ACM / IEEE - CS Joint Conference on Digital Libraries . Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \\\" An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \\\" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) . Bourne 1916 Bourne , Ella . \\\" The Messianic Prophecy in Vergil 's Fourth Eclogue \\\" , The Classical Journal 11.7 ( 1916 ) . Brants and Franz 2006 Brants , Thorsten and Alex Franz . Web 1 T 5-gram Version 1 . Philadelphia : Linguistic Data Consortium , 2006 . Brown et al . 1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \\\" Word - sense disambiguation using statistical methods \\\" , Proceedings of the 29th Conference of the Association for Computational Linguistics ( 1991 ) . Busa 1974 - 1980 Busa , Roberto . Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI . Stuttgart - Bad Cannstatt : Frommann - Holzboog , 1974 - 1980 . Busa 2004 Busa , Roberto . \\\" Foreword : Perspectives on the Digital Humanities \\\" , Blackwell Companion to Digital Humanities . Oxford : Blackwell , 2004 . Charniak 2000 Charniak , Eugene . \\\" A Maximum - Entropy - Inspired Parser \\\" , Proceedings of NAACL ( 2000 ) . Collins 1999 Collins , Michael . \\\" Head - Driven Statistical Models for Natural Language Parsing \\\" , Ph.D. thesis . Philadelphia : University of Pennsylvania , 1999 . Freund 1840 Freund , Wilhelm ( ed . ) W\\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde . Leipzig : Teubner , 1834 - 1840 . Gale et al . 1992a Gale , William , Kenneth W. Church and David Yarowsky . \\\" Using bilingual materials to develop word sense disambiguation methods \\\" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) . Glare 1982 Glare , P. G. W. ( ed . ) Oxford Latin Dictionary . Oxford : Oxford University Press , 1968 - 1982 . Grozea 2004 Grozea , Christian . \\\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \\\" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Haji\\u010d 1999 Haji\\u010d , Jan. \\\" Building a Syntactically Annotated Corpus : The Prague Dependency Treebank \\\" , Issues of Valency and Meaning . Studies in Honour of Jarmila Panevov\\u00e1 . Prague : Charles University Press , 1999 . Kilgarriff et al . 2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \\\" The Sketch Engine \\\" , Proceedings of EURALEX ( 2004 ) . Klosa et al . 2006 Klosa , Annette , Ulrich Schn\\u00f6rch , and Petra Storjohann . \\\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\\u00fcr deutsche Sprache , Mannheim \\\" , Proceedings of the 12th Euralex International Congress ( 2006 ) . Lesk 1986 Lesk , Michael . \\\" Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \\\" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) . Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . ) A Latin Dictionary . Oxford : Clarendon Press , 1879 . Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . ) A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones . Oxford : Clarendon Press , 1940 . Marcus et al . 1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz . \\\" Building a Large Annotated Corpus of English : The Penn Treebank \\\" , Computational Linguistics 19.2 ( 1993 ) . McCarthy et al . 2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll . \\\" Finding Predominant Senses in Untagged Text \\\" , Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( 2004 ) . McDonald et al . 2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\\u010d . \\\" Non - projective Dependency Parsing using Spanning Tree Algorithms \\\" , Proceedings of HLT / EMNLP ( 2005 ) . Mel'\\u010duk 1988 Mel'\\u010duk , Igor A. Dependency Syntax : Theory and Practice . Albany : State University of New York Press , 1988 . Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . ) Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) . Miller 1995 Miller , George . \\\" Wordnet : A Lexical Database \\\" , Communications of the ACM 38.11 ( 1995 ) . Miller et al . 1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \\\" A Semantic Concordance \\\" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) . Moore 2002 Moore , Robert C. \\\" Fast and Accurate Sentence Alignment of Bilingual Corpora \\\" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) . Niermeyer 1976 Niermeyer , Jan Frederick . Mediae Latinitatis Lexicon Minus . Leiden : Brill , 1976 . Nivre et al . 2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \\\" MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \\\" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) . Och and Ney 2003 Och , Franz Josef and Hermann Ney . \\\" A Systematic Comparison of Various Statistical Alignment Models \\\" , Computational Linguistics 29.1 ( 2003 ) . Pinkster 1990 Pinkster , Harm . Latin Syntax and Semantics . London : Routledge , 1990 . Sch\\u00fctz 1895 Sch\\u00fctz , Ludwig . Thomas - Lexikon . Paderborn : F. Schoningh , 1895 . Sinclair 1987 Sinclair , John M. ( ed . ) Looking Up : an account of the COBUILD project in lexical computing . Collins , 1987 . Singh and Husain 2005 Singh , Anil Kumar and Samar Husain . \\\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \\\" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) . TLL Thesaurus Linguae Latinae , fourth electronic edition . Munich : K. G. Saur , 2006 . Tufis et al . 2004 Tufis , Dan , Radu Ion , and Nancy Ide . \\\" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \\\" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) . \"}",
        "_version_":1692668503849435139,
        "score":23.009478},
      {
        "id":"52d0e7d2-dc63-4ab9-bbc6-120622de4579",
        "_src_":"{\"url\": \"http://m.bulbapedia.bulbagarden.net/wiki/Talk:Kindler_(Trainer_class)\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701149377.17/warc/CC-MAIN-20160205193909-00171-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Figure 3 . 2-D 2-class data , along with first LDA basis vector and first PCA basis vector . The classes would be well separated by a projection onto the first LDA basis vector , but poorly separated by a projection onto the first PCA basis vector . Figure 16 . Accuracies of NLDA1 and NLDA2 with various dimensionality reduced features based on 1-state ( top panel ) and 3-state HMMs ( bottom panel ) . The NLDA1 features without PCA are always 48 dimensions . Figure 17 . Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures . Figure 18 . Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \\\" ( CR ) \\\" and \\\" ( FA ) \\\" indicate the training targets obtained with the constant length ratio and forced alignment respectively . Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition . Stephen A. Zahorian 1 and Hongbing Hu 1 . Introduction . For nearly a century , researchers have investigated and used mathematical techniques for reducing the dimensionality of vector valued data used to characterize categorical data with the goal of preserving \\\" information \\\" or discriminability of the different categories in the reduced dimensionality data . The most established techniques are Principal Components Analysis ( PCA ) and Linear Discriminant Analysis ( LDA ) ( Jolliffe , 1986 ; Wang & Paliwal , 2003 ) . Both PCA and LDA are based on linear , i.e. matrix multiplication , transformations . For the case of PCA , the transformation is based on minimizing mean square error between original data vectors and data vectors that can be estimated from the reduced dimensionality data vectors . For the case of LDA , the transformation is based on minimizing a ratio of \\\" between class variance \\\" to \\\" within class variance \\\" with the goal of reducing data variation in the same class and increasing the separation between classes . There are newer versions of these methods such as Heteroscedastic Discriminant Analysis ( HDA ) ( Kumar & Andreou , 1998 ; Saon et al . , 2000 ) . However , in all cases certain assumptions are made about the statistical properties of the original data ( such as multivariate Gaussian ) ; even more fundamentally , the transformations are restricted to be linear . In this chapter , a class of nonlinear transformations is presented both from a theoretical and experimental point of view . Theoretically , the nonlinear methods have the potential to be more \\\" efficient \\\" than linear methods , that is , give better representations with fewer dimensions . In addition , some examples are shown from experiments with Automatic Speech Recognition ( ASR ) where the nonlinear methods in fact perform better , resulting in higher ASR accuracy than obtained with either the original speech features , or linearly reduced feature sets . Two nonlinear transformation methods , along with several variations , are presented . In one of these methods , referred to as nonlinear PCA ( NLPCA ) , the goal of the nonlinear transformation is to minimize the mean square error between features estimated from reduced dimensionality features and original features . Thus this method is patterned after PCA . In the second method , referred to as nonlinear LDA ( NLDA ) , the goal of the nonlinear transformation is to maximize discriminability of categories of data . Thus the method is patterned after LDA . In all cases , the dimensionality reduction is accomplished with a Neural Network ( NN ) , which internally encodes data with a reduced number of dimensions . The differences in the methods depend on error criteria used to train the network , the architecture of the network , and the extent to which the reduced dimensions are \\\" hidden \\\" in the neural network . The two basic methods and their variations are illustrated experimentally using phonetic classification experiments with the NTIMIT database and phonetic recognition experiments with the TIMIT database . Thus , in one sense , the recognizer is a hybrid neural network / Hidden Markov Model ( NN / HMM ) recognizer . However , the neural network step is used for the task of nonlinear dimensionality reduction and is independent of the HMM . It is shown that the NLDA approach performs better than the NLPCA approach in terms of recognition accuracy . It is also shown that speech recognition accuracy can be as high as or even higher using reduced dimensionality features versus original features , with \\\" properly \\\" trained systems . Background . Both approaches are used . Thus , for good training of model parameters , increasing dimensionality from 40 to 50 , considered a modest increase , could easily increase the need for more data by a factor of 1000 or more . Therefore it seems unlikely that increased database size alone is a good approach to improved ASR accuracies by training with more and more features . In this chapter , some techniques are presented for reducing feature dimensionality while preserving category ( i.e. , phonetic for the case of speech ) discriminability . Since the techniques presented for reducing dimensionality are statistically based , these methods also are subject to \\\" curse of dimensionality \\\" issues . However , since this dimensionality reduction can be done at the very front end of a speech recognition system , with fewer model parameters tuned than in an overall recognition system , the \\\" curse \\\" can be less of a problem . We first review some traditional linear methods for dimensionality reduction before proceeding to the nonlinear transformation , the main subject of this chapter . Principal Components Analysis ( PCA ) . Principal Components Analysis ( PCA ) , also known as the Karhunen - Loeve Transform ( KLT ) , has been known of and in use for nearly a century ( Fodor , 2002 ; Duda et al . , 2001 ) , as a linear method for dimensionality reduction . Operationally , PCA can be described as follows : . Let . X . [ . x . x . x . n . ] T be an n -dimensional ( column ) feature vector , and . Y . [ . y . y . y . m . ] T be an m -dimensional ( column ) feature vector , obtained as the linear transform of X , using the n by m transformation matrix A , i.e. . Y . A . T . X . Let . X . ^ . B . Y be an approximation to . X . Note that . X . Y and . X . ^ can all be viewed as ( column ) vector - valued random variables . The goal of PCA is to determine A and B , such that . E . X . X . ^ . ) is minimized . That is , . X . ^ should approximate X as well as possible , in a mean square error sense . As has been shown in several references ( for example , Duda et al . , 2001 ) , this seemingly intractable problem has a very straightforward solution , provided X is zero mean and multivariate Gaussian . The rows of transformation A T have been shown to be the eigenvectors of the covariance matrix of X , corresponding to the m largest eigenvalues of this matrix . The columns of B are also the same eigenvectors . Thus the \\\" forward \\\" and \\\" reverse \\\" transformations are transposes of each other . The components of Y are uncorrelated . Furthermore the expected value of this normalized mean square error between original and re - estimated X vectors can be shown to equal the ratio of the sum of \\\" unused \\\" eignevalues to the sum of all eigenvalues . The columns of A are called the principal components basis vectors and the components of Y are called the principal components . If the underlying assumption of zero mean multivariate Gaussian random variables is satisfied , then this method of feature reduction generally performs very well . The principal components are also statistically independent for the Gaussian case . The principal components \\\" account for \\\" or explain the maximum amount of variance of the original data . Figure 1 shows an example of scatter plot of 2-D multivariate data and the resulting orientation of the first primary principal components basis vector . As expected this basis vector , represented by a straight line , is oriented along the axis with maximum data variation . Figure 1 . Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector . Line is a good fit to data . Figure 2 depicts data which is primarily aligned with a U shaped curve in a 2-D space , and the resulting straight line ( PCA ) basis vector fit to this data . Since the original data is not multivariate Gaussian , the PCA basis vector is no longer a good way to approximate the data . In fact , since the data primarily follows a curved path in the 2-D space , no linear transform method , resulting in a straight line subspace , will be a good way to approximate the data with one dimension . Figure 2 . Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector . No straight line can be a good fit to this data . Linear Discriminant Analysis ( LDA ) . Linear transforms for the purpose of reducing dimensionality while preserving discriminability between pre - defined categories have also long been known about and used ( Wang & Paliwal , 2003 ) , and are usually referred to as Linear Discriminant Analysis ( LDA ) . The mathematical usage of this is identical to that for PCA . That is . Y . A . T . X , where X , Y are again column vectors as for PCA . The big difference is in how A is computed . For LDA , it has been shown that the columns of A correspond to the m largest eigenvalues of . S . W . S . B , where S W is the within class covariance matrix and S B is the between class covariance matrix . Often S B is computed as the covariance of the category means ; alternatively , it is sometimes computed as the \\\" grand \\\" covariance matrix over all data , ignoring category labels , identical to the covariance matrix used to compute PCA basis vectors . S W , the within class covariance matrix , is generally computed by first determining the covariance matrix for each category of data , and then averaging over all categories . The explicit assumption for LDA is that the within class covariance of each category is the same , which is rarely true in practice . Nevertheless , for many practical classification problems , features reduced by LDA often are as effective or even advantageous to original higher dimensional features . Figure 3 depicts 2-D 2-class data , and shows the first PCA basis vector as well as the first LDA basis vector . Clearly , for this example , the two basis vectors are quite different , and clearly the projection of data onto the first LDA basis vector would be more effective for separating the two categories than data projected onto the first PCA basis vector . Figure 3 . 2-D 2-class data , along with first LDA basis vector and first PCA basis vector . The classes would be well separated by a projection onto the first LDA basis vector , but poorly separated by a projection onto the first PCA basis vector . Heteroscedastic Discriminant Analysis ( HDA ) . Another linear transformation technique , related to linear discriminant analysis , but which accounts for the ( very common ) case where within class covariance matrices are not the same for all classes , is called Heterocscedastic Discriminant Analysis ( HDA ) ( Saon et al . , 2000 ) . The process for HDA is described in some detail by Saon et al . , and illustrated in terms of its ability to better separate ( as compared to LDA ) data in reduced dimensionality subspaces , when the covariance properties of the individual classes are different . HDA suffers from two drawbacks . There is no known closed form solution for minimizing the objective function required to solve for the transformation - rather a complex numerically based gradient search is required . More fundamentally , with actual speech data , HDA alone was found to perform far worse than LDA . Nevertheless , if an additional transform , called the Maximum Likelihood Linear Transform ( MLLT ) ( Gopinath , 1998 ) was used after HDA , then overall performance was found to be the best among the methods tested by Saon et al . However , ASR accuracies obtained with a combination of LDA and MLLT were nearly as good as those obtained with HDA and MLLT . A detailed summary of HDA and MLLT are beyond the scope of this chapter ; however , these methods , either by themselves , or in conjunction with the nonlinear methods described in this chapter warrant further investigation . Nonlinear dimensionality reduction . If the data are primarily clustered on curved subspaces embedded in high dimensionality feature spaces , linear transformations for feature dimensionality reduction are not well suited . For example , the data depicted in Figure 2 would be better approximated by its position with respect to a curved U - shape line rather the straight line obtained with linear PCA . ( Bishop et al . 1998 ) discusses several theoretical methods for determining these curved subspaces ( manifolds ) within higher dimensionality spaces . Another general method , and the one illustrated and explored in more detail in this chapter , is based on a \\\" bottleneck \\\" neural network ( Kramer , 1991 ) . This method relies on the general ability of a neural network with nonlinear activation functions at each node , with enough nodes and at least one hidden layer , to be able to determine an arbitrary nonlinear mapping . The general network configuration is shown in Figure 4 . Figure 4 . Architecture of Bottleneck Neural Network . If data lies along a single curved line in a higher dimensionality space , 1 node in the bottleneck layer should be sufficient . If data lies on a curved surface embedded in a higher dimensionality space , 2 nodes in the bottleneck layer should be sufficient . Nonlinear Principal Components Analysis ( NLPCA ) . Since the final NN outputs are created from the internal NN representations at the bottleneck layer , the bottleneck outputs can be viewed as the reduced dimensionality version of the data . This idea was tested using pseudo - random data generated so as to cluster on curved subspaces . NLPCA is first illustrated by an example depicted in Figure 5 . For this case , 2-D pseudo random data was created to lie along a U shaped curve , similar to the data depicted in Figure 2 . A neural network ( 2 - 5 - 1 - 5 - 2 ) was then trained as an identify map . The numbers in parentheses refer to the number of nodes at each layer , proceeding from input to output . All hidden nodes and output nodes had a bipolar sigmoidal activation function . After training with backpropagation , all data were transformed by the neural network . In Figure 5 , the original data is shown as blue symbols , and the transformed data is shown by red . Clearly , the data have been projected to a curved U shaped line , as would be expected for the best line fit to the original data . Figure 5 . Plot of input and output data for pseudo - random 2-D data . The output data ( red line ) is reconstructed data obtained after passing the input data through the trained neural network . In Figure 6 , NLPCA is illustrated by data which falls on a 2-D surface embedded in a 3-D space . For this case , 2-D data pseudo random data are created , but confined to lie on the surface of a 2-D Gaussian shaped surface , as depicted in the left panel of Figure 6 . Then a neural network ( 3 - 10 - 2 - 10 - 3 ) was trained as an identity map . After training , the outputs of the neural network are plotted in the right panel of Figure 6 . Clearly the neural network \\\" learned \\\" a 2-D internal representation , at the bottleneck layer , from which it could reconstruct the original data . Figure 6 . Input and output plot of the 3-D Gaussian before ( left ) and after ( right ) using neural network for NLPCA . Nonlinear Discriminant Analysis ( NLDA ) . Fortunately , only a minor modification to NLPCA is needed to form NLDA . The same bottleneck network architecture is used , but trained to recognize categories rather than as an identity map . In the remaining part of this chapter , two versions of NLDA based on this strategy are described , followed by a series of experimental evaluations for the phonetic classification and recognition tasks . Nonlinear dimensionality reduction architecture . In a previous work ( Zahorian et al . , 2007 ) , NLPCA was applied to an isolated vowel classification task , and the nonlinear method based on neural networks was experimentally compared with linear methods for reducing the dimensionality of speech features . A summary of this work is presented in Section 4.5 . In contrast , the nonlinear technique NLDA based on minimizing classification error was quite effective for improving accuracy . The general form of the NLDA transformer and its relationship to the HMM recognizer are depicted in Figure 7 . NLDA is based on a multilayer bottleneck neural network and performs a nonlinear feature transformation of the input data . The outputs of the network are further ( optionally ) processed by PCA to create transformed features to be the inputs of an HMM recognizer . Note that in this usage , \\\" outputs \\\" may be from the final outputs or from one of the internal hidden layers . Figure 7 . Overview of the NLDA transformation for speech recognition . The multilayer bottleneck neural network employed in NLDA contains an input layer , hidden layers including the bottleneck layer , and an output layer . The numbers of nodes in the input and output layers respectively correspond to the dimensions of the input features and the number of categories in the training target data . The targets were chosen as the 48 ( collapsed ) phones in the training data . The number of hidden layers was experimentally determined as well as the number of nodes included in those layers . However , most typically three hidden layers were used . Two NLDA approaches were investigated as different layers of networks are used to obtain dimensionality reduced data . NLDA1 . In the first approach , which is referred to as NLDA1 , the transformed features are produced from the final output layer of the network . This approach is similar to the use of tandem neural networks used in some automatic speech recognition studies ( Hermansky & Sharma , 2000 ; Ellis et al . , 2001 ) . Figure 8 illustrates the use of network outputs in NLDA1 . Figure 8 . Use of network outputs in NLDA1 . NLDA2 . Figure 9 illustrates the use of network outputs in NLDA2 . These two versions of NLDA were experimentally tested , with and without PCA following the neural network transformer , with some variations of the nonlinearities in the networks . The dimensionality of the reduced feature space is determined only by the number of nodes in the middle layer . Therefore , an arbitrary number of reduced dimensions can be obtained , independent of the input feature dimensions and the nature of the training targets . A lower dimensional representation of the input features is easily obtained by simply deploying fewer nodes in the middle layer than the input layer . This flexibility allows dimensionality to be adjusted so as to optimize overall system performance ( Hu & Zahorian , 2008 ; Hu & Zahorian , 2009 ; Hu & Zahorian , 2010 ) . In contrast with NLDA1 where dimensionality reduction is assigned to PCA , for NLDA2 , since the dimensionality reduction can be accomplished with the neural network only , the linear PCA is used specifically for reducing the feature correlation . Figure 9 . Middle layer outputs used as dimensionality reduced features in NLDA2 . Neural networks . In optimizing the design of a neural network , an important consideration is the number of hidden layers and an appropriate number of hidden nodes in each layer . A neural network with no hidden layers can form only simple decision regions , which is not suitable for highly nonlinear and complex speech features . On the number of hidden nodes , a small number reduces the network 's computational complexity . However , the recognition accuracy is often degraded . The more hidden nodes a network has , the more complex a decision surface can be formed , and thus better classification accuracy can be expected ( Meng , 2006 ) . Generally , the number of hidden nodes is empirically determined by a combination of accuracy and computational considerations , as applied to a particular application . Another important consideration is selecting an activation function , or the nonlinearity of a node . Typical choices include a linear activation function , a unipolar sigmoid function and a bipolar sigmoid function as illustrated in Figure 10 . The activation function should match the characteristic of the input or output data . For example , with training targets assigned the values of \\\" 0 \\\" and \\\" 1 \\\" , a sigmoid function with the outputs in the range of [ 0 , 1 ] is a good candidate for the output layer . Most typically a mean square error , between the desired output and actual output of the NN , is the objective function that is minimized in NN training . As another powerful approach , the softmax function takes all the nodes in a layer into account and calculates the output of a node as a posterior probability . When the outputs of the network are to be used as transformed features for the HMM recognition , a linear function or a softmax function is appropriate to generate the data with a more diverse distribution , such as one that would be well - modeled with a GMM . Moreover , equipped with various nonlinearities , the neural network is expected to have a stronger discriminative capability and thus it is enabled to cope with more complex data . Figure 10 . Illustrations of a linear activation function ( left ) , a unipolar sigmoid function ( middle ) and a bipolar sigmoid function ( right ) . The weights of the neural network are estimated using the backpropagation algorithm to minimize the distance between the scaled input features and target data . The update of the weights in each layer depends on the activation function of that layer , thus the network learning can be designed to perform different updates when dissimilar activation functions are used . In addition , a difficulty in neural network training is that the input data has a wide range of means and variances for each feature component . In order to avoid this , the input data of neural networks is often scaled so that all feature components have the same mean ( zero ) and variance ( so that range of values is approximately \\u00b1 1 ) . Investigation of basic issues . Note that unlike phonetic recognition experiments , for the case of classification , the timing labels in the database are explicitly used for both training and testing . Thus classification is \\\" easier \\\" than recognition , and accuracies typically higher , since phone boundaries are known in advance and used . In this first series of experiments , classification experiments were conducted using PCA , LDA , NLPCA , and NLDA2 transformations , as well as the original features . All the training sentences ( 4620 sentences ) were used to extract a total of 31,300 vowel tokens for training . All the test sentences ( 1680 sentences ) were used to extract a total of 11,625 vowel tokens for testing . For each vowel token , 39 DCTC - DCS features were computed using 13 DCTC terms and 3 DCS terms . For all cases , including original features , and all versions of the transformed features , a neural network classifier with 100 hidden nodes and 10 output nodes , trained with backpropagation , was used as the classifier . In addition , a Bayesian maximum likelihood Mahalanobis distance based Gaussian assumption classifier ( MXL ) was used for evaluation . For the neural network transformation cases , the first and third hidden layers had 100 nodes ( empirically determined ) . The number of hidden nodes in the second hidden layer was varied from 1 to 39 , according to the dimensionality being evaluated . For the case of NLDA2 , the network used for dimensionality reduction was also a classifier . For the sake of consistency , the outputs of the hidden nodes from the bottleneck neural network were used as features for a classifier , using either another neural network or the MXL classifier . [ 1 ] - In these initial experiments , the bottleneck neural network outputs were not additionally transformed with PCA . Experiment 1 . In the first experiment , all training data were used to train the transformations including LDA , PCA , NLPCA , and NLDA2 , and the classifiers . Figure 11 shows the results based on the neural network and MXL classifiers for each transformation method in terms of classification accuracy , as the number of features varies from 1 to 39 . For both the neural network and MXL classifiers , highest accuracy was obtained with NLDA2 , especially with a small numbers of features . For the MXL classifier , NLDA2 features result in approximately 10 % higher classification accuracies as compared to all other features . For both the neural network and MXL classifiers , accuracy with NLPCA features was very similar to that obtained with linear PCA . For reduced dimensionality features and/or a MXL classifier , the NLDA2 transformation was clearly superior to original features or any of the other feature reduction methods . However , with a neural network classifier and much higher dimensionality features , all feature sets perform similarly in terms of classification accuracy . As just illustrated , dimensionality reduction is not necessarily advantageous in terms of accuracy for classifiers trained with enough data and the \\\" right \\\" classifier . However , for the case of complex automatic speech recognition systems , there is generally not enough training data . Experiment 2 . To simulate lack of training data , another experiment was conducted . In this experiment , the training data was separated into two groups , with about 50 % in each group . One group of data ( group 1 ) was used for \\\" training \\\" transformations while the other data ( group 2 ) was used for training classifiers . Figure 11 . Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers with various types of features . The results obtained with the neural network and MXL classifiers using 10 % of the group 2 training data ( that is , 5 % of the overall training data ) are shown in Figure 12 . The numbers of features evaluated are 1 , 2 4 , 8 , 16 and 32 . For both the neural network and MXL classifiers , NLDA2 clearly performs much better than the other transformations or the original features . However , the advantage of NLDA2 decreases with an increasing number of features , and as the percentage of group 2 data increases ( not shown in figure ) . Category labels for discriminatively based transformations . For all discriminatively based transformations , either linear or nonlinear , an implicit assumption is that training data exists which has been labeled according to category . For the case of classification , such as the experiments just described , this labeled data is needed anyway , for both training and test data , to conduct classification experiments ; thus the need for category labeled data is not any extra burden . However , for other cases , such as the phonetic recognition experiments described in the remainder of this chapter , there may or may not be easily available and suitable labeled training data . This issue of category labels is described in more detail in the following two subsections . Figure 12 . Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers using 10 % of group 2 training data for training classifier . Phonetic - level targets . The training of the neural network ( NLDA1 and NLDA2 ) requires category information for creating training targets . For the case of databases such as TIMIT , the data is labeled using 61 phone categories , and the starting point for training discriminative transformations would seem to be these phonetic labels . In the neural network training , these are referred to as targets . Ideally , the targets are uncorrelated , which enables quicker convergence of weight updates . The targets can also be viewed as multidimensional vectors , with a value of \\\" 1 \\\" for the target category and \\\" 0s \\\" for the non - target categories . Figure 13 illustrates a sequence of phoneme training targets for the TIMIT database using 48 phoneme categories . These vectors have 48 dimensions and each vector consists of only one peak value to indicate the category . Note that , in the TIMIT case , other reasonable choices for targets would be 61 ( the number of phone label categories ) , or 39 ( the number of collapses phone categories ) . However , empirically , the choice of 48 categories , with only some phones combined , seemed to be the best choice for both neural network training targets and for the creation of HMM phone models . Figure 13 . Training target vectors of the neural network . State - level targets . Due to the nonstationarity of speech signals , a speech signal varies even in a very short time interval ( e.g. a phoneme ) . For speech recognition tasks , instead of phone level training targets , state ( as in hidden states of an HMM ) dependent targets could be advantageous in training a versatile network for more highly discriminative speech features . However , the boundaries between states in a phoneme are likely to be indistinct ; even more importantly , from a practical perspective is that , unlike phonemes , the ( HMM ) state boundaries are unknown in advance of training . Thus the estimation of state boundary information is required . This boundary information may be in error due to the nature of unclear state boundaries and the lack of a reliable estimation approach . Therefore , in the discriminative training process , \\\" do n't cares \\\" were used to account for this lack of precision in determining state boundaries . In the neural network training process , the errors of output nodes corresponding to \\\" do n't cares \\\" are not computed and thus these \\\" do n't cares \\\" have no effect on weight updates . The state training targets with \\\" do n't cares \\\" uses \\\" do n't care \\\" states for each phoneme model , so that one neural network trained with the targets can generate state dependent outputs . As illustrated in Figure 15 , the phone - specific training targets in Figure 14 are expanded to 144 dimensions by duplicating the phoneme specific target by the required number of the states . As time progresses during a phone , the \\\" 1 \\\" moves from state 1 to state 2 , to state 3 . Figure 14 . Figure 15 . Illustration of the state level training targets with \\\" do n't cares . \\\" The -1 values are used to denote \\\" do n't cares . \\\" Two approaches are used to determine state boundaries . The second approach determines state boundaries using the HMM - based Viterbi alignment based on already trained HMMs . As illustrated in one of the experiments presented later , the state dependent targets were shown to perform better than phone level targets . The simpler approach of using fixed ratios for state boundaries was as good as using the Viterbi alignment approach . Evaluation of feature reduction methods with phonetic recognition experiments . Given the high dimensionality of speech feature spaces used for automatic speech recognition , typically 39 or more , it is not feasible to visualize the distribution of data in feature space . It is possible that a reduced dimensionality subspace obtained by linear methods , such as PCA or LDA , forms an effective , or at least adequate subspace for implementing automatic speech recognition systems with a reduced dimensionality feature space . Note that if PCA or LDA do perform well , these methods would be preferred to the nonlinear methods , due to the much simpler implementation methods and the corresponding need for less data . However , it is also possible that one of the nonlinear methods for feature reduction is more effective , that is enable higher ASR accuracy , than any of the linear methods . The comparisons of these various methods can only be done experimentally . TIMIT database . The database used for all experiments reported in the remainder of this chapter is TIMIT . The TIMIT database was developed in the early 1980 's for expediting acoustic - phonetic ASR research ( Garofolo et al . , 1993 ; Zue et al , 1990 ) . It consists of recordings of 10 sentences from each of 630 speakers , or 6300 sentences total . Of the text material in the database , two dialect sentences ( SA sentences ) were designed to expose the specific variants of the speakers and were read by all 630 speakers . There are 450 phonetically - compact sentences ( SX sentences ) which provide a good coverage of pairs of phones . Each speaker read 5 of these sentences and each text was spoken by 7 different speakers . A total of 1890 phonetically - diverse sentences ( SI sentences ) were selected from existing text sources to add diversity in sentence types and phonetic contexts . Each speaker read 3 of these sentences , with each text being read only by a single speaker . All sentences are phonetically labeled with start and stop times for each phoneme . The database is further divided into a suggested training set ( 4620 sentences , 462 speakers ) and suggested test set ( 1680 sentences , 168 speakers ) . The training and test sets are balanced in terms of representing dialect regions and male / female speakers . Similarly , most researchers have not used the SA sentences for ASR experiments , since the identical phonetic contexts ( every speaker read the same sentences for the SA sentences ) , were thought be non representative of everyday speech . [ 2 ] - Most , but not all researchers , have used the recommended training and test sets . For all ASR experiments reported in this chapter , the SA sentences were removed , the recommended training and test sets were used , and the phone set was collapsed to the same 39 phones used in most ASR experiments with TIMIT . The NTIMIT database , used for the classification experiments described in section 4.5 , is the same one as TIMIT , except the data was transmitted over phone lines and re - recorded . Thus NTIMIT is more bandlimited ( approximately 300Hz to 3400 Hz ) , more noisy , but has the identical \\\" raw \\\" speech . DCTC / DCSC speech features . The modified DCTC is used for representing speech spectra , and the modified DCSC is used to represent spectral trajectories . Each DCTC is represented by a DCSC expansion over time ; thus the total number of features equals the number of DCTC terms times the number of DCSC terms . The number of DCTCs used was 13 , and number of DCS terms was varied from 4 to 7 , for a total number of features ranging from 52 to 91 . These numbers are given for each experiment . Additionally , as a control , one experiment was conducted with Mel - frequency Cepstral Coefficients ( MFCCs ) ( Davis & Mermelstein , 1980 ) , since these MFCC features are most typically used in ASR experiments . A total of 39 features including 13 MFCC features , delta terms , and delta - delta terms were extracted from both the training and test data . Hidden Markov Models ( HMMs ) . Left - to - right Markov models with no skip were used and a total of 48 monophone HMMs were created from the training data using the HTK toolbox ( Verion 3.4 ) ( Young et al . , 2006 ) . The bigram phone information extracted from the training data was used as the language model . Various numbers of states and mixtures were evaluated as described in the following experiments . In all cases diagonal covariance matrices were used . For final evaluations of accuracy , some of these 48 monophones were combined to create the \\\" standard \\\" set of 39 phone categories . Experiment with various reduced dimensions . The first experiment was conducted to evaluate the two NLDA versions with various dimensions in the reduced feature space with and without the use of PCA . As input features , 13 DCTCs , computed with 8 ms frames and 2 ms spacing , were represented with 6 DCSCs over a 500 ms block , for a total of 78 features ( 13 DCTCs x 6 DCSCs ) . The 48-dimensional outputs of the neural network were further reduced by PCA in NLDA1 , while the dimensionality reduction was controlled only by the number of nodes in the middle layer in NLDA2 . The features which were dimensionality reduced by PCA and LDA alone were also evaluated for the purpose of comparison . Figure 16 shows recognition accuracies of dimensionality reduced features using 1-state and 3-state HMMs with 3 mixtures per state . Note that the NLDA1 features without the PCA process are always 48 dimensions . Compared to the PCA and LDA reduced features , the NLDA1 and NLDA2 features performed considerably better for both the 1-state and 3-state HMMs . For the case of 3-state HMMs , the transformed features reduced to 24 dimensions resulted in the highest accuracy of 69.3 % for NLDA1 . A very similar accuracy of 69.2 % was obtained with NLDA2 using 36-dimensional features . The recognition accuracies were further improved by about 3 % with PCA reduced dimensionality features versus the NLDA features for most cases , showing the effectiveness of PCA in de - correlating the network outputs . The accuracies obtained with the original 78 features , and 3 mixture HMMs , are approximately 58 % ( 1 state models ) and 63 % ( 3 state models ) . NLDA1 and NLDA2 experiment with various HMM configurations . The aim of the second experiment is a more thorough evaluation of NLDA1 and NLDA2 using a varying number of states and mixtures in HMMs . The 78 DCTC / DCSCs ( computed as mentioned in previous section ) were reduced to 36 dimensions based on the results of the previous experiment . The 48 phoneme level targets were used in the training of the network . The features which are the direct outputs of the network without PCA processing were also evaluated . Figure 17 shows accuracies using 1-state and 3-state HMMs with a varying number of mixtures per state . NLDA2 performed better than NLDA1 for all conditions -- approximately 2 % higher accuracy . The NLDA2 transformed features resulted in the highest accuracy of 73.4 % with 64 mixtures , which is about 1.5 % higher than the original features for the same condition . The use of PCA improves accuracy on the order of 2 % to 10 % , depending on the conditions . In contrast , for NLDA2 , best performance was obtained with the nonlinearities used for both training and final transformations . The superiority of the NLDA transformed features is more significant when a small number of mixtures are used . For example , the NLDA2 features modeled by 3-state HMMs with 3 mixtures resulted in an accuracy of 69.4 % versus 63.2 % for the original features . Figure 16 . Figure 17 . Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures . These results imply that the middle layer outputs of a neural network are able to better represent original features in a dimensionality - reduced space than are the outputs of the final output layer . The configuration of HMMs can be largely simplified by incorporating NLDA . Experiments with large network training . The results of the previous experiment showed large performance advantages for NLDA2 over NLDA1 and the original features , when using either a small number of features , or a \\\" small \\\" HMM . However , if all original features were used , and a 3- state HMM with a large number of mixtures were used , there was very little advantage of NLDA2 , in terms of phonetic recognition accuracy . Therefore , an additional experiment was performed , using the state level targets with \\\" do n't cares , \\\" as mentioned previously , and a very large neural network for transforming features . The state targets were formed using either a constant length ratio ( ratio for 3 states : 1:4:1 ) or a Viterbi forced alignment approach , as described in Section 4.5 . The expanded neural networks had 144 output nodes and were iteratively trained . For both NLDA1 and NLDA2 , the networks were configured with 78 - 500 - 36 - 500 - 144 nodes , going from input to output . Figure 18 . Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \\\" ( CR ) \\\" and \\\" ( FA ) \\\" indicate the training targets obtained with the constant length ratio and forced alignment respectively . As shown in Figure 18 , both NLDA1 and NLDA2 using the expanded targets lead to a significant increase in accuracy . The NLDA2 accuracies are typically about 2 % higher than NLDA1 accuracies . The use of forced alignment for state boundaries resulted in the highest accuracy of 75.0 % with 64 mixtures . However , the best result using the much simpler constant ratio method is only marginally lower at 74.9 % . Similar experiments , with all identical conditions except using either phone level targets , or state level targets without \\\" do n't cares \\\" resulted in about 2 % lower accuracies . These results imply that the use of \\\" do n't cares \\\" is able to reduce errors introduced by inaccurate determination of state boundaries . Comparing these results with those from Figure 17 , the NLDA2 features in a reduced 36-dimensional space achieved a substantial improvement versus the original features , especially when a small number of mixtures were used . These results show the NLDA methods based on the state level training targets are able to form highly discriminative features in a dimensionality reduced space . MFCC experiments . For comparison , 39-dimensional MFCC features ( 12 coefficients plus energy with the delta and acceleration terms ) were reduced to 36 dimensions with the same configurations and evaluated . The results followed the same trend , but the accuracies were about 4 % lower than those of the DCTC - DCSC features for all cases , for example , 70.7 % with NLDA2 using forced alignment and 32 mixtures . Conclusions . Nonlinear dimensionality reduction methods , based on the general nonlinear mapping abilities of neural networks , can be useful for capturing most of the information from high dimensional spectral / temporal features , using a much smaller number of features . A neural network internal representation in a \\\" bottleneck \\\" layer is more effective than the representation at the output of a neural network . The neural network features also should be linearly transformed with a principal components transform in order to be effective for use by a Hidden Markov Model . For use with a multi - hidden - state Hidden Markov Model , the nonlinear transform should be trained with state - specific targets , but using \\\" do n't cares , \\\" to account for imprecise information about state boundaries . In future work , linear transforms other than principal components analysis , such as heteroscedastic linear transforms followed by maximum likelihood linear transforms , should be explored for post processing of the nonlinear transforms . Alternatively , the neural network architecture and/or training constraints could be modified so that the nonlinearly transformed features are more suitable as input features for a Hidden Markov Model . References . 2 - S. B. Davis , P. Mermelstein , 1980 Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences , IEEE Trans . on Acoustics , Speech and Signal Processing , 28 357 366 . TIMIT Acoustic - Phonetic Continuous Speech Corpus . 21 - S. A. Zahorian , D. Qian , A. J. Jagharghi , 1991 Acoustic - phonetic transformations for improved speaker - independent isolated word recognition . Proc . ICASSP'91 , 561 564 . Toronto , Ontario , Canada , May 14 - 17 , 1991 . 22 - S. Zahorian , P. Silsbee , X. Wang , 1997 Phone Classification with Segmental Features and a Binary - Pair Partitioned Neural Network Classifier . Proc . ICASSP ' 97 , 1011 1014 , Munich , Germany , April 21 - 24 , 1997 . 23 - S. A. Zahorian , A. M. Zimmer , F. Meng , 2002 Vowel Classification for Computer - based Visual Feedback for Speech Training for the Hearing Impaired , Proc . ICSLP 2002 , 973 976 , Denver , CO , Sept. 16 - 20 , 2002 . 24 - S. A. Zahorian , T. Singh , H. Hu , 2007 Dimensionality Reduction of Speech Features using Nonlinear Principal Components Analysis . Proc . INTERSPEECH ' 07 , 1134 1137 , Antwerb , Belgium , Aug. 27 - 31 , 2007 . 25 - J. Zhao , X. Zhang , A. Ganapathiraju , N. Deshmukh , J. Picone , 1999 Decision Tree - Based State Tying For Acoustic Modeling . A Tutorial , Institute for Signal and Information Processing , Department of Electrical and Computer Engineering , Mississippi State University . \"}",
        "_version_":1692670958074068992,
        "score":22.758251},
      {
        "id":"ae5f3a30-9720-41e1-8885-51a970c833c4",
        "_src_":"{\"url\": \"http://www.actionaid.org/tags/429/86\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701148475.34/warc/CC-MAIN-20160205193908-00128-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"This thesis presents a novel approach to interlingual machine translation using \\u00ce\\\"-calculus expressions as an intermediate representation . It investigates and extends existing algorithms which learn a combinatorial category grammar for semantic parsing , and introduces two new algorithms for generation out of logical forms inspired by that semantic parser . The results of a set of new experiments for generation and parsing are described , as well as an evaluation of the performance of a semantic translation system created by joining the semantic parser and generator together . Experimental results demonstrate that under certain conditions , this semantic model achieves better performance than a standard phrase - based statistical MT system in both an automated evaluation of translation output and a manual evaluation of adequacy and fluency . \"}",
        "_version_":1692670067165102081,
        "score":22.682455},
      {
        "id":"de1b7180-755c-47a5-8dce-649eee217fc9",
        "_src_":"{\"url\": \"http://sportspressnw.com/2186260/2014/hope-solo-jailed-on-domestic-assault-charges\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701158811.82/warc/CC-MAIN-20160205193918-00242-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Lexing and Parsing Continued . This article is the second part of a series which started with An Overview of Lexing and Parsing . That article aimed to discuss lexing and parsing in general terms , while trying to minimize the amount on how to actually use Marpa::R2 to do the work . In the end , however , it did have quite a few specifics . This article has yet more detail with regard to working with both a lexer and a parser . BTW , Marpa 's blog . ( For more information , see the Marpa blog or download the example files for this article . ) Brief Recap : The Two Grammars . Article 1 defined the first sub - grammar as the one which identifies tokens and the second sub - grammar as the one which specifies which combinations of tokens are legal within the target language . As I use these terms , the lexer implements the first sub - grammar and the parser implements the second . Some Context . It 's actually a copy of the image of a manual page for Graph::Easy . Note : My module Graph::Easy::Marpa is a complete re - write of Graph::Easy . After I offered to take over maintenance of the latter , I found the code so complex I literally could n't understand any of it . There are three ways ( of interest to us ) to specify the contents of this image : . As a Graphviz DOT file written in a little language . This approach uses the Graph::Easy language invented by the author ( Tels ) of the Graph::Easy Perl module . Call this teamwork.easy . It 's actually input for Graph::Easy::Marpa : . Note : In some rare cases , the syntax supported by Graph::Easy::Marpa will not be exactly identical to the syntax supported by the original Graph::Easy . As a DOT file . Call this teamwork.dot : . This article is about using GraphViz2::Marpa to parse DOT files . Of course the Graphviz package itself provides a set of programs which parse DOT files in order to render them into many different formats . Why then would someone write a new parser for DOT ? One reason is to practice your Marpa skills . Another is , perhaps , to write an on - line editor for Graphviz files . Alternately you might provide add - on services to the Graphviz package . For instance , some users might want to find all clusters of nodes , where a cluster is a set of nodes connected to each other , but not connected to any nodes outside the cluster . Yet other uses might want to find all paths of a given length emanating from a given node . Scripts for Testing . The code being developed obviously needs to be tested thoroughly , because any such little language has many ways to get things right and a horrendously large number of ways to get things slightly wrong , or worse . Luckily , because graphs specified in DOT can be very brief , it 's a simple matter to make up many samples . Further , other more complex samples can be copied from the Graphviz distro 's graphs / directed/ and graphs / undirected/ directories . svg ( output ) files . The missing files are due to deliberate errors in the input files , so they do not have output files . Why a rend.pl ? If the code ca n't reconstruct the input DOT file , something got lost in translation .... . The distribution also includes scripts which operate on a set of files . lex ( CSV files ) . parse ( CSV files ) . rend ( dot files ) . Some Modules . The STT comes in via GraphViz2::Marpa::Lexer , which produced it from within its own source code or an external CSV file . The lexer has already validated the structure of the STT 's data . Transform the STT from the input form ( spreadsheet / CSV file ) into what Set::FA::Element expects . Set up the logger . Provide the code for all the functions which handle enter - state and exit - state events . This is the code which can apply checking above and beyond what was built into the set of regexps which came from the spreadsheet . Most importantly , this code stockpiles the tokens themselves with metadata to identify the type of each token ( hence the two columns in the upcoming sample data/27 . lex just below ) . Run the DFA . Check the result of that run . Did the DFA end up in an accepting state ? Yes is okay and no is an error . Here is some sample data which ships with GraphViz2::Marpa , formatted for maximum clarity : . A DOT file , data/27 . gv , which is input to the lexer : . A token file , data/27 . lex , which is output from the lexer : . Some Notes on the STT . Firstly , note that the code allows whole - line comments ( matching m ! ^ ( ? These lines are discarded when the input file is read , and so do not appear in the STT . Working With An Incomplete BNF . Suppose you 've gone to all of the work to find or create a BNF ( Backus - Naur Form ) grammar for your input language . You might encounter the situation where you can use BNF to specify your language in general , but not precisely in every situation . DOT is one offender . DOT IDs can be surrounded by double - quotes , and in some case must be surrounded by double - quotes . Even worse , IDs can be attributes . For instance , you might want your font color to be green . Here 's the pain . In DOT , the thing to which the attribute belongs may be omitted as implied . That is , the name of the thing 's owner is optional . For instance , you might want a graph which is six inches square . Here 's how you can specify that requirement : . The double - quotes are mandatory in this context . This also works . So does this . But wait , there 's more ! The value of the attribute can be omitted , if it 's true . Hence the distro , and the demo page , have a set of tests , called data/42 . gv , which test that feature of the code . Grrrr . It must output graph ( the graph as a whole ) as the owner of the attribute in question . You ca n't solve this with regexps , unless you have amazing superpowers and do n't care if anyone else can maintain your code . Instead , be prepared to add code in two places : . At switch of state . After all input has been parsed . Indeed , GraphViz2::Marpa::Lexer::DFA contains a long sub , _ clean_up ( ) , which repeatedly fiddles the array of detected tokens , fixing up the list before it 's fit to inflict on the parser . Understanding the State Transition Table . I included this diagram in the first article : . A dot file starts with an optional strict , followed by either graph or digraph . ( Here di stands for directed , meaning edges between nodes have arrowheads on them . Yes , there are many attributes which can be attached to edges . Line One . From the first non - heading line of the STT , you can see how I ended up with : . The state defined on this line is the start state . Because the initial state ca n't be an accepting start , this column -- in the row -- must be empty . Later STT states have Yes in this column . I chose to call it initial . Other people call it start . This -- although it might not yet be clear -- is actually a regexp , / ( ? : strict)/ . The DFA adds the do - not - capture prefix / ( ? : and suffix ) / . This is the state to which to jump if a match occurs . Here , match means a match between the regexp ( event ) in the previous column and the head of the input stream . I do n't use it here , but if I did , it would mean to call a particular function when entering the named state . Similar to the entry function , this says to call a particular function when exiting the named state . : \\\\. : \\\\. I can save a set of regexps in this column and use spreadsheet formula elsewhere to refer to them . These are my notes to myself . This one says that regexp in the previous column specifies what an ID must match in the DOT language . Line Two . The event in the second line , / ( ? To clarify this point , recall that the DFA matches entries in the Event column one at a time , from the first listed against the name of the state -- here , / ( ? : strict)/ --down to the last for the given state -- here , / ( ? : \\\\s+)/ . If strict is not at the head of the input stream , and it can definitely be absent , as is seen in the above diagram , this regexp-- / ( ? Line Three . The skip takes place because the Next state is initial , the current state . In other words , discard any text at the head of the input stream which this regexp will gobble . Why does it get discarded ? That 's the way Set::FA::Element operates . Looping within a state does not trigger the exit - state and enter - state functions , and so there is no opportunity to stockpile the matched text . That 's good in this case . There 's no reason to save it , because it 's a comment . Think about the implications for a moment . Once the code has discarded a comment ( or anything else ) , you can never recreate the verbatim input stream from the stockpiled text . Hence you should only discard something once you fully understand the consequences . If you 're parsing code to execute it ( whatever that means ) , fine . If you 're writing a pretty printer or indenter , you can not discard comments . Lastly , we can say this regexp is used often , meaning we accept such comments at many places in the input stream . Line Four . The regexp \\\\s+ says to skip spaces ( in front of or between interesting tokens ) . As with the previous line , we skip to the very same state . This state has four regexps attached to it . More States . Re - examining the STT shows two introductory states , for input with and without a ( leading ) strict . I 've called these states by the arbitrary names initial and graph . If the initial strict is present , state initial handles it ( in the exit function ) and jumps to state graph to handle what comes next . If , however , strict is absent , state initial still handles the input , but then jumps to state graph_id . A ( repeated ) word of warning about Set::FA::Element . A loop within a state does not trigger the exit - state and enter - state functions . Sometimes this can actually be rather unfortunate . You may have to use this technique yourself . Be aware of it . Proceeding in this fashion , driven by the BNF of the input language , eventually you can construct the whole STT . Each time a new enter - state or exit - state function is needed , write the code , then run a small demo to test it . There is no substitute for that testing . The graph State . You reach this state simply by the absence of a leading strict in the input stream . Apart from not bothering to cater for comments ( as did the initial state ) , this state is really the same as the initial state . A few paragraphs back I warned about a feature designed into Set::FA::Element , looping within a state . That fact is why the graph state exists . If the initial state could have looped to itself upon detecting strict , and executed the exit or entry functions , there would be no need for the graph state . The graph_id State . Next , look for an optional graph i d , at the current head of the input stream ( because anything which matched previously is gone ) . Here 's the first use of a formula : Cell H2 contains ( ? : \\\\. : \\\\. The Remaining States . What follows in the STT gets complex , but in reality is more of the same . Several things should be clear by now : . The development of the STT is iterative . You need lots of tiny but different test data files , to test these steps . You need quite a lot of patience , which , unfortunately , ca n't be downloaded from the internet ... . Lexer Actions ( Callbacks ) . Matching something with a DFA only makes sense if you can capture the matched text for processing . Hence the use of state - exit and state - entry callback functions . In these functions , you must decide what text to output for each recognized input token . To help with this , I use a method called items ( ) , accessed in each function via $ myself . This method manages an stack ( array ) of items of type Set::Array . Each element in this array is a hashref : . Whenever a token is recognized , push a new item onto the stack . The value of the type string is the result of the DFA 's work identifying the token . This identification process uses the first of the two sub - grammars mentioned in the first article . A long Exit - state Function . The save_prefix function looks like : . # Warning : This is a function ( i.e. not a method ) . # Note : Because this is a function , $ myself is a global alias to $ self . A tiny Exit - state Function . Here 's one of the shorter exit functions , attached in the STT to the open_brace and start_statement states : . The code to push a new item onto the stack is just : . Using Marpa in the Lexer . Yes , you can use Marpa in the lexer , as discussed in the first article . I prefer to use a spreadsheet full of regexps -- but enough of the lexer . It 's time to discuss the parser . The Parser 's Structure . The parser incorporates the second sub - grammar and uses Marpa::R2 to validate the output from the lexer against this grammar . The parser 's structure is very similar to that of the lexer : . Initialize using the parameters to new ( ) . Declare the grammar . Run Marpa . Save the output . Marpa Actions ( Callbacks ) . As with the lexer , the parser works via callbacks , which are functions named within the grammar and called by Marpa::R2 whenever the input sequence of lexed items matches some component of the grammar . Consider these four rule descriptors in the grammar declared in GraphViz2::Marpa::Parser 's grammar ( ) method : . , ... ] . In each case the lhs is a name I 've chosen so that I can refer to each rule descriptor in other rule descriptors . That 's how I chain rules together to make a tree structure . ( See the Chains and Trees section of the previous article . ) This grammar fragment expects the input stream of items from the lexer to consist ( at the start of the stream , actually ) of three components : a strict thingy , a digraph thingy , and a graph_id thingy . Because I wrote the lexer , I can ensure that this is exactly what the lexer produces . These latter three come from the type key in the array of hashrefs built by the lexer . The three corresponding value keys in those hashrefs are yes or no for strict , yes or no for digraph , and an i d or the empty string for graph_id . As with the lexer , when in incoming token ( type ) matches expectations , Marpa::R2 triggers a call to an action , here called ( for clarity ) the same as the rhs . Consider one of those functions : . The parameter list is courtesy of how Marpa::R2 manages callbacks . $ t1 is the incoming graph i d . In data/27 . gv ( shown earlier ) , that is graph_27 . Marpa does not supply the string graph_id to this function , because there 's no need . I designed the grammar such that this function is only called when the value of the incoming type is graph_id , so I know precisely under what circumstances this function was called . That 's why I could hard - code the string graph_id in the body of the graph_id ( ) function . The Grammar in Practice . Now you might be thinking : Just a second ! That code seems to be doing no more than copying the input token to the output stream . Well , you 're right , sort of . True understanding comes when you realize that Marpa calls that code only at the appropriate point precisely because the type graph_id and its value graph_27 were at exactly the right place in the input stream . By that I mean that the location of the pair : . in the input stream was exactly where it had to be to satisfy the grammar initialized by Marpa::R2::Grammar . The role of the lexer as an intermediary is to simplify the logic of the code as a whole with a divide - and - conquer strategy . In other words , it 's no accident that that function gets called at a particular point in time during the parser 's processing of its input stream . Consider another problem which arises as you build up the set of rule descriptors within the grammar . Trees Have Leaves . The first article discussed chains and trees ( see the prolog_definition mentioned earlier in this article ) . Briefly , each rule descriptor must be chained to other rule descriptors . The astute reader will have already seen a problem : How do you define the meanings of the leaves of this tree when the chain of definitions must end at each leaf ? Here 's part of the data/27 . lex input file : . The corresponding rules descriptors look like : . The items marked as terminals ( standard parsing terminology ) have no further definitions , so attribute_key and attribute_val are leaves in the tree of rule descriptors . What does that mean ? The terminals attribute_id and attribute_value must appear literally in the input stream . Switching between attribute_key and attribute_id is a requirement of Marpa to avoid ambiguity in the statement of the grammar . Likewise for attribute_val and attribute_value . The min makes the attributes mandatory . Not in the sense that nodes and edges must have attributes , they do n't , but in the sense that if the input stream has an attribute_id token , then it must have an attribute_value token and vice versa . Remember the earlier section \\\" Working With An Incomplete BNF \\\" ? gv file used one of : . ... then the one chosen really represents the graph attribute : . To make this work , the lexer must force the output to be : . This matches the requirements , in that both attribute_id and attribute_value are present , is their ( so to speak ) owner , the object itself , which is identified by the type class_id . All of this should reinforce the point that the design of the lexer is intimately tied to the design of the parser . By taking decisions like this in the lexer you can standardize its output and simplify the work that the parser needs to don . Where to go from here . The recently released Perl module MarpaX::Simple::Rules takes a BNF and generates the corresponding grammar in the format expected by Marpa::R2 . This is a very interesting development , because it automates the laborious process of converting a BNF into a set of Marpa 's rule descriptors . Consequently , it makes sense for anyone contemplating using Marpa::R2 to investigate how appropriate it would be to do so via MarpaX::Simple::Rules . Wrapping Up and Winding Down . You 've seen samples of lexer output and some parts of the grammar which both define the second sub - grammar of what to expect and what should match precisely the input from that lexer . If they do n't match , it is in fact the parser which issues the dread syntax error message , because only it ( not the lexer ) knows which combinations of input tokens are acceptable . Just like in the lexer , callback functions stockpile items which have passed Marpa::R2 's attempt to match up input tokens with rule descriptors . This technique records exactly which rules fired in which order . After Marpa::R2 has run to completion , you have a stack of items whose elements are a ( lexed and ) parsed version of the original file . Your job is then to output that stack to a file , or await the caller of the parser to ask for the stack as an array reference . From there , the world . The Lexer and the State Transition Table - Revisited . The complexity of the STT in GraphViz2::Marpa justifies the decision to split the lexer and the parser into separate modules . Clearly that will not always be the case . Given a sufficiently simple grammar , the lexer phase may be redundant . Consider this test data file , data / sample.1 . ged , from Genealogy::Gedcom : . + ) $ / : an integer , a keyword , and a string . In this case I 'd skip the lexer , and have the parser tokenize the input . So , horses for courses . ( GEDCOM defines genealogical data ; see the GEDCOM definition for more details ) . \"}",
        "_version_":1692581021060431872,
        "score":22.57239},
      {
        "id":"041f45c4-d4d7-479e-a57c-1103df4cad2c",
        "_src_":"{\"url\": \"http://uncyclopedia.wikia.com/wiki/User:Sakamon/Off_the_Coast_of_Africa\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146302.25/warc/CC-MAIN-20160205193906-00328-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Repeated Capturing and Parsing . An interesting query was recently posted to the internal Perl mail list at work . The questioner was trying to match a pattern repeatedly , capturing all of the results in an array . But , it was n't doing quite what he expected . The message , with minor edits , went a little something like the following . I 'm trying to extract key / value pairs from a file with the following contents : . I was hoping to do something like this : . Thinking @list would be assigned the alternating key / value pairs . But the above does n't extract anything sane . Adding the /gc modifiers does n't make any difference . If I do the following , it extracts the first two key / value pairs correctly ( if the line has more than one pair ) . If I keep repeating the pattern in the second line , it keeps matching more key / value pairs . I would expect using ( ? What am I doing wrong ? When I 'm presented with a problem like this , that is some kind of structured data , I immediately think of writing a parser . I 'll get back to that in a bit , but I wanted to address the confusion about capturing in the pattern . And , in fact , that 's how the discussion on the mail list proceeded . Repeated Capturing . First , let 's simplify the example to demonstrate why our seeker of wisdom was n't getting back the list of items he expected . Capturing parentheses in Perl are treated somewhat like registers . Most Perl programmers are familiar with the $ n variables , which hold the values of a successful pattern match . For example $ 1 holds the value matched by the first set of parentheses , $ 2 holds the value of the second set , and so on . When a pattern is matched in list context , as above , it 's effectively the same as writing , . These pattern match variables are scalars and , as such , will only hold a single value . That value is whatever the capturing parentheses matched last . So , in our simplified example , $ 1 matches a , which is obvious enough . As the pattern repeats , $ 2 would be set to b , then c , and so on until the final match of e . That explains why the pattern match was n't returning the expected list . What can be done about it ? Capturing Along the Way . If we break down the sample data , we see that it generalizes to , . The first approach that came to mind is to split the data into multiple lines . Each line can then have its initial prefix removed and saved , then parsed for its key / value pairs . That 's starting to look a lot like parsing , which I promised to get to later . For the purposes of this discussion , I wanted to be able to accomplish the task with a single regular expression . To capture all of the values we want , we need to remove the repeating set of non - capturing parentheses . However , we still need to repeat the match , ideally returning all of the captured values in one statement . We can do that with the /g and /c regular expression modifiers . I 've done two things here . First , I replaced the \\\\S character classes , used to match the key and value , with \\\\w . The + pattern in a Perl regular expression is greedy , so the former character class was also matching the comma used to separate key / value pairs in the data . This left the literal comma with nothing to match , so was one source of confusion . Second , I noted that the initial prefix , while syntactically important , could be viewed in the same way as the comma and colon separators . I combined all of these separators and added a capture around them so we can later make sense of the parsed data . When matched against the data , the pattern results in a list like , . ( \\\" - \\\" , \\\" name \\\" , \\\" gcc_xo_src_clk \\\" , \\\" , \\\" , \\\" type \\\" , \\\" rcg \\\" , \\\" + \\\" , \\\" name \\\" , \\\" cxo_clk \\\" , ... ) . Even though we did all of the data extraction using a single pattern match , it looks remarkably like ... a parser ! The pattern is simply the tokenizer used to feed tokens into our state machine , the parser . Parsing . I stated at the outset that I looked at this as a parsing problem , so the solution I would use is most likely a parser . For simple , one - off scripts , I 'd use a technique similar to the one I described in the previous section . However , for more complex data or a more complex script , I 'd turn to a real parser . I wo n't go into any detail beyond showing what I wrote . As this was from an early point in the thread , the prefix is ignored in this example . ( \\\\w+ ) ( ? xms ; . Prior to Perl 5.10 , Damian Conway had written Parse::RecDescent , but with the introduction of grammar - like facilities like named captures and named backreferences , Damian improved upon his original work and presented the Perl community with Regexp::Grammars . What does a parser for this data built with Regexp::Grammars look like ? x ; . This is a trivial example and all the work is left to be done by inspecting the parse tree in % / . However , the module supports embedded code that will be called when a token or rule matches , which can be used to process the data as its parsed . \"}",
        "_version_":1692669833835970560,
        "score":22.398287},
      {
        "id":"739a6b37-3dae-4603-aac3-118ac5a2bd27",
        "_src_":"{\"url\": \"http://www.egms.de/static/en/meetings/dgnc2005/05dgnc0073.shtml\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701154221.36/warc/CC-MAIN-20160205193914-00097-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"January 2013 Archives . This article is the second part of a series which started with An Overview of Lexing and Parsing . That article aimed to discuss lexing and parsing in general terms , while trying to minimize the amount on how to actually use Marpa::R2 to do the work . In the end , however , it did have quite a few specifics . This article has yet more detail with regard to working with both a lexer and a parser . BTW , Marpa 's blog . ( For more information , see the Marpa blog or download the example files for this article . ) Brief Recap : The Two Grammars . Article 1 defined the first sub - grammar as the one which identifies tokens and the second sub - grammar as the one which specifies which combinations of tokens are legal within the target language . As I use these terms , the lexer implements the first sub - grammar and the parser implements the second . Some Context . It 's actually a copy of the image of a manual page for Graph::Easy . Note : My module Graph::Easy::Marpa is a complete re - write of Graph::Easy . After I offered to take over maintenance of the latter , I found the code so complex I literally could n't understand any of it . There are three ways ( of interest to us ) to specify the contents of this image : . As a Graphviz DOT file written in a little language . This approach uses the Graph::Easy language invented by the author ( Tels ) of the Graph::Easy Perl module . Call this teamwork.easy . It 's actually input for Graph::Easy::Marpa : . Note : In some rare cases , the syntax supported by Graph::Easy::Marpa will not be exactly identical to the syntax supported by the original Graph::Easy . As a DOT file . Call this teamwork.dot : . This article is about using GraphViz2::Marpa to parse DOT files . Of course the Graphviz package itself provides a set of programs which parse DOT files in order to render them into many different formats . Why then would someone write a new parser for DOT ? One reason is to practice your Marpa skills . Another is , perhaps , to write an on - line editor for Graphviz files . Alternately you might provide add - on services to the Graphviz package . For instance , some users might want to find all clusters of nodes , where a cluster is a set of nodes connected to each other , but not connected to any nodes outside the cluster . Yet other uses might want to find all paths of a given length emanating from a given node . Scripts for Testing . The code being developed obviously needs to be tested thoroughly , because any such little language has many ways to get things right and a horrendously large number of ways to get things slightly wrong , or worse . Luckily , because graphs specified in DOT can be very brief , it 's a simple matter to make up many samples . Further , other more complex samples can be copied from the Graphviz distro 's graphs / directed/ and graphs / undirected/ directories . svg ( output ) files . The missing files are due to deliberate errors in the input files , so they do not have output files . Why a rend.pl ? If the code ca n't reconstruct the input DOT file , something got lost in translation .... . The distribution also includes scripts which operate on a set of files . lex ( CSV files ) . parse ( CSV files ) . rend ( dot files ) . Some Modules . The STT comes in via GraphViz2::Marpa::Lexer , which produced it from within its own source code or an external CSV file . The lexer has already validated the structure of the STT 's data . Transform the STT from the input form ( spreadsheet / CSV file ) into what Set::FA::Element expects . Set up the logger . Provide the code for all the functions which handle enter - state and exit - state events . This is the code which can apply checking above and beyond what was built into the set of regexps which came from the spreadsheet . Most importantly , this code stockpiles the tokens themselves with metadata to identify the type of each token ( hence the two columns in the upcoming sample data/27 . lex just below ) . Run the DFA . Check the result of that run . Did the DFA end up in an accepting state ? Yes is okay and no is an error . Here is some sample data which ships with GraphViz2::Marpa , formatted for maximum clarity : . A DOT file , data/27 . gv , which is input to the lexer : . A token file , data/27 . lex , which is output from the lexer : . Some Notes on the STT . Firstly , note that the code allows whole - line comments ( matching m ! ^ ( ? These lines are discarded when the input file is read , and so do not appear in the STT . Working With An Incomplete BNF . Suppose you 've gone to all of the work to find or create a BNF ( Backus - Naur Form ) grammar for your input language . You might encounter the situation where you can use BNF to specify your language in general , but not precisely in every situation . DOT is one offender . DOT IDs can be surrounded by double - quotes , and in some case must be surrounded by double - quotes . Even worse , IDs can be attributes . For instance , you might want your font color to be green . Here 's the pain . In DOT , the thing to which the attribute belongs may be omitted as implied . That is , the name of the thing 's owner is optional . For instance , you might want a graph which is six inches square . Here 's how you can specify that requirement : . The double - quotes are mandatory in this context . This also works . So does this . But wait , there 's more ! The value of the attribute can be omitted , if it 's true . Hence the distro , and the demo page , have a set of tests , called data/42 . gv , which test that feature of the code . Grrrr . It must output graph ( the graph as a whole ) as the owner of the attribute in question . You ca n't solve this with regexps , unless you have amazing superpowers and do n't care if anyone else can maintain your code . Instead , be prepared to add code in two places : . At switch of state . After all input has been parsed . Indeed , GraphViz2::Marpa::Lexer::DFA contains a long sub , _ clean_up ( ) , which repeatedly fiddles the array of detected tokens , fixing up the list before it 's fit to inflict on the parser . Understanding the State Transition Table . I included this diagram in the first article : . A dot file starts with an optional strict , followed by either graph or digraph . ( Here di stands for directed , meaning edges between nodes have arrowheads on them . Yes , there are many attributes which can be attached to edges . Line One . From the first non - heading line of the STT , you can see how I ended up with : . The state defined on this line is the start state . Because the initial state ca n't be an accepting start , this column -- in the row -- must be empty . Later STT states have Yes in this column . I chose to call it initial . Other people call it start . This -- although it might not yet be clear -- is actually a regexp , / ( ? : strict)/ . The DFA adds the do - not - capture prefix / ( ? : and suffix ) / . This is the state to which to jump if a match occurs . Here , match means a match between the regexp ( event ) in the previous column and the head of the input stream . I do n't use it here , but if I did , it would mean to call a particular function when entering the named state . Similar to the entry function , this says to call a particular function when exiting the named state . : \\\\. : \\\\. I can save a set of regexps in this column and use spreadsheet formula elsewhere to refer to them . These are my notes to myself . This one says that regexp in the previous column specifies what an ID must match in the DOT language . Line Two . The event in the second line , / ( ? To clarify this point , recall that the DFA matches entries in the Event column one at a time , from the first listed against the name of the state -- here , / ( ? : strict)/ --down to the last for the given state -- here , / ( ? : \\\\s+)/ . If strict is not at the head of the input stream , and it can definitely be absent , as is seen in the above diagram , this regexp-- / ( ? Line Three . The skip takes place because the Next state is initial , the current state . In other words , discard any text at the head of the input stream which this regexp will gobble . Why does it get discarded ? That 's the way Set::FA::Element operates . Looping within a state does not trigger the exit - state and enter - state functions , and so there is no opportunity to stockpile the matched text . That 's good in this case . There 's no reason to save it , because it 's a comment . Think about the implications for a moment . Once the code has discarded a comment ( or anything else ) , you can never recreate the verbatim input stream from the stockpiled text . Hence you should only discard something once you fully understand the consequences . If you 're parsing code to execute it ( whatever that means ) , fine . If you 're writing a pretty printer or indenter , you can not discard comments . Lastly , we can say this regexp is used often , meaning we accept such comments at many places in the input stream . Line Four . The regexp \\\\s+ says to skip spaces ( in front of or between interesting tokens ) . As with the previous line , we skip to the very same state . This state has four regexps attached to it . More States . Re - examining the STT shows two introductory states , for input with and without a ( leading ) strict . I 've called these states by the arbitrary names initial and graph . If the initial strict is present , state initial handles it ( in the exit function ) and jumps to state graph to handle what comes next . If , however , strict is absent , state initial still handles the input , but then jumps to state graph_id . A ( repeated ) word of warning about Set::FA::Element . A loop within a state does not trigger the exit - state and enter - state functions . Sometimes this can actually be rather unfortunate . You may have to use this technique yourself . Be aware of it . Proceeding in this fashion , driven by the BNF of the input language , eventually you can construct the whole STT . Each time a new enter - state or exit - state function is needed , write the code , then run a small demo to test it . There is no substitute for that testing . The graph State . You reach this state simply by the absence of a leading strict in the input stream . Apart from not bothering to cater for comments ( as did the initial state ) , this state is really the same as the initial state . A few paragraphs back I warned about a feature designed into Set::FA::Element , looping within a state . That fact is why the graph state exists . If the initial state could have looped to itself upon detecting strict , and executed the exit or entry functions , there would be no need for the graph state . The graph_id State . Next , look for an optional graph i d , at the current head of the input stream ( because anything which matched previously is gone ) . Here 's the first use of a formula : Cell H2 contains ( ? : \\\\. : \\\\. The Remaining States . What follows in the STT gets complex , but in reality is more of the same . Several things should be clear by now : . The development of the STT is iterative . You need lots of tiny but different test data files , to test these steps . You need quite a lot of patience , which , unfortunately , ca n't be downloaded from the internet ... . Lexer Actions ( Callbacks ) . Matching something with a DFA only makes sense if you can capture the matched text for processing . Hence the use of state - exit and state - entry callback functions . In these functions , you must decide what text to output for each recognized input token . To help with this , I use a method called items ( ) , accessed in each function via $ myself . This method manages an stack ( array ) of items of type Set::Array . Each element in this array is a hashref : . Whenever a token is recognized , push a new item onto the stack . The value of the type string is the result of the DFA 's work identifying the token . This identification process uses the first of the two sub - grammars mentioned in the first article . A long Exit - state Function . The save_prefix function looks like : . # Warning : This is a function ( i.e. not a method ) . # Note : Because this is a function , $ myself is a global alias to $ self . A tiny Exit - state Function . Here 's one of the shorter exit functions , attached in the STT to the open_brace and start_statement states : . The code to push a new item onto the stack is just : . Using Marpa in the Lexer . Yes , you can use Marpa in the lexer , as discussed in the first article . I prefer to use a spreadsheet full of regexps -- but enough of the lexer . It 's time to discuss the parser . The Parser 's Structure . The parser incorporates the second sub - grammar and uses Marpa::R2 to validate the output from the lexer against this grammar . The parser 's structure is very similar to that of the lexer : . Initialize using the parameters to new ( ) . Declare the grammar . Run Marpa . Save the output . Marpa Actions ( Callbacks ) . As with the lexer , the parser works via callbacks , which are functions named within the grammar and called by Marpa::R2 whenever the input sequence of lexed items matches some component of the grammar . Consider these four rule descriptors in the grammar declared in GraphViz2::Marpa::Parser 's grammar ( ) method : . , ... ] . In each case the lhs is a name I 've chosen so that I can refer to each rule descriptor in other rule descriptors . That 's how I chain rules together to make a tree structure . ( See the Chains and Trees section of the previous article . ) This grammar fragment expects the input stream of items from the lexer to consist ( at the start of the stream , actually ) of three components : a strict thingy , a digraph thingy , and a graph_id thingy . Because I wrote the lexer , I can ensure that this is exactly what the lexer produces . These latter three come from the type key in the array of hashrefs built by the lexer . The three corresponding value keys in those hashrefs are yes or no for strict , yes or no for digraph , and an i d or the empty string for graph_id . As with the lexer , when in incoming token ( type ) matches expectations , Marpa::R2 triggers a call to an action , here called ( for clarity ) the same as the rhs . Consider one of those functions : . The parameter list is courtesy of how Marpa::R2 manages callbacks . $ t1 is the incoming graph i d . In data/27 . gv ( shown earlier ) , that is graph_27 . Marpa does not supply the string graph_id to this function , because there 's no need . I designed the grammar such that this function is only called when the value of the incoming type is graph_id , so I know precisely under what circumstances this function was called . That 's why I could hard - code the string graph_id in the body of the graph_id ( ) function . The Grammar in Practice . Now you might be thinking : Just a second ! That code seems to be doing no more than copying the input token to the output stream . Well , you 're right , sort of . True understanding comes when you realize that Marpa calls that code only at the appropriate point precisely because the type graph_id and its value graph_27 were at exactly the right place in the input stream . By that I mean that the location of the pair : . in the input stream was exactly where it had to be to satisfy the grammar initialized by Marpa::R2::Grammar . The role of the lexer as an intermediary is to simplify the logic of the code as a whole with a divide - and - conquer strategy . In other words , it 's no accident that that function gets called at a particular point in time during the parser 's processing of its input stream . Consider another problem which arises as you build up the set of rule descriptors within the grammar . Trees Have Leaves . The first article discussed chains and trees ( see the prolog_definition mentioned earlier in this article ) . Briefly , each rule descriptor must be chained to other rule descriptors . The astute reader will have already seen a problem : How do you define the meanings of the leaves of this tree when the chain of definitions must end at each leaf ? Here 's part of the data/27 . lex input file : . The corresponding rules descriptors look like : . The items marked as terminals ( standard parsing terminology ) have no further definitions , so attribute_key and attribute_val are leaves in the tree of rule descriptors . What does that mean ? The terminals attribute_id and attribute_value must appear literally in the input stream . Switching between attribute_key and attribute_id is a requirement of Marpa to avoid ambiguity in the statement of the grammar . Likewise for attribute_val and attribute_value . The min makes the attributes mandatory . Not in the sense that nodes and edges must have attributes , they do n't , but in the sense that if the input stream has an attribute_id token , then it must have an attribute_value token and vice versa . Remember the earlier section \\\" Working With An Incomplete BNF \\\" ? gv file used one of : . ... then the one chosen really represents the graph attribute : . To make this work , the lexer must force the output to be : . This matches the requirements , in that both attribute_id and attribute_value are present , is their ( so to speak ) owner , the object itself , which is identified by the type class_id . All of this should reinforce the point that the design of the lexer is intimately tied to the design of the parser . By taking decisions like this in the lexer you can standardize its output and simplify the work that the parser needs to don . Where to go from here . The recently released Perl module MarpaX::Simple::Rules takes a BNF and generates the corresponding grammar in the format expected by Marpa::R2 . This is a very interesting development , because it automates the laborious process of converting a BNF into a set of Marpa 's rule descriptors . Consequently , it makes sense for anyone contemplating using Marpa::R2 to investigate how appropriate it would be to do so via MarpaX::Simple::Rules . Wrapping Up and Winding Down . You 've seen samples of lexer output and some parts of the grammar which both define the second sub - grammar of what to expect and what should match precisely the input from that lexer . If they do n't match , it is in fact the parser which issues the dread syntax error message , because only it ( not the lexer ) knows which combinations of input tokens are acceptable . Just like in the lexer , callback functions stockpile items which have passed Marpa::R2 's attempt to match up input tokens with rule descriptors . This technique records exactly which rules fired in which order . After Marpa::R2 has run to completion , you have a stack of items whose elements are a ( lexed and ) parsed version of the original file . Your job is then to output that stack to a file , or await the caller of the parser to ask for the stack as an array reference . From there , the world . The Lexer and the State Transition Table - Revisited . The complexity of the STT in GraphViz2::Marpa justifies the decision to split the lexer and the parser into separate modules . Clearly that will not always be the case . Given a sufficiently simple grammar , the lexer phase may be redundant . Consider this test data file , data / sample.1 . ged , from Genealogy::Gedcom : . + ) $ / : an integer , a keyword , and a string . In this case I 'd skip the lexer , and have the parser tokenize the input . So , horses for courses . ( GEDCOM defines genealogical data ; see the GEDCOM definition for more details ) . \"}",
        "_version_":1692668998774161408,
        "score":22.203657},
      {
        "id":"1395bedd-baa5-47ec-a1d5-7f5d0b48d024",
        "_src_":"{\"url\": \"http://www.automoblog.net/2014/01/19/nani-roma-car-category-winner-2014-dakar-rally/\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701165378.58/warc/CC-MAIN-20160205193925-00036-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Multiple sequence alignments ( MSAs ) have become one of the most studied approaches in bioinformatics to perform other outstanding tasks such as structure prediction , biological function analysis or next - generation sequencing . However , current MSA algorithms do not always provide consistent solutions , since alignments become increasingly difficult when dealing with low similarity sequences . As widely known , these algorithms directly depend on specific features of the sequences , causing relevant influence on the alignment accuracy . Many MSA tools have been recently designed but it is not possible to know in advance which one is the most suitable for a particular set of sequences . In this work , we analyze some of the most used algorithms presented in the bibliography and their dependences on several features . A novel intelligent algorithm based on least square support vector machine is then developed to predict how accurate each alignment could be , depending on its analyzed features . This algorithm is performed with a dataset of 2180 MSAs . The proposed system first estimates the accuracy of possible alignments . The most promising methodologies are then selected in order to align each set of sequences . Since only one selected algorithm is run , the computational time is not excessively increased . Full - text . However , current MSA algorithms do not always provide consistent solu- tions , since alignments become increasingly difficult when dealing with low similarity sequences . As widely known , these algorithms directly depend on specific features of the sequences , causing relevant influence on the alignment accuracy . Many MSA tools have been recently designed but it is not possible to know in advance which one is the most suitable for a particular set of sequences . In this work , we analyze some of the most used algorithms presented in the bibliography and their dependences on several features . A novel intelligent algorithm based on least square support vector machine is then developed to predict how accurate each align- ment could be , depending on its analyzed features . This algorithm is performed with a dataset of 2180 MSAs . The proposed system first estimates the accuracyofpossible promising methodologies are then selected in order to align each set of sequences . Since only one selected algorithm is run , the computational time is not excessively increased . alignments . Themost INTRODUCTION Multiple sequence alignment ( MSA ) is a widely used approach in the current molecular biology . This technique involves the comparison of new sequences with well- known ones , extracting their shared information and their significant differences ( 1 ) . MSA methods have trad- itionally been essential for analyzing biological sequences and designing applications in structural modeling , func- tional prediction , phylogenetic analysis and sequence database searching ( 2 ) . Current MSA tools are also applied to comparisons of protein structures ( 3 ) , recon- structions of phylogenetic trees ( 4 ) or predictions of mu- tations ( 5 ) and interactions ( 6 ) . More recently , the interest of MSA methodologies has evenincreaseddue techniques . Current technologies provide a large amount of data that must be analyzed , processed and assessed . Consequently , newcomputational required to extract biological meanings from such infor- mation . Thus , supervised learning algorithms have been widely implemented in the analysis of genomic and prote- omic experimental data . Additionally , recent experimental methods also retrieve further biological data , which is useful for extending the information included within align- ment methods . Thus , current MSAs tools take advantage of heterogeneous features , which are provided by recent biological progress in functional , structural and genomic researches , to obtain more accurate alignments within a reasonable time ( 7 ) . Therefore , MSAs are becoming one of the more powerful and essential procedures of analysis ( 8) . Traditionally , alignment strategies are mainly incor- porated in progressive algorithm and consistency - based methods ( 7 ) . Progressive algorithms assemble previously built pairwise alignments through a clustering method and store their evaluations in a library . Some well - known programs using progressive strategies are ClustalW ( 9 ) or Muscle ( 10 ) . Tel : +34 958 241778 ; Fax : +34 958 248993 ; Email : fortuno@atc.ugr.es Published online 11 October 2012 Nucleic Acids Research , 2013 , Vol . 41 , No . 1e26 doi:10.1093/nar / gks919 ? The Author(s ) 2012 . Published by Oxford University Press . Page 2 . consideration not only the previous pairwise alignments but also if these alignments are consistent with the final result . However , neither progressive nor consistency - based methods build optimal alignments when sequences are dis- tantly related . More recent 3D - COFFEE ( 13 ) or Promals ( 14 ) , include further infor- mation ( structure , domains or homologies ) in addition to the provided sequences . Such features are usually found by experimental resources in databases such as Protein Data Bank ( PDB ) ( 15 ) , Uniprot ( 16 ) or Pfam ( 17 ) . Nevertheless , the consumed time is still excessive for these strategies and improvements are only relevant when sequences are evolutionarily less related ( 7 ) . Therefore , currentlythere methodologies based on different strategies . Moreover , each MSA tool usually depends on particular features ; thereby , there is no consensus about which one produces more accurate alignments ( 18,19 ) . A new intelligent algo- rithm based on least square support vector machine ( LS - SVM ) is proposed here in order to predict how accur- ately each MSA tool will align a set of sequences . Interesting features related to the sequences and their products have been added from several resources in order to make this prediction . To the best of our know- ledge , there are no similar studies in the current bibliog- raphy , which address the prediction of the alignment accuracy . This algorithm also estimates which methodo- logies are more significant to align those sequences . The system has been created from 218 sets of sequences provided by the BAliBASE benchmark ( 20 ) and their cor- responding features . Since our algorithm applies a priori features to predict the accuracy , only the best method is run . Consequently , the CPU cost is not excessively increased . approaches , suchas are manyalignment MATERIALS AND METHODS In this work , a novel system called ' Prediction of Accuracy in Alignments based on Computational Intelligence ' ( PAcAlCI ) is developed . PAcAlCI is composed by four independent modules ( Figure 1 ) . First , 218 groups of sequences are aligned through 10 different methodologies , producing a dataset of 2180 alignments ( ' Input Dataset ' module ) . Alignments are then evaluated in order to measure their accuracies . From these groups of sequences , several features are also retrieved from various relevant databases ( ' Feature Extraction ' module ) . The most useful features are progressively included in a subset which is used by the subsequent algorithm ( ' Feature Selection ' module ) . Finally , selected features are added to an LS - SVM model to predict alignment accuracies and , subsequently , the most suitable methodologies ( ' LS - SVM Prediction ' module ) . The PAcAlCI system was completely implemented with Matlab ( Version R2010b ) . es/ ? fortuno / PAcAlCI / PAcAlCI . zip . Input dataset A set of sequences must be considered in order to compare different alignment algorithms and develop the proposed prediction . Several datasets and techniques have usually been developed to standardize the comparison of align- ment results , e.g. Oxbench ( 21 ) , HOMSTRAD ( 22 ) , Prefab ( 10 ) or BAliBASE ( 20 ) . In this work , the BAliBASE benchmark ( v3.0 ) was chosen . BAliBASE defines several groups of sequences that can be easily aligned by standard algorithms . This dataset includes a total of 218 sets of sequences that were manually extracted from different databases , specifically the PDB ( 15 ) . This benchmark also provides a set of handmade reference alignments ( gold standard ) in order to compare them with the alignments obtained by other tools . Thus , BAliBASE calculates a Sum - of - Pairs ( SP ) score to evaluate such alignments . These SP scores are used by our system to measure the quality of each alignment . PAcAlCI scheme . The architecture is developed into four modules : input dataset , feature extraction , feature selection and LS - SVM prediction . e26 Nucleic Acids Research , 2013,Vol . 41,No . 1PAGE 2 OF 10 . Page 3 . include additional information . These questions will be solved in the ' Comparison of MSA Methodologies ' section . MSA methodologies Ten of the most relevant MSA tools are selected to be included in PAcAlCI . These tools are classified according to their implemented strategy : progressive techniques , consistency - based methods or algorithms including add- itional information ( see summary in Table 1 ) . Programs were run with their default features . Among the progres- sive methods , ClustalW ( 9 ) , Muscle ( 10 ) , Kalign ( 23 ) , Mafft ( 24 ) and RetAlign ( 25 ) were chosen . ClustalW designs a tree - computing algorithm to find the alignment by means of distance scores and a gap weighting scheme . Muscle develops a strategy based on three stages , where a quickly built alignment is refined with an iterative method and a tree - dependent partitioning approach . Kalign uses theWu - Manber string - matching improve the distance calculation of the classical progres- sive approach . Mafft identifies common homologies in se- quences through a fast Fourier transform , significantly reducing the computational cost . Lastly , RetAlign imple- ments a progressive corner - cutting algorithm to identify optimal alignments in a network of possible alignments . Otherthree consistency - based included in PAcAlCI : T - Coffee ( 11 ) , ProbCons ( 27 ) and Fast Statistical Alignment ( FSA ) ( 28 ) . T - Coffee develops a standard consistency algorithm , building pairwise align- ments and evaluating them against third sequences . ProbCons defines a probabilistic consistency based on a pair of hidden Markov models ( pair - HMMs ) to perform a novel scoring scheme for the standard consistency library . FSA estimates the insertion and deletion processes in sequences through pair - HMMs to combine their probabilities into alignments . Finally , two more complex methodologies , namely 3D - Coffee ( 13 ) and Promals ( 14 ) were also applied . 3D - Coffee introduces structural information in the standard T - Coffee evaluations from the PDB ( 15 ) , per- forming comparisons between each two structures and each sequence with its structure . On the other hand , Promalsadds information algorithm(26)to approacheswere basedon homologies , combining sequences and homologies in profiles through HMMs . Databases and feature extraction Features of BAliBASE sequences are extracted from well - known biological databases . Such databases are con- sulted to obtain interesting data which complement the sequences and to build a complete set of features . The final dataset will be composed by 23 features ( see summary in Table 2 ) . Some features related to sequences , domains , amino acid types or structures have already been successfully includedinother similar ( 18,29 ) . However , the set of features is complemented with further information based on other studies such as protein interaction prediction ( 30 ) or protein model clas- sification ( 31 ) . Therefore , a more complete feature envir- onment is presented in this work in order to study its relevance to sequence alignments . Here below , each con- sulted database is described , indicating which features have been retrieved and their nomenclature in the feature list : knowledge - based systems . BAliBASE ( 20 ) can be considered the first consulted database , as it provides the sequences that are aligned . Then , the featuresassociated sequences are the number of sequences ( f1 ) , the average length of sequences ( f2 ) and the normalized witheach setof Table 2 . Summary of features extracted from several databases FeatureSource RangeType Rank f1 f2 f3 Number of sequences BAliBASE Average length Variance length ( normalized ) Reference subset AA in ? -helixa AA in ? The rele- vance ranking was also measured according to the mRMR procedure . aThese features are calculated as the percentage of amino acids ( AA ) with that specific feature . number of occurrences per sequence . bThese features are calculated as the Table 1 . Their versions and the applied strategies are also shown . PAGE 3 OF 10Nucleic AcidsResearch , 2013 , Vol.41,No . 1 e26 . Page 4 . variance of the sequence length ( f3 ) . This information determines whether there is any dependence between alignment tools and the number / length of sequences . Since BAliBASE classifies sequences according to certain features ( see the ' Input Dataset ' section for details ) , the subset , where each set of sequences is included , is proposed as another feature ( f4 ) . Uniprot ( 16 ) consists of a wide repository of proteins with accurate , consistent and rich annotation . Several features are calculated from this database : the percent- age of amino acids in ? -helix structures ( f5 ) , the per- centage of amino acids in ? -strand structures ( f6 ) and the percentage of amino acids in the transmembrane region ( f7 ) . Data associated with similar secondary structures or locations usually indicate more related sequences or regions . Pfam ( 17 ) identifies common functional regions in families , also called domains . Domain features are per- formed from this database as : average number of domains per sequence ( f8 ) and average number of shared domains ( between each pair of sequences ) per sequence(f9 ) . Domain regions with related functionality . This functionality can be useful to understand how some sequences must be efficiently aligned or how close sequences are in their families . The Gene Ontology Annotation controlled vocabularies for the annotations of molecu- lar attributes in different model organisms . These vocabulariesare classified ontologies organized as a directed acyclic graph ( DAG ) : molecular function ( MF ) , cellular component ( CC ) and biological process ( BP ) . similaritiesusually imply ( 32 ) provides into three structured of eachprotein . The Apart from these databases , other resources have been applied in order to complete the set of features . Finally , the MSA method being executed ( see the ' MSA methodologies ' section ) is the last included feature(f23 ) . Thisfeature proposed system , as the purpose is to predict the isdeterminant in the accuracy features . Besides , the most suitable methods according to the predicted accuracies are selected from that feature . Also , the accuracies of each method are included as outputs . As explained before , this accuracy is called SP score by BAliBASE and it is defined as a similarity value against the gold - standard references . of eachmethod according toall these Feature selection based on mutual information The relevance of the previously proposed features is analyzed through a feature selection procedure . Feature selection algorithms allow reducing the number of features , filtering out those irrelevant or redundant . One of the well - known feature selection , called minimal - redun- dancy - maximal - relevance ( mRMR ) ( 34 ) , is applied in this work . First , this approach calculates the relevance of the features by using their mutual information . The obtained relevance is then assessed through the subsequent machine - learning procedure . The aim of mRMR is to select a feature at a time with a first - order incre- mental search , trying to avoid redundant features ( see Supplementary mRMR feature selection for details ) . Discrete and continuous random variables are both con- sidered in the mRMR algorithm . Such property is essen- tial in the proposed set of features , since both types of variables were included ( ' real ' and ' integer ' types in Table 2 ) . Besides , the output accuracy is also defined as a continuous variable . The mRMR method achieves a great accuracy in a reduced time . Thus , the algorithm is useful to accurately select features among a huge number of them . The proposed features arethen Subsequently , the LS - SVM model is trained and evaluated progressively increasing the number of features from this ranking . ranked frommRMR . Least squares support vector machine Features selected before are included in an LS - SVM model ( 35 ) in order to estimate different alignment accuracies . Subsequently , the algorithm also determines which tools are more likely to obtain the best alignment in term of accuracy . LS - SVMs models were generally designed for prediction approaches , but they present the most effective performance for regression problems . Since our system includes continuous values of accuracy , the proposed prediction is defined as a regression problem . Additionally , as the order of input data in LS - SVM is arbitrary , any change of the order would not affect the modeling result ( 35,36 ) . Therefore , the ap- plication of LS - SVM would be an effective and faithful solution . A kernel model must also be selected to correctly design the prediction method based on LS - SVM . The radial basis function ( RBF ) kernel was chosen to be applied in the proposed methodology . Additionally , LS - SVMs based on RBF kernels must also be performed by two kinds of hyper - parameters : the regulation parameter and the kernel parameters . These hyper - parameters were optimized by cross - validation in the proposed LS - SVM system ( details about kernels and hyper - parameters in LS - SVMs are e26Nucleic Acids Research , 2013,Vol . 41,No . 1PAGE 4 OF 10 . Page 5 . provided in the Supplementary LS - SVM models ) . The LS - SVM algorithm is developed here from the Matlab toolbox found in ( 37 ) . In order to assess the LS - SVM model , a 10-fold cross - validation procedure is performed . This procedure randomly divides the complete dataset ( 2180 problems ) into 10 subsets of 218 problems . Nine subsets are then applied to train the proposed system . The training pro- cedure includes the most relevant features and the pos- terior accuracy for each problem in the subset . Thus , hyper - parameters are tuned and the LS - SVM model is estimated . Subsequently , the last subset is used to test the estimated LS - SVM model . The accuracies from such subset are then predicted and compared with those already known . The training and test procedures are repeated 10 times with the 10 different subsets . The predicted accuracies are then validated by their errors against real ones . The prediction error is measured by means of the ' mean relative error ' ( MRE ) . Taking into account this errorvalue , proposed to select the most suitable methodologies . For a specific set of sequences , those methodologies whose accuracies exceed a confidence value ( ? s ) are selected ( see the MRE and ? s equations in the Supplementary LS - SVM validation ) . a confidence intervalis RESULTS AND DISCUSSION Comparison of MSA methodologies As described before , each MSA method proposes different solutions depending on certain conditions or features . For this reason , biologists and researchers do not agree with a generally accepted solution ( 19 ) . Some methods have been developed to unify criteria and choose the most suitable alignment tool ( 21,22 ) , but this is currently an open issue . In order to understand the performance of MSAs , accuracies from several methodologies can be compared . Previous reviews ( 7,8 ) have already compared accuracies from BAliBASE subsets ( SP scores ) against the applied strategy ( progressive , consistency - based or approaches with additional data ) . Generally , SP scores are quite similar independently of the methodologies . However , these strategies including add- itional data are clearly in disadvantage in terms of required time ( 7 ) . Thus , we could suggest that , only in special cases with less related sequences , additional data are clearly useful . This analysis supports the idea of using a system to previously decidewhich promising to obtain better alignments . Here , PAcAlCI predicts accuracies to decide whether differences are enough to select more sophisticated methods against faster ones . Therefore , this system not only predicts the most relevant methodologies , but it also estimates differ- ences between alignment performances . We could then decide which method constructs an accurate enough align- ment according to its predicted accuracy . methodologies are most Selection of feature subset The complete dataset was composed by 2180 different inputs . Such inputs were retrieved from the 218 groups of sequences of BAliBASE . They were then aligned by the 10 previously proposed algorithms . For each input alignment , a set of 23 features was also retrieved . Output values were represented by the 2180 accuracies calculated from the input alignments . As described above , the mRMR algorithm ( 34 ) was applied to select significant features . That procedure returned a ranking of features according to their relevance against calculated accuracies . An increasingly higher subset of features was then included in the subsequent system . According to this ranking ( Table 2 ) , the most relevant features were ' the number of domains ' ( f8 ) and ' the applied methodology ' ( f23 ) . Regarding the first one , domains can be considered a measure of how deeply se- quences are known . Domains are also associated with functional relationships and they involve more conserved sequence sections . Then , sequences that include more number of domains will be harder to align and , subse- quently , the system could provide accurate predictions . On the other hand , the second feature is an essential variable because it is including obligatory information . This feature must always be included in order to know for which methodology the prediction is done , developing a robust and coherent system of prediction . The features related to sequences , ' the number of sequences ' ( f1 ) and ' the average / variance of the length ' ( f2,f3 ) , were also ranked among first positions in the ranking . These features are highlighted because the avail- ability to obtain accurate alignments directly depends on sequence properties . Other features less related to se- quences but including amino acid information were found in the first half of the ranking . Features such as ' types of amino acids ' ( f18 ? f22 ) or ' the secondary struc- ture ' ( f5,f6 ) provide complementary information about the composition and formation of sequences . Thus , they can also be helpful to efficiently predict some similarities . Additionally , it is also important to analyze the occur- rences in BAliBASE of the selected features in order to understand the obtained feature selection . BAliBASE se- quences usually have known secondary structures ( ? -helix or ? -strand ) or GO terms . However , PAcAlCI was also trained with a few datasets from BAliBASE where these features are not known ; thereby , cases without this infor- mation were also considered . Thus , new datasets from users not including that information can also be accurately estimated , returning their predicted accuracies and a set of suitable methods to use . On the other hand , datasets associated with other features regions ) are considerably less included in BAliBASE . Consequently , the significance obtained for such feature was considered irrelevant and it was discarded from the selection procedure ( for example , the ' transmembrane amino acids ' feature was ranked in the 22nd position ) . ( e.g. transmembrane Prediction of alignment accuracy Features previously analyzed were added to the subse- quent LS - SVM model . PAcAlCI then predicted the PAGE 5 OF 10 Nucleic AcidsResearch , 2013 , Vol.41,No . 1e26 . Page 6 . accuracy which each methodology returned for a set of sequences . As far as we are concerned , similar accuracy predictions in MSAs have not been addressed before . PAcAlCI was performed using an incremental combin- ation of features in ascendant relevance order . Such com- bination was applied adding a feature at a time according to the previous ranking . Finally , a 10-fold cross - validation was performed to assess the algorithm . The prediction error ( MRE ) was calculated for the training and test sets . The evolution of the errors for each combination of features is shown in Figure 2 . According to such evolu- tion , the error progressively decreases with higher number of features . However , an almost optimal value is reached from around 10 features . The prediction error is then kept around 6 % for the training data and 9 % for the test data . So , we could suggest that all features are not necessary to obtain the optimal prediction . A smaller number of features was then used to perform the system without lack of accuracy . Specifically , the 10 most relevant features were added to the LS - SVM model . According to this configuration , accuracies predicted from four sets of sequences are shown in Table 3 . The total MRE value returned by PAcAlCI was 0.0587 for the training set and 0.1012 for the test . This error is distributed along the 2180 predicted accuracies as shown in Figure 3 . Analyzing more deeply the proposed system , higher error values are less frequent and they are usually associated with low accuracies ( see detail in Figure 3 ) . Alignments with low accuracies are less meaningful in our system , as their performances are totally unaccept- able . Consequently , they could not even be considered in PAcAlCI . A minimal accuracy value , called ? , was then defined as a threshold . Thus , the LS - SVM model only kept the most accurate alignments , filtering out the re- maining ones . This threshold allowed improving the sub- sequent prediction . For example , if ? \\u00bc 0:5 , the MRE value using 10 input features decreased to 0.0340 in the training set and to 0.0608 in the test . These errors improved because low values of accuracy , which led to highly wrong predic- tions , were previously filtered ( see the new distribution of errors in Figure 4 ) . Prediction errors are now considered low enough to adequately determine differences between methodologies . Then , the most suitable alignments can be selected according to their predicted accuracies . Selection of alignment methods In occasions , several alignment methodologies obtain quite similar accuracies ; thereby , no one significantly stands out from the rest . Consequently , as in other few researches ( 29,38 ) , the most promising MSA tools can be selected according to several features . In this case , a confidence interval was defined to decide those methodologies which acceptably align a set of sequences . The confidence interval covers those accuracy values that Table 3 . Accuracies obtained for four different sets of sequences AlignmentMethod Real Acc . Pred . Acc . Rel . Values in bold show accuracies included in the confidence interval . The prediction error is also measured . 05 10 1520 25 0.05 0.1 0.15 0.2 0.25 NUMBER OF FEATURES USED FOR LSSVM MEAN ABSOLUTE PERCENTAGE ERROR Training Test Figure 2 . Evolution of the MRE . The number of features progressively increases in ascendant relevance order . Training and test errors are shown . e26Nucleic Acids Research , 2013,Vol . 41,No . 1PAGE 6 OF 10 . Page 7 . are higher than a confidence value ? s ( see its formal definition inSupplementary Those methodologies whose accuracies exceed such confi- dence value were chosen as candidate methods . This confidence interval was applied to real accuracies and predicted accuracies . Two sets of suitable metho- dologies were then retrieved ( real and predicted sets ) . Thus , thenumber ofselected variable for each group , as it depends on how similar accuracies were in that specific problem . Both groups were then compared in order to know how many methodologies were correctly selected in the predicted LS - SVMvalidation ) . methodologieswas set ( see four examples in Figure 5 ) . For example , using accuracies from the performance of 10 features without ? threshold , the 83.55 % of predicted methodologies were also included in the real group . When accuracies were predicted with the limitation ? \\u00bc 0:5 , the percentage ofsuccessfully selectedmethodologies 85.89 % . Therefore , the proposed system usually per- formed an accurate group of outstanding methodologies . As shown in the examples of Figure 5 , methodologies including additional information , namely 3D - Coffee and Promals , were selected for sequences with low similarity ( RV11 subset ) . In these cases , more commonly used increasedto 0 0.10.20.30.4 0.5 0.60.7 0.80.91 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Training Error RELATIVE ERROR FREQUENCY 00.511.5 0 200 400 600 800 1000 1200 Test Error RELATIVE ERROR FREQUENCY 0.30.40.50.6 0.7 0.80.91 0 20 40 60 80 0.40.60.811.2 1.4 0 10 20 Figure 4 . Distribution of relative errors for training and test sets . Low accuracies were previously filtered to improve the LS - SVM prediction , avoiding prediction with high errors ( ? 0 0.10.2 0.30.4 0.50.6 0.70.80.91 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Training Error RELATIVE ERROR FREQUENCY 0 0.511.5 0 200 400 600 800 1000 1200 Test Error RELATIVE ERROR FREQUENCY 0.30.4 0.50.60.70.80.91 0 20 40 60 80 0.40.60.811.21.4 0 10 20 Figure 3 . Distribution of relative errors for training and test sets . The corresponding LS - SVM prediction was performed using 10 features . PAGE 7 OF 10Nucleic AcidsResearch , 2013 , Vol.41,No . 1e26 . Page 8 . aligners ( ClustalW , Kalign or Muscle ) were not selected , as they did not build accurate enough alignments . However , these more complex methods ( 3D - Coffee and Promals ) did not significantly outperform other faster methods when se- quences were more related . Consequently , we could again suggest that the prediction system is working as expected . For instance , those datasets including more than two domains per sequence selected Kalign as a suitable method ( in the 80.95 % of cases ) , whereas Mafft was appropriate for datasets with less domains ( 78 % of datasets selected it ) . Although there are other expert systems to select adequate MSA tools ( 38,39 ) , PAcAlCI was compared with AlexSys ( 29 ) , as it performs a more similar strategy ( see comparison in Table 4 ) . However , comparing both methodologies can be complicated . Both systems develop similar machine - learning approaches , but their objectives are quite different . AlexSys defines a decision - tree approach to predict whether sequences are ' strongly ' or ' weakly ' aligned with each specific method ( classification problem and binary solution ) . The best method among those classified as ' strong ' is then inferred according to their success probability or their required CPU time . This binary classification can be quite subjective in some cases . Since accuracies over 0.5 are already classified as ' strong ' , quite different accuracies , e.g. 0.5 and 0.9 , are considered identical in the AlexSys approach . In a different way , PAcAlCI first predicts accuracy values ( re- gression problem and real solution ) . The accuracy predic- tion provides a relevant improvement in order to decide whether it is worth aligning with a specific methodology . Besides , suitable methods are also selected according to the best accuracies . In general , AlexSys correctly predicts the best aligner in a 45 % of its test alignments . In another 45.5 % of the alignments , the best aligner corresponded to the second predicted method . In general , global success rates in PAcAlCI are quite similar ( 83.55 % or 85.89 % depending on the ? threshold ) , although the number of suitable methods is usually higher in our prediction . Regarding the included tools , PAcAlCI is composed by a wider group of previous methodologies ( 10 approaches compared with the six of AlexSys ) , including more complex ones as 3D - Coffee or Promals . Despite these differences , both methods may be con- sidered complementary , as both perform accurate classi- fiers but in different contexts . In any case , the final decision of selecting the most suitable methodology among the proposed ones can rely on the final user of this system . Other criteria such as the complexity of par- ameters in the methodologies or the required time could also be taken into account in order to choose the correct tool among the selected ones . CONCLUSION MSA is currently an open issue . Alignment tools must be continually improved , as they are essential in the analysis of huge amount of data provided by next - generation sequencing and high - throughput experiments . Thus , new trends in MSAs aim to integrate the major amount of information while trying to significantly reduce the used time . For this reason , efficient computational techniques are increasingly implemented . In this work , a complete study of MSA methodologies has been developed . Relevant methodologies in this field were first compared . Several types of methods were dis- cussed and we have suggested that only in special cases more sophisticated approaches including additional infor- mation are really necessary . A novel intelligent system ( PAcAlCI ) was then proposed based on the knowledge acquired from this study . Intersection of suitable and predicted methodologies ( Venn diagrams ) corresponding to the four alignments whose accuracies are shown in Table 3 . Table 4 . Comparison between PAcAlCI and AlexSys FeaturePAcAlCIAlexSys Number of aligners Kind of problem Machine - learning strategy Values of prediction 10 Regression ( real ) LS - SVM Accuracies 6 Classification ( binary ) Decision trees Weak ( Acc . \\u00bc 0:5 ) PAcAlCI is qualitatively compared with AlexSys . The performance and attributes of both procedures are shown . e26Nucleic Acids Research , 2013,Vol . 41,No . 1PAGE 8 OF 10 . Page 9 . sequences . This information gives us an idea of how ac- curately each methodology works . The mRMR feature selection technique was first applied to 23 features previ- ously retrieved from several biological databases . We have also described how the system can be performed with only the 10 most relevant features to predict accuracies with a reasonable efficiency . Finally , we have proposed the out- standing methodologies which can be used for certain se- quences according to their predicted accuracies . In this sense , the proposed algorithm is able to successfully select the most outstanding methods according to the pre- viously predicted accuracies . SUPPLEMENTARY DATA Supplementary Data are available at NAR Online : Supplementary mRMR feature selection , Supplementary LS - SVM models , Supplementary LS - SVM validation and Supplementary References [ 40 - 47]. FUNDING The Government of Andalusia [ Project P09-TIC-175476]. Funding for open access charge : the Government of Andalusia [ Project P09-TIC-175476]. SpanishCICYT [ Project SAF2010 - 20558];the Conflict of interest statement . None declared . REFERENCES 1 . Attwood , T.K. and Parry - Smith , D.J. ( 2002 ) Introduction to Bioinformatics . Pearson Education , Prentice Hall . Pei , J. ( 2008 ) Multiple protein sequence alignment . Curr . Opin . Struct . Biol . , 18 , 382 - 386 . Gelly , J.C. , Joseph , A.P. , Srinivasan , N. and de Brevern , A.G. ( 2011 ) iPBA : a tool for protein structure comparison using sequence alignment strategies . Nucleic Acids Res . , 39 , W18-W23 . Wang , L.S. , Leebens - Mack , J. , Wall , P.K. , Beckmann , K. , dePamphilis , C.W. and Warnow , T. ( 2011 ) The impact of multiple protein sequence alignment on phylogenetic estimation . IEEE- ACM Trans . Comput . Biol . Bioinform . , 8 , 1108 - 1119 . Hicks , S. , Wheeler , D.A. , Plon , S.E. and Kimmel , M. ( 2011 ) Prediction of missense mutation functionality depends on both the algorithm and sequence alignment employed . Hum . Mutat . , 32 , 661 - 668 . Li , A.X. , Marz , M. , Qin , J. and Reidys , C.M. ( 2011 ) RNA - RNA interaction prediction based on multiple sequence alignments . Bioinformatics , 27 , 456 - 463 . Kemena , C. and Notredame , C. ( 2009 ) Upcoming challenges for multiple sequence alignment methods in the high - throughput era . Bioinformatics , 25 , 2455 - 2465 . Li , H. and Homer , N. ( 2010 ) A survey of sequence alignment algorithms for next - generation sequencing . Brief . Bioinform . , 11 , 473 - 483 . Thompson , J.D. , Higgins , D.G. and Gibson , T.J. ( 1994 ) ClustalW : improving the sensitivity of progressive multiple sequence weighting , position - specific gap penalties and weight matrix choice . Nucleic Acids Res . , 22 , 4673 - 4680 . Edgar , R.C. ( 2004 ) MUSCLE : multiple sequence alignment with high accuracy and high throughput . Nucleic Acids Res . , 32 , 1792 - 1797 . Notredame , C. , Higgins , D.G. and Heringa , J. ( 2000 ) T - Coffee : a novel method for fast and accurate multiple sequence alignment . J. Mol Biol . , 302 , 205 - 217 . Liu , Y. , Schmidt , B. and Maskell , D.L. ( 2010 ) MSAProbs : multiple sequence alignment based on pair hidden Markov models and partition function posterior probabilities . Bioinformatics , 26 , 1958 - 1964 . O'Sullivan , O. , Suhre , K. , Abergel , C. , Higgins , D.G. and Notredame , C. ( 2004 ) 3DCoffee : combining protein sequences and structures within multiple sequence alignments . J. Mol . Biol . , 340 , 385 - 395 . Pei , J. and Grishin , N.V. ( 2007 ) PROMALS : towards accurate multiple sequence alignments of distantly related proteins . Bioinformatics , 23 , 802 - 808 . Berman , H.M. , Westbrook , J. , Feng , Z. , Gilliland , G. , Bhat , T.N. , Weissig , H. , Shindyalov , I.N. and Bourne , P.E. ( 2000 ) The Protein Data Bank . Nucleic Acids Res . , 28 , 235 - 242 . Nucleic Acids Res . , 32 , D115-D119 . Finn , R.D. , Mistry , J. , Tate , J. , Coggill , P. , Heger , A. , Pollington , J.E. , Gavin , O.L. , Gunasekaran , P. , Ceric , G. , Forslund , K. et al . ( 2010 ) The Pfam protein families database . Nucleic Acids Res . , 38 , D211-D222 . Nuin , P.A.S. , Wang , Z.Z. and Elisabeth , R.M. ( 2006 ) The accuracy of several multiple sequence alignment programs for proteins . BMC Bioinformatics , 7 , 471 . Sierk , M.L. , Smoot , M.E. , Bass , E.J. and Pearson , W.R. ( 2010 ) Improving pairwise sequence alignment accuracy using near - optimal protein sequence alignments . BMC Bioinformatics , 11 , 146 . Thompson , J.D. , Koehl , P. , Ripp , R. and Poch , O. ( 2005 ) BAliBASE 3.0 : latest developments of the multiple sequence alignment benchmark . Proteins , 61 , 127 - 136 . Raghava , G.P.S. , Searle , S.M.J. , Audley , P.C. , Barber , J.D. and Barton , G.J. ( 2003 ) OXBench : a benchmark for evaluation of protein multiple sequence alignment accuracy . BMC Bioinformatics , 4 , 47 . Stebbings , L.A. and Mizuguchi , K. ( 2004 ) HOMSTRAD : recent developments of the homologous protein structure alignment database . Nucleic Acids Res . , 32 , D203-D207 . Lassmann , T. and Sonnhammer , E.L. ( 2005 ) Kalign - an accurate and fast multiple sequence alignment algorithm . BMC Bioinformatics , 6 , 298 . Katoh , K. , Misawa , K. , Kuma , K. and Miyata , T. ( 2002 ) MAFFT : a novel method for rapid multiple sequence alignment based on fast Fourier transform . Nucleic Acids Res . , 30 , 3059 - 3066 . Szabo , A. , Novak , A. , Miklos , I. and Hein , J. ( 2010 ) Reticular alignment : a progressive corner - cutting method for multiple sequence alignment . BMC Bioinformatics , 11 , 570 . Wu , S. and Manber , U. ( 1992 ) Fast text searching allowing errors . Commun . ACM , 35 , 83 - 91 . Do , C.B. , Mahabhashyam , M.S.P. , Brudno , M. and Batzoglou , S. ( 2005 ) ProbCons : probabilistic consistency - based multiple sequence alignment . Genome Res . , 15 , 330 - 340 . Bradley , R.K. , Roberts , A. , Smoot , M. , Juvekar , S. , Do , J. , Dewey , C. , Holmes , I. and Pachter , L. ( 2009 ) Fast statistical alignment . PLoS Comput . Biol . , 5 , 5 . Aniba , M.R. , Poch , O. , Marchler - Bauer , A. and Thompson , J.D. ( 2010 ) AlexSys : a knowledge - based expert system for multiple sequence alignment construction and analysis . Nucleic Acids Res . , 38 , 6338 - 6349 . Wu , X.M. , Zhu , L. , Guo , J. , Zhang , D.Y. and Lin , K. ( 2006 ) Prediction of yeast protein - protein interaction network : insights from the Gene Ontology and annotations . Nucleic Acids Res . , 34 , 2137 - 2150 . Roslan , R. , Othman , R.M. , Shah , Z.A. , Kasim , S. , Asmuni , H. , Taliba , J. , Hassan , R. and Zakaria , Z. ( 2010 ) Utilizing shared interacting domain patterns and Gene Ontology information to improve protein - protein interaction prediction . Comput . Biol . Med . , 40 , 555 - 564 . Nucleic Acids Res . , 32 , D262-D266 . Mathews , C.K. , Holde , K.E.V. and Ahern , K.G. ( 2000 ) Biochemistry . Benjamin Cummings Publishing , Redwood City , CA . PAGE 9 OF 10Nucleic AcidsResearch , 2013 , Vol.41,No . 1 e26 . Page 10 . Peng , H. , Long , F. and Ding , C. ( 2005 ) Feature selection based on mutual information criteria of max - dependency , max - relevance , and min - redundancy . IEEE Trans . Pattern Anal . Mach . Intell . , 27 , 1226 - 1238 . Suykens , J.A.K. , Van Gestel , T. , De Brabanter , J. , De Moor , B. and Vandewalle , J. ( 2003 ) Least Squares Support Vector Machines . World Scientific Pub . Co. Inc. , Singapore . Li , L. , Su , H. and Chu , J. ( 2009 ) Sparse representation based on projection method in online least squares support vector machines . J. Control Theory Appl . , 7 , 163 - 168 . ac.be/sista/lssvmlab ( 21 September 2012 , date last accessed ) . Anderson , C.L. , Strope , C.L. and Moriyama , E.N. ( 2011 ) SuiteMSA : visual tools for multiple sequence alignment comparison and molecular sequence simulation . BMC Bioinformatics , 12 , 184 . Thompson , J.D. , Muller , A. , Waterhouse , A. , Procter , J. , Barton , G.J. , Plewniak , F. and Poch , O. ( 2006 ) MACSIMS : multiple alignment of complete sequences information management system . BMC Bioinformatics , 7 , 318 . Estevez , P.A. , Tesmer , M. , Perez , C.A. and Zurada , J.M. ( 2009 ) Normalized mutual information feature selection . IEEE Trans . Neural Netw . , 20 , 189 - 201 . John , G. , Kohavi , R. and Pfleger , K. ( 1994 ) Irrelevant features and the subset selection problem . In International Conference on Machine Learning , 121 - 129 . Bins , J. and Draper , B.A. ( 2001 ) Feature selection from huge feature sets . In 8th IEEE International Conference on Computer Vision , 2 , 159 - 165 . Cover , T.M. and Thomas , J.A. ( 2006 ) Elements of Information Theory . Wiley - Interscience , New York , NY , USA . Kullback , S. ( 1997 ) Information Theory and Statistics . Courier Dover Publications , New York , NY , USA . Babich , G.A. and Camps , O.I. ( 1996 ) Weighted Parzen windows for pattern classification . IEEE Trans . Pattern Anal . Mach . Intell . , 18 , 567 - 570 . Hestenes , M.R. and Stiefel , E. ( 1952 ) Methods of conjugate gradients for solving linear systems . J. Res . Natl . Bur . Stand . , 49 , 409 - 436 . Rossi , F. , Lendasse , A. , Francois , D. , Wertz , V. and Verleysen , M. ( 2006 ) Mutual information for the selection of relevant variables in spectrometric nonlinear modelling . Chemometrics Intell . Lab . Syst . , 80 , 215 - 226 . e26 Nucleic Acids Research , 2013,Vol . 41,No . 1PAGE 10 OF 10 . Data provided are for informational purposes only . Although carefully collected , accuracy can not be guaranteed . The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor . Publisher conditions are provided by RoMEO . Differing provisions from the publisher 's actual policy or licence agreement may be applicable . [ Show abstract ] [ Hide abstract ] ABSTRACT : Nowadays , the uncovering of new functional relationships between proteins is one of the major goals of biological studies . For this task , the integration of evidences from heterogeneous data sources by means of machine learning methodologies has been demonstrated to be an effective way of providing a complete genome - wide functional network and more accurate inferences of new functional associations . This work presents a new framework to be used in Artificial Neural Networks ( ANNs ) for the task of predicting functional relationships between proteins through the integration of evidences from heterogeneous data sources . The developing of such new methodology is motivated by the problems that arise when applying ANNs to this kind of problems , namely , the computational cost of ANN optimization process due to the nature of data ( large number of instances and high dimensionality ) . The method selects smaller representative / non - random subsets from the original data set selected for ANN optimization process , resulting in a reduction of the number of data to be trained and , consequently , the computational cost . [ Show abstract ] [ Hide abstract ] ABSTRACT : Abstract- Aligning multiple nucleotide sequences is a prerequisite for many if not most comparative sequence analyses in evolutionary biology . These alignments are often recognized as representing the homology relations of the aligned nucleotides , but this is a necessary requirement only for phylogenetic analyses . Unfortunately , existing computer programs for sequence alignment are not based explicitly on detecting the homology of nucleotides , and so there is a notable gap in the existing bioinformatics repertoire . If homology is the goal , then current alignment procedures may be more art than science . To resolve this issue , I present a simple conceptual scheme relating the traditional criteria for homology to the features of nucleotide sequences . These relations can then be used as optimization criteria for nucleotide sequence alignments . I point out the way in which current computer programs for multiple sequence alignment relate to these criteria , noting that each of them usually implements only one criterion . This explains the apparent dissatisfaction with computerized sequence alignment in phylogenetics , as any program that truly tried to produce alignments based on homology would need to simultaneously optimize all of the criteria . [ Show abstract ] [ Hide abstract ] ABSTRACT : Multiple sequence alignments ( MSAs ) are currently one of the most powerful procedure in bioinformatics in order to provide additional information useful to other understanding techniques such as biological function analyses , structure predictions or next - generation sequencing . Nevertheless , current MSA methodologies are providing quite different alignments for the same set of sequences depending on some particular biological features of these sequences . For this reason , the selection of a suitable tool for aligning a specific set of sequences is an important task which has not been totally solved yet . In this work , we propose a hierarchical algorithm of several binary classifiers based on support vector machines ( SVMs ) to predict \\\" a priori \\\" the MSA tool which will provide the most accurate alignment . Firstly , a set of heterogeneous biological features related to each set of sequences are retrieved from well - known databases . Subsequently , those most significant features according to each specific aligner are included in this particular classifier . Finally , the SVM classifiers are joined to decide the most suitable method according to the quality of each classification . This procedure was assessed by the benchmark BAliBASE v3.0 and compared against other similar tools , namely AlexSys and PAcAlCI . \"}",
        "_version_":1692671201434927104,
        "score":22.202911},
      {
        "id":"5dae2f2a-2487-41bc-8633-085ac74c181d",
        "_src_":"{\"url\": \"http://commonsenseatheism.com/?p=11660\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454701146196.88/warc/CC-MAIN-20160205193906-00157-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"Abstract . Motivation : Synaptic connections underlie learning and memory in the brain and are dynamically formed and eliminated during development and in response to stimuli . Quantifying changes in overall density and strength of synapses is an important pre - requisite for studying connectivity and plasticity in these cases or in diseased conditions . Unfortunately , most techniques to detect such changes are either low - throughput ( e.g. electrophysiology ) , prone to error and difficult to automate ( e.g. standard electron microscopy ) or too coarse ( e.g. magnetic resonance imaging ) to provide accurate and large - scale measurements . Results : To facilitate high - throughput analyses , we used a 50-year - old experimental technique to selectively stain for synapses in electron microscopy images , and we developed a machine - learning framework to automatically detect synapses in these images . To validate our method , we experimentally imaged brain tissue of the somatosensory cortex in six mice . We detected thousands of synapses in these images and demonstrate the accuracy of our approach using cross - validation with manually labeled data and by comparing against existing algorithms and against tools that process standard electron microscopy images . We also used a semi - supervised algorithm that leverages unlabeled data to overcome sample heterogeneity and improve performance . Our algorithms are highly efficient and scalable and are freely available for others to use . 1 INTRODUCTION . The mammalian brain can contain hundreds of millions of neurons , each with thousands of specialized connections called synapses that enable indirect communication between cells . Estimates for the number of synapses in the mammalian brain ranges into the trillions . Synapses are essential for the transfer of information across neuronal ensembles , and individual synapses can be modulated by patterns of incoming neural activity , a phenomenon thought to underlie learning and memory . Because synapse distribution is a useful and diagnostic criterion to evaluate circuit function in learning and disease , there have been a variety of methods used to estimate synaptic connectivity or overall synapse numbers . MRI - based techniques can be used to study network function at the level of brain regions or voxels , but they do not provide enough spatial resolution to estimate neural connectivity ( Sporns , 2010 ) . Anatomically , synapse densities are measured via light - microscopy to identify specialized substructures called spines that stud the dendrites of neurons or using electron microscopy ( EM ) to identify ultrastructural features that correspond to pre- and post - synaptic elements . Since the early 1990s , bioimage informatics has emerged as an important area in the analysis of biological images ( Peng , 2008 ) . Imaging datasets are usually much larger than other high - throughput biological datasets ( e.g. confocal microscopy data can range in the hundreds of gigabytes for a single imaging session ) . Accurately identifying elements of interest ( molecules , cells , synapses , etc . ) within these massive datasets requires the development of sophisticated and efficient computational models . This often involves a classification - based strategy in which a ( small ) manually labeled training set is used to learn a general model that can be used to analyze a larger collection of images automatically . The key computational challenges involve the reliability and speed at which the analysis is done , as well as dealing with the heterogeneity of biological structures and noise present in each image . Electron microscopy data suffer particularly from these problems and often contain undesired variation in intensity and contrast within and across samples and preparations . This presents a major computational challenge because model parameters learned from one sample may not generalize to other samples . 2 APPROACH . To aid in identifying and quantifying synapses in EM images , we used a 50-year - old experimental technique ( Bloom and Aghajanian , 1966 , 1968 ) to selectively stain for synapses in any animal model ( Fig . 1 A ) . Unlike conventional EM , this protocol uses ethanolic phosphotungstic acid ( EPTA ) to pronounce electron opacity at synaptic sites by targeting specific proteins in contact zones . This technique typically leaves non - synaptic membranes ( e.g. plasma membranes , neurotubules and vesicles ) unstained , though considerable variation can exist from sample to sample . We dissected brain tissue in the mouse somatosensory cortex and performed EM experiments using the EPTA method . We used mice from different ages ( P14 , P17 and P75 ) and isolated the same cortical region in each animal to gauge variance in sample quality . Experimental technique and training data collected . ( A ) Comparison of conventional and EPTA - stained EM images shows a marked difference in clarity of synaptic structures , albeit high sample - to - sample variability . ( B ) Subset of positive and negative examples ... . To demonstrate the advantages of this protocol for large - scale synapse identification , we developed a machine - learning framework to detect synapses in these images in a high - throughput and fully - automated manner . We describe a two - step approach . First , we use a highly accurate first - pass filtration step to reduce the search space of possible synapses by 1 - 2 orders of magnitude , thereby significantly reducing false positives . Second , we train a classifier to recognize synapses using texture- and shape - based features extracted from small image patches around potential objects of interest . We show that our approach is highly accurate and that it outperforms correlation - based ( Roseman , 2004 ) techniques and an automated technique designed to detect synapses in conventional EM images ( Morales et al . , 2011 ) . To further improve classification and adjust for varying experimental conditions ( different sample , different microscope , different person performing experiments , etc . ) , we developed a model that classifies synapses in images using both labeled and unlabeled data . This type of approach is known as semi - supervised learning ( Zhu , 2005 ) , as it combines ideas from supervised learning ( classification ) and unsupervised learning ( clustering ) . This technique can be used to build more robust classifiers in cases ( such as ours ) where large imaging datasets can easily be collected but where it is much harder to manually annotate these images . By integrating unlabeled data in the learning phase , a new sample can help fine - tune parameters of a model built from a previous sample , which can improve accuracy without requiring users to manually annotate images in the new sample . Indeed , we show that a classifier learned only on labeled data from our P14 samples and tested on our P75 samples performs worse than a semi - supervised classifier that also leverages unlabeled data from P75 . 3 METHODS . First , we describe the experiments we performed to gather and process mouse brain tissue for EM imaging with EPTA , and then we describe our machine - learning framework to automatically detect synapses in these images . 3.1 Experimental approach and data collected . We gathered tissue from the mouse somatosensory cortex because it is a well - characterized anatomical area in the cortex ( Fox , 2008 ) and thus serves as an important benchmark for the validity of our experimental approach . To prepare the tissue for EM analysis , we extracted , fixed ( using 2.5 % glutaraldehyde buffered with phosphate buffered saline ) and sectioned 50-\\u00b5m thick tissue from wild - type C57bl6 mice at ages P14 , P17 and P75 with two animals per time point . This range of tissue was collected to determine whether our experimental procedure and subsequent computational analysis remained robust to natural variation in tissue samples within and across time points and to variation in the image acquisition process . Each mouse whisker is somatotopically mapped to a single neocortical column . To ensure that the same cortical region was identified in each sample , we applied a mitochondrial stain ( cytochrome oxidase ) to each section to visually identify layer 4 ( called the barrel ) of the D1 column / whisker . The barrel was extracted using a dissecting light microscope . Tissue was prepared for transmission electron microscopy in a series of steps . First , the tissue was washed with three changes ( 5 min each ) of distilled water , followed by an incubation in 0.02 % NaOH for 10 min . This latter step was absent from the original procedure of Bloom and Aghajanian ( 1966 , 1968 ) , but we found that it helped increase the contrast of synapses ( Fig . 1 ) . Second , the tissue was dehydrated with an ascending series of EtOH ( 25 , 50 , 70 , 80 , 90 and 100 % ) , followed by fixation with 1 % phosphotungstic acid ( PTA ) in 100 % EtOH . Third , a small amount , 7 \\u00b5l , of 95 % ethanol , was added to each 1000 \\u00b5l of PTA stain used , and the PTA was washed from the sample with two changes of 100 % ethanol . The specimen was observed using a transmission electron microscope , and images were taken digitally . Images were taken from a single plane , and therefore no correction was necessary to account for double - counting synapses across serially sectioned images . There was significant sample - to - sample variability in the images , but synapses were typically dark with most other biological structures washed away ( Fig . 1 A ) . The raw images were provided as input to the machine - learning algorithms described later in the text . 3.2 Strategies for effectively detecting synapses . As described earlier in the text , electron microscopy images are inherently noisy owing to variations in the samples ( e.g. different age ) , in the manual processing steps and in the image acquisition process . To overcome these issues , we developed a pipeline that uses object segmentation , background information , normalization and alignment to obtain a better feature set that can be used for effective classification . Reducing the search space of possible synapses via segmentation . One popular technique to search for objects in an image is the sliding window approach ( Szeliski , 2010 ) . In this approach , non - overlapping windows of a pre - defined size are slid across the image , and each window is classified as synapse - containing ( positive ) or not ( negative ) . One drawback to this technique is that it can double - count synapses that lie in multiple windows or miss synapses that lie adjacent to each other . A more thorough approach is to use overlapping windows , but this greatly increases computational complexity when using large images , and subsequent post - processing steps still need to account for double - counting synapses . Further , this technique creates a large imbalance of negative - to - positive examples ( windows ) , which may lead to many false positive predictions . To avoid the challenge of selecting thresholds to segment the image ( which may not generalize across samples owing to differences in intensity and enhancement ) , we adopt the contrast - limiting adaptive histogram equalization algorithm ( Zuiderveld , 1994 ) . This method enhances the contrast of each window T in the image to approximately match a flattened histogram by mapping each pixel value to its value in the cumulative distribution f ( v ) computed in T : . where represents the original histogram of the image and n is the number of pixels in the window . We also limit the enhancement by clipping the histogram at a scalar value of 0.20 before enhancement . The window is then rescaled to [ 0,255]. The equalization is performed in each local window of the image , and then the windows are combined using bilinear interpolation to eliminate artificially induced boundaries . By only considering small windows of the image , this technique prevents the overamplification of noise or artifacts that only appear in localized regions of the image . Next , we binarize the equalized image using a single , sample - independent threshold , which was determined manually to be 10 % ( i.e. only the top 10 % of pixels values are kept ) . The final segmentation is produced by computing connected components ( segments ) in the binary image . Compared with a non - overlapping sliding window approach that would produce roughly 300 windows to test per 1016 \\u00d7 1024-sized pixel image on average , our segmentation produces 25 - 35 candidate segments per image and an overall ratio of negative - to - positive examples of 9:1 . We also allow for optional filtration of segments that are too small or too large to be synapses , as defined by the user . This step is only designed to produce segments and not to normalize the image , which we do separately later in the text . To validate that the segmentation step preserves synapses , we looked at two samples ( P14 and P75 ) and manually checked what percentage of the first 100 synapses encountered were correctly segmented . We found that only 1 % ( 1 in each sample ) of the synapses were lost , and these were always due to two synapses touching each other , which caused them to be merged into a single segment . Using background cues to augment synapse identification . One key indicator to decide whether a candidate segment is a synapse is the physical context in which it lies . We consider a local 75 \\u00d7 75-pixel window around the centroid of each segment to capture information about the object and the neighborhood surrounding the object . This is important because elongated synapse - like segments may also appear within mitochondria , but such segments are always surrounded by an oval - like contour marking the boundary of the mitochondria , which can be a useful cue for classification . Similarly , dark circular spots in the nucleus may also be stained ( see Fig . 1 A ) , but their neighborhoods typically contain other such spots , which also serve as strong discriminators . These are not steadfast rules ( synapses can certainly lie adjacent to mitochondria ) , but local neighborhoods are nonetheless a strong visual cue used by EM experts when annotating images ( Arbelaez et al . , 2011 ) . Handling experimental variation across samples . The aforementioned technique produces a set of windows ( in which each candidate segment is embedded ) , but the actual pixel values within these windows may vary significantly from sample - to - sample and image - to - image . Figure 1 B exemplifies the discrepancy that can exist across both negative and positive examples in the original images . Figure 1 B shows that this significantly reduces the variance across windows , which helps features dependent on pixel intensities to be compared in an equal setting . Adjusting for synapse heterogeneity . Synapses in EM images may be angled at any 2D orientation , which may add undesirable variation in training examples . To create a more invariant set of synapses , we applied the generalized Hough transformation ( Duda and Hart , 1972 ) to automatically rotate the segment ( within the candidate window ) such that its major axis points vertically . Briefly , this is done by computing the Hough transform matrix , where entry i , j of corresponds to the number of points in the segment that fall along a line parameterized in polar coordinates as . We find the element in that corresponds to the ( peak ) line for which the most segment points lie . The corresponding is used to compute the angle of rotation . We then cropped the image to 60 \\u00d7 60 pixels to remove the effects of the interpolation ( this size is still large enough to fit almost all synapses ) . The outcome ( Fig . 1 B ) shows much greater uniformity for both synapses and non - synaptic structures compared with the original images . 3.3 Supervised learning framework and features used to detect synapses . After the processing steps aforementioned , each image is reduced to a set of candidate segments defined by a ( normalized and aligned ) square window around the centroid of the segment . Next , we build an accurate and robust classifier to discriminate between positive ( synapses ) and negative candidates using texture- and shape - based features . Texture is a common cue used by humans when manually segmenting structures from electron micrographs , and its use has become popular in many image processing tasks today ( Arbelaez et al . , 2011 ; Varma and Zisserman , 2003 ) . In their seminal article , Leung and Malik ( 2001 ) defined texture by convolving an image with a bank of 48 Gaussian filters and used the filter responses at each image location to define a ' texton ' . Textons were clustered and used to represent and classify images in a reduced dimension . Recently , more effective filter banks have been proposed to represent texture based on modeling joint distributions of intensity values over small and compact neighborhoods of the image ( as opposed to the entire image ) , an approach we also adopt here . The maximum response ( MR8 ) is one such technique that is derived from a set of 38 filters : 6 orientations \\u00d7 3 scales \\u00d7 2 oriented filters + 2 isotropic filters . By recording only the maximum response across orientations , the number of responses is reduced to 8 ( Varma and Zisserman , 2003 ) . Each pixel is now represented as an 8D vector of responses at its ( x , y ) location . For each dimension d , we compute a normalized histogram with 17 bins ( larger bin sizes yielded marginal gain in performance but increased computational complexity ) composed of responses for d over all pixels in the window . Thus , the texture of the window is represented as a 8 \\u00d7 17-sized vector in . We used the default parameters for MR8 ( Varma and Zisserman , 2003 ) . Synapses also have a characteristic shape ( typically long and elongated ) that we also attempt to capture by extracting the following 10 shape descriptors for each segment . These features operate on the binary segment only ( they ignore the intensity values of the pixels ) and therefore contribute different information than texture alone . Major axis : the number of pixels constituting the major axis of the ellipse that has the same normalized second central moments ( covariance ) as the segment . Minor axis : same as above but for the minor axis of the ellipse . Orientation : the angle between the x -axis and the major axis of the ellipse . Eccentricity : the ratio of the distance between the foci of the ellipse and its major axis length . Convex area : the number of pixels in the convex hull of the segment . Solidity : the proportion of pixels in the convex hull that are also in the segment . Diameter : the diameter of the circle with the same area as the segment . Extent : the ratio of pixels in the segment to pixels in the smallest bounding box of the segment . The final feature we use is the histogram of oriented gradients ( HoG ) descriptor proposed by Dalal and Triggs ( 2005 ) . We used nine orientation bins , cell and block sizes of 10 \\u00d7 10 and 6 \\u00d7 6 , respectively , and a value of 0.2 for clipping the L2-norm . Intuitively , this 334D feature describes the appearance of an object by concatenating the distributions of intensity gradients ( edge directions ) in different subregions of the window . This descriptor has been shown to outperform other popular feature sets ( e.g. PCA - SIFT and generalized Haar wavelets ) in a variety of object detection tasks ( Dalal and Triggs , 2005 ; Skibbe et al . , 2011 ) . In total , each window is represented by a 480D feature vector in . All features are scaled to lie in [ 0 , 1]. These features were then used to build a support vector machine ( SVM ) classifier ( Chang and Lin , 2011 ) using a radial basis function kernel , and we performed a grid search to optimize parameters of the model . We also learned a Random Forest ( Breiman , 2001 ) classifier and an AdaBoost ensemble model ( Freund and Schapire , 1995 ) using 100 trees / learners , respectively . An overview of the supervised algorithm is shown in Figure 2 . Main steps of the supervised synapse - detection algorithm and corresponding pseudocode . High - contrast objects are automatically segmented using histogram equalization . Shape - based features are extracted from these objects , as well as texture - based features ... . 3.4 Semi - supervised learning . The supervised algorithm described earlier in the text only uses labeled data to train a classifier . We now show how unlabeled data can be used to further improve the classifier using co - training . The co - training algorithm ( Blum and Mitchell , 1998 ) assumes that features can be split into two different and fairly independent sets ( in our case , texture and shape ) . Blum and Mitchell ( 1998 ) proved that for a classification problem with two such feature sets , the target concept can be learned based on a few labeled and many unlabeled examples , provided that the views are compatible and uncorrelated . The compatibility condition requires that all examples are identically labeled by the two classifiers ( one for each of the feature sets ) . The uncorrelated condition means that for any pair of features , the two sets of features are independent , given the label . In real applications , these two conditions are rarely satisfied simultaneously . For example , in our task , compatibility may be hindered due to noise and imaging artifacts . Still , co - training has proven useful in several real - world applications ( Zhu , 2005 ) . In a co - training algorithm , two separate classifiers are trained with the labeled data using the two subfeature sets , respectively . Each classifier is applied to the unlabeled examples , and it outputs a list of positive ( putative synapses ) and negative examples , ranked by confidence of assignments . We consider all the predictions in each list above a given threshold and any example for which both independent classifiers agree on its label is added to the labeled dataset . We iterate this process once and then retrain a final single classifier using all features from the original set of labeled examples and the new ( predicted ) set of labeled examples obtained during this iterative process . In addition to our attempts to normalize each image , this approach provides another way to account for variability in never - before - seen samples without requiring explicit annotation , which can be cumbersome to obtain in practice . 3.5 Testing and comparing with other approaches . For experiments using supervised learning , we manually labeled 11 % ( 59 ) of the 520 images from P14 and P75 distributed equally across each sample . To do so , we performed the first - pass segmentation described earlier in the text and labeled the resulting windows ( segments ) as positive or negative . A total of 230 synapses were identified ( each showing a clear post - synaptic density and elongated shape ) along with 2062 negative examples . The MR8 , HoG and shape descriptors were extracted from each example and stored as a 480D feature vector . We performed 10-fold cross - validation and report precision , recall and area under the ROC and precision - recall curves . The confidence in each prediction was measured based on the distance from the test vector to the decision boundary in feature space ( for SVM ) and based on the proportion of tree agreements for the Random Forest classifier . For supervised learning , we did not use any unlabeled data . For the semi - supervised approach , we selected all the labeled images from one sample ( sample A , e.g. P14 ) and trained two classifiers using texture and shape features , respectively . A new single classifier was then built using the known labeled examples from sample A as well as the high - confidence examples predicted by the co - trained classifiers . We also learned a baseline classifier that was only trained on the known labels from sample A. Both classifiers were then tested for accuracy on the true labeled examples from sample B. . We compared our supervised approach against a correlation - based technique ( Roseman , 2004 ) that classifies a test window W as synapse - containing with respect to training set T if ; i.e. if the correlation coefficient between W and any positive example in the training set is , where . We also compare against Espina ( Morales et al . , 2011 ) , a tool designed to detect synapses in conventional EM images ( see Supplementary Information ) . 4 RESULTS AND DISCUSSION . 4.1 Validating against conventional EM . One potential concern with using the EPTA method is that some synapses may be washed away alongside other non - synaptic structures . To validate the correctness of our experimental procedure , we tested whether EPTA - stained images preserve roughly the same density of synapses that appear in conventional EM images of the same region . First , we isolated tissue corresponding to the D2 barrel of the mouse somatosensory cortex at P75 . In one hemisphere , we performed standard EM chemistry and in the other hemisphere , we applied our EPTA stain . We then asked an expert EM biologist ( J.S. ) and an expert neuroscientist ( A.L.B. ) to manually annotate 26 conventional EM images for high- and medium - confidence synapses , and we compared density ratios versus our automated algorithm on the EPTA - stained tissue . EPTA appears to conserve synaptic density compared with conventional EM ( Table 1 ) . In particular , the two experts found an average of 3.55 high and medium - confidence synapses per image within the conventional EM data versus 3.25 using the EPTA - stained images ( an average difference of only 8 % ) . Because synaptic density may slightly vary according to pial depth of the specimen even within D2 , some variation may be expected across hemispheres . If the EPTA stain were selectively staining for only asymmetric ( excitatory ) synapses and not symmetric ( inhibitory ) synapses , then we would expect a roughly 20 % difference ( Micheva and Beaulieu , 1995 ) between the number of synapses counted using EPTA and conventional EM . However , the close correspondence between the two methods suggests that we may be capturing both . This further demonstrates the validity of the EPTA stain as synapse - preserving ( Bloom and Aghajanian , 1966 ) and provides a way of choosing an appropriate threshold for classification . 4.2 Detecting synapses using supervised and semi - supervised learning . As aforementioned , we used texture- and shape - based features to train several different classifiers ( SVM , Random Forest and AdaBoost ) . We next compared the performance of these classifiers as well as a template - matching algorithm previously suggested for this task ( Roseman , 2004 ) ( Fig . 3 ) . The SVM trained on both texture and shape features ( MR8 , HoG and Shape ) performed best with an accuracy ranging from 54.8 to 81.3 % on the positive set and 99.6 to 96.8 % on the negative set depending on the classifier threshold used . Specifically , the default threshold of 0.5 allowed us to recall 67.8 % of the true synapses with a precision of 83.3 % ( Fig . 3 B ) . This significantly outperformed all other classifiers , as well as the correlation - based template - matching algorithm . These results suggest that the EPTA stain facilitates the high - throughput and automated detection of synapses compared with conventional EM . Such a performance would enable the large - scale use of this method to study experience - dependent plasticity in the brain and to detect abnormal changes in synaptic density owing to neurological disorders . Cross - validation accuracy of all methods . AUC of the ROC curve ( top ) and precision - recall curve ( bottom ) show that the SVM trained using both texture and shape descriptors outperforms all other methods . The shape descriptors by themselves worked well for identifying positive examples but predicted many false positives owing to similar contours in , for example , nucleus regions , which are better discriminated using texture . The HoG features especially benefited from the vertical alignment of the synapses ( a 7 - 10 % decrease in false negatives with similar false positives comparing classifiers trained with and without alignment ) . To further validate our classifier with respect to human annotation , we took 30 unlabeled images from our P14 sample and asked an independent party ( a technician in the Barth lab familiar with the EPTA protocol ) to manually annotate all high - confidence synapses . We then applied our supervised classifier to these images using the default classifier threshold of 0.5 and obtained an accuracy of 87.25 % ( 89/102 ) on the set of positive predictions ( 66.6 % recall ) and 96.28 % ( 1164/1209 ) on the negative predictions . These percentages are similar to those obtained using cross - validation . Next , we applied semi - supervised learning to detect synapses in new samples for which training data is not available ( Table 2 ) . Here , we trained a single classifier on one sample ( either P14 or P75 ) and used co - training to learn another classifier using unlabeled examples from the other sample . We found that the accuracy of increases amongst the positive class by 8 - 12 % and increases the AUC by 1 - 4 % compared with the baseline classifier ( see Section 3 ) . To explore how semi - supervised learning can account for the variance between mice at the same age , we trained on positive and negative examples from one P75 mouse and used the examples from the other P75 mouse to test ( and vice versa ) . The baseline without semi - supervised learning had an average precision - recall AUC of 68.89 % ( 78.81 and 58.97 % individually ) and an average ROC AUC of 95.80 % ( 98.48 and 93.12 % individually ) . Then , we used co - training to train a new classifier and improved the average precision - recall AUC to 73.06 % ( 82.45 and 63.66 % ) and the ROC AUC to 97.15 % ( 98.78 and 95.52 % ) . Both these results demonstrate the power of using semi - supervised learning on unlabeled data , which is often plentiful but ignored in bioimage applications . Our approach is also scalable and efficient : using unoptimized MATLAB code , we can process a single image in 3.4 s on a standard desktop machine using a single processor . 4.3 Development changes in the mouse barrel cortex . Next , we used our method to study developmental changes in synapse density and size in a defined area of the mouse brain , the representation of a single specified ( D1 ) whisker in layer 4 of somatosensory cortex . We performed additional experiments and imaged the D1 barrel in two P17 mice following the same procedure as outlined in Section 3 . Further , we found that synapse density decreases in adults to about the same level as P14 ( 2.78 synapses per image at P75 ) , which is also consistent with prior work on pruning ( White et al . , 1997 ) . Thus , our pipeline can be used to better understand the rates of developmental processes and can be used to define the precise timelines of their occurrence . Developmental changes and log - normal distribution of synaptic strength . ( A ) Histogram of synapse sizes ( as measured by length of the perimeter of the synapse ) from two P14 ( blue ) , two P17 ( red ) and two P75 ( green ) samples . There is significant growth ... . Finally , to demonstrate the utility of the EPTA stain beyond simply detecting synapses , we used shape features to characterize the strength of each synapse based on its well - established correlation with spine size ( Lee et al . , 2012 ) . Modulation in synaptic strength is an important facet of neuroplasticity and development ( Wen and Barth , 2011 ) with larger contact areas likely to transmit more current to the post - synaptic neuron . We used our classifier - predicted synapses in all samples ( P14 , P17 and P75 ) and plotted the distribution of the perimeter of all synapses detected ( larger perimeter larger size stronger synapse ) . Structurally similar synapses may appear to have different sizes when projected onto 2D because the image cross - sections may be taken at any angle . We found that the distribution of synapse size at all ages fits a log - normal distribution ( by Shapiro - Wilk and Anderson - Darling tests ; Fig . 4 A ) , which is consistent with previous electrophysiological data ( Song et al . , 2005 ) . These data suggest that developmental processes may preferentially enable the addition of new synapses without necessarily augmenting the size of already - existing synapses . The data analysis pipeline that we have established can thus be used to generate and test specific hypotheses about synapse growth and maturation in the developing neocortex in a high - throughput and statistically robust manner . 5 DISCUSSION AND CONCLUSIONS . Synaptic density and strength are dynamically modulated in the brain and are important facets for understanding neural circuitry and function . Experience - dependent plasticity , circuit development and neuropathologies have all been linked to changes in the number and strength of synapses in the brain , and thus their characterization is a useful parameter to facilitate our understanding of network function . But to do so requires both clarity in the data and robust algorithms to explore this data . We used an old experimental technique for selectively staining synapses in EM images . This technique does not require specialized animal models for enhancing synapses , and we validated it on new tissue of the mouse somatosensory cortex and against conventional EM . We also used semi - supervised learning to learn models that can adapt to variability in new samples by leveraging unlabeled data . Such an approach is suitable for several other bio - image classification problems that also face issues of sample heterogeneity . Our approach is general and scalable enough to handle large datasets and is freely available for others to use . For future work , it would be a great interest to perform immunocytochemistry using EPTA against GABA and AMPA receptors to separately classify symmetric and asymmetric synapses . Accuracy could also be improved by detecting synapses in 3D , though this would require many more images and more sophisticated computational techniques that can automatically segment , align and reconstruct synapses across serial sections . ACKNOWLEDGEMENTS . S.N. would like to thank Fernando Amat and Gustavo Rohde for advice on developing the image processing algorithms and Patrick Beukema for manual annotation of EPTA images . Funding : National Institutes of Health ( 1RO1 GM085022 ) and National Science Foundation ( DBI-0965316 ) awards to Z.B.J. . Conflict of Interest : none declared . REFERENCES . Arbelaez P , et al . Experimental evaluation of support vector machine - based and correlation - based approaches to automatic particle selection . J. Struct . Biol . [ PubMed ] . Bloom FE , Aghajanian GK . Cytochemistry of synapses : selective staining for electron microscopy . Science . [ PubMed ] . Bloom FE , Aghajanian GK . Fine structural and cytochemical analysis of the staining of synaptic junctions with phosphotungstic acid . J. Ultrastruct . Res . [ PubMed ] . Blum A , Mitchell T. 1998 . Combining labeled and unlabeled data with co - training . In Proceedings of the 11th Annual Conference on Computational Learning Theory ( COLT ) , COLT ' 98 . pp . 92 - 100 , ACM , New York , NY , USA . Bock DD , et al . Network anatomy and in vivo physiology of visual cortical neurons . Nature . [ PMC free article ] [ PubMed ] . Breiman L. Random forests . Mach . Learn . Cardona A , et al . TrakEM2 software for neural circuit reconstruction . PLoS One . 2012 ; 7 : e38011 . [ PMC free article ] [ PubMed ] . Chang C - C , Lin C - J. LIBSVM : A library for support vector machines . ACM Trans . Intell . Syst . Technol . Clare R , et al . Synapse loss in dementias . J. Neurosci . Res . [ PMC free article ] [ PubMed ] . Coggeshall RE , Lekan HA . Methods for determining numbers of cells and synapses : a case for more uniform standards of review . J. Comp . Neurol . [ PubMed ] . Cooke BM , Woolley CS . Gonadal hormone modulation of dendrites in the mammalian CNS . J. Neurobiol . [ PubMed ] . Cowan WM , et al . Regressive events in neurogenesis . Science . [ PubMed ] . da Costa NM , et al . A systematic random sampling scheme optimized to detect the proportion of rare synapses in the neuropil . J. Neurosci . Methods . [ PubMed ] . Dalal N , Triggs B. 2005 . Histograms of oriented gradients for human detection . In : Schmid , C. , et al . ( eds ) , Proceedings of the International Conference on Computer Vision and Pattern Recognition ( CVPR ) , Vol . 2 , IEEE Computer Society , Washington , DC , USA , pp . 886 - 893 . De Felipe J , et al . Inhibitory synaptogenesis in mouse somatosensory cortex . Cereb . Cortex . [ PubMed ] . Denk W , et al . Structural neurobiology : missing link to a mechanistic understanding of neural computation . Nat . Rev. Neurosci . [ PubMed ] . Duda RO , Hart PE . Use of the hough transformation to detect lines and curves in pictures . Commun . ACM . Feng L , et al . Improved synapse detection for mGRASP - assisted brain connectivity mapping . Bioinformatics . [ PMC free article ] [ PubMed ] . Fox K. Barrel Cortex . Cambridge : Cambridge University Press ; 2008 . Freund Y , Schapire RE . A decision - theoretic generalization of on - line learning and an application to boosting . In Proceedings of the Second European Conference on Computational Learning Theory ( EuroCOLT ) . pp . 23 - 37 . Springer - Verlag , London , UK . Glaze DG . Rett syndrome : of girls and mice - lessons for regression in autism . Ment . Retard Dev . Disabil . Res . Rev. 2004 ; 10 : 154 - 158 . [ PubMed ] . Hinton VJ , et al . Analysis of neocortex in three males with the fragile X syndrome . Am . J. Med . Genet . [ PubMed ] . Huttenlocher PR , Dabholkar AS . Regional differences in synaptogenesis in human cerebral cortex . J. Comp . Neurol . [ PubMed ] . Jain V , et al . Machines that learn to segment images : a crucial technology for connectomics . Curr . Opin . Neurobiol . [ PMC free article ] [ PubMed ] . Johnston MV , et al . Rett syndrome and neuronal development . J. Child Neurol . [ PubMed ] . Kim J , et al . mGRASP enables mapping mammalian synaptic connectivity with light microscopy . Nat . Methods . [ PMC free article ] [ PubMed ] . Klintsova AY , Greenough WT . Synaptic plasticity in cortical systems . Curr . Opin . Neurobiol . [ PubMed ] . Knott GW , et al . Formation of dendritic spines with GABAergic synapses induced by whisker stimulation in adult mice . Neuron . [ PubMed ] . Kreshuk A , et al . Automated detection and segmentation of synaptic contacts in nearly isotropic serial electron microscopy images . PLoS One . 2011 ; 6 : e24899 . [ PMC free article ] [ PubMed ] . Lee KF , et al . Examining form and function of dendritic spines . Neural Plast . [ PMC free article ] [ PubMed ] . Lefort S , et al . The excitatory neuronal network of the c2 barrel column in mouse primary somatosensory cortex . Neuron . [ PubMed ] . Leung T , Malik J. Representing and recognizing the visual appearance of materials using three - dimensional textons . Int . J. Comput . Vision . Mayhew TM . How to count synapses unbiasedly and efficiently at the ultrastructural level : proposal for a standard sampling and counting protocol . J. Neurocytol . [ PubMed ] . Merchan - Perez A , et al . Counting synapses using FIB / SEM microscopy : a true revolution for ultrastructural volume reconstruction . Front . Neuroanat . [ PMC free article ] [ PubMed ] . Micheva KD , Beaulieu C. An anatomical substrate for experience - dependent plasticity of the rat barrel field cortex . Proc . Natl Acad . Sci . USA . [ PubMed ] . Morales J , et al . Espina : a tool for the automated segmentation and counting of synapses in large stacks of electron microscopy images . Front . Neuroanat . [ PMC free article ] [ PubMed ] . Morshedi MM , et al . Increased synapses in the medial prefrontal cortex are associated with repeated amphetamine administration . Synapse . [ PMC free article ] [ PubMed ] . Na ES , Monteggia LM . The role of MeCP2 in CNS development and function . Horm . Behav . [ PMC free article ] [ PubMed ] . Nimchinsky EA , et al . Abnormal development of dendritic spines in FMR1 knock - out mice . J. Neurosci . [ PubMed ] . Peng H. Bioimage informatics : a new area of engineering biology . Bioinformatics . [ PubMed ] . Pfeiffer BE , Huber KM . The state of synapses in fragile X syndrome . Neuroscientist . [ PMC free article ] [ PubMed ] . Roseman AM . FindEM - a fast , efficient program for automatic selection of particles from electron micrographs . J. Struct . Biol . [ PubMed ] . Skibbe H , et al . 2011 . SHOG : spherical hog descriptors for rotation invariant 3d object detection . In Proceedings of the 33rd International Conference on Pattern Recognition , DAGM'11 . pp . 142 - 151 . Springer - Verlag , Berlin , Heidelberg . Song S , et al . Highly nonrandom features of synaptic connectivity in local cortical circuits . PLoS Biol . 2005 ; 3 : e68 . [ PMC free article ] [ PubMed ] . Sporns O. Networks of the Brain . Cambridge : MIT Press ; 2010 . Stoneham ET , et al . Rules of engagement : factors that regulate activity - dependent synaptic plasticity during neural network development . Biol . Bull . [ PubMed ] . Szeliski R. Computer Vision : Algorithms and Applications . 1st edn . New York , NY : Springer - Verlag New York , Inc ; 2010 . Van den Oever MC , et al . The synaptic pathology of drug addiction . Adv . Exp . Med . Biol . [ PubMed ] . Varma M , Zisserman A. 2003 . Texture classification : are filter banks necessary ? In Proceedings of the 2003 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , Vol . 2 , pages II-691 - 8 vol . Walz W. Patch - Clamp Analysis : Advanced Techniques . Totowa , New Jersey , USA : Humana Press , Neuromethods Series ; 2007 . Wen JA , Barth AL . Input - specific critical periods for experience - dependent plasticity in layer 2/3 pyramidal neurons . J. Neurosci . [ PMC free article ] [ PubMed ] . White EL , et al . A survey of morphogenesis during the early postnatal period in PMBSF barrels of mouse SmI cortex with emphasis on barrel D4 . Somatosens . Mot . Res . [ PubMed ] . White JG , et al . The structure of the nervous system of the nematode Caenorhabditis elegans . Philos . Trans . R. Soc . Lond . B Biol . Sci . [ PubMed ] . Yassin L , et al . An embedded subnetwork of highly active neurons in the neocortex . Neuron . [ PMC free article ] [ PubMed ] . Zhu X. Technical report . Semi - supervised learning literature survey . Computer Sciences , University of Wisconsin - Madison , Madison . Zuiderveld K. Graphics Gems IV . Chapter Contrast Limited Adaptive Histogram Equalization . San Diego , CA : Academic Press Professional , Inc ; 1994 . pp . 474 - 485 . \"}",
        "_version_":1692671098545504256,
        "score":22.06391},
      {
        "id":"96864c66-1a0a-48cf-88d4-10a0d4ceaa15",
        "_src_":"{\"url\": \"http://www.nysenate.gov/legislation/bills/2013/S1817\", \"s3\": \"s3://aws-publicdatasets/common-crawl/crawl-data/CC-MAIN-2016-07/segments/1454702039825.90/warc/CC-MAIN-20160205195359-00170-ip-10-236-182-209.ec2.internal.warc.gz\", \"text\": \"An Open Source hacker 's ramblings . QUrl in Qt 5 : validity . In the previous blog about QUrl in Qt 5 , Kenneth suggested I talk to Adam Barth about the issues with URLs in WebKit . So I went to the # webkit channel on Freenode , pinged him and we discussed a bit . He pointed me to a lot of unit tests that WebKit runs related to URL parsing and interpreting , including some of his own results about the acceptance of different browsers ( using WebKit and not ) to some of the garbage we find out there . Turns out he has a very extreme view on the subject of URLs and URIs . His position was that there is no standard for what a URL truly is and the RFCs trying to define it ( RFC 3986 and RFC 3987 ) are to be ignored . My position - which matters to you because I 'm doing QUrl for Qt 5 - is that the RFC standards are valid and specify how to handle those URIs . Everything else is undefined behaviour and could be rightfully rejected , but we wo n't because there 's just too much out there . RFC 3986 defines an ABNF grammar in Appendix A for parsing of URIs . If you look at the source code , you 'll find matches for exactly the same terms as defined by the grammar . I 'm not sure if this is fast , however : could the parser be faster if we had coded it differently ? We 'll soon find out , as I 'm about to rewrite it . As a turn of events , however , QUrl started not from the URI but instead from a broader definition of URI - reference which can be either the URI as we 're used to or a relative - ref . The latter is what you usually find in fields where URLs are expected , like the HREF attribute for the A element or the SRC attribute for the IMG element in HTML . That meant that the QUrl::isValid ( ) function was mostly useless , as most inputs were considered valid . What people expected to be invalid did match the relative - ref part of the grammar and the data ended up in the URL 's path component . So despite being strictly - conforming , the parser was actually quite liberal . Couple that with the QUrl::TolerantMode parsing which corrected mistakes in the percent - encoding , QUrl almost never rejected a URL . The only thing it started to reject were bad hostnames because I considered them a security issue ( homograph attacks ) . QUrl started to apply strict STD 3 conformance and rejected anything malformed there . For Qt 5 , I will relax the parser even further and I 'll accept some of the really strange inputs that I found in WebKit 's unit tests . QUrl in Qt 5 will accept strictly - conforming URLs as expected and will only produce standards - compliant URIs and URLs . The new parser I 'll write is actually closer to what people expect a URL to be . Take this example from the QUrl documentation : . Instead of following the grammar to parse it , I 'll just delimit at the expected boundaries and then try to correct the components as extracted . I mean , I 'll try - we 'll see if I manage or if I need to scrap this method . Hopefully , this will be a faster algorithm . What does this mean to you ? If you were passing QUrl some strict - conforming URIs and URLs , nothing will happen . In fact , it should be 1:1 and give you exactly the same as you gave it . If you had URLs that decoded some percent - encoded characters or UTF-8 sequences without causing it to become ambiguous , QUrl will also still accept your input . If you had really broken URLs which QUrl accepted and corrected in Qt 4 , there 's a good chance that QUrl in Qt 5 will continue to accept and interpret the same way . That 's because the set of unit tests for QUrl is quite extensive and I 'll do my best to keep compatibility . Finally , if you had really really broken URLs , specially those with broken hostnames , I have n't decided yet . I will accept some more URLs but , as I said , I consider them undefined behaviour . They may be accepted or they may be rejected - what 's more , the behaviour might change in new versions of Qt . If your application breaks because of parsing of URLs , please report the bug . I will pay attention to each report . If we can prove that QUrl is failing to comply with the RFC , then the bug is proven and we 'll need to fix Qt . If your input fails to comply , I 'll need convincing arguments why QUrl should accept and correct your input . Paolo : I see that secondlife \\\" URIs \\\" can use two or three slashes . In Qt 4 , three slashes equals one . In Qt 5 , I plan on maintaining the difference . When used with two slashes , the component after the slashes and before the next one is an authority . For QUrl , it must contain a valid hostname and will be treated as a hostname . That means it will be lowercased and subject to STD 3 rules . If Second Life region names match STD 3 , it will be fine . If they do n't , those URIs wo n't work . Hint for implementors : do n't use double slashes . Use plain URIs , like mailto . No idea . But it 's maybe a good explanation for Adam Barth 's extreme view . Just interesting to see how defined standards vs usage differ . We are collecting similar experiences with ISO ODF and MSOOXML in Calligra every day . Sometimes it 's indeed better to go with the standard and ignore implementations cause else you easily end in situations where old mistakes ( and the way ed2k defined urls where one ) are carried on forever . \"}",
        "_version_":1692668305675911168,
        "score":22.048943}]
  }}
