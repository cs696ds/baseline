#+title: Data Augmentation for Improved Generalizability of Natural Language Processing Models
#+OPTIONS: toc:nil
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}
#+AUTHOR: Hee Hwang and Sudarshan Raghavan
#+EMAIL: {hhwang, sraghavan}@cs.umass.edu
#+LATEX_CLASS_OPTIONS: [twocolumn]


\begin{abstract}
A rapid data augmentation framework to improve the performance of natural language processing models. To augment data for a particular downstream task, we use DepCC, A Dependency-Parsed Text Corpus from Common Crawl. First, we take the Common Crawl data and index 35M documents using Apache Solr, a search engine that uses BM25(Similar to TF-IDF) scoring model. Secondly, we prepare queries from the train/test dataset. After retrieving relevant documents using the query, we convert the documents into dense embeddings and apply  K-nearest-neighbors to the candidate passages to rank the relevant documents. We augment data using various strategies. To show performance, we measure held-out accuracy.
\end{abstract}

* Datasets
  - ACL  : Citation Intent Classification
  - Hyper: HyperPartisan News Detection
  - IMDb : Sentiment Classification
** Size of Datasets
   |-------+-----------+---------+----------+--------------|
   | Task  | train set | dev set | test set | # of Classes |
   |-------+-----------+---------+----------+--------------|
   | ACL   |      1688 |     114 |      139 |            6 |
   |-------+-----------+---------+----------+--------------|
   | Hyper |       516 |      64 |       65 |            2 |
   |-------+-----------+---------+----------+--------------|
   | IMDb  |     20000 |    5000 |    25000 |            2 |
   |-------+-----------+---------+----------+--------------|

** Size of Augmented data
   |-------+----------+------------+-------------+--------------|
   | Task  | Baseline | Strat. (i) | Strat. (ii) | Strat. (iii) |
   |-------+----------+------------+-------------+--------------|
   | ACL   |     1688 |      11005 |       10248 |        10621 |
   |-------+----------+------------+-------------+--------------|
   | Hyper |      516 |       1496 |        2184 |         2184 |
   |-------+----------+------------+-------------+--------------|
   | IMDb  |    20000 |      29492 |       67004 |        68666 |
   |-------+----------+------------+-------------+--------------|


* Results
  |-------+----------+------------+-------------+--------------|
  | Task  | Baseline | Strat. (i) | Strat. (ii) | Strat. (iii) |
  |-------+----------+------------+-------------+--------------|
  | ACL   |     62.5 |       60.5 |        64.4 |         67.1 |
  |-------+----------+------------+-------------+--------------|
  | Hyper |     85.2 |       90.2 |        88.7 |         86.7 |
  |-------+----------+------------+-------------+--------------|
  | IMDb  |     93.8 |       92.4 |        91.9 |         92.3 |
  |-------+----------+------------+-------------+--------------|

* Baseline models: 
  - An off-the-shelf RoBERTa model that has been finetuned to perform classification for each of the downstream tasks

* Augmentation Model
[[./png/da.png]]

* Algorithm
  #+BEGIN_SRC
1. Extract failed test examples from the baseline model
2. Retrieve passages/sentences from Common Crawl 
3. Apply augmentation strategy (i)-(iii)
4. Augment all the labelled CC data to the training data
5. Retrain RoBERTa on the augmented training set 
  #+END_SRC

* Augmentation Strategies 
- Strategy (i)\\
  Use baseline model (Teacher) to perform unsupervised labelling on retrieved CC data
- Strategy (ii)\\
  Using a task specific binary classifier, 
  filter out retrieved CC data that is "out-domain"\\
  Use baseline model (Teacher) to perform unsupervised labelling on the filtered "in-domain" CC data
- Strategy (iii)\\
  Using a task specific binary classifier, 
  filter out retrieved CC data that is "out-domain"\\
  Use ground truth labels of failed test examples and assign labels to the filtered "in-domain" CC data


   # * Augmentation strategy (ii):
   # ** Extract failed test examples from the baseline model
   # ** Retrieve passages/sentences from Common Crawl (CC) using Apache Solr using failed test examples as queries (no heuristic in how                                    much to retrieve)
   # ** Using a task specific binary classifier, filter out retrieved CC data that is "out-domain"
   # ** Use baseline model (Teacher) to perform unsupervised labelling on the filtered "in-domain" CC data
   # ** Augment all the labelled CC data to the training data
   # ** Retrain RoBERTa on the augmented training set (Student) and re-check performance on test data
 
   # * Augmentation strategy (iii):
   # ** Extract failed test examples from the baseline model
   # ** Retrieve passages/sentences from Common Crawl (CC) using Apache Solr using failed test examples as queries (no heuristic in how                                   much to retrieve)
   # ** Using a task specific binary classifier, filter out retrieved CC data that is "out-domain"
   # ** Use ground truth labels of failed test examples and assign labels to the filtered "in-domain" CC data
   # ** Augment all the labelled CC data to the training data
   # ** Retrain RoBERTa on the augmented training set (Student) and re-check performance on test data


* TBD
Modify Query and Retrival / oversampling / downsampling \\
Perturb Query / Vary augmentation data / Measure Binary classifier


#    - All the training examples
#    - Hard to learn training examples
#    - Randomly picking from the held-out training set

# ** Experiments with respect to retrieval
#    - Random retrieval from the common crawl dataset
#    - K-Nearest Neighbors using query embeddings, rather than Apach Solr

# ** Perturbation of query
#    - Perturbing the queries from the training set
#    - Exchange certain words in queries with an alternative
#    - Paraphrasing

# ** Plotting
#    - Vary number of augmented examples per query
#    - A tabular plot that illustrates how augmentation performance changes when a binary classifier

# ** Downsampling & Oversampling
#    - Downsampling training set in high resource setting task. e.g. Using 25% of the original training set
#    - Oversampling: we augment the training data to make it 200% of the
#      original size and evaluate how well the subsequent augmented model
#      performs.

