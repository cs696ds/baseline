#+SETUPFILE: https://fniessen.github.io/org-html-themes/org/theme-readtheorg.setup
# #+OPTIONS: tasks:todo
# #+OPTIONS: tasks:t

#+title:  Data Augmentation
#+author: Hee Hwang

* Data Augmentation Framework
** DONE 1. Index DepCC
** TODO 2. Select Query
** DONE 3. Search
** DONE 4. Encode
** IN PROGRESS 5. K-NN / K-FN
** TODO 6. Augment Train Data
** TODO 7. Evaluate Accuracy
** TODO 8. Apply CheckList

* ACTIONS
** IN PROGRESS Action #03 Build toy Classification Pipeline [100%]
  - [X] Download IMDB Data(train/test/dev) used in https://github.com/allenai/dont-stop-pretraining 
  - [X] Fine-tune ROBERTA with hyperparameters from https://github.com/allenai/dont-stop-pretraining/blob/master/environments/hyperparameters.py

** IN PROGRESS Action #04 Build toy augmentation using retrieval [0%]
  - [ ] Retrieval based data augmentation on a small subsample of common crawl.
    Based on Sudarshan's Previous work on KNN Cosine Similarity Retrieval
    #+BEGIN_SRC python
#This function reduces the size of the context by selecting top k passages in the context and concatenates them
def get_final_context_strings(questions, answers, context):
  top_k = 3 # Choose number of top passages to concatenate. Typical is 3 for Longformer.
  context_embeddings = get_context_embeddings(context)
  num_questions = len(questions)
  num_answers = len(answers)
  final_contexts = [[] for i in range(num_questions)]
  final_context_strings = ["" for i in range(num_questions)]
  top_k_sentences = []

  with torch.no_grad():
    question_embeddings = torch.from_numpy(model_ir.encode(questions)) # -> Tensor, Size: Number of Questions x Embedding Size
  cos_sim_sentences = util.pytorch_cos_sim(question_embeddings, context_embeddings) # Use cosine simularity to rate sentences. -> Tensor, Size: Number of Questions x Number of Passages in Context
  top_k_sentences = torch.topk(cos_sim_sentences,top_k)[1] #Indices Tensor (Size: Number of Questions x Indices of Top k Values in Context) - Discard values tensor at index 0
  sorted_indices = torch.sort(top_k_sentences,dim=-1)[0] ## Sort sentences into document order to preserve structure, removing extra indices tensor from sort fxn. -> Indices Tensor (Size: Number of Questions x Indices of Top k Values in Context)
  sorted_indices_list = sorted_indices.tolist()

  for qxn in range(num_questions): # Build all the final contexts
    for chunk in sorted_indices_list[qxn]:
      final_contexts[qxn].append(sentences[chunk])
    final_context_strings[qxn] = " ".join(final_contexts[qxn])
  
  return final_context_strings
    #+END_SRC

** DONE Action #94 Review Papers [100%]
  - [X] Discuss [[./papers/2004.10964.pdf][Don’t stop pretraining:Adapt language models to domains and tasks.(2020)]]
    - They augment data by using VAMPIRE, a variational auto encoder with simple bag-of-words
    - Replication(Pretraining) Failed. But the dataset, model, and classification task is good
    - Definitely good for a baseline
  - [X] Discuss [[./papers/2004.02105.pdf][Unsupervised domain clusters in pretrained language models.(2020)]] 
  - [X] Discuss [[./papers/2004.04906.pdf][Dense Passage Retrieval for Open-Domain Question Answering(2020)]]
  - [X] Discuss [[./papers/2102.01335.pdf][Neural Data Augmentation via Example Extrapolation(2020)]]


** DONE Action #95 Discuss additional ideas [100%]
  - [X] Augmentation by paraphrase generation -> NO!
  - [X] Get some ideas from image data augmentation automation -> NO!
  - [X] Autoaugment: Try different kinds of data augmentation and select policy -> NO!

** DONE Action #96 Choose Data [100%]
  - [X] Toy Classification Data
    |------+-----------------------------------------------------------------------------------------+
    | IMDB | https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/train.jsonl |
    |      | https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/test.jsonl  |
    |      | https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/imdb/dev.jsonl   |
    |------+-----------------------------------------------------------------------------------------+
  - [X] Augmentation Data
    - Common Crawl raw text: https://commoncrawl.s3.amazonaws.com/contrib/depcc/CC-MAIN-2016-07/index.html
    - Downloaded raw text on UMass gypsum cluster(468 GB with compression)
    - Need to tokenize it at a paragraph level
    - Sentence Level: gypsum:://mnt/nfs/work1/miyyer/datasets/depcc
    - Other Data: https://github.com/EleutherAI/the-pile

** DONE Action #97 Select Embedding [100%]
  - [X] Get better semantic representation(embedding) by pre-training with relevant domain corpus
    - ROBERTA, ROBERTA-DAPT, and ROBERTA-TAPT are available online.
    - https://huggingface.co/allenai
  - [X] VAMPIRE(https://github.com/allenai/vampire)
    - This is the augmentation method for the previous paper "don't stop pretraining"
    - Got it working without any issue.

** WAITING Action #98 Acquire Previous Work from Oracle [0%]
  - [ ] Waiting for response

* References [0%]
  - [ ] Embedding
    - [[./papers/1810.04805.pdf][Bert: Pre-training of deepbidirectional transformers for language understand-ing.(2019)]]
    - [[./papers/1907.11692.pdf][RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019)]]
    - [[./papers/2004.10964.pdf][Don’t stop pretraining:Adapt language models to domains and tasks.(2020)]]
    - [[./papers/1906.02242.pdf][VAMPIRE:Variational Pretraining for Semi-supervised Text Classification(2019)]]
    - [[./papers/beta_vae.pdf][Disentangled Variational Autoencoder(2017)]]

  - [ ] Augmentation by Retrieval
    - [[./papers/2004.02105.pdf][Unsupervised domain clusters in pretrained language models.(2020)]]
    - [[./papers/1707.07328.pdf][Adversarial examples for evaluating reading comprehension systems.(2017)]]
    - [[./papers/2004.04906.pdf][Dense Passage Retrieval for Open-Domain Question Answering(2020)]]

  # - [ ] Augmentation by Generation
  #   - [[./papers/2102.01335.pdf][Neural Data Augmentation via Example Extrapolation(2020)]]
  #   - [[./papers/2010.05700.pdf][Reformulating Unsupervised Style Transferas Paraphrase Generation(2020)]]
  #   - [[./papers/1804.06059.pdf][Adversarial example generation with syntactically controlled paraphrase networks.(2018)]]


  # - [ ] Auto Augmentation
  #   - [[./papers/1805.09501.pdf][AutoAugment:Learning Augmentation Strategies from Data(2018)]]
  #   - [[./papers/1709.01643.pdf][Learning to Compose Domain-Specific Transformations for Data Augmentation(2017)]]
  #   - [[./papers/1912.11188.pdf][Adversarial Autoaugment(2020)]]

  - [ ] Robustness
    - [[./papers/1804.07998.pdf][Generating natural language adversarial examples.(2018)]]
    - [[./papers/1909.12434.pdf][Learning the difference that makes a difference with counterfactually-augmented data.(2019)]]

  - [ ] Linguistic Generalization
    - [[./papers/2005.04118.pdf][Beyond Accuracy: Behavioral Testing of NLP Models with CheckList(2020)]]
    - [[./papers/2004.11999.pdf][Syntacticdata augmentation increases robustness to inference heuristics.(2020)]]

* Links
- [[https://sites.google.com/iesl.cs.umass.edu/compsci-696-ds-spring-2021/home?authuser=1 ][696 Course Homepage]]

* Week3 Notes <2021-02-16 Tue>
** Present at meeting [7/7]
   - [X] Peter
   - [X] Sudarshan
   - [X] Kalpesh
   - [X] Ari
   - [X] Sweta
   - [X] Michael
   - [X] Naveen

** Topics
   - Progress
   - Project Overview
   - Filtering
   - Encoding
   - Nearest Neighbor
   - Computational Requirement

** Notes   
*** Progress
**** Set up environments on the UMass Gypsum cluster
**** Completed a basic classifier with IMDB dataset by fine-tuning RoBERTa 
**** Collected the common crawl files

*** Project Overview
    1. Get relevant documents by filtering common crawl 
    2. Encode sentence/passage (Bag of words, Vampire, or DPR)
    3. Apply nearest/farthest neighbor using API
    4. Augment data by manually labeling data or use classifier
    5. Compare with existing results


*** Filtering
**** Kalpesh: url-based filtering 
**** Ari: Invertied index over common crawl
**** Try different heuristics
     - How long do they take?(complexity)
     - How fine-grain are they?(similarity)
     - How many documents are there after filtering?

*** Encoding
    - First of all, make sure similar sentences cluster
    - Naveen: Better to use alternatives for embedding rather than plain RoBERTa
    - e.g. Sentence BERT(Distilled RoBERTa)
    - Kalpesh: DPR: Passage Encoder; dedicated retrieval; MAP search;
    - Michael: Should use dual encoder like universal sentence encoder
    - Traditional Inverted Index: Less nuanced but worth trying
    - Time complexity: How many documents/seconds can the model process?

*** Nearest Neighbor    
    - Sweta   : Do not reinvent wheels. Good programs are available.
    - Google SCANN : https://github.com/google-research/google-research/tree/master/scann
    - Facebook FAISS: https://github.com/facebookresearch/faiss (edited) 
    
*** Computational Requirement
    - Where is the bottleneck? KNN? Encoding?
    - Kalpesh: Building and loading index may be computationally expensive in terms of space. 
    - For example, 21M index -> 120GB RAM(TODO: Check gypsum memory limit)
    - Try a single common crawl file(20-50 MB)
    - Michael: Embedding -> bit vector if RAM is the issue

*** After Augmentation
    - Sample 10-15 sentence, see if it is reasonable
    - If not, debugging.(filter problem? encoding problem? or KNN problem?)
    - Try other dataset

*** Additional Comment
    - Focus on augmentation by retrieval. Put aside sentence generation.
    - Given validation set, find wrong example, find similar sentense from embedding, retrain, and check accuracy
    - Select data with moderate accuracy; 95% is too high!
    - Start from the small data and check accuracy after augmentation both from valid set and augmented data
    - Other baseline: DAPT and TAPT from [[./papers/2004.10964.pdf][Don’t stop pretraining:Adapt language models to domains and tasks.(2020)]]
    - Just get one solution working first.
* Week1 Notes <2021-02-02 Tue>
** Present at meeting [6/6]
   - [X] Peter
   - [X] Sudarshan
   - [X] Kalpesh
   - [X] Ari
   - [X] Sweta
   - [X] Michael

** Agenda
   - Logistics 
   - Project Intro
   - Discussion

** Notes
*** Goal: Publish a paper
*** Topic: NLP Data Augmentation
**** Approach
     - Find existing examples within large corpus such as Common Crawl using embedding
     - Generating from scratch(May not efficient)
     - Modifying existing data(Style Transfer, …)

*** Focus on Classification Tasks
**** Ari's suggestion: 
     - Intent Classification on banking domain (Open a bank account)
     - Sentiment Classification(3 classes)
**** Kalpesh: 
     - Citation intent classification
     - Topic Classification

*** Evaluation:
    - Held-out Test Accuracy
    - Adversarial held-out accuracy
    - Checklist Evaluation for Linguistic Generalization
    - Kalpesh: 10-15 Test Datasets for generalization - Which paper?

*** Shared Vector Embedding space is enormous. Focus on a small subset. 
    - Linguistic phenomena
    - TF-IDF
    - Prefix String
    - Kalpesh: Dense Embedding(DPR)

*** Will discuss further after setting up toy retrieval pipeline
*** May be able to get Oracle’s previous research on retrieval
    - Collecting the farthest embedding.
    - May collect irrelevant data. Thus, think about other way 
    - e.g. Collect task-relevant examples and take the farthest embedding

