{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (2.25.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (2020.12.5)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (2.10)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (4.0.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hhwang/miniconda3/envs/38/lib/python3.8/site-packages (from requests) (1.26.3)\r\n"
     ]
    }
   ],
   "source": [
    "%pwd\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Python variables\n",
    "TR = 'tr_02_acl'\n",
    "CC = 'cc_02_acl'\n",
    "\n",
    "#TRAIN_PATH  = 'data/04-hyper/train.jsonl'\n",
    "TRAIN_PATH  = 'data/02-acl-arc/train.jsonl'\n",
    "#TRAIN_PATH  = 'data/07-imdb/train.jsonl'\n",
    "TR_TSV  = 'emb/' + TR + '.tsv'\n",
    "CC_TSV  = 'emb/' + CC + '.tsv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Filtering I (BM25 Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n",
      "Thus+,+over+the+past+few+years+,+along+with+advances+in+the+use+of+learning+and+statistical+methods+for+acquisition+of+full+parsers+(+Collins+,+1997+;+Charniak+,+1997a+;+Charniak+,+1997b+;+Ratnaparkhi+,+1997+)+,+significant+progress+has+been+made+on+the+use+of+statistical+learning+methods+to+recognize+shallow+parsing+patterns+syntactic+phrases+or+words+that+participate+in+a+syntactic+relationship+(+Church+,+1988+;+Ramshaw+and+Marcus+,+1995+;+Argamon+et+al.+,+1998+;+Cardie+and+Pierce+,+1998+;+Munoz+et+al.+,+1999+;+Punyakanok+and+Roth+,+2001+;+Buchholz+et+al.+,+1999+;+Tjong+Kim+Sang+and+Buchholz+,+2000+)+.\n",
      "Number of retrieved documents: 10\n",
      "--- 0.12327218055725098 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "start_time = time.time()\n",
    "################################################################################\n",
    "# Filtering\n",
    "################################################################################\n",
    "pretty = lambda x : json.dumps(x, indent=2, sort_keys=True)\n",
    "solr_select = 'http://localhost:8983/solr/depcc-small/select?q='\n",
    "with open(TRAIN_PATH, 'r') as train_file:\n",
    "    json_lines = []\n",
    "    lines = train_file.readlines()\n",
    "    for line in lines:\n",
    "        j = json.loads(line)\n",
    "        json_lines.append(j)\n",
    "N = len(json_lines)\n",
    "print(N)\n",
    "\n",
    "\n",
    "# Query with training example\n",
    "j = json_lines[0]\n",
    "query = j['text'].replace(' ', '+')\n",
    "#query = \"\"\n",
    "#with open('data/02-acl-arc/lda_union.txt', 'r') as file:\n",
    "#    query = file.read()\n",
    "print(query)\n",
    "#print(len(query.split()))\n",
    "#sys.exit()\n",
    "rp_retrieval = requests.get(solr_select + query).json()\n",
    "cc_docs = (rp_retrieval['response']['docs'])\n",
    "print('Number of retrieved documents: %d' % len(cc_docs))\n",
    "cc_doc0 = json.loads(cc_docs[0]['_src_'])\n",
    "cc_doc100 = \"\"\n",
    "for i in range(10):\n",
    "    cc_doc100 += json.loads(cc_docs[i]['_src_'])['text']\n",
    "#print(cc_doc100)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation by sentences(Documents -> Passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"parser\"])\n",
    "nlp.enable_pipe(\"senter\")\n",
    "doc = nlp(cc_doc100)\n",
    "cc_psgs = []\n",
    "psg = ''\n",
    "num_tokens = 0\n",
    "for sent in doc.sents:\n",
    "    if num_tokens < 100:\n",
    "        psg += sent.text\n",
    "        num_tokens += len(sent)\n",
    "    else:\n",
    "#        print(num_tokens)\n",
    "        cc_psgs.append({'doc_id' : '', 'doc_text'  : psg,  'title': ''  })\n",
    "        num_tokens = 0\n",
    "        psg = ''\n",
    "#print(len(cc_psgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encoding Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_02_acl\n",
      "Thus , over the past few years , along with advances in the use of learning and statistical methods for acquisition of full parsers ( Collins , 1997 ; Charniak , 1997a ; Charniak , 1997b ; Ratnaparkhi , 1997 ) , significant progress has been made on the use of statistical learning methods to recognize shallow parsing patterns syntactic phrases or words that participate in a syntactic relationship ( Church , 1988 ; Ramshaw and Marcus , 1995 ; Argamon et al. , 1998 ; Cardie and Pierce , 1998 ; Munoz et al. , 1999 ; Punyakanok and Roth , 2001 ; Buchholz et al. , 1999 ; Tjong Kim Sang and Buchholz , 2000 ) .\n",
      "\n",
      "cc_02_acl\n",
      "Over the past fifteen years there has been significant progress in the field of statistical parsing .Much of the work has focussed on supervised methods , where by ' ' supervised ' ' we mean that the training data consists of sentences and their associated syntactic trees ( for example , Charniak 1997 , Collins 1999 , Roark and Johnson 1999 ) .There are a number of treebank corpora , of which the Penn Treebank , based largely on ' ' Wall Street Journal ' ' text , and available from the Linguistic Data Consortium , is the most widely used .\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Encoding Preparation\n",
    "################################################################################\n",
    "\n",
    "import csv\n",
    "import subprocess\n",
    "\n",
    "num_train = len(json_lines)\n",
    "train_psgs = []\n",
    "for i in range(num_train):\n",
    "    train_dict = {'doc_id': str(i), 'doc_text': json_lines[i]['text'], 'title': ''}\n",
    "    train_psgs.append(train_dict)\n",
    "    \n",
    "print(TR)\n",
    "print(train_psgs[0]['doc_text'])\n",
    "print()\n",
    "\n",
    "with open(TR_TSV, 'w') as output_file:\n",
    "    dw = csv.DictWriter(output_file, train_psgs[0].keys(), delimiter='\\t')\n",
    "    for tp in train_psgs:\n",
    "        dw.writerow(tp)\n",
    "        \n",
    "        \n",
    "with open(CC_TSV, 'w') as output_file:\n",
    "    dw = csv.DictWriter(output_file, cc_psgs[0].keys(), delimiter='\\t')\n",
    "    for psg in cc_psgs:\n",
    "        dw.writerow(psg)\n",
    "print(CC)\n",
    "print(cc_psgs[0]['doc_text'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized host node040 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "16-bits training: False \n",
      "Reading saved model from /mnt/nfs/work1/696ds-s21/hhwang/DPR/data/checkpoint/retriever/multiset/bert-base-encoder.cp\n",
      "model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
      "Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True\n",
      "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-uncased\n",
      "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
      "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256\n",
      " **************** CONFIGURATION **************** \n",
      "batch_size                     -->   32\n",
      "ctx_file                       -->   /mnt/nfs/work1/696ds-s21/hhwang/da/emb/tr_02_acl.tsv\n",
      "device                         -->   cuda\n",
      "distributed_world_size         -->   1\n",
      "do_lower_case                  -->   True\n",
      "encoder_model_type             -->   hf_bert\n",
      "fp16                           -->   False\n",
      "fp16_opt_level                 -->   O1\n",
      "local_rank                     -->   -1\n",
      "model_file                     -->   /mnt/nfs/work1/696ds-s21/hhwang/DPR/data/checkpoint/retriever/multiset/bert-base-encoder.cp\n",
      "n_gpu                          -->   1\n",
      "no_cuda                        -->   False\n",
      "num_shards                     -->   1\n",
      "out_file                       -->   /mnt/nfs/work1/696ds-s21/hhwang/da/emb/tr_02_acl\n",
      "pretrained_file                -->   None\n",
      "pretrained_model_cfg           -->   bert-base-uncased\n",
      "projection_dim                 -->   0\n",
      "sequence_length                -->   256\n",
      "shard_id                       -->   0\n",
      " **************** CONFIGURATION **************** \n",
      "PyTorch version 1.8.0 available.\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/hhwang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/hhwang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/hhwang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/hhwang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/hhwang/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading saved model state ...\n",
      "reading data from file=/mnt/nfs/work1/696ds-s21/hhwang/da/emb/tr_02_acl.tsv\n",
      "Producing encodings for passages range: 0 to 1688 (out of total 1688)\n",
      "Encoded passages 160\n",
      "Encoded passages 320\n",
      "Encoded passages 480\n",
      "Encoded passages 640\n",
      "Encoded passages 800\n",
      "Encoded passages 960\n",
      "Encoded passages 1120\n",
      "Encoded passages 1280\n",
      "Encoded passages 1440\n",
      "Encoded passages 1600\n",
      "Writing results to /mnt/nfs/work1/696ds-s21/hhwang/da/emb/tr_02_acl_0.pkl\n",
      "Total passages processed 1688. Written to /mnt/nfs/work1/696ds-s21/hhwang/da/emb/tr_02_acl_0.pkl\n"
     ]
    }
   ],
   "source": [
    "# Encode TR (tsv-> embedding)\n",
    "\n",
    "\n",
    "!bash emb/generate_embedding.sh tr_02_acl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized host node040 as d.rank -1 on device=cuda, n_gpu=1, world size=1\n",
      "16-bits training: False \n",
      "Reading saved model from /mnt/nfs/work1/696ds-s21/hhwang/DPR/data/checkpoint/retriever/multiset/bert-base-encoder.cp\n",
      "model_state_dict keys odict_keys(['model_dict', 'optimizer_dict', 'scheduler_dict', 'offset', 'epoch', 'encoder_params'])\n",
      "Overriding args parameter value from checkpoint state. Param = do_lower_case, value = True\n",
      "Overriding args parameter value from checkpoint state. Param = pretrained_model_cfg, value = bert-base-uncased\n",
      "Overriding args parameter value from checkpoint state. Param = encoder_model_type, value = hf_bert\n",
      "Overriding args parameter value from checkpoint state. Param = sequence_length, value = 256\n",
      " **************** CONFIGURATION **************** \n",
      "batch_size                     -->   32\n",
      "ctx_file                       -->   /mnt/nfs/work1/696ds-s21/hhwang/da/emb/cc_02_acl.tsv\n",
      "device                         -->   cuda\n",
      "distributed_world_size         -->   1\n",
      "do_lower_case                  -->   True\n",
      "encoder_model_type             -->   hf_bert\n",
      "fp16                           -->   False\n",
      "fp16_opt_level                 -->   O1\n",
      "local_rank                     -->   -1\n",
      "model_file                     -->   /mnt/nfs/work1/696ds-s21/hhwang/DPR/data/checkpoint/retriever/multiset/bert-base-encoder.cp\n",
      "n_gpu                          -->   1\n",
      "no_cuda                        -->   False\n",
      "num_shards                     -->   1\n",
      "out_file                       -->   /mnt/nfs/work1/696ds-s21/hhwang/da/emb/cc_02_acl\n",
      "pretrained_file                -->   None\n",
      "pretrained_model_cfg           -->   bert-base-uncased\n",
      "projection_dim                 -->   0\n",
      "sequence_length                -->   256\n",
      "shard_id                       -->   0\n",
      " **************** CONFIGURATION **************** \n",
      "PyTorch version 1.8.0 available.\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/hhwang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/hhwang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/hhwang/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/hhwang/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "All model checkpoint weights were used when initializing HFBertEncoder.\n",
      "\n",
      "All the weights of HFBertEncoder were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use HFBertEncoder for predictions without further training.\n",
      "loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/hhwang/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "Loading saved model state ...\n",
      "reading data from file=/mnt/nfs/work1/696ds-s21/hhwang/da/emb/cc_02_acl.tsv\n",
      "Producing encodings for passages range: 0 to 416 (out of total 416)\n",
      "Encoded passages 160\n",
      "Encoded passages 320\n",
      "Writing results to /mnt/nfs/work1/696ds-s21/hhwang/da/emb/cc_02_acl_0.pkl\n",
      "Total passages processed 416. Written to /mnt/nfs/work1/696ds-s21/hhwang/da/emb/cc_02_acl_0.pkl\n"
     ]
    }
   ],
   "source": [
    "# Encode CC (tsv->embedding)\n",
    "!bash emb/generate_embedding.sh cc_02_acl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688\n",
      "416\n"
     ]
    }
   ],
   "source": [
    "MAX_TR_PSGS = len(train_psgs)\n",
    "MAX_CC_PSGS = len(cc_psgs)\n",
    "print(MAX_TR_PSGS)\n",
    "print(MAX_CC_PSGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n",
      "416 768\n",
      "Number of CC passages: 416\n",
      "(768,)\n",
      "1688 768\n",
      "Number of train passages: 1688\n",
      "[[-0.87804127  0.05203568 -0.06780454 ... -0.5197406  -0.2222211\n",
      "  -0.02371504]\n",
      " [-0.7526125  -0.15401648  0.3375842  ... -0.70269537 -0.06798308\n",
      "   0.0022837 ]\n",
      " [-0.43047038  0.07143795 -0.03356247 ... -0.5204008  -0.0784183\n",
      "   0.17523418]\n",
      " ...\n",
      " [-0.08816212 -0.47372022  0.38190478 ... -0.2598227  -0.25476226\n",
      "   0.04806864]\n",
      " [-0.58373874 -0.27933288  0.5223103  ... -0.47887757 -0.6835381\n",
      "   0.09084422]\n",
      " [-0.4655094  -0.11000601  0.3970269  ... -0.4577696   0.09669067\n",
      "  -0.4276332 ]]\n",
      "trained? True\n",
      "Total number of indexed CC passages:  416\n",
      "\n",
      "Using an indentical CC set\n",
      "================================================================\n",
      "4 nearest neighbors\n",
      "[[  0   2 256 ... 303  47 320]\n",
      " [  1 205 113 ... 306 303 320]\n",
      " [  2   3   0 ... 303  47 320]\n",
      " [  3   2 321 ...  47 303 320]\n",
      " [  4   6   5 ... 299 320  47]]\n",
      "\n",
      "distances(sanity check)\n",
      "[[  0.        30.18142   31.350739 ... 106.24573  108.085365 116.75043 ]\n",
      " [  0.        36.323853  36.505314 ... 122.65997  130.44864  136.31595 ]\n",
      " [  0.        27.522001  30.18142  ... 104.72557  104.994995 126.77202 ]\n",
      " [  0.        27.522001  28.558102 ... 104.08525  105.26668  115.010506]\n",
      " [  0.        26.270412  40.000645 ... 108.734566 118.82944  124.52625 ]]\n",
      "\n",
      "===============================================================\n",
      "Using the query(train set)\n",
      "4 nearest neighbors\n",
      "[[ 23 271  77 ...  47 303 320]\n",
      " [158 269 271 ... 313 303 320]\n",
      " [378 343 157 ...  47 173 320]\n",
      " [269 101 171 ... 306 305 320]\n",
      " [256 171  81 ... 303  47 320]]\n",
      "\n",
      "distances\n",
      "[[ 32.132477  37.836197  37.883453 ... 109.97092  121.01689  139.72974 ]\n",
      " [ 45.708572  47.546204  51.96724  ... 106.85559  113.42859  132.36404 ]\n",
      " [ 51.04306   52.569656  53.01767  ... 107.07019  110.44466  128.80391 ]\n",
      " ...\n",
      " [ 47.195084  47.25467   49.75595  ... 116.073654 121.74666  137.55148 ]\n",
      " [ 48.35608   51.15558   52.454285 ... 126.44472  133.63007  147.86935 ]\n",
      " [ 38.28984   44.62393   45.948853 ... 118.88225  118.92641  121.70952 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Nearest Neighbor (FAISS)\n",
    "################################################################################\n",
    "#print(emb[0])\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Read CC embeddings (DATABASE)\n",
    "cc_embeddings = np.load('emb/' + CC + '_0.pkl', allow_pickle=True)\n",
    "print(cc_embeddings[0][1].shape)  # Dimension of the embedding\n",
    "nb = len(cc_embeddings) # database size\n",
    "d = cc_embeddings[0][1].size\n",
    "print(nb,d)\n",
    "xb = np.zeros((nb,d), dtype='float32')\n",
    "for i in range(nb):\n",
    "    xb[i] = cc_embeddings[i][1]\n",
    "print('Number of CC passages: %d' % nb)\n",
    "\n",
    "# Read train embeddings (QUERY)\n",
    "train_embeddings = np.load('emb/' + TR + '_0.pkl', allow_pickle=True)\n",
    "print(train_embeddings[0][1].shape)  # Dimension of the embedding\n",
    "#nq = len(train_embeddings[:10]) # database size\n",
    "nq = len(train_embeddings)\n",
    "d = train_embeddings[0][1].size\n",
    "print(nq,d)\n",
    "xq = np.zeros((nq,d), dtype='float32')\n",
    "for i in range(nq):\n",
    "    xq[i] = train_embeddings[i][1]\n",
    "\n",
    "print('Number of train passages: %d' % nq)\n",
    "print(xq)\n",
    "\n",
    "\n",
    "index = faiss.IndexFlatL2(d)   # build the index\n",
    "\n",
    "print('trained? %r' % index.is_trained)\n",
    "index.add(xb)                  # add vectors to the index\n",
    "print('Total number of indexed CC passages: ', index.ntotal)\n",
    "print()\n",
    "print('Using an indentical CC set')\n",
    "k = nb                          # we want to see 4 nearest neighbors\n",
    "D, I = index.search(xb[:5], k) # sanity check\n",
    "print('================================================================')\n",
    "print('4 nearest neighbors')\n",
    "print(I)\n",
    "print()\n",
    "print('distances(sanity check)')\n",
    "print(D)\n",
    "print()\n",
    "\n",
    "\n",
    "print('===============================================================')\n",
    "print('Using the query(train set)')\n",
    "D, I = index.search(xq, k)     # actual search\n",
    "print('4 nearest neighbors')\n",
    "print(I[:5])                   # neighbors of the 5 first queries\n",
    "print('\\ndistances')\n",
    "print(D)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train passage\n",
      "It can be shown ( Berger et al. , 1996 ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .\n",
      "\n",
      "CLOSEST passages in CC:\n",
      "-------------------------------------------------------------\n",
      "Closest 0\n",
      "Although such generative models are easy to implement and are intuitive , it is\n",
      "not always the case that generative models perform best , since they are\n",
      "maximizing the joint probability of data and model , rather than directly\n",
      "maximizing conditional probability .Because we do not have gold - standard\n",
      "references for training a secondary conditional reranker , we incorporate weak\n",
      "supervision of evaluations against the perceptual world during the process of\n",
      "improving model performance .All these approaches are evaluated on the two\n",
      "publicly available domains that have been actively used in many other grounded\n",
      "language learning studies .\n",
      "-------------------------------------------------------------\n",
      "Closest 1\n",
      "For every production in the formal language grammar , a Support - Vector Machine\n",
      "( SVM ) classifier is trained using string similarity as the kernel .Meaning\n",
      "representations for novel natural language sentences are obtained by finding the\n",
      "most probable semantic parse using these classifiers .This method does not use\n",
      "any hard - matching rules and unlike previous and other recent methods , does\n",
      "not use grammar rules for natural language , probabilistic or otherwise , which\n",
      "makes it more robust to noisy input .Besides being robust , this approach is\n",
      "also flexible and able to learn under a wide range of supervision , from extra\n",
      "to weaker forms of supervision .\n",
      "-------------------------------------------------------------\n",
      "Closest 2\n",
      "Morgan and Newport ( 1981 ) showed that the reference world of Moeser and\n",
      "Bregman ( 1972 ) was successful in facilitating the acquisition of complex\n",
      "aspects of syntax because it served to demarcate the ...Publications : Learning\n",
      "for Semantic Parsing .Semantic parsing is the process of mapping a natural -\n",
      "language sentence into a formal representation of its meaning .A shallow form of\n",
      "semantic representation is a case - role analysis ( a.k.a . a semantic role\n",
      "labeling ) , which identifies roles such as agent , patient , source , and\n",
      "destination .A deeper semantic analysis provides a representation of the\n",
      "sentence in predicate logic or other formal language which supports automated\n",
      "reasoning .\n",
      "-------------------------------------------------------------\n",
      "Closest 3\n",
      "They regarded parses as being of high quality if 20 different parsers agreed\n",
      ".They used an SVM regression approach on the basis of text - based and parse -\n",
      "based features .A method for acquiring reliable predicate - argument structures\n",
      "We acquire reliable predicate - argument str ... . \" ...It is well known that\n",
      "parsing accuracy suffers when a model is applied to out - of - domain data .It\n",
      "is also known that the most beneficial data to parse a given domain is data that\n",
      "matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .\n",
      "------------------------------------------------------------\n",
      "...\n",
      "-------------------------------------------------------------\n",
      "Farthest 412\n",
      "English source text : Bah , bah , black sheep , have you any wool ?Yes sir , yes\n",
      "sir , three bags full .One for the master , one for the dame , and one for the\n",
      "little boy who lives down the lane .French translation : Bah , bah , mouton noir\n",
      ", vous ont n'importe quelles laines ?Oui monsieur , oui monsieur , trois sacs\n",
      "complètement .Un pour le maître , un pour dame , et un pour le petit garçon qui\n",
      "vit en bas de la ruelle .And back into English again : Bah , bah , black sheep ,\n",
      "have you n ' imports which wools ?\n",
      "-------------------------------------------------------------\n",
      "Farthest 413\n",
      "For the Master , for lady , and for the little boy who lives in bottom of the\n",
      "lane .( ii ) Humpty Dumpty translated into Italian and then back again into\n",
      "English , using Babel Fish .English source text : Humpty Dumpty sat on a wall\n",
      ".Humpty Dumpty had a great fall .Italian translation : Humpty Dumpty si è seduto\n",
      "su una parete .Humpty Dumpty ha avuto una grande caduta .I cavalli di tutto il\n",
      "re e gli uomini di tutto il re non hanno potuto un Humpty ancora .And back into\n",
      "English again : Humpty Dumpty has been based on a wall .\n",
      "-------------------------------------------------------------\n",
      "Farthest 414\n",
      "The horses of all the king and the men of all the king have not been able a\n",
      "Humpty still .( iii ) Humpty Dumpty translated into Italian and then back again\n",
      "into English , using Google Translate .English source text : Humpty Dumpty sat\n",
      "on a wall .Humpty Dumpty had a great fall .Italian translation : Humpty Dumpty\n",
      "sedeva su un muro .Humpty Dumpty ha avuto un grande caduta .Tutti i cavalli del\n",
      "re e tutti gli uomini del re non poteva mettere Humpty di nuovo insieme .And\n",
      "back into English again : Humpty Dumpty sat on a wall .\n",
      "-------------------------------------------------------------\n",
      "Farthest 415\n",
      "Evi 's performance was tested by the author of this paragraph .\"She \"\n",
      "immediately provided correct answers to these questions submitted by voice input\n",
      ": .In which American state is Albuquerque ?In addition , Evi may link to\n",
      "relevant websites that provide further information .Text input is also accepted\n",
      ".In this section we outline the essentials of parsing , first of all by\n",
      "describing the components of a parsing system and then discussing different\n",
      "kinds of parser .We look at one linguistic phenomenon which causes problems for\n",
      "parsing and finally examine potential solutions to the difficulties raised by\n",
      "parsing .\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "print('Train passage')\n",
    "print(train_psgs[300]['doc_text'])\n",
    "print()\n",
    "print('CLOSEST passages in CC:')\n",
    "\n",
    "for i in range(4):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Closest %d' % i)\n",
    "    closest = I[300][i]\n",
    "    print(textwrap.fill(cc_psgs[closest]['doc_text'], 80))\n",
    "\n",
    "print('------------------------------------------------------------')\n",
    "print('...')\n",
    "\n",
    "for i in range(MAX_CC_PSGS-4, MAX_CC_PSGS):\n",
    "    print('-------------------------------------------------------------')\n",
    "    print('Farthest %d' % i)\n",
    "    closest = I[300][i]\n",
    "    print(textwrap.fill(cc_psgs[closest]['doc_text'],80))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
