{"text": "However , the method we are currently using in the ATIS domain ( Seneff et al. 1991 ) represents our most promising approach to this problem .", "label": "Uses", "metadata": {}}
{"text": "The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir ( 2005 ) .", "label": "CompareOrContrast", "metadata": {}}
{"text": "We experiment with four learners commonly employed in language learning : Decision List ( DL ) : We use the DL learner as described in Collins and Singer ( 1999 ) , motivated by its success in the related tasks of word sense disambiguation ( Yarowsky , 1995 ) and NE classification ( Collins and Singer , 1999 ) .", "label": "Motivation", "metadata": {}}
{"text": "Our classification framework , directly inspired by Blum and Chawla ( 2001 ) , integrates both perspectives , optimizing its labeling of speech segments based on both individual speech-segment classification scores and preferences for groups of speech segments to receive the same label .", "label": "Uses", "metadata": {}}
{"text": "For instance , Palmer and Hearst ( 1997 ) report that the SATZ system ( decision tree variant ) was trained on a set of about 800 labeled periods , which corresponds to a corpus of about 16,000 words .", "label": "CompareOrContrast", "metadata": {}}
{"text": "Gurevych ( 2005 ) replicated the experiment of Rubenstein and Goodenough with the original 65 word pairs translated into German .", "label": "Background", "metadata": {}}
{"text": "One approach to this more general problem , taken by the ` Nitrogen ' generator ( Langkilde and Knight , 1998a ; Langkilde and Knight , 1998b ) , takes advantage of standard statistical techniques by generating a lattice of all possible strings given a semantic representation as input and selecting the most likely output using a bigram language model .", "label": "Uses", "metadata": {}}
{"text": "where mk is one mention in entity e , and the basic model building block PL ( L = 1 | e , mk , m ) is an exponential or maximum entropy model ( Berger et al. , 1996 ) .", "label": "Uses", "metadata": {}}
{"text": "Second , using continuous distributions allows us to leverage a variety of tools ( e.g. , LDA ) that have been shown to be successful in other fields , such as speech recognition ( Evermann et al. , 2004 ) .", "label": "Background", "metadata": {}}
{"text": "ASARES is presented in detail in ( Claveau et al. , 2003 ) .", "label": "Uses", "metadata": {}}
{"text": "Previous work with MaltParser in Russian , Turkish , and Hindi showed gains with CASE but not with agreement features ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; Nivre 2009 ) .", "label": "CompareOrContrast", "metadata": {}}
{"text": "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of Pollard and Sag ( 1994 , ch .", "label": "CompareOrContrast", "metadata": {}}
{"text": "In addition , we find that the Bayesian SCFG grammar can not even significantly outperform the heuristic SCFG grammar ( Blunsom et al. 2009 ) 5 .", "label": "CompareOrContrast", "metadata": {}}
{"text": "Although the approach may have potential , the shifting of complex accounting into the unification algorithm is at variance with the findings of Kiefer et al. ( 1999 ) , who report large speed-ups from the elimination of disjunction processing during unification .", "label": "CompareOrContrast", "metadata": {}}
{"text": "successfully parses , or until a quitting criterion is reached , such as an upper bound on N. Whereas in the loosely coupled system the parser acts as a filter only on completed candidate solutions ( Zue et al. 1991 ) , the tightly coupled system allows the parser to discard partial theories that have no way of continuing .", "label": "Uses", "metadata": {}}
{"text": "Zollmann and Venugopal ( 2006 ) substituted the non-terminal X in hierarchical phrase-based model by extended syntactic categories .", "label": "CompareOrContrast", "metadata": {}}
{"text": "Our work is inspired by the latent left-linking model in Chang et al. ( 2013 ) and the ILP formulation from Chang et al. ( 2011 ) .", "label": "Uses", "metadata": {}}
{"text": "After the extraction , pruning techniques ( Snover et al. , 2009 ) can be applied to increase the precision of the extracted paraphrases .", "label": "Background", "metadata": {}}
{"text": "GATE goes beyond earlier systems by using a component-based infrastructure ( Cunningham , 2000 ) which the GUI is built on top of .", "label": "Background", "metadata": {}}
{"text": "description-level lexical rules ( DLRs ; Meurers 1995 ) .5 2.2.1 Meta-Level Lexical Rules .", "label": "Background", "metadata": {}}
{"text": "In a similar vain to Skut and Brants ( 1998 ) and Buchholz et al. ( 1999 ) , the method extends an existing flat shallow-parsing method to handle composite structures .", "label": "Future", "metadata": {}}
{"text": "Note that although our current system uses MeSH headings assigned by human indexers , manually assigned terms can be replaced with automatic processing if needed ( Aronson et al. 2004 ) .", "label": "Future", "metadata": {}}
{"text": "For instance , Sells ( 1985 , p. 8 ) says that the sentence `` Reagan thinks bananas , '' which is otherwise strange , is in fact acceptable if it occurs as an answer to the question `` What is Kissinger 's favorite fruit ? ''", "label": "Motivation", "metadata": {}}
{"text": "AJAX function lets the communication works asyncronously between a client and a server through a set of messages based on HTTP protocol and XML ( Garrett , 2005 ) .", "label": "Background", "metadata": {}}
{"text": "Log-linear models have proved successful in a wide variety of applications , and are the inspiration behind one of the best current statistical parsers ( Charniak , 2000 ) .", "label": "CompareOrContrast", "metadata": {}}
{"text": "While we have observed reasonable results with both G2 and Fisher 's exact test , we have not yet discussed how these results compare to the results that can be obtained with a technique commonly used in corpus linguistics based on the mutual information ( MI ) measure ( Church and Hanks 1990 ) :", "label": "Background", "metadata": {}}
{"text": "And Collins ( 2000 ) argues for `` keeping track of counts of arbitrary fragments within parse trees '' , which has indeed been carried out in Collins and Duffy ( 2002 ) who use exactly the same set of ( all ) tree fragments as proposed in Bod ( 1992 ) .", "label": "Motivation", "metadata": {}}
{"text": "criteria and data used in our experiments are based on the work of Talbot et al. ( 2011 ) .", "label": "Uses", "metadata": {}}
{"text": "Hohensee and Bender ( 2012 ) have conducted a study on dependency parsing for 21 languages using features that encode whether the values for certain attributes are equal or not for a node and its governor .", "label": "Background", "metadata": {}}
{"text": "Our rules for phonological word formation are adopted , for the most part , from G & G , Grosjean and Gee ( 1987 ) , and the account of monosyllabic destressing in Selkirk ( 1984 ) .", "label": "Uses", "metadata": {}}
{"text": "As a generalization , Briscoe ( 2001 ) notes that lexicons such as COMLEX tend to demonstrate high precision but low recall .", "label": "Background", "metadata": {}}
{"text": "This model has previously been shown to provide excellent performance on multiple tasks , including prediction of association norms , word substitution errors , semantic inferences , and word similarity ( Andrews et al. , 2009 ; Silberer and Lapata , 2012 ) .", "label": "Extends", "metadata": {}}
{"text": "We have shown elsewhere ( Jensen and Binot 1988 ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .", "label": "Extends", "metadata": {}}
