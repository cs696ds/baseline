{"text": "A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .Building a translation lexicon from comparable , non - parallel corpora US 8234106 B2 .Abstract .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .The system may use various clues , e.g. , context and frequency , to identify and score other possible translation pairs , using the seed lexicon as a basis .", "label": "", "metadata": {}}
{"text": "A method for building a translation lexicon from non - parallel corpora by a machine translation system , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .expanding the seed lexicon by the machine translation system by identifying possible translations of words in the first and second corpora using one or more clues .The method of .claim 1 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The method of .claim 1 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}}
{"text": "claim 3 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The method of .claim 1 , wherein said one or more clues includes similar spelling .The method of .claim 1 , wherein said identifying comprises identifying cognates .The method of .claim 1 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The method of .claim 1 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}}
{"text": "claim 1 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a target word .The method of .claim 9 , further comprising generating a context vector .The method of .claim 1 , wherein said identifying comprises identifying frequencies of occurrence of words in the first and second first corpora .The method of .claim 1 , further comprising : generating matching scores for each of a plurality of clues .The method of .claim 12 , further comprising adding the matching scores .", "label": "", "metadata": {}}
{"text": "A non - transitory computer readable medium having embodied thereon a program , the program being executable by a processor for performing a method for building a translation lexicon from non - parallel corpora , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .expanding the seed lexicon by the machine translation system by identifying possible translations of words in the first and second corpora using one or more clues .The non - transitory computer readable medium of .claim 15 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}}
{"text": "The non - transitory computer readable medium of . claim 17 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar spelling .The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying cognates .The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .", "label": "", "metadata": {}}
{"text": "The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a targetword .An apparatus comprising : . a lexicon builder operative to be executed to expand the seed lexicon by identifying possible translations of words in the first and second corpora using one or more clues .The apparatus of .claim 24 , wherein the lexicon builder is configured to use the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}}
{"text": "The apparatus of 26 , the apparatus comprising : . an alignment module operative to be executed to align text segments in two nonparallel corpora , the corpora including a source language corpus and a target language corpus .The apparatus of .claim 27 , wherein the alignment module is operative to build a Bilingual Suffix Tree from a text segment from one of said two non - parallel corpora .Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a Continuation of U.S. patent application Ser .No .10/401,124 , filed on Mar. 26 , 2003 , now U.S. Pat .", "label": "", "metadata": {}}
{"text": "7,620,538 , which claims priority to U.S. Provisional Application Ser .No .60/368,070 , filed on Mar. 26 , 2002 , and U.S. Provisional Application Ser .No .60/368,447 , filed on Mar. 27 , 2002 , the disclosures of which are incorporated by reference .ORIGIN OF INVENTION .The research and development described in this application were supported by Defense Advanced Research Project Agency ( DARPA ) under grant number N66001 - 00 - 1 - 8914 .The U.S. Government may have certain rights in the claimed inventions .BACKGROUND .Machine translation ( MT ) concerns the automatic translation of natural language sentences from a first language ( e.g. , French ) into another language ( e.g. , English ) .", "label": "", "metadata": {}}
{"text": "Roughly speaking , statistical machine translation ( SMT ) divides the task of translation into two steps : a word - level translation model and a model for word reordering during the translation process .The statistical models may be trained on parallel corpora .Parallel corpora contain large amounts of text in one language along with their translation in another .Unfortunately , such corpora are available only in limited amounts and cover only in specific genres ( Canadian politics , Hong Kong laws , etc ) .However , monolingual texts exist in higher quantities and in many domains and languages .", "label": "", "metadata": {}}
{"text": "Methods for processing such resources can therefore greatly benefit the field .SUMMARY .In an embodiment , a system may be able to build a translation lexicon from comparable , non - parallel corpora .The system may identify all identically spelled words in the corpora and use these as a seed lexicon for other processes based on clues indicating possible translations .In another embodiment , a system may align text segments in comparable , non - parallel corpora , matching strings in the corpora , and using the matched strings to build a parallel corpus .The system may build a Bilingual Suffix Tree ( BST ) and traverse edges of the BST to identify matched strings .", "label": "", "metadata": {}}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system for building a translation lexicon according to an embodiment .FIG .2 is a flowchart describing a method for building a translation lexicon from non - parallel corpora .FIG .3 is a table showing results of an experiment utilizing the system of .FIG .1 .FIG .4 is a block diagram of a system for building a translation lexicon according to another embodiment .FIG .5 is a suffix tree .FIG .6 is a Generalized Suffix Tree ( GST ) .", "label": "", "metadata": {}}
{"text": "7 is a Bilingual Suffix Tree ( BST ) .FIG .8 is a portion of a BST showing example alignments .FIG .9 are portions of a BST describing left and right alignments .FIG .10 is psuedocode describing an algorithm for learning translations of unknown words .DETAILED DESCRIPTION .FIG .1 shows a system 100 for building a translation lexicon 105 according to an embodiment .The system may use non - parallel monolingual corpora 110 , 115 in two languages to automatically generate one - to - one mapping of words in the two languages .", "label": "", "metadata": {}}
{"text": "For example , in an implementation , an English - German translation lexicon was generated from a 1990 - 1992 Wall Street Journal corpus on the English side and a 1995 - 1996 German news wire ( DPA ) on the German side .Both corpora are news sources in the general sense .However , they span different time periods and have a different orientation : the World Street Journal covers mostly business news , the German news wire mostly German politics .The system 100 may use clues to find translations of words in the monolingual corpora .", "label": "", "metadata": {}}
{"text": "Due to cultural exchange , a large number of words that originate in one language may be adopted by others .Recently , this phenomenon can be seen with words such as \" Internet U or \" Aids u. \" These terms may be adopted verbatim or changed by well - established rules .FIG .2 shows a flowchart describing a method 200 for building a translation lexicon from non - parallel corpora .A word comparator 120 may be used to collect pairs of identical words ( block 205 ) .In the English German implementation described above , 977 identical words were found .", "label": "", "metadata": {}}
{"text": "The correctness of word mappings acquired in this fashion may depend highly on word length .While identical three - letter words were only translations of each other 60 % of the time , this was true for 98 % of ten - letter words .Clearly , for shorter words , the accidental existence of an identically spelled word in the other language word is much higher .Accordingly , the word comparator 120 may restrict the word length to be able to increase the accuracy of the collected word pairs .For instance , by relying only on words at least of length six , 622 word pairs were collected with 96 % accuracy .", "label": "", "metadata": {}}
{"text": "A lexicon builder 125 may expand the seed lexicon into the larger translation lexicon 105 by applying rules based on clues which indicate probable translations .The lexicon builder 125 may use seed lexicon to bootstrap these methods , using the word pairs in the seed lexicon as correct translations .As already mentioned , there are some well established transformation rules for the adoption of words from a foreign language .For German to English , this includes replacing the letters k and z by c and changing the ending -t\u00e4t to -ty .Both these rules can be observed in the word pair Elektrizitat and electricity .", "label": "", "metadata": {}}
{"text": "In the English - German implementation , 363 additional word pairs were collected , with an accuracy of 91 % .The lexicon builder 125 extracts potential translation word pairs based on one or more clues .These clues may include similar spelling , similar context , preserving word similarity , and word frequency .When words are adopted into another language , their spelling might change slightly in a manner that can not be simply generalized in a rule , e.g. , \" website \" and \" Webseite .\" This is even more the case for words that can be traced back to common language roots , such as \" friend \" and \" Freund , \" or \" president \" and \" Pr\u00e4sident . \"", "label": "", "metadata": {}}
{"text": "This can be defined as differing in very few letters .This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .This measurement may be referred to as the \" longest common subsequence ratio .\" The lexicon builder 125 may measure the spelling similarity between every German and English word , and sort possible word pairs accordingly .This may be done in a greedy fashion , i.e. , once a word is assigned to a word pair , the lexicon builder 125 does not look for another match .", "label": "", "metadata": {}}
{"text": "If the monolingual corpora are somewhat comparable , it can be assumed that a word that occurs in a certain context should have a translation that occurs in a similar context .The context may be defined by the frequencies of context words in surrounding positions .This context has to be translated into the other language , and the lexicon builder 125 can search the word with the most similar context .For each occurrence of a target word , the counts may be collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions .", "label": "", "metadata": {}}
{"text": "Finally , the raw counts are normalized .Vector comparison is done by adding all absolute differences of all components .Alternatively , the lexicon builder 125 may count how often another word occurs in the same sentence as the target word .The counts may then be normalized by a using the tf / idf method , which is often used in information retrieval .The seed lexicon may be used to construct context vectors that contain information about how a new unmapped word co - occurs with the seed words .This vector can be translated into the other language , since we already know the translations of the seed words are already known .", "label": "", "metadata": {}}
{"text": "The lexicon builder 125 may compute all possible word , or context vector , matches .The best word matches may be collected in a greedy fashion .Another clue is based on the assumption that pairs of words that are similar in one language should have translations that are similar in the other language .For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .Two words may be defined as similar if they occur in a similar context , which is the case for Wednesday and Thursday .In one approach , the context vector for each word in the lexicon may consist of co - occurrence counts in respect to a number of peripheral tokens ( basically , the most frequent words ) .", "label": "", "metadata": {}}
{"text": "Instead of comparing the co - occurrence counts directly , the Spearman rank order correlation may be applied .For each position , the tokens are compared in frequency and the frequency count is replaced by the frequency rank , e.g. , the most frequent token count is replaced with 1 and the least frequent by n. R . a .b . ) a .i .b .i . )n .n .The result is a matrix with similarity scores between all German words , and a second matrix with similarity scores between all English words .", "label": "", "metadata": {}}
{"text": "Such a vector can be translated into the other language .The translated vector can be compared to other vectors in the second language .The lexicon builder 125 may perform a greedy search for the best matching similarity vectors and add the corresponding words to the lexicon .Another clue is based on the assumption that in comparable corpora , the same concepts should occur with similar frequencies .Frequency may be defined as a ratio of the word frequencies normalized by the corpus sizes .Each of the clues provides a matching score between two words ( block 220 ) , e.g. , a German word and an English word .", "label": "", "metadata": {}}
{"text": "The lexicon builder 125 may employ a greedy search to determine the best set of lexicon entries based on these scores ( block 225 ) .First , the lexicon builder 125 searches for the highest score for any word pair .This is added to the lexicon ( block 230 ) , and word pairs that include either the German and English word are dropped from further search .This may be performed iteratively until all words are used up .The lexicon builder 125 may combine different clues by adding up the matching scores .The scores can be weighted .", "label": "", "metadata": {}}
{"text": "If two words agree in 30 % of their letters , this is generally as bad as if they do not agree in any , i.e. , the agreements are purely coincidental .FIG .3 shows results of the English - German implementation .\" Entries \" indicate the number of correct lexicon entries that were added to a seed lexicon of 1337 identically spelled words , and \" Corpus \" indicates how well the resulting translation lexicon performs compared to the actual word - level translations in a parallel corpus .The English - German implementation was restricted to nouns .", "label": "", "metadata": {}}
{"text": "They might also provide useful context information that is beneficial to building a noun lexicon .These methods may be also useful given a different starting point .For example , when building machine translation systems , some small parallel text should be available .From these , some high - quality lexical entries can be learned , but there will always be many words that are missing .These may be learned using the described methods .FIG .4 shows a system 400 for building a translation lexicon according to another embodiment .The system 400 may also be used to build parallel corpora from comparable corpora .", "label": "", "metadata": {}}
{"text": "The parts can be arbitrarily long , i.e. , the system 400 may align sequences of a few words rather than or in addition to whole sentences or whole phrases .Based on these alignments , the system 400 may generate a parallel corpus 420 and identify translations 425 of words from the source language which are not in the lexicon .For example , consider the following two sentences where the only unknown French word is \" raison \" : .\" Ce est pour cette raison que le initiative de le ministre . . .; \" and .", "label": "", "metadata": {}}
{"text": "Since \" Ce est pour cette \" can be aligned with \" It is for this \" and \" que le \" with \" that the , \" it is a reasonable assumption that \" raison \" can be translated by \" reason .\" The system 400 may search the corpora for cases similar to this example .The system 400 may use a suffix tree data structure in order to identify the alignments .The suffix tree of a string uniquely encodes all the suffixes of that string ( and thus , implicitly , all its substrings too ) .", "label": "", "metadata": {}}
{"text": "The next step is to identify unknown target language words that are surrounded by aligned substrings .The source language word that corresponds to the \" well - aligned \" unknown is considered to be a possible translation .A suffix tree stores in linear space all suffixes of a given string .Such succinct encoding exposes the internal structure of the string , providing efficient ( usually linear - time ) solutions for many complex string problems , such as exact and approximate string matching , finding the longest common substring of multiple strings , and string compression .", "label": "", "metadata": {}}
{"text": "5 shows the suffix tree 500 of string xyzyxzy .Note that if a suffix of a string is also a prefix of another suffix ( as would be the case for suffix zy of string xyzyxzy ) , a proper suffix tree can not be built for the string .The problem is that the path corresponding to that suffix would not end at a leaf , so the tree can not have the last property in the list above .To avoid this , the system 400 appends an end - of - string marker \" $ \" that appears nowhere else in the string .", "label": "", "metadata": {}}
{"text": "Each monolingual corpus given as input to the system 400 may be divided into a set of sentences .The system 400 may use a variant of suffix trees that works with sets of strings , namely Generalized Suffix Trees ( GST ) .In a GST of a set of strings , each path from the root to a leaf represents a suffix in one or more strings from the set .FIG .6 shows the GST 600 for a corpus of two sentences .The numbers at the leaves 605 of the tree show which sentences contain the suffix that ends there .", "label": "", "metadata": {}}
{"text": "Building a GST for a set of strings takes time and space linear in the sum of the lengths of all strings in the set .A Bilingual Suffix Tree ( BST ) is the result of matching a source language GST against a target language GST .Two strings ( i.e. , sequences of words ) match if the corresponding words are translations of each other according to a bilingual lexicon .In order to perform the matching operation , all paths that correspond to an exhaustive traversal of one of the trees ( the source tree ) are traversed in the other ( the target tree ) , until a mismatch occurs .", "label": "", "metadata": {}}
{"text": "FIG .7 shows two corpora 705 , 710 , a bilingual lexicon 715 , and the corresponding BST 720 .Edges drawn with dotted lines mark ends of alignment paths through the tree .Their labels are ( unaligned ) continuations of the source language substrings from the respective paths .Since there is a one - to - one correspondence between the substrings in the text and the paths in the suffix trees , the operation described above will yield all pairs of substrings in the two corpora given as input and discover all partial monotone alignments defined by the lexicon .", "label": "", "metadata": {}}
{"text": "The paths in the resulting bilingual tree will also have weights associated with them , defined as the product of the matching probabilities of the words along the path .BSTs are constructed to encode alignment information , therefore the extraction of parallel phrases amounts to a simple depth - first traversal of the tree .FIG .8 shows some alignments we can extract from the BST in .FIG .7 , a portion of which is shown in .FIG .8 .As can be seen in .For alignment extraction , we are interested in edges of the third type , because they mark ends of alignments .", "label": "", "metadata": {}}
{"text": "The fact that n has outgoing edge e indicates there is a mismatch on the subsequent words of those two sequences .Thus , in order to extract all aligned substrings , the system 400 traverses the BST on edges labeled with word pairs , and extract all paths that end either at the leaves or at nodes that have outgoing edges labeled only with source language words .The heuristic by which the system 400 discovers new word translations is shown graphically in .FIG .9 .FIG .9 ( i ) shows a branch 905 of the BST corresponding to the comparable corpus in the same figure .", "label": "", "metadata": {}}
{"text": "This may be taken as a weak indication that d and y are translations of each other .This indication would become stronger if , for example , the sequences following d and y in the two corpora would also be aligned .One way to verify this is to reverse both strings , build a BST for the reversed corpora ( a reverse BST ) , and look for a common path that diverges at the same d and y. .FIG .9 ( ii ) shows the reverse BST 910 , and in bold , the path we are interested in .", "label": "", "metadata": {}}
{"text": "For a pair of words from the two corpora , we use the terms \" right alignment \" and \" left alignment \" to refer to the aligned sequences that precede and respectively succeed the two words in each corpus .The left and right alignments and the two words delimited by them make up a context alignment .For example , the left alignment xyzabc , the right alignment xzy - acb and the words y and d in .FIG .9 ( iii ) make up a context alignment 915 .Given a comparable corpus , this procedure will yield many context alignments which correspond to incorrect translations , such as that between the words \" canadien \" and \" previous \" : . tout canadien serieux .", "label": "", "metadata": {}}
{"text": "In order to filter out such cases , the system 400 uses two simple heuristics : length and word content .The translation candidate must also be an open - class word .The algorithm 1000 for learning translations of unknown words is summarized in .FIG .10 .An advantage of the algorithm over previous approaches is that we do not provide as input to the algorithm a list of unknown words .Instead , the system automatically learns from the corpus both the unknown words and their translation , upon discovery of appropriate context alignments .It was obtained by taking two non - parallel , nonaligned segments from the Hansard corpus .", "label": "", "metadata": {}}
{"text": "A small bilingual lexicon of 6,900 entries was built using 5,000 sentences pairs ( 150,000 words for each language ) .The parallel corpus was taken from the Proceedings of the European Parliament ( EuroParl )Note that the parallel corpus belongs to a different domain than the comparable corpus .Also the parallel corpus is extremely small .For low density languages , such a corpus can be built manually .When given as input the comparable corpora described above and the bilingual lexicon of 6,900 entries , the algorithm 1000 found 33,926 parallel sequences , with length between three and seven words .", "label": "", "metadata": {}}
{"text": "The system also found translations for thirty unknown French words .Of these , nine were correct , which means a precision of 30 % .For each of the two corpora , building the monolingual GST took only 1.5 minutes .The matching operation that yields the BST is the most time - consuming : it lasted 38 hours for the forward BST and 60 hours for the reverse BST .The extractions of all parallel phrases and of the translations took about 2 hours each .The experiments were run on a Linux \u00ae system 400 with an Intel \u00ae Pentium \u00ae 3 processor of 866 Mhz .", "label": "", "metadata": {}}
{"text": "Nevertheless , it will be understood that various modifications may be made without departing from the spirit and scope of the invention .For example , blocks in the flowcharts may be skipped or performed out of order and still produce desirable results .Also , the heuristics described herein may be combined with the alignment method described herein .Accordingly , other embodiments are within the scope of the following claims .Document conversion system including data monitoring means that adds tag information to hyperlink information and translates a document when such tag information is included in a document retrieval request .", "label": "", "metadata": {}}
{"text": "Annual Meeting of the ACL Assoc . for Computational Linguistics , Morristown , NJ , 597 - 604 .Fox , H. , \" Phrasal Cohesion and Statistical Machine Translation \" Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , Jul. 2002 , pp .304 - 311 .Association for Computational Linguistics .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . abstract .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . of the 38th Annual Meeting of the Association for Computational Lingustics , ' Online !", "label": "", "metadata": {}}
{"text": "440 - 447 , XP002279144Hong Kong , China Retrieved from the Internet : retrieved on May 6 , 2004 ! abstract .Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .Annual Meeting of the ACL Assoc . for Computational Linguistics , Morristown , NJ , 80 - 87 .Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}}
{"text": "Marcu , Daniel , \" Building Up Rhetorical Structure Trees , \" 1996 , Proc . of the National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference , vol .2 , pp .1069 - 1074 .Och , F. , \" Minimum Error Rate Training in Statistical Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .Annual Meeting of the ACL .Assoc . for Computational Linguistics , Morristown , NJ , 160 - 167 .", "label": "", "metadata": {}}
{"text": "Human Language Technology Conference .Assoc . for Computational Linguistics , Morristown , NJ .Conference : EMNLP - CoNLL 2007 , Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , June 28 - 30 , 2007 , Prague , Czech Republic .We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .", "label": "", "metadata": {}}
{"text": "Proceedings of the CoNLL Shared Task Session of EMNLP - CoNLL 2007 , pp .1051 - 1055 , Prague , June 2007 .c ?upenn.edu 2L2F - INESC - ID Lisboa / IST , Rua Alves Redol 9 , 1000 - 029 , Lisboa , Portugal javg@l2f.inesc-id.pt Abstract We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks .Our suspicions are supported by the observation that no team was able to im- prove target domain performance substan- tially over a state of the art baseline . 1 Introduction Dependency parsing , an important NLP task , can be done with high levels of accuracy .", "label": "", "metadata": {}}
{"text": "This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation ( Nivre et al . , 2007 ) .The goal was to adapt a parser trained on a single source domain to a new target domain us- ing only unlabeled data .15 K sentences of labeled text from the Wall Street Journal ( WSJ ) ( Marcus et al . , 1993 ; Johansson and Nugues , 2007 ) as well as 200 K unlabeled sentences .The development data was 200 sentences of labeled biomedical oncology text ( BIO , the ONCO portion of the Penn Biomedical Treebank ) , as well as 200 K unlabeled sentences ( Kulick et al .", "label": "", "metadata": {}}
{"text": "The two test domains were a collection of medline chem- istry abstracts ( pchem , the CYP portion of the Penn Biomedical Treebank ) and the Child Language Data Exchange System corpus ( CHILDES ) ( MacWhin- ney , 2000 ; Brown , 1973 ) .We used the second or- der two stage parser and edge labeler of McDonald et al .( 2006 ) , which achieved top results in the 2006 We were given around CoNLL - X shared task .Preliminary experiments in- dicated that the edge labeler was fairly robust to do- main adaptation , lowering accuracy by 3 % in the de- velopment domain as opposed to 2 % in the source , so we focused on unlabeled dependency parsing .", "label": "", "metadata": {}}
{"text": "However , our results were obtained without adap- tation .Given our position in the ranking , this sug- gests that no team was able to significantly improve performance on either test domain beyond that of a state - of - the - art parser .After much effort in developing adaptation meth- ods , it is critical to understand the causes of these negative results .In what follows , we provide an er- ror analysis that attributes domain loss for this task to a difference in annotation guidelines between do- mains .We then overview our attempts to improve adaptation .", "label": "", "metadata": {}}
{"text": "2 Parsing Challenges We begin with an error analysis for adaptation be- tween WSJ and BIO .We divided the available WSJ data into a train and test set , trained a parser on the train set and compared errors on the test set and BIO .Accuracy dropped from 90 % on WSJ to 84 % on BIO .We then computed the fraction of er- rors involving each POS tag .For the most common 1While only 8 teams participated in the closed track with us , our score beat all of the teams in the open track .", "label": "", "metadata": {}}
{"text": "The parser was trained on the provided WSJ data .Digits are less than 4 % of the tokens in BIO .Errors result from the BIO annotations for long sequences of digits which do not appear in WSJ .Since these annotations are new with respect to the WSJ guidelines , it is impossi- ble to parse these without injecting knowledge of the annotation guidelines.3 common , comprising 33 % of BIO and 30 % of WSJ tokens , the most popular POS tag by far .Addi- tionally , other POS types listed above ( adjectives , prepositions , determiners , conjunctions)oftenattach to nouns .", "label": "", "metadata": {}}
{"text": "Adaptation performance rose on BIO from 78 % without the feature to 87 % with the feature .This indicates that most of the loss comes from missing these edges .The primary problem for nouns is the difference between structures in each domain . tion guidelines for the Penn Treebank flattened noun phrases to simplify annotation ( Marcus et al . , 1993 ) , so there is no complex structure to NPs . K\u00a8 ubler ( 2006 ) showed that it is difficult to compare the Penn Treebank to other treebanks with more com- plexnounstructures , suchasBIO.ConsidertheWSJ phrase \" the New York State Insurance Department \" .", "label": "", "metadata": {}}
{"text": "Nouns are far more The annota- 2We measured these drops on several other dependency parsers and found similar results .ery token is headed by \" Department \" .In contrast , a similar BIO phrase has a very different structure , pursuant to the BIO guidelines .For \" the detoxi- cation enzyme glutathione transferase P1 - 1 \" , \" en- zyme \" is the head of the NP , \" P1 - 1 \" is the head of \" transferase \" , and \" transferase \" is the head of \" glu- tathione \" .Since the guidelines differ , we observe no corresponding structure in the WSJ .", "label": "", "metadata": {}}
{"text": "Unlabeled data can not indicate that BIO uses a different standard .Another problem concerns appositives .For ex- ample , the phrase \" Howard Mosher , president and chief executive officer , \" has \" Mosher \" as the head of \" Howard \" and of the appositive NP delimited by commas .While similar constructions occur in BIO , there are no commas to indicate this .An example is the above BIO NP , in which the phrase \" glutathione transferase P1 - 1 \" is an appositive indicating which \" enzyme \" is meant .However , since there are no commas , the parser thinks \" P1 - 1 \" is the head .", "label": "", "metadata": {}}
{"text": "In addition to a change in the annotation guide- lines for NPs , we observed an important difference in the distribution of POS tags .NN tags were almost twice as likely in the BIO domain ( 14 % in WSJ and 25 % in BIO ) .NNP tags , which are close to 10 % of the tags in WSJ , are nonexistent in BIO ( .24 % ) .The cause for this is clear when the annotation guide- lines are considered .The proper nouns in WSJ are names of companies , people and places , while in BIO they are names of genes , proteins and chemi- cals .", "label": "", "metadata": {}}
{"text": "This decision effectively removes NNP from the BIO domain and renders all features that depend on the NNP tag ineffective .In our above BIO NP example , all nouns are labeled NN , whereas the WSJ example contains NNP tags .The largest tri - gram differences involve nouns , such as NN - NN-NN , NNP - NNP - NNP , NN - IN - NN , and IN - NN - NN .However , when we examine the coarse POS tags , which do not distinguish between nouns , these dif- ferences disappear .This indicates that while the overall distribution of POS tags is similar between the domains , the fine grained tags differ .", "label": "", "metadata": {}}
{"text": "Page 3 . hurt WSJ performance but did not affect BIO .Finally , we examined the effect of unknown words .Not surprisingly , the most significant dif- ferences in error rates concerned dependencies be- tween words of which one or both were unknown to the parser .For two words that were seen in the training data loss was 4 % , for a single unknown word loss was 15 % , and 26 % when both words were unknown .Both words were unknown only 5 % of the time in BIO , while one of the words being un- known was more common , reflecting 27 % of deci- sions .", "label": "", "metadata": {}}
{"text": "Recent theoretical work on domain adapta- tion ( Ben - David et al . , 2006 ) attributes adaptation loss to two sources : the difference in the distribu- tion between domains and the difference in label- ing functions .Adaptation techniques focus on the former since it is impossible to determine the lat- ter without knowledge of the labeling function .In parsing adaptation , the former corresponds to a dif- ference between the features seen in each domain , such as new words in the target domain .The de- cision function corresponds to differences between annotation guidelines between two domains .", "label": "", "metadata": {}}
{"text": "Therefore , significant im- provements can not be made without specific knowl- edgeofthetargetdomain'sannotationstandards .No amount of source training data can help if no rele- vant structure exists in the data .Given the results for the domain adaptation track , it appears no team successfully adapted a state - of - the - art parser .3 Adaptation Approaches We survey the main approaches we explored for this task .While some of these approaches provided a modest performance boost to a simple parser ( lim- ited data and first - order features ) , no method added any performance to our best parser ( all data and second - order features ) .", "label": "", "metadata": {}}
{"text": "We began with the first ap- proach and removed a large number of features that we believed transfered poorly , such as most features for noun - noun edges .We obtained a small improve- ment in BIO performance on limited data only .We thenaddedseveraldifferenttypesoffeatures , specif- ically designed to improve noun phrase construc- tions , such as features based on the lexical position of nouns ( common position in NPs ) , frequency of occurrence , and NP chunking information .For ex- ample , trained on in - domain data , nouns that occur more often tend to be heads .", "label": "", "metadata": {}}
{"text": "A final type of feature we added was based on the behavior of nouns , adjectives and verbs in each domain .We constructed a feature representation of words based on adjacent POS and words and clustered words using an algorithm similar to that of Saul and Pereira ( 1997 ) .For example , our clus- tering algorithm grouped first names in one group and measurements in another .We then added the cluster membership as a lexical feature to the parser .None of the resulting features helped adaptation .3.2Diversity Training diversity may be an effective source for adaptation .", "label": "", "metadata": {}}
{"text": "We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge , as well as conjunctions with edge types .This failed to improve BIO accuracy since these features were less reliable at test time .Next , we tried instance bagging ( Breiman , 1996 ) to generate some diversity among parsers .We selected with replacement 2000 training examples from the training data and trained three parsers .Each parser then tagged the remain- ing 13 K sentences , yielding 39 K parsed sentences .We then shuffled these sentences and trained a final parser .", "label": "", "metadata": {}}
{"text": "To address conflicting annota-1053 .Page 4 . tions , we added slack variables to the MIRA learn- ing algorithm ( Crammer et al . , 2006 ) used to train the parsers , without success .We measured diversity by comparing the parses of each model .The dif- ference in annotation agreement between the three instance bagging parsers was about half the differ- ence between these parsers and the gold annotations .While we believe this is not enough diversity , it was not feasible to repeat our experiment with a large number of parsers . 3.3", "label": "", "metadata": {}}
{"text": "We first mod- ified the weight given by the parser to each training sentence based on the similarity of the sentence to target domain sentences .This can be done by mod- ifying the loss to limit updates in cases where the sentence does not reflect the target domain .We tried a number of criteria to weigh sentences without suc- cess , including sentence length and number of verbs .Next , we trained a discriminative model on the pro- vided unlabeled data to predict the domain of each sentence based on POS n - grams in the sentence .", "label": "", "metadata": {}}
{"text": "Further experiments showed that any decrease in training data hurt parser perfor- mance .It would seem that the parser has no dif- ficulty learning important training sentences in the presence of unimportant training examples .A related idea focused on words , weighing highly tokens that appeared frequently in the target domain .We scaled the loss associated with a token by a fac- tor proportional to its frequency in the target do- main .We found certain scaling techniques obtained tiny improvements on the target domain that , while significant compared to competition results , are not statistically significant .", "label": "", "metadata": {}}
{"text": "A very predic- tive source domain feature is not useful if it does not appear in the target domain .However , limiting the feature space to target domain features had no effect .Instead , we scaled each feature 's value by a factor proportional to its frequency in the target do- main and trained the parser on these scaled feature values .We obtained small improvements on small amounts of training data .Target Focused Learning 4 Future Directions Given our pessimistic analysis and the long list of failed methods , one may wonder if parser adapta- tion is possible at all .", "label": "", "metadata": {}}
{"text": "First , there may be room for adaptation with our domains if a common annotation scheme is used .Second , we have stressed that typical adaptation , modifying a model trained on the source domain , will fail but there may be unsupervised parsing techniques that improve performance after adaptation , such as a rule based NP parser for BIO based on knowledge of the annotations .However , this approach is unsatisfying as it does not allow general purpose adaptation .5 Acknowledgments We thank Joel Wallenberg and Nikhil Dinesh for their informative and helpful linguistic expertise , Kevin Lerman for his edge labeler code , and Koby Crammer for helpful conversations .", "label": "", "metadata": {}}
{"text": "Any opinions , findings , and conclusions or recommen- dations expressed in this material are those of the author(s ) and do not necessarily reflect the views of the DARPA or the Department of Interior - National Business Center ( DOI - NBC ) .References Shai Ben - David , John Blitzer , Koby Crammer , and Fer- nando Pereira .Analysis of representations for domain adaptation .In NIPS .Leo Breiman .Learning , 24(2):123 - 140 .Bagging predictors .Machine R. Brown .A First Language : The Early Stages .Harvard University Press .", "label": "", "metadata": {}}
{"text": "Shwartz , and Yoram Singer .Online passive- aggressive algorithms .Journal of Machine Learning Research , 7:551 - 585 , Mar. R. Johansson and P. Nugues .constituent - to - dependency conversion for English .In Proc . of the 16th Nordic Conference on Computational Linguistics ( NODALIDA ) .Extended Sandra K\u00a8 ubler . schemes influence parsing results ? or how not to com- pare apples and oranges .In RANLP .How do treebank annotation 1054 .Page 5 . S. Kulick , A. Bies , M. Liberman , M. Mandel , R. Mc- Donald , M. Palmer , A. Schein , and L. Ungar .", "label": "", "metadata": {}}
{"text": "In Proc . of the Human Language Technol- ogy Conference and the Annual Meeting of the North American Chapter of the Association for Computa- tional Linguistics ( HLT / NAACL ) .B. MacWhinney .The CHILDES Project : Tools for Analyzing Talk .Lawrence Erlbaum .M. Marcus , B. Santorini , and M. Marcinkiewicz .Building a large annotated corpus of English : the Penn Treebank .Computational Linguistics , 19(2):313 - 330 .Ryan McDonald , Kevin Lerman , and Fernando Pereira .Multilingual dependency parsing with a two- stage discriminative parser .In Conference on Natural Language Learning ( CoNLL ) .", "label": "", "metadata": {}}
{"text": "2007 shared task on dependency parsing . of the CoNLL 2007 Shared Task .Joint Conf . on Em- pirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP- CoNLL ) .The CoNLL In Proc .Lawrence Saul and Fernando Pereira . gate and mixed - order markov models for statistical language modeling .In EMNLP . Aggre-1055 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" One of the main challenges in natural language processing ( NLP ) is to correct for biases in the manually annotated data available to system engineers .Domain adaptation problems are typically framed as adapting models that were induced on newswire to other domains , such as spoken language , literature , or social media . \" \" Naturally the corpus must be controlled so that all texts come from a similar domain and genre .Many studies have indeed shown that cross - domain learned corpora yield poor language models [ 35].", "label": "", "metadata": {}}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Background The increasing availability of Electronic Health Record ( EHR ) data and specifically free - text patient notes presents opportunities for phenotype extraction .Text - mining methods in particular can help disease modeling by mapping named - entities mentions to terminologies and clustering semantically related terms .EHR corpora , however , exhibit specific statistical and linguistic characteristics when compared with corpora in the biomedical literature domain .We focus on copy - and - paste redundancy : clinicians typically copy and paste information from previous notes when documenting a current patient encounter .", "label": "", "metadata": {}}
{"text": "In this paper , we ask three research questions : ( i ) How can redundancy be quantified in large - scale text corpora ?( ii )Conventional wisdom is that larger corpora yield better results in text mining .But how does the observed EHR redundancy affect text mining ?Does such redundancy introduce a bias that distorts learned models ?Or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus ?( iii )How can one mitigate the impact of redundancy on text mining ?Results We analyze a large - scale EHR corpus and quantify redundancy both in terms of word and semantic concept repetition .", "label": "", "metadata": {}}
{"text": "We measure the impact of redundancy on two standard text - mining applications : collocation identification and topic modeling .We compare the results of these methods on synthetic data with controlled levels of redundancy and observe significant performance variation .Finally , we compare two mitigation strategies to avoid redundancy - induced bias : ( i ) a baseline strategy , keeping only the last note for each patient in the corpus ; ( ii ) removing redundant notes with an efficient fingerprinting - based algorithm .aFor text mining , preprocessing the EHR corpus with fingerprinting yields significantly better results .", "label": "", "metadata": {}}
{"text": "While the importance of data cleaning has been known for low - level text characteristics ( e.g. , encoding and spelling ) , high - level and difficult - to - quantify corpus characteristics , such as naturally occurring redundancy , can also hurt text mining .Fingerprinting enables text - mining techniques to leverage available data in the EHR corpus , while avoiding the bias introduced by redundancy .[ Show abstract ] [ Hide abstract ] ABSTRACT : Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .", "label": "", "metadata": {}}
{"text": ", 2010a ) , a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures .These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .Learning to Classify Text .Detecting patterns is a central part of Natural Language Processing .Words ending in -ed tend to be past tense verbs ( 5 . )", "label": "", "metadata": {}}
{"text": "These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic .But how did we know where to start looking , which aspects of form to associate with which aspects of meaning ?The goal of this chapter is to answer the following questions : .How can we identify particular features of language data that are salient for classifying it ?How can we construct models of language that can be used to perform language processing tasks automatically ?What can we learn about language from these models ?", "label": "", "metadata": {}}
{"text": "We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) .Before looking at these methods , we first need to appreciate the broad scope of this topic .1 Supervised Classification .Classification is the task of choosing the correct class label for a given input .In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance .Some examples of classification tasks are : .", "label": "", "metadata": {}}
{"text": "Deciding what the topic of a news article is , from a fixed list of topic areas such as \" sports , \" \" technology , \" and \" politics . \"Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution .The basic classification task has a number of interesting variants .For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified .", "label": "", "metadata": {}}
{"text": "The framework used by supervised classification is shown in 1.1 .Figure 1.1 : Supervised Classification .( a )During training , a feature extractor is used to convert each input value to a feature set .These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section .Pairs of feature sets and labels are fed into the machine learning algorithm to generate a model .( b )During prediction , the same feature extractor is used to convert unseen inputs to feature sets .", "label": "", "metadata": {}}
{"text": "In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks .Our discussion is not intended to be comprehensive , but to give a representative sample of tasks that can be performed with the help of text classifiers . 1.1Gender Identification .In 4 we saw that male and female names have some distinctive characteristics .Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male .", "label": "", "metadata": {}}
{"text": "The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features .For this example , we 'll start by just looking at the final letter of a given name .The following feature extractor function builds a dictionary containing relevant information about a given name : .The returned dictionary , known as a feature set , maps from feature names to their values .Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' .", "label": "", "metadata": {}}
{"text": "Note .Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings .But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute .Indeed , it is even possible to use very complex and informative values , such as the output of a second supervised classifier , as features .Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels .", "label": "", "metadata": {}}
{"text": "The training set is used to train a new \" naive Bayes \" classifier .We will learn more about the naive Bayes classifier later in the chapter .For now , let 's just test it out on some names that did not appear in its training data : .Observe that these character names from The Matrix are correctly classified .Although this science fiction movie is set in 2199 , it still conforms with our expectations about names and genders .We can systematically evaluate the classifier on a much larger quantity of unseen data : .", "label": "", "metadata": {}}
{"text": "This listing shows that the names in the training set that end in \" a \" are female 33 times more often than they are male , but names that end in \" k \" are male 32 times more often than they are female .These ratios are known as likelihood ratios , and can be useful for comparing different feature - outcome relationships .Note .Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative .", "label": "", "metadata": {}}
{"text": "When working with large corpora , constructing a single list that contains the features of every instance can use up a large amount of memory .In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2Choosing The Right Features .Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model .Much of the interesting work in building a classifier is deciding what features might be relevant , and how we can represent them .", "label": "", "metadata": {}}
{"text": "Typically , feature extractors are built through a process of trial - and - error , guided by intuitions about what information is relevant to the problem .It 's common to start with a \" kitchen sink \" approach , including all the features that you can think of , and then checking to see which features actually are helpful .We take this approach for name gender features in 1.2 .def gender_features2 ( name ) : . lower ( ) . lower ( ) for letter in ' abcdefghijklmnopqrstuvwxyz ' : . count(letter ) .return features .", "label": "", "metadata": {}}
{"text": "py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features .The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus .This problem is known as overfitting , and can be especially problematic when working with small training sets .Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis .First , we select a development set , containing the corpus data for creating the model .This development set is then subdivided into the training set and the dev - test set .", "label": "", "metadata": {}}
{"text": "The test set serves in our final evaluation of the system .For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set .The division of the corpus data into different subsets is shown in 1.3 .Figure 1.3 : Organization of corpus data for training supervised classifiers .The corpus data is divided into two sets : the development set , and the test set .The development set is often further subdivided into a training set and a dev - test set .", "label": "", "metadata": {}}
{"text": "Using the dev - test set , we can generate a list of the errors that the classifier makes when predicting name genders : .We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) .The feature set can then be adjusted accordingly .The names classifier that we have built generates about 100 errors on the dev - test corpus : .", "label": "", "metadata": {}}
{"text": "For example , names ending in yn appear to be predominantly female , despite the fact that names ending in n tend to be male ; and names ending in ch are usually male , even though names that end in h tend to be female .We therefore adjust our feature extractor to include features for two - letter suffixes : .Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : .This error analysis procedure can then be repeated , checking for patterns in the errors that are made by the newly improved classifier .", "label": "", "metadata": {}}
{"text": "But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data .It is therefore important to keep the test set separate , and unused , until our model development is complete .At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification .In 1 , we saw several examples of corpora where documents have been labeled with categories .", "label": "", "metadata": {}}
{"text": "First , we construct a list of documents , labeled with the appropriate categories .For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative .words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews .Next , we define a feature extractor for documents , so the classifier will know which aspects of the data it should pay attention to ( 1.4 ) .For document topic identification , we can define a feature for each word , indicating whether the document contains that word .", "label": "", "metadata": {}}
{"text": "We can then define a feature extractor that simply checks whether each of these words is present in a given document . words ( ) ) .return features . words ( ' pos / cv957_8737 .The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) .Now that we 've defined our feature extractor , we can use it to train a classifier to label new movie reviews ( 1.5 ) .", "label": "", "metadata": {}}
{"text": "And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging .In 5 .we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word .However , this regular expression tagger had to be hand - crafted .Instead , we can train a classifier to work out which suffixes are most informative .Let 's begin by finding out what the most common suffixes are : .", "label": "", "metadata": {}}
{"text": "Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties .The classifier will rely exclusively on these highlighted properties when determining how to label inputs .In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has .Now that we 've defined our feature extractor , we can use it to train a new \" decision tree \" classifier ( to be discussed in 4 ): .", "label": "", "metadata": {}}
{"text": "Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \" , \" .Next , the classifier checks if the word ends in \" the \" , in which case it 's almost certainly a determiner .This \" suffix \" gets used early by the decision tree because the word \" the \" is so common .Continuing on , the classifier checks if the word ends in \" s \" .1.5 Exploiting Context .By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix .", "label": "", "metadata": {}}
{"text": "But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \" fly , \" knowing that the previous word is \" a \" will allow us to determine that it is functioning as a noun , not a verb .In order to accommodate features that depend on a word 's context , we must revise the pattern that we used to define our feature extractor .Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word .", "label": "", "metadata": {}}
{"text": "Example 1.6 ( code_suffix_pos_tag . py ) : Figure 1.6 : A part - of - speech classifier whose feature detector examines the context in which a word appears in order to determine which part of speech tag should be assigned .In particular , the identity of the previous word is included as a feature .It is clear that exploiting contextual features improves the performance of our part - of - speech tagger .For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \" large \" or the word \" gubernatorial \" .", "label": "", "metadata": {}}
{"text": "In general , simple classifiers always treat each input as independent from all other inputs .In many contexts , this makes perfect sense .For example , decisions about whether names tend to be male or female can be made on a case - by - case basis .However , there are often cases , such as part - of - speech tagging , where we are interested in solving classification problems that are closely related to one another .1.6 Sequence Classification .In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs .", "label": "", "metadata": {}}
{"text": "One sequence classification strategy , known as consecutive classification or greedy sequence classification , is to find the most likely class label for the first input , then to use that answer to help find the best label for the next input .The process can then be repeated until all of the inputs have been labeled .This strategy is demonstrated in 1.7 .First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far .Each tag in history corresponds with a word in sentence .", "label": "", "metadata": {}}
{"text": "Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) .Having defined a feature extractor , we can proceed to build our sequence classifier .During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk .", "label": "", "metadata": {}}
{"text": "history.append(tag ) .history.append(tag ) .return zip(sentence , history ) .1.7 Other Methods for Sequence Classification .One shortcoming of this approach is that we commit to every decision that we make .For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake .One solution to this problem is to adopt a transformational strategy instead .Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs .", "label": "", "metadata": {}}
{"text": "Another solution is to assign scores to all of the possible sequences of part - of - speech tags , and to choose the sequence whose overall score is highest .This is the approach taken by Hidden Markov Models .Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags .However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags .These probabilities are then combined to calculate probability scores for tag sequences , and the tag sequence with the highest probability is chosen .", "label": "", "metadata": {}}
{"text": "Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence .In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) .Given that restriction , it is possible to use dynamic programming ( 4.7 ) to efficiently find the most likely tag sequence .In particular , for each consecutive word index i , a score is computed for each possible current and previous tag .", "label": "", "metadata": {}}
{"text": "2 Further Examples of Supervised Classification .2.1 Sentence Segmentation .Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence .The first step is to obtain some data that has already been segmented into sentences and convert it into a form that is suitable for extracting features : .Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens .", "label": "", "metadata": {}}
{"text": "tokens[i-1].Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : .To use this classifier to perform sentence segmentation , we simply check each punctuation mark to see whether it 's labeled as a boundary ; and divide the list of words at the boundary marks .The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) .", "label": "", "metadata": {}}
{"text": "When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker .This interpretation is most straightforward for performative statements such as \" I forgive you \" or \" I bet you ca n't climb that hill .\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions .Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation .The NPS Chat Corpus , which was demonstrated in 1 , consists of over 10,000 posts from instant messaging sessions .", "label": "", "metadata": {}}
{"text": "\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts .The first step is to extract the basic messaging data .We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : .Next , we 'll define a simple feature extractor that checks what words the post contains : .Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment .", "label": "", "metadata": {}}
{"text": "To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams .Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset .The label True indicates that the entailment holds , and False , that it fails to hold .Challenge 3 , Pair 34 ( True ) .T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism .", "label": "", "metadata": {}}
{"text": "Challenge 3 , Pair 81 ( False ) .T :According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart .H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory .It should be emphasized that the relationship between text and hypothesis is not intended to be logical entailment , but rather whether a human would conclude that the text provides reasonable evidence for taking the hypothesis to be true .We can treat RTE as a classification task , in which we try to predict the True / False label for each pair .", "label": "", "metadata": {}}
{"text": "In the ideal case , we would expect that if there is an entailment , then all the information expressed by the hypothesis should also be present in the text .Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment .Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) .", "label": "", "metadata": {}}
{"text": "def rte_features ( rtepair ) : . return features .Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \" Recognizing Text Entailment \" Feature Extractor .The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference .To illustrate the content of these features , we examine some attributes of the text / hypothesis Pair 34 shown earlier : .These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True .", "label": "", "metadata": {}}
{"text": "Although this figure is not very impressive , it requires significant effort , and more linguistic processing , to achieve much better results . 2.4Scaling Up to Large Datasets .Python provides an excellent environment for performing basic text processing and feature extraction .If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages .Once these packages have been installed , NLTK can transparently invoke them ( via system calls ) to train classifier models significantly faster than the pure - Python classifier implementations .", "label": "", "metadata": {}}
{"text": "3 Evaluation .In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model .The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it .Evaluation can also be an effective tool for guiding us in making future improvements to the model . 3.1 The Test Set .Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs .", "label": "", "metadata": {}}
{"text": "When building the test set , there is often a trade - off between the amount of data available for testing and the amount available for training .For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances .But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times .", "label": "", "metadata": {}}
{"text": "When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation .Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set .The more similar these two datasets are , the less confident we can be that evaluation results will generalize to other datasets .For example , consider the part - of - speech tagging task .At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : .", "label": "", "metadata": {}}
{"text": "The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres .What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training .If there is any consistent pattern within a document - say , if a given word appears with a particular part - of - speech tag especially frequently - then that difference will be reflected in both the development set and the test set .", "label": "", "metadata": {}}
{"text": "If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : .If we build a classifier that performs well on this test set , then we can be confident that it has the power to generalize well beyond the data that it was trained on .3.2 Accuracy .The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled .The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 .", "label": "", "metadata": {}}
{"text": "For example , consider a classifier that determines the correct word sense for each occurrence of the word bank .If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 .In that case , an accuracy of 95 % would hardly be impressive , since we could achieve that accuracy with a model that always returns the financial - institution sense .However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result .", "label": "", "metadata": {}}
{"text": "Another instance where accuracy scores can be misleading is in \" search \" tasks , such as information retrieval , where we are attempting to find documents that are relevant to a particular task .Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % .It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : .", "label": "", "metadata": {}}
{"text": "True negatives are irrelevant items that we correctly identified as irrelevant .False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant .False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant .Given these four numbers , we can define the following metrics : .Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) .Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) .The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \u00d7 Precision \u00d7 Recall ) / ( Precision + Recall ) .", "label": "", "metadata": {}}
{"text": "When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made .A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i .In the following example , we generate a confusion matrix for the bigram tagger developed in 4 : .The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) .", "label": "", "metadata": {}}
{"text": "XXX explain use of \" reference \" in the legend above .3.5 Cross - Validation .In order to evaluate our models , we must reserve a portion of the annotated data for the test set .As we already mentioned , if the test set is too small , then our evaluation may not be accurate .However , making the test set larger usually means making the training set smaller , which can have a significant impact on performance if a limited amount of annotated data is available .One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation .", "label": "", "metadata": {}}
{"text": "For each of these folds , we train a model using all of the data except the data in that fold , and then test that model on the fold .Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable .A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets .If we get very similar scores for all N training sets , then we can be fairly confident that the score is accurate .", "label": "", "metadata": {}}
{"text": "4 Decision Trees .In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers .As we 've seen , it 's possible to treat these learning methods as black boxes , simply training models and using them for prediction without understanding how they work .But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set .", "label": "", "metadata": {}}
{"text": "And an understanding of the generated models can allow us to extract information about which features are most informative , and how those features relate to one another .A decision tree is a simple flowchart that selects labels for input values .This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels .To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node .This node contains a condition that checks one of the input value 's features , and selects a branch based on that feature 's value .", "label": "", "metadata": {}}
{"text": "We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value .4.1 shows an example decision tree model for the name gender task .Once we have a decision tree , it is straightforward to use it to assign labels to new input values .What 's less straightforward is how we can build a decision tree that models a given training set .But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \" decision stump \" for a corpus .", "label": "", "metadata": {}}
{"text": "It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value .In order to build a decision stump , we must first decide which feature should be used .The simplest method is to just build a decision stump for each possible feature , and see which one achieves the highest accuracy on the training data , although there are other alternatives that we will discuss below .Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) .", "label": "", "metadata": {}}
{"text": "We begin by selecting the overall best decision stump for the classification task .We then check the accuracy of each of the leaves on the training set .Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf .4.1 Entropy and Information Gain .As was mentioned before , there are several methods for identifying the most informative feature for a decision stump .One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature .", "label": "", "metadata": {}}
{"text": "In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : .Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male .For example , 4.2 shows how the entropy of labels in the name gender prediction task depends on the ratio of male to female names .Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low .", "label": "", "metadata": {}}
{"text": "P(l ) is small ) .On the other hand , if the input values have a wide variety of labels , then there are many labels with a \" medium \" frequency , where neither P(l ) nor log 2P(l ) is small , so the entropy is high .4.3 demonstrates how to calculate the entropy of a list of labels .import math def entropy ( labels ) : .Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump .", "label": "", "metadata": {}}
{"text": "The information gain is then equal to the original entropy minus this new , reduced entropy .The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain .Another consideration for decision trees is efficiency .The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree .A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples .", "label": "", "metadata": {}}
{"text": "To begin with , they 're simple to understand , and easy to interpret .This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features .Decision trees are especially well suited to cases where many hierarchical categorical distinctions can be made .For example , decision trees can be very effective at capturing phylogeny trees .However , decision trees also have a few disadvantages .One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small .", "label": "", "metadata": {}}
{"text": "One solution to this problem is to stop dividing nodes once the amount of training data becomes too small .Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test .A second problem with decision trees is that they force features to be checked in a specific order , even when features may act relatively independently of one another .For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are .", "label": "", "metadata": {}}
{"text": "And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large .A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label .Since these features make relatively small incremental improvements , they tend to occur very low in the decision tree .But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have .", "label": "", "metadata": {}}
{"text": "The fact that decision trees require that features be checked in a specific order limits their ability to exploit features that are relatively independent of one another .The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \" in parallel . \"5 Naive Bayes Classifiers .In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value .To choose a label for an input value , the naive Bayes classifier begins by calculating the prior probability of each label , which is determined by checking frequency of each label in the training set .", "label": "", "metadata": {}}
{"text": "The label whose likelihood estimate is the highest is then assigned to the input value .5.1 illustrates this process .Figure 5.1 : An abstract illustration of the procedure used by the naive Bayes classifier to choose the topic for a document .In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \" automotive \" label .But it then considers the effect of each feature .In this example , the input document contains the word \" dark , \" which is a weak indicator for murder mysteries , but it also contains the word \" football , \" which is a strong indicator for sports documents .", "label": "", "metadata": {}}
{"text": "Individual features make their contribution to the overall decision by \" voting against \" labels that do n't occur with that feature very often .In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature .The overall effect will be to reduce the score of the murder mystery label slightly more than the score of the sports label , and to significantly reduce the automotive label with respect to the other two labels .This process is illustrated in 5.2 and 5.3 .", "label": "", "metadata": {}}
{"text": "Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data .Every feature then contributes to the likelihood estimate for each label , by multiplying it by the probability that input values with that label will have that feature .The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent .5.1 Underlying Probabilistic Model .", "label": "", "metadata": {}}
{"text": "We 'll return to some of the consequences of this assumption at the end of this section .This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another .Figure 5.3 : A Bayesian Network Graph illustrating the generative process that is assumed by the naive Bayes classifier .To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label .", "label": "", "metadata": {}}
{"text": "Note .If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing .However , this simple approach can become problematic when a feature never occurs with a given label in the training set .Thus , the input will never be assigned this label , regardless of how well the other features fit the label .In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur .", "label": "", "metadata": {}}
{"text": "For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities .The nltk.probability module provides support for a wide variety of smoothing techniques .5.3 Non - Binary Features .We have assumed here that each feature is binary , i.e. that each input either has a feature or does not .Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \" color - is - red \" .", "label": "", "metadata": {}}
{"text": "5.4 The Naivete of Independence .The reason that naive Bayes classifiers are called \" naive \" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) .In particular , almost all real - world problems contain features with varying degrees of dependence on one another .If we had to avoid any features that were dependent on one another , it would be very difficult to construct good feature sets that provide the required information to the machine learning algorithm .So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ?", "label": "", "metadata": {}}
{"text": "To see how this can occur , consider a name gender classifier that contains two identical features , f 1 and f 2 .In other words , f 2 is an exact copy of f 1 , and contains no new information .When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose .Thus , the information content of these two features will be given more weight than it deserves .Of course , we do n't usually build naive Bayes classifiers that contain two identical features .", "label": "", "metadata": {}}
{"text": "For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature .For features like these , the duplicated information may be given more weight than is justified by the training set .5.5 The Cause of Double - Counting .The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined .", "label": "", "metadata": {}}
{"text": "We could then use those interactions to adjust the contributions that individual features make .To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : .Here , w[label ] is the \" starting score \" for a given label , and w[f , label ] is the contribution made by a given feature towards a label 's likelihood .We call these values w[label ] and w[f , label ] the parameters or weights for the model .", "label": "", "metadata": {}}
{"text": "However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values .6 Maximum Entropy Classifiers .The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier .But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier .In particular , it looks for the set of parameters that maximizes the total likelihood of the training corpus , which is defined as : .", "label": "", "metadata": {}}
{"text": "Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution .These iterative optimization techniques guarantee that each refinement of the parameters will bring them closer to the optimal values , but do not necessarily provide a means of determining when those optimal values have been reached .Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn .This is especially true when the size of the training set , the number of features , and the number of labels are all large .", "label": "", "metadata": {}}
{"text": "Some iterative optimization techniques are much faster than others .When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods .6.1 The Maximum Entropy Model .The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier .Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label .", "label": "", "metadata": {}}
{"text": "In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters .In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label .This will sometimes allow the model to \" generalize \" over some of the differences between related labels or features .Each combination of labels and features that receives its own parameter is called a joint - feature .", "label": "", "metadata": {}}
{"text": "Note .In literature that describes and discusses Maximum Entropy models , the term \" features \" often refers to joint - features ; the term \" contexts \" refers to what we have been calling ( simple ) features .Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model .In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] .", "label": "", "metadata": {}}
{"text": "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions .An example will help to illustrate this principle .Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) .At first , we are not told anything more about the word or the senses .There are many probability distributions that we could choose for the ten senses , such as : .", "label": "", "metadata": {}}
{"text": "On the other hand , distributions ( ii ) and ( iii ) reflect assumptions that are not supported by what we know .One way to capture this intuition that distribution ( i ) is more \" fair \" than the other two is to invoke the concept of entropy .In the discussion of decision trees , we described entropy as a measure of how \" disorganized \" a set of labels was .In particular , if a single label dominates then entropy is low , but if the labels are more evenly distributed then entropy is high .", "label": "", "metadata": {}}
{"text": "In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest .Next , suppose that we are told that sense A appears 55 % of the time .Once again , there are many distributions that are consistent with this new piece of information , such as : .But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) .Finally , suppose that we are told that the word \" up \" appears in the nearby context 10 % of the time , and that when it does appear in the context there 's an 80 % chance that sense A or C will be used .", "label": "", "metadata": {}}
{"text": "Furthermore , the remaining probabilities appear to be \" evenly distributed .\" Throughout this example , we have restricted ourselves to distributions that are consistent with what we know ; among these , we chose the distribution with the highest entropy .This is exactly what the Maximum Entropy classifier does as well .In particular , for each joint - feature , the Maximum Entropy model calculates the \" empirical frequency \" of that feature - i.e. , the frequency with which it occurs in the training set .It then searches for the distribution which maximizes entropy , while still predicting the correct frequency for each joint - feature .", "label": "", "metadata": {}}
{"text": "An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer .The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair .As a result , generative models can be used to answer the following questions : .How likely is a given input value with a given label ?What is the most likely label for an input that might have one of two values ( but we do n't know which ) ?", "label": "", "metadata": {}}
{"text": "Thus , conditional models can still be used to answer questions 1 and 2 .However , conditional models can not be used to answer the remaining questions 3 - 6 .However , this additional power comes at a price .Because the model is more powerful , it has more \" free parameters \" which need to be learned .However , the size of the training set is fixed .Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values .", "label": "", "metadata": {}}
{"text": "However , if we do need answers to questions like 3 - 6 , then we have no choice but to use a generative model .The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline .Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline .7 Modeling Linguistic Patterns .Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns .", "label": "", "metadata": {}}
{"text": "Either way , these explicit models serve two important purposes : they help us to understand linguistic patterns , and they can be used to make predictions about new language data .The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used .Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another .Other models , such as multi - level neural networks , are much more opaque .", "label": "", "metadata": {}}
{"text": "But all explicit models can make predictions about new \" unseen \" language data that was not included in the corpus used to build the model .These predictions can be evaluated to assess the accuracy of the model .Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data .These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering .7.1 What do models tell us ?It 's important to understand what we can learn about language from an automatically constructed model .", "label": "", "metadata": {}}
{"text": "Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns .For example , as we saw in 3.1 , the synonyms absolutely and definitely are not interchangeable : we say absolutely adore not definitely adore , and definitely prefer not absolutely prefer .In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns .For example , we might introduce the abstract concept of \" polar verb \" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar .", "label": "", "metadata": {}}
{"text": "In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships .Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another .If our goal is to understand the linguistic patterns , then we can use this information about which features are related as a starting point for further experiments designed to tease apart the relationships between features and patterns . 8 Summary .", "label": "", "metadata": {}}
{"text": "Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input .Supervised classifiers can perform a wide variety of NLP tasks , including document classification , part - of - speech tagging , sentence segmentation , dialogue act type identification , and determining entailment relations , and many other tasks .When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance .", "label": "", "metadata": {}}
{"text": "Otherwise , your evaluation results may be unrealistically optimistic .Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features .Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label .In naive Bayes classifiers , each feature independently contributes to the decision of which label should be used .This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another .", "label": "", "metadata": {}}
{"text": "Most of the models that are automatically constructed from a corpus are descriptive - they let us know which features are relevant to a given patterns or construction , but they do n't give any information about causal relationships between those features and patterns .9 Further Reading .Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python .For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) .Examples of these challenge competitions include CoNLL Shared Tasks , the ACE competitions , the Recognizing Textual Entailment competitions , and the AQUAINT competitions .", "label": "", "metadata": {}}
{"text": "Find out what type and quantity of annotated data is required for developing such systems .Why do you think a large amount of data is required ?Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set .Then , starting with the example name gender classifier , make incremental improvements .Use the dev - test set to check your progress .Once you are satisfied with your classifier , check its final performance on the test set .", "label": "", "metadata": {}}
{"text": "Is this what you 'd expect ?It contains data for four words : hard , interest , line , and serve .Choose one of these four words , and load the corresponding data : .Using this dataset , build a classifier that predicts the correct sense tag for a given instance .Can you explain why these particular features are informative ?Do you find any of them surprising ?Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier .", "label": "", "metadata": {}}
{"text": "How do you think that your results might be different if you used a different feature extractor ?What features are relevant in this distinction ?Build a classifier that predicts when each word should be used .However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others .For example , a ynQuestion dialog act is much more likely to be answered by a yanswer than by a greeting .Make use of this fact to build a consecutive classifier for labeling dialog acts .Be sure to consider what features might be useful .", "label": "", "metadata": {}}
{"text": "However , many words occur very infrequently , and some of the most informative words in a document may never have occurred in our training data .One solution is to make use of a lexicon , which describes how different words relate to one another .Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data .Each instance in the corpus is encoded as a PPAttachment object : .", "label": "", "metadata": {}}
{"text": "Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns .For example , given the pair of nouns \" team \" and \" researchers , \" the classifier should predict the preposition \" of \" .Explore this issue by looking at corpus data ; writing programs as needed .Tagged Questions .In machine learning and statistics , classification is the problem of identifying which of a set of categories a new observation belongs to , on the basis of a training set of data containing observations whose category membership ( label ) is known .", "label": "", "metadata": {}}
{"text": "I 've gathered a training set consisting of 600 documents belonging to both categories and the ... .Say I 'm evaluating some text classification research project using two approaches ' A ' and ' B ' .When using approach ' A ' , I get a x% increase in precision while with ' B ' , a x% increase in recall .How ... .How would you approach the following problem : I have 5 classes of images ( in total 500 images ) : car , house , trees , chair and face .", "label": "", "metadata": {}}
{"text": "this is my first question on stackoverflow , so bear with me , please .I 'm doing some corpus building , specifically trying to compose a Khmer / English parallel sentence corpus .I 'm using some manually ... .I 'm new about parallel computing in matlab .I have a function which creates a classifiers ( SVM ) and I 'd like to test it with several dataset .I 've got a 2 core workstation so I 'd like to run test in ... .Let us consider the problem of text classification .", "label": "", "metadata": {}}
{"text": "Now if ... .My question is , ... .I am trying to use weka for classfying spam message and nonspam message .With 100 's of thousands of labeled spam messages , and another 100 's of thousands labeled non - spam messages as a training data ... .I am using weka SMO classifier for classify the documents .There are many parameters for smo available like Kernal , tolerance etc .. , I tested using different parameters but i not get good result large ... .The cost matrix of my TreeBagger class and fitensemble ( Bag method ) are both [ 0 8;1 0 ] for binary classification .", "label": "", "metadata": {}}
{"text": "I am trying to implement an application that uses AdaBoost algorithm .I know that AdaBoost uses set of weak classifiers , but I do n't know what these weak classifiers are .Can you explain it to me with ... .I would appreciate ideas in this regard .Imagine I have a software ( constraint satisfaction solving ) which solves a problem and comes up with the answers like this : 100 % A is the solution , 100 % B ... .my name is titiri and happy that I found waffle library to classification .I think waffle is a good library for machine learning algorithms .", "label": "", "metadata": {}}
{"text": "After training a ... .I have been using Weka 's J48 decision tree to classify frequencies of keywords in RSS feeds into target categories .And I think I may have a problem reconciling the generated decision tree with the ... .I am trying to use Native Bayes Classifier in detecting fraud transactions .I have a sample data of around 5000 in an excel sheet , this is the data which I will use for training the classifier and i ... .There is a sample code that ... .I have a collection of documents related to a particular domain and have trained the centroid classifier based on that collection .", "label": "", "metadata": {}}
{"text": "My aim is to extract some features from images by using Gabor filter and use them in a classifier ( e.g. SVM ) .But I do n't know which are the features ? should I use all pixels as the features ?I have ...Affiliated with .Affiliated with .Affiliated with .Abstract .There are millions of public posts to medical message boards by users seeking support and information on a wide range of medical conditions .It has been shown that these posts can be used to gain a greater understanding of patients ' experiences and concerns .", "label": "", "metadata": {}}
{"text": "The main contribution of this paper is a system to de - identify the authors of message board posts automatically , taking into account the aforementioned challenges .We demonstrate our system on two different message board corpora , one on breast cancer and another on arthritis .We show that our approach significantly outperforms other publicly available named entity recognition and de - identification systems , which have been tuned for more structured text like operative reports , pathology reports , discharge summaries , or newswire .Introduction .Medical message boards ( MMBs ) serve as forums for emotional support and information exchange , usually for patients with similar conditions .", "label": "", "metadata": {}}
{"text": "Because of the sheer number , inexpensiveness , and candid nature of messages posted on these boards , many researchers have begun to treat MMB threads as \" virtual focus groups \" to gain more knowledge about patient experiences [ 1 - 3 ] .Additionally , our group is currently using MMBs as a source for identifying undocumented adverse effects from drugs and dietary supplements .As more patients gain access to the Internet and join these communities , more MMB text on patient experiences will become available , providing researchers with further opportunities to investigate .However , in order to adhere to ethical requirements in quoting from or performing research on MMB corpora , all information that may identify the user should be removed .", "label": "", "metadata": {}}
{"text": "This information includes personal and usernames , email and postal addresses , telephone numbers , and uniform resource locators ( URLs ) , collectively defined here as identifiers .There has been considerable research in the domain of Named Entity Recognition ( NER ) , the task of identifying instances of a particular type , such as people or companies within free text .Many NER systems have been developed and perform reasonably well [ 4 ] .However , since MMB text is much more unstructured and noisy than the text for which most NER systems are developed [ 5 ] , these methods are not as effective at capturing identifiers within MMB posts .", "label": "", "metadata": {}}
{"text": "This does not take into account any usernames that were present in these documents .In comparison , this same system was originally reported to be able to identify proper names with an F - score of 92.3 % over a sample of newswire [ 6 ] .We frame the task of de - identifying MMB text as a specialized form of NER .There is also a well - developed body of research regarding medical document de - identification .Many systems have already been developed to de - identify different types of free text medical documents such as pathology reports , nursing notes , and discharge summaries .", "label": "", "metadata": {}}
{"text": "Others have used statistical models in order to detect identifying information , including maximum entropy classifiers [ 10 ] , support vector machines [ 11 ] , and conditional random fields ( CRFs ) [ 12 ] .The problem of de - identifying medical records has been addressed by numerous researchers up to this point and performance of some of these systems is exceptional , with F - scores over 98 % for the best systems [ 13 ] .Unfortunately , these methods that have been tuned to the more regular text of medical documents will not translate easily to de - identifying MMB text .", "label": "", "metadata": {}}
{"text": "We focus here specifically on name de - identification .Methods .Our system first identifies email addresses , phone numbers , and URLs using regular expressions and tokenizes the rest of the document around these identifiers .It then generates a feature vector for each token .A name classifier is then used to generate tag probabilities for each of these tokens based on its associated vector .All tokens with probability of being a name greater than 0.05 are tagged as names .The token class probability estimate threshold of 0.05 was learned on a development set .", "label": "", "metadata": {}}
{"text": "Tokens that had been tagged as identifiers are then removed from the document , replaced with placeholders , and written to a de - identified file .This process is depicted in Fig . 1 .Corpora .We used two corpora to train and validate our system .The first corpus , the breast cancer ( BC ) corpus , was generated by downloading the messages from 12 different BC message board sites .Downloaded messages were then cleaned with scripts specifically tailored to the layout of each message board , to fit to a standard format .We randomly sampled messages from this corpus to generate the test set for validating our system .", "label": "", "metadata": {}}
{"text": "We selected the test set from this corpus in order to realistically determine how well our system would perform over a completely novel MMB , with different conditions and usernames than the training set .Pre - processing .Before names were identified , the corpus was passed through a pre - processing step corresponding to step 1 in Fig . 1 .In this step , e - mail addresses , URLs , and phone numbers were identified via regular expressions .For example , e - mail addresses were identified with the regular expression \" [ \\w.]+@\\w+ ( .", "label": "", "metadata": {}}
{"text": "+ \\d\\d\\d\\d \" where \\d refers to the set of digits and \\w refers to the set of alphanumeric characters and underscore .After these identifiers were discovered , the remaining text was split into tokens by whitespace and any punctuation marks .Once the text had been tokenized , our system generated a feature vector for each item in the output .Each token 's feature vector is a set of properties that describe that particular instance of the token , and are used by the name classifier to determine the likelihood that a given token is a name .", "label": "", "metadata": {}}
{"text": "The features that we used to train the CRF can be grouped into two classes : features that do not rely on the structure of MMBs and those that take advantage of the way that MMBs are structured .MMB non - structure features .The MMB non - structure features tend to be features that would be helpful in identifying names in many different media , not necessarily MMBs .The features that we used to describe each token include the token itself , the token lower - cased , and its length .The case of the token was encoded as either lower , upper , title , or mixed case , each being a binary feature .", "label": "", "metadata": {}}
{"text": "These features are helpful in identifying names in many different media , since names tend to be capitalized ( although to a lesser extent in MMB text ) and certain prefixes and suffixes may also indicate a name .We also included the distance of the token ( in number of tokens ) from the beginning and end of the message as a feature to take advantage of the fact that MMB posts often begin by addressing another user and end with the author 's name .Membership in each word list was included as a binary feature .We also included features for possible membership in a particular word list .", "label": "", "metadata": {}}
{"text": "Edit distance from one token to another was defined as the Damerau - Levenshtein distance , the number of additions , substitutions , removals , and transpositions of characters required to transform that token into the other .This feature was useful in identifying tokens that could be misspelled names , even if that particular token was not in any of the system 's word lists .The word lists used to generate these features and sources for each are listed in Table 1 .The word lists used are not specific to the domain of MMB text , for the most part .", "label": "", "metadata": {}}
{"text": "The only lists that may be domain specific are the medical term and drug word lists , which were specific to neither breast cancer nor arthritis message board posts .Users that have posted to this message board , generated from \" author \" field of each message .User variant .Users who have posted to this particular thread , with variants of these names derived automatically ( strip digits , split by delimiters / camel case / known names and words ) .The vector also includes the features of the two previous tokens and the two next tokens .", "label": "", "metadata": {}}
{"text": "This may be due to the fact that certain words strongly indicate a proper name ( e.g. , honorifics ) .MMB structure features .An MMB corpus can be segmented in several different ways .For example , one can consider each message as a separate document .Likewise , one can consider each thread , all threads within a particular message board , or all messages posted by a particular user as separate documents .Certain words will be repeated much more frequently within a document than in the entire corpus .In other words , they are \" document - specific \" .", "label": "", "metadata": {}}
{"text": "At the level of a thread , these are most likely the names of the users participating in that thread .At the level of a particular user , they would likely be their own name and other users that they frequently converse with .We use the term frequency - inverse document frequency metric ( tf - idf ) in two ways : by treating all messages that belong to a particular message board as a document , and by treating all messages that a particular user posts as a document .This metric favors terms that occur many times within the current document , but occur in very few other documents .", "label": "", "metadata": {}}
{"text": "Another virtue of this type of metric is that it tends to assign higher values to names that are rarer in general and may not occur in the proper name or even username lists .A similar feature that takes advantage of the fact that a particular name will occur multiple times in a particular document but not throughout the corpus was used by Minkov , Wang , and Cohen to identify names in e - mail messages [ 14 ] .We also used the likelihood that a token would appear near the beginning or end of a paragraph over the entire corpus scaled by the logarithm of the number of times that token appeared in the corpus as a feature .", "label": "", "metadata": {}}
{"text": "Table 2 provides an overview of the feature set that our system uses and lists examples of each feature type .Identifying names .Once feature vectors were generated for each token , a CRF name classifier was run over the tokens to estimate the marginal tag probabilities for each particular token .This corresponds to step 2 in Fig . 1 .A CRF [ 15 ] is a discriminative probabilistic model that has been widely used in natural language processing in order to tag sequences .The particular name classifier that we used was trained on a 1,000-message sample from the BC corpus containing a total of 91,344 tokens , 822 proper names , and 682 usernames .", "label": "", "metadata": {}}
{"text": "After returning tag probabilities for each token , any token with a cumulative probability greater than 0.05 of being either a proper name or username was tagged as a proper name or username ( whichever tag was more likely ) .We applied this step ( step 3 in Fig . 1 ) in order to increase the system 's recall , without sacrificing a great deal of precision , since identifying as many names as possible is more important than preventing non - name tokens from being removed .Validation metrics .We validated our system using the metrics of precision , recall , F - score , and specificity .", "label": "", "metadata": {}}
{"text": "Results .Both of these sets were manually tagged by a human coder in order to evaluate the effectiveness of our system .Any token that referred to a user of the message board or anyone that they had personal contact with was tagged as a name .This may have been overly harsh since many of these tokens were acronyms or nicknames that were unlikely to identify the user .Although the majority of these sets were tagged by a single coder ( AB ) , a subset of 120 messages was also tagged by another coder ( AC ) to produce an estimate of the sole coder 's reliability .", "label": "", "metadata": {}}
{"text": "In order to improve our system , we experimented with several different minimum name probability thresholds for tagging a token as a name and recorded the performance of our system using each of these thresholds over the development set .Fig .2 shows the precision - recall curve as the likelihood threshold is varied from 0.5 to 0.005 .We then applied the system to the arthritis test set which resulted in a precision of 61.4 % and a recall of 94.3 % .Performance of our system over development and test sets , varying the likelihood threshold The blue curve displays the precision and recall of our system over the development set , while varying the likelihood threshold .", "label": "", "metadata": {}}
{"text": "The threshold value of 0.05 was chosen , since it seemed to yield the highest recall without unnecessarily sacrificing precision over the development set .The red isolated point corresponds to the performance of our system , using the chosen threshold value of 0.05 , over the arthritis corpus test set , while the blue point corresponds to its performance over the development set .The red curve corresponds to our system 's performance over the arthritis test set .Note that this curve has a similar trajectory to the performance over the BC development set and that the point of 0.05 likelihood threshold on it corresponds to a similar precision / recall trade - off as the development curve .", "label": "", "metadata": {}}
{"text": "Note that many tokens in the original coding of the development and test sets were unlikely to give much information as to the identity of the author .Our system achieved a name recall of 99.1 % over the development set and 95.4 % over the test set after this recoding .The tokens where there was a discrepancy between the human coder and the system were categorized by type , as shown in Tables 3 and 4 .In Table 3 , \" Ambig . common words \" refers to tokens that were most often nicknames for users , but at times they were proper names that were misspelled as common ones ( e.g. , \" lard \" instead of \" lars \" ) .", "label": "", "metadata": {}}
{"text": "Prop . names \" refers to tokens that were clearly proper names , spelled correctly , although they may not have been in the system 's proper name list . \"Abbr ./Acr . \" were very short nicknames no longer than three characters and often acronyms of usernames . \" Misspelled usernames \" were usernames that had clearly been misspelled .These were determined to be misspelled usernames by referring to the author names from the original thread . \" Total N \" refers to the total number of names that were not tagged as names by our system .", "label": "", "metadata": {}}
{"text": "\" Places / Institutions \" refers to tokens referring to a location or organization .\" Medical \" tokens were tokens that referred to a drug , supplement , procedure , or some other medical concept and could be useful to researchers investigating these posts .\" Other \" tokens could not be placed in any of the previous four categories and would probably not be very useful to researchers .Some examples of these are : \" kiddo \" , \" june \" , \" april \" , \" morning \" , \" crispy \" , and \" sweetie \" .", "label": "", "metadata": {}}
{"text": "We ran the system under several conditions .The system was run first without altering any of its word lists , then by appending all the usernames on the message board to its list of ambiguous names , and finally by appending all of those usernames to the list of unambiguous names instead .The system was first judged only on its ability to identify proper names and then on its ability to identify both proper names and usernames .We also evaluated the Stanford NER trained on a collection of US and UK newswire over both these sets .", "label": "", "metadata": {}}
{"text": "Even adding the entire set of author names to the Deid system 's unambiguous name list resulted in a recall of only 67.0 % of usernames and a drop to 11.1 % precision over the development set .The poor performance of these systems over our evaluation sets does not suggest that they are bad at identifying names .It simply highlights the fact that current name identification systems must be developed for a specific domain in order to perform well in it .Table 5 .Performance of Deid system and Stanford named entity recognizer on development and test sets considering only proper names .", "label": "", "metadata": {}}
{"text": "17 ] .Like our system , MIST relies on a CRF to perform automated tagging of identifiers , and achieved the highest overall score in the de - identification task of the 2006 American Medical Informatics Association ( AMIA ) Challenges in Natural Language Processing for Clinical Data [ 13 ] .Both of these facts make MIST an excellent system to evaluate our system against .We trained MIST on the same set of 1,000 breast cancer MMB posts that our system was trained on and also included the dictionaries listed in Table 1 as part of its lexicon .", "label": "", "metadata": {}}
{"text": "Table 6 .Performance of state - of - the - art MIST de - identification system against our system , over the development and test sets .We used this definition of recall and precision , because the removal of identifying information is more important than the specific name tags they are replaced with .Discussion .Our system performs as well over MMB text as some of the other de - identification systems perform over other medical documents .In a recent challenge to remove private health information from medical discharge records [ 13 ] , out of the sixteen systems evaluated , two systems exhibited an F - score of less than 78.1 % and eight systems exhibited recall of less than 93.8 % in identifying patient names .", "label": "", "metadata": {}}
{"text": "However , it performs better over our MMB test sets than even the best of these systems ( MIST ) .We believe that it is a great step forward in developing a system that can adequately de - identify medical message board text .We chose to directly compare our system 's performance against the Deid system over the same corpus , because it was one of the few de - identification systems that were freely available .The Deid system uses a very different method of hand - tailored rules and word lists to remove identifiers .It is not surprising that this system performs poorly since it was developed for de - identifying medical records , not MMB text .", "label": "", "metadata": {}}
{"text": "This is due to the fact that the word lists were expanded with author usernames , and were tokens unlikely to be labeled as proper names ( Table 5 only evaluates the performance of these systems over proper names ) .Running the Stanford Named Entity Recognizer over a sample of 500 BC posts may be more comparable , since it also detects proper names using a CRF .Even then , its newswire - trained classifier performed with a precision of 61.7 % and recall of 81.2 % over just proper names in the development set ( 61.7 % precision/69.7 % recall over both proper and user names within the same sample ) .", "label": "", "metadata": {}}
{"text": "In particular , Table 6 suggests that it was unable to identify usernames well ( recall of 49.5 % over the development set and 34.3 % over the test set ) , even though its training set contained explicitly marked usernames .This suggests that the default feature set that MIST uses to describe tokens is not suitable for de - identifying MMB text , although it may expressive enough to discover identifiers in more regular text , such as medical records .The poor performance of these systems over our MMB corpus , suggests that current de - identification methods can not readily be applied to this new text medium , and that our specialized method is useful and novel .", "label": "", "metadata": {}}
{"text": "Neamatullah , et al .report that the Deid system was able to identify proper names in a corpus of nursing notes with 72.5 % precision and 98.9 % recall [ 9 ] .However , we show that they are unable to reliably identify names when applied to the very different medium of MMB text .We were unable to find a system specifically designed to de - identify MMB text , which is why we chose to evaluate the performance of our system against two medical record de - identification systems , Deid and MIST , and a named entity recognition system , the Stanford NER .", "label": "", "metadata": {}}
{"text": "Some examples of tokens ambiguous between common words and usernames that our system failed to classify were \" one \" , \" boo \" , \" breezy \" , \" tiger \" , \" girl \" , and \" ash \" .The majority of names that were missed were of this form .Another class of names that our system failed to tag was acronyms of names .However , it is difficult to imagine how a human reading the message would be able to discover the actual username based on this acronym .The medically - related tokens that were erroneously removed by our system are of the most concern .", "label": "", "metadata": {}}
{"text": "We believe that these tokens would not have been erroneously tagged if the classifier were trained on a larger training set .\" Carcinom \" , \" tamoxifine \" , and \" earlydetection \" would be more difficult for our classifier to leave untagged , since they are all misspellings of actual medical terms , and these tokens are unlikely to occur with high frequency in the training corpus .Table 7 .Medical words incorrectly identified as names over development and test sets .The tokens removed from the test set pose a much greater concern . \" Doxy \" was used as an abbreviation of the pharmaceutical doxycycline , which is why it was not marked as a non - name in the post - processing step .", "label": "", "metadata": {}}
{"text": "Hashimoto \" and \" sjogren \" are difficult as well , since they are ambiguous between proper names and medical terms ( Hashimoto 's disease , Sj\u00f6gren 's syndrome ) .Within our test set , they appeared as conditions that users were discussing rather than people that they knew .In spite of the low precision , the specificity of our system is very good , only removing about 0.7 % of all non - names .Future work .Although our system takes a great step in de - identifying MMB text , there are several modifications that we can make in order to improve our system 's performance .", "label": "", "metadata": {}}
{"text": "We could also include a gazetteer of locations in order to reduce the number of mistagged places .Second , we have not included the part of speech ( POS ) of the token in the feature vectors generated for tokens .Six out of the 16 systems evaluated in a 2007 discharge summary de - identification challenge [ 13 ] used POS tags to inform identification of private health information .Many statistical de - identification systems rely on this feature .As of now , we are unsure of how effective a POS tagger would be over MMB text , since these systems are often trained on text from newswires or the Wall Street Journal , which is more regular than message board text .", "label": "", "metadata": {}}
{"text": "Finally , the identifiers that our system currently removes are far from full de - identification , but they are some of the most pervasive identifiers in MMB text .We intend to improve our system by specifically identifying institution names and locations as well .The removal of these terms is currently a by - product of our name classifier and we have not evaluated its performance at removing locations and institution identifiers .As investigators continue exploring MMB text to gain a greater awareness of patients ' experiences , systems such as ours will become more important than ever in protecting the privacy of the members of these communities .", "label": "", "metadata": {}}
{"text": "We have developed a system that can de - identify MMB posts by identifying and removing both proper and usernames with acceptable precision and recall .Not only is this a boon to researchers investigating these MMBs , but it also suggests that NER can be effective in even some of the noisiest forms of free text .We welcome any improvements that others can offer to our system .Authors ' contributions .AB , SH , LU , and JH all contributed to the initial design of the system .All authors participated in discussions about the interpretation of our data and results .", "label": "", "metadata": {}}
{"text": "AB implemented the system and drafted the manuscript .All authors contributed to revising the manuscript and provided feedback on the methods used to evaluate system performance .All authors read and approved the final manuscript .Declarations .Acknowledgments .This project is supported by the National Library of Medicine ( RC1LM010342 ) .Access to the Multum database was supported by the National Center for Research Resources ( ( 5KL2RR024132 ) ) .The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health .", "label": "", "metadata": {}}
{"text": "Authors ' Affiliations .References .RH Kenen , Shapiro PJ , Friedman S , Coyne JC : Peer - support in coping with medical uncertainty : discussion of oophorectomy and hormone replacement therapy on a web - based message board .Psychooncology 2007 , 16 : 763 - 771 .View Article .Hadert A , Rodham K : The invisible reality of arthritis : a qualitative analysis of an online message board .Musculoskeletal Care 2008 , 6 ( 3 ) : 181 - 96 .PubMed View Article .Moloney MF , Strickland OL , DeRossett SE , Melby MK , Dietrich AS : The experiences of midlife women with migraines .", "label": "", "metadata": {}}
{"text": "[ see comment ] PubMed View Article .Nadeau David , Sekine Satoshi : A survey of named entity recognition and classification .Lingvisticae Investigationes 2007 , 30 : 3 - 26 .View Article .Lewin BeverlyA. , Donner Yonatan : Communication in Internet message boards .English Today 2002 , 18 : 29 - 37 .View Article .Finkel JR , Grenager T , Manning C : Incorporating non - local information into information extraction systems by Gibbs sampling .ACL 2005 2005 , 363 - 370 .View Article .Sweeney Latanya : Replacing Personally - Identifying Information in Medical Records , the Scrub System .", "label": "", "metadata": {}}
{"text": "Thomas SeanM. , Mamlin Burke , Schadow Gunther , McDonald Clement : A successful technique for removing names in pathology reports using an augmented search and replace method .Proceedings of the AMIA Symposium 2002 , 777 - 781 .Neamatullah Ishna , Douglass MargaretM. , Lehman Li - weiH. , Reisner Andrew , Villarroel Mauricio , Long WilliamJ. , Szolovits Peter , Moody GeorgeB. , Mark RogerG. , Clifford GariD. : Automated de - identification of free - text medical records .BMC Medical Informatics and Decision Making 2008 ., 8 : .Taira RickyK. , Bui AlexA.T. , Kangarloo Hooshang : Identification of patient name references within medical documents using semantic selectional restrictions .", "label": "", "metadata": {}}
{"text": "Sibanda Tawanda , Uzuner Ozlem : Role of Local Context in Automatic Deidentification of Ungrammatical , Fragmented Text .In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics .Association for Computational Linguistics ; 2006:65 - 73 .View Article .Gardner J , Xiong L : An integrated framework for de - identifying unstructured medical data .Data & Knowledge Engineering 2009 , 68 : 1441 - 1451 .View Article .Uzuner Ozlem , Luo Yuan , Szolovits Peter : Evaluating the State - of - the - Art in Automatic De - identification .", "label": "", "metadata": {}}
{"text": "PubMed View Article .Minkov Einat , Wang RichardC. , Cohen WilliamW. : Extracting personal names from Emails : applying named entity recognition to informal text .In HLT ' 05 : Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing .Association for Computational Linguistics ; 2005:443 - 450 .View Article .Lafferty John , McCallum Andrew , Pereira Fernando : Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data .Eighteenth International Conference on Machine Learning 2001 , 282 - 289 .Goldberger AL , Amaral LAN , Glass L , Hausdorff JM , Ivanov PCh , Mark RG , Mietus JE , Moody GB , Peng C - K , Stanley HE : PhysioBank , PhysioToolkit , and Physionet : Components of a New Research Resource for Complex Physiologic Signals .", "label": "", "metadata": {}}
{"text": "Aberdeen Joan , Bayer Samuel , Yeniterz Reyyan , Wellner Ben , Clark Cheryl , Hanauer David , Malin Bradley , Hirschman Lynette : The MITRE Identification Scrubber Toolkit : Design , training , and assessment .International journal of medical informatics 2010 , 79 : 849 - 859 .PubMed View Article .Bird S : NLTK : the natural language toolkit .Annual meeting of the ACL 2004 , 69 - 72 .Copyright .\u00a9 Benton et al .2011 .ABSTRACT .The Carnegie Mellon Communicator is a telephone - based dialog system that supports planning in a travel domain .", "label": "", "metadata": {}}
{"text": "Given a suitable architecture , the principal effort in development in taken up in the acquisition and processing of a domain knowledge base .This paper describes a variety of techniques we have applied to modeling in acoustic , language , task , generation and synthesis components of the system .INTRODUCTION .System development involves a great deal of knowledge engineering , which is both time - consuming and requires a variety of experts to participate in the process .Therefore methods that seek to minimize this resource , for example through training based on domain - specific corpora are preferred .", "label": "", "metadata": {}}
{"text": "THE CMU COMMUNICATOR .The Carnegie Mellon Communicator [ 8 ] is a telephone - based dialog system that supports planning in a travel domain .Currently the task is captured in an approximately 2700-word language based on corpora derived from human - human , wizard of oz and human - computer interaction .Domain information is obtained in real - time from sources on the Web .The system understands about 500 destinations worldwide , chosen on the basis of passenger statistics , with a concentration on North America .Figure 1 Percent of training corpus account for by individual talkers .", "label": "", "metadata": {}}
{"text": "The top hypothesis produced by the decoder is processed by the Phoenix parser using a domain - specific semantic grammar ( based on ATIS [ 2 ] but extended to cover Communicator - specific language ) .The resulting parse is evaluated for coherence then passed to the Agenda dialog manager [ 9 ] .Coherence is evaluated using goodness of the parse ( features such as coverage and fragmentation ) as well as word - level decoder confidence ; inputs deemed incoherent .The system monitors the frequency and pattern of rejection and uses this information to modify its strategy for interaction .", "label": "", "metadata": {}}
{"text": "Once matched , the concepts are either used directly ( i.e. , to set a target value ) or are first transformed through a call to a domain agent .Currently the system uses three major domain agents , a travel backend , a date - time module and a user profile module .The transform result is either stored in a product structure ( for this domain , an itinerary ) or an immediate action taken ( for example , notifying the user of an error ) .ACOUSTIC MODELLING .It is our belief that optimal recognition performance can be obtained most readily using domain - specific data .", "label": "", "metadata": {}}
{"text": "There are unfortunately two difficulties with this approach .Most of the early data captured for training will be from a relatively small pool of developers ; at the same time the rate of data acquisition will be slow .Figure 1 shows the distribution of data across speakers for the CMU Communicator ( through the Fall of 1999 ) .Note that although several hundred speakers are represented in the corpus , seven speakers contribute about half the data .Figure 2 shows corpus growth over time .In August the Communicator was publicized on the Web and made available for public use , increasing variability .", "label": "", "metadata": {}}
{"text": "Model performance was evaluated using two different test sets , from June 1999 ( 1759 utterances ) and from October 1999 ( 3459 utterances ) .The June set contains predominantly ( though not exclusively ) developer speech , while the October test set contains a greater proportion of public speech , as well as more challenging data ( e.g. , from cell phones ) .Model 1 : For training this model , we used all transcribed data collected between April 1998 and January 2000 , excluding the data collected during June 1999 and October 1999 .Table 1 shows performance for a 4000 tied - state model .", "label": "", "metadata": {}}
{"text": "For example , while there were 129516 possible triphones in the domain ( computed from the dictionary used for recognition ) , only 15783 were present in the training data .To compensate for this , we identified all triphones ( e.g. , from city names ) , and created a list of these that appeared under - sampled in the training corpus .This in turn was used to design a set of 500 sentences densely sampling these triphones and recordings made , totaling 3141 utterances .The addition of such focused supplementary data resulted in a relative reduction in word error rates of less than 5 % on the above test sets .", "label": "", "metadata": {}}
{"text": "We can compensate for lack of data and for overfitting by smoothing the state distributions of the models with uniform distributions .This produces a slight improvement in error rate for both test sets ( Model 1b ) .Model 2 : We investigated state tying using a rule - based tying procedure prior to building decision trees .Decision trees were built using data recorded up to October 1999 .The training data was observed to contain a large number of triphones with very poor representation .Since the data for these triphones was scarce , distributions learnt for these triphones , and decision trees built based on these distributions , were likely to be poorly estimated .", "label": "", "metadata": {}}
{"text": "The distributions of the various states of the triphones that were learnt in this manner were then used to build the final decision trees .Since this tends to merge the identity of triphones with similar transitions , the entropy of adjacent states was also considered during the decision tree building process to maintain triphone identities .Following this , HMMs with 4000 tied states were trained using all available data .Model 2 represents a significant improvement in accuracy .Table 1 .WER obtained using different acoustic models .Model 3 : It was observed that the performance of Model 1 on tracking test sets had steadily deteriorated in the Fall of 1999 , due in part to a noisier signal .", "label": "", "metadata": {}}
{"text": "Acoustic models with 5000 tied states were trained using these trees , and the same data as used by Model 2 .This however did not appear to improve performance .Model 4 : We trained models 6000 state models in a standard fashion , using all Communicator data recorded through April 2000 .Performance was better than Model 3 ( likely due to simply more data ) with better improvement for clean data over noisy data .This result contrasts with a 17.2 % error rate observed for models trained on the much larger Switchboard I corpus and adapted to the Communicator domain ( June 99 test set ) .", "label": "", "metadata": {}}
{"text": "The basis for the Communicator language is the ATIS language developed previously for a similar domain ( airline schedule information retrieval ) .Initial grammar coverage was quite low , however examination of utterances collected early on in the project yielded useful improvements .A corpus of 21890 transcriptions ( from June 1998 through February 1999 ) was used for this purpose .This was reduced to 5162 sentences through preprocessing ( essentially replacement of tokens by class type ) then analyzed in order of frequency .For the June 1999 test set , the coverage error for an initial grammar ( essentially ATIS with minimal additions to reflect new Communicator functionality ) was 13.7 % ( 10.4 % for completely in - domain utterances ) .", "label": "", "metadata": {}}
{"text": "The structure of a semantic grammar is such that , once a concept hierarchy is created , addition to language variants is a simple process .Although this process can be automated , we have not as yet done so .It has been our experience that once the core of the language for a domain is identified , it remains stable as long as the definition of the domain is not significantly altered .This is due in part to the inherent stability of certain sub - languages such as that for dates and times as well as an apparent independence between sub - domains comprising the full domain .", "label": "", "metadata": {}}
{"text": "The significance of this observation is that it implies that the language for a domain can be incrementally extended without the concomitant need for restructuring the entire grammar as its complexity increases .It further raises the possibility that language components may be reused from application to application , provided that the sub - domains in question are substantially the same .In our work we make a distinction between a core language and a variable component that includes domain - specific entities .In the case of Communicator , destination names and the names of users registering for the service .", "label": "", "metadata": {}}
{"text": "This is accommodated in language modeling through the use of a class language model .The Communicator core language contains a total of 1141 words ; it uses a total of 20 classes , of which ten are open classes ( e.g. , city names or airlines ) and ten are closed .Of the latter three are deemed closed with respect to the domain ( e.g. , holidays , ordinal numbers ) .There is a total of 1573 words in the class component of the language model .A key issue in managing the knowledge base as a whole is coordinating modifications that impact different components of the base .", "label": "", "metadata": {}}
{"text": "Nevertheless human intervention is required at two points : the identification of alternative renderings of a particular identifier ( e.g. , JFK as well as KENNEDY for an airport name ) and the choice of a pronunciation .While it is possible to automatically generate pronunciations as well as variants , human review is always necessary to ensure accuracy .Moreover contact with informants ( e.g. , the named person , or someone knowledgeable about a particular region ) is unavoidable , since many variants are culturally defined rather than generated by rule .LANGUAGE GENERATION .To date , most of the research in natural language generation ( NLG ) has focused on generating text using template - based or rule - based ( linguistic ) techniques .", "label": "", "metadata": {}}
{"text": "The corpus used consisted of 39 dialogs between a professional travel agent and her clients , a total of 970 utterances and 12852 words .Utterances were classified into 29 categories , corresponding to major speech acts ; within each utterance concepts were tagged as belonging to one of 24 classes .For each utterance category , a 5-gram language model was built then used for generation ( the NLG component was provided with a speech act and a set of concepts to transmit to the user ) .About half of all utterances in the Communicator are generated using this stochastic technique ( the remainder involve set phrases specific to the system ) .", "label": "", "metadata": {}}
{"text": "LIMITED - DOMAIN SYNTHESIS .The quality of speech output in a dialog system is important to a user 's perception of the system .That is , it must both be appropriate sounding , and also work fast enough so that user does not think something is wrong .In earlier versions of the CMU Communicator we used a general - purpose commercial speech synthesis system .More recently we have begun to experiment with synthesis tailored specifically to this domain .Unit selection synthesis , where appropriate sub - word units are selected from general speech databases , for example AT&T 's NextGen [ 3 ] , can produce very high quality synthesis .", "label": "", "metadata": {}}
{"text": "However it has been noted that unit selection synthesis is often better when the utterance to be synthesized is closer to the domain of the database .The result offers very high quality synthesis that sounds almost human .The techniques used for this are more fully described in [ 4 ] .Important to this technique is not just the resulting high quality synthesis but that we also developed the tools , documentation and techniques to allow such high quality voices to be built for other systems reliably in a very short time ( much less than a month ) .", "label": "", "metadata": {}}
{"text": "The stages involved in building such limited domain synthesizers are as follows .First we constructed a set of sentences for recording which adequately covered the desired domain .For Communicator we analyzed the utterances from logs of the most recent three months and sorted them by frequency .We then took the basic templates used by the language generation system and filled them out with the most frequent cities , airlines and ensured we had full coverage ( at least one instance ) for numbers , dates , times and other closed classes in the system .Giving a prompt list of just over 600 utterances .", "label": "", "metadata": {}}
{"text": "The recordings were autolabelled using a simple alignment technique between the naturally spoken utterances and synthesized equivalents .The utterances were then used to build a unit selection synthesizer using the algorithm first described in [ [ 5 ] ] .This technique takes all units of the same type and calculates an acoustic distance between and then using CART techniques recursively partitions the instances by questions about phonetic and prosodic context to produce clusters indexed by decision trees .In the original algorithm unit types are simple phones , but in this limited domain synthesizer we constrain this further by defining types as phones plus the word that phone came from .", "label": "", "metadata": {}}
{"text": "This technique however is not just word concatenative synthesis .It is often , in fact common , that selections for a single word come different instances of that word joined at appropriate parts of the speech .The common prompts are invariably rendered from the original full prompts , thus preserving the original quality exactly .Other utterances with variable parts such as flights times , cities etc . are also rendered with comparable quality by selecting appropriate units from different utterances in the database .The stochastic language generation process described earlier is not a problem for this technique , reformulating similar sentence forms is dealt with adequately .", "label": "", "metadata": {}}
{"text": "Rather than falling back to more general unit types for selection , which could easily produce very poor quality synthesis ( and which can not be detected automatically ) we use a standard diphone synthesizer ( from the same voice as the limited domain speaker ) .Over a period of three weeks the system synthesized 18,276 phrases , 459 of which ( 2.5 % ) contained out of vocabulary words ( 71 different words ) .These were all less frequent ( or forgotten ) places names .This work was done within the Festival Speech Synthesis Systems [ 6 ] .", "label": "", "metadata": {}}
{"text": "The Carnegie Mellon Communicator system has provided a framework for experimenting with domain - specific , corpus - driven knowledge base configuration at different levels of a spoken dialog system .ACKNOWLEDGEMENTS .We would like to thank Maxine Eskenazi and Karin Gregory for their work in managing corpus collection and transcription , as well as lexicon maintenance .We would like to thank Ricky Houghton for sharing some of his recognition results .This research was sponsored by the Space and Naval Warfare Systems Center , San Diego , under Grant No . N66001 - 99 - 1 - 8905 .", "label": "", "metadata": {}}
{"text": "REFERENCES .[ 1 ] R. Singh , B. Raj , and R. M. Stern , Domain adduced state tying for cross domain acoustic modelling , Proc .Eurospeech , 1999 .ABSTRACT .This paper presents the development of the HTK broadcast news transcription system for the November 1998 Hub4 evaluation .Overall these changes to the system reduced the error rate by 13 % on the 1997 evaluation data and the final system had an overall word error rate of 13.8 % for the 1998 evaluation data sets .Significant progress in the accurate transcription of broadcast news data has been made over the last few years so that we are now at a point where such systems can be used for a variety of tasks such as audio indexing and retrieval .", "label": "", "metadata": {}}
{"text": "The HTK Broadcast News Transcription System used in the 1997 DARPA / NIST Hub4 evaluation had an overall word error rate of 15.8 % .This paper describes a number of experiments with , and developments of , that system .Some of these were included in 1998 HTK Hub4 evaluation system .Other experiments which did n't lead to overall word error rate reductions include discriminative training using the frame discrimination method and use of the soft - clustering technique .The paper is arranged as follows .We first give details of the broadcast news data used in the experiments , then give an outline of the overall system used in the 1997 evaluation .", "label": "", "metadata": {}}
{"text": "This is followed by a description of , and the results from , the 1998 Hub4 evaluation system .The full recognition results from the various stages of operation are included .This section describes the various data sets that have been used in the experiments reported in the paper .The baseline acoustic corpus available in 1997 used recorded audio from various US broadcast news shows ( television and radio ) .This amounted to a total of 72 hours of usable data ( BNtrain97 ) .This data was annotated to ensure that each segment was acoustically homogeneous ( same speaker , background noise condition and channel ) .", "label": "", "metadata": {}}
{"text": "This was similarly transcribed at the speaker turn level but did n't distinguish between background conditions which meant that marked training segments were no longer necessarily homogeneous .The combined set of 1997 and 1998 data is denoted BNtrain98 .System development mainly used the 1997 Hub4 evaluation data , BNeval97 .BNeval97 was taken from a number of sources broadcast in October / November 1996 and was presented to the system as a single 3 hour file .The 1998 evaluation data , BNeval98 , consisted of two 1.5 hour data sets : the first drawn from a similar epoch as the 1997 data and the second drawn from June 1998 .", "label": "", "metadata": {}}
{"text": "The proportion of data of each type in BNeval97 and BNeval98 is given in Table 2 .It can be seen that there is a rather different distribution data type between the two sets : particularly for F0 , F2 , F4 and FX .The HTK Broadcast News system runs in a number of stages .This is followed by generating a lattice for each segment using the adapted triphone models with a bigram LM , expanding these lattices using a word 4-gram interpolated with a category trigram LM , and performing iterative lattice rescoring and MLLR adaptation with a set of quinphone HMMs .", "label": "", "metadata": {}}
{"text": "System details can be found in [ 16 ] .The data segmentation [ 4 ] aims to generate acoustically homogeneous speech segments and discard non - speech portions such as pure music .It uses a set of Gaussian mixture models to classify the data as to type ( wideband speech , narrow - band speech , pure music , speech and music ) , and then any pure music is discarded .A gender dependent phone recognition stage then generates a stream of gender labelled phone units .Using a clustering procedure and a set of smoothing rules the final segments to be processed by the decoder are generated .", "label": "", "metadata": {}}
{"text": "Cepstral mean normalisation ( CMN ) is applied over each segment .The triphone HMMs were estimated using BNtrain97 and contained 6684 decision - tree clustered states [ 17 ] , each with 12 Gaussians per state while the quinphone models used 8180 states and 16 Gaussians per state .The HMMs were initially trained on all the wide - band analysed training data .Narrow - band sets were estimated by using a version of the training data with narrow - band analysis ( 125 - 3750Hz ) , and gender dependent versions of each were made .The reduced bandwidth models are used for data classified as narrow band .", "label": "", "metadata": {}}
{"text": "The 1997 system used N - gram language models trained on 132 million words of broadcast news texts , the LDC - distributed 1995 newswire texts , and the transcriptions from BNtrain97 ( LMtrain97 ) .This corpus was used to estimate both word N - grams and a category N - gram based on 1000 automatically generated word classes [ 6 , 10 , 11 ] .The final hypothesis combination uses word - level confidence scores based on an N - best homogeneity measure .These are used with the NIST ROVER program [ 1 ] to produce the final output .", "label": "", "metadata": {}}
{"text": "Versions of the acoustic models ( triphones and quinphones ) used in the 1997 system were trained with BNtrain98 ( 16 mixture components per state for both triphones and quinphones ) .Experiments with no adaptation ( or cluster - based normalisation ) showed that the word error rate ( WER ) was reduced by up to 0.9 % absolute .However when MLLR adaptation and VTLN were applied ( see below ) the WER gain was reduced to 0.4 % absolute .However it was noted that the gains were across all speech conditions with the largest gains being for non - native speakers .", "label": "", "metadata": {}}
{"text": "We also did some experiments that used automatic segmentation of the extended training data to try and ensure that the segments used in training were acoustically homogeneous but this provided no additional improvements .We have previously worked on robust vocal tract length normalisation ( VTLN ) , most recently in the context of conversational telephone speech transcription [ 5 ] .We use a maximum likelihood technique to select the best data warp factor via a parabolic search .It is important when comparing the warped data likelihoods to properly take into account the effect of the transformation .", "label": "", "metadata": {}}
{"text": "The VTLN and the variance normalisation is done on a segment cluster basis .We found an overall improvement in WER with cluster - based variance normalisation of 0.3 % absolute and a further 0.6 % absolute by applying VTLN in both training and testing without adaptation .However with mean and variance MLLR adaptation the separate beneficial effect of variance normalisation and VTLN is much reduced .A summary performance on BNeval97 ( MLLR adapted triphones ) for increased training data and the use of VTLN is shown in Table 3 .Furthermore , in line with the triphone figures , the overall gain for 1998 trained MLLR adapted quinphone models was 0.4 % absolute due to VTLN .", "label": "", "metadata": {}}
{"text": "Furthermore we processed additional transcriptions of broadcast news texts supplied by Primary Source Media ( from late 1996 , 1997 and early 1998 ) so that we had a total of 190MW of such data available .Finally , we decided to use a different ( though similarly sized ) portion of newspaper texts covering 1995 to February 1998 ( about 70MW in total ) .All these sources excluded data from the designated test epochs .This corpus was denoted LMtrain98 .Previously we have constructed LMs by simply pooling the texts and weighted the acoustic data transcription counts .", "label": "", "metadata": {}}
{"text": "For efficiency and ease of use in decoding , a model merging process was employed using tools supplied by Entropic Ltd. , that gives a similar effect to explicit model interplotation but saves run - time computation and storage .The interpolation weights were chosen to minimise perplexity .Table 4 : % WER on BNeval97 for different trigram LMs with VTLN unadapted triphone HMMs with either pooled data or ( merged ) interpolated LMs .The effect of using three different LMs on BNeval97 with VTLN data and 1998 unadapted triphone HMMs is shown in Table 4 .Note that the LMtrain98 models also used a revised vocabulary which reduced the out - of - vocabulary rate on BNeval97 by about 0.1 % .", "label": "", "metadata": {}}
{"text": "The merged interpolated models gave most improvement on the spontaneous speech portions of the data .Later experiments with adapted quinphone models showed that a total improvement of 0.9 % absolute was gained from using the the new LM data and estimation procedure .The basic adaptation approach in our system remains MLLR for both means and variances [ 2 ] .In addition , for the quinphone stage of iterative unsupervised adaptation , the effect of a single full variance ( FV ) transform [ 3 ] was investigated .Table 5 : % WER on BNeval97 for BNtrain98 VTLN MLLR adapted quinphones using the 1998 fgintcat LM with / without a full - variance ( FV ) transform and SAT mean estimated models .", "label": "", "metadata": {}}
{"text": "The effect of these changes is shown in Table 5 .It can be seen that the FV transform reduces the error rate by 0.3 % absolute with SAT training contributing 0.1 % .The word error rate on BNeval97 of 14.3 % ( including FV and SAT ) represents a 13 % reduction relative to the same stage of the 1997 evaluation system [ 16 ] .We have recently experimented with discriminative training of large vocabulary systems and using the frame discrimination ( FD ) technique [ 13 ] .FD is related to maximum mutual information estimation ( MMIE ) , but uses all Gaussians ( rather than all words ) in the system to model confusion data .", "label": "", "metadata": {}}
{"text": "Our experiments with FD on broadcast news data show that overall we get very similar results to maximum likelihood training , although the training procedure gives a sizeable improvement in the FD criteria .We therefore did not include FD modelling in the 1998 evaluation system .The soft - clustering technique developed at JHU [ 9 ] had shown worthwhile reductions in word error rate on the Switchboard corpus and we performed a preliminary evaluation on Broadcast News data .The technique works by increasing the number of Gaussian components in each state distribution while not increasing the overall number by increased Gaussian sharing so that the strict context to ' ' tied - state ' ' relationship given by decision tree state - clustering [ 17 ] is not enforced .", "label": "", "metadata": {}}
{"text": "However , when bandwidth dependent , gender dependent models with variance normalisation and MLLR adaptation were used , there was no WER advantage and hence soft clustering was not used in the 1998 evaluation system .This section describes the HTK system used in the 1998 evaluation .The system takes the 1997 system and includes the additional acoustic training data in BNtrain98 ; cluster - based normalisation and VTLN ; the revised language modelling data and build procedure and full variance adaptation with SAT training .The word N - grams were trained by interpolating ( and merging ) component LMs trained on the acoustic transcriptions , the broadcast news texts and the newspaper texts .", "label": "", "metadata": {}}
{"text": "The category - trigram used 1000 automatically derived word classes and was trained using LMtrain98 .Category bigrams and trigrams were added only if the leave - one training set likelihood improved and the final category model contained 0.85 million bigrams and 9.4 million trigrams .The 65k wordlist was chosen by combining the word frequency lists from the different LM training sources with suitable weightings and choosing the most frequent words for which we already had pronunciations .Table 7 : OOV rate and perplexities of the 1998 evaluation LMs .Perplexities shown for trigram ( tg ) , 4-gram ( fg ) and word 4-gram interpolated with category trigram ( fgintcat ) .", "label": "", "metadata": {}}
{"text": "It was noted that compared to the use of the 1997 language models all OOV rates had been reduced slightly , the most being by 0.1 % on BNeval98_2 .Furthermore the 1998 4-gram language model gave a constant 15 % improvement ( over all test sets ) in perplexity over the equivalent model used in the 1997 evaluation .The overall decoding process proceeds as for the 1997 system , but with a couple of additional stages .The first pass ( P1 ) uses gender independent triphone HMMs to get an initial transcription with a trigram LM .", "label": "", "metadata": {}}
{"text": "Gender dependent VTLN models are then used ( P2 ) to provide a revised transcription which is used to estimate global mean and variance MLLR transforms for each cluster .These adapted models are then used to generate lattices ( P3/bg ) which are expanded to use the 4-gram word LM interpolated with the category - based trigram model ( P3/fgintcat ) .The system then uses quinphone models ( VTLN / SAT trained ) and MLLR with an additional FV transform to process the data ( P4 ) .This stage is repeated twice more while increasing the number of MLLR transforms ( P5/P6 ) .", "label": "", "metadata": {}}
{"text": "[ Before ROVER combination an alignment pass was run to get exact word timings .Due to the effects of automatic segmentation this process reduces the WER by about 0.1 % absolute .] Table 6 : Word error rates for each stage of the 1998 HTK broadcast news evaluation system ( also P4 FV contrast ) .Only P1 uses gender independent non - VTLN HMMs .P1 to P3 use triphones and P4-P6 quinphones .The results ( over the complete 1998 evaluation set ) for each of these stages , together with additional contrasts , is shown in Table 6 .", "label": "", "metadata": {}}
{"text": "This is a rather smaller MLLR gain than previously observed which we believe is due to the more extensive input data normalisation .There is a 6 % gain from employing the category trigram and 4-gram over the trigram alone , and a 7 % gain moving from adapted triphones to adapted quinphones : most of which ( 5 % ) was due to the full variance adaptation .This gain from the FV transform was rather greater than observed on the BNeval97 data .Finally , the effect of the automatic segmentation procedure on the BNeval98 set was investigated .", "label": "", "metadata": {}}
{"text": "However on BNeval98 the automatic segmenter faired more poorly .The WER for a first pass with wideband models was 0.7 % absolute higher with the automatic segments than with the manual segments .This poorer performance was also reflected in the number of frames assigned to multiple speaker segments : 1.6 % for BNeval97 but 4.3 % for BNeval98 .This paper has described the development and performance of the 1998 HTK broadcast news transcription system .A number of improvements to the systems accuracy have been described and , as in previous years , the system gave the lowest error rate overall on the main F0 focus condition .", "label": "", "metadata": {}}
{"text": "This work is in part supported by an EPSRC grant on ' ' Multimedia Document Retrieval ' ' reference GR / L49611 and by a grant from DARPA .Entropic Ltd. supplied software to aid in decoding and language model estimation .X. Luo of JHU supplied code to help with the soft - clustering experiments .Fiscus , J.G. ( 1997 )A Post - Processing System to Yield Reduced Word Error Rates : Recogniser Output Voting Error Reduction ( ROVER ) .Proc .IEEE Workshop on Automatic Speech Recognition and Understanding , pp .347 - 354 , Santa Barbara .", "label": "", "metadata": {}}
{"text": "Background .The availability of annotated corpora has facilitated the application of machine learning algorithms to concept extraction from clinical notes .However , high expenditure and labor are required for creating the annotations .A potential alternative is to reuse existing corpora from other institutions by pooling with local corpora , for training machine taggers .In this paper we have investigated the latter approach by pooling corpora from 2010 i2b2/VA NLP challenge and Mayo Clinic Rochester , to evaluate taggers for recognition of medical problems .The corpora were annotated for medical problems , but with different guidelines .", "label": "", "metadata": {}}
{"text": "We hope that our current work will be a useful case study for facilitating reuse of annotated corpora across institutions .Results .We found that pooling was effective when the size of the local corpus was small and after some of the guideline differences were reconciled .The benefits of pooling , however , diminished as more locally annotated documents were included in the training data .We examined the annotation guidelines to identify factors that determine the effect of pooling .Conclusions .The effectiveness of pooling corpora , is dependent on several factors , which include compatibility of annotation guidelines , distribution of report types and size of local and foreign corpora .", "label": "", "metadata": {}}
{"text": "Our findings need to be confirmed with further studies on different corpora .Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b2041 - 1480 - 4 - 3 ) contains supplementary material , which is available to authorized users .Background .Development of Natural Language Processing ( NLP ) tools generally requires a set of annotated documents in the application domain [ 1 ] .The annotations serve as a reference for constructing rule - based NLP systems and as a training corpus to derive machine learning models for concept extraction .", "label": "", "metadata": {}}
{"text": "Due to high demand , such corpora have been recently created with pioneering effort of some research groups and made available to the scientific community to support studies in clinical NLP [ 3 - 5 ] .Availability of the annotated corpora has fostered the application of machine learning algorithms to concept extraction from clinical notes [ 6 , 7 ] .Supervised machine learning taggers that achieve an accuracy of more than 80 % have been developed [ 8 , 9 ] , given their great success for general English text [ 10 ] and biomedical literature [ 11 - 13 ] .", "label": "", "metadata": {}}
{"text": "However , machine learning methods are sensitive to the distribution of data , such as the distribution of words in the vocabulary and grammar styles , which could significantly affect the portability of a trained machine learning system across institutions and , thus the value of annotated corpora .Given the barriers for preparing a large annotated corpus in individual institutions , consolidation of annotation efforts has the potential to advance clinical NLP .One way of leveraging existing efforts is to pool annotated corpora across institutions .Pooling of the annotations to train machine learning taggers may increase performance of the taggers [ 17 ] .", "label": "", "metadata": {}}
{"text": "In this paper we have investigated whether pooling of similar corpora from two different sources can improve performance of resultant machine learning taggers for medical problem detection .We hope that our study will be a useful guide for facilitating reuse of annotated corpora across institutions .Pooling Biomedical Corpora .There have been similar efforts to pool corpora in the biomedical domain .Johnson et al .[ 18 ] semi - automatically changed the format of the Protein Design Group corpus into two new formats ( WordFreak and embedded XML ) , without altering the semantics , to increase the usage of the corpus .", "label": "", "metadata": {}}
{"text": "Ohta et al .[19 ] extended the annotation of their GENIA corpus to integrate the annotation style of the GENTAG corpus , which is the other prominent and widely used biomedical corpus , so that their corpus can been pooled with others following the same format .As an extension of this work , Wang et al .[20 ] pooled these corpora ( and a third known as AIMED ) [ 21 ] hoping to achieve better performance using the large corpus .However , the performance dropped by 12 % .Subsequently they analyzed incompatibilities among the corpora .", "label": "", "metadata": {}}
{"text": "Recently , Huang et al .[ 23 ] have reported significant performance improvement of machine learning based part - of - speech ( POS ) taggers , by training them on pooled dataset .Our current effort can potentially benefit research on development of clinical taggers at healthcare institutions , by facilitating use of annotated corpora from other institutions .In the next subsection , we briefly explain the process of annotation for readers who are new to this field .Annotation of clinical text .Machine learning based taggers for detecting phrases that convey particular concepts , requires the availability of reports that have been manually marked ( annotated ) for the phrases .", "label": "", "metadata": {}}
{"text": "The exercise to manually create such a set of marked reports is initiated with the development of a guideline , which defines what to mark and also how to mark .A group of human annotators then independently follow the guideline to carry out the annotation exercise .Researchers developing a machine learning tagger at an institute have the option of training the tagger on i ) the in - house set of reports that have been manually annotated , ii ) reports annotated at another institution or iii ) a pooled set constructed by combining i and ii .", "label": "", "metadata": {}}
{"text": "In this paper , we have examined the factors associated with the use of corpus from other institutions .Overview of current work .We trained and tested taggers on a corpus from Mayo Clinic Rochester [ 24 ] and a corpus from the 2010 i2b2/VA NLP challenge [ 25 ] , and examined the effect of pooling the corpora [ 26 ] .These corpora share the property that they were annotated for the same task of developing taggers for detecting medical problems .However the corpora were constructed with different annotation guidelines .The experiments were carried out using an existing machine learning - based tagging system , MedTagger [ 9 ] , that participated in the 2010 i2b2/VA NLP challenge .", "label": "", "metadata": {}}
{"text": "The corpora used in that study were subsets of the i2b2 corpus and were annotated with the same annotation guideline .Results .Figure 1 summarizes the results of the experiments .Detailed results are tabulated in the Additional file 1 .Performance measures of the taggers .The plots A , B and C show the F1-score , precision and recall respectively .Each line in the figure corresponds to the test set ( MCR or i2b2 ) and the evaluation method : Exact ( E ) with solid lines or Overlap ( O ) with dashed lines .", "label": "", "metadata": {}}
{"text": "Intra - corpus testing .Taggers performed the best when the training and test sets were from the same corpus .F1-scores for MCR and i2b2 cross - validations were 0.58 and 0.79 for ' exact span ' evaluation , respectively .The higher F1-scores for i2b2 as compared to MCR could be due to a lower diversity of report types and annotations .Addition of more reports to MCR corpus might lead to improved performance for MCR .For the ' overlap spans ' , the F1-scores were 0.82 for MCR and 0.89 for i2b2 .", "label": "", "metadata": {}}
{"text": "Inter - corpora testing .Performance of the taggers was poor when they were trained exclusively on reports from the other corpora .For tagger trained on i2b2 and tested on MCR , the F1 score was 0.38 for exact spans .Similarly for tagger trained on MCR and tested on i2b2 , the scores was 0.40 .Supplementation of the training set with reports from other corpora decreased the performance , by 12 % points for MCR and 4 % points for i2b2 .The greater degradation for MCR is likely due to small size of the corpus as compared to the i2b2 corpus , i.e. the proportion of data supplemented to MCR training set was much larger than that for i2b2 .", "label": "", "metadata": {}}
{"text": "An exception was the improvement in the recall on MCR corpus when it was supplemented with i2b2 corpus , using ' overlap span ' evaluation .Results suggest that the corpora are incompatible for simple pooling .In an earlier study we had reported performance gain for machine learning taggers by pooling corpora across institutions and report types [ 17 ] .The corpora used in that study were subsets of the i2b2 corpus and were annotated with the same guideline .Contrastingly , in the current study , the corpora differ in their annotation guidelines , and also have different distributions of report types .", "label": "", "metadata": {}}
{"text": "Hence we investigated the effect of differences in the annotation guidelines , distributions of report types and corpora sizes , on the performance of taggers trained on pooled corpora , as described in the following sub - sections .Guideline differences .We examined the annotation guidelines to identify factors that contributed to the performance degradation of taggers trained on pooled corpora .Concept definition .Annotation guidelines for the two corpora differed slightly in definition of concepts .i2b2 annotation guideline extends definition of medical problem beyond the semantic type of signs / symptoms and disease / syndrome ( disorder ) , to include pathologic functions , mental dysfunction , molecular dysfunction , congenital abnormality , acquired abnormality , neoplastic process , and virus / bacterium .", "label": "", "metadata": {}}
{"text": "MCR annotation guideline defined signs / symptoms and disorders , which we mapped to the problem class .Signs / symptoms were defined as concepts that mapped to SNOMED - CT subset of semantic type signs / symptoms .Disease / syndrome had a looser definition that extended beyond the semantic types for ' i2b2 medical problem ' , to include injury or poisoning , behavioral dysfunction , cell dysfunction , experimental model of disease and anatomical abnormality but excluded virus / bacterium .Articles .The i2b2 annotations included articles , e.g. \" the cough \" and \" a fever \" , while MCR annotations did not .", "label": "", "metadata": {}}
{"text": "This contributes to the generally longer length of the i2b2 annotations ( Figure 2 ) .Distribution of number of tokens per annotation in the two corpora .MCR annotations ( red line ) are shorter than the i2b2 annotations ( blue dashed line ) .Possessive pronouns .i2b2 annotations included possessive pronouns , e.g. \" his cancer \" , while MCR annotations did not .3 % of i2b2 annotations began with ' his ' ( 174 ) or ' her ' ( 200 ) .Concepts not pertaining to patients .MCR annotations included concepts that were not pertaining to patients .", "label": "", "metadata": {}}
{"text": "Concepts that are not directly related to the patient were not annotated in i2b2 corpus .Prepositional phrases .The i2b2 guidelines specify that one prepositional phrase following a concept can be included in the annotation if it does not contain a markable concept and / either indicates an organ / body part .Also a preposition can be included in a concept phrase if words therein can be rearranged to express the same concept without the preposition .For example , ' pain in chest ' is a viable concept phrase according to the i2b2 guidelines schema because it indicates body part and can be rephrased without ' in ' as ' chest pain ' .", "label": "", "metadata": {}}
{"text": "MCR guidelines did not explicitly address this issue .Conjunctions .The i2b2 guidelines specify that conjunctions that denote lists are included if they occur within the modifiers or are connected by a common set of modifiers .If the portions of the lists are otherwise independent , they should not be included .For example , the text segment ' metastases in the liver and pancreas ' is a valid concept phrase including ' and ' , while the segment ' diarrhea , nausea and vomiting ' is not valid .The latter is annotated as three concept phrases .", "label": "", "metadata": {}}
{"text": "Reconciliation of annotations differences .To investigate the effect of annotation differences due to the differing guidelines , we considered curation of the annotations .Rectification of the differences in all guideline factors would require considerable manual effort .Hence , we restricted our effort to automated rectification of a subset of the factors .Specifically , we removed articles and possessive pronouns from i2b2 annotations .Fifteen percent of the i2b2 annotated phrases were modified .When this partially rectified corpus was used to supplement training data for the MCR corpus , there was lesser degradation of the performance measures .", "label": "", "metadata": {}}
{"text": "Performance measures of the taggers trained on the curated i2b2 corpus and tested on MCR corpus .The plots A , B and C show the F1-score , precision and recall respectively .There are two lines in each plot that correspond to the two evaluation methods : Exact ( E ) with solid lines and Overlap ( O ) with dashed lines .The horizontal axis indicates the training sets : i2b2 , i2b2C ( curated ) , MCR + i2b2 ( combined ) and MCR + i2b2C ( MCR combined with curated i2b2 ) .Table 2 shows the overlap in the annotated phrases in the two corpora .", "label": "", "metadata": {}}
{"text": "When one start word was ignored 55.1 % of the annotations matched and when one word was ignored 55.7 % matched .After the i2b2 corpus was curated to partially rectify the annotation differences , there was an improvement in the overlap of the corpora , as shown in Table 2 .Each cell in the table gives the percentage of annotations that matched with the other corpus before and after automated curation of the i2b2 corpus to partially rectify the annotation differences .The annotations in MCR corpus were mapped to the highest level of granularity-- to the UMLS CUIs .", "label": "", "metadata": {}}
{"text": "Consequently , these annotations can be expected to inherit the limitations of UMLS which includes lack of concept coverage .This would possibly be the reason why the taggers trained on MCR corpus and tested on i2b2 corpus showed greater degradation in performance than vice - versa .The annotation guideline for the i2b2 corpus advocated a more intuitive approach for annotation .I2b2 annotators used UMLS definitions to guide the annotation , which allowed them greater flexibility to annotate and even include phrases that were not covered in the UMLS .Hence , to facilitate reuse of the annotations for developing machine learning models for concept recognition , we suggest the following two - step approach for annotation .", "label": "", "metadata": {}}
{"text": "When normalization is not possible to any ontology node , the phrase should be marked as a ' novel ' concept .For developing machine learning applications it is critical that all the phrases that map to the concept of interest are annotated , by ensuring that even those which are not covered in the reference ontology are marked up .The first pass of ' concept recognition ' would ensure that all the concepts are covered .The second pass of ' normalization ' will facilitate the filtering / sub - classing of annotations for developing machine learning taggers for a particular sub - class .", "label": "", "metadata": {}}
{"text": "Also the ' novel ' annotation class will be useful for adding new ontology terms .Report type .In addition to the differences in the annotation guidelines , the performance of the taggers could be affected by the distribution of report types in the corpora .i2b2 corpus included discharge summaries and progress notes , while MCR corpus had a wider variety , since the reports were randomly selected from the EMR system for annotation .Named entities may vary in their distributions on report types .For instance the history and examination reports will have a high density of patient symptoms as compared to the progress notes that will mainly refer to the symptoms addressed by the current treatment .", "label": "", "metadata": {}}
{"text": "The distribution of the medical terminologies on the report types may also depend on the institution , as many institutions have their own report formats .However the reports in either corpus did not have meta - data about the type of report .Hence , the authors could not investigate the ' report - type ' factor further .Corpus size .To examine the effect of size , we measured the tagger performance on subsets of MCR corpus of various sizes , by performing 5 fold cross - validation experiments .This was compared to the performance after pooling the MCR subsets with the original and curated i2b2 corpus for training , i.e. the i2b2 corpus was used to supplement the training fraction of the MCR corpus during the cross - validations .", "label": "", "metadata": {}}
{"text": "We had increased the runs for cross - validating the subsets from 3 to 5 .The subsets were smaller in size , which increased the variation of the accuracy measurements .The additional runs were required to compensate for the increase in variation , so as to provide adequate confidence of the accuracy measurements .The results are summarized in Figure 4 and tabulated in the Additional file 1 .The plots A , B and C show the F1-score , precision and recall of the taggers respectively .The horizontal axis indicates the size of the MCR subsets used in the cross - validation .", "label": "", "metadata": {}}
{"text": "MCR , MCR + i2b2 ( MCR with training fraction supplemented by i2b2 ) and MCR + i2b2C ( MCR with training fraction supplemented by curated i2b2 ) .The two types of evaluation methods are represented using different line styles : Exact ( E ) with solid lines and Overlap ( O ) with dashed lines .The performance of taggers trained and tested on MCR corpus increases in F1-score and recall as the corpus size increases .The increase is rapid at first with increment in the corpus size , but later forms a plateau .", "label": "", "metadata": {}}
{"text": "Pooling with the i2b2 corpus nearly always increases the recall .However as the precision always degrades on pooling , the F1-score first increases with pooling and then degrades .A possible explanation is that the smaller subsets of MCR corpus are deficient in all the annotation patterns , and when these are supplemented by the i2b2 curated corpus , there is an improvement in recall .The improvement in recall for smaller sizes of MCR subsets is greater than the degradation in precision that occurs due to pooling , which translates to an improvement in F1-score .", "label": "", "metadata": {}}
{"text": "This result leads to a hypothesis that pooling with a compatible corpus from another institution may be beneficial and an economically favorable alternative to extending the in - house annotated corpus , only when the in - house corpus is below a critical size .However our analysis is limited to a single observation and further studies on other corpora are needed to investigate the combined effect of differences in the annotation guidelines , distributions of report type and sizes .The tagger performance with the curated i2b2 corpus was greater than with the original corpus for all subsets of MCR corpus .", "label": "", "metadata": {}}
{"text": "Summary .In summary , simple pooling of corpora was overall found to reduce the tagger performance .We examined the annotation guidelines for the corpora to delineate several inconsistencies that include concept definition , articles , possessive pronouns , unrelated concepts , prepositional phrases and conjunctions .Rectification of a subset of the annotation differences using an automatic approach reduced the performance degradation that occurred on pooling .The effect of distribution of report types could not be studied as the corpora were not annotated for report type .The effect of pooling was found to depend on the corpus size , as pooling was found to improve tagger performance for smaller subsets of the MCR corpus .", "label": "", "metadata": {}}
{"text": "Further studies on different corpora are needed to elucidate the relationship between the above mentioned factors and performance of taggers trained on pooled corpora .The investigation of these relationships would be a useful guide for researchers to develop machine learning taggers for clinical text .Conclusions .We investigated whether pooling of corpora from two different sources , can improve performance and portability of resultant machine learning taggers .The effect of pooling local and foreign corpora , is dependent on several factors that include compatibility of annotation guidelines , distribution of report types and corpus size .Simple automatic methods to rectify some of the guideline differences can be useful to facilitate pooling .", "label": "", "metadata": {}}
{"text": "The benefits of pooling diminish as more local annotations are created .Our findings need to be confirmed with further studies using different corpora and machine taggers .Future directions .Studies on different corpora are needed to elucidate the relationship between the above mentioned factors and performance of taggers trained on pooled corpora .We plan to investigate whether weighting of features for the machine learning tagger and filtering of the annotations using a dictionary lookup , can improve the tagger performance on pooling the corpora .Another interesting direction of investigation would be to train machine taggers separately on local and foreign corpora and then to combine the taggers using machine learning .", "label": "", "metadata": {}}
{"text": "Moreover the guidelines should also include instructions to annotate metadata about the reports , so that reports of the same type from different corpora can be readily pooled for enhancing machine learning based taggers .The authors are aware of an effort in this direction [ 27 ] , but there needs to be consensus for wider utilization of the standard for annotation of new corpora .The annotation groups involved in clinical research should come together to develop a standard annotation guideline that facilitates reuse of annotation efforts .We also suggest a two - pass annotation method that focuses first on concept recognition , followed by normalization to existing ontologies , to facilitate the pooling of corpora .", "label": "", "metadata": {}}
{"text": "Corpora .Two annotated corpora were used in this study ( Table 3 ) .The corpora differed in their sources as well their annotation guidelines , but contained annotations for the same concept type , i.e. ' medical problems ' .The first corpus consisted of 349 clinical reports from the ' 2010 i2b2/VA challenge on concept assertions and relations in clinical text ' [ 25 ] that were provided to the participants for training .Partners Healthcare , Beth Israel Deaconess Medical Center and University of Pittsburgh Medical Center contributed discharge summaries for this corpus , and University of Pittsburgh Medical Center also contributed progress reports .", "label": "", "metadata": {}}
{"text": "The second corpus had 160 clinical notes from Mayo Clinic Rochester ( MCR ) [ 24 , 28 ] .These were annotated for signs / symptoms , disorders , medications and procedures .The annotation class ' problem ' from the first corpus was equivalent to the combination of classes -- signs / symptoms and disorders in the second corpus and we carried out experiments with reference to this class . MedTagger .The experiments reported in this paper were carried out using an existing tagging system , MedTagger that participated in the 2010 i2b2/VA NLP challenge .", "label": "", "metadata": {}}
{"text": "The pipeline of this system consisted of dictionary lookup , part of speech ( POS ) tagging and machine learning for named entity prediction and concept extraction .Dictionary lookup .We used the UMLS MetaThesaurus [29 ] and a collection of words used in a clinical vocabulary viewer [ 30 ] as the domain dictionary .The input text and dictionary were normalized to facilitate flexible matching .The dictionary lookup tagged all phrase occurrences , including overlapping phrases .POS tagging .We used GENIA tagger for labeling parts of speech to all input tokens .GENIA tagger [ 31 ] is based on maximum entropy models trained on biomedical text as well as generic English text .", "label": "", "metadata": {}}
{"text": "Using the dictionary lookup and POS tagging results , we derived a set of features for each token .These were collated with other commonly used features for named entity recognition , such as words , word affixes and word shapes .The features were fed to a sequence tagger using a conditional random field ( CRF ) model [ 32 ] with a token window size of five .Experiment .We designed our experiments to examine effect of using pooled training sets on the performance of machine learning taggers for concept extraction ( Figure 5 ) .The taggers were trained to recognize medical problems , including signs / symptoms and disorders .", "label": "", "metadata": {}}
{"text": "We then performed 5-fold cross validation experiments on MCR , i2b2 and the combined ( i2b2 + MCR ) corpora .We repeated the cross - validation on MCR corpus after supplementing the training fraction with the i2b2 corpus during each of the cross validation runs .This design was repeated for the i2b2 corpus by using MCR corpus to supplement the training .The cross - validation experiments were repeated three times to average the performance scores , i.e. 3 times 5-fold cross - validation was performed .Design for different training / testing experiment reported in this paper .", "label": "", "metadata": {}}
{"text": "The dotted lines represent 5 fold cross - validation in which the fraction of the corpus being tested is excluded from the training set .Two kinds of evaluations were preformed : ' exact span ' matching and ' overlap span ' matching .In ' exact span ' matching , the annotations were counted as matches if begin and end spans matched .In case of ' overlap span ' evaluation the annotations were counted as matches , if there was any overlap in the span ranges .The overlap span is a more lenient measure of performance .", "label": "", "metadata": {}}
{"text": "Performance measures of precision , recall and F1 score were computed for the experiments ( Figure 1 ) .The measures are defined as follows : .Abbreviations .Declarations .Acknowledgements .We are thankful to the people who developed and made available the machine learning tools : GENIA tagger and Mallet , terminology services : UMLS and the labeled corpora used in this study .The i2b2 corpus of deidentified clinical reports used in this research were provided by the i2b2 National Center for Biomedical Computing funded by U54LM008748 and were originally prepared for the Shared Tasks for Challenges in NLP for Clinical Data organized by Dr. Ozlem Uzuner , i2b2 and SUNY .", "label": "", "metadata": {}}
{"text": "The authors are thankful for the constructive comments of the reviewers .This study was supported by National Science Foundation ABI:0845523 , Strategic Health IT Advanced Research Projects ( SHARP ) award ( 90TR0002 ) to Mayo Clinic from Health and Human Services ( HHS ) and National Library of Medicine ( NLM:1K99LM011389 , 5R01LM009959 - 02 ) grants .Electronic supplementary material .Competing interests .The authors declare that they have no competing interest .Authors ' contribution .KW carried out the experiments , led the study design and analysis and drafted the manuscript .MT helped with the experiments , participated in the study analysis and manuscript drafting .", "label": "", "metadata": {}}
{"text": "HL conceived the study , helped with the experiments , participated in the study design and analysis , and drafting of the manuscript .All authors read and approved the final manuscript .Authors ' Affiliations .Division of Biomedical Statistics and Informatics , Mayo Clinic .Department of Radiology , Georgetown University Medical Center .References .Demner - Fushman D , Chapman WW , McDonald CJ : What can natural language processing do for clinical decision support ?J Biomed Inform 2009 , 42 ( 5 ) : 760 - 772 .View Article .Meystre SM , Savova GK , Kipper - Schuler KC , Hurdle JF : Extracting information from textual documents in the electronic health record : a review of recent research .", "label": "", "metadata": {}}
{"text": "Uzuner O , Luo Y , Szolovits P : Evaluating the state - of - the - art in automatic de - identification .J Am Med Inform Assoc 2007 , 14 ( 5 ) : 550 - 563 .View Article .Uzuner O , Goldstein I , Luo Y , Kohane I : Identifying patient smoking status from medical discharge records .J Am Med Inform Assoc 2008 , 15 ( 1 ) : 14 - 24 .View Article .Uzuner O , Solti I , Cadag E : Extracting medication information from clinical text .J Am Med Inform Assoc 2010 , 17 ( 5 ) : 514 - 518 .", "label": "", "metadata": {}}
{"text": "Wang Y , Patrick J : Cascading classifiers for named entity recognition in clinical notes .In Proceedings of the workshop on biomedical information extraction .Borovets , Bulgaria .1859783 : Association for Computational Linguistics ; 2009:42 - 49 .Li D , Kipper - Schuler K , Savova G : Conditional random fields and support vector machines for disorder named entity recognition in clinical texts .In Proceedings of the workshop on current trends in biomedical natural language processing .Columbus , Ohio .1572326 : Association for Computational Linguistics ; 2008:94 - 95 .View Article .", "label": "", "metadata": {}}
{"text": "Phoenix , Arizona : PhD Phoenix ; 2011 .Torii M , Hu Z , Wu CH , Liu H : BioTagger - GM : a gene / protein name recognition system .J Am Med Inform Assoc 2009 , 16 ( 2 ) : 247 - 255 .View Article .Tjong Kim Sang EF , De Meulder F : Introduction to the CoNLL-2003 shared task .In Seventh conference on natural language learning .Edmonton , Canada ; 2003:142 - 147 .Wilbur J , Smith L , Tanabe T : BioCreative 2 gene mention task .In Proceedings of the second BioCreative challenge workshop .", "label": "", "metadata": {}}
{"text": "Arighi CN , Roberts PM , Agarwal S , Bhattacharya S , Cesareni G , Chatr - Aryamontri A , Clematide S , Gaudet P , Giglio MG , Harrow I , et al .: BioCreative III interactive task : an overview .BMC Bioinformatics 2011 , 12 ( Suppl 8) : S4 .View Article .Kim JD , Nguyen N , Wang Y , Tsujii J , Takagi T , Yonezawa A : The genia event and protein coreference tasks of the BioNLP shared task 2011 .BMC Bioinformatics 2012 , 13 ( Suppl 11 ) : S1 .", "label": "", "metadata": {}}
{"text": "J Am Med Inform Assoc 2010 , 17 ( 3 ) : 229 - 236 .Bakken S , Hyun S , Friedman C , Johnson S : A comparison of semantic categories of the ISO reference terminology models for nursing and the MedLEE natural language processing system .Stud Health Technol Inform 2004 , 107 ( Pt 1 ) : 472 - 476 .Haug P , Koehler S , Lau LM , Wang P , Rocha R , Huff S : A natural language understanding system combining syntactic and semantic techniques .Proc Annu Symp Comput Appl Med Care 1994 , 247 - 251 .", "label": "", "metadata": {}}
{"text": "J Am Med Inform Assoc 2011 , 18 ( 5 ) : 580 - 587 .View Article .Johnson HL , Baumgartner WA , Krallinger M , Cohen KB , Hunter L : Corpus refactoring : a feasibility study .Journal of Biomedical Discovery and Collaboration 2007 , 2 ( 1 ) : 4 .View Article .Ohta T , Kim J - D , Pyysalo S , Wang Y , Tsujii J : Incorporating GENETAG - style annotation to GENIA corpus .In Workshop on current trends in biomedical natural language processing .Stroudsburg , PA , USA : Association for Computational Linguistics ; 2009:106 - 107 .", "label": "", "metadata": {}}
{"text": "BMC Bioinformatics 2009 , 10 ( 1 ) : 403 .View Article .Wang Y , S\u00e6tre R , Kim J - D , Pyysalo S , Ohta T , Tsujii JI : Improving the inter - corpora compatibility for protein annotations .J Bioinform Comput Biol 2010 , 08 ( 05 ) : 901 .View Article .Fan JW , Prasad R , Yabut RM , Loomis RM , Zisook DS , Mattison JE , Huang Y : Part - of - speech tagging for clinical text : wall or bridge between institutions ?AMIA Annu Symp Proc 2011 , 2011 : 382 - 391 .", "label": "", "metadata": {}}
{"text": "Journal of American Medical Informatics Assocociation 2010 , 17 ( 5 ) : 507 - 513 .View Article .Uzuner O , South BR , Shen S , Duvall SL : 2010i2b2/VA challenge on concepts , assertions , and relations in clinical text .J Am Med Inform Assoc 2011 , 18 ( 5 ) : 552 - 556 .View Article .Wagholikar K , Torii M , Jonnalagadda S , Liu H : Feasibility of pooling annotated corpora for clinical concept extraction .In AMIA summit on clinical research informatics .San Francisco , CA : American Medical Informatics Summits on Translational Science Proceedings ; 2012:63 - 70 .", "label": "", "metadata": {}}
{"text": "In LREC'08 .Marrakech , Morocco : Proceedings of the Sixth International Conference on Language Resources and Evaluation LREC'08 ; 2008:3143 - 3150 .Bodenreider O : The unified medical language system ( UMLS ) : integrating biomedical terminology .Nucleic Acids Res 2004 , 32 ( Database issue ) : D267 - 270 .View Article .Friedman C , Liu H , Shagina L : A vocabulary development and visualization tool based on natural language processing and the mining of textual patient reports .J Biomed Inform 2003 , 36 ( 3 ) : 189 - 201 .", "label": "", "metadata": {}}
{"text": "Yoshimasa Tsuruoka YT , Jin - Dong K , Tomoko O , John MN , Sophia A , Junichi T : Developing a robust part - of - speech tagger for biomedical text .In Advances in informatics - 10th panhellenic conference on informatics .Heidelberg , Berlin : Springer Berlin ; 2005:382 - 392 .Copyright .\u00a9 Wagholikar et al . ; licensee BioMed Central Ltd. 2013 .This article is published under license to BioMed Central Ltd.The present invention provides method and apparatus for bilingual word alignment , method and apparatus for training bilingual word alignment model .", "label": "", "metadata": {}}
{"text": "Abstract .The present invention provides method and apparatus for bilingual word alignment , method and apparatus for training bilingual word alignment model .A method for bilingual word alignment by a processor executing instructions , comprising : . training a bilingual word alignment model using a word - aligned labeled bilingual corpus ; . word - aligning a plurality of bilingual sentence pairs in an unlabeled bilingual corpus using said bilingual word alignment model ; . determining whether the word alignment of each of said plurality of bilingual sentence pairs is correct , and when the word alignment is correct , adding the bilingual sentence pair into the labeled bilingual corpus and removing the bilingual sentence pair from the unlabeled bilingual corpus ; . retraining the bilingual word alignment model using the expanded labeled bilingual corpus ; and .", "label": "", "metadata": {}}
{"text": "training a backward bilingual word alignment model using the word - aligned labeled bilingual corpus , . said step of word - aligning a plurality of bilingual sentence pairs in an unlabeled bilingual corpus comprising : . forward - word - aligning each of said plurality of bilingual sentence pairs using said forward bilingual word alignment model ; and .backward - word - aligning each of said plurality of bilingual sentence pairs using said backward bilingual word alignment model , and . said step of determining whether the word alignment of each of said plurality of bilingual sentence pairs is correct comprising : . calculating an intersection set between the forward - word - aligning result and the backward - word - aligning result of the bilingual sentence pair ; . calculating a union set between the forward - word - aligning result and the backward - word - aligning result of the bilingual sentence pair ; and . determining , when the ratio of an element number of said intersection set to an element number of said union set is greater than a predetermined threshold , the word alignment of said bilingual sentence pair is correct .", "label": "", "metadata": {}}
{"text": "The method for bilingual word alignment according to . claim 1 , wherein said step of adding the bilingual sentence pair into the labeled bilingual corpus comprises : . calculating , when the forward - word - aligning result and the backward - word - aligning result are not identical , word translation probabilities of the different portions in the forward - word - aligning result and the backward - word - aligning result , and adding the word - aligning result having higher word translation probability into the labeled bilingual corpus .A method for training a bilingual word alignment model by a processor executing instructions , comprising : . training an initial bilingual word alignment model using a word - aligned labeled bilingual corpus ; . word - aligning a plurality of bilingual sentence pairs in an unlabeled bilingual corpus using said initial bilingual word alignment model ; . determining whether the word alignment of each of said plurality of bilingual sentence pairs is correct , and when the word alignment is correct , adding the bilingual sentence pair into the labeled bilingual corpus and removing the bilingual sentence pair from the unlabeled bilingual corpus ; and .", "label": "", "metadata": {}}
{"text": "training a backward initial bilingual word alignment model using the word - aligned labeled bilingual corpus , . said step of word - aligning a plurality of bilingual sentence pairs in an unlabeled bilingual corpus comprising : . forward - word - aligning each of said plurality of bilingual sentence pairs using said forward initial bilingual word alignment model ; and .backward - word - aligning each of said plurality of bilingual sentence pairs using said backward initial bilingual word alignment model , and . said step of determining whether the word alignment of each of said plurality of bilingual sentence pairs is correct comprising : . calculating an intersection set between the forward - word - aligning result and the backward - word - aligning result of the bilingual sentence pair ; . calculating a union set between the forward - word - aligning result and the backward - word - aligning result of the bilingual sentence pair ; and . determining , when the ratio of an element number of said intersection set to an element number of said union set is greater than a predetermined threshold , the word alignment of said bilingual sentence pair is correct .", "label": "", "metadata": {}}
{"text": "claim 4 , wherein said step of training the bilingual word alignment model comprises : . training a forward bilingual word alignment model using said expended labeled bilingual corpus ; and .training a backward bilingual word alignment model using said expended labeled bilingual corpus .The method for training bilingual word alignment model according to .claim 4 , wherein said step of adding the bilingual sentence pair into the labeled bilingual corpus comprises : . calculating , when the forward - word - aligning result and the backward - word - aligning result are not identical , word translation probabilities of the different portions in the forward - word - aligning result and the backward - word - aligning result , and adding the word - aligning result having higher word translation probability into the labeled bilingual corpus .", "label": "", "metadata": {}}
{"text": "a re - word - aligning unit configured to re - word - align the remaining bilingual sentence pairs in the unlabeled bilingual corpus using the retrained bilingual word alignment model , wherein . said model training unit trains a forward bilingual word alignment model using the word - aligned labeled bilingual corpus , and trains a backward bilingual word alignment model using the word - aligned labeled bilingual corpus , . said word - aligning unit forward - word - aligns each of said plurality of bilingual sentence pairs using said forward bilingual word alignment model , and backward - word - aligns each of said plurality of bilingual sentence pairs using said backward bilingual word alignment model , .", "label": "", "metadata": {}}
{"text": "claim 7 , . wherein said model retraining unit retrains the forward bilingual word alignment model using said expended labeled bilingual corpus , and retrains the backward bilingual word alignment model using said expended labeled bilingual corpus .The apparatus for bilingual word alignment according to .claim 7 , . wherein said determining unit further calculates word translation probabilities of the different portions in the forward - word - aligning result and the backward - word - aligning result , when the forward - word - aligning result and the backward - word - aligning result are not identical , and adds the word - aligning result having higher word translation probability into the labeled bilingual corpus .", "label": "", "metadata": {}}
{"text": "The apparatus for training bilingual word alignment model according to . claim 10 , wherein said model training unit trains a forward bilingual word alignment model using said labeled bilingual corpus expended by said determining unit , and trains a backward bilingual word alignment model using said labeled bilingual corpus expended by said determining unit .The apparatus for training bilingual word alignment model according to .Description .TECHNICAL FIELD .The present invention relates to information processing techniques , specifically , to the technique of bilingual word alignment and the technique of statistical machine translation in natural language processing .", "label": "", "metadata": {}}
{"text": "Word alignment is widely used in natural language processing .Existing word alignment technique usually uses a statistical word alignment model to make correspondence between a pair of words , each of which is a translation of the other , in a bilingual sentence .The statistical word alignment model contains statistical information used for determining a pair of words , each of which is a translation of the other , in a bilingual sentence .However , since the current statistical word alignment model uses large - scale unlabeled bilingual corpus to train a statistical word alignment model without supervision , such a statistical word alignment model could lead to producing many erroneous word alignment results .", "label": "", "metadata": {}}
{"text": "On the other hand , it is a laborious work to align the words in a large - scale bilingual corpus manually .If only a small - scale corpus needs to be manually labeled , it will not take too much labor and time .SUMMARY OF THE INVENTION .In order to solve above - mentioned problems of the existing techniques , the present invention provides a method and apparatus for bilingual word alignment as well as a method and apparatus for training a bilingual word alignment model .BRIEF DESCRIPTION OF THE DRAWINGS .It is believed that above - mentioned features , advantages and objectives of the present invention will be better understood through following description of the embodiments of the invention , taken in conjunction with the drawings in which , .", "label": "", "metadata": {}}
{"text": "1 is a flowchart showing a method for bilingual word alignment according to an embodiment of the present invention ; .FIG .2 is a flowchart showing a method for training a bilingual word alignment model according to an embodiment of the present invention ; .FIG .3 is a block diagram showing an apparatus for bilingual word alignment according to an embodiment of the present invention ; and .FIG .4 is a block diagram showing an apparatus for training a bilingual word alignment model according to an embodiment of the present invention .DETAILED DESCRIPTION OF THE INVENTION .", "label": "", "metadata": {}}
{"text": "FIG .1 is a flowchart showing a method for bilingual word alignment according to an embodiment of the present invention ; .As shown in .FIG .1 , first in Step 101 , a bilingual word alignment model is trained using a word - aligned labeled bilingual corpus .Though the larger the labeled bilingual corpus is , the better the performance of the system is , it takes labor and time to make word alignment manually , thus , the labeled corpus is very small relative to an unlabeled corpus described later .The word fertility probability p(\u03c6 i /w t is the probability with which the word wt in the target language corresponds to \u03c6 i words in the source language .", "label": "", "metadata": {}}
{"text": "In this step , using labeled bilingual corpus to train a forward bilingual word alignment model and to train a backward bilingual word alignment model .For instance , for Chinese / English bilingual corpus , it is needed to train a Chinese - English word alignment model and an English - Chinese word alignment model .Next , in Step 105 , the bilingual word alignment model trained in above Step 101 is used to word - align a plurality of bilingual sentence pairs in the unlabeled bilingual corpus .In this embodiment , a bilingual word alignment model , including a translation probability , a position distortion probability and a word fertility probability , is used .", "label": "", "metadata": {}}
{"text": "The word translation probability and the position alignment probability are used to find an optimal word alignment for each source language word so as to obtain an alignment series A 0 .On the basis of the alignment series Ai , the word translation probability , position distortion model and the word fertility model are used to find a better alignment series Ai+1 through trying exchanging any two alignments or changing an alignment .The process 2 is repeated till no better alignment series is found .Here , those skilled in the art should understand that any known and future searching algorithms can be used to search an optimal alignment series .", "label": "", "metadata": {}}
{"text": "Next , in Step 110 , it is determined whether the word alignment of each bilingual sentence pair is correct or not .If it is correct , the aligned bilingual sentence pair that is determined to be correct is added to the labeled bilingual corpus and deleted from the unlabeled bilingual corpus .R .A .A .A .A . is larger than a predetermined threshold th , the word alignment of this bilingual sentence pair is determined to be correct , otherwise , the word alignment of this bilingual sentence pair is determined to be incorrect .", "label": "", "metadata": {}}
{"text": "Besides , in this step , if the forward word alignment result of the bilingual sentence pair is not identical to the backward word alignment result , that is , if th .Thus , based on the definition .R .A .A .A .A .( suppose the predetermined threshold th is 0.7 ) .So the obtained alignments of the exemplary sentence meet the requirement .For the above example , there is .Next , in Step 115 , the expanded labeled bilingual corpus is used to retrain the bilingual word alignment model .", "label": "", "metadata": {}}
{"text": "Next , in Step 120 , above bilingual word alignment model is used to re - word - align the remaining bilingual sentence pairs in the unlabeled bilingual corpus .Thus , the method of this embodiment for bilingual word alignment can word - align sentence pairs in the unlabeled corpus more accurately than the existing word - alignment methods .Besides , according to another embodiment of the present invention , after Step 120 , above determination ( Step 110 ) , retraining ( Step 115 ) and the step of re - word - alignment ( Step 120 ) are further repeated till not any new correct word alignment is produced .", "label": "", "metadata": {}}
{"text": "Under the same inventive concept , .FIG .2 is a flowchart showing a method for training a bilingual word alignment model according to an embodiment of the present invention .Next , in conjunction with the figure , a description will be given to this embodiment .For the parts identical to that in the previous embodiment , explanation will be omitted properly .As shown in .FIG .2 , first in Step 201 , the labeled bilingual corpus is used to train an initial bilingual word alignment model .Similar to Step 101 shown in .", "label": "", "metadata": {}}
{"text": "In this step , the labeled bilingual corpus is used to train the forward bilingual word alignment model and to train the backward bilingual word alignment model .Next , in Step 205 , the trained initial bilingual word alignment model is used to word - align a plurality of bilingual sentence pair in the unlabeled bilingual corpus .Similar to Step 105 shown in .FIG .1 , in this embodiment , a bilingual word alignment model , including a translation probability , a position distortion probability and a word fertility probability , is used .The specific alignment manner is : .", "label": "", "metadata": {}}
{"text": "On the basis of the alignment series Ai , the word translation probability , position distortion model and the word fertility model are used to find a better alignment series Ai+1 through trying exchanging any two alignments or changing an alignment .The process 2 is repeated till no better alignment series is found .Here , those skilled in the art should understand that any known and future searching algorithms can be used to search an optimal alignment series .In this step , the trained initial bilingual word alignment model is used to word - align a plurality of bilingual sentence pairs in the unlabeled bilingual corpus , so as to obtain a forward word alignment result and a backward word alignment result respectively .", "label": "", "metadata": {}}
{"text": "If it is correct , the aligned bilingual sentence pair that is determined to be correct is added to the labeled bilingual corpus and deleted from the unlabeled bilingual corpus .R .A .A .A .A . is larger than a predetermined threshold th , the word alignment of this bilingual sentence pair is determined to be correct , otherwise , the word alignment of this bilingual sentence pair is determined to be incorrect .For a bilingual sentence pair with correct word alignment , it is moved from the unlabeled corpus to the labeled corpus .", "label": "", "metadata": {}}
{"text": "Next , in Step 215 , the expanded labeled bilingual corpus is used to retrain the bilingual word alignment model .In this step , the expanded labeled bilingual corpus is used to train the forward bilingual word alignment model and the backward bilingual word alignment model .Thus , it will not take too much labor and time , at the same time , the quality of the trained word alignment model can be guaranteed .Thus , the new alignment result can be used to further perfect the alignment model .FIG .3 is a block diagram showing an apparatus for bilingual word alignment according to an embodiment of the present invention .", "label": "", "metadata": {}}
{"text": "For the parts identical to that in the previous embodiments , explanation will be omitted properly .As shown in .Similar to the embodiment shown in .FIG .1 , in this embodiment , the bilingual word alignment model includes at least word translation probabilities , position distortion probabilities and word fertility probabilities .R .A .A .A .A . is larger than a predetermined threshold th , the word alignment of this bilingual sentence pair is determined to be correct , otherwise , the word alignment of this bilingual sentence pair is determined to be incorrect .", "label": "", "metadata": {}}
{"text": "The model retraining unit 304 uses the expanded labeled corpus to retrain the forward bilingual word alignment model and uses the expanded labeled corpus to retrain the backward bilingual word alignment model .Thus , the new alignment result can be used to further perfect the alignment model so as to improve the accuracy of the word alignment .Here it should be noted that the apparatus 300 for bilingual word alignment and its components can be constructed with dedicated circuits or chips , or can be realized by a computer ( processor ) through executing corresponding programs .FIG .", "label": "", "metadata": {}}
{"text": "Next , in conjunction with the figure , a description will be given to this embodiment .For the parts identical to that in the previous embodiments , explanation will be omitted properly .As shown in .Similar to the embodiment shown in .FIG .2 , in this embodiment , the bilingual word alignment model at least includes a translation probability , a position distortion probability and a word fertility probability .R .A .A .A .A . is larger than a predetermined threshold th , the word alignment of this bilingual sentence pair is determined to be correct , otherwise , the word alignment of this bilingual sentence pair is determined to be incorrect .", "label": "", "metadata": {}}
{"text": "From above description it can be seen that , the apparatus of this embodiment for training a bilingual word alignment model can implement above described methods for training a bilingual word alignment model of the embodiments of the present invention .Thus , it will not take too much labor and time , at the same time , the quality of the trained word alignment model can be guaranteed .Besides , according to another embodiment of the present invention , the model training unit 404 uses the expanded labeled corpus to train a forward bilingual word alignment model and uses the expanded labeled corpus to train a backward bilingual word alignment model .", "label": "", "metadata": {}}
{"text": "As described in above embodiments , the steps of determination , training and re - word - alignment are repeated till not any new correct word alignment is produced .Thus , the new alignment result can be used to further perfect the word alignment model .Here it should be noted that the apparatus 400 for training a bilingual word alignment model and its components can be constructed with dedicated circuits or chips , or can be realized by a computer ( processor ) through executing corresponding programs .Thus , the present invention is not limited to these embodiments , and the scope of the present invention is only defined by the appended claims .", "label": "", "metadata": {}}
{"text": "i have created a bag of words which output somthing like : ( 1 , ' saurashtra ' )( 1 , ' saumyajit ' )( 1 , ' satyendra ' ) i want ... .I have a dataframe in python which contains all of my data for binary classification .I ingest data in two iterations - once all of the data of one class and then all of the data of the other class .I ... .I 'm using a pandas data frame to hold my data which is built out of documents and class ( binary classification problem ) .", "label": "", "metadata": {}}
{"text": "Q-1 .How to change data of a corpus to appropriate format for training with ' caret ' package ?First of all , i would like to give you some environments for this question and i will be show you where i ... .I 've been looking at using AWS Machine Learning to implement a categorizer for my project .I have something on the order of 40,000 documents that have a several text - only features .For example : Name ... .I 'm trying to classify documents by sequence vector .Basically , I have a vocabulary ( more than 5000 words ) .", "label": "", "metadata": {}}
{"text": "I often see people use tf - idf vectorization in text classification task like sentiment analysis .From my understanding , it penalizes the score of the words that appear in a lot of documents .( inverse ... .I am messing around with sklearn and support vector machines to classify documents .I am trying to learn the Stanford NLP Classifier and would like to work on the problem of document classification .Can anyone suggest a place where i can find a working example ?I was also looking at ... .I just created my own Naive Bayes model from scratch and trained it on 776 documents .", "label": "", "metadata": {}}
{"text": "The ... .I have a big data set that I use to train a naive classifier using Apache Mahout .I use the classifier to classify a bunch of documents ( this is like my test set ) .The way I classify documents is as ... .I 'm new to the gensim package and vector space models in general , and I 'm unsure of what exactly I should do with my LSA output .To give a brief overview of my goal , I 'd like to enhance Naive Bayes ... .I 'm working on a document categorization project wherein I have some crawled text documents on different topics which I want to categorize into pre - decided categories like travel , sports , education etc . ... .", "label": "", "metadata": {}}
{"text": "I wish to classify the documents into whether they are about the \" majority topic \" ... .I am using libsvm library for document classification of resumes .I have multiple resumes and I need to classify them .Do I need multilabel classification OR multiclass classification in this case .I have implemented a document classification tool using Mallet which classifies each page of a document to certain categories .I have tried Weka too but Mallet is smarter than Weka on this aspect .My ... .I am trying to build a supervised multi label predictor .", "label": "", "metadata": {}}
{"text": "I have a rather limited data set upon which I am performing supervised - learning , multi - class text classification using scikit - learn .To alleviate the shortage of information slightly , I wanted to do ... .I am currently using tfidf prior to performing classification on a number of websites based on their content .Unfortunately , my training data is not uniform : about 70 % of the pre - labeled websites are ... .I am trying to classify legal case documents which are in text format , in different folders like Civil , Land , Criminal , e.t.c , I intended using Naive Bayes as Vectoriser to get the vectors from the ... .", "label": "", "metadata": {}}
{"text": "First I used StringToWordVector filter and filtered data were used with SVM classifier ( LibSVM ) for cross validation .Later I have read a blog post here It ... .I have used LibSVM wrapper for weka and successfully built a classifier for news classification ( Sports and Business ) .I have evaluated it using cross validation method and accuracy is accepted .So ... .I recently used Bag - of - Words classifier to make a Document Matrix with 96 % terms .Then I used a Decision Tree to train by model on the bag of words input to make a prediction whether the sentence is ... .", "label": "", "metadata": {}}
