{"text": "Milgram 's notorious conformity experiment replicated .The Situationist has a fantastic post on a recent replication of Stanley Milgram 's ( in)famous conformity experiment which is usually always described as being ' too unethical to perform today ' .In Milgram 's original study , participants were asked to give increasingly severe electric shocks to someone supposedly trying to learn a series of word pairs .In fact , the ' learner ' was an actor and no shocks were given , but they screamed as if they were in increasing amounts of pain , while the experimenter ordered the participant to increase the voltage .", "label": "", "metadata": {}}
{"text": "65 % of participants continued despite indications that the ' learner ' might be unconscious or dead .It 's been a hugely influential study , but was thought to be so stressful for the participants , that it has never been replicated in real life and it was assumed it would be impossible to do so .However , this replication was carefully designed by Prof Jerry Burger to be as close as possible to Milgram 's original study while being modified so it could be fully ethically approved by a research ethics committee ( the mark of all good research ) .", "label": "", "metadata": {}}
{"text": "But I also made several substantial changes .First , we stopped the procedures at the 150-volt mark .This is the first time participants heard the learner'\u00c4\u00f4s protests through the wall and his demands to be released .When we look at Milgram'\u00c4\u00f4s data , we find that this point in the procedure is something of a ' \u00c4\u00fapoint of no return . '\u00c4\u00f9 Of the participants who continued past 150 volts , 79 percent went all the way to the highest level of the shock generator ( 450 volts ) .Knowing how people respond up to this point allowed us to make a reasonable estimate of what they would do if allowed to continue to the end .", "label": "", "metadata": {}}
{"text": "Second , we used a two - step screening process for potential participants to exclude any individuals who might have a negative reaction to the experience . . . .More than 38 percent of the interviewed participants were excluded at this point .Third , participants were told at least three times ( twice in writing ) that they could withdraw from the study at any time and still receive their $ 50 for participation .Fourth , like Milgram , we administered a sample shock to our participants ( with their consent ) .However , we administered a very mild 15-volt shock rather than the 45-volt shock Milgram gave his participants .", "label": "", "metadata": {}}
{"text": "Within a few seconds after ending the study , the learner entered the room to reassure the participant he was fine .Sixth , the experimenter who ran the study also was a clinical psychologist who was instructed to end the session immediately if he saw any signs of excessive stress .Interestingly , the study found that levels of obedience were about the same now , as they were in the early 1960s when the original experiment was first run .This is not the first time that someone has tried to replicate Milgram 's experiment .The BPS Research Digest reported on a virtual reality version of the study ( admittedly , not a true replication ) , the full - text of which is available online .", "label": "", "metadata": {}}
{"text": "Link to Situationist on Milgram replication ( thanks Tom ! )Link to Wikipedia page on Milgram 's original study .Sections .Sections Select Category advertising Attention Authors books Graffiti Hearing and Language Inside the Brain Integrating Learning Linkage Moving News Nonsense Other People Reasoning Remembering Seeing Theory Togetherness Uncategorized March 30 - 31 , in conjunction with EACL 2009 in Athens , Greece .The translation task of this workshop focuses on European language pairs .Translation quality will be evaluated on a shared , unseen test set of news stories .We provide a parallel corpus as training data , a baseline system , and additional resources for download .", "label": "", "metadata": {}}
{"text": "Goals .The goals of the shared translation task are : .To investigate the applicability of current MT techniques when translating into languages other than English .To examine special challenges in translating between European languages , including word order differences and morphology .To create publicly available corpora for machine translation and machine translation evaluation .To generate up - to - date performance numbers for European languages in order to provide a basis of comparison in future research .To offer newcomers a smooth start with hands - on experience in state - of - the - art statistical machine translation methods .", "label": "", "metadata": {}}
{"text": "Changes This Year .After a number of extensions over the last years , this year 's translation task will be more focused .The motivation for this is to have a clearly defined task and to be able to gather sufficient human judgement data to make as many statistically significant distinctions as possible .News Translation Only : We will only evaluate performance on a set of news stories that we prepared for this evaluation .As in the previous year , the news stories are taken from major news outlets as the BBC , Der Spiegel , Le Monde , etc . during the time period of September - October 2008 .", "label": "", "metadata": {}}
{"text": "Official Metric is Manual Sentence Ranking :While we continue to experiment with different forms of manual and automatic evaluation , the official metric that we will use is human preference judgments on a sentence - by - sentence basis .More Training Data : The parallel corpora Europarl and News Commentary are extended , a large French - English corpus ( currently 400 million words ) was added , and we also proved 50 - 500 million word monolingual training data from the test domain of news stories .Constrained vs. Unconstrained : You may use any additional resources that you wish to ( including training data , knowledge sources such as existing translation systems ) , but you should flag that your system uses additional data .", "label": "", "metadata": {}}
{"text": "Note that basic linguistic tools such as taggers , parsers , or morphological analyzers are allowed in the constrained condition .Task Description .We provide training data for four European language pairs , and a common framework ( including a language model and a baseline system ) .The task is to improve methods current methods .This can be done in many ways .For instance participants could try to . improve word alignment quality , phrase extraction , phrase scoring .add new components to the open source software of the baseline system . augment the system otherwise ( e.g. by preprocessing , reranking , etc . ) .", "label": "", "metadata": {}}
{"text": "Participants will use their systems to translate a test set of unseen sentences in the source language .The translation quality is measured by a manual evaluation and various automatic evaluation metrics .Participants agree to contribute to the manual evaluation about eight hours of work .You may participate in any or all of the following language pairs : .French - English .Spanish - English .German - English .Czech - English .Hungarian - English .For all language pairs we will test translation in both directions .To have a common framework that allows for comparable results , and also to lower the barrier to entry , we provide a common training set , language model , and baseline system .", "label": "", "metadata": {}}
{"text": "Your submission report should highlight in which ways your own methods and data differ from the standard task .We may break down submitted results in different tracks , based on what resources were used .We are mostly interested in submission that are constraint to the provided training data , so that the comparison is focused on the methods , not on the data used .You may submit contrastive runs to demonstrate the benefit of additional training data .The provided data is mainly taken from a new release ( version 4 ) of the Europarl corpus , which is freely available .", "label": "", "metadata": {}}
{"text": "If you prepare training data from the Europarl corpus directly , please do not take data from Q4/2000 ( October - December ) , since it is reserved for development and test data .Additional training data is taken from the new News Commentary corpus .There are about 40 - 45 million words of training data per language from the Europarl corpus and 2 million words from the News Commentary corpus .You will need to to download the current version of the CzEng corpus ( version v0.7 ) from the CzEng web site .Note that in difference to previous years the released data is not tokenized and includes sentences of any length ( including empty sentences ) .", "label": "", "metadata": {}}
{"text": "The following tools allow the processing of the training data into tokenized format : .Tokenizer tokenizer.perl .Detokenizer detokenizer.perl .Lowercaser lowercase.perl .SGML Wrapper wrap - xml .perl .Development Data .To tune your system during development , we provide a development set of 2051 sentences .The data is provided in raw text format and in an SGML format that suits the NIST scoring tool .Since most statistical system use a tuning set and a test set during system development , we also provide a version of the development set split up into tuning set ( news - dev2009a ) and test set ( news - dev2009b ) , consisting of alternating sentences from the original set .", "label": "", "metadata": {}}
{"text": "English .French .Spanish .German .Czech .Hungarian .This data is a cleaned version of the 2008 test set .Additional Development Data .The following sets have been used in previous translation tasks and may be useful .Europarl dev2006 .English .French .Spanish .German .This data is identical with the 2005 development test data and the 2006 development data .News Commentary nc - dev2007 .English .French .Spanish .German .Czech .This is identical with the 2007 development data .Hunglish hung - dev2008 .", "label": "", "metadata": {}}
{"text": "English .Note that this dev set is extracted from the official Hunglish corpus .The training corpus we provide does not overlap with the dev set .Europarl devtest2006 .English .French .Spanish .German .This data is identical with the 2005 test data and the 2006 development test data .News Commentary nc - devtest2007 .English .French .Spanish .German .Czech .This data is identical with the 2006 test data ( out - of - domain part ) .Hunglish hung - devtest2008 .Hungarian .English .Note that this devtest set is extracted from the official Hunglish corpus .", "label": "", "metadata": {}}
{"text": "German .Czech .This data is identical with the 2008 test data .Europarl test2008 .English .French .Spanish .German .Test Data .The test set is similar in nature as the news - dev2008 developement set .It is taken from identical sources .Evaluation .Evaluation will be done both automatically as well as by human judgement .Manual Scoring : We will collect subjective judgments about translation quality from human annotators .If you participate in the shared task , we ask you to commit about 8 hours of time to do the manual evaluation .", "label": "", "metadata": {}}
{"text": "As in previous years , we expect the translated submissions to be in recased , detokenized , XML format , just as in most other translation campaigns ( NIST , TC - Star ) .Dates .Please email n - best results to jschroe1@inf.ed.ac.uk and indicate which submission they are from .If your n - best lists are too big to email , use this upload form and email to let us know you 've uploaded them .Thanks !January 9 : Short paper submissions ( 4 pages ) M. Dzikovska , R. Nielsen , C. Brew , C. Leacock , D. Giampiccolo , L. Bentivogli , P. Clark , I. Dagan , H. Dang .", "label": "", "metadata": {}}
{"text": "L. Kotlerman , I. Dagan , M. Gorodetsky , E. Daya .Sentence Clustering via Projection over Term Clusters .L. Kotlerman , I. Dagan , M. Gorodetsky , E. Daya .Sentence Clustering via Projection over Term Clusters .L. Kotlerman , N. Madnani , A. Cahill .ParaQuery : Making Sense of Paraphrase Collections .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .O. Levy , T. Zesch , I. Dagan , I. Gurevych .Recognizing Partial Textual Entailment .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria . A. Lotan , A. Stern , I. Dagan .", "label": "", "metadata": {}}
{"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies . A. Lotan , A. Stern , I. Dagan .TruthTeller : Annotating Predicate Truth .In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies .O. Melamud , I. Dagan , J. Goldberger , I. Szpektor .Using Lexical Expansion to Learn Inference Rules from Sparse Data .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .", "label": "", "metadata": {}}
{"text": "A Two Level Model for Context Sensitive Inference Rules .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .Shachar Mirkin , Ido Dagan , Maayan Geffet .Integrating Pattern - based and Distributional Similarity Methods for Lexical Entailment Acquisition .Shachar Mirkin , Ido Dagan , Maayan Geffet .Integrating Pattern - based and Distributional Similarity Methods for Lexical Entailment Acquisition .Vivi Nastase , Carlo Strapparava .Bridging Languages through Etymology : The case of cross language text categorization .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .", "label": "", "metadata": {}}
{"text": "Semeval-2013 Task 8 : Cross - lingual Textual Entailment for Content Synchronization .M. Negri , A. Marchetti , Y. Mehdad , L. Bentivogli , D. Giampiccolo .Semeval-2012 Task 8 : Cross - lingual Textual Entailment for Content Synchronization .S. Pad\u00f3 , J. \u0160najder , B. Zeller .Derivational Smoothing for Syntactic Distributional Semantics .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria . E. Shnarch , E. Segal Halevi , J. Goldberger , I. Dagan .PLIS : a Probabilistic Lexical Inference System .", "label": "", "metadata": {}}
{"text": "B. Zeller , J. \u0160najder , S. Pad\u00f3 .DErivBase : Inducing and Evaluating a Derivational Morphology Resource for German .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .Torsten Zesch , Omer Levy , Iryna Gurevych , Ido Dagan .UKP - BIU : Similarity and Entailment Metrics for Student Response Analysis .Torsten Zesch , Omer Levy , Iryna Gurevych , Ido Dagan .UKP - BIU : Similarity and Entailment Metrics for Student Response Analysis .S. Pado , T.-G. Noh , A. Stern , R. Wang , R. Zanoli . forthcoming .", "label": "", "metadata": {}}
{"text": "Journal of Natural Language Engineering .Accepted for publication .M. Adler , J. Berant , I. Dagan .Entailment - based Text Exploration with Application to the Health - care Domain .In Proceedings of the ACL 2012 System Demonstrations .L. Bentivogli , P. Clark , I. Dagan , D. Giampiccolo .The Seventh PASCAL Recognizing Textual Entailment Challenge .Proceedings of TAC 2011 .pdf .L. Bentivogli , P. Clark , I. Dagan , D. Giampiccolo .The Sixth PASCAL Recognizing Textual Entailment Challenge .Proceedings of TAC 2010 .pdf .L. Bentivogli , I. Dagan , H. Dang , D. Giampiccolo , M. Lo Leggio , and B. Magnini .", "label": "", "metadata": {}}
{"text": "5th International Conference on Generative Approaches to the Lexicon ( GL 2009 ) .pdf .L. Bentivogli , B. Magnini , I. Dagan , H. Dang , D. Giampiccolo .The Fifth PASCAL Recognizing Textual Entailment Challenge .Proceedings of TAC 2009 .pdf .J. Berant , I. Dagan , M. Adler , J. Goldberger .Efficient Tree - based Approximation for Entailment Graph Learning .In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL 2012 ) .J. Bos , K. Markert .Recognising Textual Entailment with Logical Inference .Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2005 ) , pp .", "label": "", "metadata": {}}
{"text": "pdf .R. Braz , R. Girju , V. Punyakanok , D. Roth , and M. Sammons .An Inference Model for Semantic Entailment in Natural Language .Twentieth National Conference on Artificial Intelligence ( AAAI-05 ) .R. Braz , R. Girju , V. Punyakanok , D. Roth , and M. Sammons .Knowledge Representation for Semantic Entailment and Question - Answering . IJCAI-05 Workshop on Knowledge and Reasoning for Answering Questions . E. Cabrio , B. Magnini , A. Ivanova .Extracting Context - Rich Entailment Rules from Wikipedia Revision History .In Proceedings of the ACL 2012 Workshop on People 's Web meets NLP .", "label": "", "metadata": {}}
{"text": "Text Semantic Similarity , with Applications . RANLP-05 .I. Dagan and O. Glickman .Probabilistic textual entailment : Generic applied modeling of language variability .In PASCAL Workshop on Learning Methods for Text Understanding and Mining , Grenoble .I. Dagan , O. Glickman , A. Gliozzo , E. Marmorshtein and C. Strapparava .Direct Word Sense Matching for Lexical Substitution .COLING - ACL 2006 . E. Daya , E. Hurvitz , M. Wasserblat , B. Magnini , G. Neumann , S. Pado , G. Fidanza , G. Gianforme , M. Meisdrock , I. Dagan .Excitement - EXploring Customer Interactions through Textual EntailMENT .", "label": "", "metadata": {}}
{"text": "R. Delmonte , 2005 .VENSES - a Linguistically - Based System for Semantic Evaluation , PLN , Procesamiento del Lenguaje Natural , Revista n \u00b0 35 , ISSN:1135 - 5948 , pp .449 - 450 .R. Delmonte , 2005 .Simulare la comprensione del linguaggio con VENSES .presented at Workshop \" Scienze Cognitive Applicate \" , Facolt ? di Psicologia dell'Universit ?Roma \" La Sapienza \" , 12/13 - 12 - 2005 .Georgiana Dinu and Rui Wang .Inference Rules and their Application to Recognizing Textual Entailment .EACL-09 .M. Dzikovska , R. Nielsen , C. Brew , C. Leacock , D. Giampiccolo , L. Bentivogli , P. Clark , I. Dagan , H. Dang .", "label": "", "metadata": {}}
{"text": "pdf .P. S. Feizabadi , S. Pado .Automatic Identification of Motion Verbs in WordNet and FrameNet for Locational Inference .In Proceedings of KONVENS 2012 .M. Geffet and I. Dagan .Feature Vector Quality and Distributional Similarity .Proceedings of The 20th International Conference on Computational Linguistics ( COLING ) .M. Geffet and I. Dagan . \"The Distributional Inclusion Hypotheses and Lexical Entailment \" , ACL 2005 , Michigan , USA . D. Giampiccolo , H. Dang , B. Magnini , I. Dagan , E. Cabrio , B. Dolan .The Fourth PASCAL Recognizing Textual Entailment Challenge .", "label": "", "metadata": {}}
{"text": "pdf .O. Glickman , I. Dagan and M. Koppel .A Probabilistic Classification Approach for Lexical Textual Entailment , Twentieth National Conference on Artificial Intelligence ( AAAI-05 ) .O. Glickman , E. Shnarch and I. Dagan .Lexical Reference : a Semantic Matching Subtask .EMNLP 2006 ( poster ) .S. Gorzitze , S. Pado .Corpus - based Acquisition of German Event- and Object - Denoting Nouns .In Proceedings of KONVENS 2012 . A. Haghighi , A. Y. Ng , and C. D. Manning .Robust Textual Inference via Graph Matching .HLT - EMNLP 2005 . S. Harabagiu and A. Hickl .", "label": "", "metadata": {}}
{"text": "COLING - ACL 2006 .J. Herrera , A. Pe\u00f1as , F. Verdejo , 2006 .Textual Entailment Recognition Based on Dependency Analysis and WordNet .MLCW 2005 .LNAI 3944 .V. Jijkoun and M. de Rijke .Recognizing Textual Entailment : Is Lexical Similarity Enough ? , In : I. Dagan , F. Dalche , J. Quinonero Candela , B. Magnini , editors , Evaluating Predictive Uncertainty , Textual Entailment and Object Recognition Systems , LNAI 3944 , pages 449 - 460 , Springer Verlag .L. Kotlerman , I. Dagan , M. Gorodetsky , E. Daya .", "label": "", "metadata": {}}
{"text": "L. Kotlerman , N. Madnani , A. Cahill .ParaQuery : Making Sense of Paraphrase Collections .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .M. Kouylekov and B. Magnini .Tree Edit Distance for Textual Entailment .RANLP 2005 .O. Levy , T. Zesch , I. Dagan , I. Gurevych .Recognizing Partial Textual Entailment .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria . A. Lotan , A. Stern , I. Dagan .", "label": "", "metadata": {}}
{"text": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies .B. MacCartney , T. Grenager , M. de Marneffe , D. Cer and C. D. Manning .Learning to Recognize Features of Valid Textual Entailments .HLT - NAACL 2006 .M. Makatchev , P. W. Jordan , K. Vanlehn .Abductive Theorem Proving for Analyzing Student Explanations to Guide Feedback in Intelligent Tutoring Systems .Journal of Automated Reasoning , 32(3 ) .O. Melamud , I. Dagan , J. Goldberger , I. Szpektor .Using Lexical Expansion to Learn Inference Rules from Sparse Data .", "label": "", "metadata": {}}
{"text": "O. Melamud , J. Berant , I. Dagan , J. Goldberger , I. Szpektor .A Two Level Model for Context Sensitive Inference Rules .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .Shachar Mirkin , Ido Dagan , Maayan Geffet .Integrating Pattern - based and Distributional Similarity Methods for Lexical Entailment Acquisition .COLING - ACL 2006 pdf .Shachar Mirkin , Ido Dagan , Eyal Shnarch .Evaluating the Inferential Utility of Lexical - Semantic Resources .EACL-09 . pdf .Shachar Mirkin , Lucia Specia , Nicola Cancedda , Ido Dagan , Marc Dymetman and Idan Szpektor .", "label": "", "metadata": {}}
{"text": "ACL-09 .pdf .Shachar Mirkin , Ido Dagan and Sebastian Pad\u00f3 .Assessing the Role of Discourse References in Entailment Inference .ACL-10 pdf .Shachar Mirkin , Jonathan Berant , Ido Dagan and Eyal Shnarch .Recognising Entailment within Discourse . COLING-10 .pdf .Shachar Mirkin , Ido Dagan , Lili Kotlerman and Idan Szpektor .Classification - based Contextual Preferences .TextInfer 2011[ 1 ] . A. H. Moin , G. Neumann .Assisting Bug Triage in Large Open Source Projects Using Approximate String Matching .In Proceedings of the Seventh International Conference on Software Engineering Advances ( ICSEA 2012 ) .", "label": "", "metadata": {}}
{"text": "Light - Weight Entailment Checking for Computational Semantics , In : P. Blackburn and M. Kohlhase , editors , International workshop on Inference in Computational Semantics ( ICoS-3 ) .R. Nairn , C. Condoravdi , and L. Karttunen .Computing relative polarity for textual inference .International workshop on Inference in Computational Semantics ( ICoS-5 ) .Vivi Nastase , Carlo Strapparava .Bridging Languages through Etymology : The case of cross language text categorization .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .M. Negri , A. Marchetti , Y. Mehdad , L. Bentivogli , D. Giampiccolo .", "label": "", "metadata": {}}
{"text": "pdf .M. Negri , A. Marchetti , Y. Mehdad , L. Bentivogli , D. Giampiccolo .Semeval-2012 Task 8 : Cross - lingual Textual Entailment for Content Synchronization .pdf .S. Pado , J. Utt .A Distributional Memory for German .In Proceedings of the KONVENS 2012 Workshop on recent developments and applications of lexical - semantic resources .S. Pad\u00f3 , J. \u0160najder , B. Zeller .Derivational Smoothing for Syntactic Distributional Semantics .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .", "label": "", "metadata": {}}
{"text": "Discovering asymmetric entailment relations between verbs using selectional preferences .COLING - ACL 2006 .V. Pekar .Acquisition of Verb Entailment from Text .HLT - NAACL 2006 . A. Pe\u00f1as , A. Rodrigo , F. Verdejo .SPARTE , a Test Suite for Recognising Textual Entailment in Spanish .Computational Linguistics and Intelligent Text Processing , CICLing 2006 .LNCS 3878 . R. Raina , A. Y. Ng , and C. Manning .Robust textual inference via learning and abductive reasoning .Twentieth National Conference on Artificial Intelligence ( AAAI-05 ) .M. Regneri , R. Wang .", "label": "", "metadata": {}}
{"text": "In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning .L. Romano , M. Kouylekov , I. Szpektor , I. Dagan and A. Lavelli .Investigating a Generic Paraphrase - based Approach for Relation Extraction .EACL 2006 .V. Rus , A. Graesser and K. Desai .Lexico - Syntactic Subsumption for Textual Entailment .RANLP 2005 .B. Sacaleanu , G. Neumann .An Adaptive Framework for Named Entity Combination .In Proceedings of the Eighth International Conference on Language Resources and Evaluation ( LREC 2012 ) .Mark Sammons , Vinod Vydiswaran , and Dan Roth .", "label": "", "metadata": {}}
{"text": "ACL-10 pdf . E. Shnarch , I. Dagan , J. Goldberger .A Probabilistic Lexical Model for Ranking Textual Inferences . E. Shnarch , E. Segal Halevi , J. Goldberger , I. Dagan .PLIS : a Probabilistic Lexical Inference System .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .R. Snow , L. Vanderwende and A. Menezes .Effectively Using Syntax for Recognizing False Entialment .HLT - NAACL 2006 . A. Stern , R. Stern , I. Dagan , A. Felner .Efficient Search for Transformation - based Inference .", "label": "", "metadata": {}}
{"text": "BIUTEE :A Modular Open - Source System for Recognizing Textual Entailment .In Proceedings of the ACL 2012 System Demonstrations .M. Tatu and D. Moldovan .A Semantic Approach to Recognizing Textual Entailment .HLT - EMNLP 2005 .M. Tatu and D. Moldovan .A Logic - based Semantic Approach to Recognizing Textual Entailment .COLING - ACL 2006 ( poster ) . A. Volokh , G. Neumann .Extending Dependency Treebanks with Good Sentences .In Proceedings of KONVENS 2012 .R. Wang , S. Li .Constructing a Question Corpus for Textual Semantic Relations .", "label": "", "metadata": {}}
{"text": "Rui Wang and G\u00fcnter Neumann .Recognizing Textual Entailment Using a Subsequence Kernel Method .AAAI-07 .Rui Wang and Yajing Zhang .Recognizing Textual Entailment with Temporal Expressions in Natural Language Texts .In Proceedings of the IEEE International Workshop on Semantic Computing and Applications ( IWSCA-2008 ) .Rui Wang and G\u00fcnter Neumann .An Accuracy - Oriented Divide - and - Conquer Strategy for Recognizing Textual Entailment .TAC 2008 Workshop - RTE-4 .Rui Wang and Yi Zhang .Recognizing Textual Relatedness with Predicate - Argument Structures .EMNLP 2009 . H. Weisman ; J. Berant ; I. Szpektor ; I. Dagan .", "label": "", "metadata": {}}
{"text": "In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning .F. M. Zanzotto and A. Moschitti .Automatic learning of textual entailments with cross - pair similarities .COLING - ACL 2006 .N. Zeichner , J. Berant , I. Dagan .Crowdsourcing Inference - Rule Evaluation .In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( ACL 2012 ) .Britta Zeller and Sebastian Pad\u00f3 .A Search Task Dataset for German Textual Entailment .In Proceedings of the 10th International Conference on Computational Semantics ( IWCS 2013 ) .", "label": "", "metadata": {}}
{"text": "DErivBase : Inducing and Evaluating a Derivational Morphology Resource for German .In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( ACL 2013 ) , Sofia , Bulgaria .Torsten Zesch , Omer Levy , Iryna Gurevych , Ido Dagan .UKP - BIU : Similarity and Entailment Metrics for Student Response Analysis .Journal papers .I. Androutsopoulos and P. Malakasiotis .A Survey of Paraphrasing and Textual Entailment Methods .Journal of Artificial Intelligence Research , vol .38 , pp .135 - 187 .[ 2 ] .Ido Dagan , Bill Dolan , Bernardo Magnini and Dan Roth .", "label": "", "metadata": {}}
{"text": "Journal of Natural Language Engineering , vol .15(4 ) , pp .i - xvii .S. Pado , T.-G. Noh , A. Stern , R. Wang , R. Zanoli . forthcoming .Design and Realization of a Modular Architecture for Textual Entailment .Journal of Natural Language Engineering .Accepted for publication .Books .Ido Dagan , Dan Roth , Mark Sammons and Fabio Massimo Zanzotto .Recognizing Textual Entailment : Models and Applications .Morgan & Claypool .[ 3 ] Passionate about IP !Since June 2003 the IPKat weblog has covered copyright , patent , trade mark , info - tech and privacy / confidentiality issues from a mainly UK and European perspective .", "label": "", "metadata": {}}
{"text": "You 're welcome to read , post comments and participate in our community .You can email the Kats here .Regular round - ups of the previous week 's blogposts are kindly compiled by Alberto Bellan .First with the news , Managing Intellectual Property magazine reported yesterday that the EU Competitiveness Council is meeting on 29 and 30 May to discuss progress on technical issues relating to the fabled Community patent .Previous attempts to clinch agreement on the Community patent , which would provide a single unitary right covering the territory of all 27 EU member states , have foundered due to concerns over language , cost and court proceedings .", "label": "", "metadata": {}}
{"text": "The latest plans however seek to employ machine translations , which are already available for German , Spanish and French at the European Patent Office .These plans were previously discussed at meetings of an EU Council working party under the Slovenian Presidency , which expires at the end of June .The translation tools would include databases of several million standardised technical terms , which would be translated into all EU languages .The machines are capable of delivering translations in about 45 seconds .Under the draft plans , machine translations would be provided for all 23 languages , ensuring equal treatment .", "label": "", "metadata": {}}
{"text": "Existing technology would also need an upgrade .Machine translations would be for information purposes and would have no legal status .For the estimated 1 % of patents that are litigated in infringement proceedings or the like , human translations would be needed .A working document with full details of the proposal is due to be published in a couple of weeks , in time for the Competitiveness Council meeting at the end of the month .The IPKat looks forward to the new game of Community Patent Whispers .You take a claim , feed it into the translation machine for the first official language ( taking them in alphabetical order ) ; when you get the output , you feed it in for translation into the second , and vice versa .", "label": "", "metadata": {}}
{"text": "Merpel says , my favourite term in European patent law is the phrase \" as such \" .If you translate it from English to Dutch , Dutch to French , French to German and then German to English you get \" ash look for \" .7 comments : .Lets not forget that human translators are not infallible .Significant errors can and do arise during a single translation , let alone if you translate the same phrase via 23 languages .As you point out - machine translations would be for information purposes only , the original would remain decisive in the case of differences .", "label": "", "metadata": {}}
{"text": "More use of this has to be a good thing .Jeremy , as i suggested recently in a guest editorial in World Patent Information , there is an easier and more accurate solution .Require electrically filed EP patents to have the disclosure ( but not the claims ) written in a word processor which constrains the writer to write text which is ' digestible ' by the particular software the EPO uses .Then the discosure could be translated more accurately to any of the languages the software supports .With more and more member states , the EU sometimes has difficulty in finding every linguistic combination for interpreting and translation .", "label": "", "metadata": {}}
{"text": "For example , they might not be able to find a translator who knowns both Finnish and Greek .To get around this problem , they translate the Greek text into a third language , say English , and then from English into Finnish .This has not thus far caused any major upsets within the EU ( as such ! )We need one official EU language - for sole use in all official documents , including patents .It should be Latin ( as it used to be ) .This gives ( roughly ) equal pain to all member countries .", "label": "", "metadata": {}}
{"text": "If a country wants an official document translated for the benefit of its citizens , it can pay for it ( subsidiarity ) .We have one master text ( so no need to argue whether ' animal varieties ' is the same as ' Tierarten ' ) .We sell it to the French on the basis that it will infuriate the Americans .And it clears the way for English to become the unofficial EU language .Jeremy , the important figure is not the 1 % of patents which may be litigated , it 's the 100 % of patents which may be used as prior art during examination before any of the world 's patent offices .", "label": "", "metadata": {}}
{"text": "The basic problem is it then ends up being your ( Japanese - speaking collegue 's ) word against the examiners , unless your willing to pay for an official translation that is .there are at least two different machine translation options : rule based ( RBMT ) and statistical ( SMT ) .RBMT for so many language pairs is beyond any hope , see EuroTra disaster .SMT for so many languages can be obtained easily and cheaply if ( human ) translations of similar texts exist ( which happens to be the case for many languages , because of Art . 65 EPC ) .", "label": "", "metadata": {}}
{"text": "Syntax is sometimes very bad , in particular if the word order in both languages differs .Moreover morphology such as agreement in gender and number is problematic .However , word sense disambiguation is typically fine .SMT works better between closely related languages ( e.g. FR - PT - ES - IT - EN ) and less good for languages with rich morphology ( German , Finnish ) , e.g. concatenation of nouns .Proposal : the EU funds a project to scan / OCR available translations and makes them available to the SMT community .In addition a Euromatrix2 funding stipulates further research on statistical machine translation , in particular domain dependent SMT using IPC classification .", "label": "", "metadata": {}}
{"text": "Further remark : Things get easier if only one source language is used ( English ) .Last remark : a bad MT is better than late and unsearchable human translations as it happens to be the case right now .So at the day of publication you may search prior art using CLIR , then obtain the original in one of the three official languages .Then you look for the original applicant .For EN , DE and FR you will find people which can judge if a patent is relevant .Want to complain ?Danny or Dennis will review your complaint , preserving the confidentiality of your identity and will let both you and us know whether your complaint is justified .", "label": "", "metadata": {}}
{"text": "Eleanore and Harold Jantz Fellowship .Dates to Remember .Deadline for applications : January 29 , 2016 by 5 pm EST . .Duke University 's Rubenstein Rare Book & Manuscript Library offers one Eleanore and Harold Jantz Fellowship , with an award of $ 1500 , for researchers whose work would benefit from access to the Jantz Collections .The Jantz Collections contain some 3,500 titles in German Baroque literature , complemented by more than 7,000 titles reflecting Dr. Harold Jantz 's other scholarly interests - namely German Americana of all periods , English - German literary relations , the Age of Goethe , Rosicruciana and Occulta .", "label": "", "metadata": {}}
{"text": "Faculty members , graduate students and independent scholars with relevant research projects are eligible to apply .All applicants must reside beyond a 100-mile radius of Durham , N.C. , and may not be current Duke University students or employees .What expenses does the grant cover ?Grant money may be used for the following : . transportation expenses ( including air , train or bus ticket charges ; car rental ; mileage using a personal vehicle ; parking fees ) .accommodations . meals . photocopying and reproduction expenses .Expenses will be reimbursed once the grant recipient has completed his or her research visit(s ) and has submitted original receipts .", "label": "", "metadata": {}}
{"text": "Research topics must be strongly supported by the Rubenstein Library 's Jantz Collections .We encourage prospective grant applicants to discuss their research projects and the materials that might support them with a Rubenstein Library reference archivist before submitting an application .Download and complete an application form ( Word document ; 54 KB ) .Applications must be completed in English .E - mail completed application and C.V. as attachments ( Word document or PDF ) to special - collections(at)duke.edu .We will send you an e - mail confirming that your application was received .Graduate students must submit a letter of recommendation from a faculty advisor .", "label": "", "metadata": {}}
{"text": "The letter may be submitted as e - mail attachment to the e - mail address above or by postal mail to : .Jantz Research Grant Program Attn : David Pavelich David M. Rubenstein Rare Book & Manuscript Library Box 90185 Duke University Durham , NC 27708 - 0185 USA A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .Building a translation lexicon from comparable , non - parallel corpora US 8234106 B2 .", "label": "", "metadata": {}}
{"text": "A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .The system may use various clues , e.g. , context and frequency , to identify and score other possible translation pairs , using the seed lexicon as a basis .An alternative system may use a small bilingual lexicon in addition to non - parallel corpora to learn translations of unknown words and to generate a parallel corpus .A method for building a translation lexicon from non - parallel corpora by a machine translation system , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}}
{"text": "The method of .claim 1 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The method of .claim 1 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .The method of .claim 3 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The method of .", "label": "", "metadata": {}}
{"text": "The method of .claim 1 , wherein said identifying comprises identifying cognates .The method of .claim 1 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The method of .claim 1 , wherein said one or more clues includes similar context .The method of .claim 1 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a target word .The method of .claim 9 , further comprising generating a context vector .", "label": "", "metadata": {}}
{"text": "claim 1 , wherein said identifying comprises identifying frequencies of occurrence of words in the first and second first corpora .The method of .claim 1 , further comprising : generating matching scores for each of a plurality of clues .The method of .claim 12 , further comprising adding the matching scores .The method of . claim 13 , further comprising weighting the matching scores .A non - transitory computer readable medium having embodied thereon a program , the program being executable by a processor for performing a method for building a translation lexicon from non - parallel corpora , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}}
{"text": "The non - transitory computer readable medium of .claim 15 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The non - transitory computer readable medium of . claim 15 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .The non - transitory computer readable medium of . claim 17 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .", "label": "", "metadata": {}}
{"text": "The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying cognates .The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar context .The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a targetword .", "label": "", "metadata": {}}
{"text": "The apparatus of .claim 24 , wherein the lexicon builder is configured to use the identically spelled words in the seed lexicon as accurate translations .The apparatus of 24 , further comprising a matching module operative to be executed to match strings in the two non - parallel corpora to generate a parallel corpus including the matched strings as translation pairs .The apparatus of 26 , the apparatus comprising : . an alignment module operative to be executed to align text segments in two nonparallel corpora , the corpora including a source language corpus and a target language corpus .", "label": "", "metadata": {}}
{"text": "claim 27 , wherein the alignment module is operative to build a Bilingual Suffix Tree from a text segment from one of said two non - parallel corpora .Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a Continuation of U.S. patent application Ser .No .10/401,124 , filed on Mar. 26 , 2003 , now U.S. Pat .No .7,620,538 , which claims priority to U.S. Provisional Application Ser .No .60/368,070 , filed on Mar. 26 , 2002 , and U.S. Provisional Application Ser .No .60/368,447 , filed on Mar. 27 , 2002 , the disclosures of which are incorporated by reference .", "label": "", "metadata": {}}
{"text": "The research and development described in this application were supported by Defense Advanced Research Project Agency ( DARPA ) under grant number N66001 - 00 - 1 - 8914 .The U.S. Government may have certain rights in the claimed inventions .BACKGROUND .Machine translation ( MT ) concerns the automatic translation of natural language sentences from a first language ( e.g. , French ) into another language ( e.g. , English ) .Systems that perform MT techniques are said to \" decode \" the source language into the target language .Roughly speaking , statistical machine translation ( SMT ) divides the task of translation into two steps : a word - level translation model and a model for word reordering during the translation process .", "label": "", "metadata": {}}
{"text": "Parallel corpora contain large amounts of text in one language along with their translation in another .Unfortunately , such corpora are available only in limited amounts and cover only in specific genres ( Canadian politics , Hong Kong laws , etc ) .However , monolingual texts exist in higher quantities and in many domains and languages .The availability of monolingual corpora has been enhanced greatly due to the digital revolution and widespread use of the World Wide Web .Methods for processing such resources can therefore greatly benefit the field .SUMMARY .In an embodiment , a system may be able to build a translation lexicon from comparable , non - parallel corpora .", "label": "", "metadata": {}}
{"text": "In another embodiment , a system may align text segments in comparable , non - parallel corpora , matching strings in the corpora , and using the matched strings to build a parallel corpus .The system may build a Bilingual Suffix Tree ( BST ) and traverse edges of the BST to identify matched strings .The BST may also identify potential translations based on words in the corpora between matched strings .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system for building a translation lexicon according to an embodiment .", "label": "", "metadata": {}}
{"text": "2 is a flowchart describing a method for building a translation lexicon from non - parallel corpora .FIG .3 is a table showing results of an experiment utilizing the system of .FIG .1 .FIG .4 is a block diagram of a system for building a translation lexicon according to another embodiment .FIG .5 is a suffix tree .FIG .6 is a Generalized Suffix Tree ( GST ) .FIG .7 is a Bilingual Suffix Tree ( BST ) .FIG .8 is a portion of a BST showing example alignments .", "label": "", "metadata": {}}
{"text": "9 are portions of a BST describing left and right alignments .FIG .10 is psuedocode describing an algorithm for learning translations of unknown words .DETAILED DESCRIPTION .FIG .1 shows a system 100 for building a translation lexicon 105 according to an embodiment .The system may use non - parallel monolingual corpora 110 , 115 in two languages to automatically generate one - to - one mapping of words in the two languages .The two monolingual corpora should be in a fairly comparable domain .For example , in an implementation , an English - German translation lexicon was generated from a 1990 - 1992 Wall Street Journal corpus on the English side and a 1995 - 1996 German news wire ( DPA ) on the German side .", "label": "", "metadata": {}}
{"text": "However , they span different time periods and have a different orientation : the World Street Journal covers mostly business news , the German news wire mostly German politics .The system 100 may use clues to find translations of words in the monolingual corpora .The first clue considered may be the existence of identical words in the two corpora .Due to cultural exchange , a large number of words that originate in one language may be adopted by others .Recently , this phenomenon can be seen with words such as \" Internet U or \" Aids u. \" These terms may be adopted verbatim or changed by well - established rules .", "label": "", "metadata": {}}
{"text": "2 shows a flowchart describing a method 200 for building a translation lexicon from non - parallel corpora .A word comparator 120 may be used to collect pairs of identical words ( block 205 ) .In the English German implementation described above , 977 identical words were found .When checked against a benchmark lexicon , the mappings were found to be 88 % correct .The correctness of word mappings acquired in this fashion may depend highly on word length .While identical three - letter words were only translations of each other 60 % of the time , this was true for 98 % of ten - letter words .", "label": "", "metadata": {}}
{"text": "Accordingly , the word comparator 120 may restrict the word length to be able to increase the accuracy of the collected word pairs .For instance , by relying only on words at least of length six , 622 word pairs were collected with 96 % accuracy .The identified identically spelled word pairs may be used as a seed lexicon 130 ( block 210 ) .A lexicon builder 125 may expand the seed lexicon into the larger translation lexicon 105 by applying rules based on clues which indicate probable translations .The lexicon builder 125 may use seed lexicon to bootstrap these methods , using the word pairs in the seed lexicon as correct translations .", "label": "", "metadata": {}}
{"text": "For German to English , this includes replacing the letters k and z by c and changing the ending -t\u00e4t to -ty .Both these rules can be observed in the word pair Elektrizitat and electricity .The lexicon builder 125 may utilize these rules to expand the seed lexicon .In the English - German implementation , 363 additional word pairs were collected , with an accuracy of 91 % .The lexicon builder 125 extracts potential translation word pairs based on one or more clues .These clues may include similar spelling , similar context , preserving word similarity , and word frequency .", "label": "", "metadata": {}}
{"text": "\" This is even more the case for words that can be traced back to common language roots , such as \" friend \" and \" Freund , \" or \" president \" and \" Pr\u00e4sident . \"Still , these words , often called \" cognates , \" maintain a very similar spelling .This can be defined as differing in very few letters .This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .This measurement may be referred to as the \" longest common subsequence ratio .", "label": "", "metadata": {}}
{"text": "This may be done in a greedy fashion , i.e. , once a word is assigned to a word pair , the lexicon builder 125 does not look for another match .Another clue is context .If the monolingual corpora are somewhat comparable , it can be assumed that a word that occurs in a certain context should have a translation that occurs in a similar context .The context may be defined by the frequencies of context words in surrounding positions .This context has to be translated into the other language , and the lexicon builder 125 can search the word with the most similar context .", "label": "", "metadata": {}}
{"text": "The counts may be collected separately for each position and then entered into a context vector with a dimension for each context word in each position .Finally , the raw counts are normalized .Vector comparison is done by adding all absolute differences of all components .Alternatively , the lexicon builder 125 may count how often another word occurs in the same sentence as the target word .The counts may then be normalized by a using the tf / idf method , which is often used in information retrieval .The seed lexicon may be used to construct context vectors that contain information about how a new unmapped word co - occurs with the seed words .", "label": "", "metadata": {}}
{"text": "The lexicon builder 125 can search for the best matching context vector in the target language , and decide upon the corresponding word to construct a word mapping .The lexicon builder 125 may compute all possible word , or context vector , matches .The best word matches may be collected in a greedy fashion .Another clue is based on the assumption that pairs of words that are similar in one language should have translations that are similar in the other language .For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .", "label": "", "metadata": {}}
{"text": "In one approach , the context vector for each word in the lexicon may consist of co - occurrence counts in respect to a number of peripheral tokens ( basically , the most frequent words ) .These counts may be collected for each position in an n - word window around the word in focus .Instead of comparing the co - occurrence counts directly , the Spearman rank order correlation may be applied .For each position , the tokens are compared in frequency and the frequency count is replaced by the frequency rank , e.g. , the most frequent token count is replaced with 1 and the least frequent by n. R . a .", "label": "", "metadata": {}}
{"text": "i .b .i . )n .n .The result is a matrix with similarity scores between all German words , and a second matrix with similarity scores between all English words .For a new word , the lexicon builder 125 may look up its similarity scores to seed words , thus creating a similarity vector .Such a vector can be translated into the other language .The translated vector can be compared to other vectors in the second language .The lexicon builder 125 may perform a greedy search for the best matching similarity vectors and add the corresponding words to the lexicon .", "label": "", "metadata": {}}
{"text": "Frequency may be defined as a ratio of the word frequencies normalized by the corpus sizes .Each of the clues provides a matching score between two words ( block 220 ) , e.g. , a German word and an English word .The likelihood of these two words being actual translations of each other may correlate to these scores .The lexicon builder 125 may employ a greedy search to determine the best set of lexicon entries based on these scores ( block 225 ) .First , the lexicon builder 125 searches for the highest score for any word pair .", "label": "", "metadata": {}}
{"text": "This may be performed iteratively until all words are used up .The lexicon builder 125 may combine different clues by adding up the matching scores .The scores can be weighted .For example , when using the spelling clue in combination with others , it may be useful to define a cutoff .If two words agree in 30 % of their letters , this is generally as bad as if they do not agree in any , i.e. , the agreements are purely coincidental .FIG .3 shows results of the English - German implementation .", "label": "", "metadata": {}}
{"text": "The English - German implementation was restricted to nouns .Verbs , adjectives , adverbs and other part of speech may be handled in a similar way .They might also provide useful context information that is beneficial to building a noun lexicon .These methods may be also useful given a different starting point .For example , when building machine translation systems , some small parallel text should be available .From these , some high - quality lexical entries can be learned , but there will always be many words that are missing .These may be learned using the described methods .", "label": "", "metadata": {}}
{"text": "4 shows a system 400 for building a translation lexicon according to another embodiment .The system 400 may also be used to build parallel corpora from comparable corpora .Given an initial bilingual lexicon 405 and two texts 410 , 415 in each of the languages , the system 400 may identify parts of the texts which can be aligned ( i.e. , are mutual translations of each other according to the lexicon ) .The parts can be arbitrarily long , i.e. , the system 400 may align sequences of a few words rather than or in addition to whole sentences or whole phrases .", "label": "", "metadata": {}}
{"text": "For example , consider the following two sentences where the only unknown French word is \" raison \" : .\" Ce est pour cette raison que le initiative de le ministre . . .; \" and .\" It is for this reason that the party has proposed . . . \" .Since \" Ce est pour cette \" can be aligned with \" It is for this \" and \" que le \" with \" that the , \" it is a reasonable assumption that \" raison \" can be translated by \" reason .\" The system 400 may search the corpora for cases similar to this example .", "label": "", "metadata": {}}
{"text": "The suffix tree of a string uniquely encodes all the suffixes of that string ( and thus , implicitly , all its substrings too ) .The system 400 may first build such a tree of the target language corpus , and then add to each substrings all the substrings from the source language corpus that align to it .The next step is to identify unknown target language words that are surrounded by aligned substrings .The source language word that corresponds to the \" well - aligned \" unknown is considered to be a possible translation .A suffix tree stores in linear space all suffixes of a given string .", "label": "", "metadata": {}}
{"text": "FIG .5 shows the suffix tree 500 of string xyzyxzy .Note that if a suffix of a string is also a prefix of another suffix ( as would be the case for suffix zy of string xyzyxzy ) , a proper suffix tree can not be built for the string .The problem is that the path corresponding to that suffix would not end at a leaf , so the tree can not have the last property in the list above .To avoid this , the system 400 appends an end - of - string marker \" $ \" that appears nowhere else in the string .", "label": "", "metadata": {}}
{"text": "Each monolingual corpus given as input to the system 400 may be divided into a set of sentences .The system 400 may use a variant of suffix trees that works with sets of strings , namely Generalized Suffix Trees ( GST ) .In a GST of a set of strings , each path from the root to a leaf represents a suffix in one or more strings from the set .FIG .6 shows the GST 600 for a corpus of two sentences .The numbers at the leaves 605 of the tree show which sentences contain the suffix that ends there .", "label": "", "metadata": {}}
{"text": "Building a GST for a set of strings takes time and space linear in the sum of the lengths of all strings in the set .A Bilingual Suffix Tree ( BST ) is the result of matching a source language GST against a target language GST .Two strings ( i.e. , sequences of words ) match if the corresponding words are translations of each other according to a bilingual lexicon .In order to perform the matching operation , all paths that correspond to an exhaustive traversal of one of the trees ( the source tree ) are traversed in the other ( the target tree ) , until a mismatch occurs .", "label": "", "metadata": {}}
{"text": "FIG .7 shows two corpora 705 , 710 , a bilingual lexicon 715 , and the corresponding BST 720 .Edges drawn with dotted lines mark ends of alignment paths through the tree .Their labels are ( unaligned ) continuations of the source language substrings from the respective paths .Since there is a one - to - one correspondence between the substrings in the text and the paths in the suffix trees , the operation described above will yield all pairs of substrings in the two corpora given as input and discover all partial monotone alignments defined by the lexicon .", "label": "", "metadata": {}}
{"text": "The paths in the resulting bilingual tree will also have weights associated with them , defined as the product of the matching probabilities of the words along the path .BSTs are constructed to encode alignment information , therefore the extraction of parallel phrases amounts to a simple depth - first traversal of the tree .FIG .8 shows some alignments we can extract from the BST in .FIG .7 , a portion of which is shown in .FIG .8 .As can be seen in .For alignment extraction , we are interested in edges of the third type , because they mark ends of alignments .", "label": "", "metadata": {}}
{"text": "The fact that n has outgoing edge e indicates there is a mismatch on the subsequent words of those two sequences .Thus , in order to extract all aligned substrings , the system 400 traverses the BST on edges labeled with word pairs , and extract all paths that end either at the leaves or at nodes that have outgoing edges labeled only with source language words .The heuristic by which the system 400 discovers new word translations is shown graphically in .FIG .9 .FIG .9 ( i ) shows a branch 905 of the BST corresponding to the comparable corpus in the same figure .", "label": "", "metadata": {}}
{"text": "This may be taken as a weak indication that d and y are translations of each other .This indication would become stronger if , for example , the sequences following d and y in the two corpora would also be aligned .One way to verify this is to reverse both strings , build a BST for the reversed corpora ( a reverse BST ) , and look for a common path that diverges at the same d and y. .FIG .9 ( ii ) shows the reverse BST 910 , and in bold , the path we are interested in .", "label": "", "metadata": {}}
{"text": "For a pair of words from the two corpora , we use the terms \" right alignment \" and \" left alignment \" to refer to the aligned sequences that precede and respectively succeed the two words in each corpus .The left and right alignments and the two words delimited by them make up a context alignment .For example , the left alignment xyzabc , the right alignment xzy - acb and the words y and d in .FIG .9 ( iii ) make up a context alignment 915 .Given a comparable corpus , this procedure will yield many context alignments which correspond to incorrect translations , such as that between the words \" canadien \" and \" previous \" : . tout canadien serieux .", "label": "", "metadata": {}}
{"text": "In order to filter out such cases , the system 400 uses two simple heuristics : length and word content .The translation candidate must also be an open - class word .The algorithm 1000 for learning translations of unknown words is summarized in .FIG .10 .An advantage of the algorithm over previous approaches is that we do not provide as input to the algorithm a list of unknown words .Instead , the system automatically learns from the corpus both the unknown words and their translation , upon discovery of appropriate context alignments .It was obtained by taking two non - parallel , nonaligned segments from the Hansard corpus .", "label": "", "metadata": {}}
{"text": "A small bilingual lexicon of 6,900 entries was built using 5,000 sentences pairs ( 150,000 words for each language ) .The parallel corpus was taken from the Proceedings of the European Parliament ( EuroParl )Note that the parallel corpus belongs to a different domain than the comparable corpus .Also the parallel corpus is extremely small .For low density languages , such a corpus can be built manually .When given as input the comparable corpora described above and the bilingual lexicon of 6,900 entries , the algorithm 1000 found 33,926 parallel sequences , with length between three and seven words .", "label": "", "metadata": {}}
{"text": "The system also found translations for thirty unknown French words .Of these , nine were correct , which means a precision of 30 % .For each of the two corpora , building the monolingual GST took only 1.5 minutes .The matching operation that yields the BST is the most time - consuming : it lasted 38 hours for the forward BST and 60 hours for the reverse BST .The extractions of all parallel phrases and of the translations took about 2 hours each .The experiments were run on a Linux \u00ae system 400 with an Intel \u00ae Pentium \u00ae 3 processor of 866 Mhz .", "label": "", "metadata": {}}
{"text": "Nevertheless , it will be understood that various modifications may be made without departing from the spirit and scope of the invention .For example , blocks in the flowcharts may be skipped or performed out of order and still produce desirable results .Also , the heuristics described herein may be combined with the alignment method described herein .Accordingly , other embodiments are within the scope of the following claims .Document conversion system including data monitoring means that adds tag information to hyperlink information and translates a document when such tag information is included in a document retrieval request .", "label": "", "metadata": {}}
{"text": "Annual Meeting of the ACL Assoc . for Computational Linguistics , Morristown , NJ , 597 - 604 .Fox , H. , \" Phrasal Cohesion and Statistical Machine Translation \" Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , Jul. 2002 , pp .304 - 311 .Association for Computational Linguistics .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . abstract .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . of the 38th Annual Meeting of the Association for Computational Lingustics , ' Online !", "label": "", "metadata": {}}
{"text": "440 - 447 , XP002279144Hong Kong , China Retrieved from the Internet : retrieved on May 6 , 2004 ! abstract .Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .Annual Meeting of the ACL Assoc . for Computational Linguistics , Morristown , NJ , 80 - 87 .Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}}
{"text": "Marcu , Daniel , \" Building Up Rhetorical Structure Trees , \" 1996 , Proc . of the National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference , vol .2 , pp .1069 - 1074 .Och , F. , \" Minimum Error Rate Training in Statistical Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .Annual Meeting of the ACL .Assoc . for Computational Linguistics , Morristown , NJ , 160 - 167 .", "label": "", "metadata": {}}
{"text": "Human Language Technology Conference .Assoc . for Computational Linguistics , Morristown , NJ .Guide to the Lester Wunderman Papers , 1946 - 2010 and undated .Lester Wunderman is an advertising executive primarily in the direct marketing industry , founder of an agency in his name now a subsidiary of Young & Rubicam , and a noted collector of Dogon ( Mali ) art works .Included are drafts , proofs and correspondence relating to Wunderman 's 1996 book Being Direct : Making Advertising Pay .Advertising agencies represented in the collection include Caspar Pinsker , Maxwell Sackheim , Wunderman Cato Johnson , Wunderman Ricotta & Kline and Young & Rubicam .", "label": "", "metadata": {}}
{"text": "Languages present include Spanish , French , Danish , German and Japanese , and have not been translated into English .Acquired as part of the John W. Hartman Center for Sales , Advertising & Marketing History .Included are drafts , proofs and correspondence relating to Wunderman 's 1996 book Being Direct : Making Advertising Pay .Advertising agencies represented in the collection include Caspar Pinsker , Maxwell Sackheim , Wunderman Cato Johnson , Wunderman Ricotta & Kline and Young & Rubicam .Also included are correspondence , photographs , negatives and other materials relating to Wunderman 's collection of Dogon ( Mali ) art works , carvings and sculptures , and their use in museum exhibits , catalogs and books on African art .", "label": "", "metadata": {}}
{"text": "The collection has been arranged into six series : Administrative Files ; Speeches ; Writings ; Memorabilia ; Oversize Materials ; and Audiovisual Materials .Administrative Files include biographical materials as well as materials relating to awards , travel , Wunderman 's activities as a consultant and member of the Boards of Directors and Trustees of various institutions , publicity notices and samples of direct marketing items .Speeches include typed and hand - written drafts of speeches given by Wunderman , along with alternate versions and correspondence relating to speaking engagements .Writings includes drafts , proofs and published articles , notes and correspondence , as well as a Book File related to Wunderman 's book Being Direct : Making Advertising Pay .", "label": "", "metadata": {}}
{"text": "Oversize Materials comprise large - format items removed from previous series .Their logical location is noted in the Detailed Description with entries in square brackets , cross referenced to their physical location in this Series .Audiovisual Materials include audiocassettes , videocassettes and films .Materials not in English have not been translated .Where possible , original folder titles have been retained .Unpublished materials are closed to non - Wunderman researchers for 15 years from the date of creation .Original audiovisual materials are closed to patron use .Use of these materials may require production of listening or viewing copies .", "label": "", "metadata": {}}
{"text": "Researchers must register and agree to copyright and privacy laws before using this collection .All or portions of this collection may be housed off - site in Duke University 's Library Service Center .The library may require up to 48 hours to retrieve these materials for research use .Please contact Research Services staff before visiting the David M. Rubenstein Rare Book & Manuscript Library to use this collection .Use Restrictions .The copyright interests in this collection have not been transferred to Duke University .For more information , consult the copyright section of the Regulations and Procedures of the David M. Rubenstein Rare Book & Manuscript Library .", "label": "", "metadata": {}}
{"text": "Where possible , original folder titles have been retained .Materials in other languages have not been translated into English .Large - format items have been removed to Oversize Materials and their logical location noted in the Description by a note or an entry enclosed in square brackets .Conditions Governing Access note .Restrictions on Access : Unpublished materials are closed to non - Wunderman researchers for 15 years from the date of creation .[ Includes copy of Green , Janice S. et al .1985 .Opportunity in Adversity : How Colleges Can Succeed in Hard Times .", "label": "", "metadata": {}}
{"text": "Inscribed to Wunderman from Brooklyn College President Robert Hess . ]Includes published articles and books , correspondence , drafts and proofs .Organized into two series : Articles and Book File .Articles includes materials relating to periodicals publications ; Book File includes materials relating to Wunderman 's book Being Direct : Making Advertising Pay .Where possible , original folder titles were retained .Arranged alphabetically by folder title .Conditions Governing Access note .Restrictions on Access : Unpublished materials are closed to non - Wunderman researchers for 15 years from the date of creation .Chronology List .", "label": "", "metadata": {}}
{"text": "Speech to Hundred Million Club ( precursor to Direct Marketing Club of New York ) ; first documented use of the term \" direct marketing \" in a speech by Wunderman .Mail Order Man of the Year , Advertising Club of New York MIT speech , elaboration of the \" direct marketing \" concept before the American Marketing Association ; speech later read into the Congressional Record .WRK merged with Young & Rubicam .Travels to Africa ; photography of Dogon life and sculpture .Chairman , WRK .Frontiers of Direct Marketing published .Direct Marketer of the Year , Direct Marketing Day in New York .", "label": "", "metadata": {}}
{"text": "Chairman , Visiting Committee of the Arts of Africa , Oceania and the Americas , Metropolitan Museum of Art .Direct Marketing Association Hall of Fame .Honorary Doctor of Humane Letters , Brooklyn College , City University of New York .Focus Award , Montreux Symposium of Direct Marketing .Circa 1986 - 1992 .Trustee , Children 's Television Workshop .WRK name change to Wunderman Worldwide .Special Exhibition of Wunderman 's Dogon art collection , Metropolitan Museum of Art , N.Y. , and the Louvre , Paris .Wunderman Worldwide merger with Cato Johnson to form Wunderman Cato Johnson .", "label": "", "metadata": {}}
{"text": "Being Direct : Making Advertising Pay published Wunderman LLC consulting firm .Lifetime Achievement Award , Direct Marketing Day in New York .Advertising Hall of Fame .Retired from Wunderman Cato Johnson .Wunderman Cato Johnson renamed Wunderman , NY ; Wunderman named Chairman Emeritus .Being Direct : Making Advertising Pay , second edition , published .Chairman Emeritus , Visiting Committee , Rockefeller Wing at The Metropolitan Museum of Art , N.Y. .In addition , Wunderman has been involved in a range of professional and personal activities : .Attended New York University , City University of New York , Brooklyn College , Columbia University .", "label": "", "metadata": {}}
{"text": "Co- Founder ( with Cornell Capa and Jacqueline Kennedy ) , International Center of Photography .Humanitarian work : Administrative Council , UNESCO International Fund for the Promotion of Culture ; Co - Chairman , Friends of Leopold Senghor Foundation ; President , Wunderman Foundation , which funds research in developing regions .Member , UNICEF , Paris .Can the living become machines ?The voltaic cell has evolved greatly over time .In the year 1780 , the Italian physicist Luigi Galvani found that when you connected copper and zinc to the ends of a frog leg nerve , the leg contracts .", "label": "", "metadata": {}}
{"text": "The theory concluded that in the frog 's legs was an electrical nerve liquid that reacted to a completed electrical circuit and caused the muscles of a dead frog to contract .Wanting to challenge Galvani 's animal electricity theory , Alessandro Volta built on Galvani 's ideas and created the voltaic pile using only non - biological materals , thus reinforcing his metal - metal contact electricity theory .Like Galvani , he had two metals to create his circuit .However , he added a brined cloth into his invention .Volta alternately stacked zinc and copper discs and put cloths soaked in brine in between the copper - zinc pairs .", "label": "", "metadata": {}}
{"text": "But possibly predating both these inventions is the Baghdad battery , a group of artifacts created in Mesopotamia , during the dynasties of Parthian or Sassanid or Persian Empire period ( the early centuries AD ) .They were discovered near Baghdad , hence the name Baghdad batter .Wilhelm K\u00f6nig , the German director of the National Museum of Iraq , believes that the ancients may have used them as battery cells to electroplate gold onto silver objects .Willard Gray recreated the battery by inferring that the copper and iron form an electrochemical couple and create voltage in the presence of an electrolyte .", "label": "", "metadata": {}}
{"text": "Creative Applications .The Baghdad battery in theory is able to hold several volts .However , gas bubbles partially insulate the electrode and increases resistance .Why do the gas bubbles limit the possible voltage of the Baghdad battery ?What differentiates the voltaic cell from Galvani 's first frog leg battery ?What differentiates the voltaic cell from the voltaic pile ?What differentiates the voltaic cell from the Baghdad battery ?Do you think that Galvani 's animal electricity theory is legitimate ?Is the metal - metal contact electricity theory legitimate ?CK-12 Overview .To use this website , please enable javascript in your browser .", "label": "", "metadata": {}}
{"text": "Oops , looks like cookies are disabled on your browser .Click here to see how to enable them .Original text .Type : Activity Attachment Assessment Audio Classwork Critical Thinking Handout Homework Image Interactive Object Lab Lesson Plan Notes Presentation Project Reading Rubric Starter / Do now Study Guide Syllabus Test / Quiz Video Web Worksheet Published Translation of Checklists from English to German .Details .We operate a website with all sorts of checklists and \" listicles \" .Our main languages are English and Dutch .We would like to experiment with some checklists and lists of content in German as well .", "label": "", "metadata": {}}
{"text": "The term checkpoints refers to the number of points in the list .We only need you to translate the text , it 's fine to deliver the translations in a Word document or similar .We prefer a friendly tone of voice ( \" Du \" , not \" Sie \" ) .I speak German fluently ( but I 'm less comfortable writing German ) , so we are able to check for quality .If the quality is sufficient , we wil consider having more lists translated by you .We do NOT want an automated translation .", "label": "", "metadata": {}}
{"text": "The Original Design ( Add image of the full OpenPCR machine here , from the Week 9 exercise .Write a paragraph description for visitors who have no idea what this is )The PCR Machine or otherwise known as a thermocycler or a DNA Amplifyer is use by scientists to create a vast quanity of a specific sequence of DNA .This method is also used widely for biological and medical applicaions .This process relies on thermal cycles to amplify the DNA to then be able and replicated it .Test Run .( Write the date you first tested Open PCR and your experience(s ) with the machine ) .", "label": "", "metadata": {}}
{"text": "Our job as for Experiment Protocol Planner was to : .Prepare the software for analysis .Prepare DNA samples for tests .Analyze the results .This software also gave us real time update of the PCR machine Second and most important part of to prepare our sample .Our sample consisted of two rows of four samples .The table representing the samples is shown below .In this part we have also created a total of 8 different dyes also shown below .The third part was to take the results from the PCR machine and analyze them with the rest of our team , via software .", "label": "", "metadata": {}}
{"text": "PCR experiment summery PCR machine goal is to take a long code of DNA and to amplify or separate a known part of that DNA .This target DNA is to be replicated with the help of PCR machine .We know which part of the DNA needs to be amplified because we know the sequence of bases of that particular part .We use the master mix which already contains the known sequence and will replicate only that known sequence leaving the the rest of the DNA .The PCR machine allows the master sample to perform its job by performing cycles of cooling and heating .", "label": "", "metadata": {}}
{"text": "When the sample is heating the DNA opens up , the heat rises up to 95 degree Celsius .Then for primers to be allowed to do their job the sample is cooled down to 57 degree Celsius .Next for the primer to add appropriate amine acids in order to build a new DNA strand the sample is heated up to 72 degree Celsius .This sample is repeated multiple times allowing more and more replicates of the targeted DNA to be made , making the amount of the unwanted DNA to be minimal compared to the amount of wanted DNA .", "label": "", "metadata": {}}
{"text": "Control N / A \" - \" .Patient 1 1 \" 1 \" .Patient 2 1 \" 2 \" .Row 2 .Contains : rep : Label .Patient 1 2 \" + - \" .Patient 2 2 \" -- \" .Patient 1 3 \" 1- \" .Patient 2 3 \" 2- \" . \"Contains \" describes what sample was inserted in the test tube \" rep \" is the replicate , we have 3 replicates of each patient \" label \" is simply what symbol we have written on the test tube so that we can identify what sample is where .", "label": "", "metadata": {}}
{"text": "Step 4 : allow other team members to start PCR machine and then load the samples into the machine 5 .Step 5 : Enter following commands to the Thermal Cycler Program so that the PCR machine knows what and how many cycle s to perform .The goal of this experiment is to understand the DNA amplification process in order to detect cancerous genes when given Template DNA ; otherwise known as DNA taken from the patient .DNA amplification will allow the cancerous genes to replicate .The cancerous gene will produce a positive result , while the non - cancer gene will give a negative result , because the primers are designed to amplify cancerous DNA .", "label": "", "metadata": {}}
{"text": "The following \" 3 steps \" are repeated for 30 cycles , each step taking about 30 seconds .Step 1 : Heat up the Template DNA , Primers , Taq Polymerase , and Magnesium Chloride ( MgCl2 ) to 95 degrees Celsius .o The bases need to me detected , in order to do so \" melting \" or unzipping needs to DNA will expose those base pairs .o When the base pairs are exposed the primers will create a forward and reverse strand appropriately .The primers are short pieces ( about 20 bases pairs long ) of DNA that are synthesized DNA that binds to the Template DNA while directing the new strand to be made .", "label": "", "metadata": {}}
{"text": "o The primers \" anneal , \" or in other words the bond .Step 3 : Heat the mixture back up to 72 degrees Celsius .o The Taq Polymerase and Magnesium Chloride will detect where the template DNA and primer and uses the template DNA strand and primer to build a new copy of DNA .( BONUS points : Use a program like Powerpoint , Word , Illustrator , Microsoft Paint , etc . to illustrate how primers bind to the cancer DNA template , and how Taq polymerases amplify the DNA .Screen - captures from the OpenPCR tutorial might be useful .", "label": "", "metadata": {}}
