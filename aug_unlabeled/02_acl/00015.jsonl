{"text": "CC License .Ancient Hebrew Syntax : Making a Searchable Database .November 24 , 2010 - robertholmstedt .There are two basic options for clause structure : a flat clause structure and a hierarchical clause structure .The flat clause structure is based on a finite state model ( the ' Markov Model ' ) in which it is argued that a clause is constructed word - by - word in a linear fashion ; clauses in this model are also called ' word chains ' .An example of a flat - structure clause is given here : .", "label": "", "metadata": {}}
{"text": "For example , in the first two examples below , the subject and verb are adjacent and so the subject - verb agreement is immediate , or ' local ' ; in the third example , though , the agreement is non - local or long distant .In contrast to the flat structure , the hierarchical approach to clause structure is not primarily linear but , as its name signals , hierarchical .The syntactic elements relate to each other in terms of how they ' cluster ' together .For example , in the clause \" she hit her sister with the teddy bear , \" we might suggest that ' she ' and ' hit ' relate to each other non - hierarchically , as the two basic halves of the clause .", "label": "", "metadata": {}}
{"text": "In this example , the element ' in the nursery ' is hierarchically dominated by ' the babies ' .This allows the plural ' the babies ' to be hierarchically adjacent to the plural verb ' cry ' , thus providing an explanation for how the subject and verb may agree even though they are separated by other words .The process of formation is from the bottom - up , that is , as each lexical item is introduced into the ' clause - in - the - making ' ( called a ' derivation ' ) , the lexical items merge with each other and project a larger structure , a phrase .", "label": "", "metadata": {}}
{"text": "Thus , a prepositional phrase is the projection of the hierarchy around a preposition , a noun phrase is the projection of a noun , a verb phrase the projection of a verb , etc .The highest level constituent is a .A clause is a single constituent consisting of a subject phrase and a verb phrase .Main clauses ( or ' independent ' ) are self - contained and thus do not function within a larger syntactic hierarchy , while subordinate ( or ' dependent ' ) clauses are contained within a phrase , typically a verb phrase in a higher clause .", "label": "", "metadata": {}}
{"text": "The point of this discussion of hierarchical clause structure has been to establish that we designed our database on a well - known linguistic theory of phrase structure , in which it is argued that constituents are contained within larger constituents , all the way up to the clause level .For each word , we and our tagging team have had to make a decision regarding the word 's location in the syntactic hierarchy - within what other constituent does it reside ?And for that resulting complex constituent , the same question must be answered , until there are no more constituents and one is left with a clause .", "label": "", "metadata": {}}
{"text": "Thus , at a basic level the hierarchy that we have followed is binary in nature .Binary - branching is a basic principle to the minimalist program of Chomskyan generative linguistics , as well as many other generative frameworks .But the addition of clause - edge constituents , such as dislocations ( casus pendens ) , vocatives , and exclamatives results in a tree that is not easy to fit into a binary structure and to do so requires a good deal of theory - internal arguments .Thus , we made the decision to depart from a basic principle of this particular theory in favor of presenting hierarchical data in a manner that is not so theory dependent , even at the risk of analytical error ( see here ) .", "label": "", "metadata": {}}
{"text": "Constituents ' .The syntactic elements at each stage of derivation are referred to as constituents .A constituent is a single syntactic unit that has a place within the hierarchy of a larger syntactic unit .It is important to recognize that morphological words and constituents may overlap but are not always identical .That is , a single word may represent more than one syntactic constituent , such as English teacher 's , in which the constituent teacher has a syntactic role that is distinct from the syntactic role of the possessive s .This is true in Hebrew , too ; moreover , the converse is also true : occasionally multiple words represent a single syntactic constituent .", "label": "", "metadata": {}}
{"text": "Therefore , syntacticians often use a different set of labels for the various constituents in a clause .\" Where 's the Direct Object ? \"No doubt some of you are looking through the short list of syntactic roles above and asking yourselves , \" Where is the direct object ?And what about the indirect object ? \" The answer is that they are not syntactic relationships that are explicitly tagged in our database .Why ?The answer to that is more complex , but here is the beginning of an explanation .The complement essentially corresponds to ' object ' , of which there are a number of sub - types .", "label": "", "metadata": {}}
{"text": "In contrast , the indirect object is limited to a small set of verbs that require a ' recipient ' ( or ' benificiary ' ) of the verbal action or process to be specified .There are two basic problems with encoding the concepts of direct and indirect object in a syntactic database , especially one for Hebrew .First , these concepts are not exclusively syntactic in nature ; one must necessarily interact with argument structure ( or thematic role ) information concerning the predication , information that is explicitly outside the scope of our syntactic database ( more on this a ways below ) .", "label": "", "metadata": {}}
{"text": "Constituent movement is a hallmark of transformational generative grammar , although it has been dismissed by much non - Chomskyan generative theory ( i.e. , ' monostratal ' theories ) .The basic idea is that the linear order of constituents in many actual clauses can not reflect the ' original ' order of those constituents .Neither defending nor criticizing this proposal , we determined that representing it in our database was not desirable or necessary .Yet , we were forced to deal with discontinuous constituents , that is , constituents that are divided into parts separated by un - related constituents .", "label": "", "metadata": {}}
{"text": "The challenge of constituent discontinuity is that , based on the hierarchy and the projection principle that a phrase contains all its complements and/or adjuncts , a verb and its modifiers together make up a single constituent .But how , then , can this be represented when they are broken by non - related intervening constituents , such as a subject ?We have used this cross - referencing system to allow us to represent more accurately three additiona phenomena : dislocation ( casus pendens ) , resumption in relative clauses , and ellipsis ( or ' gapping ' ) .", "label": "", "metadata": {}}
{"text": "A final defining principle of the Accordance syntax database that I 'll mention here is a narrow focus on syntax .That is , the tagging scheme provides phrasal , clausal , and inter - clausal information to the exclusion of semantic judgments , discourse relationships , and implicational pragmatics .For example , when the particle \u05db\u05d9 is a subordinator , we make no distinction between its use as a temporal ( ' when ' ) subordinator or a clausal ( ' because ' ) subordinator .Those distinctions are left to the user to determine .What we provide is the distinction between \u05db\u05d9 as an adjunct subordinator ( temporal or causal ) , a complement subordinator ( ' that ' ) , a conjunction ( ' but ' ) , and an exclamative ( ' indeed ! ' )", "label": "", "metadata": {}}
{"text": "The term valency derives from chemistry and has been employed in linguistics for about a half - century .Verbal valency , in particular , refers to the property of a verb that determines the syntactic environments in which it may appear .For example , in the examples below the English verb ' snored ' requires a subject , ' help ' requires both a subject and an NP complement and ' returned ' requires a subject and prepositional ( locative ) complement : .She snored .He helped the boy .They returned to the house .", "label": "", "metadata": {}}
{"text": "For the database project , it was necessary that we use valency information to determine whether the non - subject constituents associated with a given verb were complements or adjuncts .And yet , we do not identify these complements or adjuncts by any semantic categories , such as locative , temporal , means , manner , etc .Moreover , we do not include any discourse - pragmatic judgments , such as whether a complement preceding a verb has a Topic or Focus function .But let me be absolutely clear : this decision on the narrow focus of our database was made for two practical reasons : .", "label": "", "metadata": {}}
{"text": "Second , the additional semantic and pragmatic layers would add a disproportionate number of years to the project .Whereas we are confident that we will finish all our ancient Hebrew texts in the next 2 - 3 years , it would likely take a decade ( or more ) to produce a multi - layered database .From the project 's perspective , we take an agnostic stance with regard to this debate .In future posts , I will begin describing how to use the syntax database within Accordance , a sort of user 's manual - in - the - making .", "label": "", "metadata": {}}
{"text": "Do the blog authors have evidence to support this claim that goes beyond personal preference and anecdote ?They certainly do n't give any . )Bloggers who want to criticize scholars ' views in public should find a way to do so that allows them to identify themselves fully .This is a positive step towards exhibiting respect in scholarly discourse and building bridges rather than the opposite .The former should typify scholarly exchange ( sadly , it does not always ) , the latter has no place in what we do .I have no issues whatsoever with young scholars subscribing to linguistic theories other than the ones I prefer - but such differences should not be an obstacle to polite , collegial relationships , even productive friendships .", "label": "", "metadata": {}}
{"text": "I have edited my endnote out of respect for the aforementioned bloggers ; my responses to them in this exchange have often been rather direct , but less about the substance of the criticism than the way it was carried out .We all learn with each experience ; I hope he / she / they have gained something , and I hope I respond to the next such episode with a bit more patience , rather than embracing my inclination towards crustiness .Parse tree .A parse tree or parsing tree [ 1 ] or derivation tree or ( concrete ) syntax tree is an ordered , rooted tree that represents the syntactic structure of a string according to some context - free grammar .", "label": "", "metadata": {}}
{"text": "Parse trees are distinct from the abstract syntax trees used in computer programming , in that their structure and elements more concretely reflect the syntax of the input language .They are also distinct from ( although based on similar principles to ) the sentence diagrams ( such as Reed - Kellogg diagrams ) sometimes used for grammar teaching in schools .A related concept is that of phrase marker or P - marker , as used in transformational generative grammar .A phrase marker is a linguistic expression marked as to its phrase structure .This may be presented in the form of a tree , or as a bracketed expression .", "label": "", "metadata": {}}
{"text": "Contents .The interior nodes are labeled by non - terminal categories of the grammar , while the leaf nodes are labeled by terminal categories .The image below represents a constituency - based parse tree ; it shows the syntactic structure of the English sentence John hit the ball : .Each node in the tree is either a root node , a branch node , or a leaf node .[ 2 ] A root node is a node that does n't have any branches on top of it .Within a sentence , there is only ever one root node .", "label": "", "metadata": {}}
{"text": "A leaf node , however , is a terminal node that does not dominate other nodes in the tree .S is the root node , NP and VP are branch nodes , and John ( N ) , hit ( V ) , the ( D ) , and ball ( N ) are all leaf nodes .The leaves are the lexical tokens of the sentence .[ 3 ] A node can also be referred to as parent node or a child node .A parent node is one that has at least one other node linked by a branch under it .", "label": "", "metadata": {}}
{"text": "A child node is one that has at least one node directly above it to which it is linked by a branch of a tree .From the example , hit is a child node of V. The terms mother and daughter are also sometimes used for this relationship .The dependency - based parse trees of dependency grammars [ 4 ] see all nodes as terminal , which means they do not acknowledge the distinction between terminal and non - terminal categories .They are simpler on average than constituency - based parse trees because they contain fewer nodes .", "label": "", "metadata": {}}
{"text": "This parse tree lacks the phrasal categories ( S , VP , and NP ) seen in the constituency - based counterpart above .Like the constituency - based tree , constituent structure is acknowledged .Any complete sub - tree of the tree is a constituent .Thus this dependency - based parse tree acknowledges the subject noun John and the object noun phrase the ball as constituents just like the constituency - based parse tree does .The constituency vs. dependency distinction is far - reaching .Whether the additional syntactic structure associated with constituency - based parse trees is necessary or beneficial is a matter of debate .", "label": "", "metadata": {}}
{"text": "A phrase marker representing the deep structure of a sentence is generated by applying phrase structure rules ; this may then be undergo further transformations .Phrase markers may be presented in the form of trees ( as in the above section on constituency - based parse trees ) , but are often given instead in the form of bracketed expressions , which occupy less space .For example , a bracketed expression corresponding to the constituency - based tree given above may be something like : .\u00c1gel , V. , Ludwig Eichinger , Hans - Werner Eroms , Peter Hellwig , Hans Heringer , and Hennig Lobin ( eds . )", "label": "", "metadata": {}}
{"text": "Berlin : Walter de Gruyter .Carnie , A. 2013 .Syntax : A generative introduction , 3rd edition .Malden , MA : Wiley - Blackwell .Chiswell , Ian and Wilfrid Hodges 2007 .Mathematical logic .Oxford : Oxford University Press .Context Effects in Language Production : Models of Syntactic Priming in Dialogue Corpora .Download .Date .Author .Metadata .Abstract .This thesis addresses the cognitive basis of syntactic adaptation , which biases speakers to repeat their own syntactic constructions and those of their conversational partners .I address two types of syntactic adaptation : short - term priming and longterm adaptation .", "label": "", "metadata": {}}
{"text": "Both methods estimate adaptation in large datasets consisting of transcribed human - human dialogue annotated with syntactic information .Two such corpora in English are used : Switchboard , a collection of spontaneous phone conversation , and HCRC Map Task , a set of task - oriented dialogues in which participants describe routes on a map to one another .I find both priming and long - term adaptation in both corpora , confirming well - known experimental results ( e.g. , Bock , 1986b ) .I extend prior work by showing that syntactic priming effects not only apply to selected syntactic constructions that are alternative realizations of the same semantics , but still hold when a broad variety of syntactic phrase structure rules are considered .", "label": "", "metadata": {}}
{"text": "I show that the priming effect for a rule is inversely proportional to its frequency .With this methodology , I test predictions of the Interactive Alignment Model ( IAM , Pickering and Garrod , 2004 ) .The IAM claims that linguistic and situation model agreement between interlocutors in dialogue is the result of a cascade of resource - free , mechanistic priming effects on various linguistic levels .I examine task - oriented dialogue in Map Task , which provides a measure of task success through the deviance of the communicated routes on the maps .I find that long term syntactic adaptation predicts communicative success , and it does so earlier than lexical adaptation .", "label": "", "metadata": {}}
{"text": "Short - term syntactic priming differs qualitatively from long term adaptation , as it does not predict task success , providing evidence against learning as a single cognitive basis of adaptation effects .I obtain further evidence for the correlation between semantic activity and syntactic priming through a comparison of the Map Task and Switchboard corpora , showing that short - term priming is stronger in task - oriented dialogue than in spontaneous conversation .This difference is evident for priming between and within speakers , which suggests that priming is a mechanistic rather than strategic effect .I turn to an investigation of the level at which syntactic priming influences language production .", "label": "", "metadata": {}}
{"text": "To do so , I identify pairs of part - of - speech categories which consistently cross constituent boundaries defined by the phrase structure analyses of the sentences .I show that such distituents are less sensitive to priming than pairs occurring within constituents .Thus , syntactic priming is sensitive to syntactic structure .The notion of constituent structure differs among syntactic models .Combinatory Categorial Grammar ( CCG , Steedman , 2000 ) formalizes flexible constituent structure , accounting a varying degree of incrementality in syntactic sentence planning .I examine whether priming effects can support the predictions of CCG using the Switchboard corpus , which has been annotated with CCG syntax .", "label": "", "metadata": {}}
{"text": "I then show that both incremental and normal - form constituent structures exhibit priming , arguing for language production accounts that support flexible incrementality .The empirical results are reflected in a cognitive model of syntactic realization in language production .The model assumes that language production is subject to the same principles and constraints as any other form of cognition and follows the ACT - R framework ( Anderson et al ., 2004 ) .Its syntactic process implements my empirical results on priming and is based on CCG .Syntactic planning can take place incrementally and non - incrementally .", "label": "", "metadata": {}}
{"text": "Syntactic adaptation emerges due to a preferential and sped - up memory retrieval of syntactic categories describing linearization and subcategorization requirements .Long - term adaptation is explained as a form of learning , while shortterm priming is the result of a combination of learning and spreading activation from semantic and lexical material .Simulations show that the model produces the adaptation effects and their inverse frequency interaction , as well as cumulativity of long - term adaptation .A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .", "label": "", "metadata": {}}
{"text": "Abstract .A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .The unified language model includes a plurality of context - free grammars having non - terminals and a hybrid N - gram model having at least some of the same non - terminals embedded therein .instructions stored on the memory that are executable by the processor which , when implemented , execute a method to build a task dependent unified language model , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts , each of the context - free grammars having words present in a task independent corpus to form the semantic or syntactic concepts ; . parsing the task independent corpus with the plurality of context - free grammars to identify word occurrences of each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; . building a N - gram model having the non - terminal tokens ; and .", "label": "", "metadata": {}}
{"text": "The system of .claim 1 wherein the plurality of context - free grammars include at least one context - free grammar having a non - terminal token for a phrase that can be mistaken for one of the desired task dependent semantic or syntactic concepts .The system of . claim 2 wherein replacing each of the identified word occurrences with corresponding non - terminal tokens includes excluding the non - terminals added for the prevention of mistakes during parsing .The system of . claim 2 having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the set of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .", "label": "", "metadata": {}}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a language model , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . generating word phases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phases ; . querying a task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and .", "label": "", "metadata": {}}
{"text": "The system of .claim 6 wherein the language model is an N - gram language model .The system of .claim 7 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and . wherein building the N - gram language model comprises building a N - gram model having the non - terminal tokens .", "label": "", "metadata": {}}
{"text": "claim 7 and having instructions further comprising : . building a second N - gram language model from the word phases from the plurality of context - free grammars ; and .combining the first - mentioned N - gram language model and the second N - gram language model to form a third N - gram language model .The system of .claim 9 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and .", "label": "", "metadata": {}}
{"text": "The system of . claim 8 and having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .The system of .claim 6 and having instructions further comprising : . storing the identified text of the task independent corpus separate from the task independent corpus .The system of .claim 9 wherein building the second N - gram language model includes using only the identified text .", "label": "", "metadata": {}}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a unified language model for a selected application , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . building a word language model from a corpus ; and .The system of .claim 15 wherein the word language model comprises an N - gram language model and wherein the corpus comprises a task independent corpus .", "label": "", "metadata": {}}
{"text": "claim 16 and having instructions further comprising : . generating word phrases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phrases ; . querying the task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and . wherein building a N - gram language model includes using the identified text .Description .CROSS - REFERENCE TO RELATED APPLICATION .The present application is a continuation of and claims priority of U.S. patent application Ser .", "label": "", "metadata": {}}
{"text": "09/585,298 , filed Jun. 1 , 2000 , the content of which is hereby incorporated by reference in its entirety .BACKGROUND OF THE INVENTION .The present invention relates to language modeling .More particularly , the present invention relates to creating a language model for a language processing system .Accurate speech recognition requires more than just an acoustic model to select the correct word spoken by the user .In other words , if a speech recognizer must choose or determine which word has been spoken , if all words have the same likelihood of being spoken , the speech recognizer will typically perform unsatisfactorily .", "label": "", "metadata": {}}
{"text": "Speech recognition is often considered to be a form of top - down language processing .Two common forms of language processing includes \" top - down \" and \" bottom - up \" .Top - down language processing begins with the largest unit of language to be recognized , such as a sentence , and processes it by classifying it into smaller units , such as phrases , which in turn , are classified into yet smaller units , such as words .In contrast , bottom - up language processing begins with words and builds therefrom , larger phrases and/or sentences .", "label": "", "metadata": {}}
{"text": "One common technique of classifying is to use a formal grammar .The formal grammar defines the sequence of words that the application will allow .One particular type of grammar is known as a \" context - free grammar \" ( CFG ) , which allows a language to be specified based on language structure or semantically .The CFG is not only powerful enough to describe most of the structure in spoken language , but also restrictive enough to have efficient parsers .Nevertheless , while the CFG provides us with a deeper structure , it is still inappropriate for robust spoken language processing since the grammar is almost always incomplete .", "label": "", "metadata": {}}
{"text": "The advantage of a CFG 's structured analysis is thus nullified by the poor coverage in most real applications .For application developers , a CFG is also often highly labor - intensive to create .A second form of a language model is an N - gram model .Because the N - gram can be trained with a large amount of data , the n - word dependency can often accommodate both syntactic and semantic shallow structure seamlessly .However , a prerequisite of this approach is that we must have a sufficient amount of training data .", "label": "", "metadata": {}}
{"text": "Since a word - based N - gram model is limited to n - word dependency , it can not include longer - distance constraints in the language whereas CFG can .A unified language model ( comprising a combination of an N - gram and a CFG ) has also been advanced .The unified language model has the potential of overcoming the weaknesses of both the word N - gram & CFG language models .However , there is no clear way to leverage domain - independent training corpus or domain - independent language models , including the unified language models , for domain specific applications .", "label": "", "metadata": {}}
{"text": "As technology advances and speech and handwriting recognition is provided in more applications , the application developer must be provided with an efficient method in which an appropriate language model can be created for the selected application .SUMMARY OF THE INVENTION .A method for creating a language model from a task - independent corpus is provided .In a first aspect , a task dependent unified language model for a selected application is created from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .", "label": "", "metadata": {}}
{"text": "Each of the context - free grammars include words or terminals present in the task - independent corpus to form the semantic or syntactic concepts .The task - independent corpus with the plurality of context - free grammars is parsed to identify word occurrences of each of the semantic or syntactic concepts and phrases .Each of the identified word occurrences are replaced with corresponding non - terminal tokens .A N - gram model is built having the non - terminal tokens .A second plurality of context - free grammars is obtained for at least some of the same non - terminals representing the same semantic or syntactic concepts .", "label": "", "metadata": {}}
{"text": "A second aspect is a method for creating a task dependent unified language model for a selected application from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .The task - independent corpus with the plurality of context - free grammars is parsed to identify word occurrences for each of the semantic or syntactic concepts and phrases .Each of the identified word occurrences is replaced with corresponding non - terminal tokens .A N - gram model is then built having the non - terminal tokens .", "label": "", "metadata": {}}
{"text": "The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .Word phrases are generated from the plurality of context - free grammars .The context - free grammars are used for formulating an information retrieval query from at least one of the word phrases .The task - independent corpus is queried based on the query formulated and text in the task - independent corpus is identified based on the query .A language model is built using the identified text .A fourth aspect is a method for creating a language model for a selected application from a task - independent corpus .", "label": "", "metadata": {}}
{"text": "Word phrases are generated from the plurality of context - free grammars .First and second N - gram language models are built from the word phrases and the task - independent corpus , respectively .The first N - gram language model and the second N - gram language model are combined to form a third N - gram language model .A fifth aspect is a method for creating a unified language model for a selected application from a corpus .The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .", "label": "", "metadata": {}}
{"text": "Probabilities of terminals of at least some of the context - free grammars are normalized and assigned as a function of corresponding probabilities obtained for the same terminals from the word language model .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a language processing system .FIG .2 is a block diagram of an exemplary computing environment .FIG .3 is a block diagram of an exemplary speech recognition system .FIG .4 is a pictorial representation of a unified language model .FIGS .5 - 8 are flow charts for different aspects of the present invention .", "label": "", "metadata": {}}
{"text": "9 is a block diagram of another aspect of the present invention .DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS .FIG .1 generally illustrates a language processing system 10 that receives a language input 12 and processes the language input 12 to provide a language output 14 .For example , the language processing system 10 can be embodied as a speech recognition system or module that receives as the language input 12 spoken or recorded language by a user .The language processing system 10 processes the spoken language and provides as an output , recognized words typically in the form of a textual output .", "label": "", "metadata": {}}
{"text": "The language model 16 encodes a particular language , such as English .In the embodiment illustrated , the language model 16 can be an N - gram language model or a unified language model comprising a context - free grammar specifying semantic or syntactic concepts with non - terminals and a hybrid N - gram model having non - terminals embedded therein .One broad aspect of the present invention is a method of creating or building the language model 16 from a task - independent corpus , several of which are readily available , rather than from a task - dependent corpus , which is often difficult to obtain .", "label": "", "metadata": {}}
{"text": "For instance , language models of the type described above can be used in handwriting recognition , Optical Character Recognition ( OCR ) , spell - checkers , language translation , input of Chinese or Japanese characters using standard PC keyboard , or input of English words using a telephone keypad .Although described below with particular reference to a speech recognition system , it is to be understood that the present invention is useful in building artificial and natural language models in these and other forms of language processing systems .Prior to a detailed discussion of the present invention , an overview of an operating environment may be helpful .", "label": "", "metadata": {}}
{"text": "2 and the related discussion provide a brief , general description of a suitable computing environment in which the invention can be implemented .Although not required , the invention will be described , at least in part , in the general context of computer - executable instructions , such as program modules , being executed by a personal computer .Generally , program modules include routine programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .Tasks performed by the programs and modules are described below and with the aid of block diagrams and flow charts .", "label": "", "metadata": {}}
{"text": "In addition , those skilled in the art will appreciate that the invention can be practiced with other computer system configurations , including hand - held devices , multiprocessor systems , microprocessor - based or programmable consumer electronics , network PCs , minicomputers , mainframe computers , and the like .The invention can also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules can be located in both local and remote memory storage devices .With reference to .", "label": "", "metadata": {}}
{"text": "The system memory includes read only memory ( ROM ) 54 and a random access memory ( RAM ) 55 .A basic input / output system 56 ( BIOS ) , containing the basic routine that helps to transfer information between elements within the personal computer 50 , such as during start - up , is stored in ROM 54 .The hard disk drive 57 , magnetic disk drive 58 , and optical disk drive 60 are connected to the system bus 53 by a hard disk drive interface 62 , magnetic disk drive interface 63 , and an optical drive interface 64 , respectively .", "label": "", "metadata": {}}
{"text": "A number of program modules can be stored on the hard disk , magnetic disk 59 , optical disk 61 , ROM 54 or RAM 55 , including an operating system 65 , one or more application programs 66 , other program modules 67 , and program data 68 .A user can enter commands and information into the personal computer 50 through input devices such as a keyboard 70 , a handwriting tablet 71 , a pointing device 72 and a microphone 92 .Other input devices ( not shown ) can include a joystick , game pad , satellite dish , scanner , or the like .", "label": "", "metadata": {}}
{"text": "In addition to the monitor 77 , personal computers typically include other peripheral output devices such as a speaker 83 and a printer ( not shown ) .The personal computer 50 can operate in a networked environment using logic connections to one or more remote computers , such as a remote computer 79 .FIG .2 .The logic connections depicted in .FIG .2 include a local area network ( LAN ) 81 and a wide area network ( WAN ) 82 .Such networking environments are commonplace in offices , enterprise - wide computer network Intranets and the Internet .", "label": "", "metadata": {}}
{"text": "When used in a WAN networking environment , the personal computer 50 typically includes a modem 84 or other means for establishing communications over the wide area network 82 , such as the Internet .The modem 84 , which can be internal or external , is connected to the system bus 53 via the serial port interface 76 .In a network environment , program modules depicted relative to the personal computer 50 , or portions thereof , can be stored in the remote memory storage devices .As appreciated by those skilled in the art , the network connections shown are exemplary and other means of establishing a communications link between the computers can be used .", "label": "", "metadata": {}}
{"text": "FIG .3 .It should be noted that the entire system 100 , or part of speech recognition system 100 , can be implemented in the environment illustrated in .FIG .2 .For example , microphone 92 can preferably be provided as an input device to the computer 50 , through an appropriate interface , and through the A / D converter 104 .The training module 105 and feature extraction module 106 can be either hardware modules in the computer 50 , or software modules stored in any of the information storage devices disclosed in .", "label": "", "metadata": {}}
{"text": "2 and accessible by the processing unit 51 or another suitable processor .In addition , the lexicon storage module 110 , the acoustic model 112 , and the language model 16 are also preferably stored in any of the memory devices shown in .FIG .2 .Furthermore , the tree search engine 114 is implemented in processing unit 51 ( which can include one or more processors ) or can be performed by a dedicated speech recognition processor employed by the personal computer 50 .In the embodiment illustrated , during speech recognition , speech is provided as an input into the system 100 in the form of an audible voice signal by the user to the microphone 92 .", "label": "", "metadata": {}}
{"text": "The A / D converter 104 converts the analog speech signal into a sequence of digital signals , which is provided to the feature extraction module 106 .In one embodiment , the feature extraction module 106 is a conventional array processor that performs spectral analysis on the digital signals and computes a magnitude value for each frequency band of a frequency spectrum .The signals are , in one illustrative embodiment , provided to the feature extraction module 106 by the A / D converter 104 at a sample rate of approximately 16 kHz .The feature extraction module 106 divides the digital signal received from the A / D converter 104 into frames that include a plurality of digital samples .", "label": "", "metadata": {}}
{"text": "The frames are then encoded by the feature extraction module 106 into a feature vector reflecting the spectral characteristics for a plurality of frequency bands .In the case of discrete and semi - continuous Hidden Markov Modeling , the feature extraction module 106 also encodes the feature vectors into one or more code words using vector quantization techniques and a codebook derived from training data .Thus , the feature extraction module 106 provides , at its output the feature vectors ( or code words ) for each spoken utterance .The feature extraction module 106 provides the feature vectors ( or code words ) at a rate of one feature vector or ( code word ) approximately every 10 milliseconds .", "label": "", "metadata": {}}
{"text": "These probability distributions are later used in executing a Viterbi or similar type of processing technique .Upon receiving the code words from the feature extraction module 106 , the tree search engine 114 accesses information stored in the acoustic model 112 .The model 112 stores acoustic models , such as Hidden Markov Models , which represent speech units to be detected by the speech recognition system 100 .In one embodiment , the acoustic model 112 includes a senone tree associated with each Markov state in a Hidden Markov Model .The Hidden Markov models represent , in one illustrative embodiment , phonemes .", "label": "", "metadata": {}}
{"text": "The information received by the tree search engine 114 based on its accessing of the acoustic model 112 is used in searching the lexicon storage module 110 to determine a word that most likely represents the codewords or feature vector received from the features extraction module 106 .Also , the search engine 114 accesses the language model 16 .The language model 16 is a unified language model or a word N - gram or a context - free grammar that is used in identifying the most likely word represented by the input speech .The most likely word is provided as output text .", "label": "", "metadata": {}}
{"text": "As appreciated by those skilled in the art , the speech recognition system 100 can take many forms and all that is required is that it uses the language model 16 and provides as an output the text spoken by the user .As is well known , a statistical N - gram language model produces a probability estimate for a word given the word sequence up to that word ( i.e. , given the word history H ) .For example , a bi - gram ( or 2-gram ) language model considers the previous word as having an influence on the next word .", "label": "", "metadata": {}}
{"text": "Also , the probability of a word sequence is determined based on the multiplication of the probability of each word given its history .Therefore , the probability of a word sequence ( w 1 . . .wm ) is represented as follows : .The .P .w . wm . )i . m .P .w .i .H .i . )N - gram model is obtained by applying an N - gram algorithm to a corpus ( a collection of phrases , sentences , sentence fragments , paragraphs , etc ) of textual training data .", "label": "", "metadata": {}}
{"text": "These probability values collectively form the N - gram language model .Some aspects of the invention described below can be applied to building a standard statistical N - gram model .As is also well known in the art , a language model can also comprise a context - free grammar .A context - free grammar provides a rule - based model that can capture semantic or syntactic concepts of sentence structure or spoken language .For instance , by way of example , one set of context - free grammars of a larger plurality of context - free grammars for a software application or task concerning scheduling meetings or sending electronic mail may comprise : . etc . .", "label": "", "metadata": {}}
{"text": "etc .This type of grammar does not require an in - depth knowledge of formal sentence structure or linguistics , but rather , a knowledge of what words , phrases , sentences or sentence fragments are used in a particular application or task .A unified language model is also well known in the art .Referring to .FIG .4 , a unified language model 140 includes a combination of an N - gram language model 142 and a plurality of context - free grammars 144 .Specifically , the N - gram language model 142 includes at least some of the same non - terminals of the plurality of context - free grammars 144 embedded therein such that in addition to predicting words , the N - gram language model 142 also can predict non - terminals . where ( h 1 , h 2 , . . .", "label": "", "metadata": {}}
{"text": "Essentially , the N - gram language model 142 ( also known as a hybrid N - gram model ) of the unified language model 140 includes an augmented vocabulary having words and at least some of the non - terminals .In use , the speech recognition system or module 100 will access the language model 16 ( in this embodiment , the unified language model 140 ) in order to determine which words have been spoken .The N - gram language model 142 will be used to first predict words and non - terminals .Then , if a non - terminal has been predicted , the plurality of context - free grammars 144 is used to predict terminals as a function of the non - terminals .", "label": "", "metadata": {}}
{"text": "As mentioned in the Background section , the application developer should be provided with an efficient method in which an appropriate language model 16 can be created for the selected application .In some applications , a standard N - gram language model will work and any improvements in developing such a model will be valuable .While in other applications , a unified language model 140 may work the best , and accordingly , improvements in building such a model will also be valuable .As different applications are developed for language processing , task - dependent ( domain dependent ) language models may be more appropriate , due to their increased specificity , which can also make the language models more accurate than a larger , general purpose language model .", "label": "", "metadata": {}}
{"text": "To create a general purpose language model , such as an N - gram language model , a task - independent corpus of training data can be used and applied as discussed above to an N - gram algorithm .Task - independent corpora are readily available and can comprise compilations of magazines , newspapers , etc . , to name just a few .The task - independent corpora are not directed at any one application , but rather provide many examples of how words are used in a language .Task - dependent corpora , on the other hand , are typically not available .", "label": "", "metadata": {}}
{"text": "A broad aspect of the invention includes a method for creating a task or domain dependent unified language model for a selected application from a task - independent corpus .The task - dependent unified language model includes embedded context - free grammar non - terminal tokens in an N - gram language model .As discussed above , the task - independent corpus is a compilation of sentences , phrases , etc . that is not directed at any one particular application , but rather , generally shows , through a wide variety of examples , how words are ordered in a language .", "label": "", "metadata": {}}
{"text": "FIG .5 illustrates a first method 160 for creating or building a language model .The method 160 includes a step 162 for obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts .As used herein , a \" semantic or syntactic concept \" includes word or word phrases that represent particular word usages for various commands , objects , actions , etc .For instance , the task - independent corpus includes various instances of how proper names are used .For example , the task - independent corpus could have sentences like : \" Bill Clinton was present at the meeting \" and \" John Smith went to lunch at the conference \" .", "label": "", "metadata": {}}
{"text": "Step 162 represents obtaining context - free grammars having non - terminal tokens to represent the semantic or syntactic concepts in the task - independent corpus , the non - terminal tokens having terminals present in the task - independent corpus .For instance , using the proper name example provided above , an example CFG can be the following : .Commonly , a plurality of context - free grammars comprising non - terminal tokens representing various semantic or syntactic concepts are used .For instance , other semantic or syntactic concepts include geographical places , regions , titles , dates , times , currency amounts , and percentage amounts to name a few .", "label": "", "metadata": {}}
{"text": "At step 164 , the task - independent corpus is parsed with the plurality of context - free grammars obtained in step 162 in order to identify word occurrences in the task - independent corpus of each of the semantic or syntactic concepts .At step 166 , each of the identified word occurrences is replaced with the corresponding non - terminal tokens of step 164 .An N - gram model is then built at step 168 using an N - gram algorithm , the N - gram model having the non - terminal tokens embedded therein .At step 170 , a second plurality of context - free grammars is obtained suitable for the selected application .", "label": "", "metadata": {}}
{"text": "However , each of the context - free grammars of the second plurality is more appropriate for the selected application .Referring back to the proper name example provided above , the second plurality of context - free grammars could include a CFG : .Method 160 can be implemented in computer 50 wherein each of the context - free grammars and the task - independent corpus are stored on any of the local or remote storage devices .Preferably , the N - gram model having the non - terminal tokens and the second plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts are stored on a computer readable medium accessible by the speech recognizer 100 .", "label": "", "metadata": {}}
{"text": "6 illustrates a method 180 for creating a unified language model for a selected application from a task - independent corpus that includes a large number of phrases that may be of different context .Simple parsing of the task - independent corpus with context - free grammars for the task - dependent application may cause errors , which will then propagate to the N - gram model upon application of an N - gram algorithm .In order to reduce the errors during parsing , this aspect of the invention includes using at least one context - free grammar having a non - terminal token for a phrase ( word or words ) that can be mistaken for one of the desired task - dependent semantic or syntactic concepts .", "label": "", "metadata": {}}
{"text": "For example , a task - dependent application may require modeling the day of the week as a semantic concept in the N - gram model .A context - free grammar of the following form could be used during parsing of the task - independent corpus : .However , the task - independent corpus might contain references to a person called \" Joe Friday \" .In this manner , during parsing of the task - independent corpus , instances of days of the week will be identified separate from instances where \" Friday \" is the last name of an individual .", "label": "", "metadata": {}}
{"text": "At step 186 , each of the identified word occurrences for non - terminals representing concepts which are of interest to the target application is replaced with the corresponding non - terminal token as defined by the corresponding context - free grammar .An N - gram model can then be built having the non - terminal tokens embedded therein as indicated at step 188 .Step 190 is similar to Step 170 and includes obtaining a second set of context - free grammars suited for the selected application .Used during language processing such as speech recognition , the N - gram model having the non - terminal tokens and the plurality of context - free grammars associated with the task - dependent application is stored on a computer readable medium accessible by the speech recognition module 100 .", "label": "", "metadata": {}}
{"text": "The phrases associated with these grammars would not normally be spoken in the selected application .Thus , the extent or size of the plurality of context - free grammars is less during speech recognition , corresponding to less required storage space in the computer 50 than was used for parsing the task - independent corpus .In one embodiment , step 188 associated with building the N - gram model can include eliminating at least some of the associated text from the task - independent corpus for non - terminal tokens that can be mistaken for one of the desired task - dependent semantic or syntactic concepts .", "label": "", "metadata": {}}
{"text": "Appropriate context - free grammars can then be determined and included in the plurality of context - free grammars at step 182 .Steps 184 to 188 can then be performed as necessary in order to reexamine the parsed task - independent corpus or N - gram model to ascertain if the errors have been corrected .This iterative process can be repeated as necessary until the errors are corrected and a suitable N - gram model has been obtained .As discussed above , the task - independent corpus is a general corpus and in fact it is likely that most of the corpus is unrelated to the task or application that the developer is interested in .", "label": "", "metadata": {}}
{"text": "Generally , another aspect of the present invention includes using the context - free grammars for the task - dependent application to form phrases , sentences or sentence fragments that can then be used as queries in an information retrieval system .The information retrieval system examines the task - independent corpus and identifies portions similar to the query .The identified text of the task - independent corpus is more relevant to the selected task or application ; therefore , a language model derived from the identified text may be more specific than a language model based on the complete task - independent corpus .", "label": "", "metadata": {}}
{"text": "This technique narrows the task - independent corpus , but can identify yet more examples of task specific sentences , phrases , etc . .FIG .7 illustrates a method 200 for creating a language model for a selected application from a task - independent corpus in the manner discussed above .Step 202 includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .As described above , commonly the context - free grammars are written by a developer having at least some knowledge of what phrases may be used in the selected application for each of the semantic or syntactic concepts , but the extent of knowledge about such phrases is not complete .", "label": "", "metadata": {}}
{"text": "The word phrases can include some or all of the various combinations and permutations defined by the associated context - free grammars where the non - terminal tokens include multiple words .At step 206 , at least one query is formulated for an information retrieval system using at least one of the generated word phrases .The query can be generated using a statistical \" bag of words \" technique which uses TF - IDF vectors .Similarity between the query and segments of the task - independent corpus can be computed using cosine similarity measure .These are generally well - known techniques in the field of information retrieval .", "label": "", "metadata": {}}
{"text": "However , each query could be simply a separate word phrase , as appreciated by those skilled in the art .At step 208 , the task - independent corpus is queried based on the query formulated .The particular information retrieval technique used to generate and execute the query against the task - independent corpus is not critical to this feature of the present invention .Rather , any suitable query development and information retrieval technique can be used .It should simply be noted that the language model created from the identified text according to the present technique works better with information retrieval techniques that identify more relevant text of the task - independent corpus .", "label": "", "metadata": {}}
{"text": "A language model can then be built using the identified text as represented at step 212 .At this point , it should be noted that the method illustrated in .FIG .7 is not limited to a unified language model , or even an N - gram language model , but rather , can be helpful in forming language models of any type used in a language processing system where the model is based on a task - independent corpus .Nevertheless , the method 200 is particularly useful in building an N - gram language model .", "label": "", "metadata": {}}
{"text": "FIG .8 illustrates a method 220 similar to the method 200 of .FIG .7 wherein the same reference numerals have been used to identify similar steps .However , method 220 can be used to create an N - gram language model having the non - terminal tokens of the context - free grammars .In addition to the steps described above , method 220 also includes parsing the identified text of the task - independent corpus with a plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts as indicated at step 222 .", "label": "", "metadata": {}}
{"text": "Step 212 would then include building an N - gram model having non - terminal tokens .In both methods 200 and 220 , the relevant text is identified in the task - independent corpus .If desired , the identified text can be extracted , copied or otherwise stored separate from the task - independent corpus as an aid in isolating relevant text and providing easier processing .FIG .9 is a block diagram illustrating another aspect of the present invention .Generally , this aspect includes forming an N - gram language model from the word phrases obtained from the context - free grammars and combining the N - gram language model with another N - gram language model based on the task - independent corpus .", "label": "", "metadata": {}}
{"text": "FIG .9 , block 240 represents the context - free grammars obtained ( for example , authored by the developer ) for the selected task or application .The context - free grammars are used to generate synthetic data or word phrases 242 in a manner similar to step 204 of methods 200 and 220 .The word phrases 242 are then provided to an N - gram algorithm 244 to build a first N - gram language model 246 .Block 248 illustrates application of an N - gram algorithm to obtain the second N - gram language model 250 .", "label": "", "metadata": {}}
{"text": "This combination can be performed using any known smoothing technique , such as interpolation , deleted interpolation , or any other suitable technique .If desired , the second language model can be weighted based on whether the identified text is believed to be accurate .The weighting can be based on the amount of text identified in the task - independent corpus , the number of queries used , etc . .In another embodiment , non - terminal tokens representing semantic or syntactic concepts can be inserted into the identified text , or the task - independent corpus in order that the second N - gram language model includes non - terminal tokens .", "label": "", "metadata": {}}
{"text": "Of course , if this option is chosen the identified text 210 would not be provided directly to the N - gram algorithm 248 , but rather to block 264 .The non - terminal tokens inserted into the identified text or the task - independent corpus can be based on the context - free grammars obtained at block 240 , or alternatively , based on another set of context - free grammars 270 that includes other context - free grammars for the reasons discussed above .When the third N - gram language model 252 is built having non - terminals , the word phrases or synthetic data at block 242 typically will also include the non - terminals as well .", "label": "", "metadata": {}}
{"text": "The task - dependent unified language model includes embedded context - free grammar non - terminal tokens in an N - gram as well as a plurality of context - free grammars defining the non - terminal tokens .Inside each context - free grammar , the standard probabilistic context - free grammar can be used .However , without real data pertaining to the specific task or application , an estimate for each of the terminal probabilities can not be easily determined .In other words , the developer can author or otherwise obtain the plurality of context - free grammars ; however , an estimate of the probabilities for each of the terminals may not be readily known .", "label": "", "metadata": {}}
{"text": "In other words , the context - free grammar constrains or defines the allowable set of terminals from the N - gram language model .Therefore , probabilities of the terminals from the N - gram language model need to be appropriately normalized in the same probability space as the terminals present in the corresponding context - free grammar .i in W. The likelihood of W under the segmentation T is therefore .P .W .T . )i . m .P .t .i .t .i .t .i .i . m .", "label": "", "metadata": {}}
{"text": "u . t .i ._ .t .i . )In addition to tri - gram probabilities , we need to include P ( .u t .i 1 u t .i 2 . . .u t .i k ] from the context - free grammar non - terminal t i .In the case when t i itself is a word ( \u016b t .Otherwise , P ( \u016b t .P .u . t .i ._ .t .i . )[ .l .u .", "label": "", "metadata": {}}
{"text": "t .i .P .u . t .l .u . t .i . u . t .i .l . .]P .s .u . t .i ._ . )Here represents the special end - of - sentence word .Three different methods are used to calculate the likelihood of a word given history inside a context - free grammar non - terminal .i 1 u t .i 2 . . .u t .A CFG state constrains the possible words that can follow the history .", "label": "", "metadata": {}}
{"text": "i 1 u t .i 2 . . .u t .The likelihood of observing u t .i l following the history can be estimated by the uniform distribution below : P ( u t .The uniform model does not capture the empirical word distribution underneath a context - free grammar non - terminal .A better alternative is to inherit existing domain - independent word tri - gram probabilities .These probabilities need to be appropriately normalized in the same probability space .Thus we have : .P .u . t .i .", "label": "", "metadata": {}}
{"text": "h . )P . word .u . t .i .l .u . t .i .l .u . t .i .l .w .W .Q .h . )P . word .w .u . t .i .l .u . t .i .l .The normalization is performed the same as in Equation ( 7 ) .Multiple segmentations may be available for W due to the ambiguity of natural language .The likelihood of W is therefore the sum over all segmentations S(W ) : .", "label": "", "metadata": {}}
{"text": "w . )T .S .W . )P .W .T . )Although the present invention has been described with reference to preferred embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .2 Answers 2 .The term part of speech refers to the traditional word categories .Usually , somewhere between eight to twelve basic parts of speech are posited , e.g. noun , verb , adjective , adverb , adposition ( preposition , postposition , circumposition ) , coordinate conjunction , subordinate conjunction , interjection , ... .", "label": "", "metadata": {}}
{"text": "It could refer to anything that is assumed in the domain of syntax .It could refer to words , phrases , clauses , other specific word combinations .I have no clear associations with what is meant when I see syntactic type out of context .Can both terms apply for a phrase , instead of words ? -Tim Mar 27 ' 14 at 0:36 .No , the term \" part of speech \" refers to words only , not to phrases .Traditionally , the term \" phrase \" refers to a grouping of two or more words ( but there are modern uses that vary ) .", "label": "", "metadata": {}}
{"text": "If I have a context in which \" syntactic type \" is used , I could comment better about whether its use is appropriate .You might also state what your goal is here .Are you trying to write something about syntax ?Are you reading something about syntax ? -Tim Osborne Mar 27 ' 14 at 0:45 .Thanks !I do n't have context .Just general question .-Tim Mar 27 ' 14 at 1:22 .Parts of speech are lexical categories , such as noun , verb , adjective .With phrase structure grammars , syntactic types , or syntactic categories , are categories of phrases . of well formed sentence fragments , such as noun phrase , verb phrase , prepositional phrase .", "label": "", "metadata": {}}
{"text": "Parts of speech are thus also syntactic types .It is not the same concept , but there is a containment relation between the two concepts .However , in lexicalized grammars , phrasal constituents are always associated with a lexical categories , so that syntactic types no longer have much role , or at best have a lesser role .Syntactic concepts are somewhat dependent on the syntactic theory and formalisation used .This is obviously to be expected .The differences are not as clearcut as may seem ( Replying to a very relevant comment ) .By lexical , I do mean related to word .", "label": "", "metadata": {}}
{"text": "Actually , it may be a little bit more complex , if we want to keep things consistent between synthetic and isolating languages .Then I am not sure that lexical should pertain to single word in the context of an isolating language ( though that seems to be the case from what I read , or do not read ) .Following this discussion , the issue of what is lexical , or maybe of what is part of speech independently of lexicality , is not completely clear to me .Indeed the issue is not completely clear at all according to Haspelmath in \" The indeterminacy of word segmentation and the nature of morphology and syntax \" ( Folia Linguistica ) .", "label": "", "metadata": {}}
{"text": "After all , the whole notion of parts - of - speech is with reference to the syntax of a language .\" Doing so might escape the consistency problem between isolating and synthetic languages .I am not sure where the consensus lies regarding the possible variations in these definitions .There may be better characterizations , or other definitions .Being consistent across the variety of languages seems a daunting task .AUTHOR : De Belder , Marijke TITLE : Roots and Affixes SUBTITLE : Eliminating Lexical Categories from Syntax SERIES : LOT dissertation series YEAR : 2011", "label": "", "metadata": {}}
{"text": "Mercedes Tubino , Department of English , University of Seville ( Spain ) .SUMMARY .This monograph develops a new syntactic / morphological model that stems from the revision of the two main current models , Distributed Morphology ( DM , Halle & Marantz 1993 ) and the Exo Skeletal Model ( Borer 2003 ) .Like its two predecessors , this is a Late Insertion Morphological model that assumes that lexical material enters any derivation only after the narrow syntax part of the derivation is concluded .Chapter 1 introduces the book 's research questions as well as the theoretical and empirical basis assumed throughout the monograph .", "label": "", "metadata": {}}
{"text": "The aim of her thesis is to answer this question negatively .She argues that lexical vocabulary items can not contain features linking them with particular syntactic contexts .She claims that , while both examples involve the same VI , ' dog ' , the syntactic context is what determines its meaning as count or non - count .On the other hand , she argues that categorial functional heads do not exist in syntax , contra other approaches such as DM .She claims that derivational affixes , many of which are traditionally seen as the phonetic realization of categorial positions , are in fact instantiations of Root Terminal Nodes .", "label": "", "metadata": {}}
{"text": "For this author , there is a fundamental difference between lexical and functional material ; only the former is malleable and subject to coercion .She will base many of her contrasts on this fact .In Chapter 2 , De Belder develops a theory of the syntax of roots , which is a revision of previous late insertion accounts .This chapter is the result of her joint work with Jeroen van Craenenbroeck ( 2011 ) .She adopts Borer 's view that roots are void of syntactic features , so they can not select for particular categories ( contra DM 's approaches such as Harley & Noyer 2000 ) .", "label": "", "metadata": {}}
{"text": "For instance , Halle & Marantz ( 1993 ) claim that only VIs that are inserted in functional positions are subject to competition , whereas the insertion in root positions is done by free choice .Recent DM approaches ( e.g. Siddiqi 2009 ) argue in favor of competition of VIs to occupy root positions , but they also assume that lexical VIs contain features .De Belder rejects these accounts on these grounds , although she adopts a late ( as opposed to early ) insertion approach .In the first part of this chapter , she argues that only a late insertion approach to morphology can explain why we may find functional VIs occupying root positions , e.g. , ( 7 ) Martha is mijn tweede ik , ' Martha is my best friend , lit .", "label": "", "metadata": {}}
{"text": "These examples also show that roots are defined structurally and not because of their inherent properties ( e.g. the first person pronoun , ik ' I ' , does not carry person features with it when used as a root ) .In the second part of the chapter , De Belder puts forward the syntactic model that will be used throughout the book , which involves a revision of previous syntactic models on Merge ( Chomsky 2000 and subsequent work ) .She follows Zwart ( 2009 ) in noticing that previous accounts on Merge involve an asymmetry ; whereas primary Merge involves selecting two elements from numeration at once , subsequent Merge operations involve selecting one object at once .", "label": "", "metadata": {}}
{"text": "She argues that this empty syntactic position is what corresponds with root positions .This implies that each root in a syntactic structure implies a new derivation , although the specific details of this mechanism are not discussed until Chapter 5 .In the last part of the chapter , De Belder proposes a modification of the Subset Principle to ensure the insertion of functional VIs in root positions through competition .She finishes the chapter by addressing potential problems and other accounts ( e.g. Harley 2009 ; Pfau 2009 ) she terms as ' late insertion approaches with a limited type of early insertion ' .", "label": "", "metadata": {}}
{"text": "She first presents a case study on the countability of nouns to show that fine - grained distinctions in the nominal domain are determined syntactically .That is , she shows that the fact that the same VI ( e.g. water ) may be used as count and non - count is the result of syntactic features and their corresponding functional projections rather than inherent properties of the VIs .The relevant syntactic features accounting for the different readings of nouns are [ Num ] and [ Size].She also argues that a syntactic position containing [ Size ] only is not possible .", "label": "", "metadata": {}}
{"text": "She argues that their different denotation is derived from the terminal nodes this VI may realize as the complement of an adjectival projection , if it is an adjective .As a quantifier , it originates in Size\u00ba , moves up to Num\u00ba and ends up in D\u00ba.As a lexical VI , it gets its interpretation entirely from the Encyclopedia ( their denotation is learned and arbitrary ) , and as a functional VI , it gets its interpretation from the functional features it realizes in syntax .In Chapter 4 , De Belder discusses why categorial heads do not exist as primitives of language .", "label": "", "metadata": {}}
{"text": "De Belder , however , discusses the possibility that they realize ' something else ' and argues that her approach is more advantageous , especially when it comes to affixes associated with more than one category .In the first part of the chapter , De Belder puts forward a theory of homonyms , which she identifies as affixes with the same phonetic realization that are listed as separate vocabulary items .She presents three empirical tests to distinguish between homonyms and different instances of the same affix based on allomorphy , based on whether the different instances of the affix have the same synonyms and co - occurrence .", "label": "", "metadata": {}}
{"text": "In the second part of the chapter , De Belder argues that multicategorial affixes can not be the realization of categories , hence categories are not primitives of grammar .In Chapter 5 , De Belder provides an alternative approach to derivational word formation .She argues that since the meaning of derivational affixes is lexical , they realize lexical positions ( i.e. they realize roots ) , but like other VIs that realize root nodes , derivational affixes may be semi - lexical too ( i.e. lexical VIs that realize functional nodes ) .She presents the case study of Dutch collective mass nouns ( e.g. ondergoed ' underwear ' ) to illustrate this point .", "label": "", "metadata": {}}
{"text": "In this chapter , De Belder also proposes that derived words are concatenated root nodes , where the base VI realizes the lowest root node and the affixes realize the highest ones .According to De Belder , root nodes are defined by the absence of any syntactic features ( they are empty derivations that are merged with objects from Numeration in Unary Merge fashion ) .Each instantiation of the empty set in a derivation will correspond to a ' lexical ' VI .The remainder of the chapter consists of different instantiations of the model .In Chapter 6 , the author discusses the theoretical repercussions of her proposed model and briefly addresses further issues associated with her proposal .", "label": "", "metadata": {}}
{"text": "She briefly suggests that this can be extended to verbs , which would be the result of cognitively recognizing information about time , modality , and aspect in conjunction with argument introducing projections .Another issue briefly treated in this chapter is , for instance , the elimination of the distinction between inflection , derivation , and compounding , which should now be viewed as the distinction between functional and root terminal nodes .EVALUATION .De Belder 's monograph makes an outstanding contribution to syntactic and morphological theory , as well as to the analysis of the Dutch nominal domain , with plenty of examples illustrating all the issues treated , especially regarding the derivation of Dutch nominal word - forms .", "label": "", "metadata": {}}
{"text": "I did have some concerns regarding certain issues , both in content and presentation .First , the monograph would benefit from a revised organization , since the reader is constantly referred to later sections for information that may as well have been discussed all at once .The presentation of the theoretical model proposed is especially problematic , as De Belder covers some parts in Chapter 2 , and leaves the rest for Chapter 5 .Although the reader is referred to Chapter 5 for further information on how the model works , a full description of the model right at the beginning would make the proposal clearer and save the reader some unnecessary questions .", "label": "", "metadata": {}}
{"text": "However , this discussion would be stronger if examples were provided from languages other than Dutch ( it just includes a few additional examples from English ) .Also , while the proposal defended in this thesis is intended as a theoretical model for grammar , the discussion is limited to the nominal domain , while its application to the verbal domain is only suggested , but not addressed , at the end of the book .The model proposes as one of its main points the elimination of categories as part of grammar .However , traditional cases of nominalization or verbalization are not specifically , but just vaguely , addressed in the present model , which may leave the reader wondering how they would be resolved .", "label": "", "metadata": {}}
{"text": "She suggests that the restriction on VIs is to be placed on extralinguistic / conceptual knowledge and convention , an explanation that seems unsatisfactory ; while the concept associated with Vocabulary Items is clearly extralinguistic , Vocabulary Items themselves are linguistic representations .Their association with particular syntactic positions needs to be restricted somehow .Finally , the monograph would have benefited from more careful proof - reading , as the text contains a considerable number of misspellings that may be distracting to the reader .In sum , De Belder makes an excellent contribution to syntactic and morphological theory , challenging many ideas widely assumed ( i.e. the existence of syntactic categories as a primitive of grammar ) .", "label": "", "metadata": {}}
{"text": "REFERENCES .Borer , Hagit .Exo - Skeletal vs. endo - skeletal explanations .In John Moore and Maria Polinsky ( eds . ) , The Nature of Explanation in Linguistic Theory .Chicago : CSLI and University of Chicago Press .Chomsky , Noam .Minimalist inquiries : the framework .In Roger Martin , David Michaels and Juan Uriagereka ( eds . ) , Step by Step : Essays on Minimalist Syntax in Honor of Howard Lasnik .Cambridge : The MIT Press .De Belder , Marijke , & Jeroen van Craenenbroeck .How to merge a root .", "label": "", "metadata": {}}
{"text": "Halle , Morris , & Alec Marantz .Distributed Morphology and the pieces of inflection .In Kenneth Hale and Samuel Jay Keyser ( eds . ) , The view from building 20 .Cambridge : MIT Press , 111 - 176 .Harley , Heidi .Roots : Identity , Insertion , Idiosyncracies .Talk presented at the Root Bound workshop , USC , February 21 , 2009 .Harley , Heidi and Rolf Noyer .State - of - the - Article : Distributed Morphology .Glot International 4.4 : 3 - 9 .Harley , Heidi and Rolf Noyer .", "label": "", "metadata": {}}
{"text": "Evidence from nominalisations .In B. Peters ( ed . ) , The Lexicon - Encyclopedia Interface .Amsterdam : Elsevier , 349 - 374 .Pfau , Roland .Grammar as processor .A Distributed Morphology account of spontaneous speech errors .Amsterdam : John Benjamins .Siddiqi , Daniel .Syntax within the word : economy , allomorphy and argument selection in Distributed Morphology .Amsterdam : John Benjamins .ABOUT THE REVIEWER .ABOUT THE REVIEWER : Mercedes Tubino has a PhD in Linguistics from the University of Arizona .She presently teaches English at the University of Seville ( Spain ) .", "label": "", "metadata": {}}
{"text": "Gelderen , Elly van ( 2002 ) : An Introduction to the Grammar of English .Syntactic Arguments and Socio - Historical Background .Amsterdam / Philadelphia : John Benjamins , paperback ISBN 158811 1571 ( $ 29.95 ) , hardback ISBN 1588112004 ( $ 68.00 ) .xxiv + 200 pages .Reviewer : Anja Wanner , Department of English , University of Wisconsin - Madison .Overview : This book is a textbook for undergraduates in a typical introductory English Grammar class .It steers a middle course between traditional grammar ( Quirk at al .1985 ) and generative grammar ( Aarts 2001 ) and it is designed to be covered in one semester .", "label": "", "metadata": {}}
{"text": "It has numerous exercises , many of them followed by model answers , with data drawn from various sources ( including poetry , clippings from newspaper articles , and cartoons ) .Each chapter ends with a set of exercises , followed by model answers .There are three review blocks , including suggestions for exams .A 10-page glossary with references to the text completes the book ; there is no index .The approach is very hands - on for instance , it is pointed out that prepositions have characteristics of both lexical and grammatical categories , but \" for the sake of simplicity \" ( p. 17 ) they are treated as lexical throughout the book .", "label": "", "metadata": {}}
{"text": "Van Gelderen uses a simplified version of X - bar Theory , without projecting grammatical categories .She follows the generative tradition of defining the functions in a clause as something that \" can be read off the tree \" ( p. 41 ) .Grammatical functions and their representation in a a tree diagram are discussed in chapters 4 and 5 .Chapter 6 deals with the syntax of the verb phrase , which is analyzed as consisting of a \" verb group \" ( auxiliaries and lexical verbs ) and the complement .The chapter is followed by a review section .", "label": "", "metadata": {}}
{"text": "Chapter 7 introduces non - finite clauses as clausal entities with an empty subject position and \" to \" being part of the verb group .Following another review section , the structure of nonverbal phrases ( Noun Phrase , Adjectival Phrase , Adverbial Phrase , Prepositional Phrase ) is revisited in chapter 9 , which emphasizes syntactic functions within a phrase ( determiner , head , modifier , complement ) .Chapter 10 discusses phrases containing clauses , such as NPs with relative clauses ( analysis : there is no wh - movement , relative pronouns are treated as complementizers ) .", "label": "", "metadata": {}}
{"text": "Tree diagrams for these constructions are not provided , they are clearly not within the scope of this book , as they would involve the introduction of syntactic movement of phrases .The book closes with recommendations for further reading and a bibliography .Evaluation : This is a book that is geared towards students who will not take many linguistics classes and who need a practical introduction to analyzing English sentences .What makes this book stand out are the author 's conscious choices to keep the book student - friendly without oversimplifying the material that is discussed .", "label": "", "metadata": {}}
{"text": "For instance she considers the hierarchy neutral term \" constituent \" a \" stumbling block \" and consequently avoids it .The same holds for terms like \" recursion \" and the \" X \" in X - bar theory .The structure of small clauses is discussed without any reference to this term , and the empty subject in infinitives is never referred to as \" PRO \" .Another student - friendly element are the exercises throughout the book , wich are generally followed by model answers .The point of the model answers is to provide feedback to the students there is no implication that there is only one acceptable answer to a question .", "label": "", "metadata": {}}
{"text": "The book is much shorter and not as densely written as some comparable textbooks , most notably the one by Brinton ( 2000 ) , but I consider van Gelderen 's textbook more manageable as a text for the audience that it targets .I also find it considerably clearer than the introduction by Verspoor / Sauter ( 2000 ) , also published by John Benjamins , which has a confusing layout and does not make use of tree diagrams at all .In comparison with Lobeck ( 2000 ) , which is similar in its approach and its target group , van Gelderen is more focused on structure and on the practical analysis of data .", "label": "", "metadata": {}}
{"text": "For a start , the subtitle of the book Syntactic Arguments and Socio - Historical Background is somewhat misleading .There is a lot more syntax than socio - historical background in this book .As in Lobeck 's ( 2000 ) textbook , historical facts are brought up here and there , to illustrate varieties of English and the changeability of linguistic rules .The \" special topic \" sections that follow some of the chapters generally deal with a conflict between prescription and description , they are not necessarily historical , nor are they really tied thematically to the chapters that they are part of .", "label": "", "metadata": {}}
{"text": "The bibliography is short and , unfortunately , not very well edited .Some books are missing altogether for example , the \" Further Reading \" section on p. 195 mentions a text by O'Grady et al .( 1993 ) , but there is no such reference listed in the bibliography .The grammar by Quirk et al .( 1985 ) is a wonderful resource , alright , but there would have been no harm in including one of the more recent reference grammars , such as the corpus - based grammar by Biber et al .( 1999 ) .", "label": "", "metadata": {}}
{"text": "It balances linguistic argumentation and practical answers in a student - friendly manner and draws a clear line between what can be achieved in a one - semester introductory class and what should be left to further exploration .Bibliography .Aarts , Bas .( 2001 , 2nd ed . ) : English Syntax and Argumentation .Houndsmills : Palgrave .Aitchison , Jean ( 2001 , 3rd ed . ) : Language Change : Progress or Decay ?Cambridge : Cambridge University Press , 2001 .Biber , Douglas et al .( 1999 ) : Longman Grammar of Spoken and Written English .", "label": "", "metadata": {}}
{"text": "Brinton , Laurel ( 2000 ) : The Structure of Modern English : A Linguistic Introduction .Amsterdam / Philadelphia : John Benjamins .Burton - Roberts , Noel ( 1997 , 2nd ed . ) : Analysing Sentences : An Introduction to English Syntax .Longman : London .Crystal , David ( 1997 , 2nd ed . ) : The Cambridge Encyclopedia of Language .Cambridge : Cambrige University Press .Gelderen , Elly van ( 1997 ) : Verbal Agreement and the Grammar behind its \" Breakdown \" : Minimalist Feature Checking .Tuebingen : Max Niemeyer .", "label": "", "metadata": {}}
{"text": "A Preliminary Investigation .Chicago : University of Chicago Press .Lobeck , Anne ( 2000 ) : Discovering Grammar .An Introduction to English Sentence Structure .New York / Oxford : Oxford University Press .Quirk , Randolph et al .( 1985 ) : A Comprehensive Grammar of the English Language .London : Longman .Verspoor , Marjolijn / Sauters , Kim ( 2000 ) : English Sentence Analysis .An Introductory Course .Amsterdam / Philadelphia : John Benjamins .ABOUT THE REVIEWER : Anja Wanner is an assistant professor of English at the University of Wisconsin - Madison , where she teaches English syntax and syntactic theory .", "label": "", "metadata": {}}
{"text": "She currently works on argument alternations and the representation of agentivity in scientific discourse .Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .Traditional approaches to parsing are highly complex and problem specific .Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .", "label": "", "metadata": {}}
{"text": "We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .We also investigate the use of joint parsing and part - of - speech tagging in the neural paradigm .Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .", "label": "", "metadata": {}}
{"text": "We present structured perceptron training for neural network transition - based dependency parsing .We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .We also provide in - depth ablative analysis to determine which aspects of our model provide the largest gains in accuracy .", "label": "", "metadata": {}}
{"text": "Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .We introduce voting to globally consider adding entries to the lexicon , and pruning to remove entries no longer required to explain the training data .Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .", "label": "", "metadata": {}}
{"text": "The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .We present a simple and novel classifier - based preordering approach .Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .", "label": "", "metadata": {}}
{"text": "We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .To show the usefulness of such a resource , we present a case study of cross - lingual transfer parsing with more reliable evaluation than has been possible before .This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .", "label": "", "metadata": {}}
{"text": "Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .In this paper , we show that additional token constraints can be projected from a resource- rich source language to a resource - poor target language via word - aligned bitext .We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .", "label": "", "metadata": {}}
{"text": "We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .This new edition introduces syntactic annotations : words are tagged with their part - of - speech , and head - modifier relationships are recorded .The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .", "label": "", "metadata": {}}
{"text": "We propose a simple , efficient procedure in which part - of - speech tags are transferred from retrieval - result snippets to queries at training time .Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .We describe a shared task on parsing web text from the Google Web Treebank .", "label": "", "metadata": {}}
{"text": "There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .System combination approaches achieved the best results , however , falling short of newswire accuracies by a large margin .The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .We propose a multi - pass coarse - to - fine architecture for dependency parsing using linear - time vine pruning and structured prediction cascades .", "label": "", "metadata": {}}
{"text": "We observe speed - ups of up to two orders of magnitude compared to exhaustive search .Our pruned third - order model is twice as fast as an unpruned first - order model and also compares favorably to a state - of - the - art transition - based parser for multiple languages .To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .", "label": "", "metadata": {}}
{"text": "We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .This allows us to extend a standard supervised learning objective with additional loss - functions , either based on intrinsic or task - specific extrinsic measures of quality .We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .", "label": "", "metadata": {}}
{"text": "Unfortunately , most state - of - the - art constituency parsers employ large probabilistic context - free grammars for disambiguation , which renders them impractical for real - time use .Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .We use the Compute Unified Device Architecture ( CUDA ) programming model to reimplement a state - of - the - art parser , and compare its performance on two recent GPUs with different architectural features .", "label": "", "metadata": {}}
{"text": "We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser .Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .", "label": "", "metadata": {}}
{"text": "We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .", "label": "", "metadata": {}}
{"text": "2010 ) .Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .We describe a new scalable algorithm for semi - supervised training of conditional random fields ( CRF ) and its application to part - of - speech ( POS ) tagging .The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .", "label": "", "metadata": {}}
{"text": "Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .What is less known is that some parsers suffer more from domain shifts than others .We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .", "label": "", "metadata": {}}
{"text": "With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .We show that increasing the quality of the automatically parsed data used for self - training gives higher accuracy self - trained grammars .Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .", "label": "", "metadata": {}}
{"text": "The product model is most effective when the individual underlying grammars are most diverse .Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .We build a multiview learning objective that enforces agreement between monolingual and bilingual models .In our method the first , monolingual view consists of supervised predictors learned separately for each language .", "label": "", "metadata": {}}
{"text": "Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .For syntactic parsing , our bilingual predictor increases F1 by 2.1 % absolute , and retraining a monolingual model on its output gives an improvement of 2.0 % .We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .", "label": "", "metadata": {}}
{"text": "Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .However , any single pruning mask will introduce bias .We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .", "label": "", "metadata": {}}
{"text": "Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .In this paper we highlight the commonalities and the differences between the two learning paradigms .State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .", "label": "", "metadata": {}}
{"text": "First , how can we learn highly complex models ?Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .Each subsequent model refines the previous one , until a final , full - complexity model is reached .Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .", "label": "", "metadata": {}}
{"text": "In the domain of syntactic parsing , complexity is in the grammar .We present a latent variable approach which begins with an X - bar grammar and learns to iteratively refine grammar categories .For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .In the domain of acoustic modeling for speech recognition , complexity is needed to model the rich phonetic properties of natural languages .", "label": "", "metadata": {}}
{"text": "Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .In the domain of machine translation , complexity arises because there and too many target language word types .To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .We propose a multipass , coarse - to - fine approach in which the language model complexity is incrementally introduced .", "label": "", "metadata": {}}
{"text": "Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .We present a discriminative , latent variable approach to syntactic parsing in which rules exist at multiple scales of refinement .The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .", "label": "", "metadata": {}}
{"text": "In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .To enable downstream language processing , automatic speech recognition output must be segmented into its individual sentences .Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .", "label": "", "metadata": {}}
{"text": "While some previous work has included syntactic features , ours is the first to do so in a tractable , lattice - based way , which is crucial for scaling up to long - sentence contexts .Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .The resulting system significantly outperforms the most popular long - span model for sentence segmentation ( the hidden event language model ) on both reference text and automatic speech recognizer output from news broadcasts .", "label": "", "metadata": {}}
{"text": "In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .The learning procedure directly maximizes the likelihood of the training treebank , without the use of any language specific or linguistically constrained features .Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .", "label": "", "metadata": {}}
{"text": "On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .In our approach , an initial monophone HMM is iteratively refined using a split - merge EM procedure which makes no assumptions about subphone structure or context - dependent structure , and which uses only a single Gaussian per HMM state .Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .", "label": "", "metadata": {}}
{"text": "We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .On synthetic data , we recover the correct grammar without having to specify its complexity in advance .We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .", "label": "", "metadata": {}}
{"text": "We describe a method in which a minimal grammar is hier- archically refined using EM to give accurate , compact gram- mars .The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .We present several improvements to unlexicalized parsing with hierarchically state - split PCFGs .", "label": "", "metadata": {}}
{"text": "In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .Second , we compare various inference procedures for state - split PCFGs from the standpoint of risk minimization , paying particular attention to their practical tradeoffs .Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .The multimedia data was processed in three ways : the audio signal was converted to a sequence of acoustic features , automatic speech recognition provided a word - level transcription , and image features were computed for selected frames of the video signal .", "label": "", "metadata": {}}
{"text": "Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .We present experimental results from the TREC video retrieval evaluation .We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .In contrast with previous work , we are able to split various terminals to different degrees , as appropriate to the actual complexity in the data .", "label": "", "metadata": {}}
{"text": "On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .While most work on parsing with PCFGs has focused on local correlations between tree configurations , we attempt to model non - local correlations using a finite mixture of PCFGs .A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .", "label": "", "metadata": {}}
{"text": "Hand gestures are examples of fast and complex motions .Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .We illustrate this idea with experiments on hand motions similar to finger spelling .The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .", "label": "", "metadata": {}}
{"text": "This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .The thesis covers several independent topics : - Automatic White Balance : It is shown how to improve the white balancing of an omni - directional camera by using a reference color and a PID - controller . -Ball Tracking :", "label": "", "metadata": {}}
{"text": "Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .We propose a novel method for fusing the sensor data and show our results through examples of selflocalization . -Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .", "label": "", "metadata": {}}
{"text": "Background .Applications of Natural Language Processing ( NLP ) technology to biomedical texts have generated significant interest in recent years .In this paper we identify and investigate the phenomenon of linguistic subdomain variation within the biomedical domain , i.e. , the extent to which different subject areas of biomedicine are characterised by different linguistic behaviour .While variation at a coarser domain level such as between newswire and biomedical text is well - studied and known to affect the portability of NLP systems , we are the first to conduct an extensive investigation into more fine - grained levels of variation .", "label": "", "metadata": {}}
{"text": "Using the large OpenPMC text corpus , which spans the many subdomains of biomedicine , we investigate variation across a number of lexical , syntactic , semantic and discourse - related dimensions .These dimensions are chosen for their relevance to the performance of NLP systems .We use clustering techniques to analyse commonalities and distinctions among the subdomains .Conclusions .We find that while patterns of inter - subdomain variation differ somewhat from one feature set to another , robust clusters can be identified that correspond to intuitive distinctions such as that between clinical and laboratory subjects .", "label": "", "metadata": {}}
{"text": "We conclude that an awareness of subdomain variation is important when considering the practical use of language processing applications by biomedical researchers .Background .Overview of biomedical natural language processing .Research in the field of NLP is concerned with the development of systems that take textual data ( e.g. , research articles or abstracts ) as input and/or output .Examples of these systems encompass \" core \" tasks that often provide components of larger systems , such as syntactic analysis or semantic disambiguation , as well as practical applications for tasks such as summarisation , information extraction and translation .", "label": "", "metadata": {}}
{"text": "Increasingly sophisticated systems for both core tasks and applications are being introduced through academic venues such as the annual BioNLP workshops [ 1 ] and also in the commercial marketplace .This meeting of fields has proven mutually beneficial : biologists more than ever rely on automated tools to help them cope with the exponentially expanding body of publications in their field , while NLP researchers have been spurred to address important new problems in theirs .Among the fundamental advances from the NLP perspective has been the realisation that tools which perform well on textual data from one source may fail to do so on another unless they are tailored to the new source in some way .", "label": "", "metadata": {}}
{"text": "In this paper we study the phenomenon of subdomain variation , i.e. , the ways in which language use differs in different subareas of a broad domain such as \" biomedicine \" .Using a large corpus of biomedical articles , we demonstrate that observable linguistic variation does occur across biomedical subdisciplines .Furthermore , the dimensions of variation that we identify cover a wide range of features that directly affect NLP applications ; these correspond to variation on the levels of lexicon , semantics , syntax and discourse .In the remainder of this section - before moving on to describe our methods and results - we motivate our work by summarising related prior research on corpus - based analysis of domain and subdomain variation and on the recognised problem of domain adaptation in natural language processing .", "label": "", "metadata": {}}
{"text": "The notion of domain relates to the concepts of topic , register and genre that have long been studied in corpus linguistics [ 2 ] .In the field of biomedical NLP , researchers are most often concerned with the genre of \" biomedical research articles \" .There is also a long history of NLP research on clinical documentation , frequently with a focus on extracting structured information from free - text notes written by medical practitioners [ 3 - 6 ] .A number of researchers have explored the differences between non - technical and scientific language .", "label": "", "metadata": {}}
{"text": "Firstly , in academic writing additional information is most commonly integrated by modification of phrases rather than by the addition of extra clauses .For example , academic text may use the formulations the participant perspective and facilities for waste treatment where general - audience writing would be more likely to use the perspective that considers the participant 's point of view and facilities that have been developed to treat waste .Secondly , academic writing places greater demands on the reader by omitting non - essential information , through the frequent use of passivisation , nominalisation and noun compounding .", "label": "", "metadata": {}}
{"text": "We now turn to corpus studies that focus on biomedical writing .Verspoor et al .[ 8 ] use measurements of lexical and structural variation to demonstrate that Open Access and subscription - based journal articles in a specific domain ( mouse genomics ) are sufficiently similar that research on the former can be taken as representative of the latter .While their primary goal is different from ours and they do not consider variation across multiple different domains , they do compare their mouse genomics corpus with small reference corpora drawn from newswire and general biomedical sources .", "label": "", "metadata": {}}
{"text": "[ 9 ] document the \" sublanguages \" associated with two biomedical domains : clinical reports and molecular biology articles .They set out restricted ontologies and frequent co - occurrence templates for the two domains and discuss the similarities and differences between them , but they do not perform any quantitative analysis .Other researchers have focused on specific phenomena , rather than cataloguing a broad scope of variation .Cohen et al .[ 11 ] carry out a detailed analysis of argument realisation with respect to verbs and nominalisations , using the GENIA and PennBioIE corpora .", "label": "", "metadata": {}}
{"text": "Domain effects in Natural Language Processing .Recent years have seen an increased research interest in the effect of domain variation on the effectiveness of Natural Language Processing ( NLP ) technology .A fundamental assumption of statistical methods is that the data used to train a system has the same distribution as the data that will be used when applying or evaluating the system .When this assumption is violated there is no guarantee that performance will generalise well from that observed on the training data and in practice a decrease in performance is usually observed .The dimensions of variation that directly affect a given statistical tool will depend on the application and methodology involved .", "label": "", "metadata": {}}
{"text": "In many NLP tasks , a standard set of human - annotated data is used to evaluate and compare systems .These data sets are often drawn from a single register or topical domain , news text being the most common due to its availability in large quantities .For example , syntactic parsers are usually trained and evaluated on the Wall Street Journal portion of the Penn Treebank , though it is known that this gives an overoptimistic view of parser accuracy [ 13 , 14 ] .For example , Gildea [ 13 ] demonstrates that a parser trained on the Wall Street Journal section of the Penn Treebank suffers a significant drop in accuracy when tested on the Brown corpus section of the Treebank , which is composed of general American English text .", "label": "", "metadata": {}}
{"text": "When considering the transfer of NLP tools and techniques to biomedical text processing applications , the distance between source and target domains is far greater than that between the Brown and WSJ corpora or between film and electronics reviews on an on - line retailer 's website .As described in the previous section , the language of biomedical text differs from general language in many diverse ways , making an awareness of variation effects crucial .Two strategies are available to developers of tools for statistical biomedical text processing : creating a new annotated corpus of domain - specific data , and \" adapting \" a model trained on an existing out - of - domain data set to the domain of interest .", "label": "", "metadata": {}}
{"text": "There are many examples of corpora constructed to facilitate the implementation and evaluation of tools for specific problems in biomedical language processing , for example the BioScope corpus [ 21 ] for speculative language detection and the BioCreative I and II gene normalisation corpora [ 22 , 23 ] .There are also text collections that have been annotated for multiple tasks , most notably GENIA [ 24 ] , PennBioIE [ 25 ] and BioInfer [ 26 ] .One common feature of these corpora is that they have been compiled from just one or two specific subject areas , typically molecular biology .", "label": "", "metadata": {}}
{"text": "PennBioIE is also a corpus of abstracts , in this case covering topics in cancer genomics and the behaviour of enzymes affecting a particular family of proteins .BioInfer contains 1,100 sentences that relate to protein - protein interactions .While these are without a doubt extremely valuable resources for application building , their limited coverage casts doubt on the assumption that a system that performs well on one will also perform well on biomedical text in general .One of the central questions addressed in the present paper is how representative a corpus restricted to a single subdomain of biomedical text can be of the overall biomedical domain .", "label": "", "metadata": {}}
{"text": "We now describe the implementation details of our study : first , we present the OpenPMC corpus of biomedical text and its division into the subdomains that constituted our basic units of enquiry .Second , we enumerate the linguistic features we considered , and explain how we extracted them from the corpus .Third , we describe our choice of metric for measuring divergence between subdomains , and our approach to gauging its statistical significance .Fourth , we describe the clustering method we used on the raw feature distributions .Finally , we explain how the results of these methods are presented graphically .", "label": "", "metadata": {}}
{"text": "The Open Access Subset of PubMed ( OpenPMC ) is the largest publicly available corpus of full - text articles in the biomedical domain [ 27 ] .OpenPMC is comprised of 169,338 articles drawn from 1,233 medical journals indexed by the Medline citation database , totalling approximately 400 million words .Articles are formatted according to a standard XML tag set [ 28 ] .The National Institute of Health ( NIH ) maintains a one - to - many mapping from journals to 122 subdomains of biomedicine [ 29 ] .The mapping covers about a third of the OpenPMC journals , but these account for over 70 % of the total data by word count .", "label": "", "metadata": {}}
{"text": "( Figure 1 ) .Our data set is composed of journals that are assigned a single subdomain .To ensure sufficient data for comparing a variety of linguistic features , we discarded the subdomains with less than one million words of data .This makes for a total of 342 journals in 38 biomedical subdomains .We also added a reference subdomain , \" Newswire \" , composed of a 6 million word random sample from the Gigaword corpus .These subdomains were our initial objects of comparison .Distribution of OpenPMC data by subdomain .OpenPMC word count for the subdomains we consider : green coloring indicates data mapped to a single subdomain , orange indicates two subdomains , and red indicates three or more .", "label": "", "metadata": {}}
{"text": "Feature choice motivation .We considered subdomain variation across a range of lexical , syntactic , semantic , sentential and discourse features .Here , we motivate our choices and point to NLP applications that make use of specific features , and hence are potentially affected by their variation .Lexical features .Differences in vocabulary are what first come to mind when defining subdomains , and to measure this we considered lemma frequencies .A lemma is a basic word - form that abstracts beyond inflection : for example , the tokens \" runs \" , \" run \" and \" running \" would all be considered instances of the verb lemma \" run \" .", "label": "", "metadata": {}}
{"text": "Lexical features are fundamental to methods for text classification [ 30 ] , language modelling [ 31 ] and most modern parsing approaches [ 14 , 32 , 33 ] .These systems may therefore be affected by variations in lexical distributions , either as a result of misestimating frequencies or out - of - vocabulary effects .Part - of - speech ( POS ) tags capture lexical properties not preserved by lemmatisation , such as singular vs. plural and passive vs. active , as well as various function words .At the same time , POS tags abstract over potentially large classes of words such as the class of all common nouns .", "label": "", "metadata": {}}
{"text": "POS tags reflect several known features of scientific language , such as pronominal usage , verb tense and punctuation .POS tagging is a first step in many NLP tasks , such as morphological analysis and production [ 34 ] and constructing lexical databases [ 35 ] .Syntactic features .Lexical categories that describe a word 's combinatorial properties are essential to the success of some classes of lexicalised statistical parser .In the framework of combinatory categorial grammar ( CCG ) lexical categories are the essential bridge between the lexical and syntactic levels , encoding information on how a lexical item combines with its neighbours to form syntactic structures [ 36 ] .", "label": "", "metadata": {}}
{"text": "For example , the most frequent CCG category for the verb \" run \" is \" ( S[b]\\NP)/NP \" , which indicates it combines with a noun phrase to the right , then to the left , to form a sentence .CCG categories have been proposed as a good level for hand - annotation when re - training lexicalised parsers for new domains [ 19 ] , as they provide syntactic information while remaining relatively easy for non - experts to label , compared e.g. to full sentence parse - trees .Changes in their distribution would affect parsing accuracy , but could be a tractable starting - point for domain adaptation if the problem is anticipated .", "label": "", "metadata": {}}
{"text": "More complex sentences may include multiple clauses , and so further distinctions are made between clausal and non - clausal arguments ( e.g. the \" nc \" in \" ncsubj \" ) .GR distributions will reflect characteristic syntactic preferences in a domain , such as the preference for modification observed by Biber and Gray [ 7 ] in scientific text .Variation in GR distributions across subdomains may be expected to degrade parsing performance and necessitate model adaptation .Semantic features .Semantic features capture what a text is \" about \" at a more general and interpretable level than individual lexical features .", "label": "", "metadata": {}}
{"text": "These topics provided a bottom - up vocabulary for investigating semantics in the corpus that is complementary to the top - down vocabulary provided by the NIH subject headings .We also investigated a more specific kind of semantic behaviour relating to verb - argument predication .This was motivated by the observation that relations between verbs and their arguments are central to important semantic tasks such as semantic role labelling [ 37 , 38 ] .Variation in the pattern of verb - argument relations across subdomains is likely to indicate difficulty in porting tools from one subdomain to another .", "label": "", "metadata": {}}
{"text": "Sentence length is known to roughly correlate with parsing difficulty and syntactic complexity [ 39 ] .Noun phrase ( NP ) length increases as more information is \" packed \" via pre-/post - modification .Scientific language is known to aim for high information density [ 7 , 40 ] .Pronominal usage , which is touched on by POS tags , can be a stylistic indicator of scientific writing at a finer level , e.g. the avoidance of personal pronouns in laboratory sciences , and the restriction of gendered pronouns mainly to clinical sciences [ 40 ] .", "label": "", "metadata": {}}
{"text": "- reference resolution is crucial to many information extraction applications where valuable information may be linked to a referent in this fashion .Nguyen and Kim [ 12 ] compare the use of pronouns in newswire and biomedical text , using the GENIA corpus as representative of the latter , and found significant differences .Moreover , they improved the performance of a pronoun resolution system by tailoring it based on their findings , which demonstrates the practical value in considering these features .Feature extraction .Lexical and syntactic features .We first converted each OpenPMC article from XML to plain text , ignoring \" non - content \" elements such as tables and formulae , and split the result into sentences , aggregating the results by subdomain .", "label": "", "metadata": {}}
{"text": "C&C uses the morpha morphological analyser [ 34 ] , maximum entropy labellers for tagging and supertagging and a log - linear parse model .RASP - parser - style [ 42 ] grammatical relations were extracted from C&C output using deterministic rules .Tables 1 and 2 show the system 's output for the sentence \" Multiple twinning in cubic crystals is represented geometrically \" .Grammatical relations extracted from the sentence \" Multiple twinning in cubic crystals is represented geometrically \" using the C&C parser .From this output we simply counted occurrences of noun , verb , adjective and adverb lemmas , POS tags , GRs and CCG categories .", "label": "", "metadata": {}}
{"text": "We experimented with filtering low - frequency items at various thresholds , to reduce noise and improve processing speed , and settled on filtering items that occur less than 150 times in the entire corpus .Sentential and discourse features .We measured average sentence , noun phrase and base nominal lengths ( in tokens ) for each subdomain , using the parsed output from C&C.In order to filter out lines that are not true sentences , we ignored lines containing less than 50 % lowercase letters .Sentence length is defined as the number of non - punctuation tokens in a sentence .", "label": "", "metadata": {}}
{"text": "Base noun phrase length is simply the number of tokens contained in the head noun and all premodifying tokens .To filter out pleonastic pronouns we used a combination of the C&C parser 's pleonasm tag and heuristics based on Lappin and Leass [ 43 ] .Semantic features .To facilitate a robust analysis of semantic differences , we induced a \" topic model \" using Latent Dirichlet Analysis ( LDA ) [ 44 ] .LDA models each document in a corpus as a mixture of distributions over words , or \" topics \" .As preprocessing we divided the corpus into its constituent articles , removing stopwords and words shorter than 3 characters .", "label": "", "metadata": {}}
{"text": "We collated the predicted distribution over topics for each article in a subdomain , weighted by article word count , to produce a topic distribution for the subdomain .An alternative perspective on semantic behaviour is provided by mapping the distribution of syntactically - informed classes of verbs across subdomains .These classes are learned by generalising over the nouns taken by verbs as subject arguments and as direct object arguments in the corpus .The learning method is a topic model similar to the LDA selectional preference model of \u00d3 S\u00e9aghdha [ 46 ] , though instead of associating each verb with a distribution over noun classes , here each noun is associated with a distribution over verb classes .", "label": "", "metadata": {}}
{"text": "By learning classes directly from the corpus , we induced a classification that reflects the characteristics of biomedical text and its subdomains .For each grammatical relation considered ( subject and direct object ) , 100 verb classes were induced and every instance of the relation in the corpus was associated with a single class .Measuring divergence .Our goal is to illustrate the presence or absence of significant differences among the subdomains for each feature set .The feature sets ( with the exception of the sentential and discourse features ) are represented as probability distributions .We therefore calculate the Jensen - Shannon divergence ( JSD ) [ 49 ] for each feature set between each subdomain .", "label": "", "metadata": {}}
{"text": "JSD values range between 0 ( identical distributions ) and 1 ( disjoint distributions ) .Random sampling for intra - subdomain divergence .Comparability of JSD values is dependent on the dimensionality of the distributions being compared : approximations of significance break down with large dimensionality [ 50 ] .Our feature sets vary widely in this respect , from 46 ( POS ) to over 20,000 ( nouns ) .We therefore compute significance scores based on random sampling of the subdomains .For each subdomain , we divide its texts into units of 200 contiguous sentences , and build 101 million - word samples by drawing randomly from these units .", "label": "", "metadata": {}}
{"text": "This gives us 10,000 JSD values calculated between random articles drawn from this subdomain ( hereafter called \" intra - subdomain \" , in contrast to \" inter - subdomain \" ) .The significance of an inter - subdomain JSD value X between subdomains A and B is the proportion of intra - subdomain JSD values from A and B that are less than X .Basically , this uses the null hypothesis that the variation between the two subdomains is indistinguishable from random variation within the subdomains .Additionally , the intra - subdomain JSD values can be used by themselves to indicate how homogeneous a subdomain is with respect to the given feature set .", "label": "", "metadata": {}}
{"text": "Clustering .To find natural groupings of the subdomains , we perform K - means clustering directly on the distributions , using the Gap statistic [ 52 ] to choose the value for K .The Gap statistic uses within - cluster error and random sampling to find optimal parameters tailored to the data set .A typical measurement of within - cluster error , the sum of squared differences between objects and cluster centres , is compared with the performance on a data set randomly generated with statistical properties similar to the actual data set .As K increases , performance on both data sets improves , but should improve more dramatically on the actual data set as K approaches a natural choice for cluster count .", "label": "", "metadata": {}}
{"text": "Presentation .The non - distributional sentential and discourse features are directly reported as tables .The JSD values for the lexical and syntactic feature sets are presented in four figures per feature set : a heat map , a dendrogram , a distributional line plot , and a scatter plot .Heat maps .Our heat maps show three types of values : the top half shows JSD values between pairs of subdomains .The bottom half shows the significance of the JSD values ( the probability that the variation does not occur by chance ) , calculated as described in the section \" Random sampling for intra - subdomain divergence \" above .", "label": "", "metadata": {}}
{"text": "In all cases , the actual values are inscribed in each square .The significance scores are shaded from white ( 100 % significance ) to black ( 0 % significance ) .The JSD values are shaded from white ( highest JSD value for the feature set ) to black ( lowest JSD value for the feature set ) .In other words , white indicates more absolute variation ( top half ) and higher significance ( bottom half ) .Dendrograms .Dendrograms present the results of hierarchical clustering performed directly on the JSD values ( i.e. from the top half of the heat map ) .", "label": "", "metadata": {}}
{"text": "The order of these merges is recorded as a tree structure that can be visualised as a dendrogram in which the length of a branch represents the distance between its child nodes .Similarity between clusters is calculated using average cosine distance between all members , known as \" average linking \" .The tree leaves represent data instances ( subdomains ) and the paths between them are proportional to the pairwise distance .This allows visualization of multiple potential clusterings , as well as a more intuitive sense of how distinct clusters truly are .Rather than choosing a set number of flat clusters , the trees mirror the nested structure of the data .", "label": "", "metadata": {}}
{"text": "The line plots present the distribution of intra - subdomain JSD values , with each line representing a subdomain .Higher values for one subdomain versus another shows that its texts have more variety with respect to that feature .Scatter plots .The scatter plots project the optimal K - Means clustering onto the first two principal components of the data .The components are normalised , and points coloured according to cluster membership , with the subdomain written immediately above .The \" Newswire \" subdomain is not included in the plots : as an outlier , it compresses the subdomains into unreadability .", "label": "", "metadata": {}}
{"text": "Results and Discussion .General observations .The most striking general trend is the strong similarity between Biochemistry , Genetics and Molecular Biology .These subdomains form the most consistent cluster across feature sets , and are often one of the most closely - related triplets in the dendrograms ( e.g. adjectives , Figure 2 , topic modelling , Figure 3 ) .As mentioned previously , these subdomains are the basis for most annotated resources for BioNLP .Our results suggest that not only are these resources tuned for a handful of subdomains , but these subdomains exhibit a narrow range of linguistic behaviour , less representative of other biomedical subdomains .", "label": "", "metadata": {}}
{"text": "Distributions over adjective lemmas as tagged by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .", "label": "", "metadata": {}}
{"text": "Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .Distributions over noun lemmas as tagged by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .", "label": "", "metadata": {}}
{"text": "The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .Distributions over grammatical relations extracted by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .", "label": "", "metadata": {}}
{"text": "The line plot shows the intra - subdomain spread of JSD values generated by random sampling .The heatmaps show that , for all feature sets , the variation is significant between nearly all pairs of subdomains ( exceptions are discussed below ) .The intra - subdomain variation is much greater and more diverse for the vocabulary features than for the POS and GR features , with the CCG features in between .The Science subdomain 's generalist scope ( encompassing journals such as Science and Endeavour ) gives it unusually high intra - subdomain scores , and we do n't consider it further .", "label": "", "metadata": {}}
{"text": "Some clusters of subdomains recur across features , and we present a useful breakdown in Table 3 : these subdomain clusters are present in the optimal clustering for at least 8/10 of the feature sets .The first cluster includes subdomains dealing primarily with microscopic processes and can be further subdivided into groupings of biochemical ( Biochemistry , Genetics ) and cellular ( Cell Biology , Embryology ) study .The second cluster includes subdomains focused on specific anatomical systems ( Endocrinology , Pulmonary Medicine ) .The third cluster includes subdomains focused on clinical medicine ( Psychiatry ) or specific patient - types ( Geriatrics , Pediatrics ) .", "label": "", "metadata": {}}
{"text": "This is almost always the most distant cluster from the rest of PMC and usually the closest to Newswire .Properties of the feature sets .We now consider each feature set in terms of the significance of variation and clustering of subdomains .Over- and Under - use of lexical items .Before considering the lexical feature sets , we discuss a phenomenon noticed when examining the lemmas that most characterize each cluster .We compiled lemmas with extreme log - likelihood values , indicating unusual behavior relative to the corpus average [ 53 ] .We noted that they tend to define their clusters by over- or under - use of lemmas relative to the corpus average , with some favouring one extreme or the other .", "label": "", "metadata": {}}
{"text": "Conversely , common verbs that are used with subdomain - specific meanings show over- and under - use ( we give examples of this in the following section ) .These two types of variation , the introduction of completely new nouns and the modified behaviour of common verbs , call for different adaptation techniques .For example , self - training can be used to re - estimate distributional properties of common verbs but may be less successful at handling the out - of - vocabulary effects caused by unseen nouns .Lexical features .Noun distributions ( Figure 4 ) show the highest inter - subdomain divergence .", "label": "", "metadata": {}}
{"text": "Despite high intra - subdomain variation , only one pair of subdomains have a JSD value that is not statistically significant at the 99 % level : Genetics and Molecular Biology .Adjective distributions ( Figure 2 ) also have high divergence within and between subdomains .Again , genetics - related subdomains show insignificant differences , as do Virology and Microbiology .In general , we see nouns and adjectives give common - sense semantic pairings ( Tropical Medicine and Communicable Disease , Genetics and Molecular Biology ) and sharply distinguish the \" social sciences \" from the rest .", "label": "", "metadata": {}}
{"text": "The k - means clusters are similar to those for nouns , with the microscopic sciences ( cellular and biochemical ) merged into one cluster .Unlike nouns , the characteristic features include both over- and under - used terms , such as \" clinical \" and \" medical \" .Verb distributions ( Figure 6 ) have lower JSD values , but these remain significant due to lower intra - subdomain scores .The verb clusters generally agree with the noun clusters , although sometimes emphasise different similarities ( e.g. Vascular Disease and Critical Care are closer together ) .", "label": "", "metadata": {}}
{"text": "It is also interesting that these particular verbs have specialised meanings in certain subdomains , suggesting a corresponding major shift in frequency when there is a shift in meaning .Adverbs ( Figure 7 ) have the lowest JSD values of the lemma types .The subdomains are distinguished by two types of adverbs : markers of scientific discourse , and domain - specific premodiffers .The former include lemmas like \" previously \" , \" significantly \" and \" experimentally \" , with further distinctions between more qualitative and quantitative subdomains .The latter include lemmas like \" intraperitoneally \" and \" immunohistochemically \" , which are used to avoid the more complex syntax of relative clauses and leverage the specific knowledge of its audience .", "label": "", "metadata": {}}
{"text": "Distributions over verb lemmas as tagged by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .", "label": "", "metadata": {}}
{"text": "Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .", "label": "", "metadata": {}}
{"text": "Biomedical Engineering , Medical Informatics and Therapeutics make particular use of present tense verbs and determiners , and markedly less of past tense .The cluster including Communicable Disease and Critical Care shows the opposite trend , perhaps reflecting certain subdomains ' use of narrative .In notable contrast to the other feature sets , Tropical Medicine is not clustered with Communicable Disease , belonging instead to a cluster with distinctive overuse of comparative adjectives , foreign words and Wh - pronouns .The \" laboratory science \" cluster also uses many foreign words , but avoids Wh - pronouns .", "label": "", "metadata": {}}
{"text": "The clusters , however , are less interpretable : there are similarities with the lemma clusters , but oddities are mixed in .For example , while Tropical Medicine , Communicable Disease and Veterinary Medicine are still closely related , Pulmonary Medicine is close as well ( and , as mentioned , k - means decides to split the first two , contrary to other feature sets ) .Distributions over parts of speech as tagged by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .", "label": "", "metadata": {}}
{"text": "The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .Syntactic features .The GR features ( Figure 5 ) have similarities with the POS features , and overlapping interpretations : for example , both capture the over - usage of determiners by Biomedical Engineering and Medical Informatics .More unusual is that their cluster includes Ethics and Education , due to high usage of clausal modifiers .", "label": "", "metadata": {}}
{"text": "It also may indicate that Biomedical Engineering and Medical Informatics retain aspects of both scientific and general language syntax .Subdomains extremely far from the centre ( top - left ) , e.g. Endocrinology and Vascular Disease , are never grouped together in the lexical features .These outliers have particularly long average sentence length ( Vascular Disease has the longest of all the subdomains ) , suggesting a relationship between GR frequencies and sentence length .However , exceptions to this ( e.g. Gastroenterology ) indicate the relationship is more complex , and requires more detailed analysis .The CCG categories ( Figure 9 ) show the same relationship with long sentence length as GRs .", "label": "", "metadata": {}}
{"text": "As mentioned previously , the distribution of intra - subdomain JSD values for CCG shows intermediate behaviour between the open - class lexical features and closed - class features , which reflects their lexical - syntactic nature .The similarities in cluster results to both the lexical and GR features demonstrates this further .Distributions over CCG categories extracted by the C&C parser trained on Genia .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .", "label": "", "metadata": {}}
{"text": "The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The line plot shows the intra - subdomain spread of JSD values generated by random sampling .Sentential and discourse features .Table 4 shows each subdomain 's average sentence , base noun phrase , and full noun phrase lengths ( in tokens ) .As mentioned in the previous section , sentence length correlates with certain aspects of the syntactic features , particularly at longer sentence lengths .", "label": "", "metadata": {}}
{"text": "Microscopic and computational sciences , particularly Biotechnology , use long noun phrases .Newswire is towards the middle , while the social sciences form the shorter / infrequent end of the spectrum .Table 5 shows the frequency of pronominal / co - referential terms in each subdomain , and three finer - grained distinctions within co - reference : gendered third person ( personal ) , neuter 3rd person ( non - personal ) , and anaphoric determiners .Pronominal usage is highest for Newswire , Ethics and Education , reflecting scientific language 's tendency towards explicitness .Personal pronouns are far more frequent in Newswire than any biomedical subdomain , due to scientific language 's concern with objectivity .", "label": "", "metadata": {}}
{"text": "Non - personal pronouns are frequently used in the social subdomains , in contrast to other biomedical subdomains and Newswire .Finally , anaphoric determiners are much less frequent in Newswire than any of the biomedical subdomains , and usage also increases dramatically moving from social subdomains to microscopic subdomains , where they account for over half of coreferential terms ( Virology and Microbiology ) .This wide range ( from 8 % to 51 % ) could severely impact coreference resolution and systems that depend on it , such as information and relationship extraction .Semantic features .A reasonable first expectation is for the topic modelling results ( Figure 3 ) to be similar to the lexical features .", "label": "", "metadata": {}}
{"text": "Since the subject ( Figure 10 ) and direct object ( Figure 11 ) based verb cluster distributions are derived from distributions of nouns and verbs , it would be reasonable to expect them to have similarities with these feature sets .The relationship is less pronounced than between nouns and the topic models : a similar comparison of subdomain pairs has lower agreement ( 4/12 between nouns and both types of verb clusters , 6/12 and 7/12 between verbs and direct object - based and verbs and subject - based clusters , respectively ) .More significantly , the differences are often more dramatic than interpolating one or two other subdomains .", "label": "", "metadata": {}}
{"text": "Systems that use lexical frequency information at the level of syntactic arguments may need to adjust their model to account for this .Distributions over verb classes built from clustering on the semantics of the subject .Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .", "label": "", "metadata": {}}
{"text": "Clockwise from the top left : the heatmap shows the pairwise Jensen - Shannon Divergence ( top half ) and statistical significance ( bottom half ) , as well as the homogeneity ( diagonal ) .The dendrogram shows hierarchical clustering based on cosine difference between each subdomain 's JSD values .The scatter plot is colored according to the best K - means clustering ( determined by the Gap statistic ) projected onto the first two principal components ( normalized ) .The relationship between some subdomains is stronger when considering one selectional preference versus another .For example , the similarity of Critical Care and Vascular Disease is higher for selectional preferences of subject than direct object .", "label": "", "metadata": {}}
{"text": "This may reflect higher usage of subdomain - specific vocabulary in particular argument positions , but this needs more in - depth scrutiny to draw detailed conclusions about selectional preferences .It does , however , show the potential importance of considering the semantics of different verbal arguments when adapting to these subdomains .Conclusions .In this paper we have identified the phenomenon of subdomain variation and studied how it manifests itself in the domain of biomedical language .As far as we are aware , this is the first time that subdomain analysis has been applied to a corpus spanning an entire scientific domain and the first time that it has been performed with a focus on implications for natural language processing applications .", "label": "", "metadata": {}}
{"text": "As interest in biomedical applications of NLP continues and NLP systems are deployed in increasingly varied contexts , we expect the study of subdomain variation to become ever more important .One direction for future work is to directly measure the effect of subdomain variation on the performance of NLP systems for various tasks .Declarations .Acknowledgements .This work was supported by EPSRC grant EP / G051070/1 and the Royal Society , UK .Authors ' contributions .TL and DO collected and pre - processed the corpora and performed the feature extraction .TL carried out the distribution and clustering experiments and designed heat maps , dendrograms , plots and tables included this paper .", "label": "", "metadata": {}}
{"text": "All the authors took part in the analysis of the results and in the write - up of the paper .All authors read and approved this document .Authors ' Affiliations .Computer Laboratory , University of Cambridge .References .Biber D : Variation Across Speech and Writing .Cambridge : Cambridge University Press ; 1988 .Hirschman L , Sager N : Automatic information formatting of a medical sublanguage .In Sublanguage : Studies of Language in Restricted Semantic Domains .Edited by : Kittredge R , Lehrberger J. Berlin : Walter de Gruyter ; 1982 .", "label": "", "metadata": {}}
{"text": "Journal of the American Medical Informatics Association 1994 , 1 ( 2 ) : 142 - 160 .PubMed View Article .Friedman C , Shagina L , Lussier Y , Hripcsak G : Automated encoding of clinical documents based on natural language processing .Journal of the American Medical Informatics Association 2004 , 11 ( 5 ) : 392 - 402 .PubMed View Article .Roberts A , Gaizauskas R , Hepple M , Guo Y : Mining clinical relationships from patient narratives .BMC Bioinformatics 2008 , 9 ( Suppl 11 ) : S3 .PubMed View Article .", "label": "", "metadata": {}}
{"text": "Journal of English for Academic Purposes 2010 , 9 : 2 - 20 .View Article .Verspoor K , Cohen KB , Hunter L : The textual characteristics of traditional and Open Access scientific journals are similar .BMC Bioinformatics 2009 , 10 : 183 .PubMed View Article .Friedman C , Kraa P , Rzhetsky A : Two biomedical sublanguages : a description based on the theories of Zellig Harris .Journal of Biomedical Informatics 2002 , 35 ( 4 ) : 222 - 235 .PubMed View Article .Proceedings of the NAACL - HLT-10 2nd", "label": "", "metadata": {}}
{"text": "Cohen KB , Palmer M , Hunter L : Nominalization and alternations in biomedical language .PLoS ONE 2008 , 3 ( 9 ) : e3158 .PubMed View Article .Nguyen NL , Kim JD : Exploring domain differences for the design of a pronoun resolution system for biomedical text .Proceedings of COLING-08 , Manchester , UK 2008 .Gildea D : Corpus variation and parser performance .Proceedings of EMNLP-01 , Pittsburgh , PA 2001 .Clark S , Curran JR : Formalism - independent parser evaluation with CCG and DepBank .Proceedings of ACL-07 , Prague , Czech Republic 2007 .", "label": "", "metadata": {}}
{"text": "Proceedings of ACL-07 , Prague , Czech Republic 2007 .Daum\u00e9 H III , Marcu D : Domain adaptation for statistical classifiers .Journal of Artificial Intelligence Research 2006 , 26 : 101 - 126 .Jiang J , Zhai C : Instance weighting for domain adaptation in NLP .Proceedings of ACL-07 , Prague , Czech Republic 2007 .Finkel JR , Manning CD : Hierarchical Bayesian domain adaptation .Proceedings of HLT - NAACL-09 , Boulder , CO 2009 .Rimell L , Clark S : Porting a lexicalized - grammar parser to the biomedical domain .Journal of Biomedical Informatics 2009 , 42 ( 5 ) : 852 - 865 .", "label": "", "metadata": {}}
{"text": "Vincze V , Szarvas G , Farkas R , M\u00f3ra G , Csirik J : The BioScope corpus : Biomedical texts annotated for uncertainty , negation and their scopes .BMC Bioinformatics 2008 , 9 ( Suppl 11 ) : S9 .PubMed View Article .Colosimo ME , Morgan AA , Yeh AS , Colombe JB , Hirschman L : Data preparation and interannotator agreement : BioCreAtIvE Task 1B. BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S12 .PubMed View Article .Genome Biology 2008 , 9 : S3 .PubMed View Article .Kim JD , Ohta T , Tateisi Y , Tsujii J : GENIA corpus - a semantically annotated corpus for bio - textmining .", "label": "", "metadata": {}}
{"text": "PubMed View Article .Kulick S , Bies A , Liberman M , Mandel M , McDonald R , Palmer M , Schein A , Ungar L , Winters S , White P : Integrated annotation for biomedical information extraction .Proceedings of the HLT - NAACL-04 Workshop on Linking Biological Literature , Ontologies and Databases , Boston , MA 2004 .Pyysalo S , Ginter F , Heimonen J , Bj\u00f6rne J , Boberg J , J\u00e4rvinen J , Salakoski T : BioInfer : A corpus for information extraction in the biomedical domain .BMC Bioinformatics 2007 , 8 : 50 .", "label": "", "metadata": {}}
{"text": "Yang Y , Pedersen JO : A comparative study on feature selection in text categorization .Proceedings of ICML-97 , Nashville , TN 1997 .Brown PF , deSouza PV , Mercer RL , Della Pietra VJ , Lai JC : Class - based n - gram models of natural language .Computational Linguistics 1992 , 18 ( 4 ) : 467 - 479 .Klein D , Manning CD : Fast exact inference with a factored model for natural language parsing .Proceedings of NIPS-02 , Vancouver , BC 2002 .Nivre J , Hall J , Nilsson J , Chanev A , Eryi\u011fit G , K\u00fcbler S , Marinov S , Marsi E : MaltParser : A language - independent system for data - driven dependency parsing .", "label": "", "metadata": {}}
{"text": "Minnen G , Carroll J , Pearce D : Applied morphological processing of English .Natural Language Engineering 2001 , 7 ( 3 ) : 207 - 223 .View Article .Miller GA , Beckwith R , Fellbaum C , Gross D , Miller K : Introduction to WordNet :An on - line lexical database .International Journal of Lexicography 1990 , 3 ( 4 ) : 235 - 244 .View Article .Hockenmaier J : Data and Models for Statistical Parsing with Combinatory Categorial Grammar .PhD thesis .University of Edinburgh ; 2003 .Gildea D , Jurafsky D : Automatic labeling of semantic roles .", "label": "", "metadata": {}}
{"text": "View Article .Carreras X , M\u00e0rquez L : Introduction to the CoNLL-2004 shared task : Semantic role labeling .Proceedings of CoNLL-04 , Boston , MA 2004 .Sarkar A , Xia F , Joshi A : Some experiments on indicators of parsing complexity for lexicalized grammars .Proceedings of the COLING-00 Workshop on Efficiency in Large - Scale Parsing Systems , Saarbr\u00fccken , Germany 2000 .Gotti M : Investigating Specialized Discourse .Bern : Peter Lang ; 2005 .Curran J , Clark S , Bos J : Linguistically motivated large - scale NLP with C&C and Boxer .", "label": "", "metadata": {}}
{"text": "Briscoe T , Carroll J , Watson R : The second release of the RASP system .Proceedings of the COLING - ACL-06 Interactive Presentation Sessions , Sydney , Australia 2006 .Lappin S , Leass HJ : An algorithm for pronominal anaphora resolution .Computational Linguistics 1994 , 20 ( 4 ) : 535 - 561 .Blei DM , Ng AY , Jordan MI , Lafferty J : Latent Dirichlet allocation .Journal of Machine Learning Research 2003 , 3 : 993 - 1022 .View Article .Tibshirani R , Walther G , Hastie T : Estimating the number of clusters in a data set via the gap statistic .", "label": "", "metadata": {}}
{"text": "View Article .Kilgarriff A : Comparing corpora .International Journal of Corpus Linguistics 2001 , 6 : 97 - 133 .View Article .Teh YW , Jordan MI , Beal MJ , Blei DM : Hierarchical Dirichlet processes .Journal of the American Statistical Association 2006 , 101 ( 476 ) : 1566 - 1581 .View Article .Copyright .\u00a9 Lippincott et al ; licensee BioMed Central Ltd. 2011 .Tools . \" ...We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .", "label": "", "metadata": {}}
{"text": "The first is a Markovian appro ... \" .We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem - identifying phrase structure .The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state - observation dependencies .The second is an extension of constraint satisfaction formalisms .We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . 1 Introduction In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints .", "label": "", "metadata": {}}
{"text": "Working within a concrete task allows us to compare ... . by Marcia Mu\u00f1oz , Vasin Punyakanok , Dan Roth , Day Zimak - IN PROCEEDINGS OF EMNLP - WVLC&apos;99 .ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1999 . \" ...A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally .The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference .Two instantiations of this approach are studied and experimental results for Noun - Phrase ... \" .A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally .", "label": "", "metadata": {}}
{"text": "Two instantiations of this approach are studied and experimental results for Noun - Phrases ( NP ) and Subject - Verb ( SV ) phrases that compare favorably with the best published results are presented .In doing that , we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are bet- ter learned using open / close predictors than using inside / outside predictors . ... to full - sentence parsers .Shallow parsing information such as NPs and other syntactic sequences have been found useful in many large - scale language processing applications including information extraction and text summariza ... .", "label": "", "metadata": {}}
{"text": "Tile common practice for approaching this task is by tedious manual definition of possible pat - tern structures , often in the h)rm of regular expres - sions or finite automata .This paper presents a novel memory - based learning method that recognizes shal - low patterns in new text based on a bracketed train - ing corpus .The training data are stored as - is , in efficient suttix - tree data structures .Generalization is performed on - line at recognition time by compar - ing subsequences of the new text to positive and negative evidence in the corIms .", "label": "", "metadata": {}}
{"text": "The paper presents experimental results for recognizing noun phrase , subject - verb and verb - object patterns in l ! ]n - glish .Since the learning approach enables easy port - ing to new domains , we plan to apply it to syntac - tic patterns in other languages and to sub - language patterns for information extraction . ... full parsing and instead to rely only on local information .These works have shown that it is possible to identify most instances of local syntactic patterns by rules that examine only the pattern itself and its nearby context .", "label": "", "metadata": {}}
{"text": "Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse .In this work we set out to evaluate whether this direction is worthwhile by comparing a learned shallow p ... \" .Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse .We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts . \" ...", "label": "", "metadata": {}}
{"text": "The method is oriented for learning to parse any selected subset of target syntactic structures .It is local , yet can handle also compositional structures .In this paper , a memory - based parsing method is extended for handling compositional structures .The method is oriented for learning to parse any selected subset of target syntactic structures .It is local , yet can handle also compositional structures . ... full parse of free - text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .", "label": "", "metadata": {}}
{"text": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem- identifying phrase structure .The first is a Markovian approach t ... \" .We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem- identifying phrase structure .The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state - observation dependencies .", "label": "", "metadata": {}}
{"text": "We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . ... algorithms that use general classifiers to yield the inference .Working within a concrete task allows us to compare ... . by Young - Sook Hwang , So - Young Park , Hoo - Jung Chung , Yong - Jae Kwak , Hae - Chang Rim , 2001 . \" ...In this paper , we define the chunking problem as a classification of words and present a weighted probabilistic model for a text chunking .The proposed model exploits context features around the focus word .", "label": "", "metadata": {}}
{"text": "In this paper , we define the chunking problem as a classification of words and present a weighted probabilistic model for a text chunking .The proposed model exploits context features around the focus word .And to alleviate the sparse data problem , it integrates general features with specific features .In the training stage , we select useful features after measuring information gain ratio of each features and assign higher weight to more informative feature by adopting the information gain ratio .At the application time , we classify words into chunk labels while checking consistency of the begin and the end of a chunk .", "label": "", "metadata": {}}
