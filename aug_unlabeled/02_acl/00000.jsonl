{"text": "A Learning - Based Approach for Biomedical Word Sense Disambiguation .Copyright \u00a9 2012 Hisham Al - Mubaid and Sandeep Gungu .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .Abstract .In the biomedical domain , word sense ambiguity is a widely spread problem with bioinformatics research effort devoted to it being not commensurate and allowing for more development .This paper presents and evaluates a learning - based approach for sense disambiguation within the biomedical domain .", "label": "", "metadata": {}}
{"text": "However , the advances in automatic text annotation and tagging techniques with the help of the plethora of knowledge sources like ontologies and text literature in the biomedical domain will help lessen this limitation .The proposed method utilizes the interaction model ( mutual information ) between the context words and the senses of the target word to induce reliable learning models for sense disambiguation .The method has been evaluated with the benchmark dataset NLM - WSD with various settings and in biomedical entity species disambiguation .The evaluation results showed that the approach is very competitive and outperforms recently reported results of other published techniques .", "label": "", "metadata": {}}
{"text": "Word sense disambiguation is the task of determining the correct sense of a given word in a given context .In the general language domain , and within natural language processing ( NLP ) , the word sense disambiguation ( WSD ) problem has been studied and investigated extensively over the past few decades [ 1 , 2 ] .In the biomedical domain , on the other hand , WSD is more widely spread in the biological and medical texts and sometimes with more severe consequences .The amount of WSD research in the biomedical domain is not proportional to the extent of the problem .", "label": "", "metadata": {}}
{"text": "[ 3 ] as follows : organism function , diagnostic procedure , and laboratory or test result .Thus , if this term blood pressure is found in a medical text , the reader has to manually judge and determines which one of these three senses is intended in that text .Word sense disambiguation contributes in many important applications including the text mining , information extraction , and information retrieval systems [ 1 , 2 , 4 ] .It is also considered a key component in most intelligent knowledge discovery and text mining applications .The main classes of approaches of word sense disambiguation include supervised methods and unsupervised methods .", "label": "", "metadata": {}}
{"text": "The unsupervised methods , on the other hand , are based on knowledge sources like ontology , for example , from UMLS , or text corpora [ 2 , 4 , 7 , 8 ] .Our approach in this paper is a supervised approach .In this paper , we present and evaluate a supervised method for biomedical word sense disambiguation .The method is based on machine learning and uses some feature selection techniques in constructing feature vectors for the words to be disambiguated .We conducted the evaluation using the NLM - WSD benchmark corpus and species disambiguation dataset .", "label": "", "metadata": {}}
{"text": "Related Work .In the biomedical domain , the applications of text mining and machine learning techniques were quite successful and encouraging [ 6 ] .Moreover , the bioinformatics literature shows that biomedical WSD has been a quite active area of research with a number of approaches proposed and applied to biomedical data [ 1 , 2 , 4 , 8 , 12 , 13 ] .Agirre et al .proposed a graph - based WSD technique which is considered unsupervised but relies on UMLS [ 2 ] .The concepts of UMLS are represented as a graph , and WSD is done using personalized page rank algorithm [ 2 ] .", "label": "", "metadata": {}}
{"text": "In [ 1 ] , Stevenson et al .use supervised learners with linguistic features extracted from the context of the word in combination with MeSH terms for disambiguation .The UMLS has been used , by Humphrey et al . , as a knowledge source for assigning the correct sense for a given word [ 13 ] .They used journal descriptor indexing of the abstract containing the term to assign a semantic type from UMLS metathesaurus [ 3 , 13 ] .In bioinformatics and computational biology , there are quite a few tasks similar to WSD like biomedical term disambiguation , gene protein name disambiguation , and disambiguating species for biomedical named entities [ 9 - 11 ] .", "label": "", "metadata": {}}
{"text": "In NER , biomedical entity names , for example , gene names , are recognized and extracted from the text .In the biomedical named entity disambiguation , the extracted entity names ( e.g. , gene product names ) will be applied onto a process such that each occurrence should be disambiguated as either gene name or protein name as the same name can refer to a gene or protein .For example , the biomedical entity name SBP2 can be a gene name or a protein name depending on the context [ 10 , 11 ] .Furthermore , in species disambiguation , the term c - myc is a gene , but it can be either in a human gene ( homo sapiens ) or mouse gene ( mus musculus ) depending on the context [ 9 - 11 , 14 - 16 ] .", "label": "", "metadata": {}}
{"text": "devised a rule based system to disambiguate biomedical entity names , like gene products , based on species .In that approach [ 9 ] , some parsing techniques are used and syntactic parse tree with paths between words to determine if there exists a path between species word and the entity name .They employed and examined several parsers in the task including C&C , Enju , Minipar , and Stanford - Genia [ 9 , 15 , 16 ] .A Method for WSD .A word sense disambiguation method is an algorithm that assigns the most accurate sense to a given word in a given context .", "label": "", "metadata": {}}
{"text": "The method is based on a word classification and disambiguation technique that we have proposed in a preliminary work [ 17 ] .In the previous work , [ 17 ] , we introduced a method for term disambiguation and evaluated it with biomedical terms to disambiguate gene and protein names in medical texts .The method relies on representing the instances of the word to be disambiguated , . , as a feature vector , and the components of this vector are neighborhood context words in the training instances .In the context of the target word , . , we select the words with the high discriminating capabilities as the components of the vectors .", "label": "", "metadata": {}}
{"text": "The trained models ( classifiers ) produced from the learning phase will then be used to disambiguate unseen and unlabeled examples in the testing phase .That is , during the learning phase , the constructed feature vectors of the training instances will be used as labeled examples to train classifiers .The classifier will be then used to disambiguate unseen and unlabeled examples in the application phase .One of the main strength of this method is that the features are selected for learning and classification .Feature Selection The features selected from the training examples have great impact on the effectiveness of the machine learning technique .", "label": "", "metadata": {}}
{"text": "The labeled training instances will be used to extract the word features for the feature vectors .Suppose the word .labeled with sense . or .( i.e. , in the set .or in the set . can be viewed as . and .in one set W ( s.t . .Each context word .or with .or combination and in any distribution .We want to determine that , if we see a context word . suggests that this example belongs to .or to .Thus , we use as features those context words .", "label": "", "metadata": {}}
{"text": "For that , we use feature selection techniques such as mutual information ( MI ) [ 19 , 20 ] as follows .For each context word .that do not contain .that do not contain .Therefore , the mutual information ( MI ) can be defined as .M .I . and . is the total number of training examples .MI is a well - known concept in information theory and statistical learning .MI is a measure of interaction and common information between two variables [ 22 ] .In this work , we adapted MI to represent the interaction between the context words .", "label": "", "metadata": {}}
{"text": "We utilized the training corpus of the labeled instances of the word to be disambiguated to compile the list of all context words ( . as explained above ; all instances of one sense are under one class label .We notice that if the context word , . , is mostly occurring in class .( or mostly in . , then the MI indicates this as shown in ( 2 ) .Thus , MI can be used as a means to estimate the amount of information interaction between a context work and a class label .So , MI is used to select the context words with the highest discriminating capability between . and .", "label": "", "metadata": {}}
{"text": "Moreover , following the same intuitive reasoning of mutual information , MI , we define another method , M2 , for selecting the words as features to be included in the feature vectors as follows : .M .In the following example , assume that the target word .has 10 instances already labeled with one of two senses as shown in Table 1 .Class . are the instances of .with the first sense , while .are the instances of . instances in the second sense .Each instance is shown with its context words within certain window size .", "label": "", "metadata": {}}
{"text": "In this example , . is the total number of training examples .The values of a , b , c , d for .is more highly related with the class . than . , and so it has more discriminating power than . , and this is quantified by their MI values .MI values for . and . are 1.8 and 1.2 , respectively .Then , MI ( or M2 ) value is computed for all context words .Then , the context words . are ordered based on their MI values , and the top .", "label": "", "metadata": {}}
{"text": "with highest MI values are selected as features .In this research , we experimented with .values of 100 , 200 , and 300 .With . , for example , each training example will be represented by a vector of 100 entries such that the first entry represent the context word .with the highest MI value , and the second entry represents the context word with the second highest MI value and so on .Table 2 shows the top 10 context words with the ten highest MI values for the ambiguous word \" cold \" in the NLM - WSD benchmark corpus explained in Section 3 .", "label": "", "metadata": {}}
{"text": "For example , a simple feature vector of size 5 can be as follows : .This feature vector represents an instance that has the first , third , and fourth context words available in its context , and 1.23 is the MI value of the context word with the highest MI .Table 2 : Context words with the top MI values for the ambiguous word \" cold \" .The Learning Phase From the labeled training examples of the word , we build the feature vectors using the top context words selected by MI or M2 as features .", "label": "", "metadata": {}}
{"text": "SVM has been shown as one of the most successful and efficient machine learning algorithms and is well founded theoretically and experimentally [ 7 , 17 , 18 , 23 ] .The applications of SVM are abound ; in particular , in NLP domain like text categorization , relation extraction , named entity recognition , SVM proved to be the best performer .The Disambiguation Step In the testing step , we want to disambiguate an instance .of the word .We construct a feature vector .for the instance .the same way as in the learning step .", "label": "", "metadata": {}}
{"text": "to one of the two senses .Evaluation and Experiments .Biomedical WSD ( NLM - WSD ) .Dataset We used the benchmark dataset NLM - WSD for biomedical word sense disambiguation [ 24 ] .This dataset was created as a unified and benchmark set of ambiguous medical terms that have been reviewed and disambiguated by reviewers from the field .Most of the previous work on biomedical WSD uses this dataset [ 1 , 2 , 4 ] .The NLM - WSD corpus contains 50 ambiguous terms with 100 instances for each term for a total of 5000 examples .", "label": "", "metadata": {}}
{"text": "The instances of these ambiguous terms were disambiguated by 11 annotators who assigned a sense for each instance [ 24 ] .The assigned senses are semantic types from UMLS .When the annotators did not assign any sense for an instance , then that instance is tagged with \" none \" .Only one term \" association \" with all of its 100 instances were annotated none and so dropped from the testing .Text Preprocessing On this benchmark corpus , we have carried out some text preprocessing steps .( i )Converting all words to lowercase .", "label": "", "metadata": {}}
{"text": "( iii )Performing word stemming using Porter stemming algorithm [ 25 ] .Moreover , unlike other previous work , words with less than 3 or more than 50 characters are not ignored currently ( unless dropped by the stopword removal step ) .Also words with parentheses or square brackets are not ignored and part of speech is not used .After the text preprocessing is completed , for each word we convert the instances into numeric feature vectors .Then , we use SVM for training and testing with 5-fold cross validation 5FCV such that 80 % of the instances are used for training and the remaining 20 % are used for testing , and this is repeated five times by changing the training - testing portions of the data .", "label": "", "metadata": {}}
{"text": "A .c .c . u .r . a .c .y .n .o . . .o .f .i .n .s .t . a .n .c .e . s .w .i .t .h .c .o .r .r .e . c . t . a .s .s .i . g .n .e . d .s .e . n .s .e . s .t .o .t . a .", "label": "", "metadata": {}}
{"text": "n .o . . .o .f .t .e . s .t .e . d .i .n .s .t . a .n .c .e . s . . .( .We also use the baseline method which is the most frequent sense ( mfs ) for each word .This lead , to a total of 31 words tested in this evaluation , and 18 words were dropped because they do not have at least two instances annotated for each one of two senses .For example , the word \" depression \" has two senses : mental or behavioral dysfunction and functional concept .", "label": "", "metadata": {}}
{"text": "Likewise , the word \" discharge \" was not tested as it has only one instance tagged with the first sense , 74 instances tagged with the second sense , and 25 instances tagged with None .We used . , and the window size is 5 .The accuracy results of this first evaluation ( EV1 ) are shown in Table 4 .The detailed results of this evaluation are included in Table 5 .In the second evaluation ( EV2 ) and third evaluation ( EV3 ) , we changed the parameter and the word / features selection formula .", "label": "", "metadata": {}}
{"text": "In EV3 , we kept .Table 5 contains the results of EV2 and EV3 .To judge on performance of our method and compare our results with similar techniques , we included several reported results from three recent publications from 2008 to 2010 [ 1 , 2 , 4 ] with our results in Table 6 under the same experimental settings .Species Disambiguation .In biomedical text , named entities , like gene name , are used the same way irrespective of the species of the entity .As a result , it will be difficult to extract relevant medical information automatically from texts using information extraction system .", "label": "", "metadata": {}}
{"text": "In one instance , c - myc might refer to a human gene , while in another instance it refers to a mouse gene .For example , in Table 3 , the biomedical entity name BCL-2 ( a protein name ) in the first text ( no . 1 ) is human while in the second one is a mouse protein .We examined our system on this task of species disambiguation .We obtained the data from the project of Wang et al .[ 9 ] .From their data , we tested the biomedical entity names that occur in at least two species with at least 3 occurrences in each species .", "label": "", "metadata": {}}
{"text": "If the entity has 5 or more occurrences in one species , we repeat five times using 5FCV as in Section 4.1 .We extracted and tested our system on a total 465 instances of entity names with an average of 8 instances per species for each entity name .In the original dataset ( gold standard ) , 90 % of the terms have all their instances occurring in only one species [ 9 ] and so can not be tested in our system .Our system requires that each term should have instances in two or more species with at least 3 occurrences in each species .", "label": "", "metadata": {}}
{"text": "Table 8 : Precision , recall , and F1 results of our method on the fivefold in the species disambiguation experiments .Discussion and Conclusion .The main weakness of the supervised and machine - learning - based methods for WSD is their dependency on the annotated training text which includes manually disambiguated instances of the ambiguous word [ 2 , 17 ] .However , over the time , the increasing volumes of text and literature in very high rates and the new algorithms and techniques for text annotation and concept mapping will alleviate this problem .Moreover , the advances in ontology development and integration in the biomedical domain will facilitate even more the process of automatic text annotation .", "label": "", "metadata": {}}
{"text": "The approach was evaluated with a benchmark dataset , NLM - WSD , to facilitate the comparison with the results of previous work .The average accuracy results of our method , compared to some recent reported results ( Table 6 ) , are promising and proving that our method outperforms those recently reported methods .Table 6 contains the results for 11 methods : baseline method ( mfs ) , our method ( last column ) , and 9 other methods from recent work published in 2008 to 2010 ( from [ 1 , 2 , 4 ] ) .", "label": "", "metadata": {}}
{"text": "Our method also outperforms all 10 other methods in 12 out of 31 words followed by NB which outperforms the rest in 7 words .Stevenson et al . in their paper [ 1 ] report extensive accuracy results of their method ( we call it Stevenson-2008 ) along with four other methods including Joshi-2005 and McInnes-2007 , with various combinations of words from NLM - WSD corpus used for testing .For example , Joshi-2005 tested their system on 28 words ( out of the whole set 50 words ) and other techniques used 22 words , 15 words , or the whole set [ 1 ] .", "label": "", "metadata": {}}
{"text": "[ 1 ] .These three methods are supervised methods and used various machine learning algorithm and wide sets of features .For example , Stevenson-2008 used linguistic features , CUI 's , MeSH terms , and combination of these features .They employed three learners VSM ( vector space model ) , Na\u00efve Bayes ( NB ) , and SVM .The results included in Table 6 are their best results with VSM and ( linguistic + MeSH ) features [ 1 ] .The method of Joshi-2005 uses five supervised learning methods and collocation features , while McInnes-2007 uses NB [ 1 ] .", "label": "", "metadata": {}}
{"text": "We obtained the results of the other methods on these 31 words from the references shown in Table 6 to allow for direct comparison .The best result reported in their paper is 87.8 % using all words with VSM model and for McInnes 85.3 % also with the whole set [ 1 ] .The best result of Stevensons-2008 for subsets was 85.1 % using a subset of 22 words defined by Stevenson et al .[ 1 ] .The results of the three methods ( single , subset , full ) in Table 6 are taken directly from Agirre et al .", "label": "", "metadata": {}}
{"text": "In another work , Jimeno - Yepes and Aronson evaluate four unsupervised methods on the whole NLM - WSD set [ 4 ] as well as NB and combination of the four methods .The accuracy of the four methods ranges from 58.3 % to 88.3 % ( NB ) on the whole set , and NB was found to be the best performer followed by CombSW ( 76.3 % ) [ 4 ] .The average accuracy results of NB and two combinations ( NB , CombSW , and CombV ) on our 31 word - subset are 86 % , 73.1 % , and 72.1 % respectively which are lower than our results , see Table 6 .", "label": "", "metadata": {}}
{"text": "The evaluation results of our method compare very well with those reported in [ 9 ] as shown in Table 7 .From their results ( Table 7 ) , we notice that the best overall performance was obtained with the ML method ( machine learning ) with precision , recall , and F1 values being equal at 82.69 .Our results as shown in Table 8 are not directly comparable with those in Table 7 due to the difference in the size of test set .However , we can see that our method 's performance is reasonably well standing in terms of precision , recall , and F1 .", "label": "", "metadata": {}}
{"text": "These weights enable the learner to induce quite reliable models for sense disambiguation .All the results showed that the technique is fairly successful and effective in the disambiguation task .Thus , more research work should be exerted to carry out further improvements on the performance of this technique .In future work of this research , we plan to investigate the possibility of disambiguating entity names when all instances of that entity are occurring in one species .Currently , our method is supervised and required annotated instances in both classes to be able to test new samples .", "label": "", "metadata": {}}
{"text": "M. Stevenson , Y. Guo , R. Gaizauskas , and D. Martinez , \" Knowledge sources for word sense disambiguation of biomedical text , \" in Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing ( BioNLP ' 08 ) , pp .80 - 87 , 2008 .B. L. Humphreys , D. A. B. Lindberg , H. M. Schoolman , and G. O. Barnett , \" The unified medical language system : an informatics research collaboration , \" Journal of the American Medical Informatics Association , vol .5 , no . 1 , pp . 1 - 11 , 1998 .", "label": "", "metadata": {}}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Xu , M. Markatou , R. Dimova , H. Liu , and C. Friedman , \" Machine learning and word sense disambiguation in the biomedical domain : design and evaluation issues , \" BMC Bioinformatics , vol .7 , article 334 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. K. Savova , A. R. Coden , I. L. Sominsky et al . , \" Word sense disambiguation across two domains : biomedical literature and clinical notes , \" Journal of Biomedical Informatics , vol .", "label": "", "metadata": {}}
{"text": "6 , pp .1088 - 1100 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Chen and H. Al - Mubaid , \" Context - based term disambiguation in biomedical literature , \" in Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference ( FLAIRS ' 06 ) , pp .62 - 67 , Orlando , Fla , USA , May 2006 .View at Scopus . H. Al - Mubaid , \" Context - based technique for biomedical term classification , \" in Proceedings of the IEEE Congress on Evolutionary Computation ( CEC ' 06 ) , pp .", "label": "", "metadata": {}}
{"text": "M. Stevenson , et al . , \" Disambiguation of biomedical text using a variety of knowledge sources , \" BMC Bioinformatics , vol .9 , supplement 11 , article S7 , 2008 .View at Google Scholar .57 , no . 1 , pp .96 - 113 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Stevenson , E. Agirre , and A. Soroa , \" Exploiting domain information for Word Sense Disambiguation of medical documents , \" Journal of the American Medical Informatics Association .In press . Y. Miyao , K. Sagae , R. S\u00e6tre , T. Matsuzaki , and J. Tsujii , \" Evaluating contributions of natural language parsers to protein - protein interaction extraction , \" Bioinformatics , vol .", "label": "", "metadata": {}}
{"text": "394 - 400 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Chen and H. Al - Mubaid , \" Context - based term disambiguation in biomedical literature , \" in Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference ( FLAIRS ' 06 ) , pp .62 - 67 , Orlando , Fla , USA , May 2006 .View at Scopus .G. Forman , \" An Extensive Empirical study of feature selection metrics for text classification , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}}
{"text": "1289 - 1305 , 2003 .View at Google Scholar .L. Galavotti , F. Sebastiani , and M. Simi , \" Experiments on the use of feature selection and negative evidence in automated text categorization , \" in Proceedings of the 4th European Conference on Research and Advanced Technology for Digital Libraries , 2000 . Y. Yang and J. P. Pedersen , \" A comparative study on feature selection in text categorization , \" in Proceedings of the 4th International Conference on Machine Learning and Computing , 1997 .Z. Zheng and R. Srihari , \" Optimally combining positive and negative feature for text categorization , \" in Proceedings of the Workshop on Learning from Imbalanced Data Sets II ( ICML ' 03 ) , 2003 .", "label": "", "metadata": {}}
{"text": "T. Joachims , \" Text categorization with support vector machines : learning with many relevant features , \" in Proceedings of the 10th European Conference on Machine Learning , 1998 .M. Weeber , J. Mork , and A. Aronson , \" Developing a test collection for biomedical word sense disambiguation , \" in Proceedings of the Symposium American Medical Informatics Association ( AMIA ' 01 ) , 2001 .M. F. Porter , \" An algorithm for suffix stripping , \" Program , vol .14 , pp .130 - 137 , 1980 .View at Google Scholar Born - Bunge Foundation , University of Antwerp - UIA , B2610 Antwerp , Belgium 2 .", "label": "", "metadata": {}}
{"text": "Minnesota Supercomputer Institute , University of Minnesota , MPLS , MN , 55455 USA .Abstract .Many physical systems of interest to scientists and engineers can be modeled using a partial differential equation extended along the dimensions of time and space .These equations are typically nonlinear with real - valued parameters that control the classes of behaviors that the model is able to produce .Unfortunately , these control parameters are often difficult to measure in the physical system .Consequently , the first task in developing a model is usually to search for appropriate parameter values .", "label": "", "metadata": {}}
{"text": "We have applied evolutionary algorithms ( EAs ) to the problem of parameter selection in models of biologically realistic neurons .Our objective was not to find the \" best \" solution , but rather we sought to produce the manifold of high fitness solutions that best accounts for biological variability .The Problem in Neuroscience .Neurons are the fundamental units responsible for the transmittal and transduction of information in the brain .Information is transmitted between neurons via diffusable chemicals .The chemical signals from many convergent presynaptic neurons are spatiotemporally integrated and transduced into an electrical response by the postsynaptic neuron ( Figure 1a ) .", "label": "", "metadata": {}}
{"text": "These suprathreshold electrical events are referred to as action potentials , bursts , or spikes .How information is encoded in the rate and pattern of electrical signaling events in single neurons and populations of neurons is currently the subject of hot debate among neuroscientists ( Ferster and Spruston 1995 ) .However , it is fair to say that at this time , we do n't really know how neurons encode information and the answer to this question is at the heart of understanding brain function .Spike production in neurons is governed by a class of membrane - spanning proteins known as voltage - gated and/or calcium - dependent ( VGCD ) channels ( Figure 1b ) .", "label": "", "metadata": {}}
{"text": "A large variety of VGCD channels exist , and they contribute substantially to the electrical properties of the neuron ( Johnston et al 1996 , Spruston et al 1994 , Hille 1992 ) .The contribution of VGCD channel subtypes to information encoding varies adaptively , however .A significant body of work in Aplysia suggests that some forms of animal learning can be attributed to the activity - dependent modulations of VGCD channels ( Klein and Kandel 1978 , Fitzgerald et al 1990 , Edmonds et al 1990 ) .Therefore , there are many combinations of VGCD channel dynamics that are congruent with the \" normal \" functioning of neurons .", "label": "", "metadata": {}}
{"text": "Figure 1 ( A )CA3 hippocampal neuron morphology digitized from an experimental preparation .Presynaptic neurons transmit chemical signals onto receptor sites in the basal and apical dendrites .If the spatiotemporally propagated electrical signal becomes sufficiently depolarized in the region where the axon extends away from the soma ( the axon hillock ) , action potentials are generated and transmitted via the axon to the next set of neurons in the network .S , soma ; BD , basal dendrites ; AD , apical dendrites ; AX , axon .( B )VGCD channels are distributed throughout the lipid bilayer membrane of the neuron .", "label": "", "metadata": {}}
{"text": "Some conformations are more conductive to the flow of ions by , for example , releasing an inactivation gate blocking the aqueous pore .A region of charged animo acid residues at the mouth of the channel serves as a filter for ions of a particular charge or size .AP , aqueous pore ; SF , selectivity filter ; IG , inactivation gate .To address the extent of VGCD channel influence on information processing , we need a substantial amount of information about the types and numbers of VGCD channels present in a given class of neurons .", "label": "", "metadata": {}}
{"text": "We need to know their distribution along the spatial extent of the neuron and how these distributions may change over time .Finally , we need tools to manipulate the channel distributions , conductances and/or kinetics so that we can observe how channel modulations affect the behavior of the entire neuron .Substantial advances have been made over the past decade in the tools and techniques available for the measurement and manipulation of VGCD channels and currents .However , we still do n't have the ability to experimentally identify and control VGCD channels in a precise manner .Spatial limitations prevent the measurement of individual channels on the smaller dendritic processes .", "label": "", "metadata": {}}
{"text": "At this point in time , it is not possible to manipulate the spatial expression of channels nor is it possible to impose a specific localized change in the conductance of a single channel type .As an alternative approach , biophysically realistic computational models of neurons ( Segev et al .1989 , De Schutter and Bower 1994 ) allow observation of the state variables , such as voltage and calcium concentration , and control of the scaling parameters , such as the kinetics of channels and their spatial distributions , with a precision unattainable in experimental preparations .", "label": "", "metadata": {}}
{"text": "The power of modeling and ease of model implementation are persuasive arguments for studying the influence that VGCD channels can have on the bioelectrical behaviors of neurons within a computational framework .The outstanding problem in the development of these models has been to determine parameter sets that represent the spatial distributions of the VGCD channels .Models of neurons typically have from tens to thousands of unconstrained parameters .Because these models exist in high dimensional spaces , it is not possible to evaluate every combination of plausible parameters .We would like to know ALL the parameter sets that produce behaviors appropriate to a given electrical signaling behavior .", "label": "", "metadata": {}}
{"text": "We would like to examine the relative ranges of each parameter manifold as a measure of sensitivity of the model along each parameter axis .Finally , we would like an efficient method of producing these solutions .High Performance Computing Aspects of the Problem .Because this IMA volume and associated workshop were intended to focus on the high performance computing ( HPC ) aspects of EA implementations , it seems reasonable to briefly review the computational structure of the model .This will additionally help the reader to better understand the source and the significance of the parameters we would like to optimize .", "label": "", "metadata": {}}
{"text": "In many ways , the process and goals of model building are shared across HPC applications .We have a computational model of a physical process .We have many unconstrained parameters in our description .We want the behaviors , e.g. the time series , from our simulated model to correspond well with data collected from the physical system .We would like to interpret parameter sets that yield good correspondences to validate the appropriateness of the model or to make predictions about the physical system .Our approach should therefore be generalizable to a number of HPC endeavors .", "label": "", "metadata": {}}
{"text": "The cable equation is a second order partial differential equation ( PDE ) of the parabolic type .It is analytically similar to the heat or diffusion equations .The equation describes voltage as a function of space and time .The voltage term , V , represents the transmembrane voltage difference as a deviation from the resting value .X and T represent the dimensions of distance and time over a specified domain .The Method of Lines replaces the PDE problem by a mathematically equivalent system of ordinary differential equations ( ODE ) by discretizing the spatial domain .", "label": "", "metadata": {}}
{"text": "This resulting ODE system can be subsequently solved using numerical integration .The derivation follows .Figure 2 ( A )A length of a neuronal process is discretized into three cables that are spatially adjacent to each other and coupled through a linear resistance term .( B )The electric circuit diagram for three electrical compartments has an axial resistance term coupling the compartments .Each region of membrane in this figure has two sources of local current : membrane ( passive leak ) and capacitive .x is the distance along the axis of a membrane cylinder ( cm ) and t is time ( ms ) .", "label": "", "metadata": {}}
{"text": "Known as the length constant and the time constant , respectively , they are metrics for the distance or time required for the voltage to attenuate to 1/e of the initial value when a deviation from the resting value is applied .They are defined in terms of the resistance and capacitance per unit length or area .R m is the specific membrane resistance ( \u03a9cm 2 ) , C m is the specific membrane capacitance ( \u03bcF / cm 2 ) , R 1 is the axial resistance ( \u03a9cm ) , and r is the radius of the cylinder ( cm ) .", "label": "", "metadata": {}}
{"text": "The number of compartments [ 2 ] , N , depends on the number of discretized elements required for the system to numerically converge .The number of adjacent elements is determined by the specific morphological representation of a given compartment [ 3 ] .Equation 7 describes the Laplacian for an element with two neighboring compartments .It is generally assumed that C m and R 1 are constant for all compartments in the model , while R m is often varied ( Holmes and Rall 1992 ) .The capacitance of the membrane is typically estimated to be a constant value of 1.0 \u03bcF / cm 2 ( Hodgkin and Rushton 1946 ) .", "label": "", "metadata": {}}
{"text": "This co - dependency requires that the entire system be solved simultaneously as a vector - matrix equation .The matrix is structurally symmetric , although not necessarily numerically symmetric when compartments of nonuniform physical dimensions are allowed .The matrix is sparse and constant ( with respect to the location of zeros ) for a given discretized morphology ( Hines 1984 , Mascagni 1989 , Eichler West and Wilcox 1996 ) .Numerical methods optimized for sparse matrices , such as the minimal ordering method , eliminate unnecessary operations on zero - valued matrix elements ( Press et al 1992 ) .", "label": "", "metadata": {}}
{"text": "Any carrier , pore , or mechanism for transferring ions across the membrane is considered a subset of this current type .Injected current , , is a driving term typically applied to the soma compartment .This term models microelectrode current injections during biological experiments .The membrane leak current , , stabilizes the voltage near the resting potential .is the conductance ( \u03bcS ) and is the reversal potential ( mV ) determined by the Nernst potential of the permeable ion species .and are typically constant - valued .is the sum of the currents produced by multipleVGCD channel types .", "label": "", "metadata": {}}
{"text": "The conductances of the VGCD channel subtypes are functions of voltage , calcium concentration , and/or time .Ion channels are assumed to exist in either an open ( conductive ) or closed state .Using the potassium delayed rectifier parameter as an example , n represents the normalized fraction of the population that is in an open state , \u03b1 is the rate at which the channels open , and \u03b2 is the rate at which the channels close .The rate constants \u03b1 n and \u03b2 n are functions of voltage and/or calcium concentration [ 4 ] .", "label": "", "metadata": {}}
{"text": "The driving potential , determined in part by the concentration gradient of the major ion species , favors inward currents through sodium and calcium permeable channels and outward currents through the potassium permeable channels .The conductance scaling factors , G n , are typically the unknown parameters in compartmental models .As described in the next sections , EA strategies can be used to determine appropriate values for these parameters .Many channel types have more complicated kinetics due to the existence of multiple subconductance states ( m , eqn .18 ; n , eqn .19 ) and/or inactivation gates ( h , eqn .", "label": "", "metadata": {}}
{"text": "It should be noted that the classic biophysical kinetic description of Hodgkin - Huxley is only an approximation of the ensemble behavior of channels ( Hille 1992 , Keynes 1992 ) .Further , this model may be inappropriate to describe the behavior of some channel types ( Goldman 1943 ) .The state parameter ODEs ( eqn .16 ) must be solved concurrently with the voltage ODEs ( eqn 11 ) ; however , the state parameter equations depend only on the local values of voltage and/or calcium .Inclusion of these current sources potentially increases the problem size substantially .", "label": "", "metadata": {}}
{"text": "The Traub ( 1991 )CA3 hippocampal neuron model , which we have used in the modeling work presented in this paper , contains 11N equations .This model uses a simplified 19 compartment representation of a neuron for a total of 209 ODEs and 114 unknown conductance scaling factors .The De Schutter - Bower ( 1994 ) cerebellar Purkinje cell model contains many more channel types and an experimentally - derived morphology for a total of 32,000 ODEs and 19,200 unknown conductance scaling factors .The trend in neuroscientific modeling is to build larger models that include more experimental detail .", "label": "", "metadata": {}}
{"text": "The Traub ( 1991 ) model offered many advantages for a proof - of - concept EA experiment ; each simulation required minimal computer resources and the model expressed a range of characteristic hippocampal neuron behaviors .A depolarizing afterpotential ( DAP ) was produced after a stimulus subthreshold for burst elicitation and suprathreshold bursts were followed by afterhyperpolarizations ( AHPs ) .The model demonstrated a transition between single spiking to bursting regimes that corresponded well with experimentally - observed frequency - intensity responses .Finally , a network of these neurons was able to reproduce picrotoxin - induced synchronized multiple bursts and has been since refined as a model of gamma - frequency EEG activity ( Traub et al 1996 ) .", "label": "", "metadata": {}}
{"text": "The test stimulus was a 0.3 nA depolarizing current injection into the soma compartment because Traub described physiologically interesting behaviors produced by this stimulus .The model defined kinetic equations for six channel conductances within each compartment : a sodium channel , a hybrid high threshold calcium channel , a potassium delayed rectifier channel , a potassium A channel , a potassium AHP channel , and a calcium - dependent potassium channel .The EA in this study searched the 114 dimensional space containing the six channel conductances in each of the 19 compartments .Traub selected his parameters by trial - and - error .", "label": "", "metadata": {}}
{"text": "We wanted to demonstrate the ability of an EA to automatically select a volume of solutions that either included the parameter set of Traub or demonstrated an improved correspondence to experimental observations in comparison to Traub 's parameter choice .We defined two low dimensional ( single value ) metrics to characterize the parameter space for the purposes of post - analysis and initialization of the random seed parameters .The total conductance is the sum of all conductances over all compartments and the excitatory / inhibitory conductance ratio ( EIR ) is the sum of all excitatory ( inward ) conductances over all compartments divided by the sum of all inhibitory ( outward ) conductances over all compartments .", "label": "", "metadata": {}}
{"text": "Evolutionary Algorithms ( EAs ) represent an efficient and robust method for parameter fitting in high dimensional spaces ( Holland 1975 , Forrest 1993 ) .A statistical proof called the Schema Theorem guarantees improvement of the search over time ( Holland 1975 ) .EAs have not previously been applied to the field of compartmental neuronal modeling , although reduced dimensional parameter searches using systematic ( Bhalla and Bower 1993 ) or stochastic ( Foster et al 1993 ) search methods have been reported .Would these or methods other than an EA perform better ?Descriptions of many optimization methods are available both conceptually in textbooks and algorithmically in computer executable codes obtainable from public domain sites on the Internet .", "label": "", "metadata": {}}
{"text": "With the large number of methods and their variations available , how does one choose the best for one 's application ?Wolpert and Macready ( 1995 ) propose that there are \" No Free Lunches \" ( NFL ) for effective optimization .Using theoretical arguments , they claim that all search algorithms perform exactly the same when averaged over all cost functions .Consequently , it is a flawed strategy to choose a method by comparing initial algorithmic performance and then to search completely based on these preliminary results .Wolpert and Macready warn that if one does not incorporate any information about the function into the algorithm , then success must rely on a fortuitous matching between the features of function and the search path determined by the algorithm .", "label": "", "metadata": {}}
{"text": "Culberson ( 1996 ) extended the NFL theorem with a corollary proposing that all algorithms perform the same as a random enumerative search when the method is blind .These assertions suggest that we will not likely find a canned method to solve our optimization problem .A hand - tuned method is required , taking into account as much information about the function and solution space as we know .The need to incorporate domain knowledge into a search is not newly revealed by the NFL - related reports .Based on empirical studies , Davis ( 1991 ) suggested that the difficulty of search is not a property of a given problem , but rather depends on the relationship between the problem and the algorithm .", "label": "", "metadata": {}}
{"text": "Specifically , they state : . \" These results suggest that future analyses of the Genetic Algorithm must pay close attention to the relationship between the algorithmic parameters of the GA and the function space from which the fitness function is selected .Since any fixed set of algorithmic parameters can not enable the GA to efficiently optimize an arbitrary function in a broad class like the one we have considered , we must consider smaller classes or distributions of functions for which those parameters are most appropriate .Alternatively , if we have a function we wish to optimize , we must carefully select our algorithmic parameters if we wish to optimize the function efficiently with the Genetic Algorithm .", "label": "", "metadata": {}}
{"text": "For an algorithm to be effective , the biases in how search space is explored must correlate well with salient features in the cost function .The problem is , of course , to know what is salient about the function when the whole point of the search is that little is known !( Otherwise , why would one be searching over it ? )To optimize the parameters for the compartmental neuronal model , one must first choose a method that sounds reasonable , gain some intuition into how the optimization algorithm works , and attempt to modify it with domain - based knowledge .", "label": "", "metadata": {}}
{"text": "Armed only with intuition and a predisposition towards this technique because of the familiarity of the biological metaphor , our initial optimization efforts focused on understanding the applicability of EAs for the compartmental neuron channel distribution problem .Designing an EA Strategy .The specific mechanisms of selection and recombination applied to a particular problem space determine the success of an EA .Selection identifies and retains favorable parameter combinations .Crossover enhances the search mechanism through the breakup and recombination of parameter combinations , maximizing a good balance between these two extremes .Thus , the representation of the problem along the string and the type of crossover function applied are important factors in the rate of success of a search because probability favors the breakup of longer parameter combinations .", "label": "", "metadata": {}}
{"text": "Representation .The representation of a problem is the mapping between problem space and the data space operated on by the algorithm .Three aspects of representation must be considered for an EA application : 1 ) embedding information onto string values ; 2 ) ordering values onto the linear array of the string ; and 3 ) choosing the cardinality of the alphabet ( binary vs. real - valued ) .These aspects should be examined with respect to the choice of recombination operators one intends to apply .We considered two embedding spaces for our problem : 1 ) a string of values representing each of the individual parameters ; and 2 ) a string of scaled functions which could be applied over the normalized distance from the soma .", "label": "", "metadata": {}}
{"text": "However , other PDE systems might consider the utility of applying scaled functions of the normalized length of the spatial domain along one or more dimensions .Figure 5 Only three parameters are needed to encode channel distributions as functions : function type ( ex .linear , exponential , sinusoidal ) ; a maximum function value ; and a minimum function value .In this example , the function was applied over a bidirectional normalization of distance .However , this choice was arbitrary and other pertinent system metrics might be more appropriate .For the string of individual values , two orderings appear to be natural for the problem : parameter ordering and compartment ordering .", "label": "", "metadata": {}}
{"text": "For example ( using two channel conductance parameters ) : .Na1Na2 ... NaN ...K(DR)1K(DR)2 ... K(DR)N ( 20 ) .where Na and K(DR ) represent the sodium and potassium delayed rectifier conductance parameters , and the subscripts refer to compartment number ( N total compartments ) .In compartment ordering , all parameters from the same compartment maintain adjacency along the string .For example : . Na1K(DR)1 ... Na2K(DR)2 ... NaNK(DR)N ( 21 ) .Such an encoding would yield a string of length MN , where M is the number of parameter types and N is the number of compartments .", "label": "", "metadata": {}}
{"text": "For the Traub model , we have a string length of 114 .The De Schutter - Bower model would be searched rather inefficiently with this representation , as the string would contain a total of 19,200 parameters .A scaled function embedding requires assumptions about the parameter distributions that we expect to see in nature .It is reasonable to assume that the actual distributions , with respect to the linear distance from the soma , may be described as noisy instances of a constant , linear , exponential , or sinusoidal function ( Stuart and Sakmann 1994 , Masukawa et al 1991 ) .", "label": "", "metadata": {}}
{"text": "While this approach has the advantage of incorporating domain - based observations , we are limiting the EA to exploring combinations of these plausible distributions .It would be somehow more satisfying to start from random conditions and have the EA converge on a set of parameters that could be fit by scaled functions .In this embedding space ( Figure 5 ) , each parameter distribution is defined by three string values : a value encoding function type ; a maximum function value ; and a minimum function value .An additional term could be optionally added to include a degree of noise to the function distributions .", "label": "", "metadata": {}}
{"text": "Such an encoding would yield a string of length 4 M , where M is the number of parameter types .This representation is independent of any assumptions regarding the number of compartments appropriate for the model .The function would be scaled over the range determined by the maximum and minimum values .The parameter value for each compartment would be the evaluation of the function relative to the position of that compartment over the normalized length of the neuron .The ordering of parameters along the string affects the defining length , e.g. , the difference in the position along the string of those parameters that contribute most to achieving high fitness .", "label": "", "metadata": {}}
{"text": "Consequently , different orderings of the same problem will be differentially affected by the same crossover operator .On one hand , it is useful for crossover to be disruptive so that a thorough exploration of parameter combinations can take place .However , too much disruption may destroy any high order combinations of optimal parameters that the EA has discovered , rendering the overall search inefficient .It is difficult to know a priori which of the three embedding descriptions would be most effective .If we chose a parameter ordering , the first parameter type and the last parameter type along the length of the string would be disrupted with a high probability ( with respect to 1-pt crossover analysis ) .", "label": "", "metadata": {}}
{"text": "Which is more important to maintain - the longitudinal compartment distribution of parameters or the inner - compartment ratio of parameters types to one another ?Without any empirical or theoretical basis for the decision , one approach is to develop more than one crossover operator so that both orderings will be operated upon .Estimating the disruptive effect of crossover on the scaled function embedding is not quite as intuitive because each parameter value for the model is embedded in three ( four ) dimensions instead of one in the string parameter space .Previous work suggests that EAs are most effective when each string parameter can be optimized independently ( Davis 1991 ) .", "label": "", "metadata": {}}
{"text": "Determining the pros and cons of each ordering will require empirical exploration .Genetic algorithms have traditionally used binary representations , primarily because of the relative simplicity in analysis offered by low cardinality alphabets .The parameters in neuron models are real - valued , however .Transformation of real values into a binary representation would have required a trade - off between the precision and efficiency of search .For example , if the resolution we desire for a given parameter is 0.1 arbitrary units , and if we assume a search range of 0.0 - 100.0 , then each binary representation of a parameter would require 10 bits .", "label": "", "metadata": {}}
{"text": "This is a relatively long string over which we would expect recombination to be inefficient .Increasing the number of compartments would exacerbate this problem .The EA would spend too much time searching the least significant digits of the string early in the search .Once the significant digits had converged , they would no longer require searching .However , genetic drift would have caused a certain degree of convergence on the least significant digits such that there would no longer be a random distribution in the population to sufficiently explore that area of parameter space .Real encodings offer many advantages over binary encodings , including the convenience and psychological comfort that comes with the direct correspondence between the mapping spaces .", "label": "", "metadata": {}}
{"text": "For reasons of convenience , we decided to implement a real - valued string .To enable a more direct comparison to Traub 's parameters , we chose to represent the space as a string of 114 values .Since we lacked sufficient information to choose between parameter ordering or compartment ordering , we decided to develop crossover operators that transposed and exploited both orderings .Recombination Operators .Historically , crossover has been associated with genetic algorithms and genetic programming , whereas mutation had been predominantly studied in evolutionary programming and evolution strategies .Exclusive use of either operator is becoming less prevalent , especially in applications .", "label": "", "metadata": {}}
{"text": "The results do not provide us with much guidance , however .The main conclusion is that the success of a recombination strategy is specific to the problem being optimized and can not be predicted a priori .Commonly studied and applied forms of crossover include the 2-point and uniform operators .Early research suggested that the number of crossover points should be low to minimize disruption ( Holland 1975 , De Jong 1975 ) .In contrast , many recent applications have demonstrated superior performance with more disruptive recombination operators ( Syswerda 1989 ) .It has been theoretically demonstrated that 2-pt crossover [ 7 ] is the least disruptive crossover operator .", "label": "", "metadata": {}}
{"text": "The question remains , which crossover operator should we apply to which ordering of our problem ?Mutation is both a mechanism for local search and a mechanism to prevent solution components lost during crossover from being eliminated for all successive generations .Mutation is applied in binary representations by flipping the value of the bit in the randomly selected position .In real - valued strings , more sophisticated scaling or replacement methods are required .Genetic algorithms typically apply mutation as a background operator at low rates , for example 0.1 % ( Goldberg 1989 ) .In contrast , it has been shown that a high mutation rate can be beneficial in some applications ( Bramlette 1991 ) .", "label": "", "metadata": {}}
{"text": "Because we were left with questions about which rate and kind of real - valued mutation would be successful for our problem , we decided to let the EA tell us which strategy worked best .This was implemented as a self - adaptive strategy for choosing the rate of application of all the operators of potential interest to us .This approach has been used to select mutation rates in evolution strategies ( B\u00e4ck et al 1991 , Saravanan et al 1995 ) and crossover rates in genetic algorithms ( Spears 1995 ) .Further , Parsons et al ( 1995 ) report a synergistic effect gained by retaining many types of operator , suggesting additional benefits gained from this strategy .", "label": "", "metadata": {}}
{"text": "The set of recombination operators includes : crossover 1 ( 2-point , compartment ordering ) , crossover 2 ( 2-point , parameter ordering ) , single parameter mutate , parameter - type scale , and scale all ( Figure 6 ) .Two versions of the 2-point crossover operator were applied to exploit both compartment ordering and parameter ordering .This allowed us to reduce positional bias while maintaining low disruption .The single parameter mutation explored solutions adjacent in a single dimension to current solutions .We assumed that the ratios of the conductances of each of the channel types were important determinants of high fitness solutions and subsequently developed scaling operators , scale all and parameter - type scale , to explore this assumption .", "label": "", "metadata": {}}
{"text": "Subsequent probabilities were based on the success of a given operator relative to that of the others .The minimum probability of operator selection was fixed at 1 % .A success was awarded when a parameter set produced by a given operator scored a fitness value above the average fitness of the population .Figure 6 The crossover and mutation operators for the EA parameter search .Two parent parameter strings ( represented with red and blue borders ) were randomly selected from the mating pool for crossover operations .One parameter string was randomly selected from the mating pool for mutation operations .", "label": "", "metadata": {}}
{"text": "Two points along the string are randomly selected .The parent strings contribute alternate regions to the offspring string .b , Crossover 2 ( 2-pt , parameter ordering ) .The procedure is the same as in ( a ) , except the parameters were first reordered such the same compartment parameters were adjacent .c , Single Parameter Mutate .Two random numbers were selected .The first number determined which of the parameters would be operated on .The second number was scaled to the range appropriate to that parameter and replaced the original value of the selected parameter .", "label": "", "metadata": {}}
{"text": "Two random numbers were selected .The first number determined which of the parameter types would be operated on .The second number was a scaling factor that ranged from 0.0 to 2.0 .All conductances of the selected channel type were multiplied by the scaling factor .e , Scale All Mutation .Random number selection ( a coin flip with probability 0.5 ) determined whether all values in a string were scaled by constant multiplicative factor 0.9 or 1.1 .Selection Mechanisms .Selection identifies and retains high fitness strings .Selection mechanisms are characterized based on strength .", "label": "", "metadata": {}}
{"text": "Such a strategy could potentially lead to premature convergence on a less than optimal solution , but the answer is obtained quickly .Weak selection maintains a highly diverse population , but at the expense of increased search time .Very weak selection results in a random walk search .Since we want to evolve a manifold and not just a \" best \" single solution , we will avoid the stronger methods .This would potentially cost us an increased number of function evaluations , but we hoped that we would find better solutions .We developed the following selection mechanism .", "label": "", "metadata": {}}
{"text": "Population members from both current and previous generations were admitted to the mating pool if their individual fitness values exceeded the extinction threshold .This threshold - based \" elite selection \" mechanism allowed a highly fit individual to contribute to the mating pool for multiple generations if its fitness was sufficiently high , thus preventing the loss of good strings ( B\u00e4ck and Hoffmeister 1991 ) .Many EA implementations allow multiple copies of high fitness strings to contribute to the mating pool in proportion to the absolute or scaled fitness value of that string relative to the other strings ( Forrest 1993 , Goldberg 1989 ) .", "label": "", "metadata": {}}
{"text": "This strategy reduced the possibility of premature convergence caused by the overexpression of proportionately higher fitness strings .The initial population size was 1000 parameter sets .Preliminary results suggested that this population size provided a sufficient compromise between diversity and efficiency [ 10 ] .The population size must be large enough to maintain a diverse gene pool .Otherwise , the EA must primarily rely on mutations to explore the space .In such a case , the rate of improvement is reduced to that of a random walk search .However , an extremely large population could increase the amount of computer time needed to solve the problem and might not improve the rate at which the problem is solved ( Macready et al 1996 ) .", "label": "", "metadata": {}}
{"text": "Preliminary results ( data not shown ) suggested that parameter sets with values outside of these bounds did not yield fitnesses greater than 0.1 .The recombination operators allowed subsequent generations to explore beyond these initial bounds .Termination .Termination criteria in the EA literature are somewhat arbitrary because the statistical nature of the search prohibits knowing when the population will evolve to a given fitness value .Our arbitrary termination criterion was satisfied after 200 CPU hours had elapsed on each of eight dedicated Silicon Graphics Power Challenge R8000 90 MHz processors ( approximately 125 Tera floating point operations total ) .", "label": "", "metadata": {}}
{"text": "We find that a CPU - based termination criteria gives us a much more informative measure of what we can potentially solve with the CPU time we have available .Designing a Fitness Measure .It was not clear how much information and what kind of information should be included in our fitness criteria .However , it would seem to be possible to manipulate the algorithm to produce whatever results we want .Therefore , any claims regarding the power of the EA parameter optimizing approach would be compromised if we tweaked the fitness measure until we obtained good performance .", "label": "", "metadata": {}}
{"text": "While the specific quantitative aspects of our criteria were extracted from the biological literature , our approach to fitness measure design was intended to be a generally applicable to any time series .We partitioned the information into three categories : 1 ) mathematical requirements ; 2 ) etiological satisfaction ; and 3 ) waveform decomposition .The decision to award 20 % of the fitness points to the mathematical requirements and etiological satisfaction categories and 60 % to the waveform decomposition was arbitrary .Given the variability in real physiological responses , we distributed the assignment of scores using a gradient rather than a step function .", "label": "", "metadata": {}}
{"text": "By coaxing the evolution along through the assignment of partial credit , we provide the EA with more information to guide the search given a constant number of fitness criteria .Mathematical requirements : Time series exhibit damped , periodic , chaotic , or random characteristics .The stimulus we applied to the model should generate approximately periodic bursting .Therefore , the first goal was to partition the space of all solutions to eliminate parameter spaces that yield damped or steady state time series [ 11 ] .Credit was assigned for fulfilling the following successive criteria .Does the solution demonstrate spiking at all ?", "label": "", "metadata": {}}
{"text": "Does it spike during the last 20 % of the time series ?Etiological criteria : Does the parameter set produce periodic solutions for the wrong reasons ?The somatic time series should yield spiking in response to the depolarizing current injection .Dendritic compartments with sustained , elevated voltages could provide a source of depolarizing axial current that would inappropriately ( in the absence of synaptic currents in this model ) lead to somatic spiking .Thinking like an experimental biologist , we considered these solutions equivalent to neurons which had their dendritic membranes damaged during experimental preparation and rejected them .", "label": "", "metadata": {}}
{"text": "Instead , we decomposed the evaluation of each spatiotemporal dataset into a set of characteristics extended along multiple scales ( figure 7 ) .The largest scale feature was the somatic burst rate .We then zoomed into the characteristics of the individual bursts to evaluate such features as the peak - to - burst ratio , burst width , and the least squares fit to an experimental CA3 burst waveform .At the finest scale , we evaluated the postburst afterhyperpotential amplitude as well as quantitative aspects of individual spikes such as the width and height .Lastly , spatial behaviors such as the relative calcium influx and peak height were examined and scored .", "label": "", "metadata": {}}
{"text": "( A ) Global behavior , such as the burst rate , was evaluated and scored .Time series R and S received full credit for satisfaction of this criteria because the burst rates , approximatley 0.8 and 1.2 Hz respectively , are within the experimentally observed range .( B ) Qualities of the individual bursts , such as peak - to - burst ratio and burst width , were evaluated and scored .The left burst received maximal credit for a peak - to - burst ratio and burst width within the acceptable ranges .( C )", "label": "", "metadata": {}}
{"text": "Finally , we specifically included observations noted by Traub into our evaluation .It does not appear that Traub proposed a list of criteria and then optimized the parameters to meet these criteria .HPC and Parallelism .The simulation executed for five seconds of neuron - time so that sufficient data were available for evaluation by the fitness filter .Each simulation required 49 seconds on a Silicon Graphics R8000 90 MHz processor .For a model such as the Purkinje cell ( De Schutter and Bower 1994 ) , each simulation could require hours .The evaluation of each parameter set required a significant amount of computer time .", "label": "", "metadata": {}}
{"text": "Our EA searches have been implemented in parallel in the following four ways : 1 ) fork / exec functions ; 2 ) rcp / rsh scripts ; 3 ) Loadleveler / DQS application management ; and 4 ) MPI / PVM libraries .We used combinations of these approaches to maximize our access to heterogeneous computing environments .This environment included a 9 processor Cray C916/10512 , a 12 processor Silicon Graphics Power Challenge ( R8000 75 MHz ) , a 8 processor Silicon Graphics Power Onyx ( R8000 90 MHz ) , 8 IBM RS/6000 Model 590 workstations , and a 14 processor IBM SP Supercomputer .", "label": "", "metadata": {}}
{"text": "No special software or libraries are required .Load balancing is handled by the operating system .However , the implementation is limited to shared memory architectures such as machines offered by Cray and Silicon Graphics .Our EA program executed each simulation independently as a child process .Each simulation was assigned an identification number using the putenv function .The simulator used the identification number to open the correct parameter file and to name the output file containing the fitness measure .The wait function monitored the completion status of the child processes .We found this method sufficient for most of our small to medium sized applications .", "label": "", "metadata": {}}
{"text": "This method required that an executable copy of the simulation program was installed on every machine .In the interest of security , many system administrators disable remote commands to prevent password - free access to their machines .While we were able to access many departmental workstations , we could not access the HPC machines in this fashion .To maximize efficiency , we wrote load balancing routines to determine the CPU load on each machine before submitting jobs .Unfortunatley , if other users submitted jobs after we started a simulation , we had to wait for simulations on the loaded machines to complete .", "label": "", "metadata": {}}
{"text": "We found this method of obtaining parallelism the least effective with respect to the machines to which we had access .However , this is a great way for researchers without access to machine time at a HPC center to steal extra CPU cycles from idle workstations overnight !LoadLeveler is a sofware product developed by IBM for managing parallel applications .Versions of this product are available for IBM RS/6000 architectures , Sun , HP and Silicon Graphics workstations .This is reliable and easy to use software featuring machine specific configurations , load balancing , checkpointing , and a graphical interface for developing applications and monitoring the status of jobs currently executing .", "label": "", "metadata": {}}
{"text": "The good news and bad news is that this is a commercial product ; while it is technically well - supported by IBM , it is not free .As a low budg et al ternative , there are several shareware products available via anonymous ftp that claim similar functionality and features .We did not evaluate the claims of the shareware products , but we found LoadLeveler to be a useful and stable tool .MPI ( Message Passing Interface ) and PVM ( Parallel Virtual Machine ) are libraries of routines for passing messages between processors that potentially represent a range of architectures .", "label": "", "metadata": {}}
{"text": "The libraries are available via anonymous ftp , but must be installed on every machine participating in the virtual network .They offer a wide variety of communication classes including gather , scatter , and all - to - all using communication modes such as standard , synchronous , ready , or buffered in blocking and non - blocking forms ( Snir et al 1995 , Gest et al 1994 ) .These libraries have limited error handling routines , do not support dynamic task spawning , and require the user to incorporate load balancing algorithms .However , the widespread interest in these libraries by developers and the adoption of these libraries by multiple vendors have resulted in a number of third party applications now available to manage CPU resources when the libraries themselves are insufficient .", "label": "", "metadata": {}}
{"text": "This is because the communication time required for the data transfer is large relative to the computation time performed by each distributed CPU ( Kumar et al 1994 ) .Most parallel architectures , ( ex .Thinking Machines Corporation 's CM-5 , Cray 's T3E ) have dedicated hardware and parallel versions of standard programming languages ( ex .Our programs yielded approximately linear speedup [ 12 ] with both the message passing and file transfer implementations because of the relatively large amount of CPU time required per simulation .However , it has been our observation from the EA literature that most fitness functions do not require as much CPU time as we needed .", "label": "", "metadata": {}}
{"text": "However , depending on the application and the available architectures , process / file - based parallelism is easy to implement and may be just as efficient as other methods .A Success Story .We judged our application to be a success by two criteria .First , the average fitness of the population quickly exceeded the fitness score achieved by Traub 's parameter set .Second , the high fitness parameter sets produced time series that resembled those observed experimentally .Similar results were obtained in three separate experiments , each using different random initial conditions .As a bonus , we identified manifolds for our low dimensional descriptors ( the total conductance and the excitatory / inhibitory conductance ratio ) of the evolved high dimensional parameter space which will allow us to reduce the parameter space in future refining searches .", "label": "", "metadata": {}}
{"text": "The EA evolved a population of high fitness parameter strings from random initial conditions .The initial generation yielded an average fitness of 0.04 .The EA improved the overall fitness of the population , rapidly at first but more slowly at higher fitness .The average fitness per generation exceeded the fitness of Traub 's parameter set ( 0.85 ) by generation 62 .The best fitness overall ( 0.92 ) was obtained in generation 101 .Figure 8 The average fitness per generation demonstrates rapid improvement .Because the population size is dynamic , the x - axis is only approximate .", "label": "", "metadata": {}}
{"text": "Neither the EA - produced parameter sets nor Traub 's parameter set satisfied all the fitness criteria , suggesting that additional channel kinetic equations may be necessary to achieve a higher correspondence to experimental results .A quantitative description is available elsewhere ( Eichler West 1996 ) .Figure 9 The solution with highest fitness yield spatiotemporal behaviors consistent with those observed experimentally .The y - axis range is -85 mV to +40 mV.The length of the x - axis is approximately 80 ms . .We examined the manifolds for the total conductance and the excitatory / inhibitory conductance ratio .", "label": "", "metadata": {}}
{"text": "Thus , while selecting a parameter set within these ranges does not guarantee a high fitness solution , selection of any parameter set outside of this range yielded low fitness solutions .Figure 10 Solution manifolds for the low dimensional metrics as a function of fitness .a , Total conductance manifold as a function of fitness score for the search space explored by the EA .b , The excitatory / inhibitory conductance ratio manifold as a function of fitness score .Note that the range with respect to the y - axis decreases with increasing fitness .Conclusions .", "label": "", "metadata": {}}
{"text": "Further , the automation aspects of the search process promises to improve the efficiency of model development .By robustness , we mean that the manifold of high fitness solutions produced by the EA application demonstrates that the model produces appropriate behaviors over a range of parameter values , reflecting a variability analogous to that observed in nature .The results of our proof - of - concept experiment yielded improved correspondence between simulated and experimental behaviors .The method should find general applicability in a broad range of HPC simulation endeavors outside of neurophysiological modeling .The manifold approach suggests methods of analysis that would not have been possible with previous approaches to parameter fitting .", "label": "", "metadata": {}}
{"text": "While we carefully designed our recombination operators to explore outside of the boundaries of our initial conditions , high fitness solutions confined themselves to a more restricted area of parameter space .The interpretation and significance of the individual channel parameter manifolds require a much greater description of the biological system and are thus not within the scope of this chapter [ 13 ] .However , all the channel distribution manifolds resembled noisy instances of simple functions : linear ( potassium A , calcium - dependent potassium ) ; exponential ( sodium , potassium delayed rectifier ) ; or , sinusoidal ( calcium , potassium AHP ) .", "label": "", "metadata": {}}
{"text": "The manifolds were subsequently analyzed to reveal covariance relationships between parameters , thereby providing an indication of the sensitivity of parameters with respect to modulations in the other parameters .The manifolds identified varying degrees of parameter sensitivity for the potassium A and the calcium - dependent potassium channels and a strong positive covariance between the sodium and potassium delayed rectifier channel distributions .The complementary colocalization of the calcium and potassium AHP distributions is suggestive of an experimentally testable role for calcium - dependent conductances in the control of spatiotemporal plasticity ( Eichler West 1996 ) .One future goal made evident by these preliminary studies is the need to reduce the number of function evaluations required to sufficiently search parameter space by applying better EA selection strategies .", "label": "", "metadata": {}}
{"text": "The search strategy we used required access to significant amounts of CPU time on supercomputer - class machines .We believe that our EA - based approach to neuronal modeling offers a powerful new set of methods and interpretive paradigms , but it will not be generally useful to all neural modelers until it is implementable by those who do not have access to high performance computers .How do we know when we are including too much or too little information to the fitness measure ?The concern is that additional criteria will increase the computational time required to evaluate the response of each simulation with little or no gain for parameter manifold refinement .", "label": "", "metadata": {}}
{"text": "The set of high fitness solutions created from the \" training \" criteria can be evaluated for criteria for which there is no explicit selection ( the \" testing set \" ) .Those testing criteria which fail must necessarily become members of the new training set .We are first and foremost neuroscientists with collective intuition into our system gained by reviewing the experimental literature , performing experiments in the laboratory , and simulating models of neurons .In this first attempt at applying an EA , we tried to incorporate much domain - based intuition into the algorithm design .", "label": "", "metadata": {}}
{"text": "However , with respect to the NFL theorem , we are still left with two nagging questions .Would another method perform better , given a similar attentiveness to the incorporation of domain - based information ?Did we get lucky ?Answers to these questions will require significantly more empirical research .Acknowledgements .This work was supported by grants from Cray Research / Minnesota Supercomputer Institute and NIMH R01-MH52903 .The authors gratefully acknowledge generous access to supercomputing facilities provided by the Minnesota Supercomputer Institute , the IBM Shared University Research Project , and the Laboratory for Computational Sciences and Engineering at the University of Minnesota .", "label": "", "metadata": {}}
{"text": "RMEW would also like to thank Ihab Awad and Joe Haberman for their technical support with parallelization issues , and Jon Gottesman and David Yuen for thought - provoking discussions .References .Abbott , L. F. , Rolls , E. T. , and Tovee , M. J. Representational capacity of face coding in monkeys .Cerebral Cortex 6 : 498 - 505 , 1996 .Back , T. , and Hoffmeister , F. \" Extended selection mechanisms in genetic algorithms . \"In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kaufmann , 92 - 99 , 1991 .", "label": "", "metadata": {}}
{"text": "In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kaufmann , 2 - 9 , 1991 .Bagchi , S. et al .\" Exploring problem - specific recombination operators for job shop scheduling .\"In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kaufmann , 10 - 17 , 1991 .Bhalla , U. S. , and Bower , J. M. Exploring parameter space in detailed single neuron models : simulations of the mitral and granule cells of the olfactory bulb .", "label": "", "metadata": {}}
{"text": "Bramlette , M. F. \" Initialization , mutation and selection methods in genetic algorithms . \"In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kauffman , 100 - 107 , 1991 .Chetkovich , D. M. et al .N - Methyl - D - Aspartate receptor activation increases cAMP levels and voltage - gated Ca2 + channel activity in area CA1 of hippocampus .Proceedings of the National Academy of Sciences of the USA 88 : 6467 - 6471 , 1991 .", "label": "", "metadata": {}}
{"text": "University of Alberta Technical Report TR96 - 18 1996 .Davis , L. \" Bit - climbing , representational bias , and test suite design . \"In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kaufmann , 18 - 23 , 1991 .Davis , L. et al . \"A genetic algorithm for survivable network design . \"In Fifth International Conference on Genetic Algorithms in University of Illinois at Urbana - Champaign , edited by Forrest , S. , Morgan Kaufmann , 408 - 415,1993 .", "label": "", "metadata": {}}
{"text": "De Schutter , E. A consumer guide to neuronal modeling software .Trends in Neurosciences 15 : 462 - 464 , 1992 .De Schutter , E. , and Bower , J. M. An active membrane model of the cerebellar purkinje cell .I. Simulation of current clamps in slice .Journal of Neurophysiology 71 : 375 - 400 , 1994 .Denk , W. , Strickler , J. H. , and Webb , W. W. Photon laser scanning fluorescence microscopy .Science 248 : 73 - 76 , 1990 .Ebner , T. J. , and Chen , G. Use of voltage - sensitive dyes and optical recordings in the central - nervous - system .", "label": "", "metadata": {}}
{"text": "Edmonds , B. et al .Contributions of two types of calcium channels to synaptic transmission and plasticity .Science 250 : 1142 - 1146 , 1990 .Eichler West , R. M. \" On the development and interpretation of parameter manifolds for biophysically robust compartmental models of CA3 hippocampal neurons .\" Doctoral Thesis , University of Minnesota , 1996 .Eichler West , R. M. , and Wilcox , G. L. A renumbering method to decrease matrix banding in equations describing branched neuron - like structures .Journal of Neuroscience Methods 68 : 15 - 19 , 1996 .", "label": "", "metadata": {}}
{"text": "Science 270 : 756 - 757 , 1995 .Fitzgerald , K. et al .Multiple forms of non - associative plasticity in Aplysia : a behavioral , cellular , and pharmacological analysis .Philos Trans R Soc Lond 329 : 171 - 178 , 1990 .Forrest , S. Genetic algorithms : principles of natural selection applied to computation .Science 261 : 872 - 878 , 1993 .Foster , W. R. , Ungar , L. H. , and Schwaber , J. S. Significance of conductances in Hodgkin - Huxley models .Journal of Neurophysiology 70 : 2502 - 2518 , 1993 .", "label": "", "metadata": {}}
{"text": "PVM : Parallel Virtual Machine .A Users ' Guide and Tutorial for Networked Parallel Computing .Cambridge , MA : MIT Press , 1994 .Georgopoulos , A. P. , Taira , M. , and Lukashin , A. Cognitive neurophysiology of the motor cortex .Science 260 : 47 - 52 , 1993 .Goldberg , D. E. Genetic algorithms in search , optimization , and machine learning .Reading , MA : Addison - Wesley , 1989 .Goldberg , D. E. Real - coded genetic algorithms , virtual alphabets , and blocking .Complex Systems 5 : 139 - 168 , 1991 .", "label": "", "metadata": {}}
{"text": "The Journal of General Physiology 27 : 37 - 60 , 1943 .Hart , W. E. , and Belew , R. K. \" Optimizing an arbitrary function is hard for Genetic Algorithms . \"In Fourth International Conference on Genetic Algorithms in University of California , San Diego , edited by Belew , R. K. , and Booker , L. B. , Morgan Kaufmann , 190 - 195 , 1991 .Hille , B. Ionic Channels of Excitable Membranes . second ed . , Sunderland , MA : Sinauer Associates Inc. , 1992 .Hines , M. Efficient computation of branched nerve equations .", "label": "", "metadata": {}}
{"text": "Hodgkin , A. L. , and Huxley , A. F. A quantitative description of membrane current and its application to conduction and excitation in nerve .Journal of Physiology 117 : 500 - 544 , 1952 .Hodgkin , A. L. , and Rushton , W. A. H. The electrical constants of a crustacean nerve fibre .Proceedings of the Royal Society of London Series B 133 : 444 - 479 , 1946 .Holland , J. H. Adaptation in natural and artificial systems .Ann Arbor , MI : The University of Michigan Press , 1975 .Holmes , R. W. , and Rall , W. Estimating the electrotonic structure of neurons with compartmental models .", "label": "", "metadata": {}}
{"text": "Jantsch , E. The self - organizing universe : scientific and human implications of the emerging paradigm of evolution .Elmsford , New York : Pergamon , 1980 .Johnston , D. et al .Active properties of neuronal dendrites .Annual Review of Neuroscience 19 : 165 - 186 , 1996 .Kallen , R. G. , Cohen , S. A. , and Barchi , R. L. Structure , function , and expression of voltage - dependent sodium channels .Molecular Neurobiology 7 : 383 - 428 , 1993 .Keynes , R. D. A new look at the mechanism of activation and inactivation of voltage - gated ion channels .", "label": "", "metadata": {}}
{"text": "Kido , M. et al .Mantle viscosity derived by genetic algorithm using oceanic geoid and tomography for whole - mantle versus blocked - flow situations .Phys .Earth Planet Int .Klein , M. , and Kandel , E. R. Presynaptic modulation of voltage - dependent Ca2 + current : mechanism for behavioral sensitization in Aplysia californica .Proceedings of the National Academy of Sciences of the USA 75 : 3512 - 3516 , 1978 .Kuhar , M. J. , and Unnerstall , J. R. Quantitative receptor mapping by autoradiography : some current technical problems .Trends in Neurosciences 49 - 53 , 1985 .", "label": "", "metadata": {}}
{"text": "Introduction to Parallel Computing : Design and Analysis of Algorithms .Benjamin - Cummings Addison - Wesley Publishing Company , 1994 .Laurent , G. Dynamical representation of odors by oscillating and evolving neural assemblies .Trends in Neurosciences 19 : 489 - 496 , 1996 .Li , M. et al .Convergent regulation of sodium channels by protein kinase C and cAMP - dependent protein kinase .Science 261 : 1439 - 1442 , 1993 .Macready , W. G. , Siapas , A. G. , and Kauffman , S. A. Criticality and parallelism in combinatorial optimization .", "label": "", "metadata": {}}
{"text": "Maletic - Savatic , M. , Lenn , N. J. , and Trimmer , J. S. Differential spatiotemporal expression of K+ channel polypeptides in rat hippocampal neurons developing in situ and in vitro .Journal of Neuroscience 15 : 3840 - 3851 , 1995 .Mascagni , M. V. : Numerical methods for neuronal modeling .In Methods in Neuronal Modeling , edited by Koch , C. , and Segev , I. , Cambridge , Ma : MIT Press , 1989 , p. 439 - 483 .Masukawa , L. M. , Hansen , A. J. , and Shepherd , G. Distribution of single - channel conductances in cultured rat hippocampal neurons .", "label": "", "metadata": {}}
{"text": "Monster , A. W. , and Chan , H. Isometric force production by motor units of extensor digitorum communis in man .Journal of Neurophysiology 40 : 1432 - 1443 , 1977 .Numann , R. , Caterall , W. A. , and Scheuer , T. Functional modification of brain sodium channels by protein kinase C phosphorylation .Science 254 : 115 - 118 , 1991 .Parsons , R. J. , Forrest , S. , and Burks , C. Genetic algorithms , operators , and DNA fragment assembly .Machine Learning 21 : 11 - 33 , 1995 .", "label": "", "metadata": {}}
{"text": "Drug Development Research 33 : 295 - 318 , 1994 .Press , W. H. et al .Numerical recipes .The art of scientific computing . second ed . , Cambridge : Cambridge University Press , 1992 .Rall , W. Theory of physiological properties of dendrites .Annals New York Academy of Science 96 : 1071 - 1092 , 1962 .Rall , W. : Cable theory for dendritic neurons .In Methods in Neuronal Modeling , edited by Koch , C. , and Segev , I. , Cambridge , Mass : MIT Press , 1989 , p. 9 - 62 .", "label": "", "metadata": {}}
{"text": "Voltage - gated K+ channel beta - subunits - expression and distribution of KV - Beta-1 and KV - Beta-2 in adult rat brain .Journal of Neuroscience 16 : 4846 - 4860 , 1996 .Sakmann , B. , and Neher , E. Single Channel Recording .New York : Plenum , 1983 .Saravanan , N. , Fogel , D. B. , and Nelson , K. M. A Comparison of Methods for Self - Adaptation in Evolutionary Algorithms .BioSystems 36 : 157 - 166 , 1995 .Segev , I. , Fleshman , J. W. , and Burke , R. E. : Compartmental models of complex neurons .", "label": "", "metadata": {}}
{"text": "Snir , M. et al .MPI : The Complete Reference .Cambridge , MA : MIT Press , 1995 .Spears , W. M. \" Adapting Crossover in Evolutionary Algorithms . \"In Proceedings of the Fourth Annual Conference on Evolutionary Programming in San Diego , CA , 1991 .Spears , W. M. , and De Jong , K. A. \" An analysis of multi - point crossover . \"In Proceedings of the Foundations of Genetic Algorithms Workshop in Bloomington , IN , 1990 .Spruston , N. , Jaffe , D. B. , and Johnston , D. Dendritic attenuation of synaptic potentials and currents - the role of passive membrane properties .", "label": "", "metadata": {}}
{"text": "Spruston , N. , and Johnston , D. Perforated patch - clamp analysis of the passive membrane properties of three classes of hippocampal neurons .Journal of Neurophysiology 67 : 508 - 529 , 1992 .Stuart , G. J. , and Sakmann , B. Active propagation of somatic action potentials into neocortical pyramidal cell dendrites .Nature 367 : 69 - 72 , 1994 .Syswerda , G. \" Uniform crossover in genetic algorithms . \"In Third International Conference on Genetic Algorithms in edited by Shaffer , J. D. , Morgan Kaufmann , 1989 .Theunissen , F. E. et al .", "label": "", "metadata": {}}
{"text": "Journal of Neurophysiology 75 : 1345 - 1364 , 1996 .Toro , L. , and Stefani , E. Calcium - activated K+ channels - metabolic regulation .Journal of Bioengineering - B 23 : 561 - 576 , 1991 .Traub , R. D. et al .Analysis of gamma rhythms in the rat hippocampus in vitro and in vivo .Journal of Physiology 493 : 471 - 484 , 1996 .Traub , R. D. et al .A model of a CA3 hippocampal pyramidal neuron incorporating voltage - clamp data on intrinsic conductances .Journal of Neurophysiology 66 : 635 - 650 , 1991 .", "label": "", "metadata": {}}
{"text": "Biophysical Journal 46 : 73 - 84 , 1984 .Turner , D. A. , and Schwartzkroin , P. A. Steady - state electrotonic analysis of intracellularly stained hippocampal neurons .Journal of Neurophysiology 44 : 184 - 199 , 1980 .Westenbroek , R. E. , Ahlijanian , M. K. , and Catterall , W. A. Clutsering of L - type calcium channels at the base of major dendrites in the hippocampal pyramidal neurons .Nature 347 : 281 - 284 , 1990 .Whitley , D. et al .\" Comparing Heuristic , Evolutionary and Local Search Approaches to Scheduling . \"", "label": "", "metadata": {}}
{"text": "Wolpert , D. H. , and Macready , W. G. No free lunch theorems for search .Santa Fe Institute , 1995 .Wonderlin , W. F. , French , R. J. , and Arispe , N. J. : Recording and analysis of currents from single ion channels .In Neurophysiological Methods , edited by Vanderwolf , C. H. , Clifton , NJ : Humana Press , 1990 , p. 35 - 142 .MR Image Based Approach for Metal Artifact Reduction in X - Ray CT . 1 Department of Industrial Engineering and Management , Faculty of Technical Sciences , University of Novi Sad , Trg Dositeja Obradovica 7 , 21000 Novi Sad , Serbia 2 Department of Medical Imaging , University Hospital , R\u00e4mistrasse 100 , 8091 Z\u00fcrich , Switzerland .", "label": "", "metadata": {}}
{"text": "Academic Editors : C. Kappas and C. S. Morris .Copyright \u00a9 2013 Andras Anderla et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .Abstract .For decades , computed tomography ( CT ) images have been widely used to discover valuable anatomical information .Metallic implants such as dental fillings cause severe streaking artifacts which significantly degrade the quality of CT images .In this paper , we propose a new method for metal - artifact reduction using complementary magnetic resonance ( MR ) images .", "label": "", "metadata": {}}
{"text": "The proposed algorithm corrects reconstructed CT images .The projected data which is affected by dental fillings is detected and the missing projections are replaced with data obtained from a corresponding MR image .A simulation study was conducted in order to compare the reconstructed images with images reconstructed through linear interpolation , which is a common metal - artifact reduction technique .The results show that the proposed method is successful in reducing severe metal artifacts without introducing significant amount of secondary artifacts .Introduction .Trimodality systems are capable of acquiring computed tomography ( CT ) , positron emission tomography ( PET ) , and magnetic resonance ( MR ) datasets in a single session .", "label": "", "metadata": {}}
{"text": "One of them is the reduction of metal artifacts in CT images using complementary MR images .Computed tomography ( CT ) imaging systems create cross - sectional images of soft tissue , internal organs , bones , and blood vessels .Although these systems provide detailed information about patient 's internal structure , they do not provide information on tissue function which is important for differentiation between normal and pathologic functions [ 1 ] .Positron emission tomography ( PET ) provides information about functional processes within human body .However , PET images are not enough for precise localization of organs or lesions .", "label": "", "metadata": {}}
{"text": "Another benefit from combining these two modalities is that CT images can be used to generate attenuation correction factors in PET emission data [ 3 ] .Magnetic resonance ( MR ) imaging creates images of atoms ' nuclei using the property of nuclear magnetic resonance .This allows MRI systems to extract more detailed information about the human body than is possible to get with X - rays .Artifacts occur in MR images in the presence of ferromagnetic metal , but the study conducted by Eggers et al .[ 4 ] shows that dental fillings did not reduce the quality of images from an MRI sequence .", "label": "", "metadata": {}}
{"text": "[5 ] .Metallic implants in CT images cause dark and bright streaking artifacts because of the high atomic number of metal .Low - energy X - ray photons passing through these objects are highly attenuated , and this leads to loss of projection data .In addition to this type of effects , different physical phenomena are reported in the literature , which lead to CT images affected by the artifacts .Some of them are beam hardening , noise , Compton scattering , partial volume effect , cardiac , and respiratory motion [ 1 , 2 ] .", "label": "", "metadata": {}}
{"text": "Low energy photons in an X - ray beam are preferentially absorbed and as a result , the beam 's energy gradually increases .When the beam is harder , it is less attenuated further .Yan et al .[ 6 ] presented a reconstruction algorithm using a polychromatic X - ray beam and they were able to remove a substantial portion of beam hardening artifacts .There are several types of image noise that can affect the imaging process , but the ultimate source is the random , statistical noise [ 7 ] .The author showed that this type of noise can be avoided by increasing the X - ray exposure .", "label": "", "metadata": {}}
{"text": "These photons are then useless for the reconstruction .Artifacts in the direction of highest attenuation are the strongest artifacts [ 8 ] .Partial volume effect is present when a voxel is partially filled with certain substance .When the voxel is reconstructed , it represents the weighted average of the attenuations for all substances in that particular voxel .Decreasing slice thickness can decrease the partial volume effect .Several algorithms have been proposed for the correction of these artifacts [ 9 ] .Although these methods gave promising results in computer simulations , they failed when applied to clinical data .", "label": "", "metadata": {}}
{"text": "There are several types of motion that can occur in such conditions : discrete motion , pulsating motion , and continuous motion .All of these cause different types of artifacts .If the acquisition speed is increased , the probability of motion artifacts is decreased .An algorithm which tries to solve this issue is presented by Glover and Pelc [ 10 ] .The presence of metal objects leads to the amplification of the effects of listed phenomena .Metal objects are removed from the patient , if possible , but usually this is not the case in modern medical practice .", "label": "", "metadata": {}}
{"text": "The easiest solution for metal artifact avoidance would be to use less attenuating materials , such as titanium .Another possibility is to use X - ray beams with higher energy , but this approach does not give very good results and also the patient is exposed to a larger dose of radiation .Therefore , numerous techniques have been proposed aimed at the elimination or reduction of the effects caused by metallic objects .These approaches can be divided into two groups : implicit and explicit methods .Implicit methods try to suppress artifacts without the need to apply algorithmic mathematical methods for metal - artifact correction .", "label": "", "metadata": {}}
{"text": "On the other hand , explicit methods are more general and they represent the main focus of researchers .An extensive list of existing techniques and methods can be found in [ 11 ] .According to the authors , explicit techniques can be grouped into four categories ( Figure 1 ): corrections in the sinogram domain , corrections in the image domain , iterative reconstruction algorithms , and hybrid sinogram correction .Corrections in the sinogram domain can be further divided into interpolation - based and noninterpolation - based sinogram correction techniques .Some of the interpolation - based algorithms are described in [ 10 , 12 , 13 ] .", "label": "", "metadata": {}}
{"text": "The affected projection bins are usually found with simple thresholding techniques .After the correction is performed in the sinogram domain , data is back - projected in order to reconstruct the image .Noninterpolation - based algorithms use different approaches instead of interpolation , such as Monte Carlo simulation [ 14 ] .An approach which uses bilateral filtering is presented by Cheng and Liu [ 15 ] .First they start with the metal region identification .This was achieved by thresholding , since pixels in the metallic region have higher pixel values .In addition to thresholding , a connectivity test was conducted to obtain the regions with metal .", "label": "", "metadata": {}}
{"text": "Filtered image was then forward projected and new values were assigned to affected projection bins .Finally , segmented metallic objects were superimposed back onto the reconstructed image .Methods in the sinogram domain generally give better results than other methods .On the other hand , working with raw data is often cumbersome due to its large size and the need to account for proprietary data formats and specific system geometries .Corrections in the image domain deal with reconstructed images .Kennedy et al .[16 ] proposed a method which uses a Bayes classifier applied in an annular region around groups of saturated pixels .", "label": "", "metadata": {}}
{"text": "Detected pixel values were substituted with pixel values of soft tissue or bone .Sohmura et al .proposed an approach [ 17 ] in which a dental cast model is used to replace regions affected with artifacts .The drawback of image - domain methods is that affected pixel values are most often replaced with constant values leading to further degradation of the visual image quality .However , working with reconstructed images is faster and more generic , as it can be applied across different acquisition systems .Iterative reconstruction is carried out after the detection of affected projection bins .", "label": "", "metadata": {}}
{"text": "The main reason for this is that data - sets in CT are much larger making the process computationally very intensive .Iterative reconstruction algorithms include algebraic and statistical techniques .Several methods have been proposed in [ 18 - 20 ] .In addition to the techniques mentioned , several hybrid techniques exist , which try to combine different basic methods .Nuyts and Stroobants proposed a method which is a combination of linear interpolation and iterative reconstruction [ 21 ] .Another hybrid approach is presented by Tuy , where the author used a combination of interpolation and noninterpolation - based sinogram correction [ 22 ] .", "label": "", "metadata": {}}
{"text": "Since metal objects create regions with missing projection data , their results strongly depend on the size and number of metal implants present in the patient 's body .Recent trend in availability of trimodality systems provides an opportunity to use complementary MR data for reduction of artifacts in CT images .This area is not yet sufficiently explored and one of the first efforts in this field is presented by Delso et al .[ 23 ] .In this paper , we propose a new method for the reduction of metal artifacts in CT images using additional information obtained from MR data .", "label": "", "metadata": {}}
{"text": "Material and Methods .Our algorithm is tested on data from two patients who were referred for a PET / CT scan and volunteered for additional MR scan .Before PET scan , the patients had a resting period after the injection of 18 F - FDG .During that period an MR scan was performed .All clinical procedures were approved by the local ethics committee .The trimodality system consists of GE Healthcare Discovery 690 PET / CT and Discovery 750 w MR scanner .The MR is located in a room next to PET / CT scanner .", "label": "", "metadata": {}}
{"text": "With this possibility , the patient did not have to alter his position .The MR sequence was a LAVA - Flex , a 3D fast spoiled gradient echo sequence .It is a two - point Dixon method that provides four separate contrasts ( water , fat , in - phase , and out - of - phase ) in one single acquisition .This sequence is routinely used on all whole - body trimodality patients to provide T1-weighted whole - body images for anatomical localization .The Integrated Registration tool which is available at the GE Advantage Workstation was used to verify the proper alignment of the different datasets .", "label": "", "metadata": {}}
{"text": "Metal Artifact Detection .The detection of artifacts in the image is achieved using Otsu 's thresholding method [ 24 ] .This method belongs to the class of clustering - based thresholding methods , which means that the gray level data undergoes a clustering analysis , with the number of clusters always set to two .The approach is based on the concept of finding the threshold that minimizes the weighted within - class variance and operates directly on the gray level histogram .The method produces good results when the number of pixels in each class is balanced .", "label": "", "metadata": {}}
{"text": "( i )According to the threshold , the pixels are separated into two clusters .( ii )The mean of each cluster is found .( iii )The difference between the means is squared .( iv )The number of pixels in one cluster is multiplied by the number in the other .The optimal threshold is the one that minimizes the within - class variance , or , conversely , maximizes the between - class variance .As the number of classes increases , selected thresholds become less valid .Experimenting with images with artifacts , we found that 50 classes are an optimal choice for metal region identification .", "label": "", "metadata": {}}
{"text": "As it can be seen in Figure 2 , the original CT image contains severe dental artifacts with streaking lines ( Figure 2(a ) ) .After Otsu 's thresholding , the obtained mask shows that the streaking lines are detected ( Figure 2(b ) ) .Figure 2 : Original CT image with dental streaking artifacts ( a ) and detected artifacts with Otsu 's thresholding method ( b ) .Correction Algorithm .Once the artifact regions are identified , we use the correction algorithm to modify pixel values and thus reduce the effects of present metal objects .", "label": "", "metadata": {}}
{"text": "Corrupted pixels are indicated by white pixels in the threshold mask .CT images have 512 \u00d7 512 gray scale pixel resolution .The proposed algorithm is as follows .Step 1 .A two dimensional 5 \u00d7 5 window is slid over the CT image .Step 2 .If the central pixel is a corrupted pixel , then its value is changed according to the next step .Otherwise , it is considered uncorrupted and the window is slid to the next position .Step 3 .The algorithm finds the same pixel position on the corresponding MR image and then looks for the position of the pixel most similar to the central pixel in the 5 \u00d7 5 window .", "label": "", "metadata": {}}
{"text": "We examine the CT image pixel with the same position as the newly found MR pixel position .If that pixel is uncorrupted in CT , then its value is assigned to the central corrupted pixel in CT .In case that pixel is also corrupted in CT , we look at the next most similar pixel in MR image and repeat this step until we find an uncorrupted pixel .Figure 3 shows the artifact mask over the CT image with artifacts ( Figure 3(a ) ) and the corresponding MR image ( Figure 3(b ) ) , respectively .", "label": "", "metadata": {}}
{"text": "Figure 3 : Detected artifacts on CT image ( a ) and corresponding MR image ( b ) .Since the algorithm can not be applied to border pixels , the first two and the last two columns are replicated at the front and rear end of the CT image , respectively .Similarly , the first two and the last two rows are replicated at the top and bottom of the CT image , respectively .Validation of Results .In order to test the possibilities of the proposed method , we performed a simulation study in which we compared our results to results obtained through linear interpolation .", "label": "", "metadata": {}}
{"text": "The simulation study was carried out using a cone - beam projection / reprojection tool implemented in Matlab .CT data without dental implants was used to achieve this .Different numbers and sizes of implants were introduced and the images in dataset were forward projected to simulate the effects of dental implants .Resulting images were then back - projected and corrected using linear interpolation method and the proposed method .Results and Discussion .The presented approach is an image - based technique for metal - artifact reduction , which differs from other algorithms mainly in the approach to the correction of metal artifacts .", "label": "", "metadata": {}}
{"text": "However , MR images may suffer from signal voids which are caused by the implants .The MR artifacts are present around the implant and they can vary depending on both size and composition .This lack of information represents the limitation of the current version of the method and it could be overcome in the future using alternative MR sequences which are capable of imaging near metal .Carl et al .[ 25 ] investigated the potential of combining ultrashort echo time with multiple acquisition with variable resonance image combination to image tissues adjacent to metallic implants and they found out that it is possible to significantly reduce typical artifacts near metal .", "label": "", "metadata": {}}
{"text": "The resulting images do not have secondary artifacts , which are very often an issue with other image based artifact reduction methods .It can be observed that our method is particularly effective in reducing streaking lines in the area outside the skull and , therefore , also suitable for the creation of PET attenuation maps .Figure 4 : Axial view of CT dataset before ( a ) and after correction ( b ) .CT images are commonly used for attenuation maps for correcting PET data and therefore it is important to provide CT images with artifacts reduced as much as possible .", "label": "", "metadata": {}}
{"text": "However , there are some studies which try to utilize MR - based attenuation correction [ 26 , 27 ] .The most difficult task in creating attenuation maps from MR images is the discrimination of bone tissue .This is an objective of current research .The results from the simulation study are shown in Figure 5 .Images in the first column contain simulated artifacts , the second column contains images reconstructed with the linear interpolation method , and the third column shows images which are the result of the proposed method .One should note that the performance of the linear interpolation method decreases as the number of implants increases .", "label": "", "metadata": {}}
{"text": "A study conducted on 152 patients [ 23 ] showed that most of patients ( 80 percent ) had dental fillings .That is the reason why we primarily focused on dental implants .Nevertheless our approach could also be applied in cases other than dental fillings , such as subclavian ports or hip implants .In certain cases , we had incorrect registration between CT and MR images , which had , as a consequence , undesired occlusion of the airways .Image misregistration can occur due to hardware and patient - induced errors [ 28 ] .Hardware errors are related to mechanical tolerances of the shuttle system .", "label": "", "metadata": {}}
{"text": "On the other hand , patient - induced errors have nonrigid nature and they depend on patient 's condition , his comfort , and anxiety .It can be noticed that these factors have more significant impact as the time increases between the individual scans .A possible solution for this is to use on both scanners accurate and reproducible laser landmarking of the patient .This method can almost eliminate registration errors along the axial direction .Accurate table height adjustment can minimize the misregistration along lateral direction .Samarin et al .[29 ] reported recently that the mean offset between MR and PET / CT was below 1 centimeter .", "label": "", "metadata": {}}
{"text": "The image registration is done by registering the MR data towards CT , assuming accurate geometric calibration of the PET / CT system .However , this method has some weaknesses .The achieved improvement is limited by bone tissue extraction .It is a common problem to isolate the bone tissue from thresholding .Since bone structures have high density , they appear bright on CT images and it is hard to separate them from metal implants which often occur in their vicinity .MR images could help to overcome this problem .One possible solution is to use fast MR sequences which are capable of detecting bone tissues and in that case MR images could be used to isolate high - density structures other than metal .", "label": "", "metadata": {}}
{"text": "We could also examine how the position , size , and number of metal objects influence the results .Conclusion .In this paper , we proposed a new method for metal artifact reduction .A combination of Otsu 's thresholding and paired MR images is used to achieve the correction of CT images .The method was shown to be effective in eliminating streaking artifacts that originate from metal .Our approach is fast and simple and it is fully automated .There is no need for manual interaction , nor for defining the region of interest .Another benefit of the proposed method is that there is no need to work on complex raw CT data .", "label": "", "metadata": {}}
{"text": "However , the number of such institutions should increase significantly in the near future .Further research will be focused on soft tissue reconstruction and on a larger population of patients .Also , working in the sinogram domain instead of image domain should improve our results and help to introduce this method in everyday clinical practice .Acknowledgment .This work has been supported in part by the project COST Action TD1007-\"Bimodal PET - MRI molecular imaging technologies and applications for in vivo monitoring of disease and biological processes \" .G. Eggers , M. Rieker , B. Kress , J. Fiebach , H. Dickhaus , and S. Hassfeld , \" Artefacts in magnetic resonance imaging caused by dental material , \" Magnetic Resonance Materials in Physics , Biology and Medicine , vol .", "label": "", "metadata": {}}
{"text": "103 - 111 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Martinez - Moller , M. Souvatzoglou , G. Delso et al . , \" Tissue classification as a potential approach for attenuation correction in whole - body PET / MRI : evaluation with PET / CT data , \" Journal of Nuclear Medicine , vol .50 , no .4 , pp .520 - 526 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. H. Yan , R. T. Whalen , G. S. Beaupr\u00e9 , S. Y. Yen , and S. Napel , \" Reconstruction algorithm for polychromatic CT imaging : application to beam hardening correction , \" IEEE Transactions on Medical Imaging , vol .", "label": "", "metadata": {}}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. M. Hanson , \" Noise and contrast discrimination in computed tomography , \" Radiology of the Skull and Brain : Technical Aspects of Computed Tomography , vol .5 , pp .3941 - 3955 , 1981 .View at Google Scholar .M. Abdoli , R. A. J. O. Dierckx , and H. Zaidi , \" Metal artifact reduction strategies for improved attenuation correction in hybrid PET / CT imaging , \" Medical Physics , vol .39 , no .6 , pp .", "label": "", "metadata": {}}
{"text": "View at Google Scholar .R. M. Lewitt and R. H. T. Bates , \" Image reconstruction from projections-3 .Projection completion methods ( theory ) , \" Optik , vol .50 , no . 3 , pp .189 - 204 , 1978 .View at Google Scholar \u00b7 View at Scopus . H. T. Hinderling , P. Ruegsegger , M. Anliker , and C. Dietschi , \" Computed tomography reconstruction from hollow projections : an application to in vivo evaluation of artificial hip joints , \" Journal of Computer Assisted Tomography , vol .3 , no . 1 , pp .", "label": "", "metadata": {}}
{"text": "View at Google Scholar \u00b7 View at Scopus .R. L. Morin and D. E. Raeside , \" A pattern recognition method for the removal of streaking artifact in computed tomography , \" Radiology , vol .141 , no . 1 , pp .229 - 233 , 1981 .View at Google Scholar \u00b7 View at Scopus .L. Cheng and J. Liu , \" Metal artifacts reduction in Computed Tomography : a bilateral reprojection approach , \" in Proceedings of the 4th International Conference on Bioinformatics and Biomedical Engineering ( iCBBE ' 10 ) , June 2010 .", "label": "", "metadata": {}}
{"text": "J. A. Kennedy , O. Israel , A. Frenkel , R. Bar - Shalom , and H. Azhari , \" The reduction of artifacts due to metal hip implants in CT - attenuation corrected PET images from hybrid PET / CT scanners , \" Medical and Biological Engineering and Computing , vol .45 , no .6 , pp .553 - 562 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. Sohmura , H. Hojoh , N. Kusumoto , M. Nishida , K. Wakabayashi , and J. Takahashi , \" A novel method of removing artifacts because of metallic dental restorations in 3-D CT images of jaw bone , \" Clinical Oral Implants Research , vol .", "label": "", "metadata": {}}
{"text": "6 , pp .728 - 735 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . K. Lange and R. Carson , \" EM reconstruction algorithms for emission and transmission tomography , \" Journal of Computer Assisted Tomography , vol . 8 , no . 2 , pp .306 - 316 , 1984 .View at Google Scholar \u00b7 View at Scopus .G. Wang , M. W. Vannier , and P.-C. Cheng , \" Iterative X - ray cone - beam tomography for metal artifact reduction and local region reconstruction , \" Microscopy and Microanalysis , vol .", "label": "", "metadata": {}}
{"text": "58 - 65 , 1999 .View at Google Scholar \u00b7 View at Scopus .G. Delso , S. Wollenweber , A. Lonn , F. Wiesinger , and P. Veit - Haibach , \" MR - driven metal artifact reduction in PET / CT , \" Physics in Medicine and Biology , vol .58 , no . 7 , pp .2267 - 2280 , 2013 .View at Google Scholar .N. Otsu , \" A threshold selection method from gray - level histograms , \" Automatica , vol .11 , pp .23 - 27 , 1975 .", "label": "", "metadata": {}}
{"text": "M. Hofmann , B. Pichler , B. Sch\u00f6lkopf , and T. Beyer , \" Towards quantitative PET / MRI : a review of MR - based attenuation correction techniques , \" European Journal of Nuclear Medicine and Molecular Imaging , vol .36 , no . 1 , pp .93 - 104 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. Catana , A. Van Der Kouwe , T. Benner et al . , \" Toward implementing an MRI - based PET attenuation - correction method for neurologic studies on the MR - PET brain prototype , \" Journal of Nuclear Medicine , vol .", "label": "", "metadata": {}}
{"text": "1431 - 1438 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Veit - Haibach , F. P. Kuhn , F. Wiesinger , G. Delso , and G. von Schulthess , \" PET - MR imaging using a tri - modality PET / CT - MR system with a dedicated shuttle in clinical routine , \" Magma , vol .26 , no . 1 , pp .25 - 35 , 2013 .View at Google Scholar . A. Samarin , F. Kuhn , D. Crook et al . , \" Image registration accuracy of a sequential , tri - modality pet / ct plus mr imaging setup using dedicated patient transporter systems , \" European Journal of Nuclear Medicine and Molecular Imaging , vol .", "label": "", "metadata": {}}
{"text": "View at Google Scholar Systematic mapping of protein - protein interactions , or ' interactome ' mapping , was initiated in model organisms , starting with defined biological processes and then expanding to the scale of the proteome .Although far from complete , such maps have revealed global topological and dynamic features of interactome networks that relate to known biological properties , suggesting that a human interactome map will provide insight into development and disease mechanisms at a systems level .Here we describe an initial version of a proteome - scale map of human binary protein - protein interactions .", "label": "", "metadata": {}}
{"text": "This data set , called CCSB - HI1 , has a verification rate of approximately 78 % as revealed by an independent co - affinity purification assay , and correlates significantly with other biological attributes .The CCSB - HI1 data set increases by approximately 70 % the set of available binary interactions within the tested space and reveals more than 300 new connections to over 100 disease - associated proteins .This work represents an important step towards a systematic and comprehensive human interactome project . \"High - and low - throughput experimental assays are available to screen physical interactions among proteins within and between species ( Rual et al . , 2005 ; Konig et al . , 2008 ) .", "label": "", "metadata": {}}
{"text": "Here , systems biology strategies are exploited to identify novel molecular mechanisms and protein indicators of brain injury .To this end , we performed a meta - analysis of four distinct high - throughput gene expression studies involving different animal models of TBI .By using canonical pathways and a large human protein - interaction network as a scaffold , we separately overlaid the gene expression data from each study to identify molecular signatures that were conserved across the different studies .At 24 hr after injury , the significantly activated molecular signatures were nonspecific to TBI , whereas the significantly suppressed molecular signatures were specific to the nervous system .", "label": "", "metadata": {}}
{"text": "We selected three proteins from this subnetwork , postsynaptic density protein 95 , nitric oxide synthase 1 , and disrupted in schizophrenia 1 , and hypothesized that their abundance would be significantly reduced after TBI .In a penetrating ballistic - like brain injury rat model of severe TBI , Western blot analysis confirmed our hypothesis .In addition , our analysis recovered 12 previously identified protein biomarkers of TBI .The results suggest that systems biology may provide an efficient , high - yield approach to generate testable hypotheses that can be experimentally validated to identify novel mechanisms of action and molecular indicators of TBI .", "label": "", "metadata": {}}
{"text": "Despite the significant advances being made the last decade , the human interactome is still largely uncharted and the accumulated knowledge is biased towards well - studied proteins [ 1 , 6].\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Phage display is a powerful technique for profiling specificities of peptide binding domains .The method is suited for the identification of high - affinity ligands with inhibitor potential when using highly diverse combinatorial peptide phage libraries .Such experiments further provide consensus motifs for genome - wide scanning of ligands of potential biological relevance .", "label": "", "metadata": {}}
{"text": "One of the main applications of such proteomic libraries has been the elucidation of antibody epitopes .This review is focused on the use of proteomic phage display to uncover protein - protein interactions of potential relevance for cellular function .The method is particularly suited for the discovery of interactions between peptide binding domains and their targets .We discuss the largely unexplored potential of this method in the discovery of domain - motif interactions of potential biological relevance . \"A thorough analysis of virus - host interactomes may reveal insights into viral infection and pathogenic strategies and help identify novel drug targets [ 8 ] and decipher the molecular aetiology of some complex diseases [ 9].", "label": "", "metadata": {}}
{"text": "A thorough analysis of virus - host interactomes may reveal insights into viral infection and pathogenic strategies .In this study , we presented a landscape of virus - host interactomes based on protein domain interaction .Compared to the analysis at protein level , this domain - domain interactome provided a unique abstraction of protein - protein interactome .Through comparisons among DNA , RNA , and retrotranscribing viruses , we identified a core of human domains , that viruses used to hijack the cellular machinery and evade the immune system , which might be promising antiviral drug targets .", "label": "", "metadata": {}}
{"text": "Further analysis at functional level highlighted that different viruses perturbed the host cellular molecular network by common and unique strategies .Most importantly , we creatively proposed a viral disease network among viral domains , human domains and the corresponding diseases , which uncovered several unknown virus - disease relationships that needed further verification .Overall , it is expected that the findings will help to deeply understand the viral infection and contribute to the development of antiviral therapy .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .The impact factor represents a rough estimation of the journal 's impact factor and does not reflect the actual current impact factor .", "label": "", "metadata": {}}
{"text": "Differing provisions from the publisher 's actual policy or licence agreement may be applicable .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper deals with a novel concept of an exponential IDF in the BM25 formulation and compares the search accuracy with that of the BM25 with the original IDF in a content - based video retrieval ( CBVR ) task .Our video retrieval method is based on a bag of keypoints ( local visual features ) and the exponential IDF estimates the keypoint importance weights more accurately than the original IDF .The exponential IDF is capable of suppressing the keypoints from frequently occurring background objects in videos , and we found that this effect is essential for achieving improved search accuracy in CBVR .", "label": "", "metadata": {}}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper discusses a new extension of hidden Markov models that can capture clusters embedded in transitions between the hidden states .In our model , the state - transition matrices are viewed as representations of relational data reflecting a network structure between the hidden states .We specifically present a nonparametric Bayesian approach to the proposed state - space model whose network structure is represented by a Mondrian Process - based relational model .We show an application of the proposed model to music signal analysis through some experimental results .", "label": "", "metadata": {}}
{"text": "Although several types of multi - spectral camera systems have been developed , all of them , except for the six - band HDTV camera system developed by Ohsawa et al [ Ohsawa et al .2004 ] , are multi - shot and none can take still images of moving objects and moving pictures .However , Ohsawa et al . 's system requires very complex and expensive customized optics whose optical elements must be arranged precisely , which makes it far from practical .In order to make multi - spectrum video systems pervasive , the equipment costs must be reduced by ensuring they have as much compatibility with existing video camera systems as possible .", "label": "", "metadata": {}}
{"text": "In this paper , we propose a system that applies their concept to existing 4 K digital cinema cameras and show the possibility using the proposed system for cinematography .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper proposes a novel representation of music that can be used for similarity - based music information retrieval , and also presents a method that converts an input polyphonic audio signal to the proposed representation .The representation involves a 2-dimensional tree structure , where each node encodes the musical note and the dimensions correspond to the time and simultaneous multiple notes , respectively .", "label": "", "metadata": {}}
{"text": "In the conventional approaches to music representation from audio , note extraction is usually performed prior to structure analysis , but accurate note extraction has been a difficult task .In the proposed method , note extraction and structure estimation is performed simultaneously and thus the optimal solution is obtained with a unified inference procedure .That is , we propose an extended 2-dimensional infinite probabilistic context - free grammar and a sparse factor model for spectrogram analysis .An efficient inference algorithm , based on Markov chain Monte Carlo sampling and dynamic programming , is presented .", "label": "", "metadata": {}}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT :In this paper , we describe our approaches that were tested in the TRECVID 2011Content - Based Copy Detection ( CBCD ) task .We use our fingerprinting technologies called the Coarsely - quantized Area Matching method ( CAM ) and Divide And Locate method ( DAL ) for video detection and audio detection tasks , respectively .The CAM consists of a feature degeneration and sparse feature selection process .The DAL is based on spectral partitioning and vector quantization .The audio and video are processed independently , and we merged the audio and video results to generate submission runs .", "label": "", "metadata": {}}
{"text": "The features and search parameters for intermediate search results are common to all the runs .The intermediate results are processed with a time consistency filter .We varied the filter parameters as follows .NTT - CSL .m.nofa.0 : strict filter for both audio and video NTT - CSL .m.balanced.1 : strict filter for video and weak for audio NTT - CSL .m.balanced.2 : weak filter for video and strict for audio NTT - CSL .m.balanced.3 : weak filter for both audio and video .[ Show abstract ] [ Hide abstract ] ABSTRACT : We propose a music segment detection method for audio signals .", "label": "", "metadata": {}}
{"text": "This task is important because music is almost always overlapped by speech or other environmental sounds in visual materials such as TV programs .Our method consists of feature extraction , dimension reduction , and statistical discrimination steps .For each step , we analyzed a set of methods to maximize the detection accuracy .With a simple post processing step , we achieved a framewise error rate as low as 8 % even when the mixed speech was louder than the target music by 10dB. [ Show abstract ] [ Hide abstract ] ABSTRACT : This paper proposes a new formulation and optimization procedure for grouping frequency components in frequency - domain blind source separation ( BSS ) .", "label": "", "metadata": {}}
{"text": "With ICA , grouping the frequency components corresponds to aligning the permutation ambiguity of the ICA solution in each frequency bin .With T - F masking , grouping the frequency components corresponds to classifying sensor observations in the time - frequency domain for individual sources .The grouping procedure is based on estimating anechoic propagation model parameters by analyzing ICA results or sensor observations .More specifically , the time delays of arrival and attenuations from a source to all sensors are estimated for each source .The focus of this paper includes the applicability of the proposed procedure for a situation with wide sensor spacing where spatial aliasing may occur .", "label": "", "metadata": {}}
{"text": "Full - text \u00b7 Article \u00b7 Aug 2007 \u00b7 IEEE Transactions on Audio Speech and Language Processing .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper describes the frequency - domain blind source separation ( BSS ) of convolutively mixed acoustic signals using independent component analysis ( ICA ) .The most critical issue related to frequency domain BSS is the permutation problem .This paper presents two methods for solving this problem .Both methods are based on the clustering of information derived from a separation matrix obtained by ICA .The first method is based on direction of arrival ( DOA ) clustering .", "label": "", "metadata": {}}
{"text": "The second method is based on normalized basis vector clustering .This method is less intuitive than the DOA based method , but it has several advantages .First , it does not need sensor array geometry information .Secondly , it can fully utilize the information contained in the separation matrix , since the clustering is performed in high - dimensional space .Experimental results show that our methods realize BSS in various situations such as the separation of many speech signals located in a 3-dimensional space , and the extraction of primary sound sources surrounded by many background interferences .", "label": "", "metadata": {}}
{"text": "The method analyzes the mixing system information estimated with independent component analysis ( ICA ) .When we use widely spaced sensors or increase the sampling rate , spatial aliasing may occur for high frequencies due to the possibility of multiple cycles in the sensor spacing .In such cases , the estimated information would imply multiple possibilities for a source location .This causes some difficulty when analyzing the information .We propose a new method designed to overcome this difficulty .This method first estimates the model parameters for the mixing system at low frequencies where spatial aliasing does not occur , and then refines the estimations by using data at all frequencies .", "label": "", "metadata": {}}
{"text": "Experimental results show the effectiveness of the new method .Our method can cope with 2- or 3-dimensionally distributed sources with a 2- or 3-dimensional sensor array .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper presents a prototype system for blind source separation ( BSS ) of many speech signals and describes the techniques used in the system .Our system uses 8 microphones located at the vertexes of a 4 cm times 4 cm times 4 cm cube and has the ability to separate signals distributed in three - dimensional space .", "label": "", "metadata": {}}
{"text": "We carried out experiments in an ordinary office and obtained more than 20 dB of SIR improvement .[ Show abstract ] [ Hide abstract ] ABSTRACT : This paper presents a method for estimating location information about multiple sources .The proposed method uses independent component analysis ( ICA ) as a main statistical tool .The near - field model as well as the far - field model can be assumed in this method .As an application of the method , we show experimental results for the direction - of - arrival ( DOA ) estimation of three sources that were positioned 3-dimensionally .", "label": "", "metadata": {}}
{"text": "The enhancement is performed blindly , i.e. without knowing the number of total sources or information about each source , such as position and active time .We consider a general case where the number of sources is larger than the number of sensors .We employ a two - stage processing technique where a spatial filter is first employed in each frequency bin and time - frequency masking is then used to improve the performance further .To obtain the spatial filter we employ independent component analysis and then select the component of the target source .Time - frequency masks in the second stage are obtained by calculating the angle between the basis vector corresponding to the target source and a sample vector .", "label": "", "metadata": {}}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Musical noise is a typical problem with blind source separation using a time - frequency mask .We report that a fine - shift and overlap - add method reduces the musical noise without degrading the separation performance .[ Show abstract ] [ Hide abstract ] ABSTRACT : This work presents a method for solving the permutation problem of frequency domain blind source separation ( BSS ) when the number of source signals is large , and the potential source locations are omnidirectional .We propose a combination of small and large spacing sensor pairs with various axis directions in order to obtain proper geometrical information for solving the permutation problem .", "label": "", "metadata": {}}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT :In this paper , we propose a method for separating speech signals when there are more signals than sensors .Several methods have already been proposed for solving the underdetermined problem , and some of these utilize the sparseness of speech signals .These methods employ binary masks to extract the signals , and therefore , their extracted signals contain loud musical noise .To overcome this problem , we propose combining a sparseness approach and independent component analysis ( ICA ) .First , using sparseness , we estimate the time points when only one source is active .", "label": "", "metadata": {}}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Blind source separation ( BSS ) for convolutive mixtures can be efficiently achieved in the frequency domain , where independent component analysis is performed separately in each frequency bin .However , frequency - domain BSS involves a permutation problem , which is well known as a difficult problem , especially when the number of sources is large .This paper presents a method for solving the permutation problem , which works well even for many sources .The successful solution for the permutation problem highlights another problem with frequency - domain BSS that arises from the circularity of the discrete frequency representation .", "label": "", "metadata": {}}
{"text": "With these two methods , we can separate many sources with a practical execution time .Moreover , real - time processing is currently possible for up to three sources with our implementation .[ Show abstract ] [ Hide abstract ] ABSTRACT : The paper presents a method for solving the permutation problem of frequency domain blind source separation ( BSS ) when source signals come from the same or similar directions .Geometric information such as the direction of arrival ( DOA ) is helpful for solving the permutation problem , and a combination of the DOA based and correlation based methods provides a robust and precise solution .", "label": "", "metadata": {}}
{"text": "We show that an interpretation of the ICA solution by a near - field model yields information about spheres on which source signals exist , which can be used as an alternative to the DOA .Experimental results show that the proposed method can robustly separate a mixture of signals arriving from the same direction .The Rational Third - Kind Chebyshev Pseudospectral Method for the Solution of the Thomas - Fermi Equation over Infinite Interval .Received 12 January 2013 ; Revised 14 May 2013 ; Accepted 29 May 2013 .Academic Editor : Mufid Abudiab .Copyright \u00a9 2013 Majid Tavassoli Kajani et al .", "label": "", "metadata": {}}
{"text": "Abstract .We propose a pseudospectral method for solving the Thomas - Fermi equation which is a nonlinear ordinary differential equation on semi - infinite interval .This approach is based on the rational third - kind Chebyshev pseudospectral method that is indeed a combination of Tau and collocation methods .This method reduces the solution of this problem to the solution of a system of algebraic equations .Comparison with some numerical solutions shows that the present solution is highly accurate .Introduction .Many science and engineering problems of current interest are set in unbounded domains .We can apply different spectral methods that are used to solve problems in semi - infinite domains .", "label": "", "metadata": {}}
{"text": "The second approach is replacing semi - infinite domain with . , sufficiently large .This method is named domain truncation [ 5 ] .The third approach is reformulating original problem in semi - infinite domain to singular problem in bounded domain by variable transformation and then using the Jacobi polynomials to approximate the resulting singular problem [ 6 ] .The fourth approach of spectral method is based on rational orthogonal functions .Boyd [ 7 ] defined a new spectral basis , named the rational Chebyshev functions on the semi - infinite interval , by mapping to the Chebyshev polynomials .", "label": "", "metadata": {}}
{"text": "[ 8 ] introduced a new set of the rational Legendre functions which are mutually orthogonal in .They applied a spectral scheme using the rational Legendre functions for solving the Korteweg - de Vries equation on the half line .Boyd et al .[ 9 ] applied pseudospectral methods on a semi - infinite interval and compared the rational Chebyshev , Laguerre , and mapped Fourier - sine methods .The authors of [ 10 - 12 ] applied spectral method to solve nonlinear ordinary differential equations on semi - infinite intervals .Their approach was based on a rational Tau method .", "label": "", "metadata": {}}
{"text": "Furthermore , the authors of [ 13 , 14 ] introduced the rational second- and third - kind Chebyshev - Tau method for solving the Lane - Emden equation and Volterra 's population model as nonlinear differential equations over infinite interval .One of the most important nonlinear singular ordinary differential equations that occurs in semi - infinite interval is the Thomas - Fermi equation , which is given as follows [ 15 , 16 ] : .The Thomas - Fermi equation is useful for calculating the form factors and for obtaining effective potentials which can be used as initial trial potentials in self - consistent field calculations .", "label": "", "metadata": {}}
{"text": "[ 17 - 19 ] used perturbative approach to determine analytic solutions for The studies in Thomas - Fermi equation .Bender et al .[17 ] replaced the right - hand side of a this equation by one which contains the parameter .This procedure reduced ( 1 ) into a set of linear equations with associated boundary conditions .Laurenzi [ 19 ] applied perturbative method by combining it with an alternate choice of the nonlinear term of ( 1 ) to produce a rapidly converging analytic solution .Cedillo [ 18 ] wrote ( 1 ) in terms of density , and then the .", "label": "", "metadata": {}}
{"text": "Adomian [ 20 ] applied the decomposition method for solving the Thomas - Fermi equation , and then Wazwaz [ 21 ] proposed a nonperturbative approximate solution to this equation by using the modified decomposition method in a direct manner without any need for a perturbative expansion or restrictive assumptions .He combined the series obtained with the Pad\u00e9 approximation which provided a promising tool to handle problems on an unbounded domain .Liao [ 22 ] solved the Thomas - Fermi equation by the homotopy analysis method .This method provided a convenient way to control the convergence of approximation series and adjusted convergence regions when necessary , which was a fundamental qualitative difference in analysis between the homotopy analysis method and all other reported analytic techniques .", "label": "", "metadata": {}}
{"text": "In [ 24 ] , the quasilinearization approach was applied for solving ( 1 ) .This method approximated the solution of a nonlinear differential equation by treating the nonlinear terms as a perturbation about the linear ones , and , unlike perturbation theories it is not based on the existence of some kind of a small parameter .Ramos [ 25 ] presented two piecewise quasilinearization methods for the numerical solution of ( 1 ) .Both methods were based on the piecewise linearization of ordinary differential equations .The first method ( C1-linearization ) provided global smooth solutions , whereas the second one ( C0-linearization ) provided continuous solutions .", "label": "", "metadata": {}}
{"text": "In this paper , we introduce the rational third - kind Chebyshev ( RTC ) functions , and , for the first time , we derive the operational matrix of the derivatives of RTC functions .We then introduce a combination of Tau and pseudospectral methods based on RTC functions to illustrate its efficiency in solving differential equations on a semi - infinite interval .The proposed method requires the definition of RTC functions , the operational matrix of the derivative , and the rational third - kind Chebyshev - Gauss collocation points and weights .The application of the method to the Thomas - Fermi equation leads to a nonlinear algebraic system .", "label": "", "metadata": {}}
{"text": "This paper is arranged as follows .In Section 2 , we describe the formulation and some properties of the rational third - kind Chebyshev functions required for our subsequent development .Section 3 summarizes the application of this method for solving the Thomas - Fermi equation , and a comparison is made with existing methods in the literature .The results show preference of this method in comparison with the others .The conclusions are described in the final section .according to the physical properties of the problem , and then experiment .The criterion for optimum is rate of convergence .", "label": "", "metadata": {}}
{"text": "For this particular reason , very accurate approximations of .calculated by the present paper with values obtained by Liao [ 22 ] , Khan and Xu [ 23 ] , and Yao [ 27 ] is given in Table 2 , which shows that the present solution is highly accurate .Table 3 shows the approximations of .Conclusion .and whose boundary condition occurs in infinity .In the above discussion , the pseudospectral method with RTC functions , which have the property of orthogonality , is employed to achieve this goal .Advantages of this method are that we do not reform the problem to a finite domain and that with a small . very accurate results are obtained .", "label": "", "metadata": {}}
{"text": "Comparing the computed results by this method with the others shows that this method provides more accurate and numerically stable solutions than those obtained by other methods .Acknowledgments .The authors express their sincere thanks to the referees for their comments on the earlier version of this paper and their helpful suggestions .The second author also gratefully acknowledges that this paper was partially supported by the Universiti Putra Malaysia under the ERGS Grant Scheme having Project no .5527068 .J. P. Boyd , C. Rangan , and P. H. Bucksbaum , \" Pseudospectral methods on a semi - infinite interval with application to the hydrogen atom : a comparison of the mapped Fourier - sine method with Laguerre series and rational Chebyshev expansions , \" Journal of Computational Physics , vol .", "label": "", "metadata": {}}
{"text": "56 - 74 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Tavassoli Kajani and F. Ghasemi Tabatabaei , \" Rational Chebyshev approximations for solving Lane - Emde equation of index m , \" in Proceeding of the International Conference on Computationaland Applied Mathematics , pp .840 - 844 , Bangkok , Thailand , March 2011 . H. T. Davis , Introduction to Nonlinear Differential and Integral Equations , Dover , New York , NY , USA , 1962 .View at MathSciNet .S. Chandrasekhar , An Introduction to the Study of Stellar Structure , Dover , New York , NY , USA , 1957 .", "label": "", "metadata": {}}
{"text": "S. Liao , Beyond Perturbation , Introduction to the Homotopy Analysis Method , vol .2 of CRC Series : Modern Mechanics and Mathematics , Chapman & Hall / CRC , Boca Raton , Fla , USA , 2004 .View at MathSciNet .S. Kobayashi , T. Matsukuma , S. Nagi , and K. Umeda , \" Accurate value of the initial slope of theordinary TF function , \" Journal of the Physical Society of Japan , vol .10 , pp .759 - 762 , 1955 .View at Google Scholar Links relacionados .Compartilhar .Pesqui .", "label": "", "metadata": {}}
{"text": "vol.31 no.3 Rio de Janeiro set ./dez .Heuristics for implementation of a hybrid preconditioner for interior - point methods .Postal 6065 , 13083 - 859 Campinas , SP , Brazil .Ant\u00f4nio Carlos , 6627 , Pampulha , 31270 - 010 Belo Horizonte , MG , Brazil .E - mail : ffcampos@dcc.ufmg.br .This article presents improvements to the hybrid preconditioner previously developed for the solution through the conjugate gradient method of the linear systems which arise from interior - point methods .The hybrid preconditioner consists of combining two preconditioners : controlled Cholesky factorization and the splitting preconditioner used in different phases of the optimization process .", "label": "", "metadata": {}}
{"text": "This approach works better than direct methods for some classes of large - scale problems .This work has proposed new heuristics for the integration of both preconditioners , identifying a new change of phases with computational results superior to the ones previously published .Moreover , the performance of the splitting preconditioner has been improved through new orderings of the constraint matrix columns allowing savings in the preconditioned conjugate gradient method iterations number .Experiments are performed with a set of large - scale problems and both approaches are compared with respect to the number of iterations and running time .", "label": "", "metadata": {}}
{"text": "Since the emergence of the interior - point method , sophisticated codes have been implemented in order to decrease the computational effort and improve its efficiency ( Adler et al . , 1989 ; Lustig et al . , 1990 ; Mehrotra , 1992 ; Czyzyk et al . , 1999 ) .The most expensive step to each iteration consists of the resolution of one or two linear systems .The most used approach for the solution of these systems is the Cholesky factorization ( Golub & Van Loan , 1996 ) , a procedure that may be expensive in large - scale problems .", "label": "", "metadata": {}}
{"text": "The conjugate gradients method has shown to be the most efficient for the solution of the linear equations systems with large positive definite matrix .To obtain the convergence of the iterative methods , it is fundamental to construct a preconditioner for the matrix of the linear system .These preconditioners should be easily built with relatively low computational cost and simultaneously , it should provide the convergence of the iterative method in a small number of iterations .In this work , two specific preconditioners will be considered : controlled Cholesky factorization ( Campos & Birkett , 1998 ) and the splitting preconditioner ( Oliveira & Sorensen , 2005 ) , presented in Section 3 .", "label": "", "metadata": {}}
{"text": "( 2007 ) it has been proved that these two pre - conditioners determine a different behavior for the conjugate gradients method in the solution of the linear systems which arise from interior - point methods .At the initial iterations of the interior - point method , the method of the conjugate gradients solves the linear systems involved more efficiently using the preconditioner obtained by the controlled Cholesky factorization ( Campos & Birkett , 1998 ) .This opposite behavior was availed by Bocanegra et al .( 2007 ) developing a hybrid approachwhere both preconditioners are used for the solution of the linear systems in the same problem ofoptimization by interior - point methods .", "label": "", "metadata": {}}
{"text": "This work presents new heuristics to identify the moment in which the preconditioners change .Moreover , improvements to the efficiency of the splitting preconditioner are presented , consequently decreasing the number of iterations of the conjugate gradients method .The results are presented using large - scale problems and comparing the results with the ones obtained by Bocanegra et al .( 2007 ) .2 PRIMAL - DUAL INTERIOR POINTS METHODS .A linear optimization problem may be presented in the standard form , as following : .Associated with the primal problem ( 1 ) , the dual problem is represented by : .", "label": "", "metadata": {}}
{"text": "The method is obtained from the application of the Newton 's method to the non - linear system F ( x , y , z ) ( Equation 3 ) formed by the conditions of optimality , but not considering the non - negativity restrictions : .The Predictor - Corrector method ( Monteiro et al . , 1990 ; Mehrotra , 1992 ) is considered the most efficient approach for the solution of generic problems of linear programming .This method uses three components to calculate the direction .A predictor direction ( \u0394 , \u0394 , \u0394 ) or the affine - scaling directions calculated from the system ( 4 ) .", "label": "", "metadata": {}}
{"text": "For the solution of this system , direct methods may be used through Cholesky factorization or iterative methods .The most used iterative method is the preconditioned conjugategradient .3 PRECONDITIONERS .The preconditioning of a matrix is used to facilitate the convergence of iterative methods in the solution of linear systems .Most of the known preconditioners are obtained from an incomplete Cholesky factorization without promising results due to the ill condition of the matrix in the second phase of the process of optimization .Mehrotra ( 1992b ) and ( Lustig et al . , 1990 ) define a preconditioner for the solution of linear systems by the conjugate gradient through the incomplete Cholesky factorization .", "label": "", "metadata": {}}
{"text": "The tests were carried out in the problems of Netlib .Wang & O'Leary ( 2000 ) developed a preconditioner for the solution of the linear system by an \" adaptive \" method .The method identifies when we should use the direct method and when weshould use the iterative method by the preconditioned conjugate gradient .The initial preconditioner would be Cholesky factorization of the matrix generated in one of the iterations of the interior - point method .When the gap varies in the course of the iterations of the method , the preconditioner is calculated through up to n updates of 1-rank according to the variation of the matrix D .", "label": "", "metadata": {}}
{"text": "( 2004 ) describe a preconditioner for the solution by iterative methods of the augmented system in the solution of problems of linear , non - linear and quadratic optimization .When we use the complete factorization of the matrix as a preconditioner , the computational cost when the problem is linear is the same as when the direct method is used .3.1 Hybrid preconditioner .The hybrid preconditioner ( Bocanegra et al . , 2007 ) is formed by the junction of two preconditioners : controlled Cholesky factorization and the splitting .The preconditioners are used in different phases of the process of optimization : controlled Cholesky factorization in the first phase and the splitting in the final phase where the system is very ill - conditioned .", "label": "", "metadata": {}}
{"text": "The Controlled Cholesky Factorization ( CCF ) proposed by Campos and Birkett ( 1998 ) is a variation of the incomplete Cholesky factorization proposed by Jones and Plassmann ( 1995 ) .The matrix is used as preconditioner matrix .It is constructed from the selection of a fixed number of elements per columns with the largest absolute values .Consider the following problem : .The calculation of the norm is divided into two summations .In the first , we get the difference among the elements of the matrix L and the elements chosen which will form the matrix .", "label": "", "metadata": {}}
{"text": "The quantity selected is equal to the number of nonzero elements below the diagonal ( m j ) plus the extra number of nonzero elements allowed per columns ( \u03b7 ) .In the second summation the difference is not realized , because in these positions the matrix will have elements equal to zeros .Note that the higher the filling of the matrix the lower will be its difference with L obtained by the complete Cholesky factorization .When \u03b7 has a positive value we will have a column j with filling higher than the column j of the matrix Q .", "label": "", "metadata": {}}
{"text": "From tests carried out ( Bocanegra et al . , 2007 ) , it has been proved that this factorization presents good results in the first iterations of the interior - point method , however it may deteriorates itself in the last ones , as the matrix Q gets very ill - conditioned . 3.3 Splitting preconditioner .The splitting preconditioner was proposed in ( Oliveira & Sorensen , 2005 ) for the solution of the linear systems that arise from interior - point methods by iterative approaches , specifically by the conjugate gradient method .The splitting preconditioner was originally developed for the augmented matrix of form : .", "label": "", "metadata": {}}
{"text": "In ( Oliveira & Sorensen , 2005 ) , the use of a diagonal matrix in the first iterations is proposed and the splitting preconditioner in the final phase of the optimization .This approach does not converge to many problems , because the diagonal preconditioner fails in many of them .The highest computational cost in the calculation of this preconditioner consists of the construction of the block B of the matrix A , where m columns linearly independent are chosen .Theway in which these columns are chosen is fundamental for the good performance of the pre - conditioner .", "label": "", "metadata": {}}
{"text": "The splitting preconditioner builds a partition B of linearly independent columns and the way these columns are chosen influences its performance .The authors ( Oliveira & Sorensen , 2005 ) initially used the matrix D .This new approach improved the performance ofthe preconditioner .This choice hasshown better results in the performance of the splitting preconditioner .The number of iterationsof the conjugate gradient method in the solution of the linear system was reduced .3.5 Change of preconditioner .In the hybrid approach ( Bocanegra et al . , 2007 ) , several conditions were verified for the change of preconditioners as well as for the increase of the number of nonzero elements in the controlled Cholesky factorization .", "label": "", "metadata": {}}
{"text": "However , each problem has a different behavior and consequently the phases are not easily identified .Bocanegra et al .( 2007 ) realizes the change in the following conditions : .It indicates that the process of optimization is very advanced and it may be the moment to change the preconditioner .The number of iterations of the conjugate gradient in the solution of the linear system is near m /2( m is the number of rows of the matrix A ) .This may indicate that the controlled Cholesky factorization is not having a good performance .", "label": "", "metadata": {}}
{"text": "In the construction of CCF , there may be non - positive pivots and to avoid this problem , a value is added to the diagonal ( correction in the diagonal ) and the matrix is recalculated .If none of these three conditions is satisfied , then the \u03b7 is increased by 10 and the method continues with the controlled Cholesky factorization .In the study of this approach , it has been observed that these conditions are not always satisfactory .In all the tested problems , when a change of phase is realized very far from solution , the method does not reach the convergence .", "label": "", "metadata": {}}
{"text": "This new heuristic implies that the only condition that should be verified is the value of parameter \u03b7 .The new change of phase is realized when : .If the iterations of the conjugate gradient are greater than m /6 then it is verified if \u03b7 reached a maximum value .If so , the change of the phases is realized , otherwise , \u03b7 is increased by 10 the process continues with the preconditioner obtained by the controlled Cholesky factorization .Note that \u03b7 is only used in the first phase .For reasonable values of \u03b7 the preconditioner obtained by the controlled Cholesky factorization is cheaper in its construction and requires less memory than the splitting preconditioner and therefore the highest number of iterations possible should be maintained .", "label": "", "metadata": {}}
{"text": "In the numerical tests presented in the following section these results will be verified .4 NUMERICAL EXPERIMENTS .The hybrid preconditioner was integrated to the PCx code ( Czyzyk et al . , 1999 ) .This code implements a variant of the predictor - corrector algorithm with multiple corrections .The procedures to solve the linear systems are written in the language C , with the exception of the code of the controlled Cholesky factorization which was implemented in FORTRAN .The same parameters of PCx were used with the exception of the multiple corrections that are not allowed .", "label": "", "metadata": {}}
{"text": "All the tested problems are of public domain .The QAP problems are from the QAPLIB library ( Burkard et al . , 1991 ) with the modifications described by Padberg and Rijal ( 1996 ) .Some of the NUG problems were modified and the letter M , at the end of the name , was added as indicative .The Table 1 summarizes the problems tested showing the number of rows and columns after the preprocessing .The performance of the hybrid approach proposed by Bocanegra et al .( 2007 ) was compared with the performance of the new approach with the new condition of change of phases and the new ordering by the 2-norm .", "label": "", "metadata": {}}
{"text": "In the column \" Iterations \" , the number of iterations of both approaches to reach the optimal is shown .In the column TIME , we have the computational times measured in an Intel Pentium IV 3.4GHz machine with 2Gb of memory using the Linux operating system .The initial value of \u03b7 in most of the problems was obtained from the average of the nonzero elements of the matrix A ( Mel ) .In problems of higher scale as in the NUG15 the initial \u03b7 is obtained adding 100 to the value of Mel .In most of the tested problems it was possible to decrease the computational time and for all of them the optimal was reached .", "label": "", "metadata": {}}
{"text": "( 2007 ) realizes the change of preconditioners in a stage in the process of optimization where the splitting preconditioner still does not obtain a good computational performance .The tests have shown that the best results are obtained when the CCF is used until the end of the optimization process .The column \" Change \" shows the iteration where the change of preconditioners was realized and the \" Average \" column shows the average of iterations carried out after the change .In the comparison between the two approaches considering the iteration of change of phase we can verify that with the new approach the change is realized in a more advanced stage of the process of optimization .", "label": "", "metadata": {}}
{"text": "5 CONCLUSIONS .The problems presented in this work were tested by Bocanegra et al .( 2007 ) using the PCx with the classic approach using direct methods for the solution of the linear systems .In most of the problems tested , it was possible to overcome the time of execution , the number of iterations and in some cases it was even possible to run new problems .In this work , a new study of the hybrid approach applied to large - scale problems has been carried out .The study presents a new change of phase and a new reordering for the calculation of the matrix B of the splitting preconditioner .", "label": "", "metadata": {}}
{"text": "The preconditioner obtained with CCF is much cheaper and uses less memory than the splitting preconditioner .This suggests that it may be more advantageous to maintain it for the most number of iterations possible and with this new condition of change this is guaranteed .An initial proposal we are testing for the limit of \u03b7 is the average of the nonzero values of the matrix ( bearing in mind that the initial value of \u03b7 is calculated from this average , but with negative sign ) .The splitting preconditioner is very efficient in the last iterations of the interior - point method when the matrix D obtained from the values of x and z has a well defined separation .", "label": "", "metadata": {}}
{"text": "The ideas developed in this work propose a new approach of ordering for the choice of this matrix that , as it has been shown , works better for all the problems tested .However , this ordering is not always respected , because from it the first m columns linearly independent are chosen and it may occur that the last columns are part of the matrix B .This makes the construction of B very expensive and still does not produce the expected results .Future works aim to improve this performance and eliminate these problems .REFERENCES .", "label": "", "metadata": {}}
{"text": "Data structures and programming techniques for the implementation of Karmarkar 's algorithm .[Links ] .[ 2 ] BERGAMASCHI L , GONDZIO J & ZILLI G. 2004 .Preconditioning Indefinite Systems in Interior Point Methods for Optimization .[Links ] .[ 3 ] BOCANEGRA S , CAMPOS FF & OLIVEIRA ARL .Using a hybrid preconditioner for solving large - scale linear systems arising from interior point methods .Special issue on \" Linear Algebra Issues arising in Interior Point Methods \" .[Links ] .[ 4 ] BURKARD RS , KARISCH S & RENDL F. 1991 .", "label": "", "metadata": {}}
{"text": "Links ] .[5 ] CAMPOS FF & BIRKETT NRC .An efficient solver for multi - right - hand - side linear systems based on the CCCG ( \u03b7 ) method with applications to implicit time - dependent partial differential equations .[Links ] .[ 6 ] CZYZYK J , MEHROTRA S , WAGNER M & WRIGHT SJ .PCx an interior point code for linear programming .[Links ] .[ 7 ] GOLUB GH & VAN LOAN CF .1996 .Matrix Computations .Third Edition , The Johns Hopkins University Press , Baltimore .", "label": "", "metadata": {}}
{"text": "Links ] .[ 8 ] JONES MT & PLASSMANN PE .An improved Incomplete Cholesky Factorization .[Links ] .[ 9 ] LUSTIG IJ , MARSTEN RE & SHANNO DF .On Implementing Mehrotra 's Predictor - Corrector Interior Point Method for Linear Programming .TR SOR 90 - 03 .[Links ] .[ 10 ] MEHROTRA S. 1992 .On the Implementation of a Primal - Dual Interior Point Method .[Links ] .[ 11 ] MEHROTRA S. 1992b .Implementations of Affine Scaling Methods : Approximate Solutions of Systems of Linear Equations Using Preconditioned Conjugate Gradient Methods .", "label": "", "metadata": {}}
{"text": "Links ] .[ 12 ] MONTEIRO RDC , ILAN ADLER & RESENDE MGC .A Polynomial - time Primal - Dual Affine Scaling Algorithm for Linear and Convex Quadratic Programming and its Power Series Extension .[Links ] .[ 13 ] OLIVEIRA ARL & SORENSEN DC .A new class of preconditioners for large - scale linear systems from interior point methods for linear programming .[Links ] .[14 ] PADBERG M & RIJAL MP .Location , Scheduling , Design and Integer Programming , Kluwer Academic , Boston .[Links ] .", "label": "", "metadata": {}}
{"text": "Adaptive use of iterative methods in predictor - corrector interior point methods for linear programming .[Links ] .[16 ] WRIGHT SJ .Primal - Dual Interior - Point Methods .SIAM Publications , SIAM , Philadelphia , PA , USA .[Links ] Abstract .Genetic studies in mouse models have played an integral role in the discovery of the mechanisms underlying many human diseases .The primary mode of discovery has been the application of linkage analysis to mouse crosses .This approach results in high power to identify regions that affect traits , but in low resolution , making it difficult to identify the precise genomic location harboring the causal variant .", "label": "", "metadata": {}}
{"text": "However , power in this panel is limited by the availability of inbred strains .Previous studies have suggested combining results across multiple panels as a means to increase power , but the methods employed may not be well suited to structured populations , such as the HMDP .In this article , we introduce a meta - analysis - based method that may be used to combine HMDP studies with F2 cross studies to gain power , while increasing resolution .Due to the drastically different genetic structure of F2s and the HMDP , the best way to combine two studies for a given SNP depends on the strain distribution pattern in each study .", "label": "", "metadata": {}}
{"text": "Using our method to map bone mineral density , we find that two previously implicated loci are replicated with increased significance and that the size of the associated is decreased .We also map HDL cholesterol and show a dramatic increase in the significance of a previously identified result .MODEL organisms continue to play a pivotal role in the research of human diseases .The use of mouse models in particular has been extremely effective for the identification of genes underlying Mendelian disorders .The traditional mode of discovery used to identify loci underlying such disorders has been the F2 cross .", "label": "", "metadata": {}}
{"text": "By applying linkage analysis to F2 populations , regions harboring causal variants are identified with high statistical power .Unfortunately , these approaches have had limited success in identifying genetic variations underlying complex , polygenic traits due to the low resolution of the studies ( Flint and Mott 2001 ; Bennett et al .2010 ) , meaning that the regions found to harbor causal variants are very large .As an alternative to F2 mapping , a number of groups have proposed the use of genome - wide association study ( GWAS ) methodologies to map traits in inbred populations ( Pletcher et al .", "label": "", "metadata": {}}
{"text": "2007 ) .Such approaches result in increased resolution , as inbred strains have a more diverse genetic structure , in which only small portions of the genome are shared between any two strains .The initial results were promising , but it was later found that the significant population structure within inbred strains causes a large number of spurious associations and inflates the significance of true associations .Upon correction for population structure , most of the associations identified as significant were found to be spurious ( Kang et al .2008 ; Manenti et al .2009 ) .", "label": "", "metadata": {}}
{"text": "They showed that when performing association mapping within this panel they achieved higher resolution than when performing mapping only using RI strains and showed that they achieved higher power than when performing mapping with only inbred strains .However , the power to detect small effects remains quite low , a problem that is due to an inherent limitation in the design of the HMDP : the limit on the availability of inbred strains .The core concepts behind these methods , all of which are formed on linkage - based methodologies , may be adapted to work in association analysis .", "label": "", "metadata": {}}
{"text": "For example , a shared feature of these linkage - based methods is the attribution of equal informativeness to each study .Such an assumption may not hold in studies with population structure , as the informativeness of a given panel will be locus dependent .In this case , methods attributing equal weight to each population may result in suboptimal power .In this article , we propose a method with which to combine studies in a locus - specific manner , weighting each study relative to its level of informativeness , and show that our method achieves optimal power within the proposed framework .", "label": "", "metadata": {}}
{"text": "In a meta - analysis , the statistics obtained for each SNP in two separate studies are used to obtain a meta - statistic , which combines information across studies .The most common methods for performing meta - analysis are based on the fixed effect weighted sum of Z -scores ( WSoZ ) ( de Bakker et al .2008 ) , in which Z -scores from each study are combined using a predefined weighting scheme .Typically , weights are set as proportional to the number of individuals in the study .Using this basic idea , we propose a meta - analysis method for combining the results obtained from mapping in the HMDP with those obtained from mapping within an F2 population .", "label": "", "metadata": {}}
{"text": "We introduce a method that accounts for the genetic structure within each population when combining results .Using a mixed - model - based approach to correct for population structure , we derive a meta - statistic based on the WSoZ. By applying an optimal weighting scheme , our method achieves both higher power and increased resolution over mapping performed only within one population .We note that the HMDP is only one of several recently proposed strategies for increasing the resolution of mouse genetic studies over traditional crosses .Other strategies include the collaborative cross ( Aylor et al .", "label": "", "metadata": {}}
{"text": "2009 ) .The meta - analysis method we introduce is flexible and may be used to combine studies conducted within these panels as well .We evaluate our method through simulation and by applying it to real phenotype data for which previous discoveries have been made .First , we evaluate both power and resolution through a simulation framework .We find under many different settings that the meta - analysis approach results in higher power when compared to either single panel .We also find that when applying the meta - analysis approach , resolution is increased 1.5-fold with respect to the HMDP and 3.5-fold with respect to an F2 panel .", "label": "", "metadata": {}}
{"text": "2009 , 2011 ) .In our results , two previously implicated loci are recovered with increased significance .We also find that our method results in increased resolution over results obtained through linkage analysis .Finally , we apply our method to map HDL cholesterol in 687 HMDP mice and 164 F2 mice ( van Nas et al .2009 ) and find that a gene ( Apoa2 ; Warden et al .1993 ) known to be associated is identified with increased significance .Materials and Methods .Association studies .Let us assume that we have measured a phenotype within a population i that contains n i individuals .", "label": "", "metadata": {}}
{"text": "y .i . [ .y .i .y .i . . . .y .i .n . i . ]y .i .x .i .We denote the estimate of \u03b2 in population i as .^ .i , where .^ .i .N .s .i . and .s .i . 2 denotes the squared standard error of the estimate in population i .Z .i .^ .i .s .i .Traditional Meta - analysis .Most traditional methods for meta - analysis employ the WSoZ approach ( Willer et al .", "label": "", "metadata": {}}
{"text": "2008 ; Soranzo et al .2009 ) .In this method , a meta - statistic for each SNP is calculated using Equation 3 , where w i denotes a weight given to each Z -score for a population i .We note that our meta - statistic formulation in Equation 3 uses a different notation , with respect to standard meta - analysis literature , which represents the meta - statistic as a sum over effect sizes .However , both formulations are equivalent : .Z . meta .i . w .i .Z .", "label": "", "metadata": {}}
{"text": "i . w . i .( 3 )The weights , w i , are often a function of the sample size of their respective population , so that larger population samples obtain a higher weight ( de Bakker et al .2008 ) .This weighting scheme make sense intuitively as we may want to attribute greater confidence to studies with more individuals .The resulting meta - statistic is the so - called pooled inverse variance - weighted \u03b2 - coefficient ( de Bakker et al .2008 ) .As has been done for case - control studies ( Zaitlen and Eskin 2010 ) , it is possible to show that this particular weighting scheme is optimal in the sense that these weights maximize the power of detecting an effect of size \u03b2 .", "label": "", "metadata": {}}
{"text": "^ .i . w .i .s .i . )i . w .i .i . w .i .s .i .i . w .i .s .i . )Association studies in structured populations .Although the traditional approach to association mapping is often used , a number of issues arise when performing this basic analysis .Mixed effects models are often used to correct this problem ( Yu et al .2006 ; Kang et al .2008 , 2010 ) .Methods employing a mixed effects correction account for the genetic similarity between individuals with the introduction of a random variable into the traditional model from Equation 1 : .", "label": "", "metadata": {}}
{"text": "i .x .i .u . i .( 4 )In the model in Equation 4 , the random variable u i represents the vector of genetic contributions to the phenotype for individuals in population i .This random variable is assumed to follow a normal distribution with .u . i .N .g .K .i . ) , where K i is the n i \u00d7 n i kinship coefficient matrix for population i .With this assumption , the total variance of y i is given by .i . g .", "label": "", "metadata": {}}
{"text": "i .e .I .^ .i .To avoid complicated notation , we introduce a more basic matrix form of the model in Equation 4 : .y .i .X .i .u . i .( 5 )In Equation 5 , X i is a n i \u00d7 2 matrix encoding the global mean and SNP vectors and \u0393 is a 2 \u00d7 1 coefficient vector .We note that this form also easily extends to models with multiple covariates .The maximum - likelihood estimate for \u0393 in population i is given by .", "label": "", "metadata": {}}
{"text": "i .X .i .i .X .i . )X .i .i .y .i , which follows a normal distribution with a mean equal to the true \u0393 and variance .X .i .i .X .i . )^ .i : .Z .i .[ .R .X .i .i .X .i . )R .R .^ .i ( 6 ) .Q .i .R .^ .i ( 7 ) .Q .", "label": "", "metadata": {}}
{"text": "[ .R .X .i .i .X .i . )R .Meta - analysis in structured populations .To perform meta - analysis using multiple structured populations , we adopt the weighted sum of z -scores approach shown in Equation 3 , where the z -score for population i is given in Equation 7 . w .i .Q .i .R .w .i .Again we employ the use of the Cauchy - Schwarz inequality , shown in Equation 9 , to show that the optimal weights are given by .", "label": "", "metadata": {}}
{"text": "i .Q .i .We may also arrive at this result by noting that .Q .i . 2 from Equation 7 is the mixed - model equivalent to s from section ( traditional meta - analysis ) .However , this result is more general , allowing for a more flexible hypothesis - testing framework in which any linear combinations of the elements of \u0393 may be evaluated : . w .i .Q .i .R .w .i .Q .i .R .( 9 )By substituting the optimal weights we arrive at the final meta - statistic given in Equation 10 with its distribution under the alternative hypothesis given in Equation 11 : .", "label": "", "metadata": {}}
{"text": "Q .i .R .^ .i .Q .i .N .Q .i .It should be noted that when \u03a3 i is unknown , it must be estimated from the data .In this case , Z meta may not follow a standard normal distribution under the null , due to the unaccounted uncertainty in the estimation of \u03a3 i .However , we are able to side step this issue by using a global search technique ( Kang et al .2008 , 2010 ) to find an optimal estimate of \u03a3 i for each population .", "label": "", "metadata": {}}
{"text": "Simulations were performed using a previously designed framework ( Kirby et al .2010 ; Bennett et al .2010 ) .For both power and resolution , phenotypes were generated by sampling a phenotype for each strain while assuming the model from Equation 4 .The genetic variance .g .g . g .e .n . g .Tr .S .K . i .S . )( 12 )The power and resolution for each effect size ( \u03b2 ) was determined by first applying the association mapping procedures to each simulated phenotype .", "label": "", "metadata": {}}
{"text": "For resolution , association was applied to each SNP on the same chromosome as the causal SNP .The distance between the true causal SNP and the peak associations were recorded .This procedure helps to reduce the mean shift that occurs because of low power within a region .Significance threshold estimation .Significance thresholds were estimated for each method using a technique utilized previously ( Bennett et al .2010 ; Kirby et al .2010 ) .Ten thousand null phenotypes were generated and association statistics were calculated for each phenotype over all SNPs .We selected the minimum P -value for each phenotype , resulting in a set of 10,000 minimum null P -values .", "label": "", "metadata": {}}
{"text": "This P -value then represents our threshold controlling for 5 % FDR .Mouse association data .Genotypes for the F2 cross were obtained from a previous study ( Estrada - Smith et al .2004 ; Wang et al .2006 ; van Nas et al .2009 ; Farber et al .2011 ) .The original cross contained 311 mice , but we randomly sampled only 300 for our simulation studies .Each mouse was genotyped at about 1200 markers spread across the genome and it was this set of markers that was used previously to perform linkage analysis .", "label": "", "metadata": {}}
{"text": "Fortunately , since the parental strains for the F2s are part of the HMDP , genotyping is not necessary .Instead we perform imputation to determine the state of each marker , which is typed in the HMDP but is not part of the markers typed in the F2 cross .By applying the imputation algorithm described below , we obtained a set of 113,650 SNPs that were polymorphic in both the HMDP and the F2 cross .This is compared to the total set of markers available for the HMDP , which is of size 132,285 .We utilize a straightforward approach to imputation by noting the simple structure of the F2 genomes .", "label": "", "metadata": {}}
{"text": "Let two adjacent markers be x i and x i + k , where k is the number of intervening markers .Likewise , if both x i and x i + k share the same state as parent 2 , the intervening markers will be set to those from parent 2 .If there is a switch in state between the two adjacent markers , this indicates a recombination .In this case , we are not able to determine the state of the intervening markers and these will be labeled as unknown .This process assumes that the probability of a double recombination occurring between genotyped markers is close to zero .", "label": "", "metadata": {}}
{"text": "Combining the HMDP with an F2 cross increases power .We show that by combining the mapping results obtained in the HMDP with those obtained in an F2 cross through meta - analysis , we achieve higher power than when mapping within only one panel .Simulations are performed with genotypes for 300 F2 mice , which were obtained from a previously generated cross ( Estrada - Smith et al .2004 ; Wang et al .2006 ; van Nas et al .2009 ) .The F2s were genotyped at about 1200 markers and imputation was performed ( see Materials and Methods ) to obtain genotypes at all markers typed in the HMDP strains .", "label": "", "metadata": {}}
{"text": "2010 ; Kirby et al .2010 ) .We randomly selected a set of 10,000 SNPs that are polymorphic in both the F2 cross and the HMDP .For each SNP we generated a phenotype with a 25 % genetic background effect and a SNP effect of a given size .The genetic background effect can be thought of as the heritability of the trait .Association between each SNP and its corresponding set of generated phenotypes was tested using the efficient mixed - model association ( EMMA ) method ( Kang et al .2008 ) for the F2 and HMDP panels alone .", "label": "", "metadata": {}}
{"text": "Significance thresholds for each panel were obtained through a parametric bootstrap procedure ( see Materials and Methods ) .Figure 1 shows the comparison of power between the meta - analysis approach and mapping within the individual panels .In these simulations , we varied both the number of F2 mice and the number of HMDP replicates .Power is reported on the y -axis and the magnitude of the SNP effect is reported on the x -axis .The SNP effect is reported in terms of \u03b2 from Equation 4 and the actual variance explained for a given value will depend on the SNP as well as the genetic background .", "label": "", "metadata": {}}
{"text": "The meta - analysis method has higher power than mapping within the single populations in all simulations .As power within each of the single populations increases , so does the power of the meta - analysis method .For a large number of F2 mice and HMDP mice , the power to detect small effects increases dramatically by applying meta - analysis .When combining the results through meta - analysis , the power increases to 75 % ( Figure 1 D ) .Mapping power is increased by combining populations using meta - analysis .( A ) 100 HMDP , 100 F2s .", "label": "", "metadata": {}}
{"text": "( C ) 300 HMDP , 300 F2s .( D ) 500 HMDP , 300 F2s .We performed simulations assuming a background genetic effect of 25 % .Power was calculated as the percentage of associations detected at a given level of significance for SNPs simulated to be causal .The meta - analysis method is shown to provide increased power at all effect sizes .Meta - analysis leads to an increase in resolution over HMDP and F2 Mapping .We evaluate the mapping resolution when using the HMDP , F2 , and the meta - analysis approaches through simulation .", "label": "", "metadata": {}}
{"text": "Figure 2 compares the distribution of these distances under each mapping method .Simulations were performed assuming a 25 % genetic background effect and a SNP effect accounting for 10 - 15 % of the phenotypic variance .The combined mapping result has higher resolution than that of the HMDP or F2 mapping .The distribution of distances from the true causal SNP to the most significant association are shown in units of megabases .We considered peak associations that are within 15 Mb of the true causal SNP .As expected , the HMDP has much higher resolution than the F2 cross .", "label": "", "metadata": {}}
{"text": "Using one replicate for the HMDP , we find that the mean distance of the peak association to the true causal SNP is 3.17 Mb .This compares with a mean of 7.5 Mb obtained when mapping within the F2 panel .When combining results through the meta - analysis approach , the mean distance is decreased to 2.21 Mb .This is an almost 1.5-fold increase in resolution over the HMDP and an almost 3.5-fold increase in resolution over the F2 panel .Application to bone mineral density .We obtained a set of BMD measurements from the femurs of 865 HMDP mice and 161 male F2 mice .", "label": "", "metadata": {}}
{"text": "2008 ) and applied the meta - analysis approach as well .Manhattan plots summarizing these results are shown in Figure 3 .Two loci ( chromosomes 4 and 7 ) showed an increase in significance relative to the associations in either the F2 or HMDP .The original QTL on chromosome 7 ( Bmd41 ) had a 1.5 LOD support interval of 80 Mb ( 24.9 - 104.9 Mb ) ( Farber et al .2009 ) .We approximate the associated region obtained via meta - analysis by employing a simple approach .Thus defined , the chromosome 7 meta - analysis interval extending from 17.2 to 25.2 Mb is much smaller than the previously obtained support interval .", "label": "", "metadata": {}}
{"text": "Meta - analysis results in increased significance and increased resolution for two loci known to be associated with BMD .Two loci , one on chromosome 4 and one on chromosome 7 , were previously found to be associated with BMD ( Bmd7 and Bmd41 , respectively ) ( Farber et al .2009 ) .After applying meta - analysis , we found that the peak associated SNPs for both of these loci had increased significance with respect to the F2 and HMDP mapping panels .Thresholds for significance are indicated by the horizontal black bars in each plot .", "label": "", "metadata": {}}
{"text": "Application to HDL cholesterol .We obtained a set of HDL measurements for 687 male mice each a member of the HMDP and a set of 164 male F2s ( van Nas et al .2009 ) .We applied association mapping in the HMDP and F2 panels separately using EMMA ( Kang et al .2008 ) and then applied our meta - analysis approach .Figure 4 shows the results of this experiment .As shown in the original article introducing the HMDP , the peak association for HDL is found on distal chromosome one , in which a well - known association with the Apoa2 ( Doolittle et al .", "label": "", "metadata": {}}
{"text": "The mapping results obtained from the F2 panel ( Figure 4 A ) resemble a linkage peak , due to the large amount of linkage disequilibrium within the F2 genomes .Figure 4 B shows the mapping result obtained with the meta - analysis procedure .Meta - analysis increases significance of known association .We compare the ( A ) association mapping results obtained from the HMDP , F2 cross and ( B ) meta - analysis for HDL cholesterol on chromosome 1 .It was previously shown that the Apoa2 gene underlies the chromosome 1 HDL locus ( Warden et al .", "label": "", "metadata": {}}
{"text": "Discussion .In this article , we introduce a study design in which the HMDP inbred panel is combined with an F2 cross to perform association mapping .We show that by utilizing a meta - analysis approach that accounts for the genetic structure of the populations , both association power and resolution are increased when compared with mapping within either of the individual panels .The reason for increased power can be understood intuitively as , in general , increased sample sizes lead to increases in power .However , an increase in resolution when combining a high - resolution panel with a low - resolution panel is somewhat counterintuitive .", "label": "", "metadata": {}}
{"text": "Our results have focused on the case when the HMDP panel is combined with one F2 cross .However , by using the methodology we present , any number of panels can be combined .One obvious potential for this is that by adding additional F2 panels , we may increase power much further .A significant amount of cross data exist in publicly accessible databases such as MGI ( Blake et al .2011 ) .By utilizing existing cross data , researchers will be able to use our technique to increase the power of their studies without spending money to generate F2s of their own .", "label": "", "metadata": {}}
{"text": "2011 ) and heterogeneous stock ( Huang et al .2009 ) .However , one potential issue that may arise when combining the HMDP with such panels is that of heterogeneity of effect size .That is , the magnitude of main effects may vary between different mapping panels due to the difference in the overall genetic structure .In this case , our method may be easily extended to utilize approaches that account for such heterogeneity between effects ( Han and Eskin 2011 ) .Heterogeneity between effect sizes is also known to be a problem between sexes within the same population .", "label": "", "metadata": {}}
{"text": "The Genetics Society of America ( GSA ) , founded in 1931 , is the professional membership organization for scientific researchers and educators in the field of genetics .Our members work to advance knowledge in the basic mechanisms of inheritance , from the molecular to the population level .Affiliated with .Abstract .Background .Modelling proteins with multiple domains is one of the central challenges in Structural Biology .Although homology modelling has successfully been applied for prediction of protein structures , very often domain - domain interactions can not be inferred from the structures of homologues and their prediction requires ab initio methods .", "label": "", "metadata": {}}
{"text": "Results .Here we focus on interacting domain pairs that are part of the same peptide chain and thus have an inter - domain peptide region ( so called linker ) .We have developed a method called pyDockTET ( tet hered - docking ) , which uses rigid - body docking to generate domain - domain poses that are further scored by binding energy and a pseudo - energy term based on restraints derived from linker end - to - end distances .The method has been benchmarked on a set of 77 non - redundant pairs of domains with available X - ray structure .", "label": "", "metadata": {}}
{"text": "Among them , our method pyDockTET finds the correct assembly within the top 10 solutions in over 60 % of the cases .Conclusion .Our results show that rigid - body docking approach plus energy scoring and linker - based restraints are useful for modelling domain - domain interactions .These positive results will encourage development of new methods for structural prediction of macromolecules with multiple ( more than two ) domains .Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 9 - 441 ) contains supplementary material , which is available to authorized users .", "label": "", "metadata": {}}
{"text": "It is estimated that two thirds of proteins in prokaryotes and four fifths of those in eukaryotes are multi - domain proteins [ 1 , 2 ] , many of which have important functions in cell regulation and signalling .From a structural point of view , they range from those with significant and stable interactions between domains , which can usually be defined by X - ray and NMR , to those with flexible linkers and few domain - domain interactions that endow them with large conformational freedom .Crystallography of multi - domain proteins that have flexible linkers is more problematic .", "label": "", "metadata": {}}
{"text": "For multi - domain proteins with no structural information , their domain orientations may be predicted through homology modelling .However , homologous multi - domain templates are not always available .Furthermore , even if a homologous template exists , its domains might not interact in the same way as the protein to model ( see the review of Aloy and Russell [ 3 ] ) .To minimize the chance of inferring wrong interaction data from the templates , Aloy and Russell tried to model putative interactions by assessing residue contacts in the interfaces of known three - dimensional protein structures [ 4 ] .", "label": "", "metadata": {}}
{"text": "For instance , Wollacott and co - workers [ 5 ] modelled domain - domain assemblies by placing the domains at the N- and C - terminal of the linker structure , whose conformation is sampled during the procedure Their approach successfully identified near - native assemblies in 50 % of the studied cases .Another promising tool for ab initio modelling of multi - domain proteins is docking .However , although protein - protein docking could be directly applied to model domain - domain interactions , only a few specific cases have been reported ( perhaps because ranking of domain - domain poses is still challenging ) .", "label": "", "metadata": {}}
{"text": "Their work suggests that data - driven docking is useful in modelling domain assembly as well .Furthermore , Inbar and co - workers [ 8 ] have extended the docking approach to multi - domain and multi - molecular assemblies , by using a heuristic that applies hierarchical construction to represent the assembly process and a greedy algorithm to select candidate complexes .The modelling of multi - domain proteins has also further promising applications in the field of modelling protein - protein complexes where any of the components has multiple domains .In this line , Ben - Zeev et al .", "label": "", "metadata": {}}
{"text": "In this paper , we describe a new approach , pyDockTET , for pair - wise assembly of domains that are connected by an inter - domain linker .We also discuss here the dependence of this scoring function on the linker length and on the quality of the domain models used for the docking .Results and discussion .Structural analysis of linkers in multi - domain proteins .We begin our analysis by examining the inter - domain peptide region ( so called linker ) , whose information is applied in the scoring function of pyDockTET .The goal is to study the relationship between the linker sequence length ( in number of residues ) and the distance between the linker ends ( defined as the distance between the C\u03b1 atoms of the two ends of a linker ) .", "label": "", "metadata": {}}
{"text": "The sequence length of the 542 linkers in this database varies from 2 to 29 amino acids .The linker distribution shows a higher frequency for linkers with shorter lengths , while linkers of 18 residues or more show lower frequencies ( Figure 1a ) .The relation between length and frequency or end - to - end distance of linkers .( a )The relation between length and frequency of linkers in our database of 542 linker structures .( b )The relation between length and end - to - end distance of linkers .The length N on the x - axis refers to those linkers that have length with residue number N and N-1 .", "label": "", "metadata": {}}
{"text": "The average linker end - to - end distances increased almost linearly with sequence length for linkers less than 18 residues long ( Figure 1b ) .This is consistent with the fact that we can not find conformational preferences in the structural data set for linkers of up to 17 residues length ( Figure 2 ) , and with previous studies showing that most linkers lack secondary structure [ 15 ] .However , given our broad definition of contacting domain - domain pairs ( i.e. at least one inter - domain atomic contact ) , concern existed that those cases in our data set with very few inter - domain contacts could introduce errors in the derived statistical parameters .", "label": "", "metadata": {}}
{"text": "We also checked that when removing the additional 42 cases that had up to 20 contact residues , the statistical parameters were basically the same ( data not shown ) .Inter - domain linkers in our data set .Inter - domain linkers in our data set classified according to their sequence lengths .From these average end - to - end distance values and their standard deviations , we have derived a scoring function pyDockTET for docking of domain pairs ( see Methods ) .Given the low frequency ( and correspondingly higher variation of average end - to - end distance value ) of linkers with length larger than 17 , the docking sets we used to benchmark pyDockTET included only domain pairs that have inter - domain linkers with length between 2 - 17 residues .", "label": "", "metadata": {}}
{"text": "We have used docking to rebuild a data set of 77 non - redundant proteins formed by two interacting domains ( see Methods ) .Docking poses were further rescored with distance restraints derived from the linker database , according to linker sequence length ( pyDockTET method ) .The docking results for all cases , as scored by pyDock and by pyDockTET , are shown in Figure 3a .When we re - scored the docking poses using linker - length distance restraints with pyDockTET , the success rates for top 10 and top 50 solutions increased to 61 % and 78 % , respectively ( Figure 3b ) .", "label": "", "metadata": {}}
{"text": "Actually , the restraint - based energy helps especially in those cases where the quality of the docking solutions is not so good ( as in the 14 cases that have acceptable but not good solutions ; Figure 3d ) .The overall success rate of pyDock and pyDockTET .( b ) Success rates for identifying acceptable solutions when considering only those cases with at least one acceptable solution within the ZDOCK docking set .( d ) Success rates for identifying acceptable solutions considering only those cases that had acceptable but not good solutions .In the four panels , the results of pyDockTET are shown with solid lines and the results of pyDock are shown with dashed lines with circle markers .", "label": "", "metadata": {}}
{"text": "The use of pyDock to identify domain assemblies from docking sets clearly gives well over random scoring , and the introduction of linker - based restraints as in pyDockTET further improves the results .Of course , for a realistic case , one has to rely on the docking procedure to generate near - native orientations .We have used here the known FFT - based docking method ZDOCK , but it is expected that the increasing success of rigid - body docking methods will also improve the predictive rates of pyDockTET .Dependence of the predictive success of pyDockTET scoring function on linker length .", "label": "", "metadata": {}}
{"text": "Here we analyse the success rates of pyDockTET for different linker lengths , considering only those cases of our domain - domain set that have at least one acceptable solution .We classified linker lengths into five groups : 2 - 4 , 5 - 7 , 8 - 10 , 11 - 13 and 14 - 17 amino acids .As shown in Figure 4 , pyDockTET gave consistently better predictive success rates in top 50 solutions than pyDock , being the improvement of the restraint - based scoring function particularly significant for linker lengths 5 - 7 ( which is also the most populated group ) .", "label": "", "metadata": {}}
{"text": "Comparison of pyDock and pyDockTET according to the linker length between domains .The success rates of pyDock ( light grey bars ) and pyDockTET ( dark grey bars ) in selecting at least one acceptable solution within the top 50 solutions according to the linker length ( considering only cases with acceptable solutions ) .Dependence of the predictive success of pyDockTET scoring function on the type of domain - domain interface : docking energy and number of contacts .The scoring function of pyDockTET consists of a pseudo - energy term derived from linker end - to - end distances , in addition to the original pyDock function that is formed by electrostatics and desolvation energies .", "label": "", "metadata": {}}
{"text": "First , we will check whether the pyDockTET improvement over pyDock depends on the average pyDock energy obtained for the pool of docking poses , which is different for each case .For each one of the cases that have at least an acceptable docking solution , we sorted the docking solutions by pyDock energy ( defined as only electrostatics and desolvation ) and computed the average of the best 100 energy values .Figure 5a shows the dependence of the success rates of pyDockTET and pyDock on the average energy ( electrostatics plus desolvation ) of the top 100 solutions .", "label": "", "metadata": {}}
{"text": "Thus , as would be expected , the most useful contribution made by tethered domain docking is when the average energy value of the top 100 solutions is low .Comparison of pyDock and pyDockTET according to the energy and the number of contact residues .( a )The success rates of pyDock ( light grey bars ) and pyDockTET ( dark grey bars ) in selecting at least one acceptable solution within the top 50 solutions according to the average electrostatics plus desolvation energy .( b )The success rates of pyDock ( light grey bars ) and pyDockTET ( dark grey bars ) according to the number of domain - domain contact residues ; the percentage of cases in which ZDOCK generated at least one acceptable solution is also shown ( white bars ) .", "label": "", "metadata": {}}
{"text": "However , we observed in Figure 5b that the docking results actually depended quite significantly on the interface size .Figure 5b shows the global success rates of pyDock and pyDockTET , with regard to the number of contact residues in the interface ( defined as residues within 5\u00c5 distance from any atom of the other domain ) .It also shows the percentage of cases with acceptable solutions within the docking set ( this actually limited the maximum success rates we could expect from pyDock or pyDockTET ) .Strikingly , ZDOCK found acceptable solutions only in one of the 13 cases with less than 20 contact residues ( and no acceptable solution was found for the cases with less than 10 contact residues ) , which indicates a clear limitation of the FFT - based docking generation .", "label": "", "metadata": {}}
{"text": "In summary , the linker - based restraints of pyDockTET were able to largely improve the predictive results on those cases particularly difficult for unrestricted docking ( i.e. with poor docking energies and/or small number of contact residues ) .Assembly of domains from modelled individual structures .So far we have shown that pyDockTET gives excellent performance in assembling domain pairs where the structures of the two domains have been obtained from crystal structures , after separating them and remodelling the side - chains of the isolated domains .However , in real situations the individual domains will have been obtained from independent crystal structures or from homology models .", "label": "", "metadata": {}}
{"text": "As can be seen in Table 1 , the pyDockTET predictions were always better ( or in any case similar , but never significantly worse ) than those of pyDock .Table 1 .Domain - domain assembly with pyDockTET using homology models or X - ray structures of the interacting domains .a The name of the structure is shown with the format : ( PDB ID)_(chain name)_(the first residue of the linker)_(the last residue of the linker ) .b The best ranking of any acceptable solution from docking the crystal structures .RMSD ( \u00c5 ) of the second domain is shown in brackets . '", "label": "", "metadata": {}}
{"text": "RMSD ( \u00c5 ) of the second domain is shown in brackets . 'In Figure 6 we compare the results on the sub - set of domains modelled from homologues , with the same sub - set of domains taken from the crystal structures .The top 10 and top 50 success rates for the same sub - set , when coordinates are taken from the crystal structure , are 82 % and 91 % , respectively ( success rates of pyDock alone are 73 % and 82 % , respectively ) ( Figure 6b ) .In some of the cases we use templates with high sequence identity ( see additional file 1 : The 20 unbound ( modelled ) structures ) ; however this does not significantly affect the results .", "label": "", "metadata": {}}
{"text": "Thus the overall results of domain docking do not seem to depend too much on the details of the models .Table 1 shows that most of the cases with poor pyDockTET predictions ( 1gk8 , 1nez , 1s9v , 1etp , 1onq , 1edh , 1hnf , 1jk8 , 1k2d ) had similarly bad predictions when using the X - ray structures .The important issue is that , in most cases , the pyDockTET results we obtained when docking domain models were similarly good to those when docking X - ray structures .Moreover , there are even cases where modelled domains yielded better predictive rates than X - ray structures ( 1ar4 , 1mb8 ) .", "label": "", "metadata": {}}
{"text": "The success rate of pyDock and pyDockTET for predicting crystal and modelled domain assemblies .( a )The success rate of pyDock and pyDockTET in selecting at least one acceptable solution for a sub - set where domain structures have been modelled based on homologue templates , considering only those cases with at least one acceptable solution generated by ZDOCK .( b )The success rate of pyDock and pyDockTET in selecting at least one acceptable solution for the same sub - set when coordinates are taken from crystal structures , considering only those cases with at least one acceptable solution generated by ZDOCK .", "label": "", "metadata": {}}
{"text": "Comparison to other domain - domain assembly approaches .We have evaluated the performance of pyDockTET with respect to other computational methods that have been recently reported for domain - domain assembly .Lise et al .[ 7 ] tested their contact prediction method by generating 10 domain - domain orientations with the docking server GRAMM - X. For 5 of these 12 cases , the best model ( in terms of fraction of native contacts ) was ranked first by their contact scoring function .We can test pyDockTET in this benchmark .However , most of the cases in their benchmark have two linkers between the domains .", "label": "", "metadata": {}}
{"text": "Thus , we have applied our method to the only three cases of their benchmark where the domains are joined by a single linker .When we used the open configurations ( with their side - chains remodelled by SCWRL ) , we found only one case with acceptable solutions , 1jmc .As a note of caution , the overall results of Lise et al .[ 7 ] strongly depended on the ability of GRAMM to generate acceptable docking poses in such small number of alternative poses .Both criteria are used in CAPRI , in combination also with the interface RMSD , but no by separate .", "label": "", "metadata": {}}
{"text": "Inbar et al .[ 8 ] recently described their combinatorial docking approach ( CombDock ) for multi - domain and multi - molecular assembly .However , they reported only three cases of domain - domain docking ( the other reported cases were either docking of secondary structure elements within a single domain , or multi - molecular docking ) : 1a47 , 1b23 , and 1d0n .For all of them they found near - native assemblies within the top 10 solutions .However , our method is not directly applicable to these cases , since they have more than two domains ( we could dock one domain onto the other two domains taken as a single rigid - body , but that would not be a realistic test for our method ) .", "label": "", "metadata": {}}
{"text": "[5 ] recently reported a domain - domain assembly method based on conformational sampling of the inter - domain linker with their Rosetta program .Although they did not use computational docking , they provided an interesting test set to evaluate the performance of our approach .They divided their benchmark set according to their predictive results .We applied our method to 18 of these successful cases ( the other ones were defined as single domains by SCOP , or the linkers were too long for our method ) , and found acceptable solutions for 15 of them .", "label": "", "metadata": {}}
{"text": "[5 ] ) , so we wanted to check whether our method would be able to improve their results .These results are also consistent with the ones obtained in the set of good cases in Wollacott et al .[5 ] and with those in our data set of 77 cases .Thus , our method failed for some of the cases for which Wollacott et al .[5 ] had excellent results , but succeeded in some of the cases where Wollacott et al .[5 ] did not have good predictions .a The best ranking of the near - native prediction by pyDock and pyDockTET .", "label": "", "metadata": {}}
{"text": "We show the best rank of the lowest - RMSD solutions .c A solution is found with RMSD 9.2\u00c5 that is ranked 168 by pyDock , and ranked 20 by pyDockTET .d The information of templates for modelling domain 1 ( left ) and 2 ( right ) .The format of the template information is PDB i d _ chain ( sequence range ) .We also performed an additional test on the difficult cases of Wollacott et al .[5 ] , by independently modelling the two domains based on different templates .Then we ran pyDockTET on these independently modelled domains , instead of on the X - ray structures .", "label": "", "metadata": {}}
{"text": "Our final set was formed by the six cases shown in Table 2 .In four of these six cases , we obtained a reasonable model within the top 10 docking solutions as ranked by pyDockTET ( Table 2 ) .Particularly interesting are cases 1qcs and 1a6q , where pyDockTET found acceptable docking solutions with rank 1 and 2 , respectively .The best solutions for all these cases are shown in Figure 7 .The success rates are quite encouraging , especially considering that these examples were highly challenging cases for other domain - domain assembly methods .", "label": "", "metadata": {}}
{"text": "Best models generated by pyDockTET for the difficult cases of Wollacott et al .[5 ] .The modelled structure for each case is represented in stick mode , with the first domain in white colour .The second domain orientation predicted by pyDockTET is shown in red .The real X - ray structure is shown for comparison in ribbon mode : the first domain in white colour , the second domain in grey , and the linker in cyan .The first domains of the real and modelled structures are superimposed .Conclusion .We have described here a procedure to build multi - domain proteins from the structure ( experimental or modelled ) of their individual domains , using a combination of rigid - body docking , binding energy scoring , and linker - length based distance restraints .", "label": "", "metadata": {}}
{"text": "Provided that the rigid - body generation method is able to produce acceptable domain - domain orientations , our scoring function ( based on docking energy plus restraints ) finds the correct assembly within the top 10 solutions in about 60 - 70 % of the cases .Methods .Rigid - body docking and restraint - based scoring function .Rigid - body docking was performed on the interacting domains by ZDOCK2.1 [ 17 ] .The resulting domain - domain orientations were firstly evaluated with the standard pyDock protocol , which uses Coulombic electrostatics with distance - dependent dielectric constant plus ASA - based desolvation optimized for protein - protein docking as previously described [ 10 ] .", "label": "", "metadata": {}}
{"text": "The X m value is calculated as the average of the end - to - end distance values of linkers that have same length ( i.e. same number of residues ) in the 542 linker structures collected from multi - domain proteins in Protein Data Bank ( PDB ) .The X m value and its corresponding standard deviation , SD , are then used to develop a function , E linker ( Figure 8 ) , which is further incorporated into the pyDock energy function for the final rescoring of domain - domain poses ( equation 1 ) .", "label": "", "metadata": {}}
{"text": "The function of E linker of pyDockTET , where X m is the average end - to - end distance of a linker with specific length , and SD is the standard deviation of the X m . where E elec represents electrostatics and E desolv represents desolvation energy .Predictive success rates evaluation .For a pair of domain structures we generated 2,000 rigid - body docking orientations by ZDOCK2.1 [ 17 ] .Domain - domain structural test set .For a more realistic domain assembly test , we used SCWRL 3.0 [ 16 ] in order to re - model all side chains of the individual domains before docking .", "label": "", "metadata": {}}
{"text": "This sub - set was generated from the previously described benchmark of 77 pairs , after selecting those cases in which both domains had available templates and thus could be independently modelled .Fugue [ 22 ] was used to find templates in those cases in which BLAST failed and also to generate all the sequence - structural alignments .Finally MODELLER [ 23 ] was used to generate models for each domain .The modelled cases are listed in Table 1 .The template structures and the sequence identities ( computed from the structural alignments ) can be found in the additional file 1 : The 20 unbound ( modelled ) structures .", "label": "", "metadata": {}}
{"text": "Acknowledgements .We are grateful for the suggestions received from the anonymous reviewers , especially with regard to the analysis of domain - domain contacts .T.M.K.C. is recipient of a Cambridge Overseas Trust Fellowship .This work is supported by the Plan Nacional I+D+I grant BIO2005 - 06753 from the Spanish Ministry of Science .Electronic supplementary material .1471 - 2105 - 9 - 441-S1.doc Additional file 1 : The 20 unbound ( modelled ) structures .Each structure is shown as ( PDB ID ) _ ( chain ) _ ( the first residue of the linker ) _ ( the last residue of the linker ) .", "label": "", "metadata": {}}
{"text": "1471 - 2105 - 9 - 441-S2.doc Additional file 2 : The 77 non - redundant bound structures .Each domain complex is shown as ( PDB ID ) _ ( chain ) _ ( the first residue of the linker ) _ ( the last residue of the linker ) .( DOC 36 KB ) .Authors ' contributions .TLB and JFR devised the concept and directed the research .JFR designed the procedure .TMKC design the implementation of the program and performed the calculations .TMKC and JFR analysed the data .TMKC drafted the paper .", "label": "", "metadata": {}}
{"text": "All authors read and approved the final manuscript .Authors ' Affiliations .Department of Biochemistry , University of Cambridge .Barcelona Supercomputing Center .References .Apic G , Gough J , Teichmann SA : Domain combinations in archaeal , eubacterial and eukaryotic proteomes .J Mol Biol 2001 , 310 : 311 - 325 .View Article PubMed .Chothia C , Gough J , Vogel C , Teichmann SA : Evolution of the protein repertoire .Science 2003 , 300 : 1701 - 1703 .View Article PubMed .Aloy P , Russell RB : The third dimension for protein interactions and complexes .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Aloy P , Russell RB : InterPreTS : protein interaction prediction through tertiary structure .Bioinformatics 2003 , 19 : 161 - 162 .View Article PubMed .Wollacott AM , Zanghellini A , Murphy P , Baker D : Prediction of structures of multidomain proteins from structures of the individual domains .Protein Sci 2007 , 16 : 165 - 175 .View Article PubMed .Xu D , Baburaj K , Peterson CB , Xu Y : Model for the Three - Dimensional Structure of Vitronectin : Predictions for the Multi - Domain Protein fromThreading and Docking .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Lise S , Walker - Taylor A , Jones DT : Docking protein domains in contact space .BMC Bioinformatics 2006 , 7 : 310 .View Article PubMed .Inbar Y , Benyamini H , Nussinov R , Wolfson HJ : Combinatorial docking approach for structure prediction of large proteins and multi - molecular assemblies .Phys Biol 2005 , 2 : S156 - 165 .View Article PubMed .Ben - Zeev E , Kowalsman N , Ben - Shimon A , Segal D , Atarot T , Noivirt O , Shay T , Eisenstein M : Docking to single - domain and multiple - domain proteins : old and new challenges .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Cheng TM , Blundell TL , Fernandez - Recio J : pyDock : electrostatics and desolvation for effective scoring of rigid - body protein - protein docking .Proteins 2007 , 68 : 503 - 515 .View Article PubMed .Grosdidier S , Pons C , Solernou A , Fernandez - Recio J : Prediction and scoring of docking poses with pyDock .Proteins 2007 , 69 : 852 - 858 .View Article PubMed .Finn RD , Mistry J , Schuster - B\u00f6ckler B , Griffiths - Jones S , Hollich V , Lassmann T , Moxon S , Marshall M , Khanna A , Durbin R , et al .", "label": "", "metadata": {}}
{"text": "Nucleic Acids Res 2006 , ( Database 34 ) : D247-D251 .Murzin AG , Brenner SE , Hubbard T , Chothia C : SCOP : a structural classification of proteins database for the investigation of sequences and structures .J Mol Biol 1995 , 247 : 536 - 540 .PubMed .Flory PJ : Statistical Mechanics of Chain Molecules .Interscience , New York 1969 .Argos P : An investigation of oligopeptides linking domains in protein tertiary structures and possible candidates for general gene fusion .J Mol Biol 1990 , 211 : 943 - 958 .", "label": "", "metadata": {}}
{"text": "Canutescu AA , Shelenkov AA , Dunbrack RLJ : A graph - theory algorithm for rapid protein side - chain prediction .Protein Sci 2003 , 12 : 2001 - 2014 .View Article PubMed .Chen R , Li L , Weng Z : ZDOCK :An Initial - stage Protein - Docking Algorithm .Proteins 2003 , 52 : 80 - 87 .View Article PubMed .Vajda S : Classification of protein complexes based on docking difficulty .Proteins 2005 , 60 : 176 - 180 .View Article PubMed .Kowalsman N , Eisenstein M : Inherent limitations in protein - protein docking procedures .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Altschul SF , Gish W , Miller W , Myers EW , Lipman DJ : Basic local alignment search tool .J Mol Biol 1990 , 215 : 403 - 410 .PubMed .Sali A , Blundell TL : Definition of general topological equivalence in protein structures .A procedure involving comparison of properties and relationships through simulated annealing and dynamic programming .J Mol Biol 1990 , 212 : 403 - 428 .View Article PubMed .Shi J , Blundell TL , Mizuguchi K : FUGUE : sequence - structure homology recognition using environment - specific substitution tables and structure - dependent gap penalties .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Sali A , Blundell TL : Comparative protein modelling by satisfaction of spatial restraints .J Mol Biol 1993 , 234 : 779 - 815 .View Article PubMed .Copyright .\u00a9 Cheng et al .2008 .This article is published under license to BioMed Central Ltd.A Note on Solutions of the SIR Models of Epidemics Using HAM .Received 10 July 2013 ; Accepted 1 September 2013 .Academic Editors : H. Chung , Y.-D. Kwon , and G. Wang .Copyright \u00a9 2013 M. Sajid et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}}
{"text": "Recently , Awawdeh et al .( 2009 ) discussed the solutions of SIR epidemics model using homotopy analysis method .This comment points out some crucial flaws in ( Awawdeh et al .2009 ) .Particularly , results presented in Figure 1 of the ( Awawdeh et al .2009 ) do not represent the 20 term solution of the considered problem as stated .The present paper also provides a new approach for solving SIR epidemics model using homotopy analysis method .The new approach is based on dividing the entire domain into subintervals .In each subinterval the three - term HAM solution is sufficient for obtaining accurate and convergent results .", "label": "", "metadata": {}}
{"text": "Introduction .The authors in [ 1 ] have considered the classic SIR epidemic model for the homotopy analysis method ( HAM ) solution .The constant population in SIR model is divided into susceptible , infectious , and recovered classes [ 2 , 3 ] .The expressions for the susceptible , infectious , and recovered population presented in [ 1 ] for five- and nine - term HAM solutions clearly indicate that , for ., the initial population size is 20 , 15 , and 10 , respectively .However , Figure 1 illustrates a different population size .", "label": "", "metadata": {}}
{"text": "However , the authors in [ 1 ] obtained same expressions as presented in [ 4 - 7 ] , but the graphical results are different from all these studies which are not possible .Thus Figure 1 present in [ 1 ] is not found through 20-term HAM solution .The objective of the present paper is to revisit the HAM solution for the nonlinear initial value problems considered by Awawdeh et al .[ 1 ] and to provide a HAM solution which agrees well with the numerical results already obtained for the same problem .", "label": "", "metadata": {}}
{"text": "In this new approach the whole domain of the problem is divided into subintervals , and the HAM solution is evaluated separately in each subinterval .A three - term HAM approximation in each subinterval is sufficient for an accurate and convergent solution .SIR Model and Solution .The classic SIR epidemic model is given by the following system of nonlinear ordinary differential equations [ 1 ] : . subject to initial conditions ( 11 ) , and we obtain the initial conditions for the third subinterval .In this manner , the numerical values obtained at the final point of each subinterval are the values at the initial point of the next subinterval .", "label": "", "metadata": {}}
{"text": "Following this procedure , the system of ( 6 ) - ( 8 ) are subsequently solved from one subinterval to the next subinterval .The convergence and accuracy of the obtained solutions are ensured by keeping the length of subinterval small .Results and Discussion .The procedure discussed in Section 2.2 is implemented using the symbolic software Mathematica .The system of ( 6 ) - ( 8 ) in each subinterval is solved by using the same set of initial values of susceptible , infectious , and recovered population as given in [ 1 ] .The Runge - Kutta method is used to solve the initial value problems given in ( 1 ) - ( 2 ) numerically .", "label": "", "metadata": {}}
{"text": "Figures 1 , 2 , and 3 present the comparison between the solution given in [ 1 ] , the numerical solution , and solution with the new approach .Figure 1 elucidate that a twentieth order standard HAM solution ( dotted line ) is valid for .are small , and for large values the domain becomes further short for the accurate and convergent solution .Beyond this time , the population is going in the negative direction which is not physically possible .The similar observations are made for the infectious and recovered populations and are presented in Figures 2 and 3 .", "label": "", "metadata": {}}
{"text": "It is evident from Figure 4 that the results obtained are in excellent agreement with the numerical solutions .It is important to mention here that Figure 1 given in [ 1 ] could not be obtained by the HAM solution presented in [ 1 ] and some other solutions are given in Figure 1 of [ 1 ] .Conclusions .In this paper we present a new approach for solving the SIR epidemic model problem using homotopy analysis method .It is found that this new approach is an effective method for providing numerical and analytic closed form solutions of such problems .", "label": "", "metadata": {}}
