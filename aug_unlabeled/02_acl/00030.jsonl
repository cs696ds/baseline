{"text": "Tools . by Ted Briscoe , John Carroll - In Proceedings of the 5th ACL Conference on Applied Natural Language Processing , 1997 . \" ...We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora .Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English .An initial experiment , on a sample of 14 verb ... \" .We describe a novel technique and implemented system for constructing a subcategorization dictionary from textual corpora .Each dictionary entry encodes the relative frequency of occurrence of a comprehensive set of subcategorization classes for English .", "label": "", "metadata": {}}
{"text": "We also demonstrate that a subcategorization dictionary built with the system improves the accuracy of a parser by an appreciable amount 1 . ... rs ) .It is ' shallow ' in that no atof which thetempt is made to fully analyse unbounded dependencies .Furthe ... . \" ...We present a critical overview of the state - of - the - art in parser evaluation methodologies and metrics .A discussion of their relative strengths and weaknesses motivates a new --- and we claim more informative and generally applicable --- technique of measuring parser accuracy , based on the use of gramma ... \" .", "label": "", "metadata": {}}
{"text": "A discussion of their relative strengths and weaknesses motivates a new --- and we claim more informative and generally applicable --- technique of measuring parser accuracy , based on the use of grammatical relations .We conclude with some preliminary results of experiments in which we use this new scheme to evaluate a robust parser of English . \" ...Manual development of large subcategorised lexicons has proved difficult because predicates change behaviour between sublanguages , domains and over time .Yet access to a comprehensive subcategorization lexicon is vital for successful parsing capable of recovering predicate - argument relations , and pr ... \" .", "label": "", "metadata": {}}
{"text": "Yet access to a comprehensive subcategorization lexicon is vital for successful parsing capable of recovering predicate - argument relations , and probabilistic parsers would greatly benefit from accurate information concerning the relative likelihood of different subcategorisation frames ( scfs ) of a given predicate .Acquisition of subcategorization lexicons from textual corpora has recently become increasingly popular .Although this work has met with some success , resulting lexicons indicate a need for greater accuracy .One significant source of error lies in the statistical filtering used for hypothesis selection , i.e. for removing noise from automatically acquired scfs .", "label": "", "metadata": {}}
{"text": "Our investigation shows that statistical filters tend to work poorly because not only is the underlying distribution zipfian , but there is also very little correlation between conditional distribution of . by Michael A. Covington - In Proceedings of the 39th Annual ACM Southeast Conference , 2001 . \" ...Abstract - This paper presents a fundamental algorithm for parsing natural language sentences into dependency trees .Unlike phrase - structure ( constituency ) parsers , this algorithm operates one word at a time , attaching each word as soon as it can be attached , corresponding to properties claimed for ... \" .", "label": "", "metadata": {}}
{"text": "Unlike phrase - structure ( constituency ) parsers , this algorithm operates one word at a time , attaching each word as soon as it can be attached , corresponding to properties claimed for the parser in the human brain .Like phrasestructure parsing , its worst - case complexity is O(n 3 ) , but in human language , the worst case occurs only for small n. 1 Overview .This paper develops , from first principles , several variations on a fundamental algorithm for parsing natural language into dependency trees .This is an exposition of an algorithm that has been known , in some form , since the 1960s but is not presented systematically in the extant literature .", "label": "", "metadata": {}}
{"text": "There is good evidence that the parsing process used by the human mind has these properties [ 1].2 Dependency grammar .2.1 The key concept .There are two ways to describe sentence structure in natural language : by breaking up the sentence into constituents ( phrases ) , which are then broken into smaller constituents ( Fig . 1 ) , or by drawing links connecting . ... tuency grammar , NP and VP are atomic symbols not related to N and V , a fact all too seldom appreciated . )Thus , constituency grammar as currently practiced is very close to being a notational variant of dependency grammar .", "label": "", "metadata": {}}
{"text": "A bar over a .. s , adjectives and nouns ) by including separate rules for each pattern of possible complementation in English .Although ... . \" ...We describe an implemented system for robust domain - independent syntactic parsing of English , using a unification - based grammar of part - ofspeech and punctuation labels coupled with a probabilistic LR parser .We present evaluations of the system 's performance along several different dimensions ; ... \" .We describe an implemented system for robust domain - independent syntactic parsing of English , using a unification - based grammar of part - ofspeech and punctuation labels coupled with a probabilistic LR parser .", "label": "", "metadata": {}}
{"text": "Currently , the system is able to parse around 80 % of sentences in a substantial corpus of general text containing a number of distinct genres .On a random sample of 250 such sentences the system has a mean crossing bracket rate of 0.71 and recall and precision of 83 % and 84 % respectively when evaluated against manually - disambiguated analyses . ... bs , adjectives and nouns ) by including separate rules for each pattern of possible complementation in English . \" ...In this article I present a series of arguments that syntactic structures are built incrementally , in a strict left - to - right order .", "label": "", "metadata": {}}
{"text": "In this article I present a series of arguments that syntactic structures are built incrementally , in a strict left - to - right order .By assuming incremental structure building it becomes possible to explain the differences between the range of constituents available to different diagnostics of constituency , including movement , ellipsis , coordination , scope and binding .In an incremental derivation structure building creates new constituents , and in doing so may destroy existing constituents .The article presents detailed evidence for the prediction of incremental grammar , that a syntactic process may refer to only those constituents that are present at the point in the derivation when the process applies .", "label": "", "metadata": {}}
{"text": "Introduction Tests of constituency are basic components of the syntactician 's toolbox .By investigating which strings of words can and can not be moved , deleted ... .In order to ach ... . by Sabine Buchholz - In Proceedings of the ESSLLI-98 Workshop on Automated Acquisition of Syntax and Parsing , 1998 . \" ...The automatic distinction between complements and adjuncts , i.e. between subcategorized and non - subcategorized constituents , is crucial for the automatic acquisition of subcategorization lexicons from corpora .In this paper we present memory - based learning experiments for the task of distinguishing ... \" .", "label": "", "metadata": {}}
{"text": "In this paper we present memory - based learning experiments for the task of distinguishing complements from adjuncts .Data is extracted from the part - of - speech tagged and parsed version of the Wall Street Journal Corpus .Memory - based learning algorithms classify test instances by using the class of the most similar training instance .By providing the algorithm with different subsets of features in the data , we can explore the importance of different features .By using only syntactic information about the category itself and its neighboring constituents , we achieve an accuracy of 91.6 % for the complement adjunct distinction , which corresponds to 89.7 % correctly classified subcategorization frames .", "label": "", "metadata": {}}
{"text": "These contain about 50,000 sentences , totalling more than a million words .Thus complements are NPs without any func ... . \" ...We describe a recently developed corpus annotation scheme for evaluating parsers that avoids some of the shortcomings of current methods .The scheme encodes grammatical relations between heads and dependents , and has been used to mark up a new public - domain corpus of naturally occurring English text ... \" .We describe a recently developed corpus annotation scheme for evaluating parsers that avoids some of the shortcomings of current methods .The scheme encodes grammatical relations between heads and dependents , and has been used to mark up a new public - domain corpus of naturally occurring English text .", "label": "", "metadata": {}}
{"text": "Research on word se ... \" .Research on word segmentation has shown that learners can use transitional probabilities between syllables to segment speech into word - like units ( Saffran , Aslin , & Newport , 1996 ) .In the present research , we combine and extend these two sets of findings , asking whether learners can use transitional probabilities between words ( or word classes ) to segment sentences into phrases , and use this phrasal information to fully acquire the syntax of a miniature language .Adult subjects were exposed to sentences from a miniature language .", "label": "", "metadata": {}}
{"text": "All conditions outperformed controls in learning the language .The best learning occurred with all properties combined , despite the fact that this language was the most complex .These data address the important question of how language learning is successful in the face of the massive complexity of natural languages .In our experiments , learning got better , not worse , when properly structured complexity was added to a language .The results also show that the same type of statistical computation useful in word segmentation might be used as well in learning syntax , suggesting that the range of statistics needed for acquiring various types of structure in natural languages might be suitably small .", "label": "", "metadata": {}}
{"text": "Morgan and Newport ( 1981 ) showed that the reference world of Moeser and Bregman ( 1972 ) was successful in facilitating the acquisition of complex aspects of syntax because it served to demarcate the ...Affiliated with .Affiliated with .Abstract .Background .We present a model for tagging gene and protein mentions from text using the probabilistic sequence tagging framework of conditional random fields ( CRFs ) .The mechanics of CRFs and their relationship to maximum entropy are discussed in detail .Results .We employ a diverse feature set containing standard orthographic features combined with expert features in the form of gene and biological term lexicons to achieve a precision of 86.4 % and recall of 78.7 % .", "label": "", "metadata": {}}
{"text": "Background .Information extraction from biomedical text has attracted increasing research interest over the past few years .Several large scale annotated corpora have been developed [ 1 ] or are being developed [ 2 ] to facilitate this process .The first step in most information extraction systems is to identify the named entities that are relevant to the concepts , relations and events described in the text .In molecular biology , named entities related to genes , proteins or other biologically - active molecules are especially important .We present here a method for identifying gene and protein mentions in text with conditional random fields ( CRFs ) [ 6 ] , which are discussed in the next section .", "label": "", "metadata": {}}
{"text": "[ 3 ] suggest that rule - based systems make decisions on sets of textual indicator features and that these features may easily be exploited by supervised statistical approaches .Our method does just this by turning many of the post - processing steps of Tanabe and Wilbur [ 5 ] into features used in the extraction CRF .This is a single probabilistic tagging model with no application - specific pre- or post - processing steps or voting over multiple classifiers .This makes the model quite general in that it may be extended to various other biological entities , provided appropriate lexicons are available .", "label": "", "metadata": {}}
{"text": "Implementation .Presented here is an outline of conditional random fields and the implementation specifics of the model we use .Conditional random fields .Gene identification as a tagging problem .A sample tagging of a sentence using the beginning , inside and outside tag labels .The sentence has two gene mentions , Varicella - zoster virus ( VZV ) glycoprotein gI and type 1 transmembrane gylcoprotein .In this definition , we assume that the j th input token is represented by a set o j of predicates that hold of the token or its neighborhood in the input sequence .", "label": "", "metadata": {}}
{"text": "The most probable tag sequence for a given input sequence o can be obtained by applying a Viterbi - style algorithm [ 6 , 8 ] to the maximization .The predicate set o j used to represent the j th input token picks out useful properties of the token and its context .The tag sequence uses the possible tags B - GENE , I - GENE and O , representing the beginning , inside and outside of a gene mention respectively .As noted before , each feature function relates input properties and a k -gram of tags .", "label": "", "metadata": {}}
{"text": "The weight \u03bb i for each feature should ideally be highly positive if the feature tends to be on for the correct labeling , highly negative if the feature tends to be off for the correct labeling , and around zero if the feature is uninformative .To achieve this , the model is trained so that the weights maximize the log - likelihood of the training data , : .To avoid degeneracy when some features are perfectly correlated and to reduce overfitting for rarely occurring features , we penalize the likelihood with a spherical Gaussian prior over feature weights [ 9 ] : .", "label": "", "metadata": {}}
{"text": "If we force the weights to be relatively small by choosing an appropriate small variance , the possibility of a single large weight dominating a decision is reduced .The log - likelihood function of conditional random fields , which generalizes the well - known case of logistic regression , is easily seen to be concave [ 6 ] , as is the penalized likelihood ( 3 ) .To maximize the penalized log - likelihood , we compute its partial derivatives with respect to the weights : . is the empirical feature count of feature f i and E [ f i ] is the model expectation of feature f i .", "label": "", "metadata": {}}
{"text": "These include iterative scaling techniques [ 10 ] and gradient - based techniques [ 11 , 12 ] .All these methods require the calculation of the empirical and model expectations at each iteration .The empirical expectations are trivially calculated by counting in the training data the number of times each feature occurs .The model expectations are given by : .Computing this sum directly is impractical because the number of possible tag sequences is exponential on training instance length .This is not a problem for maximum entropy models [ 13 ] because they model only single - label decisions so their expectations are just over the next label , not whole label sequences .", "label": "", "metadata": {}}
{"text": "First , we define a function that maps a single state at input position j to a set of allowed next states at position j + 1 , T j ( s ) .These values are calculated by the following recurrences : .The computation given by these recurrences is commonly called the forward - backward algorithm [ 8 ] and has O ( S 2 n ) running time , where S is the number of states and n the length of the sequence .Having calculated the model expectations it is then possible to calculate the gradient of the objective function .", "label": "", "metadata": {}}
{"text": "The gradient provides a search direction and a step size \u03b7 , which can be chosen statically or be maximized dynamically by a line search .If chosen statically it must be sufficiently small to guarantee convergence .However , the smaller \u03b7 is , the slower the algorithm will converge .Furthermore , gradient ascent does not take into account the curvature of the function , which also fundamentally slows its convergence speed .In order to consider the function 's curvature , we require second order derivative information in the form of a Hessian .However , this matrix is far too large to invert ( for use in Newton 's method ) or even store in memory , and the computation of individual elements is quite expensive [ 12 ] .", "label": "", "metadata": {}}
{"text": "These methods have been shown to be quite effective [ 11 , 12 ] for training log - linear models like CRFs .We use a history with four previous update directions , and train until the change in log - likelihood is sufficiently small .We used the MALLET [ 15 ] implementation of CRFs and limited - memory quasi - Newton training .In addition , we relied on MALLET 's feature induction capability [ 16 ] , which is discussed in the next section .Relationship with maximum entropy .Conditional random fields are closely related to other conditional formalisms in the literature .", "label": "", "metadata": {}}
{"text": "Maximum entropy models give the conditional probability of a class given an observation by : .To apply maximum entropy classification to tagging , we see the problem as a sequence of probabilistic decisions in which tag t j is chosen depending on previous tags and the input sequence .In the notation of the previous section , we would have : .Such a model is trained by taking each tag occurrence and its conditioning context in the training data as a separate training instance [ 17 ] .This training process is simpler than that for CRFs as it does not require forward - backward computations .", "label": "", "metadata": {}}
{"text": "While these models are easy to train , they are independently normalized at each position j ( equation 4 ) .In contrast , conditional random fields have a single combined normalizing denominator of Z ( o ) for the entire tag sequence t ( equations 1 and 2 ) .This independent normalization prevents decisions at different positions from being weighed against each other .This label bias problem [ 6 ] motivated the development of CRFs , and has been shown to adversely affect accuracy in realistic tagging tasks [ 12 ] .It might be argued that the lower accuracy of maximum entropy taggers is compensated by their faster training , which allows more complex models to be considered , for instance models with higher Markov order or more feature types .", "label": "", "metadata": {}}
{"text": "However , any changes to a maximum entropy tagging model can be trivially applied to the corresponding CRF model , including changes in model order and feature choice .For all the tagging tasks that we have attempted , CRF models can be trained in under 20 hours ( and in most cases under 10 hours ) , which is quite practical since training is done only a few times .This seems a small cost to pay for significant accuracy increases , such as a 10 % relative reduction in error rate [ 12 ] .Applying the trained model uses exactly the same Viterbi algorithm for CRFs as for maximum - entropy classification .", "label": "", "metadata": {}}
{"text": "However , our experiments rely on feature induction ( described later ) , so the comparison would need extending feature induction to maximum entropy tagging , which is conceptually straightfoward but would require a significant implementation effort .Feature set .Feature - based models like CRFs are attractive because they reduce each problem to that of finding a feature set that adequately represents the task at hand .These predicates help the system recognize informative substrings ( e.g. ' homeo ' or ' ase ' ) in words that were not seen in training .This may seem redundant , but prefix and suffix predicates also take into account the position of the n -gram in the word , which can often be informative .", "label": "", "metadata": {}}
{"text": "We include predicates that indicate whether the current token occurs within brackets or inside quotations .This defines the complete set of orthographic predicate used by the system .The observation list for each token will include a predicate for every regular expression that token matches .Even with this very simple set of predicate - based features , performance on the development data was reasonable ( see Table 2 row A ) .In order to add expert knowledge to the model , we focused our attention on the gene and protein tagger ABGene [ 5 ] .ABGene is a hybrid model that uses a statistical part - of - speech tagger to identify candidate genes by labeling them with a special part - of - speech ' GENE ' .", "label": "", "metadata": {}}
{"text": "Specifically , ABGene uses a set of lexicons to remove false positives and recover false negatives .These include general biological terms , amino acids , restriction enzymes , cell lines , organism names and non - biological terms meant to identify tokens that have been mislabeled as ' GENE ' .To recover false negatives , ABGene utilizes large gene lexicons coupled with context lists to identify possible mentions .Another post - processing step identifies tokens that contain low frequency trigrams , compiled from MEDLINE , to identify possible gene candidates , since gene and proteins names often contain unusual character trigrams .", "label": "", "metadata": {}}
{"text": "B ) Same as A , except feature induction is used .C ) Same as B , except features using the infrequent trigram lexicon are used .D ) Same as B , except features using the non - gene lexicons are used .E ) Same as B , except features using the gene lexicon are used .F ) Same as B , except features using all lexicons are used .A straightforward method of integrating these post - processing steps into our model is to create predicates indicating whether a token occurs in one of the ABGene lexicons .", "label": "", "metadata": {}}
{"text": "Table 2 rows C through F summarizes the effect of adding these lexicons to the system .Rows C through F assume the use of feature induction , which is explored in the next section .Feature induction .So far we have only described features over a single predicate .Often it is useful to create features based on the conjunction of several predicates .For instance , the following feature would be useful : .This is because the token p-53 can either be in a gene mention ( when it is followed by the word ' mutant ' ) or be in a mutation mention ( when it is followed by the word ' mutations ' ) .", "label": "", "metadata": {}}
{"text": "The system already has tens of thousands of singleton features , making it infeasible to create all such conjunctions .Even if it were computationally feasible to train a model with all conjunctions , it would be difficult to gather sufficient statistics on them since most conjunctions occur rarely if ever .To solve this problem , McCallum [ 16 ] describes an implementation of feature induction for CRFs that automatically creates a set of useful features and feature conjunctions .The method scores candidate features f with their log - likelihood gain : . where F is the current set of model features , the log - likelihood of the data using feature set F and the log - likelihood of the data with the model extended with feature f .", "label": "", "metadata": {}}
{"text": "Only those candidates causing the highest gain are included into the current set of model features .Intuitively , features causing high gain provide strong evidence for many decisions .Thus , feature induction tends to discard infrequent features or non - discriminating features since , independently , their overall effect on the likelihood of the entire training set is usually marginal .There are serious computation issues with feature induction , primarily due to the fact that for each iteration and for each feature considered , the normalization term for every training sequence must be recomputed .However , the problem can be somewhat alleviated by making certain independence assumptions on the parameters as well as only including statistics on positions in the sequence that are mislabeled by the current parameter settings .", "label": "", "metadata": {}}
{"text": "Table 2 rows A and B show the difference in performance using feature induction instead of just the predefined singleton features .Results .Our system was initially trained on 7500 annotated MEDLINE sentences with a development set of 2500 sentences .Training with feature induction took approximately 15 hours , which is substantially longer than training without feature induction .Once trained , the system can annotate sentences in less than a second .In terms of labor , the system took only a few days to build .This was primarily due to the availability of MALLET [ 15 ] , which includes effcient implementations of both conditional random fields and feature induction .", "label": "", "metadata": {}}
{"text": "For evaluation , we added the development set to the training data and evaluated on 5000 new unannotated sentences .The results are shown in Table 3 .Entities were correctly identified by the system if and only if all and only the tokens of the entity were correctly detected .Table 3 .Precision and recall numbers for the system on the unseen evaluation data .Precision and recall numbers for the system on the unseen evaluation data .Precision is measured by the fraction of predicted gene mentions that are correct and recall by the fraction of actual gene mentions that were identified .", "label": "", "metadata": {}}
{"text": "The first is for the system that contains only features extracted from the training data .These results are presented in the row No Lexicons .The second set of results are for the system that also contains features extracted from external lexicons .These results are presented in the row Lexicons .Discussion .Adding the ABGene lexicons made a significant improvement to both precision and recall .This is a very good indicator that additional domain knowledge may help to further improve the accuracy of the system .To determine which lexicons gave the best performance , we conducted experiments examining the effect of adding each type of lexicon individually to the model and tested the model on the development data .", "label": "", "metadata": {}}
{"text": "Each list made a small improvement to the overall accuracy of the system , with the gene lexicon contributing the largest improvement .Table 2 also shows the performance of the system without lexicons and feature induction .An examination of system errors on the development data shows that a primary source of error came from properly labeled mentions that are off by one or more tokens .If the score is relaxed so that tagged entities are considered true positives if and only if one or more tokens overlap with a correct entry , then performance on the development data increases 7.5 % absolute from 79.9 % to 87.4 % F1 measure .", "label": "", "metadata": {}}
{"text": "The gold standard identifies 4 different entities within this string , interleukin-1 , IL-1 , tumor necrosis factor - alpha and TNF - alpha .The net result is that recall is penalized four times and precision is penalized once due to the four false negatives and one false positive .It appears that it is relatively easy to find pieces of text mentioning genes , but much harder to determine the exact boundaries of that mention .This hurts the system performance significantly since the scoring metric requires exact matches .However , entity tagging primarily exists to give some structure to text for higher level information extraction systems such as relation detection , fact generation and question answering .", "label": "", "metadata": {}}
{"text": "Conclusion .Overall , our experiments show that CRF models with carefully designed features can identify gene and protein mentions with fairly high accuracy even without features containing domain specific knowledge .However , such features , which in our case take the form of lexicon membership , can lead to improved system performance .Declarations .Acknowledgements .The authors would like to thank our collaborators Mark Liberman , Andy Schein , Pete White and Scott Winters for useful discussions and suggestions .We would also like to thank Lorraine Tanabe for making the ABGene lexicons available to us .", "label": "", "metadata": {}}
{"text": "This work was supported in part by NSF grant ITR 0205448 .Authors ' Affiliations .Department of Computer and Information Science , University of Pennsylvania , Levine Hall .References .Ohta T , Tateisi Y , Kim J , Lee S , Tsujii J : GENIA corpus : A semantically annotated corpus in molecular biology domain .Proceedings of the ninth International Conference on Intelligent Systems for Molecular Biology 2001 .Kulick S , Bies A , Liberman M , Mandel M , McDonald R , Palmer M , Pancoast E , Schein A , Ungar L , White P , Winters S : Integrated annotation for biomedical information extraction .", "label": "", "metadata": {}}
{"text": "Narayanaswamy M , Ravikumar KE , Vijay - Shanker K : A Biological Named Entity Recognizer .Proceedings of Pacific Symposium on Biocomputing 2003 .Kazama J , Makino T , Ohta Y , Tsujii J : Tuning Support Vector Machines for Biomedical Named Entity Recognition .Proceedings of Natural Language Processing in the Biomedical Domain , ACL 2002 .Tanabe L , Wilbur WJ : Tagging gene and protein names in biomedical text .Bioinformatics 2002 . , 18(8 ) : .Lafferty J , McCallum A , Pereira F : Conditional random fields : Probabilistic models for segmenting and labeling sequence data .", "label": "", "metadata": {}}
{"text": "McCallum A : Effciently inducing features of conditional random fields .Proceedings of Conference on Uncertainty in Artificial Intelligence 2003 .McCallum A , Freitag D , Pereira F : Maximum entropy Markov models for information extraction and segmentation .Proceedings of ICML 2000 .Kudo T , Matsumoto Y : Chunking with Support Vector Machines .Proc NAACL 2001 ACL 2001 .Toutanova K , Klein D , Manning CD , Singer Y : Feature - Rich Part - of - Speech Tagging with a Cyclic Dependency Network .Proceedings of Human Language Technology and North American Chapter of the Association for Computatitonal Linguists 2003 .", "label": "", "metadata": {}}
{"text": "\u00a9 McDonald and Pereira 2005 .This article is published under license to BioMed Central Ltd.ORIENTATION :The concepts of the Protean Career and the Boundaryless Career show potential as frameworks for research and practice in the contemporary world of work .Briscoe , Hall and DeMuth ( 2006 ) developed the Protean and Boundaryless Career Attitude Scales , which consist of the Self - Directed Career Management , Values Driven , Boundaryless Mindset and Mobility Preference subscales .However , the standardisation and replication studies conducted by Briscoe et al . , left some questions unanswered in terms of the psychometric properties of the subscales .", "label": "", "metadata": {}}
{"text": "RESEARCH DESIGN , APPROACH AND METHOD : Responses of adults to the items of the Protean and Boundaryless Career Attitude Scales were analysed with factor analytic and Rasch item response model techniques .MAIN FINDINGS :Factor and Rasch analyses revealed that three of the four postulated dimensions were replicated , but the Values Driven dimension split into two factors .Misfitting items were identified and sources of their misfit were uncovered .The Rasch analysis showed that three of the four subscales provide most of their psychometric information at the lower ends of their respective latent traits ( where relatively few persons are located ) .", "label": "", "metadata": {}}
{"text": "PRACTICAL / MANAGERIAL IMPLICATIONS : Overall , the quality of the Protean and Boundaryless Career Attitude Scales is satisfactory , but some aspects that may be improved are identified .Researchers may use at least three of the four subscales with confidence , but more work is possibly needed on the Values Driven subscale .CONTRIBUTION / VALUE - ADD : The study provides researchers with information on the psychometric properties of the Protean and Boundaryless Career Attitude Scales .The study also highlights ways in which the scales may be improved .Keywords : contemporary careers ; Rasch analysis ; South Africa ; dimensionality ; latent trait analysis .", "label": "", "metadata": {}}
{"text": "The Protean Attitude Scale consists of the Self - Directed Career Management ( SDCM ) and Values Driven ( VD ) subscales , whereas the Boundaryless Attitude Scale consists of the Mobility Preference ( MP ) and Boundaryless Mindset ( BM ) subscales .Standardisation and replication studies conducted by Briscoe et al .( 2006 ) left unanswered some questions about the dimensionality of the subscales , the quality of the individual items and the measurement precision of the subscales across their respective latent trait continuums .It is hoped that by finding answers to these questions , this study will contribute towards the validation of the Protean and Boundaryless Career Attitude Scales and stimulate further research on the protean and boundaryless career concepts .", "label": "", "metadata": {}}
{"text": "Briscoe , Hall and DeMuth ( 2006 ) introduced the Protean and Boundaryless Career Attitude Scales to operationalise the concept of protean ( Hall , 1976 ; 2002 ) and boundaryless ( Arthur , 1994 ) career .The protean career refers to a career driven and developed by the individual and not by the organisation .On the other hand , the boundaryless career transcends traditional psychological and physical boundaries associated with the career concept ( Baruch , 2006 ) .The protean and boundaryless career concepts are useful for the description and understanding of careers in a working / organisational environment that is becoming increasingly fluid and unpredictable ( Baruch , 2006 ) .", "label": "", "metadata": {}}
{"text": "Briscoe et al .( 2006 ) expressed the hope that the new scales will serve to stimulate empirical research in this area .Much of the success of such research efforts will rely on the accuracy and precision with which the scales operate .Moreover , the general usefulness of the protean and boundaryless career concepts may be tied to the transportability of the constructs and the corresponding scales beyond the borders of the United States of America .The remainder of the introduction examines the contemporary world of work and the challenges that it poses to individual career development .", "label": "", "metadata": {}}
{"text": "Thirdly , Briscoe et al . 's ( 2006 ) description of the development and initial validation of the four new scales are discussed .Finally , the explicit research goals of the study are presented .The contemporary world of work .Traditionally , individual careers in organisations were seen as linear , predictable and secure ; an individual entered an organisation and strove to rise through the ranks in an attempt to reach higher positions with clearly defined boundaries .Individuals who performed satisfactorily could be expected to be promoted .An employee 's loyalty was rewarded by an organisation in the form of reciprocated loyalty .", "label": "", "metadata": {}}
{"text": "Whereas the psychological contract between individual and organisation used to be long - term and relational , it has become short - term and transactional .The new psychological contract requires individuals to engage in continuous learning and to modify their work - related self - perceptions and identities ( Baruch & Hall , 2004 ; Granrose & Baccili , 2006 ) .In the contemporary world of work , organisations can expect employees to be loyal only as long as the employee 's short - term expectations are met .In turn , individuals can expect organisations to be loyal only as long as their skills and performance fulfil the organisation 's current needs ( Hall & Mirvis , 1996 ) .", "label": "", "metadata": {}}
{"text": "During the 20th century , employees might have expected the organisation to plan and control their careers , but in an era of increased uncertainty they need to take greater responsibility for their own career development .Hansen ( 1997 ) emphasised that because individuals . can no longer rely on their work for security and stability , [ they ] will become self - directed persons who develop their own careers , gain respect for others and value difference .They will learn to expect change .( Hansen , 1997 , p. 247 ) .Employees will need to become increasingly adaptable and multiskilled , which implies that continuous professional development and learning how to learn will take on greater importance in individuals ' careers .", "label": "", "metadata": {}}
{"text": "The protean career attitude .Hall ( 1976 ; 2004 ) defined the protean career as a career where the individual , rather than the organisation , is in charge .Protean individuals value individual freedom and growth and define career success in terms of psychological criteria , such as the degree of job satisfaction , self - actualisation , personal accomplishment and a feeling of fulfillment ( Hall & Chandler , 2005 ; Hall & Mirvis , 1996 ) .This description contrasts with a more traditional view where career success is defined in terms of external criteria such as promotions , salary and occupational status .", "label": "", "metadata": {}}
{"text": "The first component , namely Self - Directed Career Management is tied to the meta - competency of adaptability and is seen in individuals ' ability to adapt to changing conditions and to take responsibility for their own career development .The second component , namely Values Driven , is tied to the meta - competency of self - awareness and is seen when individuals ' personal values guide their careers and become the yardstick against which they measure their career success ( Hall & Chandler , 2005 ) .The boundaryless career attitude .The boundaryless career refers to the .", "label": "", "metadata": {}}
{"text": "( Briscoe et al . , 2006 , p. 2 ) .Arthur , Khapova and Wilderom ( 2005 ) emphasised that the boundaryless career concept refers to mobility across physical and psychological career boundaries .Sullivan and Arthur ( 2006 ) describe physical mobility as the actual movement across boundaries between organisations , jobs and departments , whereas psychological mobility refers to the perceived capacity to make such career transitions and changes .Hence , individuals who measure high in boundarylesness may be expected to initiate and pursue work - related relationships across physical organisational boundaries , whereas individuals who measure low in boundarylesness may prefer to pursue their careers within traditional organisational boundaries .", "label": "", "metadata": {}}
{"text": "The development and validation of the self - directed career management , values driven , boundaryless mindset and mobility preference subscales .Briscoe et al .( 2006 ) collected and analysed data in three waves in terms of constructing and validating the SDCM , VD , BM and MP subscales .The data of the first wave , the standardisation study , were used for the construction of the four subscales .The data of the second wave , the replication study , were used to confirm the factor structure of the subscales and to check the reliabilities .", "label": "", "metadata": {}}
{"text": "In the paragraphs that follow , the results of the standardisation and replication studies are examined more closely .The standardisation study .Briscoe et al .( 2006 ) subjected responses to items written to reflect the SDCM and VD concepts to a principal components analysis and items written to reflect the BM and MP concepts to a separate principal components analysis .In the analysis of the SDCM and VD items , two components were retained ( on the basis of the eigenvalues - greater - than - one criterion and the scree - test ) and rotated eight SDCM items and seven VD items .", "label": "", "metadata": {}}
{"text": "The reliabilities ( Cronbach 's coefficient alpha ) of the SDCM and VD subscales for the total standardisation sample were 0.81 and 0.69 , respectively .The relatively low reliability of the VD scale suggests that the common factor underlying the items is not very strong .Briscoe et al .( 2006 ) also retained two components in their analysis of responses to the BM and MP items .They rotated eight BM items and five MP items , which constitute the final BM and MP subscales .The two components corresponded strongly with the corresponding BM and MP subscales .", "label": "", "metadata": {}}
{"text": "The replication study .The data of the replication sample were subjected to a confirmatory factor analysis ( which is based on the common factor model ) , where four factors corresponding with the SDCM , VD , BM and MP subscales were specified .Each item was specified to load its corresponding factor only .Briscoe et al .( 2006 ) reported that model fitted the data , that all the items had statistically significant factor pattern coefficients and that the correlations between all the factors were statistically significant .The reliabilities of the four subscales in the total replication sample were as follows : .", "label": "", "metadata": {}}
{"text": "Three aspects of the exploratory and confirmatory factor analyses reported by Briscoe et al .( 2006 ) raise questions about the dimensionality of the SDCM , VD , MP and BM subscales .Firstly , Briscoe et al . chose the principal components model rather than the common factor model for their exploratory analyses .Secondly , the items of the four subscales were not jointly analysed in an exploratory analysis .Thirdly , the fit of the confirmatory factor analysis reported by Briscoe et al . was unsatisfactory .On a conceptual level principal components analysis is appropriate if the aim is the reduction of a large number of variables into a smaller number of summary variables .", "label": "", "metadata": {}}
{"text": "The principal components model analyses the total variance of a variable and therefore does not distinguish between error variance and common variance .When items are analysed , it may lead to inflated loadings of the items and to a distorted picture of the strength of the relations between items and the components ( Gorsuch , 1997 ) .In contrast , the common factor model separates error variance from the common variance and analyses only the common variance of the variables ( Cudeck , 2000 ) .Given that responses to individual items are generally unreliable and contain much error variance , the common factor model may give a better representation of the sources of common variance underlying the items of the four subscales ( Gorsuch , 1997 ) .", "label": "", "metadata": {}}
{"text": "( 2006 ) standardisation sample are potentially problematic .A common factor analysis might reveal these items to have even weaker loadings .These methods , together with a parallel analysis ( cf .O ' Connor , 2000 ) , possibly constitute more sophisticated methods than the eigenvalues - greater - than - one criterion and the scree - test and may detect sources of common variance among the items that Briscoe et al .( 2006 ) were unable to detect .Briscoe et al .( 2006 ) chose to analyse the two protean attitude scales and the two boundaryless attitude subscales in two separate principal components analyses .", "label": "", "metadata": {}}
{"text": "However , a joint analysis of all 27 items might have uncovered additional sources of common variance across the two sets of scales and/or might have detected items that have loadings on unintended factors .Failure to detect such additional - and often unwanted - sources of common variance and/or problem items , can lead to factors that have different meanings than originally intended and these ' errors ' may lead to distorted or unsatisfactory results in subsequent analyses .Briscoe et al .did subject the responses of a new sample to the items of all four subscales to a confirmatory factor analysis and reported the following fit indices : .", "label": "", "metadata": {}}
{"text": "'s model does not appear to fit the data very well .The results of an unrestricted common factor analysis of all the items together may provide important clues in regard to the sources of the misfit ( McDonald , 2005 ) .A further aspect that remains unanswered is the measurement precision of the four subscales across their respective latent trait continuums .Briscoe et al .( 2006 ) reported classical test theory reliability coefficients for each of the four subscales , but these coefficients give a single estimate of a scale 's measurement precision for all persons and therefore assume that each individual 's standing on the latent trait is estimated with equal precision .", "label": "", "metadata": {}}
{"text": "Item response theory acknowledges that a scale does not measure with equal precision across the entire range of the underlying trait .This is made explicit by the test information function curve , which indicates the amount of information that a test or scale provides at different points on the latent trait continuum ( Wilson , 2005 ) .Test information curves may provide important clues as to how well the SDCM , VD , MP and BM subscales function and how they may be improved .Aims of the study .Jointly , these aims are related to the construct validity and measurement precision of the protean and boundaryless career concepts and the scales that operationalise them .", "label": "", "metadata": {}}
{"text": "RESEARCH DESIGN .Research approach .This study may be characterised as a psychometric study where the focus falls on the internal psychometric properties of psychological scales .Research method .Research participants .Research participants included 427 adult workers ( 301 men and 126 women ) in a large multi - national petro - chemical organisation .Ten participants did not specify their race .One person did not indicate their management level .Proportionally , female participants and Black participants were overrepresented in the non - management level and underrepresented in the middle and senior management levels .", "label": "", "metadata": {}}
{"text": "Measuring instruments .Protean career attitude : The protean career attitude is operationalised by two subscales : Self - Directed Career Management ( SDCM ) and Values Driven ( VD ) .The VD scale consists of six Likert - type items that employs the same five - point response scale .For the SDCM , Briscoe et al .( 2006 ) reported reliability coefficients ( as estimated by Cronbach 's coefficient alpha ) for six different samples .The first three samples constituted the standardisation group , whereas the fourth , fifth and sixth samples constituted a replication group : .", "label": "", "metadata": {}}
{"text": "The corresponding reliability coefficients for the VD scale were : .These reliabilities are lower than those of the SDCM , but may be regarded as marginally adequate for research purposes .Boundaryless career attitude : The boundaryless career attitude is operationalised by two subscales : Boundaryless Mindset ( BM ) and Mobility Preference ( MP ) .The BM and MP consist of eight and five Likert - type items , respectively .Participants respond on the same five - point scale with ordered categories that is used for the SDCM and VD subscales , but all the items of the MP scale need to be reverse scored .", "label": "", "metadata": {}}
{"text": "( 2006 ) reported the following reliability coefficients ( as estimated by Cronbach 's coefficient alpha ) for six different samples : .Overall , these reliabilities appear satisfactory for research purposes .The corresponding reliability coefficients for the MP scale were : .There appears to be much variability in the reliability coefficients of the MP scale , suggesting that the measurement precision of the scale is sensitive to contextual differences .Research procedure .The Protean and Boundaryless Career Attitude Scales formed part of a larger battery of psychometric instruments , which were administered and interpreted as part of a strategic leadership development project in a large multi - national petro - chemical organisation .", "label": "", "metadata": {}}
{"text": "Participants received feedback from professional psychologists and all the data were treated confidentially .Statistical analysis .Confirmatory factor analysis : As an initial step , we examined whether each of the four subscales were unidimensional by fitting a single - factor Spearman model to each of the four item sets ( cf .Anderson & Gerbing , 1982 ; McDonald , 1999 ) .The fit of the Spearman models was judged by means of the RMSEA and the Root Mean Square Standardised Residual ( RMR ) .Each model was identified by constraining the factor variance to unity and all error variances were specified to be uncorrelated .", "label": "", "metadata": {}}
{"text": "The measurement model was equivalent to the model tested by Briscoe et al .( 2006 ) and specified that items 1 - 8 define the SDCM factor , items 9 - 14 the VD factor , items 15 - 22 the BM factor and items 23 - 27 the MP factor .All error variances were specified to be uncorrelated and the model was identified by fixing the factor variances to unity .In accordance with Briscoe et al . , the fit of the model to the data was evaluated by means of the likelihood chi - square test , the RMSEA , the Comparative Fit Index ( CFI ) , the Normed Fit Index ( NFI ) and the Incremental Fit Index ( IFI ) .", "label": "", "metadata": {}}
{"text": "Although unrestricted , the analysis was conducted in a confirmatory spirit with the aim of identifying sources of misfit in the restricted confirmatory factor analysis ( McDonald , 2005 ) .The scree - plot and the results of a parallel analysis , which are eigenvalues based techniques , were examined to decide the number of factors to retain .In addition , the RMR , the RMSEA and the ECVI , which are residual based techniques , were examined for four- , five- and six - factor solutions .The factors were obliquely rotated to an independent clusters solution ( cf .", "label": "", "metadata": {}}
{"text": "The analyses were done with the Comprehensive Exploratory Factor Analysis computer software ( Browne , Cudeck , Tataneni & Mels , 2004 ) .The Rasch model represents an ideal for unidimensional measurement against which existing and new scales can be judged ( Andrich , 1988 ) .The model was used to identify items that do not conform to the requirement of unidimensional measurement and to examine the measurement precision of the Protean and Boundaryless Career Attitude scales .The partial credit model estimates the location of each item and each person on the same latent trait continuum .", "label": "", "metadata": {}}
{"text": "Similarly , any individual is expected to obtain lower scores on items with higher locations on the latent trait than on items with lower locations ( Bond & Fox , 2001 ) .The item and person parameters were estimated with the Rumm 2020 computer programme ( Andrich , Sheridan , & Luo , 2007 ) .The estimated parameters are used to calculate expected item scores for each person .The expected item scores can then be compared with the observed scores .The programme employs several fit statistics , of which the Pearson chi - square and standardised residual statistics are reported here .", "label": "", "metadata": {}}
{"text": "The overall fit of the items is summarised in a total item chi - square .Unlike classical test theory methods , which assumes that the precision with which a test measures is constant across the underlying trait continuum , Rasch models acknowledge that tests or scales operate differently for individuals with different trait levels ( Wilson , 2005 ) .The test information curve may be used to identify the areas of the latent trait in which the test or scale operates most efficiently and in which areas it operates less efficiently .Rasch analyses produce a summary index of measurement precision , namely the Person Separation Reliability Index ( Wright & Masters , 1982 ) , which is similar in interpretation to Cronbach 's coefficient alpha .", "label": "", "metadata": {}}
{"text": "The MSE is subtracted from the total variance of the person measures to obtain an estimate of the true variance .The ratio of the true variance to the total variance gives the Person Separation Reliability Index ( Wright & Masters , 1982 ) .RESULTS .Confirmatory factor analysis .Next , responses to the entire set of 27 items were subjected to a maximum likelihood confirmatory factor analysis .Similarly , the CFI ( 0.860 ) , NFI ( 0.800 ) , IFI ( 0.861 ) and RMR ( 0.071 ) suggested less than satisfactory fit from a practical perspective .", "label": "", "metadata": {}}
{"text": "It is noteworthy that five of the six problematic items were from the VD and BM subscales which demonstrated the poorest fit with the Spearman model when analysed separately .Unrestricted factor analysis .The data were subjected to an unrestricted maximum likelihood factor analysis to identify possible sources of misfit in the confirmatory factor analysis model ( cf .Hoyle , 2000 ; McDonald , 2005 ) .The scree - plot and parallel analysis suggested five factors .Table 1 contains values of the RMSEA , ECVI and RMR for four- , five- and six - factor solutions .", "label": "", "metadata": {}}
{"text": "The Direct Quartimin rotated five - factor solution 1 demonstrates that the first three factors represent the BM , MP and SDCM subscales , respectively ( see Table 2 ) .However , the fourth and fifth factors represent a split of the VD scale and are labeled VD1 and VD2 , respectively .Items 9 , 10 , 11 and 13 defined factor 4 , whereas items 12 , 13 and 14 defined factor 5 .The correlations between the factors were low and ranged from -0.020 ( SDCM and MP ) to 0.271 ( SDCM and VD2 ) ( see Table 3 ) .", "label": "", "metadata": {}}
{"text": "The overall fit and person separation reliability indices are summarised in Table 4 .The corresponding Cronbach alpha coefficient was 0.654 .Low reliability reduces the power to detect misfit ( Andrich , 1988 ) , which possibly accounts for the observed good fit of the VD scale .The individual item fit statistics highlighted five poorly fitting items : items 1 and 8 ( SDCM scale ) , items 15 and 21 ( BM scale ) and item 23 ( MP scale ) .These items discriminated weaker than predicted by the Rasch model .Removing these items from the analyses resulted in improved fit and person separation reliability for the SDCM , BM and MP subscales .", "label": "", "metadata": {}}
{"text": "Table 5 also shows that the PSRI of the three scales were satisfactory and ranged from 0.792 ( SDCM ) to 0.882 ( MP ) .These values corresponded very closely with the values of Cronbach 's coefficient alpha .This component accounted for approximately 13 % of the variance in the Rasch residuals , when in theory there should be no discernable structure in the Rasch residuals if the data fits the model ( Smith , 2002 ) .Figures 1 - 4 demonstrate that the SDCM and BM subscales ( and to a lesser extent the VD scale ) provided most of their psychometric information at the lower ends of the person distributions where relatively few persons were located .", "label": "", "metadata": {}}
{"text": "In contrast , the test information curve of the MP scale and the comparison of the person distributions and category threshold distributions , demonstrates that the locations of the items match the central location of the persons ( see Figure 3 ) .This scale operates most effectively in the middle range of the person distribution , but operates less efficiently at the extreme upper and lower ends of the latent trait .DISCUSSION .Briscoe et al .( 2006 ) recently introduced the Protean and Boundaryless Career Attitude scales to operationalise the protean and boundaryless career attitude constructs .", "label": "", "metadata": {}}
{"text": "The present study aimed to shed light on three unanswered questions in terms of the four subscales ( Briscoe et al .The first relates to the dimensionality of the four subscales , the second relates to the quality of the individual items and the third relates to the measurement precision across the latent trait continuum of each subscale .The most important findings may be summarised as follows : .( 2006 ) study .In the paragraphs that follow the theoretical and practical implications of these findings are discussed in more detail .The dimensionality of the self - directed career management , values driven , boundaryless mindset and mobility preference subscales .", "label": "", "metadata": {}}
{"text": "Unrestricted maximum likelihood factor analysis showed that five factors were necessary to provide a satisfactory account of the covariances of the 27 items .The five - factor model provides support for the construct validity of the SDCM , BM and MP subscales .Each of these factors corresponds strongly with Briscoe et al . 's ( 2006 ) scoring key and , with a few minor exceptions , the items of each scale measure an essentially unidimensional trait .It appears safe to conclude that these three factors largely accord with their corresponding theoretical constructs .However , the six items of the VD scale appear to measure two psychologically distinct latent traits .", "label": "", "metadata": {}}
{"text": "From this perspective , being values driven implies that individuals have strong core beliefs that they use as normative standards to guide their behavior .Factor VD2 appears to reflect the agentic or motivational aspect of being values driven .Items 9 , 10 , 11 and 13 focus on the pursuit of the individual 's own career needs and goals rather than the needs and goals of the organisation , or of other people .High scores reflect personal autonomy , individual agency and perhaps a clear sense of identity .Individuals with high scores define career success on their own terms rather than those of the organisation or other external parties and pursue their own career related needs .", "label": "", "metadata": {}}
{"text": "For instance , individuals may obtain an average score on the VD scale firstly because they are high on VD1 and low on VD2 , secondly because they are low on VD1 and high on VD2 , or thirdly because they are average on both VD1 and VD2 .These ambiguities can be resolved by developing separate VD1 and VD2 scales , or by improving the homogeneity of the VD items so that they more closely constitute a unidimensional scale .A disadvantage of the five - factor model is that it diverges slightly from the underlying theoretical model , which specifies four factors and could be seen to work against the accumulation of research results .", "label": "", "metadata": {}}
{"text": "A cross - cultural perspective on the dimensionality of the values driven subscale .It is not clear whether the split of the VD scale into two factors will be observed in other countries or cultures .Briscoe and Hall ( 2006 ) and Briscoe et al .( 2006 ) paint a picture of high VD scorers as persons who strive towards independence , autonomy and individual agency .They may be seen as persons who have a clear sense of identity and who succeed in implementing their self - concepts in their career related choices and decisions .", "label": "", "metadata": {}}
{"text": "However , in many other societies , greater emphasis is placed on community than on individuality and autonomy .African societies may be said to endorse a collectivistic or an interdependence worldview .This does not mean that these individuals do not have personal aspirations , dreams , interests and values .It probably does mean , however , that these aspirations , dreams and interests need to be balanced with and possibly be placed in a position of lesser importance than the aspirations , dreams and interests of the community .Against this background it appears likely that items 9 , 10 and 11 of the VD scale , which focus on individuals putting their needs above that of the organisation and other people , can differ in psychological meanings for individuals from individualistic / independence and collectivistic / interdependence cultures .", "label": "", "metadata": {}}
{"text": "In contrast , a highly values driven individual from a collectivistic / interdependence culture may find it difficult to agree with item 11 because of a cultural norm that dictates that the opinions of other people are important and need to be respected .It is possible that such differences in interpretation may have contributed toward the observed multidimensionality of the VD scale .The quality of the individual items .Overall , the results of the factor and Rasch analyses demonstrate that the majority of items function satisfactorily .However , items 1 , 8 , 15 , 21 and 23 elicited unexpected responses conditional on a person 's standing on the latent trait .", "label": "", "metadata": {}}
{"text": "( 2006 ) standardisation study , items 1 and 8 had the weakest and third weakest loadings on the SDCM component , item 23 had the second weakest loading on the MP component and items 15 and 21 had the weakest and third weakest loadings on the BM component .Hence , relative to the entire set of 27 items , items 1 , 8 , 15 , 21 and 23 also were weak indicators of their respective constructs in the Briscoe et al .( 2006 ) standardisation study 2 .Items 1 , 8 ( SDCM scale ) and item 21 ( BM scale ) concern career related attitudes in an individual 's past , whereas the majority of the items in the entire set concern an individual 's present career related attitudes .", "label": "", "metadata": {}}
{"text": "( 2006 ) , the SDCM and BM subscales measure career related attitudes rather than traits .Attitudes may change over time .Hence , it is possible , for instance , to find an individual who previously might have held a traditional attitude in regard to his or her career development to hold a protean and/or boundaryless attitude in the present .This may explain why items 1 , 8 and 21 were identified by the Rasch model as eliciting unexpected responses .It appears undesirable to mix in the same scale items that enquire about present and past attitudes and behaviors .", "label": "", "metadata": {}}
{"text": "Item 15 is the only one of the BM set that does not explicitly refer to experiences or contacts across departmental / organisational boundaries .Individuals with low standings on the trait may endorse this item just because they like learning new things , even though they have no desire to implement what they learn outside the departmental / organisational boundaries of their current job .From this perspective some individuals may have low standings on the BM latent trait and yet obtain high scores on item 15 .There is no plausible explanation for the misfit of item 23 .", "label": "", "metadata": {}}
{"text": "Hence , the fit of item 23 appears to be satisfactory from a practical measurement perspective .The measurement precision of the subscales across the latent trait continuum .Three of the four subscales , namely SDCM , VD and BM , provide most of their psychometric information at the lower end of the person distributions .Hence , relatively precise measures of individuals with low standings on the three latent traits are obtained , but relatively imprecise measures of individuals with high standings are obtained ( Wright & Stone , 1979 ) .Figures 1 , 2 and 4 show that the items are poorly targeted , with the majority of persons located higher on the latent trait continuum than the item category thresholds .", "label": "", "metadata": {}}
{"text": "These subscales , however , may be more useful as general measures that can be used with individuals across the entire trait ranges of the protean and boundaryless career concepts .Moreover , the relations of these three subscales with other variables may be attenuated in groups with a large proportion of high scoring individuals .Another undesirable consequence is that the subscales may fail to detect inter - individual differences or intra - individual changes over time among individuals with high scores .This may restrict the general utility of the scales and/or cast unjustified doubt on the construct validity of the scales .", "label": "", "metadata": {}}
{"text": "This should increase the ranges over which the scales provide precise person measures ( Wright & Stone , 1979 ) .Conclusion .Limitations and recommendations .The principal limitation of the study is that the factor and Rasch analyses are not replicated on an independent sample , which may raise questions about the generality of the results .In particular , readers may question whether items 1 , 8 , 15 , 21 and 23 also function poorly in other samples and whether the split of the Values Driven factor can be observed in other samples .The items found to function poorly in this study can also be seen to function relatively poorly in the Briscoe et al .", "label": "", "metadata": {}}
{"text": "Moreover , semantical and conceptual analyses provide a plausible substantive explanation for the poor fit of the four weakest items .Also , semantical and conceptual analyses provide a plausible explanation for the split of the VD factor and its low reliability .Although not immediately explicit , close examination of Briscoe et al . 's exploratory factor analysis of the VD and SDCM items also shows that the items of the VD scale do not constitute a homogenous unidimensional scale .However , only subsequent studies will confirm the replicability of these findings .A second limitation is that the study focuses only on the internal psychometric properties of the scales .", "label": "", "metadata": {}}
{"text": "The construction of the Protean and Boundaryless Career Attitude Scales constitute a potentially important advancement , which may stimulate empirical research and theory development on adult career development .The present study highlighted some weaknesses in the subscales which might inhibit such efforts .The present study also revealed the sources of the weaknesses and these can and should be corrected .In this regard the following recommendations are made : .Improve the homogeneity of the items that constitute the Values Driven subscale , or , if researchers deem it important to include both the normative and agentic aspects of being values driven in the Protean Attitudes Scale , it will be necessary to separately develop these subscales .", "label": "", "metadata": {}}
{"text": "Replace items that enquire about individuals ' past career behaviours and attitudes with items that focus on current career behaviors and attitudes .Implications for future research .Future research may also focus more explicitly on the cross - cultural equivalence of the Protean and Boundaryless Career Attitude Scales across cultures by investigating aspects such construct equivalence and differential item functioning .These efforts are bound to lead to a deeper understanding of the constructs and shed light on the conditions and contexts where the protean and boundaryless career concepts are most useful .ACKNOWLEDGEMENTS .I am grateful toward the following people who commented on a draft version of this manuscript : Johann Schepers , Freddie Crous , Koos Uys , Gert Roodt , Marcel Harper and Karina de Bruin .", "label": "", "metadata": {}}
{"text": "Anderson , J.C. , & Gerbing , D.W. ( 1982 ) .Some methods for respecifying measurement models to obtain unidimensional construct measurement .Journal of Marketing Research , 19 , 453 - 460 .[Links ] .Andrich , D. ( 1988 ) .Rasch models for measurement .Thousand Oaks : Sage .[Links ] .Andrich , D. , de Jong , J.H.A.L. , & Sheridan , B.E. ( 1997 ) .Diagnostic opportunities with the Rasch model for ordered response categories .In J. Rost & R. Langeheine ( Eds . ) , Applications of latent trait and latent class models in the social sciences ( pp .", "label": "", "metadata": {}}
{"text": "New York : Waxmann .[Links ] .Andrich , D. , Sheridan , B. , & Luo , G. ( 2007 ) .Rumm 2020 : Rasch Unidimensional Measurement Models , Version 4.1 [ Computer programme].Perth : Rumm Laboratory .[Links ] .Arthur , M.B. ( 1994 ) .The boundaryless career : A new perspective for organizational inquiry .Journal of Organizational Behavior , 15 , 295 - 306 .[Links ] .Arthur , M.B. , Khapova , S.N. , & Wilderom , C.P.M. ( 2005 ) .Career success in a boundaryless career world .", "label": "", "metadata": {}}
{"text": "[Links ] .Baruch , Y. ( 2006 ) .Career development in organizations and beyond : Balancing traditional and contemporary viewpoints .Human Resource Management Review , 16 , 125 - 138 .[Links ] .Baruch , Y. , & Hall , D.T. ( 2004 ) .The academic career : A model for future careers in other sectors ?Journal of Vocational Behavior , 64 , 241 - 262 .[Links ] .Bond , T.G. , & Fox , C.M. ( 2001 ) .Applying the Rasch model : Fundamental measurement in the human sciences .", "label": "", "metadata": {}}
{"text": "[Links ] .Briscoe , J.P. , & Hall , D.T. ( 2006 ) .The interplay of boundaryless and protean careers : Combinations and implications .Journal of Vocational Behavior , 4 - 18 .[Links ] .Briscoe , J.P. , Hall , D.T. , & DeMuth , R.L.F. ( 2006 ) .Protean and boundaryless careers : An empirical investigation .Journal of Vocational Behavior , 69 , 30 - 47 .[Links ] .Browne , M.W. , Cudeck , R. , Tateneni , K. , & Mels , G. ( 2004 ) .", "label": "", "metadata": {}}
{"text": "Cook , E.P. , Heppner , M.J. , & O'Brien , K.M. ( 2002 ) .Career development of women of color and white women : Assumptions , conceptualization , and interventions form an ecological perspective .Career Development Quarterly , 50 , 291 - 305 .[Links ] .Cross , S.E. , & Markus , H.R. ( 1999 ) .The cultural constitution of personality .In L.A. Pervin & O.P. John ( Eds . ) , Handbook of personality : Theory and research ( 2nd ed . , pp .378 - 396 ) .New York : Guilford .", "label": "", "metadata": {}}
{"text": "Links ] .Cudeck , R. ( 2000 ) .Exploratory factor analysis .In H.E.A. Tinsley & S.D. Brown ( Eds . ) , Handbook of applied multivariate statistics and mathematical modeling ( pp .265 - 296 ) .San Diego : Academic Press .[Links ] .De Bruin , G.P. , & De Bruin , K. ( 2006 ) .Career assessment .In G.B. Stead & M.B. Watson ( Eds . ) , Career psychology in the South African context ( 2nd edn . , pp .129 - 136 ) .Pretoria : Van Schaik .", "label": "", "metadata": {}}
{"text": "Links ] .Embretson , S.E. , & Reise , S.P. ( 2000 ) .Item response theory for psychologists .Mahwah : Erlbaum .[Links ] .Gorsuch , R.L. ( 1997 ) .Exploratory factor analysis : Its role in item analysis .Journal of Personality Assessment , 68 , 532 - 560 .[Links ] .Granrose , C.S. , & Baccili , P.A. ( 2006 ) .Do psychological contracts include boundaryless or protean careers ?Career Development International , 11 , 163 - 182 .[Links ] .Hall , D.T. ( 1976 ) .", "label": "", "metadata": {}}
{"text": "Glenview : Scott , Foresman .[Links ] .Hall , D.T. ( 2002 ) .Protean careers in and out of organizations .Thousand Oaks : Sage .[Links ] .Hall , D.T. ( 2004 ) .The protean career : A quarter - century journey .Journal of Vocational Behavior , 65 , 1 - 13 .[Links ] .Hall , D.T. , & Chandler , D.E. ( 2005 ) .Psychological success : When the career is a calling .Journal of Organizational Behavior , 26 , 155 - 176 .[", "label": "", "metadata": {}}
{"text": "Hall , D.T. , & Mirvis , P.H. ( 1996 ) .The new protean career : Psychological success and the path with a heart .In D.T. Hall ( Ed . ) , The career is dead , long live the career : A relational approach to careers ( pp .15 - 45 ) .San Francisco : Jossey Bass .[Links ] .Hansen , L.S. ( 1997 ) .Integrative Life Planning : Critical tasks for career development and changing life patterns .San Francisco : Jossey - Bass .[Links ] .Hoyle , R.H. ( 2000 ) .", "label": "", "metadata": {}}
{"text": "In H.E.A. Tinsley & S.D. Brown ( Eds . ) , Handbook of applied multivariate statistics and mathematical modeling ( pp .466 - 497 ) .San Diego : Academic Press .[Links ] .Hu , L. , & Bentler , P.M. ( 1999 ) .Cutoff criteria for fit indexes in covariance structure analysis : Conventional criteria versus new alternatives .Structural Equation Modeling , 6 , 1 - 55 .[Links ] .Inkson , K. ( 2006 ) .Protean and boundaryless careers as metaphors .Journal of Vocational Behavior , 69 , 48 - 63 .", "label": "", "metadata": {}}
{"text": "Links ] .McDonald , R.P. ( 1999 ) .Test theory : A unified treatment .Mahwah : Erlbaum .[Links ] .McDonald , R.P. ( 2005 ) .Semiconfirmatory factory analysis : The example of anxiety and depression .Structural Equation Modeling , 12 , 163 - 172 .[Links ] .McGrath , R.E. ( 2005 ) .Conceptual complexity and construct validity .Journal of Personality Assessment , 85 , 112 - 124 .[Links ] .O ' Connor , B. P. ( 2000 ) .SPSS and SAS programs for determining the number of components using parallel analysis and Velicer 's MAP test .", "label": "", "metadata": {}}
{"text": "[Links ] .Rasch , G. ( 1960 ) .Probabilistic models for some intelligence and attainment tests .Copenhagen : Denmarks Paedagogiske Institut .[Links ] .Smith , E.V. , Jr. ( 2002 ) .Understanding Rasch measurement : Detecting and evaluating the impact of multidimenstionality using item fit statistics and principal component analysis of residuals .Journal of Applied Measurement , 3 , 205 - 231 .[Links ] .Sue , D.W. , & Constantine , M.G. ( 2003 ) .Optimal human functioning in people of color in the United States .", "label": "", "metadata": {}}
{"text": "Counseling psychology and optimal human functioning ( pp .151 - 169 ) .Mahwah : Erlbaum .[Links ] .Sullivan , S.E. , & Arthur , M.B. ( 2006 ) .The evolution of the boundaryless career concept : Examining physical and psychological mobility .Journal of Vocational Behavior , 69 , 19 - 29 .[Links ] .Wegener , D.T. , & Fabrigar , L.R. ( 2000 ) .Analysis and design for nonexperimental data : Adressing causal and noncausal hypotheses .In Reis , H.T. & Judd , C.M. ( Eds . ) , Handbook of research methods in social and personality psychology ( pp .", "label": "", "metadata": {}}
{"text": "Cambridge : Cambridge University Press .[Links ] .Wilson , M. ( 2005 ) .Constructing measures : An item response modeling approach .Mahwah : Erlbaum .[Links ] .Wood , J.M. , Tataryn , D.J. , & Gorsuch , R.L. ( 1996 ) .Effects of under- and overextraction on principal axis factor analysis with varimax rotation .Psychological Methods , 1 , 354 - 365 .[Links ] .Wright , B.D. , & Masters , G.N. ( 1982 ) .Rating scale analysis .Chicago : MESA .[Links ] .", "label": "", "metadata": {}}
{"text": "Best test design .Chicago : MESA .[Links ] .Correspondence to : Gideon de Bruin Postal address : Department of Industrial Psychology and People Management , University of Johannesburg PO Box 524 , Auckland Park Johannesburg 2006 , South Africa email : deondb@uj.ac.za .Received : 08 Nov. 2009 Accepted : 20 Sept. 2010 Published : 03 Dec. 2010 .The Authors .Licensee : OpenJournals Publishing .This work is licensed under the Creative Commons Attribution License .We also rotated a four - factor solution , which on the surface appeared to correspond with the four theoretical constructs .", "label": "", "metadata": {}}
{"text": "In the Briscoe et al .( 2006 ) standardisation study the component loadings of the three items were 0.348 for item 1 , 0.414 for item 8 and 0.563 for item 15 .It should be kept in mind that these loadings are inflated due to the use of the principal components model rather than the common factor model .Manually constructing multilingual translation lexicons can be very costly , both in terms of time and human effort .Although there have been many efforts at ( semi-)automatically merging bilingual machine readable dictionaries to produce a multilingual lexicon , most of these approaches place quite specific requirements on the input bilingual resources .", "label": "", "metadata": {}}
{"text": "We describe a low cost method for constructing a multilingual lexicon using only simple lists of bilingual translation mappings .The method is especially suitable for under - resourced language pairs , as such bilingual resources are often freely available and easily obtainable from the Internet , or digitised from simple , conventional paper - based dictionaries .The precision of random samples of the resultant multilingual lexicon is around 0.70 - 0.82 , while coverage for each language , precision and recall can be controlled by varying threshold values .Given the very simple input resources , our results are encouraging , especially in incorporating under - resourced languages into multilingual lexical resources .", "label": "", "metadata": {}}
{"text": "Hey Guys !I can definitely use your help with this .I am trying to get an assessment on whether or not I have a chance at passing the Comlex Step 1 .I took the Kaplan Comlex Qbank and right now , have been averaging 56 - 66 % ( lately , I have been getting 60 's ) .I have 4 weeks left for the exam .Here are my study materials .Kaplan Step 1 Comlex Qbank First Aid USMLE Step 1 High Yield Neuroanatomy High Yield Genetics ( not studying for Comlex ) BRS Pathology BRS Physiology OMT Review .", "label": "", "metadata": {}}
{"text": "I am going to be sepdning the next week with the High Yield Neuro , OMT Review , and will probably go throguh the BRS Pathology the week after .Does anyone have any tips as of right now ?Is getting 60%-66 % on the Q bank good enough to possibly pass ?I have no idea how these scores work .I am so nervous I can hardly think straight rigyht now !Keep it simple , there is not a lot of molecular / biochem / genetics / mechanisms on this exam .Memorize the basics from FA , do plenty of practice questiions and read explanations , and memorize basics of OMM .", "label": "", "metadata": {}}
{"text": "Keep it simple for COMLEX ! ! !Keep it simple , there is not a lot of molecular / biochem / genetics / mechanisms on this exam .Memorize the basics from FA , do plenty of practice questiions and read explanations , and memorize basics of OMM .I would also not worry at all about Neuro .Keep it simple for COMLEX !Click to expand ... .I 'm sorry , but I have to completely disagree w/ the bolded comment .Neuro is traditionally fairly heavy on the comlex and certainly was on my test last year .", "label": "", "metadata": {}}
{"text": "whatever you do , DO NOT , DO NOT skip neuro for the COMLEX !my version was very neuro , psych pharm , and micro heavy .OMM is also something that if you ace it will truly help you , and the book you have , OMT Review is definitley the BEST out there .I recommend you take the COMSAE maybe a week or 2 prior to the exam to get an idea of where you might score .The Kaplan COMLEX QBank is too easy compared to the real test , IMO , but it 's really the only thing out there to practice with .", "label": "", "metadata": {}}
{"text": "You still have 4 weeks and that is tons of time to kill neuro and hit micro and pharm pretty hard , others may have advice on other subjects that were high yield .I 'm sorry , but I have to completely disagree w/ the bolded comment .Neuro is traditionally fairly heavy on the comlex and certainly was on my test last year .If your goal is to pass you could probably get by w/o it .... but if you want to do well I would spend some time w/ the section in FA and HY Neuro .", "label": "", "metadata": {}}
{"text": "I had a ridiculous amount of neuro on my test ; in fact , so many similar questions that I almost wondered if there was some kind of error .Most of the questions were very straight forward , but IMHO , the Neuro section of FA may be the weakest in the book , so it 's a very good idea to supplement w/ HY , Road Map , etc .I would try to be very solid on cranial nerves , bleeds , blood supply , aphasias , etc . before walking into the test .Although it 's usefulness as a predictor is questionable , I would also recommend you take the practice NBOME test , COMSAE , just to get some exposure to the format and style of the questions .", "label": "", "metadata": {}}
{"text": "Most of it , if not all of it , is straight forward .Is FA Neuro weak ?Yes , maybe it is .But , it is enough for COMLEX .You do not want to start recruiting multiple books to assist you in studying for any section of this exam .Pick few resources and know them really well .FA in combination with a good question bank will prepare you very well for everything that you may encounter on COMLEX .This is all I am saying .The BRS physiology is probably overkill for COMLEX .", "label": "", "metadata": {}}
{"text": "Hey everyone !Thank you all for the wonderful advice .I heard that there were a lot of heavy Neuro questions , which is why I bought an extra HY Neuro book and spending almost a week with Neuro and OMT Review .Where can I take the COMSAE practice test ?I would like to sit down thsi weekend and take one of those exams to see where I stand right now .I have 4 weeks to recover if I am not doing well .Keep in mind that you can not go over your answers nor review Qs at a later time once they have been submitted .", "label": "", "metadata": {}}
{"text": "Once you submit one block there is no going back .And remember no matter how much you study there will be PLENTY of WTF Qs that another 4 weeks of studying could not have prepared you for .It sucks , bbut unfortunately that 's the way it is .Make sure you know your bugs and drugs , some path , some upper / lower limb anatomy and OMM .If you get those down you should pass .I 'm not saying to skip anything but you definitely DON'T want these to be your weaknesses going into the exam .", "label": "", "metadata": {}}
{"text": "Start it early since some topics need repeat review to sink in . .....Is FA Neuro weak ?Yes , maybe it is .But , it is enough for COMLEX .... FA in combination with a good question bank will prepare you very well for everything that you may encounter on COMLEX .... .Click to expand ... .I disagree .Mine was waaaaaay neuro heavy .I think I had 12 questions on the same brain bleed , and then more on tracts , nuclei , etc . .My study would be filled with bugs and drugs , anything remotely connected to the autonomic nervous sytem , neuro , OMM ( know your levels like the back of your hand ! ! ! ! ) anatomy of the extremities , and basic path-- probably in that order .", "label": "", "metadata": {}}
{"text": "My focus on comlex will be hammering bugs , drugs and omm ... with less emphasis on every other subject .I will def not invest a great deal of time in physio , path but neuro / ob - gyn path seem to account for a good deal of points on comlex .So yea do n't study it like you would for usmle if you are taking both exams .remember the key is to understand the difference in high yield material tested on both exams .Good luck !Has anyone heard anything about the Kaplan comlex books ?", "label": "", "metadata": {}}
{"text": "Click to expand ... .When I prepared for COMLEX Level 1 in ' 07 , Kaplan had one extra review book for OMT ( which was terrible ) and all the other texts were the same for the USMLE prep course .I preferred using Simmons ( though Savaresse is just as good ) to cover OMT .Do n't forget to memorize sympathetic / para innervations .Each year seems to emphasize different topics .When I took it ... micro , pharm ( know at least 2 - 3 drugs for each bug ! ) , ob / gyn and psych were really emphasized .", "label": "", "metadata": {}}
{"text": "Physio was a joke .I had to interpret at least 2 head CT 's ( very easy ) and 3 - 4 EKG 's ( relatively simple , though they were hard to see ... pictures on COMLEX are horrible ) .No biostats .Most people did n't have much biochemistry , but I had at least a dozen questions ( all were easy ... there is a list floating about of 10 reactions that sum up pretty much every type of biochem question you could possibly be asked on COMLEX ) .There are also a lot of repeat questions you 'll encounter .", "label": "", "metadata": {}}
{"text": "I preferred using Simmons ( though Savaresse is just as good ) to cover OMT .Do n't forget to memorize sympathetic / para innervations .Each year seems to emphasize different topics .When I took it ... micro , pharm ( know at least 2 - 3 drugs for each bug ! ) , ob / gyn and psych were really emphasized .There was a lot of Path , but it was very straight forward .Physio was a joke .I had to interpret at least 2 head CT 's ( very easy ) and 3 - 4 EKG 's ( relatively simple , though they were hard to see ... pictures on COMLEX are horrible ) .", "label": "", "metadata": {}}
{"text": "Most people did n't have much biochemistry , but I had at least a dozen questions ( all were easy ... there is a list floating about of 10 reactions that sum up pretty much every type of biochem question you could possibly be asked on COMLEX ) .There are also a lot of repeat questions you 'll encounter .Click to expand ... .hehe exactly why i want to take the comlex after 2 weeks of studying .i ca n't afford to spend my time toward comlex when usmle has so much to cover .My focus on comlex will be hammering bugs , drugs and omm ... with less emphasis on every other subject .", "label": "", "metadata": {}}
{"text": "So yea do n't study it like you would for usmle if you are taking both exams .remember the key is to understand the difference in high yield material tested on both exams .Good luck !Click to expand ... .Ob - gyn path is high yield ?I do n't know what 's really high yield for COMLEX except for OMT , bugs / drugs , neuro .Ob - gyn path is high yield ?I do n't know what 's really high yield for COMLEX except for OMT , bugs / drugs , neuro .", "label": "", "metadata": {}}
{"text": "Lamborghini has n't even taken it yet .So take his advice with a huge grain of salt .He does n't know what he 's talking about .Like I mentioned , the high yield subjects tend to be cyclical every other year .Ob / Gyn path ( as in images / histology ) is NOT high yield .However , the basics of ob / gyn such as : the menstrual cycle , the difference between various contraceptives ( especially the OCP 's ) , female development ( age of menarche , menopause , tanner staging ) , physiology of fetal circulation ... all of that IS high yield .", "label": "", "metadata": {}}
{"text": "So take his advice with a huge grain of salt .He does n't know what he 's talking about .Like I mentioned , the high yield subjects tend to be cyclical every other year .Ob / Gyn path ( as in images / histology ) is NOT high yield .However , the basics of ob / gyn such as : the menstrual cycle , the difference between various contraceptives ( especially the OCP 's ) , female development ( age of menarche , menopause , tanner staging ) , physiology of fetal circulation ... all of that IS high yield .", "label": "", "metadata": {}}
{"text": "huh speak for yourself ...i may not have taken it but i have friends who have and i get their input as much as you like to give yours to others .I may not have worded my intentions right but the point was the emphasis on ob / gyn , sorry for any confusion .I would disagree w/ path and physiology not being high yield on the COMLEX .Although the COMLEX is not as heavy into multi - step path & physio as the USMLE is , they are definitely still on there in a good amt .", "label": "", "metadata": {}}
{"text": "It was pretty obvious stuff for the most part ....Reed - Sternberg cells , etc .I hardly had any OB / GYN path , but I did have a few ?s on hormone levels during menstruation , ovulation , etc .Know your bugs & drugs .I would also know your neuro , especially signs & symptoms correlating w/ what vessel is occluded or what lobe is affected .I even had 2 cerebral angiograms to interpret ( but they were n't hard - HY neuro had some good pics for this ) .They love viscero - somatic reflexes so know all the levels that the major organs correspond to .", "label": "", "metadata": {}}
{"text": "s on viscerosomatics alone - easy points if you memorize them .They also seem to like giving pt signs & symptoms of a pt on a particular drug and then want you to pick out the drug that the pt is on .Remember , the exams are varied as to what gets hit hard ( renal heavy vs. cardio heavy etc ) but if you know your bugs & drugs plus basic path & phys along w/ viscero - somatics , you 'll be golden !When I prepared for COMLEX Level 1 in ' 07 , Kaplan had one extra review book for OMT ( which was terrible ) and all the other texts were the same for the USMLE prep course .", "label": "", "metadata": {}}
{"text": "Do n't forget to memorize sympathetic / para innervations .Each year seems to emphasize different topics .When I took it ... micro , pharm ( know at least 2 - 3 drugs for each bug ! ) , ob / gyn and psych were really emphasized .There was a lot of Path , but it was very straight forward .Physio was a joke .I had to interpret at least 2 head CT 's ( very easy ) and 3 - 4 EKG 's ( relatively simple , though they were hard to see ... pictures on COMLEX are horrible ) .", "label": "", "metadata": {}}
{"text": "Most people did n't have much biochemistry , but I had at least a dozen questions ( all were easy ... there is a list floating about of 10 reactions that sum up pretty much every type of biochem question you could possibly be asked on COMLEX ) .There are also a lot of repeat questions you 'll encounter .I think the list he is referring to was from Kaplan , but the rxns are also all in FA .It 's basically the rate - limiting rxn for glycolysis , gluconeogenesis , etc plus knowing maple syrup urine dz ( where the block is , what they are missing ) & a few other commonly tested genetic dz 's like CGD & alkaptonuria . Glucose-6", "label": "", "metadata": {}}
{"text": "HMG - CoA Reductase 3 .Dihydrofolate Reductase 4 .Gamma - Glutamyl Carboxylase ( Vitamin K Carboxylase ) 5 .Glucuronosyltransferase ( Crigler - Najjar ) 6 .Methyl - Malonyl CoA Mutase 7 .Homocysteine Methyl Transferase 8 .Hexosaminidase A ( Tay - Sachs ) 9 .HGPRT ( HPRT ) ( Lesch - Nyhan ) 10 .Branched - Chain Ketoacid Dehydrogenase ( Maple Syrup Urine ) .Which reminds me .I had at least three questions about Lesch - Nyhan syndrome .Go figure .So I know these are HYs : -bugs / drugs -anatomy : especially peripheral / UE -neuro : vascular lesions , anything else ? -CMB / Genetics : Know diseases ?", "label": "", "metadata": {}}
{"text": "-Behavioral : biostats and what else ?-Heme : Know the diseases and causes ?-Cardio : I 'd guess pathologies , EKGs , what else ?-Pulm : pathologies and what else ?-GI : ? ?-Derm : ? ?Thanks for the help guys , this should definitely help everyone studying to take this beast in their respective time frame !Translingual Information Access .Abstract .We present an attempt at a coherent vision of an end - to - end translingual information retrieval system .We begin by presenting a sample of the broad range of possibilities .", "label": "", "metadata": {}}
{"text": "Ranking retrieved documents , query - relevant summarization , assimilation of retrieved information , and system evaluation are all discussed in turn .Introduction : Beyond Traditional Information Retrieval .Traditional information retrieval ( IR ) offers a suite of very useful technologies , such as inverted key - word files , word - weighting methods including TF - IDF , precision and recall metrics , vector - space document modeling , relevance feedback , controlled - vocabulary indexing of text collections , and so on .Traditional IR technologies , however , have significant limitations , among others the following : .", "label": "", "metadata": {}}
{"text": "The query and the document collection must be in the same language , typically English .Success metrics are confined to relevance - only measures ( Salton and McGill 1983 ) , i.e. precision and recall , without regard to redundancy or suitability of the documents retrieved .An IR system that retrieves 10 copies of the same document is considered superior ( by TREC standards ) to one that retrieves 9 very relevant but very different documents .Retrieved texts are presented verbatim to the user , possibly with keywords highlighted , rather than summarized , gisted , grouped , or otherwise processed .", "label": "", "metadata": {}}
{"text": "Recent advances in statistical learning methods are making it possible to semi or fully automate the text categorization process .Translingual Query , Retrieval , and Summarization .Let us first cast the problem of translingual IR ( TIR ) in its simplest terms , and subsequently revisit the surrounding complexity .Assume a query Q S in the the user 's language ( definitionally , the source language , SL ) .The central problem is to retrieve relevant documents not just in the same source language , but also in each target language of interest ( TL 1 , TL 2 , ...", "label": "", "metadata": {}}
{"text": "Let us start with a single target language .Let [ D S ] be a document collection to be searched in the source language , and [ D T ] be another document collection to be searched , but in the target language .In order to search the [ D S ] with Q S , all of the standard IR techniques can be used directly ( weighted key - words , vector cosine similarity , latent semantic indexing ( LSI ) , and so forth ) .But none of these techniques apply directly to search [ D T ] with Q S .", "label": "", "metadata": {}}
{"text": "Translate the collection -- Convert [ D T ] into a collection in the source language [ D ' S ] by manual or machine translation .Then search [ D ' S ] with Q S .Translate the query -- Convert Q S into Q ' T by manual or machine translation .Then search [ D T ] with Q ' T , and if desired translate the retrieved documents manually or by MT from TL into SL .Then query [ D T ] with Q S directly using the LSI method , and if desired , translate the retrieved documents in TL into SL .", "label": "", "metadata": {}}
{"text": "Then , new documents in either SL or TL can be retrieved via a single common set of index terms .These conceptual structures are essentially the same as produced in the first stage of Machine Translation , and can be used to directly index documents in multiple languages ( Mitamura , et al . , 1991 ) .Clearly , these methods increase in complexity , and the techniques range from using statistical learning , to crude translation for indexing , to precise conceptual analysis for both indexing and translation .It seems to us that an appropriate research strategy is to begin by protoyping the simpler methods , based on current technology .", "label": "", "metadata": {}}
{"text": "If the simpler methods prove insufficient , we propose to explore more complex and methods , rather than scaling up the simpler methods .To date , the LSI method has been investigated for cross - linguistic query , starting with work at Bellcore by Landauer & Littman and later Dumais , with partial success ( Dumais et al .1996 ) .But the other methods have not yet been investigated in any depth , nor have there been many systematic cross - method comparisons .In addition to Davis & Dunning 's work ( Davis & Dunning 1996 ) , we have recently performed an experiment systematically comparing various methods ( Carbonell et al .", "label": "", "metadata": {}}
{"text": "In our experiments , a simpler technique related to LSI but without its attempt at dimension reduction produced equivalent results at lower processing cost : the Generalized Vector Space Model ( GVSM ) .Integrating Translingual IR into an Analyst 's Workstation .We envision the following stages in translingual processing within such an analyst 's workstation : .Actual Translingual Retrieval .In this stage , we either transform the source language query into a query or set of queries in the target language(s ) , or we avoid the need to translate the query at all .In either event , the result is a set of target language documents believed to match the query to some degree .", "label": "", "metadata": {}}
{"text": "As in monolingual IR , the documents retrieved must be ranked .We believe a new metric , MMR , described below , is much better suited to the current deluge of information than previous techniques .Summarization of the Results .The retrieved documents , in several languages , are summarized in the source ( query ) language ; the summaries are then translated and made available to the information gatherer , who may decide to initiate a full translation of a particular document of interest .Selective Assimilation of the Results .Retrieved documents which are of interest are fully translated from the target language to the source ( query ) language , for assimilation by the analyst .", "label": "", "metadata": {}}
{"text": "These steps are illustrated in Figure 1 , and will be described in the remainder of this paper , followed by a suggestion for evaluating overall system performance .We expect that a multi - stage design will be necessary in many actual real - world applications , with fast but crude techniques performing an intial filter before higher - quality but slower techniques are applied .Specific Translingual Information Retrieval techniques .Of the different methods for moving towards true TIR , two of the most immediately practical are to exploit a parallel corpus with ( pseudo-)relevance - feedback or to apply knowledge - based methods to translate the query .", "label": "", "metadata": {}}
{"text": "If even a modest parallel corpus of documents can be located ( or created ) for the source and target language pair , then we can exploit the tried - and - true relevance feedback method to perform translingual query , as outlined in Figure 2 .Using the Rocchio formula , we can improve on this pseudo - relevance feedback ( RF ) process if the analyst is willing to provide relevance judgements for the retrieved SL documents from the parallel corpus .With such judgements , we can construct a better term - weighted query for the TL search , essentially producing true translingual RF .", "label": "", "metadata": {}}
{"text": "The envisioned mechanism is shown in Figure 2 , and encompasses the following steps : .The analyst types in a source language query Q S ; .Parallel corpus ( source half ) is searched by an IR engine using Q S ; .One of the following methods is used to search the TL document database : .From retrieved SL / TL document pairs , the TL document contents are used as a new query Q T to search the TL document database ; or .The retrieved SL / TL document pairs are first given back to the analyst , in order to scan the SL documents for relevance ; then the Rocchio formula is used for both SL and TL document database search .", "label": "", "metadata": {}}
{"text": "A major advantage of this method is that no translation at all needs to be done in the early stages of information gathering , when the largest volumes of information must be processed .Knowledge - Based Methods for Translingual IR .When the translingual information retrieval is within a well - defined domain , we expect that the infusion of knowledge - based techniques from the fields of natural language processing ( NLP ) and machine translation ( MT ) can provide the following benefits : .Effective short - query translation through expansion translation ; .Improved recall through linguistic generalization ; .", "label": "", "metadata": {}}
{"text": "Truly semantic multilingual search through conceptual querying .Query Expansion Translation : When translating a query , it is not possible to do a reliable exact translation , especially because queries tend to include isolated words and phrases out of context , as well as possibly full clauses or sentences .Any automated translation risks selecting the wrong meaning of the query terms , and therefore the wrong translation .Hence , we propose to perform an expansion translation , in which all meanings of all query terms are generated , properly weighed for base - line and co - occurrence statistics , so that no meaning is lost .", "label": "", "metadata": {}}
{"text": "One reason for this is that the documents themselves serve as filters , since it is unlikely that a single document will hit one sense of each term unless they are coherent sense - choices .To the degree that we can enhance the query translation by template extraction and deeper semantic analysis , as described later in this section , we should be able to recover the loss in precision .Once the query is expanded / translated into Q T , it is used for retrieval , and the retrieved D T 's can be summarized and/or translated as discussed in following sections .", "label": "", "metadata": {}}
{"text": "He is immediately faced with two issues for which NLP techniques provide assistance : .Determining the set of relevant keywords .Keyword search will perform an exact match , but in many cases the information gatherer is interested in finding documents which match keyword synonyms , as well .This form of generalization can be similar to thesaurus lookup ( e.g. , finding all words with the same part of speech that have identical or similar meaning , e.g. , ' ' acquire ' ' , ' ' buy ' ' , ' ' take over ' ' , etc . ) .", "label": "", "metadata": {}}
{"text": "Recall can be improved significantly if this kind of generalization is provided automatically for the information gatherer .Existing IR systems achieve partial success through the use of general thesauri , which can be less useful in specialized semantic domains ( for example , linking ' ' acquire ' ' to ' ' get ' ' may not help precision or recall in a joint venture domain ) .Determining relevant variations on keywords .Having a system that can automatically determine the right set of morphological variants would cut down on the human effort that is otherwise required to boost recall .", "label": "", "metadata": {}}
{"text": "In fact , increasing the number of possible search terms increases the chance of a false match due to random coincidence of terms .To make the most of linguistic generalization , it is necessary to improve precision through the use of linguistic relations .The real cause of impreciseness in multiple keyword search is that all information about the relationships between the words is lost .A typical example is the noun phrase ' ' victims of teenage crime ' ' , which is likely to match documents describing ' ' teenage victims of crime ' ' in a keyword - style search .", "label": "", "metadata": {}}
{"text": "If it were possible to parse each document and represent the linguistic structure of the portions of text which seem to match the query , this type of bad match could be ruled out .For example , a syntactic parse results in different structures for the two ' ' victim ' ' sentences ; if matching during search included parsing and unification , these two structures would not match , thus boosting precision .Conceptual Querying :The ultimate goal in retrieval is to match all the documents whose contents are relevant , regardless of their surface form .", "label": "", "metadata": {}}
{"text": "We can extend the notion of a ' ' thesaurus ' ' for keyword synonyms to include terms from multiple languages , but this becomes difficult to manage as the lexicon of search terms and number of languages grows .Ideally , we would like to combine the notion of linguistic generalization and linguistic relations into a language - neutral query form which can be used to retrieve multilingual documents .The notion of conceptual querying for English is not new .The FERRET system ( Mauldin , 1991b ) presented the idea of matching on the conceptual structure of a text , rather than keywords , to improve precision and recall .", "label": "", "metadata": {}}
{"text": "For example , the KANT system ( Mitamura et al . , 1991 ) uses a conceptual form called interlingua to model the linguistic relations among the source terms in a deep semantic representation .The interlingua captures domain actions , objects , properties , and relations in a manner which is language - independent , and is used as an intermediate form during translation .We believe a synthesis of these approaches , referred to here as conceptual query , can provide a convenient framework for the use of linguistic generalization ( to improve recall ) and linguistic relations ( to improve precision ) , while also providing support for multilingual search .", "label": "", "metadata": {}}
{"text": "The user specifies a query in his native language ; .The query is parsed into conceptual form , preserving relevant linguistic relations in corresponding semantic slots ( semantic roles ) ; .All relevant surface generalizations ( in the source language ) are compiled into a set of search templates , which contain all the relevant morphologic variations , category switches , etc . ; .All relevant surface generalizations ( in the supported target languages ) are compiled into a set of search templates , which contain all the relevant morphologic variations , category switches , etc . ; .", "label": "", "metadata": {}}
{"text": "A rapid - deployment approach which is under investigation involves the use of existing keyword indices ( e.g. , Lycos ) to locate potentially relevant texts , so that the more computationally - intensive methods required for template matching are focussed on a small subset of the entire database .Ranking retrieved documents : Maximal Marginal Relevance .Searching any very large document data base , including the World Wide Web , quickly leads to the realization that there are not just more documents than anyone could ever assimilate , but also there are more relevant documents than anyone could ever assimilate .", "label": "", "metadata": {}}
{"text": "If only we could reduce or eliminate this redundancy we would have a much more manageable subset of relevant documents that nonetheless span the information space .The MMR formula below gives a credit for relevance and a penalty for redundancy , with a tunable parameter lambda ( where DB is the total document database , and RL the already produced ranked list of documents ) : .Summarization of Results .We envision the following framework for presenting multilingual search results to the information gatherer : .Based on the quality of the match to the query that was used to retrieve each document , documents are scored and sorted using MMR ; .", "label": "", "metadata": {}}
{"text": "A short summary is produced for each selected document , and that summary is translated to the source ( query ) language ; .Information about each document ( origin , language , score , summary ) is presented to the user in an interface which allows him to select specific documents for additional processing ( such as selective assimilation , described below ) .We are currently developing methods of automatically creating summaries of documents .Summarization is particularly useful for TIR because document access is far faster if only the summaries of retrieved target language documents need be translated , with further material only translated upon drill - down requests .", "label": "", "metadata": {}}
{"text": "Selective Assimilation of Results .Multi - Engine Machine Translation ( MEMT )( Frederking and Nirenburg 94 ) is designed for general purpose , human - aided MT .The MEMT architecture is well - suited to the task of selective assimilation of retrieved documents .In the MEMT architecture , shown in Figure 3 , an input text is sent to several MT engines in parallel , with each engine employing a different MT technology .These target language segments are indexed in the chart based on the positions of the corresponding source language segments .Thus the chart contains multiple , possibly overlapping , alternative translations .", "label": "", "metadata": {}}
{"text": "These selection techniques attempt to produce the best overall result , taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments .In essence , we do Bayesian - style training to maximize the probability of a correct translation given the available options , their estimated quality , and the well - formedness of the output translation as determined by a trigram language model .Thus our MEMT techniques are an example of integrating statistical and symbolic approaches ; the MT engines that we have employed to date have all been symbolic engines , while the integration of their outputs is primarily statistical ( Brown and Frederking 95 ) .", "label": "", "metadata": {}}
{"text": "Differences in translation quality and domain size can be exploited by merging the best results from different engines .In the earlier Pangloss unrestricted translation system , when Knowledge - Based MT ( KBMT ) could produce high - quality , in - domain translations , its results were used ; when Example - Based MT ( EBMT ) found a high - quality match , its results were used .When neither of these engines produced a high - quality result , the wider - coverage transfer - based engine supplied a lower - quality translation , which was still much better than leaving the source language untranslated .", "label": "", "metadata": {}}
{"text": "93 ) , which greatly increased the usefulness of the transfer - based translations .The application of this architecture to the current problem is clear .Once the user selects a translated summary for further investigation , the selected document is translated by the MEMT system , with KBMT , EBMT , transfer , and possibly other MT engines combining to give the user an initial , fully - automatic rough translation .Evaluating System Performance .In order to evaluate the usefulness of more advanced techniques in a given domain , experiments must be undertaken to measure the recall and precision of existing methods ( e.g. , keyword search ) vs. more sophisticated techniques such as template matching and partial template matching .", "label": "", "metadata": {}}
{"text": "Since the more sophisticated symbolic methods we are considering require both additional processing time ( impacting the user ) and development time ( impacting start - up cost ) , it is important to determine whether the improvements in recall , precision , and multilinguality are sufficient payoff .In future work we will complete an initial implementation of the conceptual search method , and evaluate its usefulness on a variety of domains .Conclusion .We have attempted to present a coherent vision of a translingual information retrieval system , illustrating the broad range of possibilities with two specific examples .", "label": "", "metadata": {}}
{"text": "1997 ) , much experimentation remains to be done , in order to see whether combinations of old techniques or novel translingual techniques provide the necessary performance to produce a useful translingual analyst 's workstation .References .Carbonell , J. G. , 1996 . ''Query - Relevant Document Summarization ' ' .CMU Technical Report , Carnegie Mellon University .Carbonell , J. , Yang , Y. , Frederking , R. , Brown , r. , Geng , Y. , Lee , D. , 1997 . ''A Realistic Evaluation of Translingual Information Retrieval Methods ' ' , submitted to SIGIR-97 .", "label": "", "metadata": {}}
{"text": "A TREC evaluation of query translation methods for multi - lingual text retrieval ' ' , In Proceedings of the 4th Text Retrieval Conference ( TREC-4 ) .Dumais , S. , Landauer , T. and Littman , M. , 1996 . ''Automatic Cross - Linguistic Information Retrieval using Latent Semantic Indexing ' ' , In Proceedings of SIGIR-96 , Zurich .Frederking , R. , Grannes , D. , Cousseau , P. , and Nirenburg , S. , 1993 .An MAT Tool and Its Effectiveness .In Proceedings of the DARPA Human Language Technology Workshop , Princeton , NJ .", "label": "", "metadata": {}}
{"text": "Three Heads are Better than One . ' ' Proceedings of the fourth Conference on Applied Natural Language Processing , ANLP-94 , Stuttgart , Germany .Jacobs , P. , Krupka , G. , Rau , L. , Mauldin , M. and Kaufmann , T. , 1992 . ''Description of the TIPSTER / SHOGUN System as used for MUC-4 ' ' , Proceedings of the Fourth Message Understanding Conference , McLean , Virginia .Luhn , H. P. , 1958 . ' ' Automatic Creation of Literature Abstracts ' ' , IBM Journal , pp .159 - 165 .", "label": "", "metadata": {}}
{"text": "Retrieval Performance in FERRET : A Conceptual Information Retrieval System ' ' , Proceedings of the 14th International Conference on Research and Development in Information Retrieval , Chicago .Mitamura , T. , E. Nyberg and J. Carbonell .An Efficient Interlingua Translation System for Multi - lingual Document Production . ' ' Proceedings of the Third Machine Translation Summit , Washington , D.C. .Paice , C. D. , 1990 . ''Constructing Literature Abstracts by Computer : Techniques and Prospects ' ' .Information Processing and Management , Vol .26 , pp .171 - 186 .", "label": "", "metadata": {}}
{"text": "Introduction to Modern Information Retrieval ' ' , ( New York : McGraw - Hill ) .Salton , G. , 1970 . ''Automatic Processing of Foreign Language Documents ' ' , Journal of American Society for Information Sciences , Vol .21 , pp .187 - 194 .A Dictionary of Selected Synonyms ( 1949 ) .Tools . by Gideon S. Mann , David Yarowsky , Bridge Languages - In Proceedings of NAACL 2001 , 2001 . \" ...This paper presents a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages .", "label": "", "metadata": {}}
{"text": "Translation lexicons for arbitrary distant language pairs are t ... \" .This paper presents a method for inducing translation lexicons based on transduction models of cognate pairs via bridge languages .Bilingual lexicons within languages families are induced using probabilistic string edit distance models .Translation lexicons for arbitrary distant language pairs are then generated by a combination of these intra - family translation models and one or more cross - family online dictionaries .Up to 95 % exact match accuracy is achieved on the target vocabulary ( 30 - 68 % of inter - family test pairs ) .", "label": "", "metadata": {}}
{"text": "Using that technique , he claimed to demonstrate genetic relationship between many languages : four families in Africa ( Greenberg 1963 ) , previously unclassified languages of Papua and vic ... \" .he may be best remembered for is his advocacy and prolific use of a methodology he called multilateral comparison .Indeed , Greenberg clearly believed that the technique was capable of demonstrating relationships among all languages .The prospect of making such great progress in uncovering the phylogeny of human language has excited many people and inspired them to apply multilateral comparison techniques to demonstrate the existence of very large genetic groups .", "label": "", "metadata": {}}
{"text": "200sconcepts for each of the languages . \" ... related to the field of emotions .Expressions such as I 'm deeply touched or touching words are widely used in English .Hans Kurath ( 1921 ) had already classified sense perception in respect to emotions and stated how ' the kinaesthetic , the visceral , and the tactual perceptions have a relatively stron ... \" . related to the field of emotions .Expressions such as I 'm deeply touched or touching words are widely used in English .Kurath explained this transfer of meaning from sense perception to emotion on the basis of the similarity of feeling that both domains share .", "label": "", "metadata": {}}
{"text": "In the 90 's , Eve Sweetser ( 1990 ) proposed the same mappings between these two domains within the framework of cognitive linguistics .One of the main claims of this model ( Johnson 1987 , Lakoff 1987 , Langacker 1987 , 1991 ) is that linguistic structure is based on human cognition , i.e. on human perception and . by N Kazanas , Omilos Meleton , Athens Jan - feb - in Journal of Indo - European Studies vol 31 .Two clarifications seem to be needed at the outset .Two clarifications seem to be needed at the outset .", "label": "", "metadata": {}}
{"text": "The purpose of the series is to make available to specialists and the interested public the results of research that , because of its unconventional or controversial nature , might otherwise go unpublished .Th ... \" .The purpose of the series is to make available to specialists and the interested public the results of research that , because of its unconventional or controversial nature , might otherwise go unpublished .The editor actively encourages younger , not yet well established , scholars and independent authors to submit manuscripts for consideration .Contributions in any of the major scholarly languages of the world , including Romanized Modern Standard Mandarin ( MSM ) and Japanese , are acceptable .", "label": "", "metadata": {}}
{"text": "Although the chief focus of Sino - Platonic Papers is on the intercultural relations of China with other peoples , challenging and creative studies on a wide variety of philological subjects will be entertained .This series is not the place for safe , sober , and stodgy presentations .Sino - Platonic Papers prefers lively work that , while taking reasonable risks to advance the field , capitalizes on brilliant new insights into the development of civilization .The only style - sheet we honor is that of consistency .Where possible , we prefer the usages of the Journal of Asian Studies .", "label": "", "metadata": {}}
{"text": "Sino - Platonic Papers emphasizes substance over form .Submissions are regularly sent out to be refereed and extensive editorial suggestions for revision may be offered .Manuscripts should be double - spaced with wide margins and submitted in duplicate .A set of \" Instructions for Authors \" may be obtained by contacting the editor .Ideally , the final draft should be a neat , clear camera - ready copy with high blackand - white contrast .Sino - Platonic Papers is licensed under the Creative Commons Attribution - NonCommercial - NoDerivs 2.5 License .To view a copy of this license , visit . ... revolve ; to be round \" found in Northern Chinese dialects and Bodic languages and dialects .", "label": "", "metadata": {}}
{"text": "\" The purpose of the series is to make available to specialists and the interested public the results of research that , because of its unconventional or controversial nature , might otherwise go unpublished .Th ... \" .The purpose of the series is to make available to specialists and the interested public the results of research that , because of its unconventional or controversial nature , might otherwise go unpublished .The editor actively encourages younger , not yet well established , scholars and independent authors to submit manuscripts for consideration .Contributions in any of the major scholarly languages of the world , including Romanized Modern Standard Mandarin ( MSM ) and Japanese , are acceptable .", "label": "", "metadata": {}}
{"text": "Although the chief focus of Sino - Platonic Papers is on the intercultural relations of China with other peoples , challenging and creative studies on a wide variety of philological subjects will be entertained .This series is not the place for safe , sober , and stodgy presentations .Sino - Platonic Papers prefers lively work that , while taking reasonable risks to advance the field , capitalizes on brilliant new insights into the development of civilization . ... as a wheat plant with two awns .This correspondence between OC and PIE shows the western origin of wheat .", "label": "", "metadata": {}}
{"text": "RECONSTRUCTING THE GEOCENTRIC SYSTEM OF PROTO OCEANIC .Whatever the case , these geographicallybased strategies will be here designated by the technical term \" geocentric \" .Such a system can be found in Guugu - Yimithirr ( Haviland 1993 ) , in Tzeltal ( Levins ... . \" ...In English noun ellipsis constructions such as ( 1a ) ONE must appear .This so called ONE insertion is impossible in Standard Dutch ( 1b ) , but we find a similar phenomenon in three varieties of Dutch , i.e. some Northern Brabantish dialects ( 1c ) , Frisian ( 1d ) and some Groningen dialects ( 1e).1 ... \" .", "label": "", "metadata": {}}
{"text": "This so called ONE insertion is impossible in Standard Dutch ( 1b ) , but we find a similar phenomenon in three varieties of Dutch , i.e. some Northern Brabantish dialects ( 1c ) , Frisian ( 1d ) and some Groningen dialects ( 1e).1 . ... or in some cases the superlative suffix -ste to the numeral ( 7c ) .With \u00e9\u00e9n ' one ' we find suppletion ( 7d ) .This suppletion can not be triggered by phonological conditions , in view of the fact that no suppletion occurs with tien'ten ' , which has the same coda as \u00e9\u00e9n ' o ..", "label": "", "metadata": {}}
{"text": "Such a region is called a ' ' flat minimum ' ' ( Hochreiter & Schmidhuber , 1995 ) .To get an intuitive feeling for why a flat minimum is interesting , consider this : a ' ' sharp ' ' minimum ( see figure 2 ) corresponds to weights which have to be specified with high precision .A flat minimum ( see figure 1 ) corresponds to weights many of which can be given with low precision .In the terminology of the theory of minimum description ( message ) length ( MML , Wallace , 1968 ; MDL , Rissanen , 1978 ) , fewer bits of information are required to describe a flat minimum ( corresponding to a ' ' simple ' ' or low complexity - network ) .", "label": "", "metadata": {}}
{"text": "Similarly , the standard Bayesian view favors ' ' fat ' ' maxima of the posterior weight distribution ( maxima with a lot of probability mass -- see , e.g. , Buntine & Weigend , 1991 ) .We will see : flat minima are fat maxima .Unlike , e.g. , Hinton and van Camp 's method ( 1993 ) , our algorithm does not depend on the choice of a ' ' good ' ' weight prior .It finds a flat minimum by searching for weights that minimize both training error and weight precision .This requires the computation of the Hessian .", "label": "", "metadata": {}}
{"text": "Automatically , the method effectively reduces numbers of units , weights , and input lines , as well as output sensitivity with respect to remaining weights and units .Unlike , e.g. , simple weight decay , our method automatically treats / prunes units and weights in different layers in different reasonable ways .Section 5 reports experimental generalization results with feedforward and recurrent networks .For instance , in an application to stock market prediction , flat minimum search outperforms the following , widely used competitors : ( 1 ) conventional backprop , ( 2 ) weight decay , ( 3 ) ' ' optimal brain surgeon ' ' / ' ' optimal brain damage ' ' .", "label": "", "metadata": {}}
{"text": "The appendix presents a detailed theoretical justification of our approach .Using a variant of the Gibbs algorithm , appendix A.1 defines generalization , underfitting and overfitting error in a novel way .By defining an appropriate prior over input - output functions , we postulate that the most probable network is a ' ' flat ' ' one .Appendix A.2 formally justifies the error function minimized by our algorithm .Affiliated with .Affiliated with .Affiliated with .Abstract .Background .Our goal in BioCreAtIve has been to assess the state of the art in text mining , with emphasis on applications that reflect real biological applications , e.g. , the curation process for model organism databases .", "label": "", "metadata": {}}
{"text": "The task was to produce the correct list of unique gene identifiers for the genes and gene products mentioned in sets of abstracts from three model organisms ( Yeast , Fly , and Mouse ) .Results .Eight groups fielded systems for three data sets ( Yeast , Fly , and Mouse ) .For Fly , the top F - measure was 0.82 out of 11 systems and for Mouse , it was 0.79 out of 16 systems .Conclusion .This assessment demonstrates that multiple groups were able to perform a real biological task across a range of organisms .", "label": "", "metadata": {}}
{"text": "These results hold out promise that the technology can provide partial automation of the curation process in the near future .Background .Task 1B , the normalized gene list task , is intermediate in the BioCreAtIvE tasks .It builds on task 1A , the gene mention identification task [ 1 ] , but it is much simpler and requires far less understanding of the underlying biology than task 2 , functional annotation from text [ 2 ] .It reflects a step in the curation process for the model organism databases : once an article is selected for curation , an important step is to list those genes discussed in the article that have sufficient experimental evidence to merit curation - see discussion in [ 3 ] .", "label": "", "metadata": {}}
{"text": "We chose to use Fly [ 4 ] , Mouse [ 5 ] , and Yeast [ 6 ] model organism databases as our three sources of data .Figure 1 shows a sample abstract from MEDLINE , together with the gene list for that abstract ( top ) from FlyBase .Evaluation for task 1B is straightforward : it consists of comparing lists of unique identifiers .This makes it much easier to evaluate than the other tasks in BioCreAtIvE. Task 1A required the comparison of annotated text segments , raising issues of how to annotate complex gene names ( e.g. , TTF-1-binding sites ( TBE ) 1 , 3 , and 4 ) , as well as questions about gene name boundaries .", "label": "", "metadata": {}}
{"text": "Originally , for task 1B , we had also wanted evidence for each answer , parallel to the evidence passages required for task 2 , but our instructions for this were not clear , different people submitted different things and we did not evaluate this .In order to make the task uniform across the different model organisms and easily accessible to non - biologists , we extracted synonym lists from each of the three model organism databases .For each organism , the synonym list consisted of the list of unique gene identifiers and their associated gene symbol and synonyms .", "label": "", "metadata": {}}
{"text": "Figure 1 ( bottom ) shows two entries from the synonym list for Fly .By providing a uniform set of lexical resources for each model organism , we hoped to encourage experimentation with techniques that could readily generalize to new organisms .However , participants were also allowed to use additional lexical resources , and a several groups took advantage of this .We chose to use abstracts as the basis for the gene list task , rather than full text articles .This simplified the task for the participants , since abstracts are much shorter and easier to process than full text article ( because they are around 250 words long and are available in ASCII ) .", "label": "", "metadata": {}}
{"text": "However , using abstracts meant that we had to prune the gene lists provided by the model organism database , since these were usually based on the full text articles .Table 1 shows the size of the training , development test and blind test data sets .To prepare the training data , we developed an automated pruning procedure to remove genes from the gene list that were not mentioned in the abstract .As discussed in [ 3 ] , this was a \" noisy \" process .We delivered the noisy training data \" as is \" but we hand corrected the development test data and the blind test data .", "label": "", "metadata": {}}
{"text": "The model organism databases do not curate every gene mentioned in a paper - they curate only those genes that meet a set of ( organism - specific ) criteria , including presentation of experimental evidence related to gene or gene included in the gene list .However , we felt that the abstract might not provide enough context to determine whether a gene had sufficient evidence for curation or was mentioned only in passing , so for the test data sets , the annotators added , by hand , all genes mentioned in the abstract .This was not done for the automatically generated training data , so the automatically generated training set had significant recall errors ( see Tables 2 , 3 , 4 ) .", "label": "", "metadata": {}}
{"text": "Tables 2 , 3 , 4 show the scores from each participating system , by group and run ( each run was considered a system ) for Yeast ( Table 2 ) , Fly ( Table 3 ) and Mouse ( Table 4 ) .Each group was allowed to submit up to three systems for each organism .The systems were scored against the manually created \" gold standard \" for each abstract in the test set ( 250 abstracts per organism ) .The results are presented in terms of the following metrics : .True Positives : Number of correctly detected genes .", "label": "", "metadata": {}}
{"text": "Misses : Number of genes NOT detected by the system .Precision : True Positives / ( True Positives + False Positives ) .Recall : True Positives/ ( True Positives + Misses ) .The first two rows of each table show first the Gold Standard compared to itself , which always yields a score of 100 % or 1 .The second line , Noisy Training , shows the results of comparing the test data run through the \" automatic cleaning \" procedure and compared to the Gold Standard .This provides an estimate of the quality of the automatically generated training data .", "label": "", "metadata": {}}
{"text": "In addition to the tables , Figure 2 shows a composite graph of precision versus recall for all systems and all organisms .This graph also shows the estimates of training data quality ( marked as Yeast Train , Fly Train and MouseTrain in the legend and in solid symbols on the graph ) .The diagonal line indicates balanced precision versus recall .The results demonstrate several things , in particular , that there are significant differences among organisms .Yeast is the easiest .The F - measures of the systems tended to be high , with several groups achieving an F - measure of over 0.90 , and a median F - measure of 0.86 .", "label": "", "metadata": {}}
{"text": "Fly was harder than Yeast : the high F - measure was 0.82 , and there was much greater variability in performance ( median F - measure was 0.66 ) .The training data quality for Fly was significantly lower than for Yeast ( 0.83 ) .Fly was hard because there are many ambiguous terms , and there is also extensive overlap between Fly gene name abbreviations and English words , such as \" not \" , \" period \" , \" was \" , etc . .Mouse was the hardest as measured by system performance ( best F - measure 0.79 ) , although the median system performance for Mouse was better than for Fly ( 0.74 ) .", "label": "", "metadata": {}}
{"text": "The poor training data quality was related to the stringent Mouse curation criteria .Because of this , there were relatively many more genes that were mentioned in the article but not judged to be appropriate for curation ( and therefore , not on the list of curated genes from the MGI database ) .These genes were not included in the automatically generated training data , hence the low recall and low F - measure for the training data .Of course , such mentions were added manually into the development test data and blind test data .Indeed , for Mouse , the median system F - measure was actually higher than the training data F - measure , indicating that the systems did a good job in generalizing away from the noise .", "label": "", "metadata": {}}
{"text": "For both Yeast and Fly , the estimated training data quality was just a shade higher than the final top performing systems .Methods .This section discusses the methods used to prepare the evaluation materials .Data preparation .In order to evaluate the performance of the systems , the organizers prepared a hand - coded gold standard , as described in [ 3 ] .First , each abstract was associated with the gene ID list from the appropriate model organism database .Since we were using abstracts rather than full text , the gene list from the model organism database then had to be adjusted to conform to the names mentioned in the abstract .", "label": "", "metadata": {}}
{"text": "Removing gene IDs that were not found in the abstract , but were found in the underlying full text article .This was done automatically , using the synonym list , to generate large quantities of \" noisy \" training data .This corresponds to the Noisy Training column on the tables for the model organism performance data .Hand checking to make sure that the automatic procedure did not eliminate genes that were present in the abstract ( development test set and blind test set only ) .This could occur if , for example , the mention in the text was a variant of the synonyms provided in the lexical resource , e.g. , \" polgamma B \" versus \" polgamma 2 \" .", "label": "", "metadata": {}}
{"text": "This was necessary because each model organism database curates genes according to a certain set of criteria , so not all genes mentioned are necessarily on the gene list .There might , for example , be additional genes mentioned \" in passing , \" such as genes located near a gene of interest , or possible homologues etc . .Overall , we estimate that it took between 1 - 2 staff weeks of time from an experienced curator to edit and check a 250 abstract test set .The checking was particularly important because we detected significant interannotator variability , particularly for the Mouse annotations - see [ 3 ] for a detailed discussion of the data preparation and interannotation agreement studies .", "label": "", "metadata": {}}
{"text": "An analysis of the lexical resources provides insight into the differences in difficulty observed for the three organisms .Table 5 gives a picture of the amount of synonymy in the different lexicons .It shows the number of unique identifiers ( IDs ) , the number of terms in the lexicon , and the average number of terms per identifier ( synonyms ) for each organism .We can see that the Yeast resources are the most parsimonious ( 1.9 synonyms per ID ) .Fly is the richest with 2.9 synonyms per ID , but note also the large standard deviation of 3.9 : 42 % of Fly identifiers have only one term and only 15 % have more than 4 synonyms per ID .", "label": "", "metadata": {}}
{"text": "Again , Yeast is very compact , with barely over one word per term ; this almost certainly contributed to the high performance on Yeast .Mouse has the longest terms on average , at 2.77 words per synonym , but with a large standard deviation ( 2.6 ) Overall , 58 % of Mouse terms were one word long and 81 % of the terms were four words long or less .The complexity of the Mouse terms ( as measured by length ) may have contributed to recall problems in identifying gene mentions , since longer names tend to be more descriptive and therefore , to show significant syntactic variation .", "label": "", "metadata": {}}
{"text": "The resources for these organisms also differ in amount of ambiguity among the terms , as shown in Table 6 .The 4 th column of this table lists the absolute number of terms that were associated with multiple gene identifiers .Again we observe that Yeast is the least ambiguous ( 168 terms and an average of 1.013 identifiers per term , column 5 ) , while Fly , with the most terms on average per gene , is also the most ambiguous , at 1.085 gene identifiers per terms .Again , Fly has the largest standard deviation : only 3.6 % of Fly terms are ambiguous - the remaining 96.4 % of Fly terms are associated with a single ID .", "label": "", "metadata": {}}
{"text": "Lexical resources for Yeast , Fly and Mouse : identifiers , terms , and ambiguity .Figure 3 shows the distribution of terms associated with multiple gene identifiers as a log - log plot of number of terms plotted against degree of ambiguity for all three organisms .Distribution of ambiguous synonyms in Fly , Mouse and Yeast task 1B lexical resources .In addition , Table 6 shows the ambiguity between gene terms and English vocabulary .The 6 th column shows the absolute number of synonyms that overlap with the 5000 most common English words , and the last column shows the average number of ambiguities per synonym ( measured against the list of 5000 common words ) .", "label": "", "metadata": {}}
{"text": "Again , we see that there is very little overlap with English for Yeast ( 2 terms out of 15,000 ) , it is much higher for Mouse ( 205 out of 53,000 terms ) and higher still for Fly ( 396 terms out of 28,000 ) .These figures correlate with the differences in difficulty between Yeast , Fly and Mouse .Yeast was relatively easy , with few problems of ambiguity ; Fly and Mouse were both significantly harder , for somewhat different reasons .The Fly lexical resources had the most terms per gene ID , and were also the most ambiguous ( with respect to gene identifiers and also with respect to overlap with regular English words ) .", "label": "", "metadata": {}}
{"text": "This may mean that there were variants of complex names that did not appear in the lexicon , requiring more complex procedures to match gene mention and gene ID .However , this was offset in part by the fact that Mouse had less ambiguity than in Fly .Finally , Mouse had the most noisily annotated training data ( recall estimated at 55 % ) , which may have contributed to the difficulty of that task .Discussion .There were eight groups participating in task 1B ; 7 groups submitted 15 systems for Yeast ; 6 groups submitted 11 systems for Fly ; and 7 groups submitted 16 systems for Mouse .", "label": "", "metadata": {}}
{"text": "Four systems are documented in articles in this issue [ 7 - 10 ] .For descriptions of the other two systems , see [ 11 , 12 ] in the BioCreAtIvE Workshop Handout [ 13 ] .The remainder of this section discusses the challenges presented by task 1B and how the participating systems approached these challenges .Technical challenges for Task 1B .The requirements for task 1B can be divided into four steps : .Identifying gene mentions in the text .Associating gene mentions to one or more unique gene identifiers .Selecting the correct gene identifier in cases of ambiguity .", "label": "", "metadata": {}}
{"text": "These steps were highly interdependent .There are complex recall / precision trade - offs that occur in capturing candidate gene mentions and in assigning a unique ( and correct ) gene identifier to these mentions .This is because of significant ambiguity among gene terms ( one word might be a term for multiple genes ) and also because of significant overlap between gene synonyms ( \" white \" , \" dorsal \" ) and English vocabulary .For example , the entry for FBgn0000009 consists of the terms \" A \" , \" Abnormal \" and \" Abnormal abdomen \" .", "label": "", "metadata": {}}
{"text": "Furthermore , there are some 20 other genes that have the term \" A \" as one of their allowed synonyms .Complicating this further , the term lists provided by the model organism databases , while extensive , were by no means exhaustive .As noted above , the lexical resources differed by organism in number of terms per gene identifier and in ambiguity of terms within the resource .Precision errors could be caused by : .False alarms for gene mentions ( for example , taking an English word to be a gene name ) ; .", "label": "", "metadata": {}}
{"text": "Assignment of gene identifiers to genes from non - relevant organisms ( e.g. , human genes are often discussed in Mouse abstracts , but should not be entered into the gene list ) .Recall errors could be caused by : .Failure to recognize a gene mention ( perhaps due to mismatch with the organism - specific synonym list ) .Incorrect disambiguation of ambiguous gene names .Finding gene mentions .The participating groups took a variety of approaches to these challenges .For gene mentions , the approaches fell into roughly two groups : .Matching against the lexical resource ; in many cases , an approximate matching approach was used .", "label": "", "metadata": {}}
{"text": "This was followed by application of a classifier to select the candidates to appear on the final normalized gene list .The approach described in [ 12 ] used an enriched lexical resource to achieve high recall ( but lower precision ) results for Mouse and Yeast .Gene mention identification as done for task 1A , adapted to the three specific organisms in 1B [ 11 ] .To do this , Hachey et al used a technique to generate \" noisy \" training data similar to that described in [ 14 ] .Association with unique gene identifier .", "label": "", "metadata": {}}
{"text": "For groups that used a task 1A - type gene mention tagger , they were then able to use the table look up to filter out erroneous gene mention candidates .However , recall at this step was limited by the completeness of the synonym list from the model organism database .While the term lists contained many variant forms ( see the example with Est-6 in Figure 1 ) , there were still more variations that had to be handled .The incompleteness of the lexical resources could lead to recall errors .This was also the stage at which ambiguity was flagged , since some terms could refer to multiple genes ( see Table 6 ) .", "label": "", "metadata": {}}
{"text": "The systematic editing and expansion of the underlying lexical resources was at the core of two high performing systems [ 7 , 9 ] .Both Tamames [ 10 ] and Liu [ 12 ] used the same tokenization for the lexicon as was used for the gene mention identification ; both systems also used stemming to improve the matching between lexicon terms and candidate gene names in the text .For Tamames [ 10 ] , these stages were also combined .Disambiguation .The next stage , disambiguation for gene synonyms associated with multiple identifiers , turned out to be the most interesting feature of task 1B. The extensive ambiguity of gene names , particularly for Fly and to a lesser extent , for Mouse ( see Figure 3 ) , required that systems include techniques for disambiguation .", "label": "", "metadata": {}}
{"text": "Pruning the lexicon was an attractive option , given the highly skewed distribution of ambiguity in both Mouse and Fly .For Mouse , there were 1900 ambiguous terms ( out of 126,000 - 1.5 % ) ; for Fly , there were 2700 out of 75,000 ambiguous terms ( 3.6 % ) .Hanisch et al .[ 7 ] used a multi - stage process that included correlating abbreviations with their long forms and also a filter for abstracts based on organism specificity .Liu [ 12 ] used features derived from rich lexical resources to create feature vectors used in word sense disambiguation .", "label": "", "metadata": {}}
{"text": "[ 8 ] followed their high recall pattern matching system with a maximum entropy classifier trained to distinguish correct matches from bad matches .Hachey et al [ 11 ] used information retrieval techniques to associate candidate gene identifiers with term frequencies in a document .They used this to filter gene identifiers for a given abstract , based on similarity to term occurrences associated with the gene identifiers in abstracts from the training data .Generating the final gene list .Once these stages were completed , the systems assembled the final gene list for each abstract as output .", "label": "", "metadata": {}}
{"text": "Increasing the threshold traded recall for precision , e.g. , in [ 7 ] and [ 12 ] .One group [ 8 ] was able to achieve reasonable performance ( well above the median of the reported systems ) using a single approach across all three organisms , based on high recall pattern matching , followed by a maximum entropy classifier for remove bad matches .Many groups found that it was possible to use much simpler techniques for Yeast than for Mouse or Fly , due to the more tightly constrained nomenclature .Conclusion .BioCreAtIvE demonstrated the ability of automated systems to do gene normalization for a range of organisms , given a simple lexical resource consisting of the set of unique gene identifiers and their names and synonyms , and a corpus ( 5000 abstracts ) of noisy training data .", "label": "", "metadata": {}}
{"text": "Factors included the number of genes , the number of synonyms per gene identifier , the consistency of naming conventions , the length and complexity of names , and the degree of ambiguity in the naming conventions .The more ambiguity ( among genes , between genes and English ) and the more complex the names ( descriptions versus simple gene symbols ) , the harder the problem .Yeast naming is relatively simple and regular - and good performance could be achieved with relatively simple methods ( such as expanded lexical look - up ) .Mouse is hard because names are often long and descriptive , subject to many variants ( grammatical as well as syntactic and typographic ) .", "label": "", "metadata": {}}
{"text": "Overall , we judged that the BioCreAtIvE task 1B evaluation was a success .We attracted 8 groups from five countries with participation from some of the major groups involved in information extraction in biology .Results demonstrated that the generation of normalized gene lists is well within the range of current technology , although further experiments are needed to determine what performance would be required for a production system used in some semi - automated curation pipeline .The task raised some interesting research questions : .How to achieve high recall - achieving high precision seems relatively easy , but only one system achieved high recall , at the expense of precision [ 12 ] .", "label": "", "metadata": {}}
{"text": "This problem requires word sense disambiguation , but this is a new way of framing the problem that should provide an interesting testing ground for various approaches to the problem .How to do rapid adaptation to different task domains , given appropriate lexical resources ( synonym list for the organism gene identifiers ) .Some of the successful systems found that the different organisms benefited from somewhat different approaches .And several systems made use of additional lexical resources .Only one group tried to apply a uniform method across all three organisms [ 8 ] , with interesting results .", "label": "", "metadata": {}}
{"text": "This reduced the cost of data preparation significantly , but the cost of preparing the training and test sets was greater than we expected : 1 - 2 person weeks of expert annotator time for a 250 abstract test set .And the difficulties of achieving reliable interannotator agreement were greater than we expected [ 3 ] .The training and test data are now available for other groups to use in further experiments .As we begin to think about a follow on evaluation , the question arises : should this task be repeated ?The real task that curators perform uses full text articles ( not abstracts , although the Yeast curators do curate from abstracts most of the time ) .", "label": "", "metadata": {}}
{"text": "It would be far easier for the organizers to prepare \" real \" data sets , because it would require none of the editing that was performed for this year 's BioCreAtIvE task 1B. On the other hand , it would be harder for the participants , because they would have to handle full text and they would have to replicate biological decisions in terms of which genes to list .In conclusion , we look forward to receiving feedback from the participants in defining follow - on tasks for the next BioCreAtIvE evaluation .Declarations .Acknowledgements .This paper reports on work done in part at the MITRE Corporation under the support of the MITRE Sponsored Research Program and the National Science Foundation ( contract number EIA-0326404 ) .", "label": "", "metadata": {}}
{"text": "All rights reserved .Authors ' Affiliations .References .Yeh AS , Morgan A , Colosimo M , Hirschman L : BioCreAtIvE task 1A : gene mention finding evaluation .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S2 .View Article PubMed .Blaschke C , Leon EA , Krallinger M , Valencia A : Evaluation of BioCreAtIvE assessment of task 2 .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S16 .View Article PubMed .Colosimo M , Morgan A , Yeh A , Colombe J , Hirschman L : Data Preparation and Interannotator Agreement : BioCreAtIvE Task 1B. BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S12 .", "label": "", "metadata": {}}
{"text": "Hanisch D , Fundel K , Mevissen H - T , Zimmer R , Fluck J : ProMiner : Organism - specific protein name detection using approximate string matching .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S14 .View Article PubMed .Crim J , McDonald R , Pereira F : Automatically Annotating Documents with Normalized Gene Lists .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S13 .View Article PubMed .Fundel K , G\u00fcttler D , Zimmer R , Apostolakis J : A simple approach for protein name identification : prospects and limits .", "label": "", "metadata": {}}
{"text": "View Article PubMed .Tamames J : Text Detective : Text Dectective : A rule - based system for gene annotation in biomedical texts .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S10 .View Article PubMed .Morgan A , Hirschman L , Colosimo M , Yeh A , Colombe J : Gene Name Identification and Normalization Using a Model Organism Database .J Biomedical Informatics 2004 , 37 : 396 - 410 .View Article .Copyright .\u00a9 Hirschman et al 2005 .This article is published under license to BioMed Central Ltd. Special circumstance allegations mean death sentence could be possible .", "label": "", "metadata": {}}
{"text": "Laura Lutrell Purviance , 25 , wearing a yellow jail - issued top , was led handcuffed into Van Nuys Superior Court Tuesday for a preliminary hearing to determine if she should be tried for her mother 's murder .Purviance was held to answer to one count of murder with a special circumstance allegation of lying in wait following the preliminary hearing presided over by Los Angeles County Superior Court Judge Michael Kellogg .She is scheduled to be arraigned Feb. 18 .Sitting erect beside her lawyer , wearing glasses and looking at detectives testifying about the Feb. 13 , 2013 , shooting , Purviance watched the proceedings intently .", "label": "", "metadata": {}}
{"text": "Three homicide detectives testified at her daughter 's preliminary hearing , including one veteran of the Los Angeles Police Department who said he received a phone call from Laura Purviance at 3 a.m. Feb. 15 , 2013 .The two talked for 45 minutes , said Detective Jeffrey Briscoe .\" She told me she was sorry and that she shot her mom , \" he said .Laura Purviance , who attended Saugus High in the early 200s but transferred to Sequoia Charter School her senior year , was apprehended in Oregon the same day she talked to Briscoe .", "label": "", "metadata": {}}
{"text": "At one point during questioning , Deputy District Attorney Elizabeth Ratinoff asked Briscoe how long Laura Purviance had planned to kill her mother . \"As I recall , she had been contemplating it most of her life , \" Briscoe said .\" It was n't just a spur of the moment .She decided now was the time to put her plan into action . \"According to what Purviance reportedly told Briscoe , she put her plan into action the day she asked her mother to drive her to her boyfriend 's house in Hollywood .\"", "label": "", "metadata": {}}
{"text": "She said she needed a book for school and she wrapped the gun into a shirt and placed in it in a messenger bag , \" Briscoe said .\"She said she got the gun that was under the bed . \"After Laura Purviance secured the gun , she and her mother returned to the daughter 's Woodland Hills home and she offered her Carolyn Purviance lemon bars , her favorite , Briscoe said .While her mother was sitting on the bed making notes about new cabinets for the residence , the daughter took the gun into a bathroom to load it , running the water to disguise the \" click \" of chambering a bullet , Briscoe said .", "label": "", "metadata": {}}
