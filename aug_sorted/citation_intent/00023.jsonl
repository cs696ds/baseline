{"text": "Relative Importance of TM and LM .In the previous section , experiments have been run using the same training set size for language and translation models .In general , there is a large difference in terms of cost of retrieving training data for language and translation models ; the former can be trained using monolingual data , while the second needs bilingual texts .", "label": "", "metadata": {}, "score": "40.829025"}
{"text": "Development and test sets are fixed .One instance of the SMT system has been run for each of all possible combinations of the language and translation training data sizes .BLEU score value has been associated to each pair : language and translation set size .", "label": "", "metadata": {}, "score": "42.287376"}
{"text": "This setting has been applied to Europarl and Giga corpus datasets using Moses as SMT system . vary across the datasets and correspond to an increase of 1.3 to 1.5 BLEU point for the LM and 1.8 to 1.9 for the TM , for each doubling of the data .", "label": "", "metadata": {}, "score": "43.62457"}
{"text": "Authors in [ 30 ] reported \" almost linear \" improvements in BLEU score by doubling the training set size .In the presentation [ 32 ] , the claim is that BLEU increases with each doubling of the training set size , by 0.5 and 2.5 BLEU points for the language and translation models , respectively , in the context of Arabic - English translation .", "label": "", "metadata": {}, "score": "45.08108"}
{"text": "The optimization procedure increases the quality of the translations .This improvement does not seem to be significant after a certain size of the development set .In fact , when we increase the development set size beyond 2,000 sentence pairs , BLEU does not change significantly .", "label": "", "metadata": {}, "score": "45.081783"}
{"text": "A rapid decline should be expected .However , the mapping between words is stored in a very redundant way within the TM , and this depends on the way the translation table is created , based on sentence alignments .Once an alignment has been found between two sentences , essentially every n -gram ( for every value of n ) is a candidate for insertion in the translation table .", "label": "", "metadata": {}, "score": "45.123024"}
{"text": "Both learning curves show a big improvement when moving from the word - to - word translation ( phrase length equal to one ) to the phrase - based model ( higher phrase lengths ) .Figure 4 : BLEU versus n -gram length .", "label": "", "metadata": {}, "score": "45.274803"}
{"text": "The final BLEU score evaluation report shows how well the machine translations match the reference translations .A training corpus with each phrase annotated with the hierarchical structure of the language , such as parts of speech , word function , etc . .", "label": "", "metadata": {}, "score": "45.75785"}
{"text": "There are two alignment processes .In corpus preparation , the alignment process creates aligned data .During training , the alignment process uses a program such as MGIZA++ to create word alignment files .BLEU score .SMT .BLEU stands for B i-", "label": "", "metadata": {}, "score": "46.002266"}
{"text": "When extracting longer phrases , we expect training set performance to be higher , but test performance to drop ( overfitting ) .Optimizing test performance requires the right trade - off .In this section , we analyze how the phrase length can affect the performance in terms of BLEU score .", "label": "", "metadata": {}, "score": "47.704872"}
{"text": "In each case , we count the number of phrases of each length that were actually used to produce the translation .The right panel of Figure 5 reports these distribution .It shows that , while the models use a fair amount of longer phrases to translate the training material , these longer phrases are essentially never used for translating the test set : 98 % of the phrases are 5-grams or shorter .", "label": "", "metadata": {}, "score": "47.93648"}
{"text": "A.3 .Role of Test Set Size on Measuring Performance .BLEU score , the metric used in this work to evaluate the quality of the translation , is test set dependent .It means that different test sets regardless of the dimension can produce variation in the value of the BLEU score .", "label": "", "metadata": {}, "score": "48.005547"}
{"text": "This new network had an improved localization accuracy of 0.87 cm , while localization time was lengthened to about 800 microseconds .", "label": "", "metadata": {}, "score": "48.351536"}
{"text": "Small test set sizes produce a big variance in BLEU score .When increasing the test set size , the error bars tend to reduce .using two different test sets of the same size depend on the test set choice and not on different techniques .", "label": "", "metadata": {}, "score": "48.538147"}
{"text": "We will later analyze the contribution of each component to the overall score .The typical processing pipeline is as follows .Given a parallel training corpus , long sentences are filtered out , and the remaining material is lowercased and tokenized .These sentences are used to train the language and translation models .", "label": "", "metadata": {}, "score": "48.563538"}
{"text": "Figure 5 shows that the number of phrases peaks around 4-grams and 5-grams , then steadily decreases .This means that the phrase extraction algorithm finds it more and more difficult to extract longer phrases .We investigate this further by plotting the distribution of phrases actually used while translating .", "label": "", "metadata": {}, "score": "48.609844"}
{"text": "Our results suggest that performance , as measured by BLEU , increases by a constant factor for each doubling of the data .Although that factor varies depending on corpus and language pair , this result seems consistent over all experimental conditions we tried .", "label": "", "metadata": {}, "score": "49.072872"}
{"text": "Table 5 : Performance obtained training the regressor on 80 % of the data and testing on 20 % .This process has been iterated 1,000 times .Experiments have been performed independently on the Europarl and Giga corpus dataset .Role of Phrase Length in the Translation Table ( Model Selection ) .", "label": "", "metadata": {}, "score": "49.088966"}
{"text": "This is useful because the language model feature typically favours shorter sentences ( because each additional trigram can only lower the language model probability ) .This is a simple , yet effective feature .The process of training a machine translation system involves estimating the various parameters of the model : the log - linear parameters . as well as the parameters internal to the feature functions , such as the phrase translation probabilities and language model n -gram and backoff probabilities .", "label": "", "metadata": {}, "score": "49.114155"}
{"text": "This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse - trees of source and target language sentences .", "label": "", "metadata": {}, "score": "50.040833"}
{"text": "We show for the first time that incorporating the predictions of a word sense disambigua - tion system within a typical phrase - based statistical machine translation ( SMT ) model consistently improves translation quality across all three different IWSLT Chinese - English test sets , as well as producing st ... \" .", "label": "", "metadata": {}, "score": "50.231102"}
{"text": "They contain different modules to preprocess data and train the language models and the translation models .These models can be tuned using minimum error rate training [ 17 ] .Both use standard external tools for training the language model , such as SRILM [ 23 ] , and Moses also uses GIZA++ [ 24 ] for word alignments .", "label": "", "metadata": {}, "score": "50.82691"}
{"text": "These experiments suggest that the phrase length has a limited impact on actual test performance .Going to larger n- grams seems to bring little benefit in terms of performance as the model continues to prefer short phrases during the decoding phase .This is due to both the diminishing number of longer phrases in the table and to the lower probability that these longer sequences match the test material .", "label": "", "metadata": {}, "score": "50.89892"}
{"text": "The language pairs cover European as well as non - European languages , and the sizes range from 1.2 M to 22.5 M sentence pairs .We expect that translation between European languages will be easier than from Chinese to English ; however , we are not so much interested in the actual translation performance as in the way this performance evolves with increasing data and under a number of conditions .", "label": "", "metadata": {}, "score": "51.21541"}
{"text": "Any way to enforce linguistic constraints might result in a reduced need for data , and ultimately in more complete models , given the same corpus [ 34 ] .Neither approach would change the statistical nature of the system , but they would help it bypass the phrase acquisition bottleneck .", "label": "", "metadata": {}, "score": "51.253517"}
{"text": "With this small model , there are many untranslated words , and the quality of the translations is very low .In the next steps , we 'll show you how to train a model for a new language pair , using a larger training corpus that will result in higher quality translations .", "label": "", "metadata": {}, "score": "51.42817"}
{"text": "Statistics on the phrase pair are accumulated over the entire corpus .In our experiments below , we rely on word - to - word IBM models [ 2 ] for alignment .Although more elaborate techniques have appeared more recently [ 13 , 14 ] , their impact on the resulting machine translation quality is still unclear [ 15 ] .", "label": "", "metadata": {}, "score": "51.47252"}
{"text": "In any case , the addition of massive amounts of data from the same distribution will result in small improvements in the performance .The small error bars that we have obtained also allow us to regard the stability of the SMT when trained on the same training set size .", "label": "", "metadata": {}, "score": "51.482304"}
{"text": "At the tuning step , an LM is built from the target side of the training data ( unless --no - corpus - lm is specified ) .This controls which code is used to build it .The default is a BerkeleyLM java class that computes a Kneser - Ney LM with a constant discounting and no count thresholding .", "label": "", "metadata": {}, "score": "51.627106"}
{"text": "Clearly , all measures correlate strongly with each other , such that the choice of the performance measure is fairly arbitrary , as long as one is consistent .For this reason , we have chosen to use BLEU throughout this paper as it is the most widely used automatic score in machine translation .", "label": "", "metadata": {}, "score": "51.62796"}
{"text": "The training part is used to obtain the language model and phrase tables .The development set is used to estimate the log - linear weights . using MERT , and the test set is set aside during the estimation process in order to provide an unbiased estimate of the translation performance .", "label": "", "metadata": {}, "score": "51.70485"}
{"text": "A \" language model \" or \" lm \" is a statistical description of one language that includes the frequencies of token - based n - grams occurrences in a corpus .The \" lm \" is trained from a large monolingual corpus and saved as a file .", "label": "", "metadata": {}, "score": "51.70984"}
{"text": "Some will be quick to point out that maximizing , for example , BLEU may neither be necessary for , nor guarantee good translation performance .Although we acknowledge that automatic MT metrics may not tell the whole story as far as translation quality is concerned , our systematic study aims at characterizing the behaviour of SMT systems that are built by maximizing such metrics .", "label": "", "metadata": {}, "score": "51.918453"}
{"text": "In a third set of experiments , we determined that the estimation of the translation model has a bigger effect than the estimation of the language model , on performance .We therefore reach the conclusion that estimating entries in the phrase translation tables is the dominant factor in determining performance .", "label": "", "metadata": {}, "score": "52.410606"}
{"text": "We have isolated 4,000 pairs of sentences from the Europarl training set , and we have selected from the remaining part 629,957 pairs .A Moses model is trained using this set .Using the 4,000 sentences pairs , we have created 10 random subsets for each of the 16 chosen sizes , where each size can contain a number of pairs from 250 to 4,000 by a step of 250 pairs .", "label": "", "metadata": {}, "score": "52.809196"}
{"text": "Our findings are also consistent with the curves presented by [ 33 ] , although their results are limited to a much lower data set size ( less than . sentences ) and presented on a linear scale .Incidentally , that paper also presents a recent attempt into using active learning for improving MT and meets the challenge of \" diminishing returns ' ' identified in the learning curves : a constant performance improvement requires increasing amounts of data .", "label": "", "metadata": {}, "score": "52.913696"}
{"text": "More detail can be found in Chiang ( 2007 ) [ PDF ] .SAMT grammars make use of a source- or target - side parse tree on the training data , projecting constituent labels down on the phrasal alignments in a variety of configurations .", "label": "", "metadata": {}, "score": "53.04381"}
{"text": "This learning curve reports BLEU score versus the percentage of perturbation applied .These results have been obtained using a fixed training set size equal to 62,995 and 629,957 pairs of sentences and Moses as translation system .Figure 7 : Unlearning curves .", "label": "", "metadata": {}, "score": "53.1203"}
{"text": "If you want a language model built from the target side of your training data , you 'll also need to pass in the training corpus ( --corpus ) .You can also specify an arbitrary number of additional language models with one or more --lmfile flags .", "label": "", "metadata": {}, "score": "53.12661"}
{"text": "/usr / bin / perl # # truecase - map .cat training / training . map .Finally , recase the lowercased 1-best translation by running the SRILM disambig program , which takes the map of alternative capitalizations , creates a confusion network , and uses truecased LM to find the best path through it : .", "label": "", "metadata": {}, "score": "53.189384"}
{"text": "When you are aligning tens of millions of words worth of data , the word alignment process will take several hours to complete .While it is running , you can skip ahead and complete step 4 , but not step 5 .Step 4 : Train a language model .", "label": "", "metadata": {}, "score": "53.297615"}
{"text": "In Section 3.4 , we report the correlation coefficient between all the measures .Each correlation coefficient is computed using the results of the 160 experiments described above .Acknowledgments .This work is supported by the EU IST Project SMART ( FP6 - 033917 ) .", "label": "", "metadata": {}, "score": "53.32906"}
{"text": "Our expe ... \" .In this paper , we propose a novel string - todependency algorithm for statistical machine translation .With this new framework , we employ a target dependency language model during decoding to exploit long distance word relations , which are unavailable with a traditional n - gram language model .", "label": "", "metadata": {}, "score": "53.402756"}
{"text": "We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data . ...English sentences with a state - of - the - art statistical parser ( Collins , 1999 ) .", "label": "", "metadata": {}, "score": "53.63055"}
{"text": "When the models have been created , the development set is used to run the minimum error rate training ( MERT ) algorithm [ 17 ] to optimize their weights .We refer to that step as the optimization step in the rest of the paper .", "label": "", "metadata": {}, "score": "53.71134"}
{"text": "One , containing 1,159,914 pairs of sentences , has been used to train the model .This step has been done only once , and all the experiments use the same translation , language , and reordering models .The second set has 100,000 pairs of sentences , and it is used to randomly select the development sets .", "label": "", "metadata": {}, "score": "53.733887"}
{"text": "So if we remove an n -gram , chances are that other similar ( longer or shorter ) n -grams are present and can take over .In this way , it is not possible to directly compare the unlearning curve for the n -grams part with that for the numeric part of the tables .", "label": "", "metadata": {}, "score": "53.873287"}
{"text": "Much research has focused on devising improved principles for the statistical estimation of the parameters in language and translation models .The introduction of discriminative graphical models has marked a departure from traditional maximum likelihood estimation principles , and various approaches have been proposed .", "label": "", "metadata": {}, "score": "54.17488"}
{"text": "Our experiments show that the estimation part of the error is the dominant one , suggesting that performance can still improve if the appropriate entries were available in the language and translation models .The second concern was to distinguish between the role of the numerical and lexical parts in the language and translation models .", "label": "", "metadata": {}, "score": "54.726646"}
{"text": "In ( a ) , \" words swap TM ' ' has been obtained by swapping the target phrases inside the TM .In ( b ) , two unlearning curves have been compared . \"Numerical swap LM ' ' has been obtained applying numerical swaps only to the LM and \" numerical swap TM ' ' applying numerical swaps only to the LM .", "label": "", "metadata": {}, "score": "54.867847"}
{"text": "We have undertaken a large scale experimental and theoretical investigation of these questions .We use this data to inform a discussion about learning curves .We have also investigated the model - selection properties of n -gram size , where the n -grams are the phrases used as building blocks in the translation process .", "label": "", "metadata": {}, "score": "54.89066"}
{"text": "For each subset , a new instance of the PBSMT system has been created , for a total of 200 models .Two hundred experiments have then been run on an independent test set ( of 2,000 sentences , also not included in any other phase of the experiment ) .", "label": "", "metadata": {}, "score": "54.915607"}
{"text": "Work related to our learning curve experiments can also be found in [ 10 ] .It is important to remark that while there are many discussions about automatic evaluation of SMT systems , this work does not consider them .We work within the well - defined setting where a loss function has been agreed upon , that can measure the similarity between two sentences , and a paired training set has been provided .", "label": "", "metadata": {}, "score": "55.030228"}
{"text": "We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser . \" ...Previous work has used monolingual parallel corpora to extract and generate paraphrases .", "label": "", "metadata": {}, "score": "55.04508"}
{"text": "es.tok.lc 671429 16721564 training / subsampled / subsample .en.tok.lc671429 17670846 training / subsampled / subsample .es.tok.lc .Step 3 : Create word alignments .Before extracting a translation grammar , we first need to create word alignments for our parallel corpus .", "label": "", "metadata": {}, "score": "55.066074"}
{"text": "Yet SMT translation qual - ity still obviously suffers from inaccurate lexical choice .In this paper , we address this problem by investigating a new strat - egy for integrating WSD into an SMT sys - tem , that performs fully phrasal multi - word disambiguation .", "label": "", "metadata": {}, "score": "55.17062"}
{"text": "In addition to the SMT decoder , the toolki ... \" .We describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models .", "label": "", "metadata": {}, "score": "55.21308"}
{"text": "Data .We used three different sentence - aligned corpora , covering different language pairs and sizes : ( 1 ) Europarl Release v3 Spanish - English [ 7 ] , ( 2 ) UN Chinese - English corpus provided by the Linguistic Data Consortium , ( 3 ) Giga corpus French - English [ 8 ] .", "label": "", "metadata": {}, "score": "55.364014"}
{"text": ", ten experiments have been run .In this case , we randomly select two fixed training set sizes equal to 62,995 and 629,957 pairs of sentences form the Europarl corpora and use Moses as translation system .The unlearning curves are shown in Figure 6 .", "label": "", "metadata": {}, "score": "55.46209"}
{"text": "Although many language pairs would yield different translation performance , in this paper , we are not interested in the translation performance per se : we focus our attention on analyzing the SMT system as a learning system .Since our goal was to obtain high - accuracy learning curves , that can be trusted both for comparing different system settings and to extrapolate performance under unseen conditions , we conducted a large - scale series of tests , to reduce uncertainty in the estimations and to obtain the strongest possible signals .", "label": "", "metadata": {}, "score": "55.553535"}
{"text": "BLEU and NIST are based on averaging n -gram precisions , combined with a length penalty which penalizes short translations containing only sure words .These metrics differ on the way the precisions are combined and on the length penalty .Meteor evaluates a translation by computing a score based on the word alignment between the translation and a given reference translation .", "label": "", "metadata": {}, "score": "55.756516"}
{"text": "In practice this trade - off is easily observed , by noticing how the training error can be driven to zero by using a rich hypothesis class , which typically results into overfitting and increased test error .In the context of statistical machine translation ( SMT ) , where large bilingual corpora are used to train adaptive software to translate text , this task is further complicated by the peculiar distribution underlying the data , where the probability of encountering new words or expressions never vanishes .", "label": "", "metadata": {}, "score": "55.76161"}
{"text": "On test data extracted by the heuristic strategy , however , performance of the two training sets is similar , with AERs of 13.2 % and 14.7 % respectively .Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase .", "label": "", "metadata": {}, "score": "56.13771"}
{"text": "It is unlike other unsupervised word alignment tools in that it is able to create a phrase table using a fully statistical model , no heuristics .As a result , it is able to build phrase tables for phrase - based machine translation that achieve competitive results but are only a fraction of the size of those created with heuristic methods .", "label": "", "metadata": {}, "score": "56.18071"}
{"text": "This is because the rate of improvement of translation performance is at best logarithmic with the training set size .We estimate that bridging the gap between training and test error would require about .paired bilingual sentences , which is larger than the current estimated size of the web .", "label": "", "metadata": {}, "score": "56.18564"}
{"text": "Although the rate of improvement may depend on both the data and the estimation method , it is unlikely that the general shape of the learning curve will change without major changes in the modeling and inference phases .Possible research directions that address this issue include the integration of linguistic rules or the development of active learning procedures .", "label": "", "metadata": {}, "score": "56.30629"}
{"text": "Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy that pairs initial ( presumably summary ) sentences from different news stories in the same cluster .We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation .", "label": "", "metadata": {}, "score": "56.409164"}
{"text": "The grammar extraction step takes three pieces of data : ( 1 ) the source - language training corpus , ( 2 ) the target - language training corpus ( parsed , if an SAMT grammar is being extracted ) , and ( 3 ) the alignment file .", "label": "", "metadata": {}, "score": "56.442444"}
{"text": "This yields an efficient algorithm for obtaining the exact solution of each line search in Powell 's method and therefore provides a way to iteratively optimize the log - linear weights . using MERT .A number of alternatives have been proposed , such as on - line discriminative training [ 19 , 20 ] .", "label": "", "metadata": {}, "score": "56.60757"}
{"text": "The high performance observed in the train - on - test conditions shows that there exists at least one choice of tunable parameters with which the phrase - based translation system can deliver much higher performance .This is useful to bound the space of \" possible performances , ' ' although in ideal situations .", "label": "", "metadata": {}, "score": "56.823814"}
{"text": "Our results provide the first known empir - ical evidence that lexical semantics are in - deed useful for SMT , despite claims to the contrary . \" ...In this paper , we propose a novel string - todependency algorithm for statistical machine translation .", "label": "", "metadata": {}, "score": "56.929764"}
{"text": "The log - linear parameters are then estimated by minimum error rate training ( MERT ) .The weights .is the set of sentence pairs over which MERT is performed .Solving ( 3 ) is difficult because the decoding necessary to produce the hypothesis translation is expensive .", "label": "", "metadata": {}, "score": "57.151928"}
{"text": "SRILM is a toolkit for building and applying statistical language models ( LMs ) , primarily for use in speech recognition , statistical tagging and segmentation , and machine translation .Tools . by Hieu Hoang , Alexandra Birch , Chris Callison - burch , Richard Zens , Marcello Federico , Nicola Bertoldi , Chris Dyer , Brooke Cowan , Wade Shen , Christine Moran , Ond\u0159ej Bojar , Alexandra Constantin , Evan Herbst , 2007 . \" ...", "label": "", "metadata": {}, "score": "57.280647"}
{"text": "A recaser model is a special translation model translates lower cased data to \" natural \" cased text ( upper and lower casing ) .reordering table .SMT .A \" reordering table \" contains the statistical frequencies that describe the changes in word order between source and target languages , such as \" big house \" versus \" house big \" .", "label": "", "metadata": {}, "score": "57.36432"}
{"text": "Figure 3 : Chinese - English learning curve obtained using UN corpus and Portage .In all the figures , the curves are increasing linearly or slightly more slowly than that , suggesting a learning curve that is \" at best ' ' logarithmically increasing with the training set size .", "label": "", "metadata": {}, "score": "57.43553"}
{"text": "\" Test on training set ' ' is a test set selected by the training set for each training set size and no optimization phase .In the \" test on test set ' ' learning curve , there seems to be no significant advantage to using phrases longer than 4 words .", "label": "", "metadata": {}, "score": "57.45029"}
{"text": "The first statistical models were word based [ 2 , 11 ] , combining a Markovian language model with a generative word - to - word translation model , in a noisy channel model inspired by speech recognition research .Current state - of - the - art SMT uses phrase - based models , which generalized and superseded word - based models .", "label": "", "metadata": {}, "score": "57.635933"}
{"text": "Note that these are not phrases in the linguistic sense , but simply subsequences of words .For a sentence .referred to as a phrase table .Part of the overall MT training process is to estimate this table and the associated probabilities .", "label": "", "metadata": {}, "score": "57.67486"}
{"text": "We investigate unsupervised techniques for acquiring monolingual sentence - level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web - based news sources .Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy ... \" .", "label": "", "metadata": {}, "score": "57.84931"}
{"text": "Effect of Data Size in Optimization Set .In this section , we study the role of the optimization / development set with regard to the quality of translation .In particular , we analyze how different sizes of the development set affect the performance and the computational cost of the optimization phase .", "label": "", "metadata": {}, "score": "57.88119"}
{"text": "In other words , the essential limiting factor for phrase - based SMT systems seems to be the Zipf law found in natural language .Our large - scale analysis also suggests that the current bottleneck of the phrase - based SMT approach is the lack of sufficient data , not the function class used for the representation of translation systems .", "label": "", "metadata": {}, "score": "57.92739"}
{"text": "The -order 3 tells srilm to produce a trigram language model .You can set this to a higher value , and srilm will happily output 4-gram , 5-gram or even higher order language models .The -kndiscount tells SRILM to use modified Kneser - Ney discounting as its smoothing scheme .", "label": "", "metadata": {}, "score": "57.939163"}
{"text": "Role of Training Set Size on Performance on New Sentences .In this section , we analyze how training set size affects the performance by creating learning curves ( BLEU score versus training set size ) .The general framework for this set of experiments consists of creating subsets of the complete corpus by subsampling from a uniform distribution without replacement .", "label": "", "metadata": {}, "score": "57.941628"}
{"text": "The combined method is about twenty times faster than ultistart LM localization with comparable accuracy .In a second network with only one hidden layer , the outputs were the amplitudes of 193 evenly distributed Gaussian functions holding a soft distributed representation of the dipole location .", "label": "", "metadata": {}, "score": "57.996536"}
{"text": "In each case , we found the same overall behaviour , of a logarithmic growth in performance with training set size .The question becomes as follows : on which aspect of these systems should we act to achieve better performance ?We have performed an extensive series of experiments to separately measure how different factors affect the performance of phrase - based SMT systems .", "label": "", "metadata": {}, "score": "58.014786"}
{"text": "This example contains 1,000 sentences of Urdu - English data ( the full dataset is available as part of the Indian languages parallel corpora with 100-sentence tuning and test sets with four references each .Running the pipeline requires two main steps : data preparation and invocation .", "label": "", "metadata": {}, "score": "58.11936"}
{"text": "Average .and error on the Europarl and Giga corpus datasets are shown in Table 5 .The proposed model is able to approximate well enough the BLEU score using Moses as translation system and in - domain test sets .According to this setting and assuming that we are in the standard case where .", "label": "", "metadata": {}, "score": "58.163998"}
{"text": "lowercased -f fr -e en -alignment grow - diag - final - and -reordering msd - bidirectional - fe -lm 0:5:working - dir / lm / europarl .lm:0 .Tuning ( i.e. , Optimize System Component Weights , a.k.a .Minimum Error Rate Training ) .", "label": "", "metadata": {}, "score": "58.187717"}
{"text": "For each percentage value , ten experiments have been run .All the perturbations have been applied on a model trained with 629,957 pairs of sentences randomly selected form the Europal data using Moses as translation system .The unlearning curves are shown in Figure 7 .", "label": "", "metadata": {}, "score": "58.25022"}
{"text": "It is interesting to see that the decline of the language model is much less pronounced than that of the translation model .The set of experiments in Figure 7(a ) is harder to explain without discussing the inner workings of the translation model and Moses .", "label": "", "metadata": {}, "score": "58.26088"}
{"text": "Traditional approaches to machine translation ( MT ) [ 1 ] relied to a large extent on linguistic analysis .The ( relatively ) recent development of statistical approaches [ 2 ] and especially phrase - based machine translation , or PBMT [ 3 , 4 ] , has put the focus on the intensive use of large parallel corpora .", "label": "", "metadata": {}, "score": "58.441803"}
{"text": "Here , \" 7 \" is the maximum length of a phrase , and 0.5 is the \" pseudo - count \" to be added for smoothing .$ script / itgstats .lex .Finally , you can combine the phrase table ( TM ) , language model ( LM ) , and reordering model ( DM ) together using the following command .", "label": "", "metadata": {}, "score": "58.489548"}
{"text": "We investigate the learning - theoretic implications of this setting , including the interplay between approximation error and estimation error , model selection , and accuracy in parameters estimation .We do not address more general themes about the opportunity for SMT to be evaluated by automatic metrics .", "label": "", "metadata": {}, "score": "58.60859"}
{"text": "Before decoding the test set , you 'll need to extract a translation grammar for the foreign phrases in the test set test / newstest2009 .es.tok.lc : . /model\\ test / newstest2009 .es.tok.lc.grammar.raw \\ test / newstest2009 .es.tok.lc & .", "label": "", "metadata": {}, "score": "58.82924"}
{"text": "2 In what follows we compare two strategies for unsupervised construction of such a corpus , one employing string similarity and the other associating sentences that may overlap very little at the s .. \" ...We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .", "label": "", "metadata": {}, "score": "59.00547"}
{"text": "When using the Berkeley aligner , you 'll want to pay attention to how much memory you allocate to it with --aligner - mem ( the default is 10 g ) .aligner - chunk - size SIZE ( 1,000,000 ) .The number of sentence pairs to compute alignments over . --alignment FILE .", "label": "", "metadata": {}, "score": "59.018093"}
{"text": "SMT .Corpus preparation is the general process to extract , transform , categorize various documents from their original purpose to and align the resulting data into a parallel corpus for training a translation model .The evaluation process uses a translation model of components created in the training process and configured with the tuning process to translate several thousand source language sentences in the eval set .", "label": "", "metadata": {}, "score": "59.044777"}
{"text": "This unrealistic case is not affected by the Zipf 's law , because almost all the words necessary to translate the training material have , by definition , already been observed .The model is therefore able to match long phrases when producing the \" test on training set ' ' translations .", "label": "", "metadata": {}, "score": "59.201996"}
{"text": "295 - 302 , Philadelphia , Pa , USA , 2001 .R. C. Moore , W.-T. Yih , and A. Bode , \" Improved discriminative bilingual word alignment , \" in Proceedings of the 21stInternational Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics ( ACL ' 06 ) , pp .", "label": "", "metadata": {}, "score": "59.259537"}
{"text": "For the second set of experiments , data has been randomly sampled in training and test sets one thousand times .Training set has been used to estimate the alphas and the residual , and test set to predict the BLEU score values .", "label": "", "metadata": {}, "score": "59.44763"}
{"text": "We present Minimum Bayes - Risk ( MBR ) decoding for statistical machine translation .This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of l ... \" .", "label": "", "metadata": {}, "score": "59.644405"}
{"text": "263 - 311 , 1994 .View at Google Scholar .P. Koehn , F. J. Och , and D. Marcu , \" Statistical phrase - based translation , \" in Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology , pp .", "label": "", "metadata": {}, "score": "59.96489"}
{"text": "en.tok.lc -rps 1 # references per sentence -p mert / params .txt # parameter file -m BLEU 4 closest # evaluation metric and its options -maxIt 10 # maximum MERT iterations -ipi 20 # number of intermediate initial points per iteration -cmd mert / decoder_command # file containing commands to run decoder -decOut mert / news - dev2009 .", "label": "", "metadata": {}, "score": "60.068844"}
{"text": "The procedure is similar to ( Chiang , 2007 ) except that we maintain tree structures on the target side , instead of strings .We use a statistical CFG parser to parse the English side of the training data , and extract dependency trees with Magerman 's rules ( 1995 ) .", "label": "", "metadata": {}, "score": "60.139706"}
{"text": "The distortion feature controls the reordering between phrases .Note that only very short - range reordering may be handled within phrases .Long - range reordering must be handled by target phrase permutations .This feature allows to regulate the amount of reordering depending on , for example , the language pair .", "label": "", "metadata": {}, "score": "60.24382"}
{"text": "Given that the English side of the parallel corpus is a relatively small amount of data in terms of language modeling , it only takes a few minutes a few minutes to output the LM .The uncompressed LM is 144 megabytes large ( du -h europarl.en.trigram.lm ) .", "label": "", "metadata": {}, "score": "60.246338"}
{"text": "We also provide insight into the way statistical machine translation learns from data , including the respective influence of translation and language models , the impact of phrase length on performance , and various unlearning and perturbation analyses .Our results support and illustrate the fact that performance improves by a constant amount for each doubling of the data , across different language pairs , and different systems .", "label": "", "metadata": {}, "score": "60.27041"}
{"text": "In Figure 9 , it increases roughly linearly with the development set size .It is nice to note how the computational time is strongly related to the number of optimization steps in Figure 10 . A.2 .Role of the Unknown Words .", "label": "", "metadata": {}, "score": "60.273495"}
{"text": "R. Zens , F.-J. Och , and H. Ney , \" Phrase - based statistical machine translation , \" in Proceedings of the 25th Annual German Conference on AI ( KI ' 02 ) , pp .18 - 32 , Springer , London , UK , 2002 .", "label": "", "metadata": {}, "score": "60.302628"}
{"text": "This phrase - based machine translation approach relies on a specific representation of the translation process , such as the choice of contiguous word sequences ( phrases ) as basic units in the language and translation models .How far can this representation take us towards the target of improving translation quality ?", "label": "", "metadata": {}, "score": "60.374886"}
{"text": "Unfortunately , current estimation procedures are unable to reach such high - performing regions of the parameter space .This was also noted by a recent paper by Wisniewski et al . who note that \" the current bottleneck of translation performances is not the representation power of the [ phrase - based translation systems ] but rather in their scoring functions '' [ 38 ] .", "label": "", "metadata": {}, "score": "60.45939"}
{"text": "The first is to generate phrase pairs by using grammatical or various linguistic rules ( e.g. , turning existing entries into new entries , by applying various forms of inflection ) .The second is to allow the system to make queries , active learning style , in order to produce phrase - table entries without having to wait for them to appear by sampling additional textual data .", "label": "", "metadata": {}, "score": "60.539436"}
{"text": "The small translation grammar contains 15,939 rules -- you can get the count of the number of rules by running gunzip -c example / example .The first part of the rule is the left - hand side non - terminal .The second and third parts are the right - hand side .", "label": "", "metadata": {}, "score": "60.65329"}
{"text": "Multiple --tune and --test flags are not currently allowed .Normalizing punctuation and text ( e.g. , removing extra spaces , converting special quotations ) .There are a few language - specific options that depend on the file extension matching the two - letter ISO 639 - 1 designation .", "label": "", "metadata": {}, "score": "60.694756"}
{"text": "This is reflected in the two main components of the typical SMT model : the language model and the translation model .The language model is typically built using a table of n -grams , with associated probabilities , which is sufficient to define a Markov chain .", "label": "", "metadata": {}, "score": "60.739437"}
{"text": "This file contains the n - best translations , under the model .The first 10 lines that you see above are 10 best translations of the first sentence .Each line contains 4 fields .To get the 1-best translations for each sentence in the test set without all of the extra information , you can run the following command : . nbest.srilm.out \\ example / example .", "label": "", "metadata": {}, "score": "60.86978"}
{"text": "This is also compatible with what is seen in the learning curve .It is important to notice , however , that introducing a more aggressive type of noise ( Figure 7(b ) ) that essentially replaces entire parameters with random values does lead to a more significant decline in performance .", "label": "", "metadata": {}, "score": "61.131744"}
{"text": "The first network is a multilayer perceptron ( MLP ) which takes the sensor measurements as inputs , uses two hidden layers , and outputs source location in Cartesian coordinates .After training with random dipolar sources contaminated by real noise , localization of a single dipole could be performed within 300 microseconds on an 800 Mhz Athlon workstation , with an average localization error of 1.15 cm .", "label": "", "metadata": {}, "score": "61.314217"}
{"text": "868 - 876 , 2007 .J. Blatz , E. Fitzgerald , G. Foster , et al . , \" Confidence estimation for machine translation , \" in Proceedings of the 20th international Conference on Computational Linguistics ( COLING ' 04 ) , vol .", "label": "", "metadata": {}, "score": "61.325703"}
{"text": "Earlier results ( which are often expensive ) need not be recomputed .To facilitate these tasks , the pipeline script : - Runs the complete SMT pipeline , from corpus normalization and tokenization , through model building , tuning , test - set decoding , and evaluation .", "label": "", "metadata": {}, "score": "61.3303"}
{"text": "The second kind of noise that we add to the model is based on a swap of a particular quantity inside two entries of language or translation model .This is meant to test how robust the system is to perturbations of the all - important associations between phrases / numbers and to the associations between source / target phrases .", "label": "", "metadata": {}, "score": "61.40571"}
{"text": "By default , the pipeline uses a Java program from the Berkeley LM package that constructs an Kneser - Ney - smoothed language model in ARPA format from the target side of your training data .If you wish to use SRILM instead , you need to do the following : .", "label": "", "metadata": {}, "score": "61.43461"}
{"text": "To be consistent and to avoid anomalies due to overfitting or particular data combinations , each set of pairs of sentences has been randomly sampled .The number of pairs is fixed , and a program selects them randomly from the whole original training , development , or test set using a uniform distribution .", "label": "", "metadata": {}, "score": "61.455517"}
{"text": "Other arguments are as follows .This determines the language model code that will be used when decoding .These implementations are described in their respective papers ( PDFs : KenLM , BerkeleyLM ) .--lmfile FILE .Specifies a pre - built language model to use when decoding .", "label": "", "metadata": {}, "score": "61.539875"}
{"text": "Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions . \" ...This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .", "label": "", "metadata": {}, "score": "61.674473"}
{"text": "These steps are discussed below , after a few intervening sections about high - level details of the pipeline .Grammar options .Joshua can extract two types of grammars : Hiero - style grammars and SAMT grammars .As described on the file formats page , both of them are encoded into the same file format , but they differ in terms of the richness of their nonterminal sets .", "label": "", "metadata": {}, "score": "61.919083"}
{"text": "Performance of an SMT system is a function of the dimension of the training data that can be logarithmic as seen in the previous section .We have modelled this relation in the following way : . of the data as increasing set size .", "label": "", "metadata": {}, "score": "61.93365"}
{"text": "Testing .For each of the tuner runs , Joshua takes the tuner output file and decodes the test set .Afterwards , by default , minimum Bayes - risk decoding is run on the 300-best output .This step usually yields about 0.3 - 0.5 BLEU points but is time - consuming , and can be turned off with the --no - mbr flag .", "label": "", "metadata": {}, "score": "61.998566"}
{"text": "They are automatically filled during the training phase , when a bilingual corpus is used to identify both phrases and their probabilities .Since future translations are produced by maximizing a scoring function estimating translation quality , using the content of the two tables , we see that the contents of the translation and language models tables correspond to the tunable parameters of the learning system .", "label": "", "metadata": {}, "score": "62.095802"}
{"text": "Note that , in this case , you will want to skip data preparation and alignment using --first - step thrax ( the first step after alignment ) and also to specify --no - prepare - data so as not to retokenize the data and mess with your alignments .", "label": "", "metadata": {}, "score": "62.321266"}
{"text": "The unknown words are the direct effect of Zipf 's law in a language , as new words can come , but the training set is not flexible enough to cover them .We have created 10 random subsets for each of the 10 chosen sizes , where each size represents 10 . , and so forth of the complete corpus .", "label": "", "metadata": {}, "score": "62.34337"}
{"text": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .This method requires a source - language dependency parser , target language word segmentation and an unsupervised word alignment component .", "label": "", "metadata": {}, "score": "62.56556"}
{"text": "You can see how much the subsampling step reduces the training data , by yping wc -lw es - en / full - training / training . tok.lc es - en / full - training / subsampled / subsample . tok.lc : .1411589 39411018 training / training .", "label": "", "metadata": {}, "score": "62.641403"}
{"text": "Using long phrases will help when the system has to translate sequences of words that match what was encountered in the training corpus , but this becomes increasingly unlikely as the phrases become longer .On the other hand , short sentences are more often reused , but may also be more ambiguous and lead to errors more often .", "label": "", "metadata": {}, "score": "62.653824"}
{"text": "What follows below are step - by - step instructions .This may look like a long list at first glance , but it should make it straightforward to build a machine translation system and all its components , and it should make the process of tuning , testing , and evaluating it transparent .", "label": "", "metadata": {}, "score": "62.73104"}
{"text": "Each model has been tested on the test set and on a subset of 2,000 pairs the training set .The optimization step has not been run .For each model , we count the unknown words .Figure 11 shows unknown words as function of the training model .", "label": "", "metadata": {}, "score": "62.853104"}
{"text": "L. Specia , M. Turchi , Z. Wang , J. Shawe - Taylor , and C. Saunders , \" Improving the confidence of machine translation quality estimates , \" in Proceedings of 12th the Machine Translation Summit , 2009 . D. Vilar , J. Xu , L. F. D'Haro , and H. Ney , \" Error analysis of statistical machine translation output , \" in Proceedings of the 5th International Conference on Language Resources and Evaluation ( LREC ' 06 ) , Genova , Italy , 2006 .", "label": "", "metadata": {}, "score": "62.92593"}
{"text": "In [ 39 ] , a classification of different types of error has been proposed .In this section , we focus our attention on a particular type of error : unknown words .This type of error is considered a source error because it depends on the source sentence and not the translation process .", "label": "", "metadata": {}, "score": "63.12211"}
{"text": "tried by MERT , new hypothesis translations are added to the list .As the number of hypotheses produced by the decoder is finite , this is guaranteed to converge , and in practice , it does fairly quickly .An additional difficulty is that the landscape of the cost , for example , BLEU , is piecewise constant and highly irregular .", "label": "", "metadata": {}, "score": "63.541733"}
{"text": "These chunked files are all created in a subdirectory of RUNDIR / data / train / splits , named corpus .LANG.0 , corpus .LANG.1 , and so on .The pipeline parameters affecting alignment are : .Which aligner to use .", "label": "", "metadata": {}, "score": "63.586853"}
{"text": "One of our key findings is that the current performance of phrase - based statistical machine translation systems is not limited by the representation power of the hypothesis class , but rather by model estimation from data .In other words , we demonstrate that parameter choices exist that can deliver significantly higher performance , but that inferring them from finite samples is the problem .", "label": "", "metadata": {}, "score": "63.67123"}
{"text": "en.trigram.lm .( Note : the above assumes that you are on a 64-bit machine running Mac OS X. If that 's not the case , your path to ngram - count will be slightly different . )This will train a trigram language model on the English side of the parallel corpus .", "label": "", "metadata": {}, "score": "63.739876"}
{"text": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .Compared to the standard practice of intersecting predictions of independently - trained models , joint training provides a 32 % reduction in AER .", "label": "", "metadata": {}, "score": "63.838715"}
{"text": "We propose a theory that gives formal semantics to word - level alignments defined over parallel corpora .We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human ... \" .", "label": "", "metadata": {}, "score": "63.877613"}
{"text": "Aligned data are the elements of a parallel corpus consisting of two or more languages .Each element in one language matches the corresponding element in the other language(s ) .The elements , sometimes called segments , can be block - aligned , paragraph - aligned , sentence - aligned , phrase - aligned or token - aligned . alignment process .", "label": "", "metadata": {}, "score": "63.91529"}
{"text": "Hardware .All the experiments have been run on high - performance clusters of machines .Additional information : ClearSpeed accelerator boards on the thick nodes ; SilverStorm Infiniband high - speed connectivity throughout for parallel code message passing ; General Parallel File System ( GPFS ) providing data access from all the nodes with a total of 11 terabytes of storage .", "label": "", "metadata": {}, "score": "63.954052"}
{"text": "We 'll use the word alignments to create a translation grammar similar to the Chinese one shown in Step 1 .The translation grammar is created by looking for where the foreign language phrases from the test set occur in the training set , and then using the word alignments to figure out which foreign phrases are aligned .", "label": "", "metadata": {}, "score": "64.106575"}
{"text": "We present an extensive experimental study of Phrase - based Statistical Machine Translation , from the point of view of its learning capabilities .Very accurate Learning Curves are obtained , using high - performance computing , and extrapolations of the projected performance of the system under different conditions are provided .", "label": "", "metadata": {}, "score": "64.130325"}
{"text": "It 's located in the tarball under the scripts directory .To use it type the following commands : . en.tok .Normalization .After tokenization , we recommend that you normalize your data by lowercasing it .The system treats words with variant capitalization as distinct , which can lead to worse probability estimates for their translation , since the counts are fragmented .", "label": "", "metadata": {}, "score": "64.2016"}
{"text": "TrueCase.5gram.lm \\ -keep - unk \\ -order 5 \\ -map model / lm / true - case . map \\ -text test / mt09 .output.1best.recased .Where strip - sent - tags .perl is : .Step 9 : Score the translations .", "label": "", "metadata": {}, "score": "64.376335"}
{"text": "In this ... \" .This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .In the nal evaluation , the statistical approach was found to perform best among ve competing approaches .", "label": "", "metadata": {}, "score": "64.38529"}
{"text": "conf ): .# # word - align .conf # # ---------------------- # # This is an example training script for the Berkeley # # word aligner .In this configuration it uses two HMM # # alignment models trained jointly and then decoded # # using the competitive thresholding heuristic .", "label": "", "metadata": {}, "score": "64.48488"}
{"text": "We also believe that in this particular situation , the presence of the error bars may help to better understand the stability of the system .Using the framework described above , four different settings have been set to produce learning curves , see Table 3 .", "label": "", "metadata": {}, "score": "64.50957"}
{"text": "35 - 43 , Columbus , Ohio , USA , 2008 . Y. Al - Onaizan , J. Curin , M. Jahr , et al . , \" Statistical machine translation : final report , \" Tech .Rep. , Johns Hopkins University , Summer Workshop on Language Engineering , Center for Speech and Language Processing , Baltimore , Md , USA , 1999 .", "label": "", "metadata": {}, "score": "64.56584"}
{"text": "What is the best function class to map Spanish documents into English documents ?This is a question of linguistic nature and has been the subject of a long debate .With the growing availability of bilingual parallel corpora , the 1990 s saw the development of statistical machine translation ( SMT ) models .", "label": "", "metadata": {}, "score": "64.618546"}
{"text": "Joshua uses the synchronous context free grammar ( SCFG ) formalism in its approach to statistical machine translation , and the software implements the algorithms that underly the approach .The Berkeley Aligner - this software is used to align words across sentence pairs in a bilingual parallel corpus .", "label": "", "metadata": {}, "score": "64.63339"}
{"text": "This value is required if you start at the grammar extraction step .When alignment is complete , the alignment file can be found at RUNDIR / alignments / training . align .It is parallel to the training corpora .There are many files in the alignments/ subdirectory that contain the output of intermediate steps .", "label": "", "metadata": {}, "score": "64.73778"}
{"text": "Non - source languages are referred to as \" target \" languages .For Moses SMT , parallel data takes the form of one source and one target language text file where both files contain corresponding translation of sentences line by line .phrase table .", "label": "", "metadata": {}, "score": "64.77858"}
{"text": "While we refer to \" words swap ' ' when , given two entries of the translation model , we swap the target language phrases .Three different sets of experiments have been run applying \" numerical swap ' ' only to the language model , \" numerical swap ' ' only to the translation model and \" words swap ' ' only to the translation model .", "label": "", "metadata": {}, "score": "65.11916"}
{"text": "( i ) Unlearning by Adding Noise .A percentage of noise has been added to each probability , . , in the Language model , including conditional probability , and translation model , bidirectional phrase translation probabilities and lexicalized weighting .The aim of this set of experiments is to test how robust the system is with respect to a reduced accuracy of its numeric parameters .", "label": "", "metadata": {}, "score": "65.19313"}
{"text": "Sentence alignment .In this exercise , we 'll start with an existing sentence - aligned parallel corpus .Download this tarball , which contains a Spanish - Engish parallel corpus , along with a dev and a test set : data.tar.gz .The data tarball contains two training directories training/ , which includes a subset of the corpus , and full - training , which includes the full corpus .", "label": "", "metadata": {}, "score": "65.27757"}
{"text": "es.tok.lc.grammar .You will also need to create a small \" glue grammar \" , in a file called model / hiero .glue that contains these rules that allow hiero - style grammars to reach the goal state : .Step 6 : Run minimum error rate training .", "label": "", "metadata": {}, "score": "65.48699"}
{"text": "es.tok.lc cat es - en / test / newstest2009 .en.tok.lc .Subsampling ( optional ) .Sometimes the amount of training data is so large that it makes creating word alignments extremely time - consuming and memory - intesive .We therefore provide a facility for subsampling the training corpus to select sentences that are relevant for a test set .", "label": "", "metadata": {}, "score": "65.48751"}
{"text": "The reordering table is translation model components .source language .SMT .The source language is the language of the text that is to be translated .Typically , this is the authored language of the text .The source language is the same as the TMX specification \" srclang \" attribute of the tag .", "label": "", "metadata": {}, "score": "65.607765"}
{"text": "Replacements are not allowed .These choices also depend on the high computational cost of the tuning algorithm .For each size , ten random sets have been selected .For each set , an instance of the system has been run .The optimized model is used to test , and the results are evaluated .", "label": "", "metadata": {}, "score": "65.89784"}
{"text": "This is mostly limited by Zipf 's law , since the probability of encountering phrases that have not been seen in the training set does not vanish even after observing very large corpora .The question therefore becomes as follows : how can we fill translation tables with phrase pairs while Zipf 's law seems to prevent us from generating them from the data alone ?", "label": "", "metadata": {}, "score": "65.926315"}
{"text": "Using alignment techniques from phrasebased statistical machine translation , we show how paraphrases ... \" .Previous work has used monolingual parallel corpora to extract and generate paraphrases .We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .", "label": "", "metadata": {}, "score": "66.09327"}
{"text": "Aligning the English phrase to be paraphrased was the German - English section of the Europarl corpus , version 2 ( Koehn , 2002 ) .Because we wanted to test our method independently of the quality of word alignment algorithms , we also developed a gold standard of word alignments for the set of phrases that we wanted to paraphr ... .", "label": "", "metadata": {}, "score": "66.09366"}
{"text": "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments . ... ol .", "label": "", "metadata": {}, "score": "66.36102"}
{"text": "With each iteration , the tuning process repeats the steps until it reaches an optimized translation quality .HOW - TO GUIDE :Installing and running the Joshua Decoder .Note : these instructions are several years out of date .This document gives instructions on how to install and use the Joshua decoder .", "label": "", "metadata": {}, "score": "66.41077"}
{"text": "Both grammar formats are extracted with the Thrax software .By default , the Joshua pipeline extract a Hiero grammar , but this can be altered with the --type samt flag .Other high - level options .The following command - line arguments control run - time behavior of multiple steps : . --threads N ( 1 ) .", "label": "", "metadata": {}, "score": "66.53955"}
{"text": "The default is \" 2 g \" , but you will want to increase it for larger language models .If SRILM is used , it is called with the following arguments : .$ SRILM / bin / i686-m64/ngram - count -interpolate SMOOTHING -order 5 -text TRAINING - DATA -unk -lm lm.gz .", "label": "", "metadata": {}, "score": "66.54439"}
{"text": "When increasing the dimension of the training set , the number of unknown words decreases .These curves reflect how machine translation is strongly affected by Zipf 's law and confirm the results of the previous sections .A briefly discussion about the presence of unknown words when we test on a subset of the training set is given by Section 4.3 .", "label": "", "metadata": {}, "score": "66.7071"}
{"text": "# lm config .# tm config .# pruning config .# nbest config .# remote lm server config , we should first prepare remote_symbol_tbl before starting any jobs . /voc.remote.sym ./remote.lm.server.list .# parallel deocoder : it can not be used together with remote lm .", "label": "", "metadata": {}, "score": "66.73233"}
{"text": "A BLEU score indicates how closely the token sequences in one set of data , such as machine translation output , correlate with ( match ) the token sequences in another set of data , such as a reference human translation .See : evaluation process .", "label": "", "metadata": {}, "score": "66.7488"}
{"text": "The default amount of memory is 3100 m , which is likely not enough ( especially if you are decoding with SAMT grammar ) .You can alter the amount of memory for Joshua using the --joshua - mem MEM argument , where MEM is a Java memory specification ( passed to its -Xmx flag ) .", "label": "", "metadata": {}, "score": "66.77748"}
{"text": "If not , feel free to contact pialign - users for help at any time .Options .The following options can be used with pialign . default is small ( 0.01 ) to prevent overly long alignments -base The type of base measure to use ( m1 g is generally best ) .", "label": "", "metadata": {}, "score": "67.03271"}
{"text": "These sites come from the Canadian government , the European Union , the United Nations , and other international organizations .In addition to covering a wide range of themes , they also contain documents with different styles and genres .We estimate that the rate of misaligned sentence pairs was around 13 % .", "label": "", "metadata": {}, "score": "67.2482"}
{"text": "MERT is a method for setting the weights of the different feature functions the translation model to maximize the translation quality on the dev set .Translation quality is calculated according to an automatic metric , such as Bleu .Our implementation of MERT allows you to easily implement some other metric , and optimize your paramters to that .", "label": "", "metadata": {}, "score": "67.325455"}
{"text": "An automatic score measures the quality of machine - translated sentences by comparing them to a set of human translations , called reference sentences .The score needs to be able to discriminate good translations from bad ones , whilst considering aspects such as adequacy and fluency .", "label": "", "metadata": {}, "score": "67.438255"}
{"text": "To each of these prefixes , a \" .\" is appended , followed by each of SOURCE ( --source ) and TARGET ( --target ) , which are file extensions identifying the languages .The SOURCE and TARGET files must have the same number of lines .", "label": "", "metadata": {}, "score": "67.53281"}
{"text": "Memory usage is a major consideration in decoding with Joshua and hierarchical grammars .In particular , SAMT grammars often require a large amount of memory .Many steps have been taken to reduce memory usage , including beam settings and test - set- and sentence - level filtering of grammars .", "label": "", "metadata": {}, "score": "67.54533"}
{"text": "It is of course important to remark that these limitations only refer to the current systems , where language is modelled as a Markov chain , and by entirely changing language model , different limitations could be found .Appendix . A. Supplementary Results .", "label": "", "metadata": {}, "score": "67.5966"}
{"text": "TrueCase.5gram.lm .Next , you 'll need to create a list of all of the alternative ways that each word can be capitalized .This will be stored in a map file that lists a lowercased word as the key and associates it with all of the variant capitalization of that word .", "label": "", "metadata": {}, "score": "67.62648"}
{"text": "In an age where the creation of intelligent behaviour is increasingly data driven , this is a question of great importance to all of artificial intelligence .In phrase - based approaches to statistical machine translation , translations are generated in response to some input source text .", "label": "", "metadata": {}, "score": "67.68999"}
{"text": "Does the performance improve with more data because certain parameters are estimated better , or just because the lists are growing ?In the second case , it is likely that more sophisticated statistical algorithms to improve the estimation of probabilities will have limited impact .", "label": "", "metadata": {}, "score": "67.80596"}
{"text": "The evaluation of a machine translation system is a lively and hotly debated topic in this field .Ideally , human beings can evaluate the quality of a translated sentence .However , this is unfeasible for rapid development of automatically trained systems with multiple parameter tuning , as human evaluation is expensive , slow , and sometimes inconsistent and subjective .", "label": "", "metadata": {}, "score": "67.85034"}
{"text": "The underlying log - linear model may be interpreted as a maximum entropy model : . is linear in the log domain , which motivates the description of this framework as \" log - linear model ' ' [ 3 , 4 , 12 ] .", "label": "", "metadata": {}, "score": "68.05376"}
{"text": "Moses uses language model to select the most \" probably \" target language sentence from a large set of \" possible \" translations it generated using the phrase table and reordering table .language model types .SMT .Language model files contain statistical data generated by one of various programs .", "label": "", "metadata": {}, "score": "68.15503"}
{"text": "138 - 145 , Morgan Kaufmann Publishers , San Francisco , Calif , USA , 2002 .S. Banerjee and A. Lavie , \" Meteor : an automatic metric for mt evaluation with improved correlation with human judgments , \" in Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics , Ann Arbor , Mich , USA , 2005 .", "label": "", "metadata": {}, "score": "68.23954"}
{"text": "Again , there are language - specific tokenizations for a few languages ( English , German , and Greek ) .( Training only ) Removing all parallel sentences with more than --maxlen tokens on either side .By default , MAXLEN is 50 .", "label": "", "metadata": {}, "score": "68.41174"}
{"text": "As this is an enormous search space , it is no wonder that both algorithmic and statistical challenges are encountered when training these systems .From a statistical learning point of view , this raises interesting questions : How much of the overall error of the translation system is due to representation limitations , and how much to the difficulty of extracting suitable Translation and Language model tables from a finite sample ?", "label": "", "metadata": {}, "score": "68.43346"}
{"text": "All experiments using Moses have been run using the default parameter configuration .The training , development , and test set sentences are tokenized and lowercased .The maximum number of tokens for each sentence in the training pair has been set to 50 , whilst no limit is applied to the development or test set .", "label": "", "metadata": {}, "score": "68.487335"}
{"text": "When the grammar is extracted , it is compressed and placed at RUNDIR / grammar .gz .Language model .Before tuning can take place , a language model is needed .A language model is always built from the target side of the training corpus unless --no - corpus - lm is specified .", "label": "", "metadata": {}, "score": "68.64297"}
{"text": "The first unlearning curve ( Figure 6 ) , obtained by adding to each parameter a random number ( sampled from within a range ) proportional to its size , is meant to test the role of detailed tuning of parameters .While the orders of magnitude are respected , the fine structure of the parameter set is randomized .", "label": "", "metadata": {}, "score": "69.09822"}
{"text": "F. J. Och and H. Weber , \" Improving statistical natural language translation with categories and rules , \" in Proceedings of the 17th International Conference on Computational Linguistics , vol .2 , pp .985 - 989 , Stroudsburg , Pa , USA , 1998 .", "label": "", "metadata": {}, "score": "69.19202"}
{"text": "F. J. Och , \" Minimum error rate training in statistical machine translation , \" in Proceedings of the 41st Annual Meeting on Association for Computational Linguistics , pp .160 - 167 , Sapporo , Japan , 2003 .W. H. Press , S. A. Teukolsky , W. T. Vetterling , and B. P. Flannery , Numerical Recipes in C++ , Cambridge University Press , Cambridge , Mass , USA , 2002 . A. Arun and P. Koehn , \" Online learning methods for discriminative training of phrase based statistical machine translation , \" in Proceedings of 11th the Machine Translation Summit , Copenhagen , Denmark , 2007 .", "label": "", "metadata": {}, "score": "69.385574"}
{"text": "P. Koehn , \" Statistical significance tests for machine translation evaluation , \" in Proceedings of the Conference on Empirical Methods in Natural Language Processing , pp .388 - 395 , Barcelona , Spain , 2004 . A. Stolcke , \" Srilm - an extensible language modeling toolkit , \" in Proceedings of the International Conference on Spoken Language Processing , Denver , Colo , USA , 2002 .", "label": "", "metadata": {}, "score": "69.56924"}
{"text": "ur , and so on .Assuming no problems arise , this command will run the complete pipeline in about 20 minutes , producing BLEU scores at the end .As it runs , you will see output that looks like the following : .", "label": "", "metadata": {}, "score": "69.74448"}
{"text": "SRILM is freely available for noncommercial purposes .Tazti - A multi function speech recognition software for Windows 7 , Windows 8 and Windows 8.1 based PC 's that allows a user to create keybinds , macros and various mashups to user created speech commands .", "label": "", "metadata": {}, "score": "69.78383"}
{"text": "The target language is the language the source language text should be translated to .Tuning is a process that finds the optimized configuration file settings for a translation model when used a specific purpose .The tuning process translates thousands of source language phrases in the tuning set with a translation model , compares the model 's output to a set of reference human translations , and adjusts the settings with the intention to improve the translation quality .", "label": "", "metadata": {}, "score": "69.831696"}
{"text": "Alignment .You might want to start here if you want to skip data preprocessing .PARSE : Parsing .This is only relevant for building SAMT grammars ( --type samt ) , in which case the target side ( --target ) of the training data ( --corpus ) is parsed before building a grammar .", "label": "", "metadata": {}, "score": "69.97977"}
{"text": "For the Hansard corpus , we took the human annotation of word alignment ... . \" ...We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .", "label": "", "metadata": {}, "score": "70.03409"}
{"text": "We have created 10 random subsets of the complete Europarl corpus containing 629,957 pairs of sentences .For each subset , ten PBMT systems have been estimated .Each instance of Moses has been trained using a different maximum phrase length , from 1 to 10 .", "label": "", "metadata": {}, "score": "70.18275"}
{"text": "Toolkits and SDKs are available for the each technology .Company product and support information is available .Speech Filing System - Tools for Speech Research - SFS provides a computing environment for conducting research into the nature of speech .It comprises free software tools ; special file and data formats ; subroutine libraries for I / O , signal processing and graphics ; use and documentation standards ; and special programming languages .", "label": "", "metadata": {}, "score": "70.20662"}
{"text": "N ( 1 ) .This enables parallel operation over a cluster using the qsub command .This feature is not well - documented at this point , but you will likely want to edit the file $ JOSHUA / scripts / training / parallelize / LocalConfig .", "label": "", "metadata": {}, "score": "70.21599"}
{"text": "The pipeline will notice that it is parsed and will not reparse it .Parsing is affected by both the --threads N and --jobs N options .The former runs the parser in multithreaded mode , while the latter distributes the runs across as cluster ( and requires some configuration , not yet documented ) .", "label": "", "metadata": {}, "score": "70.252975"}
{"text": "This is an active area of research in machine translation [ 35 - 37 ] .The results of the perturbation analysis in Section 5 suggest that the limiting factor in the translation tables is not in the numeric part of the model - the parameters being estimated - but in the phrases contained in it , the entries of the phrase table .", "label": "", "metadata": {}, "score": "70.323456"}
{"text": "Richer hypothesis classes can fit the training data more accurately but generalize less well than poorer classes , a phenomenon known as overfitting .The choice of the appropriate expressive power , within a parametrized class of models , is called model selection and is one of the most crucial steps in the design of learning systems .", "label": "", "metadata": {}, "score": "70.3448"}
{"text": "How much space for improvement is there , given new data or new statistical estimation methods or given different models with different complexities ?Before we present experimental results that address these questions , we will describe the setup that was used to obtain these results .", "label": "", "metadata": {}, "score": "70.49275"}
{"text": "Grammar extraction with Thrax .If you jump to this step , you 'll need to provide an aligned corpus ( --alignment ) along with your parallel data .TUNE : Tuning .With this option , you need to specify a grammar ( --grammar ) or separate tune ( --tune - grammar ) and test ( --test - grammar ) grammars .", "label": "", "metadata": {}, "score": "70.5233"}
{"text": "pl /full / path / to / out / align.1 . pt /full / path / to / lm.txt --dm - file /full / path / to / out / align.1 . ini .There are also a number of settings controlling the LM order , TM order , etc . described in the beginning of make-moses-config.pl , so please take a look to make sure there is nothing that you need to change .", "label": "", "metadata": {}, "score": "71.23586"}
{"text": "Step 7 : Decode a test set .When MERT finishes , it will output a file mert / joshua .config .ZMERT.final that contains the news weights for the different feature functions .You can copy this config file and use it to decode the test set .", "label": "", "metadata": {}, "score": "71.25607"}
{"text": "One way to achieve this could be to either introduce an oracle to which the system can ask for annotation when needed or a process that uses linguistic knowledge to create new table entries based on existing table entries and some grammatical rules .", "label": "", "metadata": {}, "score": "71.383316"}
{"text": "SRILM , RandLM and IRSTLM toolkits include tools that train the new language model files .KenLM , however , only reads ARPA standard language model files which can be created by SRILM , IRSTLM .A linguistic corpus of two or more languages where each element in one language corresponds to an element with the same meaning in the other language(s ) .", "label": "", "metadata": {}, "score": "71.46082"}
{"text": "This can be accomplished with the --first - step and --last - step flags , which take as argument a case - insensitive version of the following steps : .FIRST :Data preparation .Everything begins with data preparation .This is the default first step , so there is no need to be explicit about it .", "label": "", "metadata": {}, "score": "71.667496"}
{"text": "The last step .This is the default target of --last - step .We now discuss these steps in more detail .DATA PREPARATION .Data prepare involves doing the following to each of the training data ( --corpus ) , tuning data ( --tune ) , and testing data ( --test ) .", "label": "", "metadata": {}, "score": "72.05794"}
{"text": "The file system is Ibrix and provides data access from all nodes , with a total of 17 TB of storage .Experiments using Portage are distributed over several CPUs , the total number of which depends on the various stages in the estimation process .", "label": "", "metadata": {}, "score": "72.71129"}
{"text": "2142 - 2147 , Genova , Italy , 2006 .N. Draper and H. Smith , Applied Regression Analysis , John Wiley & Sons , New York , NY , USA , 1981 .F. J. Och , \" Statistical machine translation : foundations and recent advances , \" in Proceedings of the Tutorial at MT Summit , 2005 .", "label": "", "metadata": {}, "score": "72.92583"}
{"text": "A \" phrase table \" is a statistical description of a parallel corpus of source - target language sentence pairs .The frequencies that n - grams in a source language text co - occur with n - grams in a parallel target language text represent the probability that those source - target paired n - grams will occur again in other texts similar to the parallel corpus .", "label": "", "metadata": {}, "score": "73.05899"}
{"text": "MOSES .Moses is a statistical machine translation system that allows you to automatically train translation models for any language pair .All you need is a collection of translated texts ( parallel corpus ) .An efficient search algorithm finds quickly the highest probability translation among the exponential number of choices .", "label": "", "metadata": {}, "score": "73.13738"}
{"text": "Alignments are between the parallel corpora at RUNDIR / data / train / corpus .To prevent the alignment tables from getting too big , the parallel corpora are grouped into files of no more than ALIGNER_CHUNK_SIZE blocks ( controlled with a parameter below ) .", "label": "", "metadata": {}, "score": "73.221085"}
{"text": "This is because the Berkeley aligner generally expects to test against a set of manually word - aligned data : . cd es - en / full - training/ mkdir -p example / test .After you 've created the word - align .", "label": "", "metadata": {}, "score": "73.30825"}
{"text": "input/ train .SOURCE train .TARGET tune .SOURCE tune .TARGET test .SOURCE test .TARGET .These files should be parallel at the sentence level ( with one sentence per line ) , should be in UTF-8 , and should be untokenized ( tokenization occurs in the pipeline ) .", "label": "", "metadata": {}, "score": "73.36517"}
{"text": "Run the example model .To test to make sure that the decoder is installed properly , we 'll translate 5 sentences using a small translation model that loads quickly .The sentences that we will translate are contained in example / example .", "label": "", "metadata": {}, "score": "73.57539"}
{"text": "Translation of training sentences allows us to estimate the training error .The learning curves in Figure 4 illustrate how the performance is affected by the phrase length .The \" test on test set ' ' curve is less influenced by the phrase length than the \" test on training set ' ' curve .", "label": "", "metadata": {}, "score": "73.593704"}
{"text": "You can score your output using the JoshuaEval class , Joshua 's built - in scorer : .en.output \\ -ref dev / dev2006 .en.small \\ -m BLEU 4 closest July 15 - 16 , in conjunction with ACL 2010 in Uppsala , Sweden .", "label": "", "metadata": {}, "score": "73.6232"}
{"text": "HHMM /training / train - factored - phrase - model.perl -ngram - count /path - to - srilm / bin/ i686 /ngram - count -corpus working - dir / lm / europarl .tok -dir recaser .Recase the output bin / moses - scripts / scripts- YYYYMMDD - HHMM /recaser / recase .", "label": "", "metadata": {}, "score": "73.66734"}
{"text": "In fact , a common belief in SMT is that learning curves follow logarithmic laws ; to analyze this in our experiments , we show all the learning curves in the linear log scale , where we can study if the curve has a linear behaviour .", "label": "", "metadata": {}, "score": "73.71866"}
{"text": "The search for the optimal translation in ( 2 ) is also referred to as decoding as , in the original analogy of the noisy channel , it corresponds to retrieving the clean message ., we usually assume that the feature functions decompose linearly across basic constituents of the sentences .", "label": "", "metadata": {}, "score": "73.81167"}
{"text": "The impressive capability of current machine translation systems is not only a testament to an incredibly productive and creative research community , but can also be seen as a paradigm for other artificial intelligence tasks .Data - driven approaches to all main areas of AI currently deliver the state - of - the - art performance , from summarization to speech recognition to machine vision to information retrieval .", "label": "", "metadata": {}, "score": "74.03511"}
{"text": "es.tok.lc : . /model\\ mert / news - dev2009 .es.tok.lc.grammar.raw \\ dev / news - dev2009 .es.tok.lc & .Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u mert / news - dev2009 .", "label": "", "metadata": {}, "score": "74.19323"}
{"text": "Detokenize the output scripts / detokenizer .output.detokenized .Wrap the output in SGML scripts / wrap - xml . perl wmt08/devtest / devtest2006-ref .output.sgm .Score with NIST BLEU scoring tool mteval-v11b.pl -r wmt08/devtest / devtest2006-ref . en.sgm -t working - dir / evaluation / devtest2006 .", "label": "", "metadata": {}, "score": "74.36786"}
{"text": "Software .Several software packages are available for training PBSMT systems .In this work , we use both Moses [ 5 ] and Portage [ 6 ] .Moses is a complete open - source phrase - based translation toolkit for academic purposes , while Portage is a similar package available to partners of the National Research Council Canada .", "label": "", "metadata": {}, "score": "74.37839"}
{"text": "When SAMT grammars are being built ( --type samt ) , the target side of the training data must be parsed .The pipeline assumes your target side will be English , and will parse it for you using the Berkeley parser , which is included .", "label": "", "metadata": {}, "score": "74.42595"}
{"text": "These are called recasing and detokenization , respectively .We can do recasing using SRILM , and can do detokenization with a perl script .To build a recasing model first train a language model on true cased English text : .$ SRILM / bin / macosx64/ngram - count \\ -unk \\ -order 5 \\ -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -kndiscount5 \\ -text training / training .", "label": "", "metadata": {}, "score": "74.489746"}
{"text": "output.nbest .After the decoder has finished , you can extract the 1-best translations from the n - best list using the following command : . output.nbest \\ test / newstest2009 .output.1best .Step 8 : Recase and detokenize .You 'll notice that your output is all lowercased and has the punctuation split off .", "label": "", "metadata": {}, "score": "74.96005"}
{"text": "A MERT configuration file .A separate file with the list of the feature functions used in your model , along with their possible ranges .Create a MERT configuration file .In this example we name the file mert / mert . config .", "label": "", "metadata": {}, "score": "75.07809"}
{"text": "If you would like more details about the method or want to cite pialign in your research , please reference : .In order for Moses to run , you will also need a language model , which you can train using SRILM or IRSTLM ( you can reference the Moses step - by - step tutorial for more details on how to train a model ) .", "label": "", "metadata": {}, "score": "75.26471"}
{"text": "Insert weights into configuration file scripts / reuse - weights .perl working - dir / tuning / moses . weight - reused . ini .Run System on Development Test Set .Tokenize test set mkdir -p working - dir / evaluation scripts / tokenizer .", "label": "", "metadata": {}, "score": "75.43647"}
{"text": "To find the foreign phrases in the test set , we first create an easily searchable index , called a suffix array , for the training data .java -Xmx500 m -cp $ JOSHUA / bin/ \\ joshua.corpus.suffix_array.Compile \\ training / subsampled / subsample .", "label": "", "metadata": {}, "score": "75.46252"}
{"text": "Joshua provides support for n - gram language models , either through a built in data structure , or through external calls to the SRI language modeling toolkit ( srilm ) .To use large language models , we recommend srilm .If you successfully installed srilm in Step 1 , then you should be able to train a language model with the following command : . mkdir -p model / lm $ SRILM / bin / macosx64/ngram - count \\ -order 3 \\ -unk \\ -kndiscount1 -kndiscount2 -kndiscount3 \\ -text training / training .", "label": "", "metadata": {}, "score": "75.66305"}
{"text": "You must preprocess your dev and test sets in the same way you preprocess your training data .Run the following commands on the data that you downloaded : . cat es - en / dev / news - dev2009 .es.tok.lc cat es - en / dev / news - dev2009 .", "label": "", "metadata": {}, "score": "75.74594"}
{"text": "[ train - copy - ur ] cached , skipping ... ... .This indicates that the caching module has discovered that the step was already computed and thus did not need to be rerun .This feature is quite useful for restarting pipeline runs that have crashed due to bugs , memory limitations , hardware failures , and the myriad other problems that plague MT researchers across the world .", "label": "", "metadata": {}, "score": "75.80535"}
{"text": "Prepare Data .Tokenize training data mkdir -p working - dir / corpus scripts / tokenizer . tok.fr scripts / tokenizer . tok.en .Filter out long sentences bin / moses - scripts / scripts- YYYYMMDD -HHMM /training / clean - corpus - n . perl working - dir / corpus / europarl .", "label": "", "metadata": {}, "score": "76.041214"}
{"text": "Fast robust MEG source localization using MLPs .Jun , Sung Chan and Pearlmutter , Barak A. and Nolte , Guido ( 2002 )Fast robust MEG source localization using MLPs .In : Proceedings of the 13th International Conference on Biomagnetism , August 10 - 14 , 2002 , Jena , Germany .", "label": "", "metadata": {}, "score": "76.41006"}
{"text": "You can use the grammar to translate the test set by running .config.srilm \\ example / example .test.in \\ example / example .nbest.srilm.out .For those of you who are n't very familiar with Java , the arguments are the following : . -Xmx1 g -- this tells Java to use 1 GB of memory . -cp", "label": "", "metadata": {}, "score": "76.54588"}
{"text": "A language model built from the target side of the training data is placed at RUNDIR / lm .gz .Interlude : decoder arguments .Running the decoder is done in both the tuning stage and the testing stage .A critical point is that you have to give the decoder enough memory to run .", "label": "", "metadata": {}, "score": "76.610344"}
{"text": "en.tok.lc \\ training / subsampled / training .en.tok.lc-es .tok.lc.align \\ model .This compiles the index that Joshua will use for its rule extraction , and puts it into a directory named model .Extract grammar rules for the dev set .", "label": "", "metadata": {}, "score": "76.72013"}
{"text": "test.in -- This is the input file containing the sentences to translate . example / example .nbest.srilm.out -- This is the output file that the n - best translations will be written to .You can inspect the output file by typing head example / example .", "label": "", "metadata": {}, "score": "76.864975"}
{"text": "A name is needed to distinguish this test set from the previous ones .Output for this test run will be stored at RUNDIR / test / NAME . --joshua - config CONFIG .A tuned parameter file is required .This file will be the output of some prior tuning run .", "label": "", "metadata": {}, "score": "76.97623"}
{"text": "Once the parsing is complete , there will be two parsed files : .RUNDIR / data / train / corpus.en.parsed : this is the mixed - case file that was parsed .RUNDIR / data / train / corpus.parsed.en : this is a leaf - lowercased version of the above file used for grammar extraction .", "label": "", "metadata": {}, "score": "77.139854"}
{"text": "This can either represent the effect of insufficient statistics in estimating them , or the use of imperfect parameter estimation biases .These parameters are probabilities , phrases , and associations between source / target phrases contained inside translation and language model tables .", "label": "", "metadata": {}, "score": "77.15379"}
{"text": "/path - to - srilm / bin/ i686 /ngram - count -order 5 -interpolate -kndiscount -text working - dir / lm / europarl .lowercased -lm working - dir / lm / europarl .lm .Train Model .Run training script : bin / moses - scripts / scripts- YYYYMMDD -", "label": "", "metadata": {}, "score": "77.34028"}
{"text": "CMU Sphinx - An open source toolkit for speech recognition , which includes a recognizer library written in C ; an adjustable , modifiable recognizer written in Java ; language model tools ; and acoustic model training tools .CTMaker - Real - time application generator for unified messaging systems ; supports TAPI , SAPI , sharing information between SQL - databases , emails , web - pages , phones , pagers .", "label": "", "metadata": {}, "score": "77.47345"}
{"text": "You cat then look at the 1-best output file by typing cat example / example .nbest.srilm.out.1best : . the goal of gene scientists is to provide diagnostic tools to found of the flawed genes , are still provide a to stop these genes treatments .", "label": "", "metadata": {}, "score": "77.4876"}
{"text": "That 's the end of the pipeline !Joshua also supports decoding further test sets .This is enabled by rerunning the pipeline with a number of arguments : . --first - step TEST .This tells the decoder to start at the test step . --name", "label": "", "metadata": {}, "score": "77.74501"}
{"text": "reference.tok .Lowercase test set scripts / lowercase .input scripts / lowercase . reference .Filter the model to fit into memory bin / moses - scripts / scripts- YYYYMMDD -HHMM /training / filter - model - given - input.pl working - dir / evaluation / filtered .", "label": "", "metadata": {}, "score": "77.749054"}
{"text": "Note the correspondences with the files defined in the first step above .The prefixes can be either absolute or relative pathnames .This particular invocation assumes that a subdirectory input/ exists in the current directory , that you are translating from a language identified \" ur \" extension to a language identified by the \" en \" extension , that the training data can be found at input / train .", "label": "", "metadata": {}, "score": "78.07649"}
{"text": "In our experience , this works fine , but you should note the following caveats : .It is of crucial importance that you have enough physical disks .We have found that having too few , or too slow of disks , results in a whole host of seemingly unrelated issues that are hard to resolve , such as timeouts .", "label": "", "metadata": {}, "score": "78.27822"}
{"text": "Conclusion .Data - driven solutions to classic AI problems are now commonplace , ranging from computer vision to information retrieval tasks , and machine translation is one of the main successes of this approach .The idea of putting learning systems at the centre of all AI methodologies introduces however the need to understand the properties and limitations of these learning components .", "label": "", "metadata": {}, "score": "78.36765"}
{"text": "Lowercasing .This creates a series of intermediate files which are saved for posterity but compressed .For example , you might see .data/ train/ train.en.gz train.tok.en.gz train.tok.50.en.gz train.tok.50 .lc.en .The file \" corpus .LANG \" is a symbolic link to the last file in the chain .", "label": "", "metadata": {}, "score": "78.694"}
{"text": "And in the current directory , you will see the following files ( among other intermediate files generated by the individual sub - steps ) .data/ train/ corpus.ur corpus.en thrax - input - file tune/ tune.tok.lc.ur tune.tok.lc.en grammar.filtered.gz grammar.glue test/ test.tok.lc.ur test.tok.lc.en grammar.filtered.gz grammar.glue alignments/ 0/ [ berkeley aligner output files ] training.align thrax - hiero . conf thrax.log grammar.gz lm.gz tune/ 1/ decoder_command joshua.config params.txt joshua.log mert.log joshua.config.ZMERT.final final - bleu .", "label": "", "metadata": {}, "score": "78.77435"}
{"text": "Allows you to jump into and out of the pipeline at a set of predefined places ( e.g. , the alignment stage ) , so long as you provide the missing dependencies .The Joshua pipeline script is designed in the spirit of Moses ' train-model.pl , and shares many of its features .", "label": "", "metadata": {}, "score": "79.036026"}
{"text": "For example , .Also , should Thrax fail , it might be due to a memory restriction .By default , Thrax requests 2 GB from the Hadoop server .If more memory is needed , set the memory requirement with the --hadoop - mem in the same way as the --joshua - mem option is used .", "label": "", "metadata": {}, "score": "79.11591"}
{"text": "W. Locke and A. Booth , Machine Translation of Languages , MIT Press , Cambridge , Mass , USA , 1955 .P. F. Brown , S. D. Pietra , V. J. D. Pietra , and R. L. Mercer , \" The mathematic of statistical machine translation : parameter estimation , \" Computational Linguistics , vol .", "label": "", "metadata": {}, "score": "79.23053"}
{"text": "Tuning is run till convergence in the RUNDIR / tune directory .By default , tuning is run just once , but the pipeline supports running the optimizer an arbitrary number of times due to recent work pointing out the variance of tuning procedures in machine translation , in particular MERT .", "label": "", "metadata": {}, "score": "79.24168"}
{"text": "185 - 188 , Prague , Czech Republic , 2007 .P. Koehn , \" Europarl : a parallel corpus for statistical machine translation , \" in Proceedings of the 10 th Machine Translation Summit , pp .79 - 86 , Phuket , Thailand , 2005 .", "label": "", "metadata": {}, "score": "79.93873"}
{"text": "It functions as a sophisticated dictionary between the source and target languages .Phrase tables and reordering tables are translation model components .pipeline .SMT .A \" pipeline \" is a toolchain of processes connected by standard streams , so that the output of each process ( stdout ) feeds directly as input ( stdin ) to the next one . recaser model .", "label": "", "metadata": {}, "score": "80.11983"}
{"text": "Google in [ 30 ] has shown that performance improves logarithmically in the linear scale with the number of tokens in the language model training set when this quantity is huge ( from billions to trillions of tokens ) .In this section , we are interested to understand whether there is a trade - off between the training data size used to build language and translation models and how performances are affected by their differences .", "label": "", "metadata": {}, "score": "80.13579"}
{"text": "Understanding how sophisticated behaviour can be learnt from data is hence not just a concern for machine learning , or to individual applied communities , such as statistical machine translation , but rather a general concern for modern artificial intelligence .The analysis of learning curves and the identification of the various limitations to performance are a crucial part of the machine learning method , and one where statistics and algorithms interact closely .", "label": "", "metadata": {}, "score": "80.153076"}
{"text": "The pipeline takes a set of inputs ( training , tuning , and test data ) , and creates a set of intermediate files in the run directory .By default , the run directory is the current directory , but it can be changed with the --rundir parameter .", "label": "", "metadata": {}, "score": "80.24602"}
{"text": "clean 1 40 .Lowercase training data scripts / lowercase .lowercased.fr scripts / lowercase .lowercased.en .Build Language Model .Tokenize English language model data mkdir -p working - dir / lm scripts / tokenizer .tok .Lowercase language model data scripts / lowercase . lowercased .", "label": "", "metadata": {}, "score": "80.25242"}
{"text": "Tutorials and the demo version of the library are available online .VDK is offered by NetResults S.r.l .a spin off company from the University Of Pisa in Italy .Voxi - Swedish company which develops and licenses a general - purpose speech recognition software platform , Intelligent Speech Interfaces ( TM ) , natural language understanding , and high - quality audio feedback .", "label": "", "metadata": {}, "score": "80.37363"}
{"text": "If you have a tuned model file , you can test new corpora by passing in a test corpus with references ( --test ) .You 'll need to provide a run name ( --name ) to store the results of this run , which will be placed under test / NAME .", "label": "", "metadata": {}, "score": "80.86834"}
{"text": "Estimating a machine translation system is therefore similar to learning the mapping between the source / input and the target / output , a problem which has been extensively studied in statistics and in machine learning .This justifies our view of a typical phrase - based machine translation model as a learning system and motivates our analysis of the performance on that system .", "label": "", "metadata": {}, "score": "80.87336"}
{"text": "es.tok.lc.grammar.raw \\ -o test / newstest2009 .es.tok.lc.grammar .Once the grammar extraction has completed , you can edit the joshua.config file for the test set .cp mert / joshua .config .ZMERT.final test / joshua . config .es.tok.lc.grammar .After you have done that , you can decode the test set with the following command : . config \\ test / newstest2009 .", "label": "", "metadata": {}, "score": "81.5644"}
{"text": "Desktop application users can keybind application controls to speech commands .Robots can be controlled via voice .Other features include voice mouse clicker .Vangard Voice Systems - Provides software tools to voice enable mobile and business applications .Voice recognition software tools for public safety , inventory management and logistics , medical IT , government , and customer records management .", "label": "", "metadata": {}, "score": "81.65744"}
{"text": "To accommodate this kind of variation , the pipeline script allows you to specify both ( a ) the amount of memory used by the Joshua decoder instance and ( b ) the amount of memory required of nodes obtained by the qsub command .", "label": "", "metadata": {}, "score": "81.79808"}
{"text": "Each run can be found in a directory RUNDIR / tune / N .When tuning is finished , each final configuration file can be found at either .RUNDIR / tune / N / joshua.config .ZMERT.final RUNDIR / tune / N / joshua.config . PRO.final .", "label": "", "metadata": {}, "score": "82.17926"}
{"text": "Free Subscription to Speech Technology - Speech Technology Magazine is recognized worldwide as a source of information on speech technology solutions that are changing communications and technology needs of organizations worldwide .HTK3 - Hidden Markov Toolkit - Development and distribution site for HTK3 , a hidden markov toolkit designed for speech recognition .", "label": "", "metadata": {}, "score": "82.37133"}
{"text": "This page describes the Joshua pipeline script , which manages the complexity of training and evaluating machine translation systems .The pipeline eases the pain of two related tasks in statistical machine translation ( SMT ) research : .Training SMT systems involves a complicated process of interacting steps that are time - consuming and prone to failure .", "label": "", "metadata": {}, "score": "83.25598"}
{"text": "If the program finishes right away , then it probably terminated with an error .You can read the nohup.out file to see what went wrong .Common problems include a missing example / test directory , or a file not found exception .", "label": "", "metadata": {}, "score": "83.54741"}
{"text": "Decode with Moses moses / moses - cmd / src / moses -config working - dir / evaluation / filtered .devtest2006/moses . ini -input - file working - dir / evaluation / devtest2006 .output .Evaluation .Train recaser bin / moses - scripts / scripts- YYYYMMDD - HHMM /recaser / train - recaser .", "label": "", "metadata": {}, "score": "83.75688"}
{"text": "The two effects interact with richer classes being better approximators of the target behaviour but requiring more training data to reliably identify the best hypothesis .The resulting trade - off , equally well known in statistics and in machine learning , can be expressed in terms of bias versus variance , capacity control , or model selection .", "label": "", "metadata": {}, "score": "83.82009"}
{"text": "GIZA++ is an extension of the program GIZA ( part of the SMT toolkit EGYPT ) which was developed by the Statistical Machine Translation team during the summer workshop in 1999 at the Center for Language and Speech Processing at Johns - Hopkins University ( CLSP / JHU ) .", "label": "", "metadata": {}, "score": "83.90803"}
{"text": "Please refer to the Moses website for an updated version .Install Moses Support Libraries .Copy GIZA++ and mkcls to a bin location for Moses Scripts mkdir -p bin cp GIZA++-v2/GIZA++ bin/ cp GIZA++-v2/snt2cooc . out bin/ cp mkcls - v2/mkcls bin/ .", "label": "", "metadata": {}, "score": "83.92789"}
{"text": "You may also use Giza++ to create the alignments , although that program is a little unwieldy to install .To run the Berkeley aligner you first need to set up a configuration file , which defines the models that are used to align the data , how the program runs , and which files are to be aligned .", "label": "", "metadata": {}, "score": "84.51202"}
{"text": "The Europarl corpus contains material extracted from the proceedings of the European parliament , and the UN data contains material from the United Nations .Both therefore cover a wide range of themes , but are fairly homogeneous in terms of style and genre .", "label": "", "metadata": {}, "score": "84.65413"}
{"text": "You can lowercase your tokenized data with the following script : . cat es - en / full - training / training .en.tok.lc cat es - en / full - training / training .es.tok.lc .Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "85.02368"}
{"text": "# Choose the training sources , which can either be directories or files that list files / directories trainSourcessubsampled/ .sentencesMAX .# 1-best output .competitiveThresholding .To run the Berkeley aligner , first set an environment variable saying where the aligner 's jar file is located ( this environment variable is just used for convenience in this document , and is not necessary for running the aligner in general : .", "label": "", "metadata": {}, "score": "85.18087"}
{"text": "Next , create a file called mert / decoder_command that contains the following command : . config \\ dev / news - dev2009 .es.tok.lc \\ mert / news - dev2009 .output.nbest .Next , create a configuration file for joshua at mert / joshua .", "label": "", "metadata": {}, "score": "85.45893"}
{"text": "# lm order weight .lm 1.0 .# phrasemodel owner column(0-indexed ) weight . phrasemodel pt 0 1.4037585111897322 . phrasemodel pt 1 0.38379188013385945 . phrasemodel pt 2 0.47752204361625605 .# arityphrasepenalty owner start_arity end_arity weight .# arityphrasepenalty pt 0 0 1.0 .", "label": "", "metadata": {}, "score": "85.929794"}
{"text": "A fully functional version can be downloaded for free containing over 100 built - in commands .SCARF - A Segmental Conditional Random Fields Speech - Recognition Tool Kit by Microsoft Research .It supports a broad spectrum of research into segmental features and detection - based speech recognition , and it is intended to support core academic research in this area .", "label": "", "metadata": {}, "score": "86.50853"}
{"text": "( This mode is triggered when $ HADOOP is undefined ) .Theoretically , any grammar extractable on a full Hadoop cluster should be extractable in standalone mode , if you are patient enough ; in practice , you probably are not patient enough , and will be limited to smaller datasets .", "label": "", "metadata": {}, "score": "86.90567"}
{"text": "The pipeline script needs to be told where to find the raw training , tuning , and test data .A good convention is to place these files in an input/ subdirectory of your run 's working directory ( NOTE : do not use data/ , since a directory of that name is created and used by the pipeline itself ) .", "label": "", "metadata": {}, "score": "87.21028"}
{"text": "This alters the amount of memory available to Hadoop mappers ( passed via the mapred.child.java.opts options ) .--thrax - conf FILE .Use the provided Thrax configuration file instead of the ( grammar - specific ) default .The Thrax templates are located at $ JOSHUA / scripts / training / templates / thrax - TYPE .", "label": "", "metadata": {}, "score": "87.24162"}
{"text": "HTML Citation JSON METS MODS MPEG-21 DIDL Multiline CSV Multiline CSV ( Staff ) OpenURL ContextObject OpenURL ContextObject in Span RDF+N - TriplesRDF+N3 RDF+XML Refer Reference Manager Simple Metadata .Abstract .Source localization from MEG data in real time requires algorithms which are robust , fully automatic , and very fast .", "label": "", "metadata": {}, "score": "87.32065"}
{"text": "For example , perhaps the decoder ran out of memory .This allows you to adjust the parameter ( e.g. , --joshua - mem ) and rerun the script .Of course , if you change one of the parameters a step depends on , it will trigger a rerun , which in turn might trigger further downstream reruns .", "label": "", "metadata": {}, "score": "87.387665"}
{"text": "Add the --lm - gen srilm flag to your pipeline invocation .More information on this is available in the LM building section of the pipeline .SRILM is not used for representing language models during decoding ( and in fact is not supported , having been supplanted by KenLM and BerkeleyLM ) .", "label": "", "metadata": {}, "score": "88.80397"}
{"text": "You can see a list of the other parameters available in our MERT implementation by running this command : . java -cp $ JOSHUA / bin joshua.zmert.ZMERT -h .Next , create a file called mert / params .txt that specifies what feature functions you are using in your mode .", "label": "", "metadata": {}, "score": "88.917786"}
{"text": "Used at hundreds of research sites world wide .Institute for Language and Speech Processing - Founded in Athens ( Greece ) in 1991 under the auspices of the General Secretariat of Research and Technology of the Ministry of Development .Language tools , Greek text to speech , language processing .", "label": "", "metadata": {}, "score": "89.34755"}
{"text": "FEEDBACK .Annosoft Lipsync Toolkit - Offers speech tools SDK for building automatic realtime animation lipsync , closed captioning and annotation applications .Clever downloadable demo animation lipsyncs to your microphone speech or .WAV files .Audio Search SDK - Compure develops software technologies to analyze audio .", "label": "", "metadata": {}, "score": "89.525764"}
{"text": "You should debug on the smaller set to avoid wasting time .Tokenization .Joshua uses whitespace to delineate words .For many languages , tokenization can be as simple as separating punctation off as its own token .For languages like Chinese , which do n't put spaces around words , tokenization can be more tricky .", "label": "", "metadata": {}, "score": "89.65686"}
{"text": "A single reference will have the format TUNE.TARGET , while multiple references will have the format TUNE.TARGET.NUM , where NUM starts at 0 and increments for as many references as there are .The following processing steps are applied to each file .Copying the files into RUNDIR / data / TYPE , where TYPE is one of \" train \" , \" tune \" , or \" test \" .", "label": "", "metadata": {}, "score": "89.73022"}
{"text": "VAX VoIP Software Development Kit - Enable applications and Web pages to have voice conversation over the internet protocol ( VoIP ) .It includes VoIP activeX ( OCX ) , VoIP COM DLL , VoIP LIB and VoIP CAB , to provide wide implementation choice .", "label": "", "metadata": {}, "score": "90.41467"}
{"text": "After tokenization and lowercasing , the file looks like this ( head -3 es - en / full - training / training .en.tok.lc ): . resumption of the session i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "90.64195"}
{"text": "# phrasemodel mono 0 0.5 .# wordpenalty weight . wordpenalty -2.721711092619053 .Finally , run the command to start MERT : . nohup java -cp $ JOSHUA / bin \\ joshua.zmert.ZMERT \\ -maxMem 1500 mert / mert . config & .", "label": "", "metadata": {}, "score": "90.69704"}
{"text": "Hadoop can be installed in a \" pseudo - distributed \" mode that allows it to use just a few machines or a number of processors on a single machine .The main issue is to ensure that there are a lot of independent physical disks , since in our experience Hadoop starts to exhibit lots of hard - to - trace problems if there is too much demand on the disks . )", "label": "", "metadata": {}, "score": "92.5025"}
{"text": "You can type pwd to get the absolute path to the sirlm/ directory that you created .Once you 've figured out the path , set an SRILM environment variable by typing : .Where \" /path / to / srilm \" is replaced with your path .", "label": "", "metadata": {}, "score": "92.62681"}
{"text": "The pipeline will unroll a standalone installation and use it to extract your grammar .This behavior will be triggered if $ HADOOP is undefined .Make sure that the environment variable $ JOSHUA is defined , and you should be all set .", "label": "", "metadata": {}, "score": "93.28256"}
{"text": "tok scripts / tokenizer .tok .Lowercase tuning sets scripts / lowercase .Run tuning script Note that this step can take many hours , even days , to run .bin / moses - scripts / scripts- YYYYMMDD -HHMM /training / mert - moses . pl working - dir / tuning / input working - dir / tuning / reference moses / moses - cmd / src / moses working - dir / model / moses . ini --working - dir working - dir / tuning --rootdir bin / moses - scripts / scripts- YYYYMMDD -", "label": "", "metadata": {}, "score": "93.58226"}
{"text": "Installation .The pipeline has no required external dependencies .However , it has support for a number of external packages , some of which are included with Joshua .GIZA++ is the default aligner .It is included with Joshua , and should compile successfully when you typed ant all from the Joshua root directory .", "label": "", "metadata": {}, "score": "93.98155"}
{"text": "For Mac OS X this usually is done by typing : .These variables will need to be set every time you use Joshua , so it 's useful to add them to your . bashrc , .bash_profile or .profile file .", "label": "", "metadata": {}, "score": "95.36433"}
{"text": "cd moses / scripts/ make release .This will create a folder named bin / moses - scripts / scripts- YYYYMMDD - HHMM with released versions of all the scripts .You will call these versions when training / tuning Moses .Moses scripts also require a SCRIPTS_ROOTDIR environment variable to be set .", "label": "", "metadata": {}, "score": "95.48712"}
{"text": "COMMON USE CASES AND PITFALLS .If the pipeline dies at the \" thrax - run \" stage with an error like the following : .JOB FAILED ( return code 1 ) hadoop / bin / hadoop : line 47 : /some / path / to / a / directory / hadoop / bin / hadoop - config .", "label": "", "metadata": {}, "score": "97.23548"}
{"text": "Learning to Translate : A Statistical and Computational Analysis .Received 15 July 2011 ; Accepted 30 January 2012 .Academic Editor : Peter Tino .Copyright \u00a9 2012 Marco Turchi et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "98.42254"}
{"text": "Run the pipeline .The following is the minimal invocation to run the complete pipeline : .$ JOSHUA / scripts / training / pipeline.pl \\ --corpus input / train \\ --tune input / tune \\ --test input / devtest \\ --source SOURCE \\ --target TARGET .", "label": "", "metadata": {}, "score": "98.52956"}
{"text": "After you have downloaded the srilm tar file , type the following commands to install it : . mkdir srilm mv srilm.tgz srilm/ cd srilm/ tar xfz srilm.tgz make .If the build fails , please follow the instructions in SRILM 's INSTALL file .", "label": "", "metadata": {}, "score": "99.12036"}
{"text": "Another useful flag is the --rundir DIR flag , which chdir()s to the specified directory before running the pipeline .By default the rundir is the current directory .Changing it can be useful for organizing related pipeline runs .Relative paths specified to other flags ( e.g. , to --corpus or --lmfile ) are relative to the directory the pipeline was called from , not the rundir itself ( unless they happen to be the same , of course ) .", "label": "", "metadata": {}, "score": "100.1501"}
{"text": "If you have a Hadoop installation , simply ensure that the $ HADOOP environment variable is defined , and the pipeline will use it automatically at the grammar extraction step .If you are going to attempt to extract very large grammars , it is best to have a good - sized Hadoop installation .", "label": "", "metadata": {}, "score": "102.48421"}
{"text": "Restarting failed runs .If the pipeline dies , you can restart it with the same command you used the first time .If you rerun the pipeline with the exact same invocation as the previous run ( or an overlapping configuration - one that causes the same set of behaviors ) , you will see slightly different output compared to what we saw above : .", "label": "", "metadata": {}, "score": "103.83752"}
{"text": "joshua.decoder.JoshuaDecoder -- This is the class that is run .If you want to look at the the source code for this class , you can find it in src / joshua / decoder / JoshuaDecoder.java .example / example .config.srilm -- This is the configuration file used by Joshua .", "label": "", "metadata": {}, "score": "104.222305"}
{"text": "Running ant will compile the Java classes and link in srilm .If everything works properly , you should see the message BUILD SUCCESSFUL .If you get a BUILD FAILED message , it may be because you have not properly set the paths to SRILM and JAVA_HOME , or because srilm was not compiled properly , as described above .", "label": "", "metadata": {}, "score": "104.25961"}
{"text": "Check out the Moses code via Subversion : mkdir -p moses svn co https://mosesdecoder.svn.sourceforge.net/svnroot/mosesdecoder/trunk moses .Compile Moses cd moses ./regenerate - makefiles . sh .Install Moses Scripts .Compile Moses Scripts The support scripts used by Moses are \" released \" by a Makefile which edits their paths to match your local environment .", "label": "", "metadata": {}, "score": "105.76727"}
{"text": "If you already have a grammar and wish to skip this step , you can do so passing the grammar with the --grammar GRAMMAR flag .The main variable in grammar extraction is Hadoop .If you have a Hadoop installation , simply ensure that the environment variable $ HADOOP is defined , and Thrax will seamlessly use it .", "label": "", "metadata": {}, "score": "111.52408"}
{"text": "You should really try to install physical disks that are dedicated to Hadoop scratch space .Here are some flags relevant to Hadoop and grammar extraction with Thrax : . --hadoop /path / to / hadoop .This sets the location of Hadoop ( overriding the environment variable $ HADOOP ) .", "label": "", "metadata": {}, "score": "116.3169"}
{"text": "NoClassDefFoundError: org / apache / hadoop / fs / FsShell Caused by : java.lang.ClassNotFoundException : org.apache.hadoop.fs.FsShell .This occurs if the $ HADOOP environment variable is set but does not point to a working Hadoop installation .To fix it , make sure to unset the variable : . # in bash unset HADOOP .", "label": "", "metadata": {}, "score": "122.55262"}
