{"text": "[ Charniak , Eugene , \" Statistical Language Learning \" , 1993 , MIT Press ] .Very generally in the VSM , the n - dimensional vector used to characterize the vocabulary for a particular document can be viewed as a signal , although the order of the terms in the vector is not related to chronological or narrative order .", "label": "", "metadata": {}, "score": "32.47401"}
{"text": "Thus , the search returns the documents containing a specific keyword by examining a single matrix .Once this term - by - document matrix is constructed , the individual documents within a dataset do not need to be searched when forming a response to a keyword query .", "label": "", "metadata": {}, "score": "33.04174"}
{"text": "Additional flexibility may also be gained by carrying out these procedures within the wavelet transform so that locally significant coefficients are retained .This effectively produces a \" local \" reduced vocabulary .In the preferred embodiment , the resultant matrix contains rows associated with the N topics and columns associated with the ( N+M ) topics and cross - terms .", "label": "", "metadata": {}, "score": "33.506107"}
{"text": "The process can then be repeated until all of the inputs have been labeled .This strategy is demonstrated in 1.7 .First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far .", "label": "", "metadata": {}, "score": "33.82802"}
{"text": "It aims to distill the most important information from a set of documents to generate a compressed summary .Given a sentence graph generated from a set of documents where vertices represent sentences and edges indic ... \" .Multi - document summarization has been an important problem in information retrieval .", "label": "", "metadata": {}, "score": "34.400894"}
{"text": "We present LDA - SP , which utilizes LinkLDA ( Erosheva et al ., 2004 ) to model selectional preferences .By simultaneously inferring latent topics and topic distri ... \" .The computation of selectional preferences , the admissible argument values for a relation , is a well - known NLP task with broad applicability .", "label": "", "metadata": {}, "score": "34.620125"}
{"text": "Before investigating in detail , it may be worth reviewing why this is a good null hypothesis .Many probabilistic language models treat language production as a stationary , ergodic process , and this idealization has been applied in a wide variety of research .", "label": "", "metadata": {}, "score": "34.834732"}
{"text": "In such an implementation , the reduced rank matrix may not be completely recombined , but instead the information from the matrix is used in its decomposed form .In step 160 , information retrieval is performed using the matrix A . k in response to a query .", "label": "", "metadata": {}, "score": "35.303726"}
{"text": "Rather , the item(s ) in question must be systematically over - represented in some documents , and systematically under - represented in other documents , relative to the variation that is expected .( It is also logically possible that the variance is less than predicted .", "label": "", "metadata": {}, "score": "36.68473"}
{"text": "For example , nearly all parametric statistical tests commonly used in the social sciences , such as the t -test and ANOVA , assume that samples are drawn from a stationary , ergodic process .Some readers may be more familiar with the equivalent phrasing that samples are INDEPENDENTLY AND IDENTICALLY DISTRIBUTED .", "label": "", "metadata": {}, "score": "36.91619"}
{"text": "In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has .Now that we 've defined our feature extractor , we can use it to train a new \" decision tree \" classifier ( to be discussed in 4 ): .", "label": "", "metadata": {}, "score": "37.246063"}
{"text": "Most algorithms choose the word with the highest frequency of occurrence among matching words based on a given corpus .Both trigram- and digram - based disambiguation have proven effective at the character or word level .While disambiguation works on a sequence of keystrokes mapped to a word or portion of a word , word prediction and completion methods use \" forecasting \" to add characters to a word stem to form a complete word .", "label": "", "metadata": {}, "score": "37.31282"}
{"text": "( b )During prediction , the same feature extractor is used to convert unseen inputs to feature sets .These feature sets are then fed into the model , which generates predicted labels .In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks .", "label": "", "metadata": {}, "score": "37.34141"}
{"text": "This process can be repeated for all desired topics .Once the matching is performed between the estimated singular values for each topic and all of the actual singular values of the term - by - document matrix , the result is a partitioning , by topic , of the actual term - by - document singular values .", "label": "", "metadata": {}, "score": "37.45829"}
{"text": "In particular , if the true variance of a process is much higher than the sample variance , there is a greatly inflated risk of a false positive ( Type I error ) .Many types of linguistic distributions follow a power law , and the sample variance for power law distributions is generally far smaller than the true variance ( Baayen , 2001 ) .", "label": "", "metadata": {}, "score": "37.67292"}
{"text": "( 3 ) creating a mathematical signal from the chronologically ordered , reduced vocabulary and the first several principal components from the association matrix or the full association matrix , .( 4 ) applying a discrete wavelet transform to this signal , and .", "label": "", "metadata": {}, "score": "38.10392"}
{"text": "Subject - matter knowledge and sematic priming . are in LSA .Predictive learning and text comprehension for humans .The research compares the effectiveness of five different text . algorithms ( Rocchio 's Algorithm , Decision Tree , Naive Bayes , Bayes .", "label": "", "metadata": {}, "score": "38.430992"}
{"text": "In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships .Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another .", "label": "", "metadata": {}, "score": "38.495598"}
{"text": "In step 110 , a term - by - document matrix A is formed .This matrix is an indexing matrix that can be used to identify the documents containing a certain term or terms .The matrix A can be formed by explicitly examining each term in each document of the set of documents to be searched .", "label": "", "metadata": {}, "score": "38.533676"}
{"text": "The result is better ordering of candidate words through contextual information .Simulations using various sizes of keypads show that the likelihood of the desired word appearing at the top of the list is significantly improved , and that the average number of extra keystrokes to cycle through a word list is reduced .", "label": "", "metadata": {}, "score": "38.9131"}
{"text": "Tree is PolyAnalyst 's fastest algorithm when dealing with large .amounts of attributes .Decision Tree report provides an easily .interpreted decision tree diagram and a predicted versus real table .Problems to Solve : .Classification of cases into multiple categories .", "label": "", "metadata": {}, "score": "39.00576"}
{"text": "None of this body of work consider the query information .More recently , some research effort has been made to incorporate the query information into the topic model .The model uses a multinomial distribution to select whether to sample a word from a document - specific , a query ... . \" ...", "label": "", "metadata": {}, "score": "39.13164"}
{"text": "The text engine will also reduce the amount of words contained within the document corpus by a variety of methods .For example , a text engine might remove stop words , stem words , filter the corpus according to word frequency or topicality , or perform some combination of these functions .", "label": "", "metadata": {}, "score": "39.163727"}
{"text": "PREDICTIVE DISAMBIGUATION WITH SMALL KEYPADS .Standard dictionary - based disambiguation uses word frequency information to decide the order of a list of matching words .The goal is to minimize the keystrokes for inputting more frequently used words .Given the results of many research studies on the problem of word prediction and/or disambiguation [ e.g. , 29 , 21 , 16 ] , it is reasonable to hypothesize that better word list ordering is achievable by considering previously entered text as contextual information .", "label": "", "metadata": {}, "score": "39.19387"}
{"text": "It is also said that it can produce a matrix that can be considered less \" noisy .\" Such a reduced rank matrix also can retrieve related term entries that would have been excluded based on the original term - by - document matrix due to synonymy .", "label": "", "metadata": {}, "score": "39.375576"}
{"text": "Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis .First , we select a development set , containing the corpus data for creating the model .This development set is then subdivided into the training set and the dev - test set .", "label": "", "metadata": {}, "score": "39.388325"}
{"text": "The Output : .The Decision Tree report starts of by giving measures resulting from .the decision tree .These measures are the Number of non - terminal .nodes , Number of leaves , and depth of the constructed tree .", "label": "", "metadata": {}, "score": "39.4691"}
{"text": "These functions each combine two terms , one which encourages the summary to be representative of the corpus , and the other which positively rewards diversity .Critically , our functions are monotone nondecreasing and su ... \" .We design a class of submodular functions meant for document summarization tasks .", "label": "", "metadata": {}, "score": "39.483635"}
{"text": "That is , the original matrix lists only the words actually in each document , whereas we might be interested in all words related to each document - generally a much larger set due to synonymy .The consequence of the rank lowering is that some dimensions are combined and depend on more than one term : .", "label": "", "metadata": {}, "score": "39.597282"}
{"text": "The results showed that there is more variation in the segmental frequency distribution of child input than expected under the unigram assumption .Thus , English segmental frequency distributions are not unigram .This fact has a deeper consequence .Recall that parametric statistical tests such as the t -test and ANOVA assume that samples are drawn from a stationary , ergodic process .", "label": "", "metadata": {}, "score": "39.654465"}
{"text": "This paper further proposes to use the multi - modality manifold - ranking algorithm for extracting topic - focused summary from multiple documents by considering the within - document sentence relationships and the cross - document sentence relationships as two separate modalities ( graphs ) .", "label": "", "metadata": {}, "score": "39.662197"}
{"text": ", 2004 ) to model selectional preferences .By simultaneously inferring latent topics and topic distributions over relations , LDA - SP combines the benefits of previous approaches : like traditional classbased approaches , it produces humaninterpretable classes describing each relation 's preferences , but it is competitive with non - class - based methods in predictive power .", "label": "", "metadata": {}, "score": "39.708107"}
{"text": "In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2Choosing The Right Features .Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model .", "label": "", "metadata": {}, "score": "40.02954"}
{"text": "Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % .It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : .", "label": "", "metadata": {}, "score": "40.051044"}
{"text": "j .j .i .l . ] j .i .j .j .l .j .j .t . )p .j .p .t .j .t .FIG .7 is a functional block diagram depicting an Internet search engine system according to an exemplary embodiment of the invention .", "label": "", "metadata": {}, "score": "40.059727"}
{"text": "In this article , the items and comparison set will be segments ( Experiment I ) or consonantal manner classes ( Experiment II ) unless explicitly noted otherwise .Characterizing linguistic frequency distributions .A phrase like \" the probability of [ l ] is 0\u00b7035 \" evokes a mental model known technically as a STATIONARY , ERGODIC PROCESS .", "label": "", "metadata": {}, "score": "40.53236"}
{"text": "The objective is to improve the ordering of candidate words using semantic and syntactic information in the sentence context .This is followed with the results of a simulation ( Section 6 ) and a user study ( Section 7 ) .", "label": "", "metadata": {}, "score": "40.57272"}
{"text": "In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters .In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label .", "label": "", "metadata": {}, "score": "40.897484"}
{"text": "Similarly , to get a frequency effect , artificial grammar learning studies of phonotactics ( Goldrick & Larson , 2010 ) and morphosyntax ( Hudson - Kam & Newport , 2009 ) have required much larger asymmetries than the aggregate differences observed here .", "label": "", "metadata": {}, "score": "40.928616"}
{"text": "Otherwise , your evaluation results may be unrealistically optimistic .Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features .Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label .", "label": "", "metadata": {}, "score": "40.9692"}
{"text": "This is the approach taken by Hidden Markov Models .Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags .However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags .", "label": "", "metadata": {}, "score": "41.081352"}
{"text": "More specifically , we propose two strategies to incorporate the query information into a probabilistic model .Experimental results on two different genres of data show that our proposed approach can effectively extract a multi - topic summary from a document collection and the summarization performance is better than baseline methods .", "label": "", "metadata": {}, "score": "41.12405"}
{"text": "There are also alternatives to statistical models , such as Stocky et al . 's system using semantic information derived from a \" commonsense knowledge base .\" [ 29 ] .The use of context is promising .There are at least two possibilities : physical context , such as location , time , or weather , and textual context , referring to the surrounding text .", "label": "", "metadata": {}, "score": "41.219074"}
{"text": "That is the why this ' social summary ' was conducted .Preprocessing II : phonological look - up .After preprocessing , a phonological representation of each input file was obtained by dictionary look - up .Each word was replaced by the phonological form listed in the CMU pronouncing dictionary ( version 0\u00b77a ; https://cmusphinx.svn.sourceforge.net/svnroot/cmusphinx/trunk/cmudict/cmudict.0.7a ) .", "label": "", "metadata": {}, "score": "41.44817"}
{"text": "Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels .Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set .", "label": "", "metadata": {}, "score": "41.513203"}
{"text": "It not only clearly defines different sets of text .categorization algorithms but also compare and contrast them for . different datasets so as to help system designers chose an appropriate .algorithm given their system constraints and application domains .( see .", "label": "", "metadata": {}, "score": "41.64541"}
{"text": "For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities .", "label": "", "metadata": {}, "score": "41.69639"}
{"text": "Thus the abscissa of the signal , in the preferred embodiment , is the narrative index in a view of the document without stop words which starts at one and goes to K. The terms that are either a topic or cross term ( i.e. survived the various filters : stop word list , stemming , document frequency and topicality ) are assigned their matching column of the Association Matrix .", "label": "", "metadata": {}, "score": "41.700138"}
{"text": "One way of conducting a query is to select a particular topic word or set of words and magnify the wavelet energy contained in the channels associated with those words .If the query words are topic terms for the article then there is little change required in the computational algorithm .", "label": "", "metadata": {}, "score": "41.78684"}
{"text": "LSA is an automatic mathematical algorithm for extracting . relationships in word usage in passages .It does n't use dictionaries , . external knowledge , or grammar rules .First , represent words as a . matrix , each row is a word , and each column is a text passage or . context .", "label": "", "metadata": {}, "score": "41.868866"}
{"text": "Do you find any of them surprising ?Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier .", "label": "", "metadata": {}, "score": "41.997925"}
{"text": "Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \" , \" .Next , the classifier checks if the word ends in \" the \" , in which case it 's almost certainly a determiner .", "label": "", "metadata": {}, "score": "42.002357"}
{"text": "Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data .These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering .", "label": "", "metadata": {}, "score": "42.038494"}
{"text": "[ 10 ] MATLAB and Python implementations of these fast algorithms are available .Unlike Gorrell and Webb 's ( 2005 ) stochastic approximation , Brand 's algorithm ( 2003 ) provides an exact solution .In recent years progress has been made to reduce the computational complexity of SVD ; for instance , by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality .", "label": "", "metadata": {}, "score": "42.089874"}
{"text": "If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing .", "label": "", "metadata": {}, "score": "42.163895"}
{"text": "The reference pattern may then be extracted from the given article or from a completely different context , e.g. one or more similar articles .More specifically , instead of taking a difference between two adjacent windows , the second vector in the difference is the sensor values averaged over the query terms .", "label": "", "metadata": {}, "score": "42.194054"}
{"text": "In general , simple classifiers always treat each input as independent from all other inputs .In many contexts , this makes perfect sense .For example , decisions about whether names tend to be male or female can be made on a case - by - case basis .", "label": "", "metadata": {}, "score": "42.26701"}
{"text": "This step helps to reduce the dimensionally of the vocabulary needed to describe the original document and produce a more focused list of words .Also in the text engine utilized in a preferred embodiment , a suffix and a prefix list may be used to help reduce each word to its stem .", "label": "", "metadata": {}, "score": "42.382164"}
{"text": "Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values .As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions .", "label": "", "metadata": {}, "score": "42.414906"}
{"text": "FIG .1 .This exemplary method 130 can establish the correspondence between the singular values of the term - by - document matrix A and the topics covered by those documents .This correspondence can be accomplished by estimating the singular values of the topics .", "label": "", "metadata": {}, "score": "42.612495"}
{"text": "Continuing on , the classifier checks if the word ends in \" s \" .1.5 Exploiting Context .By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix .", "label": "", "metadata": {}, "score": "42.73383"}
{"text": "Once we have a decision tree , it is straightforward to use it to assign labels to new input values .What 's less straightforward is how we can build a decision tree that models a given training set .But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \" decision stump \" for a corpus .", "label": "", "metadata": {}, "score": "42.75249"}
{"text": "LSA can use a term - document matrix which describes the occurrences of terms in documents ; it is a sparse matrix whose rows correspond to terms and whose columns correspond to documents .This matrix is also common to standard semantic models , though it is not necessarily explicitly expressed as a matrix , since the mathematical properties of matrices are not always used .", "label": "", "metadata": {}, "score": "42.759243"}
{"text": "The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree .A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples .", "label": "", "metadata": {}, "score": "42.805447"}
{"text": "c . i .l . ) prob .l . )The weight of each term in the sum is the probability of a document of length I appearing within the topic .These probabilities can be derived by sampling , or can be given as inputs from the Corpus Model .", "label": "", "metadata": {}, "score": "42.870773"}
{"text": "A decision tree is a simple flowchart that selects labels for input values .This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels .To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node .", "label": "", "metadata": {}, "score": "42.973442"}
{"text": "To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label .Every feature is assumed to be entirely independent of every other feature , given the label .", "label": "", "metadata": {}, "score": "42.98149"}
{"text": "3 , the recursive approach can provide more accurate estimations and can be more computationally efficient .In Equation 2 , l corresponds to the length of the documents and p i corresponds to the probability of the i th term in the current topic being processed .", "label": "", "metadata": {}, "score": "43.020508"}
{"text": "A potential limitation of the context - aware method is that the same ambiguous keystroke sequence can produce word lists with different orderings due to the current context .This concern is most relevant to expert users , and longitudinal studies could be conducted to determine whether initial benefits in text entry performance continue to outweigh any longer - term limitations .", "label": "", "metadata": {}, "score": "43.137787"}
{"text": "For example , 3DEF-6MNO-4GHI matches \" dog \" and \" fog \" .List ordering occurs through a combination of word or n -gram ( sequence of n characters ) frequency lists , user preferences , or prior word usage .Examples of commercial dictionary - based predictive disambiguation text entry systems are T9 by Tegic , iTap by Motorola , and eZiText by Zi Corp.", "label": "", "metadata": {}, "score": "43.25408"}
{"text": "In particular , for each consecutive word index i , a score is computed for each possible current and previous tag .This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences .", "label": "", "metadata": {}, "score": "43.25994"}
{"text": "The most relevant documents can be those documents that include the most instances of the keyword .For extremely large sets of documents with multiple keyword terms , the term - by - document matrix can become too large to manipulate during a keyword query .", "label": "", "metadata": {}, "score": "43.281105"}
{"text": "An alternative approach would be to simply delete the terms not in the reduced vocabulary and use the narrative index of the resulting compressed article as the abscissa .The critical element in creating the signal is that each word is assigned a vector of values that contains the interrelationships to all or an important subset of words in the document .", "label": "", "metadata": {}, "score": "43.30649"}
{"text": "The model can be applied without modification on new collections .( + )The model gives a very good retrieval quality in the SMART environment .( - )It is not ( yet ) clear what happens if the model is applied to documents that are longer and more complicated than the short documents ( abstracts ) for which it was developed .", "label": "", "metadata": {}, "score": "43.377964"}
{"text": "The returned dictionary , known as a feature set , maps from feature names to their values .Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' .", "label": "", "metadata": {}, "score": "43.427666"}
{"text": "history.append(tag ) .history.append(tag ) .return zip(sentence , history ) .1.7 Other Methods for Sequence Classification .One shortcoming of this approach is that we commit to every decision that we make .For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake .", "label": "", "metadata": {}, "score": "43.577263"}
{"text": "According to the Viterbi algorithm , we calculate n paths from the sentence delimiter to each t j of the n possible POS categories of word w i .However , for dictionary - based predictive disambiguation , the primary concern is not which POS tag is the most likely tag for word w i .", "label": "", "metadata": {}, "score": "43.63276"}
{"text": "The disadvantages of using such a function is that it is slow in . sorting out queries and irrelevant attributes can fool the neighbor .Decision Tree .The Decision Tree exploration engine , new to PolyAnalyst 4.1 , helps .solve the task of classifying cases into multiple categories .", "label": "", "metadata": {}, "score": "43.80497"}
{"text": "Stemming helps to reduce the dimensionality of the vocabulary needed to describe the original document and produce a more focused list of words .After stemming words may be referred to as terms ; two initially different words may now be mapped into the same term .", "label": "", "metadata": {}, "score": "43.82162"}
{"text": "Predictions from a SVM are not Probabilistic .In regression the SVM outputs a point estimate , and in classification , a ' hard ' binary decision .The initial SVM model was designed to separate two classes .In other words there was n't originally a SVM that was designed for multiple class separation .", "label": "", "metadata": {}, "score": "43.92106"}
{"text": "More particularly , a need exists in the art for selectively identifying the singular values of interest related to a dataset that has been annotated and stored in some matrix format .SUMMARY OF THE INVENTION .The inventive method of Selective Latent Semantic Indexing ( SLSI ) comprises a technique for reducing the rank of a term - by - document matrix in a way that can reduce or prevent the loss of topical coverage and that can give control over the amounts of each topic to cover .", "label": "", "metadata": {}, "score": "43.977196"}
{"text": "What can we learn about language from these models ?Along the way we will study some important machine learning techniques , including decision trees , naive Bayes ' classifiers , and maximum entropy classifiers .We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) .", "label": "", "metadata": {}, "score": "44.07287"}
{"text": "This approach has been shown to be effective with certain types of signals .In a quantization type scheme , the significant wavelet coefficients may be retained to a small precision ( i.e. if the original signal is in double precision and the wavelet coefficients are stored in single precision ) .", "label": "", "metadata": {}, "score": "44.13299"}
{"text": "Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features .We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value .", "label": "", "metadata": {}, "score": "44.136963"}
{"text": "The original term - document matrix is presumed noisy : for example , anecdotal instances of terms are to be eliminated .From this point of view , the approximated matrix is interpreted as a de - noisified matrix ( a better matrix than the original ) .", "label": "", "metadata": {}, "score": "44.151405"}
{"text": "As noted in Kukich 's 1992 survey [ 13 ] , bi - gram language models , built both on words and on word POS , are useful for correcting errors in user - entered text .However , the POS models described were based solely on bi - gram information , possibly missing other valuable syntactic relationships farther away .", "label": "", "metadata": {}, "score": "44.208168"}
{"text": "Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment .Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) .", "label": "", "metadata": {}, "score": "44.23807"}
{"text": "However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result .( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall .", "label": "", "metadata": {}, "score": "44.549347"}
{"text": "In Naive Bayes , Work well on numeric and textual data .What is the relevance for text classification ?Hello : Following is the explanation : In KNN , a decision boundary is the space or boundary , which separates different class samples from each other in the data .", "label": "", "metadata": {}, "score": "44.592262"}
{"text": "Samples were excluded if they contained less than 100 segments ( 4 - 5 utterances ) , since samples this small do not provide enough data to estimate relative frequencies for stationary , ergodic processes .Parametric statistics ( such as the t -test and ANOVA ) are well - established in the social sciences , in part because they make the simple and intuitive assumption that samples are drawn from a stationary , ergodic process .", "label": "", "metadata": {}, "score": "44.655716"}
{"text": "[ 2 ] Boggess , L. 1988 .Two simple prediction algorithms to facilitate text production .In Proceedings of the Second Conference on Applied Natural Language Processing ( Austin , TX , February 9 - 12 , 1998 ) .Association for Computational Linguistics , Morristown , NJ , 33 - 40 .", "label": "", "metadata": {}, "score": "44.689026"}
{"text": "Other notable studies in which phonological factors affect the acquisition of word meanings may be found in Stager and Werker ( 1997 ) and Imai , Kita , Nagumo and Okada ( 2008 ) .Thus , while the relation between form and meaning is not always completely arbitrary , the principle is so robustly established that it should be assumed until there is evidence to the contrary .", "label": "", "metadata": {}, "score": "44.72776"}
{"text": "database marketing , to automatically diagnosing patient in medicine , . and to determining customer attrition causes . in banking and insurance .Target Attribute : .The target attribute of a Decision Tree exploration must be of a .Boolean ( yes / no ) or categorical data type .", "label": "", "metadata": {}, "score": "44.736637"}
{"text": "Word prediction and completion are often used in alternative and augmentative communication ( AAC ) to reduce the keystrokes required by motor impaired users .Boggess [ 2 ] experimented with two simple prediction algorithms .Results showed that for a single user 's vocabulary , a prediction window of 50 words yielded about a 50 % success rate in predicting the next word .", "label": "", "metadata": {}, "score": "44.740154"}
{"text": "Also how do you index the data , are you using the Vector Space Model with simple boolean indexing or a more complicated measure such as TF - IDF ?Considering the low number of categories in your scenario a more complex measure will be beneficial as they can account for term importance for each category in relation to its importance throughout the entire dataset .", "label": "", "metadata": {}, "score": "44.819206"}
{"text": "The mathematical basis for this approach is the standard Vector Space Model ( VSM ) used in IR .In the VSM each document is represented as a vector of weights with each weight corresponding to a particular word or concept in the text .", "label": "", "metadata": {}, "score": "44.83734"}
{"text": "How do you think that your results might be different if you used a different feature extractor ?What features are relevant in this distinction ?Build a classifier that predicts when each word should be used .However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others .", "label": "", "metadata": {}, "score": "44.84177"}
{"text": "To begin with , they 're simple to understand , and easy to interpret .This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features .", "label": "", "metadata": {}, "score": "44.90349"}
{"text": "We could then use those interactions to adjust the contributions that individual features make .To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : .", "label": "", "metadata": {}, "score": "45.00064"}
{"text": "Figure 3 .Pseudocode for the context - aware predictive disambiguation method .ALGORITHM COEFFICIENTS .An important step is to linearly combine the frequency , semantic relatedness , and POS validities of each matching candidate word into overall estimated validities , so that all the candidate words can be properly sorted .", "label": "", "metadata": {}, "score": "45.116905"}
{"text": "What was done here was to set the subsample to include k documents from each corpus , with k varying from 1 to 10 .For a given run , here is what occurred .First , k documents were selected randomly ( without replacement ) from the child corpus , and another k were selected from the adult corpus .", "label": "", "metadata": {}, "score": "45.147907"}
{"text": "Once again , there are many distributions that are consistent with this new piece of information , such as : .But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) .", "label": "", "metadata": {}, "score": "45.15807"}
{"text": "A typical set of rules might include having at least three intervening token sequences between boundaries and specifying that all boundaries must be moved to the end of the nearest paragraph .In the VSM , certain \" filters \" are often used to identify the best words to characterize a document .", "label": "", "metadata": {}, "score": "45.195362"}
{"text": "The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features .For this example , we 'll start by just looking at the final letter of a given name .", "label": "", "metadata": {}, "score": "45.36022"}
{"text": "When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance .", "label": "", "metadata": {}, "score": "45.45987"}
{"text": "But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word .Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) .", "label": "", "metadata": {}, "score": "45.469387"}
{"text": "Given a sentence graph generated from a set of documents where vertices represent sentences and edges indicate that the corresponding vertices are similar , the extracted summary can be described using the idea of graph domination .In this paper , we propose a new principled and versatile framework for multi - document summarization using the minimum dominating set .", "label": "", "metadata": {}, "score": "45.470604"}
{"text": "ACM Press , New York , NY , 121 - 128 .[17 ] MacKenzie , I. S. , Kober , H. , Smith , D. , Jones , T. , and Skepner , E. 2001 .LetterWise : Prefix - based disambiguation for mobile text input .", "label": "", "metadata": {}, "score": "45.508118"}
{"text": "1.6 Sequence Classification .In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs .In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence .", "label": "", "metadata": {}, "score": "45.510605"}
{"text": "return features . words ( ' pos / cv957_8737 .The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) .", "label": "", "metadata": {}, "score": "45.545227"}
{"text": "Although it can be possible to gain insight by studying them , it typically takes a lot more work .But all explicit models can make predictions about new \" unseen \" language data that was not included in the corpus used to build the model .", "label": "", "metadata": {}, "score": "45.548042"}
{"text": "For a column representing a particular document , the elements going down the column can represent some function of the existence of terms within the document .For example , if term A is not used in document B , then the element in a term - by - document matrix that is in both row A and column B could be a zero to represent the absence of the term in the document .", "label": "", "metadata": {}, "score": "45.58217"}
{"text": "One way to capture this intuition that distribution ( i ) is more \" fair \" than the other two is to invoke the concept of entropy .In the discussion of decision trees , we described entropy as a measure of how \" disorganized \" a set of labels was .", "label": "", "metadata": {}, "score": "45.590405"}
{"text": "The framework used by supervised classification is shown in 1.1 .Figure 1.1 : Supervised Classification .( a )During training , a feature extractor is used to convert each input value to a feature set .These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section .", "label": "", "metadata": {}, "score": "45.60677"}
{"text": "The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used .Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another .", "label": "", "metadata": {}, "score": "45.65326"}
{"text": "Please read carefully the Google Answers Terms of Service .Thank you .Tools . \" ...We present an exploration of generative probabilistic models for multi - document summarization .Beginning with a simple word frequency based model ( Nenkova and Vanderwende , 2005 ) , we construct a sequence of models each injecting more structure into the representation of document set content and exhib ... \" .", "label": "", "metadata": {}, "score": "45.754257"}
{"text": "As evident from Figure 1 , the actual distribution of relative frequencies of [ l ] is considerably wider and flatter than the distribution that is expected under the null hypothesis of stationarity and ergodicity .In terms of the process that generated the child corpora , this can only mean that /l/ is systematically under - represented in some documents relative to its absolute frequency , and likewise over - represented in other documents .", "label": "", "metadata": {}, "score": "45.81793"}
{"text": "The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label .The label whose likelihood estimate is the highest is then assigned to the input value .5.1 illustrates this process .", "label": "", "metadata": {}, "score": "45.841766"}
{"text": "A document frequency filter specifies that a term must occur in at least A% of documents and in no more than B% of documents in order to be kept in the VSM vocabulary .Zipf 's Law states that the majority of words will occur once or twice , a few words will occur very often , but the most useful words for document discrimination occur a moderate amount of times .", "label": "", "metadata": {}, "score": "45.872936"}
{"text": "Thus , the input will never be assigned this label , regardless of how well the other features fit the label .In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur .", "label": "", "metadata": {}, "score": "45.9064"}
{"text": "Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word .This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features .", "label": "", "metadata": {}, "score": "45.933212"}
{"text": "Application of the wavelet transform to the signal .Mathematically the definition of the Haar wavelet coefficients is # # EQU4 # # where m is the channel , k is the multi - resolution level , and j corresponds approximately to the narrative index at which the filter is centered .", "label": "", "metadata": {}, "score": "45.949066"}
{"text": "query - document similarity for relevant documents , and will . simultaneously minimize .query - document similarity for non relevant documents .If we look at the basic formula for the Ricchio 's Algorithm , the .intuitive idea of Ricchio 's Algorithm is to iteratively increase the . weights of those terms contained in labeled relevant documents while .", "label": "", "metadata": {}, "score": "46.01865"}
{"text": "The SVD is typically computed using large matrix methods ( for example , Lanczos methods ) but may also be computed incrementally and with greatly reduced resources via a neural network -like approach , which does not require the large , full - rank matrix to be held in memory .", "label": "", "metadata": {}, "score": "46.030396"}
{"text": "The method of .A method for generating a reduced rank approximation for information retrieval , comprising : . forming on a computer a term - by - document matrix A , wherein the elements of the matrix A represent a plurality of terms within a plurality of documents , each of the documents being related to at least one of plurality of topics ; . identifying via the computer a plurality of actual singular values each having a corresponding singular vector ; . estimating via the computer a plurality of estimated singular values each corresponding to at least one of the topics ; . selecting via the computer at least one of the actual singular values based on the estimated singular values ; and .", "label": "", "metadata": {}, "score": "46.044952"}
{"text": "Having described the null hypothesis in some detail , the article turns now to the data which forms the empirical basis for this study - adult- and child - directed corpora from which ' the input ' samples were extracted .In addition , a ' social summary ' of the CHILDES ( child - directed ) corpus is given , including summary statistics as to how much speech was produced by various participant types .", "label": "", "metadata": {}, "score": "46.121243"}
{"text": "It would be interesting to conduct a longitudinal study to see if users can improve their performance by learning the way our system predicts word orderings .More performance gains may be achieved .Overall , while yielding a performance improvement , the context - aware method does not noticeably increase overall attention demands .", "label": "", "metadata": {}, "score": "46.179108"}
{"text": "A method for generating a reduced rank approximation for information retrieval , comprising : . forming on a computer a term - by - document matrix A , wherein the elements of the matrix A represent a plurality of terms within a plurality of documents , the documents related to a plurality of topics ; . estimating via the computer a plurality of singular values corresponding to at least one of the topics ; . identifying via the computer a plurality of actual singular values each having a corresponding singular vector associated with the matrix A ; . selecting via the computer a subset of the actual singular values based on actual singular values that correspond to at least one of the estimated singular values ; and .", "label": "", "metadata": {}, "score": "46.193535"}
{"text": "The choice of a sentence as the unit of context is based on results from two experiments using sentences and paragraphs as two forms of context .Our model is asymmetric since it is conditional - probability - based .Also note that the semantic relatedness model is built on word stems instead of the words themselves .", "label": "", "metadata": {}, "score": "46.270214"}
{"text": "The computer program embodiment depicted in .FIG .7 is exemplary and is not intended to limit the inventive indexing method which can also be applied to genetics , bioinformatics , financial data , social science data , or numerous other data mining or information retrieval applications .", "label": "", "metadata": {}, "score": "46.373543"}
{"text": "The test set serves in our final evaluation of the system .For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set .The division of the corpus data into different subsets is shown in 1.3 .", "label": "", "metadata": {}, "score": "46.410397"}
{"text": "The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent .", "label": "", "metadata": {}, "score": "46.429604"}
{"text": "Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs .The Brill tagger , described in ( 1 ) , is a good example of this strategy .", "label": "", "metadata": {}, "score": "46.43596"}
{"text": "( This guarantees that the mean of the expected distribution will align with the mean of the actual distribution . )A single run R k consisted of the following .The generated relative frequency for this document R k , i was defined as the total number of /l/ 's generated , divided by n[i ] .", "label": "", "metadata": {}, "score": "46.537605"}
{"text": "In this step , the reduced rank matrix A . k is generated by recombining the k selected singular values and their corresponding singular vectors to compute the matrix A . k , which is a reduced rank approximation of the matrix A. .", "label": "", "metadata": {}, "score": "46.660778"}
{"text": "Generating the visualization .A 2-D signal may also be created by choosing narrative index to be one variable and date of publication to be a second variable .Wavelet analysis is readily applicable to such multi - D signals .", "label": "", "metadata": {}, "score": "46.747345"}
{"text": "There is no sound theoretical basis for the assumption that documents that resemble the query , are relevant for the question .It s main advantage over the others is that it is easy to learn and implement although the classification accuracy may not be of the highest quality .", "label": "", "metadata": {}, "score": "46.788425"}
{"text": "In the preferred embodiment a mathematical signal is created from the narrative order of the words in the text .For example , suppose that there are K total words including duplicates left in the document after removal of stop words .", "label": "", "metadata": {}, "score": "46.802864"}
{"text": "This article adopts the position that this independence may be interpreted in a statistical sense ; this intuition forms the mathematical basis for the formal definition of ' the null hypothesis ' given in the next section .Prior to this , it must be acknowledged that language acquisition is unique in offering up so many exceptions to the arbitrariness of form - meaning relationships .", "label": "", "metadata": {}, "score": "46.84734"}
{"text": "Efforts to speed this process has led to research in the area of Information Retrieval ( IR ) , which has set a precedent for certain approaches as has research in applied Mathematics and Statistics .An example of this work is in automatic text theme identification with the end being to provide automated textual summaries of documents .", "label": "", "metadata": {}, "score": "46.847878"}
{"text": "For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified .", "label": "", "metadata": {}, "score": "46.94949"}
{"text": "See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK .3 Evaluation .In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model .The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it .", "label": "", "metadata": {}, "score": "47.069122"}
{"text": "As one example , Dominowska et al .[ 6 ] used contextual information to enhance a text communication system for disabled users by adjusting vocabularies based on user location .This was implemented , however , using preprogrammed context data rather than actual sensor data .", "label": "", "metadata": {}, "score": "47.080753"}
{"text": "Graph - based manifold - ranking methods have been successfully applied to topic - focused multi - document summarization .This paper further proposes to use the multi - modality manifold - ranking algorithm for extracting topic - focused summary from multiple documents by considering the within - document sentence ... \" .", "label": "", "metadata": {}, "score": "47.135113"}
{"text": "In the present case , the linguistic level of interest is the segment , rather than the word .It is a simple matter to define a BAG - OF - SEGMENTS model by analogy - there is a fixed probability \u03bc(\u03d5 ) for each manner class \u03d5 , and the probability of a document \u03b4 i containing the sequence [ \u03d5 1 \u03d5 2 . n \u03bc(\u03d5 j ) .", "label": "", "metadata": {}, "score": "47.205395"}
{"text": "FIG .1 is a logical flow diagram depicting a method 100 for selectively generating a reduced rank matrix approximation for information retrieval according to an exemplary embodiment of the invention .In initial step 105 , a corpus model is identified .", "label": "", "metadata": {}, "score": "47.21634"}
{"text": "But more importantly we can now treat the term and document vectors as a \" semantic space \" .The vector then has entries mapping it to a lower - dimensional space dimensions .These new dimensions do not relate to any comprehensible concepts .", "label": "", "metadata": {}, "score": "47.284676"}
{"text": "In the preferred embodiment , after the application of the stop word filter , the stemming filter and the document frequency filter , a topicality filter is applied .For each pseudo corpus window , a word frequency count expectation is calculated .", "label": "", "metadata": {}, "score": "47.458534"}
{"text": "Some documents in a given collection will be more or less popular .In addition , any given topic will usually have some terms that are much more common within that topic than other terms are .Random sampling and trivial data analysis techniques , apparent to one skilled in the art , lead to the ability to make realistic assumptions about the approximate parameters of a corpus model that would generate a given collection of documents .", "label": "", "metadata": {}, "score": "47.55543"}
{"text": "As a practical consideration , words with less than 10 occurrences were removed .In the end , vocabularies of about 16,170 word tokens were used in the simulations .As stated , the semantic relatedness model was built on word stems instead of entire words .", "label": "", "metadata": {}, "score": "47.557556"}
{"text": "When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made .A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i .", "label": "", "metadata": {}, "score": "47.592613"}
{"text": "Queries in the preferred embodiment .It is in the implementation of queries that the importance of the association matrix becomes clear .If zero - order statistics such as word frequency are used , as in the approach of Hearst , there will be no recognition of the synonymous use of different words .", "label": "", "metadata": {}, "score": "47.63237"}
{"text": "The reasoning for this is that some measures are better aimed toward feature selection / extraction , while others are preferable for feature weighting specifically in your document vectors ( i.e. the indexed data ) .This is generally due to dimension reduction measures being determined on a per category basis , whereas index weighting measures tend to be more document orientated to give superior vector representation .", "label": "", "metadata": {}, "score": "47.635468"}
{"text": "This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another .Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set .", "label": "", "metadata": {}, "score": "47.641003"}
{"text": "Dimensionality is reduced to approximate human cognition .LSA is a . theory of knowledge representation .Dimensionality reduction solves .the problem of \" insufficient evidence \" or \" poverty of the stimulus \" .LSA uses a matrix decomposition algorithm to reduce the . dimensionality .", "label": "", "metadata": {}, "score": "47.691574"}
{"text": "Average performances are plotted over each of these four periods .An important observation is that context - aware predictive disambiguation works in the background without significantly altering the way a user interacts with the text entry method .That is , the changes are almost transparent as far as the interaction is concerned .", "label": "", "metadata": {}, "score": "47.715603"}
{"text": "One of ordinary skill in the art will appreciate that certain steps of this method can be reusable across repeated applications or iterations of the method 100 without departing from the scope or spirit of the present invention .For example , the reduced rank matrix A . k developed in step 150 can be used repeatedly to process multiple information retrieval queries in step 160 .", "label": "", "metadata": {}, "score": "47.72173"}
{"text": "The Markov Random Walk model has been recently exploited for multi - document summarization by making use of the link relationships between sentences in the document set , under the assumption that all the sentences are indistinguishable from each other .However , a given document set usually covers a f ... \" .", "label": "", "metadata": {}, "score": "47.724564"}
{"text": "DESCRIPTION OF THE PREFERRED EMBODIMENT(S ) .The preferred embodiment of the present invention utilizes the following steps : .( 1 ) Creation of a pseudo - corpus of words from an individual document using an overlapping window partition , .", "label": "", "metadata": {}, "score": "47.819637"}
{"text": "Thanks .SUMMARY Vector Space Model The vector space model , is the most popular model and is claimed to be the best performing model for general document collections .Advantages and disadvantages : ( + )The model is relatively easy to understand .", "label": "", "metadata": {}, "score": "47.826447"}
{"text": "Note .Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings .But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute .", "label": "", "metadata": {}, "score": "47.847366"}
{"text": "The upshot of these studies is that whether a researcher is interested in syntax , phonology , semantics , or any other domain of language acquisition , careful attention must be given to how the samples were collected for any kind of naturally occurring data .", "label": "", "metadata": {}, "score": "47.85431"}
{"text": "The resultant signal may then be used to graphically display and interact with the sub - topic structure of the document .A method for automatically determining a semantic structure of an electronically formatted natural language based document consisting essentially of words , the method comprising the steps of : . a ) providing a numerical representation as a digital signal of the words within the document wherein said numerical representation contains some information relating the semantic content of the word to the semantic content of the document .", "label": "", "metadata": {}, "score": "47.85682"}
{"text": "Beginning with a simple word frequency based model ( Nenkova and Vanderwende , 2005 ) , we construct a sequence of models each injecting more structure into the representation of document set content and exhibiting ROUGE gains along the way .Our final model , HIERSUM , utilizes a hierarchical LDA - style model ( Blei et al .", "label": "", "metadata": {}, "score": "47.992302"}
{"text": "Otherwise , it is concluded that the null hypothesis provides an adequate explanation of the data .In the case of segmental frequency distributions , the relevant null hypothesis is that the distribution is stationary and ergodic .As noted previously , if a process is truly stationary and ergodic , we know how much variance there should be .", "label": "", "metadata": {}, "score": "47.998512"}
{"text": "Because the algorithm that finds a separating .hyperplane in the feature space can be stated entirely in terms of . vectors in the input space and dot products in the feature space , a .support vector machine can locate the hyperplane without ever . representing the space explicitly , simply by defining a function , . called a kernel function , that plays the role of the dot product in .", "label": "", "metadata": {}, "score": "48.163555"}
{"text": "Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs .This test set typically has the same format as the training set .", "label": "", "metadata": {}, "score": "48.16705"}
{"text": "The predictive disambiguation improvements in DA and KSPC metrics become meaningful to these users , as they reduce by about 15 % the number of extra keystrokes otherwise required .And about 4 % more often , the desired word is found immediately .", "label": "", "metadata": {}, "score": "48.17382"}
{"text": "[ 7 ] .Another model , termed Word Association Spaces ( WAS ) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs .", "label": "", "metadata": {}, "score": "48.219006"}
{"text": "This measure can be as simple as a count of how many times the term occurs within the document .Likewise , a more involved metric can be employed .Forming a term - by - document matrix with such elements lends to statistical notions for the use of the matrix and enables more detailed query responses .", "label": "", "metadata": {}, "score": "48.299065"}
{"text": "Previous research sought to improve predictive disambiguation using , for example , character - level digrams ( two - letter sequences ) [17 ] , grammatical and linguistic information [ 25 ] , and a commonsense knowledge base [ 29].", "label": "", "metadata": {}, "score": "48.322464"}
{"text": "Conventional Latent Semantic Indexing ( LSI ) employs a reduced rank version of the term - by - document matrix as an approximation of the original matrix .The approximation obtained has also been shown to be useful in increasing the overall information retrieval performance .", "label": "", "metadata": {}, "score": "48.33908"}
{"text": "To achieve an approximation of the term - by - document matrix that is of reduced rank k , the conventional LSI approach only retains the k largest singular values and sets all of the other singular values to zero .The resultant matrix is an approximation of the original term - by - document matrix but with a lower rank of k ( i.e. , including only the k largest singular values ) .", "label": "", "metadata": {}, "score": "48.422657"}
{"text": "It is to be hoped that this study underscores the importance of sample considerations in guarding against Type I and Type II errors at all stages of a research project , including data collection , data analysis , data interpretation , and peer review .", "label": "", "metadata": {}, "score": "48.42571"}
{"text": "give better classifier performance compared with other types of . features .SUMMARY .The paper addresses the problem of evaluating the effectiveness of .summarization techniques for the task of document categorization .It . is argued that for a large class of automatic categorization . algorithms , extraction - based document categorization can be viewed as .", "label": "", "metadata": {}, "score": "48.441963"}
{"text": "One example method for discovering the topics can be to compute several reduced rank - k LSI approximations of a term - by - document matrix A to find the sets of terms that are used most similarly across a collection of documents .", "label": "", "metadata": {}, "score": "48.457504"}
{"text": "This study examines frequency variation in manner - of - articulation classes in child and adult input .The null hypothesis is that segmental frequency distributions of language varieties are unigram ( modelable by stationary , ergodic processes ) , and that languages are unitary ( modelable as a single language variety ) .", "label": "", "metadata": {}, "score": "48.500336"}
{"text": "FIG .3 .From step 670 , the method 310 C proceeds to step 320 ( .FIG .3 ) .Equation 7 below provides a format for elements of the M matrix when the term - by - document matrix elements are either zero or one indicating the presence or absence of a term in a document and when all terms are equally likely .", "label": "", "metadata": {}, "score": "48.549088"}
{"text": "2 is a logical flow diagram depicting a method 130 for selecting a quantity k of singular values to retain in a reduced rank matrix A . k based on the singular values corresponding to the computed singular values of multiple topics according to an exemplary embodiment .", "label": "", "metadata": {}, "score": "48.60044"}
{"text": "The short text phrases were selected from MacKenzie and Soukoreff ' 's set of 500 phrases [ 19].Examples are shown in Table 2 .Computer simulation shows that the KSPC values for inputting the entire set of testing phrases using the standard predictive disambiguation and our context - aware predictive disambiguation methods are 1.4467 and 1.3273 , respectively .", "label": "", "metadata": {}, "score": "48.641186"}
{"text": "Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens .Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' :", "label": "", "metadata": {}, "score": "48.65575"}
{"text": "Heller J. , , Pierrehumbert J. B. , & Rapp D. N ..( Year : 2010 ) .Predicting words beyond the syntactic horizon : Word recurrence distributions modulate on - line long - distance lexical predictability .Paper presented at Architectures and Mechanisms for Language Processing ( AMLaP ) 2010 .", "label": "", "metadata": {}, "score": "48.690872"}
{"text": "False positives are a pernicious issue in behavioral research owing to publication bias ( Rosenthal , 1979 ) , and the bias against publishing null results may be especially strong in child language research .Thus , the goal of Experiment I is to check whether segmental frequency distributions actually are stationary and ergodic .", "label": "", "metadata": {}, "score": "48.713303"}
{"text": "Critically , our functions are monotone nondecreasing and submodular , which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality .When evaluated on DUC 2004 - 2007 corpora , we obtain better than existing state - of - art results in both generic and query - focused document summarization .", "label": "", "metadata": {}, "score": "48.760895"}
{"text": "For example , consider the part - of - speech tagging task .At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : .", "label": "", "metadata": {}, "score": "48.820976"}
{"text": "We call these values w[label ] and w[f , label ] the parameters or weights for the model .Using the naive Bayes algorithm , we set each of these parameters independently : .However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values .", "label": "", "metadata": {}, "score": "48.827057"}
{"text": "( i )The discourse topic can induce large variations in the local segmental frequency .Presumably this point holds even more strongly for other linguistic domains , such as the occurrence of lexical items and syntactic constructions .When comparing two sets of samples , it is important to control the discourse topic to the same extent in both sets .", "label": "", "metadata": {}, "score": "48.853264"}
{"text": "There are many probability distributions that we could choose for the ten senses , such as : .Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other .", "label": "", "metadata": {}, "score": "48.856575"}
{"text": "The improvement also approximates the predicted theoretical reduction of 8.3 % of the total keystrokes required .Table 3 Text entry speeds and error rates for frequency - based and context - aware predictive disambiguation methods .The last column gives the p value associated with the t test for the difference between the two values .", "label": "", "metadata": {}, "score": "48.892815"}
{"text": "Categorical or Boolean ( Yes / No ) attribute .Output Format : .Classification statistics .Predicted versus Real table ( confusion matrix ) .Decision Tree diagram .Optimal Number of Records : .Minimum of 100 records .Maximum of 5,000,000 records .", "label": "", "metadata": {}, "score": "48.932865"}
{"text": "This term - by - document matrix structure enables response to keyword search queries .The row of the term - by - document matrix that corresponds to the queried keyword is examined by the information retrieval system .Elements in that row indicate inclusion of that keyword term within the documents represented by those columns .", "label": "", "metadata": {}, "score": "48.976337"}
{"text": "In that case , \" term \" means a word or phrase and a \" document \" is a collection of terms .However , generalized meanings of \" term \" and \" document \" can apply , as discussed hereinafter .", "label": "", "metadata": {}, "score": "49.082573"}
{"text": "For example , the singular values corresponding to one or more topics may be known from models of the data set , other estimation procedures , or empirical calculations made directly on the actual data set or model data sets .In step 210 , a topic can be selected from the set of desired topics .", "label": "", "metadata": {}, "score": "49.104824"}
{"text": "In a preferred embodiment , a principal component analysis ( PCA ) is then performed on the ( N ) rows in the Association Matrix .In proof of principal experiments designed to demonstrate the efficacy of the present invention , the mean was not subtracted out ; however this might be advantageous , especially since wavelet analysis is insensitive to the mean .", "label": "", "metadata": {}, "score": "49.114876"}
{"text": "The blind selection of the k largest singular values can result in the removal of information that loses the connection between a topic and certain keywords .Retaining only the largest singular values can allow the term - document relationships of more common topics to dominate the reduced rank matrix at the cost of the removal of the term - document relationships of less frequently represented topics .", "label": "", "metadata": {}, "score": "49.146553"}
{"text": "It is very useful to help users grasp the main information related to a query .Existing work can be mainly classified into two categories : supervised method and unsupervised method .T ... \" .Query - oriented summarization aims at extracting an informative summary from a document collection for a given query .", "label": "", "metadata": {}, "score": "49.153"}
{"text": "However , in that study the conclusion was that the null hypothesis ( that the event of interest did not occur in the input ) might be falsely accepted , i.e. a Type II error .The present study argues the complementary point that even for relatively frequent items , there is a substantial risk of mis - estimating relative frequency , owing to the large degree of variability between samples .", "label": "", "metadata": {}, "score": "49.162872"}
{"text": "Discussion of the Simulation Results .From Table 1 , it is clear that the semantic relatedness and POS models consistently helped the dictionary - based predictive disambiguation method to achieve better disambiguation performances in terms of both the DA and KSPC metrics .", "label": "", "metadata": {}, "score": "49.17569"}
{"text": "Simple stochastic tagging may be based on the probability of the occurrence of a given sequence of tags .This is like an n -gram / Hidden Markov model , where the best tag is determined by the n previous tags [ 20 , 31].", "label": "", "metadata": {}, "score": "49.20746"}
{"text": "Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn .This is especially true when the size of the training set , the number of features , and the number of labels are all large .", "label": "", "metadata": {}, "score": "49.217873"}
{"text": "The terms which have survived the previous filters then go on to be classified as topics or cross - terms .In the preferred embodiment there is a different document frequency filter for topics and cross - terms .Those terms with the largest topicality measure are called topics .", "label": "", "metadata": {}, "score": "49.222042"}
{"text": "A method for automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure .Specifically , the document is converted to an electronic signal and a wavelet transform is then performed on the signal .A method for automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure .", "label": "", "metadata": {}, "score": "49.26287"}
{"text": "The topicality filter is in part a denoising algorithm as is the application of stop word list , stemming algorithms , and document frequency filters .However , denoising may be accomplished via the wavelet transform itself , so the application of these filters may not be necessary .", "label": "", "metadata": {}, "score": "49.30861"}
{"text": "Thus significant advantage in computational speed is gained by compressing via thresholding ( replacing small entries with zeros ) .Multi - dimensional Scaling ( MDS ) is a standard statistical method used on multi - variate data .In MDS , N objects are represented as d - dimensional vectors with all pairwise similarities or dissimilarities ( distances ) defined between the N objects .", "label": "", "metadata": {}, "score": "49.3666"}
{"text": "These posts have all been labeled with one of 15 dialogue act types , such as \" Statement , \" \" Emotion , \" \" ynQuestion \" , and \" Continuer .\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts .", "label": "", "metadata": {}, "score": "49.414448"}
{"text": "Of course , this assumption is unrealistic ; features are often highly dependent on one another .We 'll return to some of the consequences of this assumption at the end of this section .This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another .", "label": "", "metadata": {}, "score": "49.439384"}
{"text": "Recent studies have shown that Ricchio 's Algorithm has a poor .performance when the proportion of relevant documents in the whole .corpus is low .Naive Bayes .Naive Byes algorithms are among the most successful known algorithms .for learning to classify text documents .", "label": "", "metadata": {}, "score": "49.446823"}
{"text": "It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value .In order to build a decision stump , we must first decide which feature should be used .", "label": "", "metadata": {}, "score": "49.45011"}
{"text": "The logic of this method is to explicitly simulate a process according to some ( null ) hypothesis , and generate data samples some large number of times ( e.g. 1000 ) .The generated data are compared to real data .", "label": "", "metadata": {}, "score": "49.563103"}
{"text": "For document topic identification , we can define a feature for each word , indicating whether the document contains that word .To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus .", "label": "", "metadata": {}, "score": "49.565018"}
{"text": "Matrix transformations are used in .information retrieval and human cognition models .A web site provides .LSA based word or passage vectors , similarities between words and .words , words and passages , and passages and passages .LSA is able to .", "label": "", "metadata": {}, "score": "49.621967"}
{"text": "As with glides , a few high - frequency words comprise most of the observed numerical difference ; so glides do not appear to be unique in this property . )What this means is that if the words you , what , your , what 's , and want were removed from both the child corpus and the adult corpus , the resulting relative frequencies of glides between the child and adult samples would be essentially identical .", "label": "", "metadata": {}, "score": "49.644497"}
{"text": "Individual features make their contribution to the overall decision by \" voting against \" labels that do n't occur with that feature very often .In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature .", "label": "", "metadata": {}, "score": "49.65355"}
{"text": "4 .In the preferred embodiment , the 3-D representation is created by first selecting several energy level and multi - resolution level pairs for various locations on the \" Wave .\" This in turn will define a collection of thematic chunks at each multi - resolution level as described above .", "label": "", "metadata": {}, "score": "49.69579"}
{"text": "These conclusions contrast with the findings of Lee and Davis ( 2010 ) , the only other study to specifically compare segmental frequencies in child- versus adult - directed speech .Those researchers found significant differences for every manner investigated ( as well as for other segmental frequencies investigated , such as stop places of articulation ) .", "label": "", "metadata": {}, "score": "49.75557"}
{"text": "How likely is a given input value with a given label ?What is the most likely label for an input that might have one of two values ( but we do n't know which ) ?The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier .", "label": "", "metadata": {}, "score": "49.761497"}
{"text": "We can treat RTE as a classification task , in which we try to predict the True / False label for each pair .Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level .", "label": "", "metadata": {}, "score": "49.771572"}
{"text": "These values are solid proofs of the success of the model I used .This is what I have done so far .Now working on clustering and reduction models , have tried LDA and LSI and moving on to moVMF and maybe spherical models ( LDA + moVMF ) , which seems to work better on corpus those have objective nature , like news corpus .", "label": "", "metadata": {}, "score": "49.804955"}
{"text": "English segmental frequencies lack the unigram property - the between - document variation significantly exceeds what is expected if segments were generated by a stationary , ergodic process .Therefore , it is in general unsafe to use parametric statistics such as the t -test or ANOVA to compare segmental frequency distributions from different language varieties .", "label": "", "metadata": {}, "score": "49.890213"}
{"text": "They specifically noted a similar effect for velar - stop - containing toys ( p. 780 ) .Since these differences are conditioned by the specific toys involved , they represent properties of the samples rather than properties of infant input versus adult input as a whole .", "label": "", "metadata": {}, "score": "49.90506"}
{"text": "We first develop an entity - aspect LDA model to simultaneously cluster both sentences and words into aspects .We then apply frequent subtree pattern mining on the dependency parse trees of the clustered and labeled sentences to discover sentence patterns that well represent the aspects .", "label": "", "metadata": {}, "score": "49.960274"}
{"text": "This technique avoids the computational burden of .explicitly representing the feature vectors .Latent Semantic Analysis .Latent Semantic Analysis ( LSA ) analyzes word - word , word - passage , and . passage - passage relationships .There 's a good relationship between .", "label": "", "metadata": {}, "score": "49.968422"}
{"text": "In step 420 , a vector v is formed with t elements representing the probabilities of the terms in the topic .For example , if a first term is more likely to occur within a topic than a second term , then the element of vector v corresponding to the first term will be larger than the element of vector v corresponding to the second term .", "label": "", "metadata": {}, "score": "49.99353"}
{"text": "In the preferred embodiment , only the first several principal components of the Association Matrix were utilized , and the columns were selected from this compressed matrix representation .Each channel is then identified with a PCA component rather than a specific term .", "label": "", "metadata": {}, "score": "50.056847"}
{"text": "Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) .", "label": "", "metadata": {}, "score": "50.06993"}
{"text": "So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ?One problem that arises is that the classifier can end up \" double - counting \" the effect of highly correlated features , pushing the classifier closer to a given label than is justified .", "label": "", "metadata": {}, "score": "50.079994"}
{"text": "40631 mack@cse.yorku.ca .ABSTRACT We present a design methodology for small ambiguous keypads , where input often produces a list of candidate words for a given desired word .The methodology uses context through semantic relatedness and a part - of - speech language model to improve the order of candidate words and , thus , reduce the overall number of keystrokes per character entered .", "label": "", "metadata": {}, "score": "50.170616"}
{"text": "The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline .Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline .", "label": "", "metadata": {}, "score": "50.23262"}
{"text": "In Experiment II , this question is addressed by repeatedly taking small subsamples from the child and adult corpora and , for each manner , determining how often the mean relative frequency of the manner is greater ( smaller ) in the child subsample than in the adult subsample .", "label": "", "metadata": {}, "score": "50.25313"}
{"text": "When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation .Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set .", "label": "", "metadata": {}, "score": "50.352894"}
{"text": "I examined and experimented all the methods you bring forward and others .Pre - processing is also an important aspect for this task as you mentioned .I used certain types of string elimination for refining the data as well as morphological parsing and stemming .", "label": "", "metadata": {}, "score": "50.358208"}
{"text": "This corpus model can be the source of the probability distributions used in this method .The corpus model can be identified by estimating the model or the method can be provided the model as an input .Given a document collection , realistic simplifications can be made about the corpus model underlying it .", "label": "", "metadata": {}, "score": "50.418274"}
{"text": "transformation .Save the log word frequency , and the entropy for each .row and column of the word .Weight each word occurrence by an estimate . of its importance in the passage .Knowing a word provides information .", "label": "", "metadata": {}, "score": "50.447563"}
{"text": "Yet another example metric uses a term frequency technique where the value of each element in the matrix represents how often the corresponding term appears within the corresponding document .Various other term - by - document matrix formations will be apparent to those of ordinary skill in the art .", "label": "", "metadata": {}, "score": "50.519325"}
{"text": "28 ] Seymore , K. and Rosenfeld , R. 1996 .Scalable backoff language models .In Proceedings of the Fourth International Conference on Spoken Language Processing ( Philadelphia , PA , October 3 - 6 , 1996 ) .ICSLP ' 96 .", "label": "", "metadata": {}, "score": "50.535797"}
{"text": "One solution to this problem is to stop dividing nodes once the amount of training data becomes too small .Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test .", "label": "", "metadata": {}, "score": "50.571922"}
{"text": "The selected k singular values will be used to form an approximation of the matrix A having a reduced rank of k , as discussed in more detail hereinafter with reference to step 150 .The singular values can be selected based on the expected singular values of one or more topics .", "label": "", "metadata": {}, "score": "50.605225"}
{"text": "Latent semantic analysis ( LSA ) is a technique in natural language processing , in particular distributional semantics , of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms .", "label": "", "metadata": {}, "score": "50.629196"}
{"text": "Request for Answer Clarification by mcsemorgan - ga on 28 Aug 2003 12:48 PDT .I have some questions in the advantages and disadvantages part : 1 .Do you have a reference for this statement ?In Rocchio 's Algorithm , what is \" Constant and are empirical \" ? ?", "label": "", "metadata": {}, "score": "50.657127"}
{"text": "The Decision Tree exploration engine is used for task such as .classifying records or predicting outcomes .You should use decision .trees when you goal is to assign your records to a few broad . categories .Decision Trees provide easily understood rules that can .", "label": "", "metadata": {}, "score": "50.667343"}
{"text": "The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) .return sents . 2.2 Identifying Dialogue Act Types .When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker .", "label": "", "metadata": {}, "score": "50.82396"}
{"text": "For example , a compression algorithm such as hard thresholding would be an example of a straightforward approach .Alternatively , a particular type of soft thresholding may be best suited for certain signals .As an alternative to the preferred embodiment , the fast wavelet transform algorithm is important to improve the efficiency of the procedure , especially for large documents where long filters need to be applied .", "label": "", "metadata": {}, "score": "50.855118"}
{"text": "the document and , in this context , its impact can be compared with . state - of - the - art feature selection techniques especially devised to .provide good categorization performance .Such a framework provides for .a better assessment of the expected performance of a categorizer if .", "label": "", "metadata": {}, "score": "50.872406"}
{"text": "given image is , say , the number 9 .The SVM algorithm operates by mapping the given training set into a .possibly high - dimensional feature space and attempting to locate in .that space a plane that separates the positive from the negative . examples .", "label": "", "metadata": {}, "score": "50.89443"}
{"text": "We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) .", "label": "", "metadata": {}, "score": "50.904846"}
{"text": "Fit ( LLSF ) mapping and a NaiveBayes ( NB ) classifier .We focus on the robustness of these methods in dealing .with a skewed category distribution , and their performance .as function of the training - set category frequency .", "label": "", "metadata": {}, "score": "50.953342"}
{"text": "In step 610 , a possible document length is selected .In step 620 , a square matrix M with t rows and t columns is formed according to the type of term - by - document matrix that is being analyzed .", "label": "", "metadata": {}, "score": "50.96082"}
{"text": "One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation .In particular , we subdivide the original corpus into N subsets called folds .", "label": "", "metadata": {}, "score": "50.99991"}
{"text": "The study was performed in a controlled laboratory environment .Before testing , participants completed a questionnaire on background information concerning their use of mobile devices .They were then introduced to the standard dictionary - based predictive disambiguation text entry method and the operation of our 5-key keypad design , including inputting characters , cycling through candidate lists , committing words and phrases , and correcting errors .", "label": "", "metadata": {}, "score": "51.015793"}
{"text": "[ 13 ] Kukich , K. ( 1992 ) .Technique for automatically correcting words in text .ACM Computing Surveys 24 ( Dec. 1992 ) , 377 - 439 .[14 ] Levine , S. H. and Goodenough - Trepagnier , C. 1990 .", "label": "", "metadata": {}, "score": "51.0246"}
{"text": "Each . word frequency in the Next , LSA applies singular value decomposition .( SVD ) to the matrix , which is factor analysis .The decomposition .results in dimensionality reduction .Extract words from the passage . into a word matrix , do a linear decomposition of the matrix , then . reduce the dimensionality of the matrix .", "label": "", "metadata": {}, "score": "51.04816"}
{"text": "The Ricchio 's Algorithm is based on the Relevancy Feedback Algorithms . for Document Relevancy .Relevancy Feedback Models are an effective way . of modifying and expanding user queries ( such as search engines ) .Ricchio 's Algorithm is one of the earliest methods used for queries .", "label": "", "metadata": {}, "score": "51.0504"}
{"text": "This kind of summary templates can be useful in various applications .We first develop an entity - aspect LDA model to simultaneously cluster both sentences and words int ... \" .In this paper , we propose a novel approach to automatic generation of summary templates from given collections of summary articles .", "label": "", "metadata": {}, "score": "51.068123"}
{"text": "The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres .What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training .", "label": "", "metadata": {}, "score": "51.238846"}
{"text": "Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set .Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution .", "label": "", "metadata": {}, "score": "51.269165"}
{"text": "But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have .If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label .", "label": "", "metadata": {}, "score": "51.303497"}
{"text": "Term frequency counts within a particular topic can be calculated which will give an estimate of the term distributions within a topic .Document lengths and topic probabilities can also be estimated by sampling , giving every parameter of a corpus model that could have generated a collection of documents .", "label": "", "metadata": {}, "score": "51.323975"}
{"text": "A custom script was used to parse the header .Utterances were then extracted from the file and parsed to determine the speaker ( there was no need to determine utterance boundaries since they are marked in CHILDES ) .Utterances spoken by the target were discarded to isolate the input to the target ( only the orthographic form was copied ; information such as POS tags was discarded ) .", "label": "", "metadata": {}, "score": "51.339375"}
{"text": "the input space occupied by the training examples .With an .appropriately chosen feature space of sufficient dimensionality , any .consistent training set can be made separable .However , translating .the training set into a higher - dimensional space incurs both . computational and learning - theoretic costs .", "label": "", "metadata": {}, "score": "51.40169"}
{"text": "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions .An example will help to illustrate this principle .Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) .", "label": "", "metadata": {}, "score": "51.469246"}
{"text": "Creating the Pseudo - corpus .To get the best terms possible for individual document analysis , a pseudo - corpus for an individual document is created prior to making a digital signal .The pseudo - corpus is created as the original document is partitioned into overlapping windows of a fixed word size and word overlap .", "label": "", "metadata": {}, "score": "51.503304"}
{"text": "SVMs elegantly sidestep both difficulties .They avoid overfitting by .choosing the maximum margin separating hyperplane from among the many .that can separate the positive from negative examples in the feature . space .Also , the decision function for classifying points with respect .", "label": "", "metadata": {}, "score": "51.53051"}
{"text": "In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : .Furthermore , the remaining probabilities appear to be \" evenly distributed .", "label": "", "metadata": {}, "score": "51.55149"}
{"text": "1 is a logical flow diagram depicting a method for selectively generating a reduced rank matrix approximation for information retrieval according to an exemplary embodiment of the invention .FIG .2 is a logical flow diagram depicting a method for selecting k singular values based on their correspondence to singular values of multiple topics according to an exemplary embodiment of the invention .", "label": "", "metadata": {}, "score": "51.557663"}
{"text": "Experimental results on the DUC benchmark datasets demonstrate the effectiveness of the proposed multi - modality learning algorithms with all the three fusion schemes .A term - by - document ( or part - by - collection ) matrix can be used to index documents ( or collections ) for information retrieval applications .", "label": "", "metadata": {}, "score": "51.55903"}
{"text": "The resulting dimensions might be difficult to interpret .For instance , in .However , it is very likely that cases close to . will occur .This leads to results which can be justified on the mathematical level , but have no interpretable meaning in natural language .", "label": "", "metadata": {}, "score": "51.63666"}
{"text": "What our algorithm learns are the mixture weights over such shells .We provide a risk bound guarantee when learning in a large - margin structured - prediction setting using a projected subgradient method when only approximate submodular optimization is possible ( such as with submodular function maximization ) .", "label": "", "metadata": {}, "score": "51.66017"}
{"text": "On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score .4 Decision Trees .In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers .", "label": "", "metadata": {}, "score": "51.669495"}
{"text": "Therefore , before looking into more complicated feature selection measures there are a number of much simpler possibilities that will typically require much lower resource consumption .Do you pre - process the documents before performing tokensiation / representation into the bag - of - words format ?", "label": "", "metadata": {}, "score": "51.670982"}
{"text": "Let 's begin by finding out what the most common suffixes are : .Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features .Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties .", "label": "", "metadata": {}, "score": "51.69702"}
{"text": "The wavelet transform may be a fast wavelet transform , a redundant wavelet transform , a non - orthogonal wavelet transform , a local cosine transform , or a local sine transform .The output of the wavelet transform may then be utilized to generate a visual representation of the semantic structure of the document .", "label": "", "metadata": {}, "score": "51.703762"}
{"text": "7 is a functional block diagram depicting an Internet search engine system according to an exemplary embodiment of the invention .DETAILED DESCRIPTION OF EXEMPLARY EMBODIMENTS .The inventive information retrieval method can comprise steps for selecting certain singular values to retain in a reduced rank approximation of a term - by - document indexing matrix .", "label": "", "metadata": {}, "score": "51.716927"}
{"text": "One solution is to make use of a lexicon , which describes how different words relate to one another .Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data .", "label": "", "metadata": {}, "score": "51.774536"}
{"text": "It 's important to understand what we can learn about language from an automatically constructed model .One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models .Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns .", "label": "", "metadata": {}, "score": "51.78044"}
{"text": "It is a further object of the present invention to utilize the output of the wavelet transform to partition the document .The partition maybe according to the semantic content of the document at a single level , or at multiple levels to produce an outline of the document .", "label": "", "metadata": {}, "score": "51.812157"}
{"text": "Finally , segmental frequencies were tallied for each file , and the database of tallies was used in the experiments .This process was done separately for the Buckeye / adult corpus and the CHILDES / child corpus .The description focuses mainly on the child input , which came from a more heterogeneous corpus ; the process for the adult corpus was analogous , though more straightforward .", "label": "", "metadata": {}, "score": "51.81682"}
{"text": "However , conditional models can not be used to answer the remaining questions 3 - 6 .However , this additional power comes at a price .Because the model is more powerful , it has more \" free parameters \" which need to be learned .", "label": "", "metadata": {}, "score": "51.85073"}
{"text": "3.2 Accuracy .The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled .The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 .", "label": "", "metadata": {}, "score": "51.94621"}
{"text": "The null hypothesis - in that article , a bag - of - words model - would predict an exponential distribution in co - occurrence intervals ( that is , the distribution over intervals , measured in number of words , between successive occurrences of the same word ) .", "label": "", "metadata": {}, "score": "51.982834"}
{"text": "All these methods have fairly standard implementations that you can get access to and run --- if you let us know which language you 're using , I or someone else will be able to point you in the right direction .", "label": "", "metadata": {}, "score": "52.03151"}
{"text": "Scaling Up to Large Datasets .Python provides an excellent environment for performing basic text processing and feature extraction .If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages .", "label": "", "metadata": {}, "score": "52.041195"}
{"text": "Select only the instances where inst.attachment is N : .Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns .For example , given the pair of nouns \" team \" and \" researchers , \" the classifier should predict the preposition \" of \" .", "label": "", "metadata": {}, "score": "52.09794"}
{"text": "And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large .A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label .", "label": "", "metadata": {}, "score": "52.10573"}
{"text": "For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances .But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times .", "label": "", "metadata": {}, "score": "52.13527"}
{"text": "In a stationary , ergodic process with a finite number of outcomes per trial , each outcome \u03d5 will have some constant probability \u03bc \u03d5 of occurrence .One way to evaluate whether a distribution is stationary and ergodic is to determine whether the actual variance around the mean is as predicted .", "label": "", "metadata": {}, "score": "52.178513"}
{"text": "Decision Tree algorithm helps solving the task of classifying cases . into multiple categories .In many cases , this is the fastest , as well . as easily interpreted machine learning algorithm of PolyAnalyst .The .DT algorithm provides intuitive rules for solving a great variety of .", "label": "", "metadata": {}, "score": "52.248825"}
{"text": "Applied Ergonomics 21 ( Mar. 1990 ) , 55 - 62 .[15 ] Levine , S. H. , Goodenough - Trepagnier , C. , Getschow , O.C. , and Minneman , L.S. 1987 .Multi - character key text entry using computer disambiguation .", "label": "", "metadata": {}, "score": "52.25345"}
{"text": "Implications for segmental acquisition .An immediate , theoretically appealing consequence of these findings is that frequency variation of the type studied here is unlikely to matter for language development .For several logically possible sorts of variation ( listener - specific variation , age - graded variation ) , the mean numerical differences were simply swamped out by the ' background ' variation in segmental frequencies experienced by every listener - infant , child , and adult .", "label": "", "metadata": {}, "score": "52.25697"}
{"text": "These uses would not depart from the spirit or scope of the present invention .Normalization steps that are common in matrix processing techniques are typically performed before and/or after matrix rank reductions .One of ordinary skill in the art will appreciate that any such normalization , scaling , filtering thresholding , or reformatting of the results or intermediate values obtained or used by the inventive methods or systems do not depart from the spirit or scope of the present invention .", "label": "", "metadata": {}, "score": "52.318108"}
{"text": "Given the definitions above , context - aware predictive disambiguation is straightforward .It first finds the list of matching words , ks i , and the estimated validity of each matching word .Next , it sorts the list by validity values and returns the sorted list .", "label": "", "metadata": {}, "score": "52.367443"}
{"text": "In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns .For example , we might introduce the abstract concept of \" polar verb \" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar .", "label": "", "metadata": {}, "score": "52.38007"}
{"text": "We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : .Next , we 'll define a simple feature extractor that checks what words the post contains : .Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment .", "label": "", "metadata": {}, "score": "52.45915"}
{"text": "the majority of training documents are unlabelled , and .most of the events have a short duration in time .We . adapted several supervised text categorization methods , . specifically several new variants of the k - Nearest Neighbor .", "label": "", "metadata": {}, "score": "52.460217"}
{"text": "Certain articles of speech , conjunctions , certain adverbs ( collectively called stop words ) are thought to be devoid of theme content and are usually omitted from the document in VSM - based analysis .Various methods in IR have been also been used to compress vocabulary by looking at how words are associated with one another .", "label": "", "metadata": {}, "score": "52.526817"}
{"text": "In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high .In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest .", "label": "", "metadata": {}, "score": "52.55021"}
{"text": "Rocchio 's Algorithm 2 .Naive Bayes 3.K - Nearest Neighbor(KNN ) 4 .Decision Tree 5 .Support Vector Machine(SVM ) 6 .Voted classification 7 .Latent Semantic Analysis .Subject : Re : categorization Answered By : leader - ga on 25 Jul 2003 00:05 PDT Rated : .", "label": "", "metadata": {}, "score": "52.570747"}
{"text": "For example a dilation factor of the square root of two would provide information from averaging over window sizes intermediate between those computed in the preferred embodiment .This would produce an image of the CWT which is smoother than that obtained in the preferred embodiment .", "label": "", "metadata": {}, "score": "52.59474"}
{"text": "Selective latent semantic indexing method for information retrieval applications US 7630992 B2 .Abstract .A term - by - document ( or part - by - collection ) matrix can be used to index documents ( or collections ) for information retrieval applications .", "label": "", "metadata": {}, "score": "52.618355"}
{"text": "Again , since the total improvements resulting from the two combined language models is greater than those when either of them is used separately , it should be reasonable to use both models to achieve maximum possible benefits if resource limitations are not primary concerns .", "label": "", "metadata": {}, "score": "52.625065"}
{"text": "To illustrate this approach , consider the N topics sensors attached to each word in the current reduced vocabulary as a set , W. The sensors attached to the query are members of another subset Q -- a fixed set .Let w be a number 0 .", "label": "", "metadata": {}, "score": "52.62563"}
{"text": "LSA captures .synonymity by knowledge . of captured vocabularies .TOEFL vocabulary simulates human performance .between word choice .LSA errors were compared to student errors .The . role of dimension reduction was analyzed .LSA simulates word sorting .", "label": "", "metadata": {}, "score": "52.6342"}
{"text": "SUMMARY .Automated tracking of events from chronologically ordered .document streams is a new challenge for statistical . text classification .Existing learning techniques . must be adapted or improved in order to effectively . handle difficult situations where the number of positive .", "label": "", "metadata": {}, "score": "52.671345"}
{"text": "This resulted in ten lists of 9909 word stems on average .The stem lists were then used to build the semantic relatedness models .Table 1 shows the theoretical maximum predictive disambiguation performances ( in DA and KSPC metrics ) for our context - aware method and the standard ( unmodified ) method for select keypad sizes .", "label": "", "metadata": {}, "score": "52.6765"}
{"text": "The keypad designs tested were the optimal constrained keypad designs from previous research [ 10].Simulation Results .For each run of the ten - fold cross validation simulation , nine- tenths of the BNC Baby corpus was used for building the vocabulary and training various models .", "label": "", "metadata": {}, "score": "52.68061"}
{"text": "Existing work can be mainly classified into two categories : supervised method and unsupervised method .The former requires training examples , which makes the method limited to predefined domains .While the latter usually utilizes clustering algorithms to find ' centered ' sentences as the summary .", "label": "", "metadata": {}, "score": "52.7676"}
{"text": "Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set .But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data .", "label": "", "metadata": {}, "score": "52.85951"}
{"text": "In this article , ' document ' will be used to refer to the contents of one such file after preprocessing .The goal of preprocessing was to isolate the phonological input to a single listener . 'Sample ' will be used interchangeably with ' document ' .", "label": "", "metadata": {}, "score": "52.89647"}
{"text": "we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word .However , this regular expression tagger had to be hand - crafted .", "label": "", "metadata": {}, "score": "52.910862"}
{"text": "In contrast , the dataset in the present study contained a variety of topics , and such a diversity of them that the variation across documents washed out most other effects .Taken together , these results suggest that segmental frequency differences will appear between two ' varieties ' of a language if and only if the topical and lexical distributions of each variety are both highly constrained , and highly different .", "label": "", "metadata": {}, "score": "52.974316"}
{"text": "Figure 1 : Standard 12-key mobile phone keypad .[ 22 ] .Dictionary - based predictive disambiguation requires one initial keystroke per character entered .Stored linguistic knowledge then determines candidate words for a given key sequence .For instance , 8TUV-4GHI-3DEF might produce \" the \" from the possible letter combinations .", "label": "", "metadata": {}, "score": "53.027134"}
{"text": "hyperplane will lead to maximal generalization when predicting the . classification of previously unseen examples ( Vapnik , Statistical .Learning Theory , 1998 ) .The SVM algorithm can also be extended to cope .with noise in the training set and with multiple classes ( Cristianini . and Shawe - Taylor , An Introduction to Support Vector Machines , 2000 ) .", "label": "", "metadata": {}, "score": "53.02897"}
{"text": "Here p i corresponds to the probability of the i th term in the current topic .M . ij .p .i . )l .i .j .p .i . )l .p .j . )", "label": "", "metadata": {}, "score": "53.04531"}
{"text": "However , instead of simply retaining the largest singular values , the SLSI method can determine which singular values to retain using information about how the singular values relate to the topics .Identifying the singular values that correspond to particular topics can enable the selection of specific singular values to retain and to abandon so as to retain any possible coverage of all topics of interest spanned by the documents .", "label": "", "metadata": {}, "score": "53.11016"}
{"text": "The entries contain the conditional probabilities modified by the independent probabilities .In the preferred embodiment B was taken as 2.0 .The window frequency count is then incorporated as a penalty term .It is not necessary to include any information about how many times a word appears in a window , only whether it appears or not .", "label": "", "metadata": {}, "score": "53.116726"}
{"text": "Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : .", "label": "", "metadata": {}, "score": "53.12899"}
{"text": "After testing , participants were asked if they noticed any differences in interacting with each method , and whether they felt that one method was more efficient .Figure 4 .Testing program using a 3-key constrained keypad design in editing mode .", "label": "", "metadata": {}, "score": "53.158535"}
{"text": "No . 08/713313 , filed Sep. 13 , 1996 entitled \" System for information Discovery \" and available from ThemeMedia Inc. , Richland Wash. is utilized .Stop words are very common words such as articles of speech , prepositions , and some adverbs .", "label": "", "metadata": {}, "score": "53.17102"}
{"text": "One skilled in the art will recognize efficient techniques for accomplishing this task .One exemplary technique is to sum the n th powers of the eigenvalues of M. .In step 640 , a coefficient of index zero , written as c 0 ( l ) , is initialized to the value of one .", "label": "", "metadata": {}, "score": "53.19995"}
{"text": "Use the dev - test set to check your progress .Once you are satisfied with your classifier , check its final performance on the test set .How does the performance on the test set compare to the performance on the dev - test set ?", "label": "", "metadata": {}, "score": "53.209343"}
{"text": "The inventive method 100 can be used to produce and employ a reduced rank approximation of this matrix for information retrieval .In step 120 , the singular values of the matrix are computed .These singular values can be computed using singular value decomposition ( SVD ) , which yields the singular values together with their corresponding singular vectors .", "label": "", "metadata": {}, "score": "53.242714"}
{"text": "Sig .Level .Average Text Entry Speed ( WPM ) .Average Error Rate ( % errors per character ) .Results also showed a 21.2 % reduction in error rates , from 0.104 errors per word with the existing method to 0.082 errors per word with the context - based predictive disambiguation method .", "label": "", "metadata": {}, "score": "53.27544"}
{"text": "The frequency , semantic relatedness , and POS models were trained using the remaining nine data sets .Using this design , the effect of biased training and testing data sets was minimized .The results of the ten runs were evaluated statistically to check the significance of any differences .", "label": "", "metadata": {}, "score": "53.295727"}
{"text": "ACM Press , New York , NY , 754 - 755 .[20 ] Manning , C. D. and Schutze , H. 2003 .Foundations of Statistical Natural Language Processing .MIT Press .[21 ] Masui , T. 1999 .", "label": "", "metadata": {}, "score": "53.33593"}
{"text": "linearly ) with increasing number of data columns .At the same time , it . grows more than linearly with the growing number of data records - as .Yet , for data of about . 100,000 records , the DT algorithm is often the fastest exploration . algorithm of PolyAnalyst .", "label": "", "metadata": {}, "score": "53.351814"}
{"text": "We begin by selecting the overall best decision stump for the classification task .We then check the accuracy of each of the leaves on the training set .Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf .", "label": "", "metadata": {}, "score": "53.36591"}
{"text": "However , bias in estimating probabilities often may not . make a difference in practice -- it is the order of the probabilities , . not their exact values , that determine the classifications .Studies comparing classification algorithms have found the Na\u00efve .", "label": "", "metadata": {}, "score": "53.40657"}
{"text": "In other words , in the face of so much variation , positive effects are potentially unreliable ; as noted above , false positives may be especially problematic in child language research , since publication bias is potentially quite high in this field .", "label": "", "metadata": {}, "score": "53.45724"}
{"text": "As a global observation , kNN , LLSF and neural network .method had the best performance , except for a Naive Bayes approach , .the other algorithms performed relatively well .The paper describes four machine learning techniques that are common .", "label": "", "metadata": {}, "score": "53.492596"}
{"text": "LSA does n't use .first - hand knowledge of the world , but extracts from \" episodes \" of .word content .LSA does n't use word order or logic relationships .LSA .uses unitary expressions of meaning instead of relationships between . successive words .", "label": "", "metadata": {}, "score": "53.503937"}
{"text": "A mixture of such shells can then also be so instantia ... \" .We introduce a method to learn a mixture of submodular \" shells \" in a large - margin setting .A submodular shell is an abstract submodular function that can be instantiated with a ground set and a set of parameters to produce a submodular function .", "label": "", "metadata": {}, "score": "53.506386"}
{"text": "FIG .2 .The reduced rank matrix is referred to in the figures as A . k .In step 140 , the non - selected singular values are set to zero .Setting the non - selected singular values to zero effectively cancels the corresponding singular vectors related to those values from the term - by - document matrix A. With those values canceled , the reduced rank matrix A . k can be calculated .", "label": "", "metadata": {}, "score": "53.524895"}
{"text": "SdeWaC comes in two versions , in POS - tagged / lemmatized version or as a one sentence per line format , each supplemented with metadata ( e.g. parse error rate ) .More information on sdewac .Italian .itWaC : a 2 billion word corpus constructed from the Web limiting the crawl to the .", "label": "", "metadata": {}, "score": "53.554928"}
{"text": "We apply our method on five Wikipedia entity categories and compare our method with two baseline methods .Both quantitative evaluation based on human judgment and qualitative comparison demonstrate the effectiveness and advantages of our method . \" ...We introduce a method to learn a mixture of submodular \" shells \" in a large - margin setting .", "label": "", "metadata": {}, "score": "53.57277"}
{"text": "CONCLUSION .On the basis of the bedrock principle that the form - meaning relationship is arbitrary , it was argued that the null hypothesis should be that for segmental frequencies in particular , child input is not different from adult input .", "label": "", "metadata": {}, "score": "53.655457"}
{"text": "The method of claim 3 wherein the visual representation of the semantic structure of the document is selected from the group comprising a text based representation and a graphical representation and combinations thereof .The method of claim 1 further comprising the step of utilizing the output of the wavelet transform to partition the document .", "label": "", "metadata": {}, "score": "53.66909"}
{"text": "Computer - based information systems can store large amounts of data .Despite the potentially enormous size of such data collections , information retrieval queries over a dataset attempt to be as informative , rapid , and accurate as possible .Information retrieval systems often employ indexing techniques to improve precision , improve recall performance and rapidly access specific information within a dataset .", "label": "", "metadata": {}, "score": "53.699783"}
{"text": "give me the answer such a discussion . \" for example , few documents need to be classify , they are not multi - modality and simple , so we can apply which method , and why we choose this method ... like this ! !", "label": "", "metadata": {}, "score": "53.706615"}
{"text": "First , we construct a list of documents , labeled with the appropriate categories .For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative .words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews .", "label": "", "metadata": {}, "score": "53.717106"}
{"text": "To check how reliable the resulting classifier is , we compute its accuracy on the test set .And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging .", "label": "", "metadata": {}, "score": "53.775944"}
{"text": "In other words , Na\u00efve Bayes classifiers assume that the effect of an .variable value on a given class is independent of the values of other . variable .This assumption is called class conditional independence .It . is made to simplify the computation and in this sense considered to be .", "label": "", "metadata": {}, "score": "53.828796"}
{"text": "I will refer to these as the LEXICAL and PHONOLOGICAL explanations , respectively , since the first attributes the glide asymmetry to the frequency of specific lexical items like you , while the second attributes the glide asymmetry to a global preference for words containing glides .", "label": "", "metadata": {}, "score": "53.842663"}
{"text": "Burstiness effects are one important case that can not be modeled by the null hypothesis .Even though the null hypothesis does not predict burstiness or its effects , it has proven an excellent statistical model of language , useful in pure and applied research across a variety of linguistic domains .", "label": "", "metadata": {}, "score": "53.859657"}
{"text": "Moreover , most of existing work assumes that documents related to the query only talks about one topic .Unfortunately , statistics show that a large portion of summarization tasks talk about multiple topics .In this paper , we try to break limitations of the existing methods and study a new setup of the problem of multi - topic based query - oriented summarization .", "label": "", "metadata": {}, "score": "53.861588"}
{"text": "However , a given document set usually covers a few topic themes with each theme represented by a cluster of sentences .The topic themes are usually not equally important and the sentences in an important theme cluster are deemed more salient than the sentences in a trivial theme cluster .", "label": "", "metadata": {}, "score": "53.927765"}
{"text": "As shown in FIG .3 , the x - axis is the narrative word order , the y - axis is the multi - resolution level on a log scale , and the z - axis is the energy level .", "label": "", "metadata": {}, "score": "53.95302"}
{"text": "The null hypothesis ' is that the segmental frequency distribution of a language ( here , English ) is unigram and unitary .The distribution of a language variety is unigram if it can be modeled as a stationary , ergodic process .", "label": "", "metadata": {}, "score": "53.966743"}
{"text": "The reader may wonder why I did not undertake to calculate more exactly the percentage of speech that is child - directed in the corpus as a whole ( or restrict my attention only to child - directed utterances ) .For example , one could imagine selecting a representative sample of corpus files , and coding each utterance binarily as child - directed or not , and reporting the percentage .", "label": "", "metadata": {}, "score": "54.00108"}
{"text": "For each manner and register , the mean relative frequency was calculated by averaging across all k documents in the subsample .Then , the means were compared .For each manner , a 1 was entered into a vector if the child mean was greater than the adult mean , and 0 was entered otherwise .", "label": "", "metadata": {}, "score": "54.011627"}
{"text": "These operations result in the full set of estimated singular values for the topic .Once this completion is determined , decision step 370 transitions to step 230 ( .FIG .2 ) .Thus , method 220 can be generalized for use with different forms of the term - by - document matrix .", "label": "", "metadata": {}, "score": "54.021145"}
{"text": "For example , it is arguably a universal aspect of communicative competence to exaggerate the pitch range when in a noisy environment or speaking to children , but it seems a priori unlikely that caregivers would know how to implement a strategy like ' avoid nasals ' .", "label": "", "metadata": {}, "score": "54.039978"}
{"text": "The information gain is then equal to the original entropy minus this new , reduced entropy .The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain .", "label": "", "metadata": {}, "score": "54.048126"}
{"text": "Learning to Classify Text .Detecting patterns is a central part of Natural Language Processing .Words ending in -ed tend to be past tense verbs ( 5 . )Frequent use of will is indicative of news text ( 3 ) .", "label": "", "metadata": {}, "score": "54.06744"}
{"text": "order of words to capture relationships in word choice .LSA uses a . pre - processing step for word correlation over many passages and .contexts .LSA uses a very large number of relationships .Theories of .human cognition can not be settled by theoretical and philosophical .", "label": "", "metadata": {}, "score": "54.086933"}
{"text": "Semantic structure is the order in which the topics are discussed in the document narrative .As will be further apparent to those skilled in the art , different methods of producing the signal will provide varying levels of noise in the resultant signal .", "label": "", "metadata": {}, "score": "54.12773"}
{"text": "A method for index matrix rank reduction can involve computing a singular value decomposition and then retaining singular values based on the singular values corresponding to singular values of multiple topics .The expected singular values corresponding to a topic can be determined using the roots of a specially formed characteristic polynomial .", "label": "", "metadata": {}, "score": "54.13411"}
{"text": "A somewhat better approach is to ensure that the training set and test set are taken from different documents : .If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : .", "label": "", "metadata": {}, "score": "54.155434"}
{"text": "We therefore adjust our feature extractor to include features for two - letter suffixes : .Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : .", "label": "", "metadata": {}, "score": "54.170547"}
{"text": "BACKGROUND .Research in predictive disambiguation text entry started with the work of Shannon in 1950s [ 27].In 1976 , Rabiner [ 24 ] analyzed the ambiguity problem of the telephone keypad to enter text .Later research looked at improving keypad designs and disambiguation algorithms .", "label": "", "metadata": {}, "score": "54.173317"}
{"text": "This process is illustrated in 5.2 and 5.3 .Figure 5.2 : Calculating label likelihoods with naive Bayes .Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data .", "label": "", "metadata": {}, "score": "54.177147"}
{"text": "As was mentioned before , there are several methods for identifying the most informative feature for a decision stump .One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature .", "label": "", "metadata": {}, "score": "54.178185"}
{"text": "( iii )In general , it is unsafe to use parametric statistics to compare the frequency of an item or items across language varieties .If it is truly necessary to compare the frequency of an item across two varieties , the researcher is advised to give the utmost care to selecting samples so that they are otherwise matched .", "label": "", "metadata": {}, "score": "54.183487"}
{"text": "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier .Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label .", "label": "", "metadata": {}, "score": "54.18673"}
{"text": "In this research , significant improvements were made to existing dictionary - based predictive disambiguation text entry methods .Novel semantic relatedness and part - of - speech models were developed and utilized by improved predictive disambiguation methods to achieve better disambiguation accuracy and fewer required keystrokes .", "label": "", "metadata": {}, "score": "54.20251"}
{"text": "Some questions I 'm still not understand .Why is non English ? ?In Rocchio 's Algorithm , you have n't answer me what is \" Constant and are empirical \" ? ?For instance news messages .Request for Answer Clarification by mcsemorgan - ga on 27 Sep 2003 17:48 PDT . ragingacademic , I would like to thank leader - ga , his answer was very well .", "label": "", "metadata": {}, "score": "54.228065"}
{"text": "Recent work by Budanitsky [ 4 ] surveyed five WordNet - based measures of lexical semantic relatedness .In Manning and Schutze [ 20 ] , semantic relatedness is defined as entities that are likely to co - occur .Therefore , instead of using the real meanings of words , a simpler model might just use word co - occurrence data in a large corpus .", "label": "", "metadata": {}, "score": "54.260742"}
{"text": "Since there is no prior expectation as to which direction a difference should run , a 2-tailed test is appropriate , meaning the normal significance threshold should be divided by two .Moreover , because six manners are being tested , it is necessary to do a Bonferroni correction , further dividing the significance threshold by six .", "label": "", "metadata": {}, "score": "54.33391"}
{"text": "For example , decision trees can be very effective at capturing phylogeny trees .However , decision trees also have a few disadvantages .One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small .", "label": "", "metadata": {}, "score": "54.3587"}
{"text": "One of ordinary skill in the art also will appreciate that any of the foregoing techniques can provide indexing on terms or on documents .While the topic based estimations discussed above find clusters of documents , the same techniques can be used to find clusters of terms .", "label": "", "metadata": {}, "score": "54.428623"}
{"text": "Likewise , the vector is an approximation in this lower - dimensional space .We write this approximation as .Synonymy is the phenomenon where different words describe the same idea .Thus , a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query .", "label": "", "metadata": {}, "score": "54.435425"}
{"text": "Thus , the null hypothesis is that English may be modeled as a single language variety that is generated by a stationary , ergodic process .( A salient alternative hypothesis is that English segmental frequencies must be modeled as consisting of at least two distinct language varieties , i.e. one for child input and another for adult input . )", "label": "", "metadata": {}, "score": "54.442013"}
{"text": "Gong , J. , Tarasewich , P. , and MacKenzie , I. S. ( 2008 ) .Improved word list ordering for text entry on ambiguous keyboards .Proceedings of the Fifth Nordic Conference on Human - Computer Interaction - NordiCHI 2008 , pp .", "label": "", "metadata": {}, "score": "54.454575"}
{"text": "For dictionary - based methods , a word can not be input unless the correct sequence of keystrokes is made .However , because of the less formal style of mobile text entry , it can be difficult for users to ensure that every keystroke is correct .", "label": "", "metadata": {}, "score": "54.512"}
{"text": "In addition , a vector of expected y - values was generated by evaluating each density distribution \u03c1 k at the same x - value ( 1000 values ) .The median of this vector was used as the predicted y - value ; the dashed lines represent the 2\u00b75 and 97\u00b75 percentiles , i.e. the 95 percent confidence interval .", "label": "", "metadata": {}, "score": "54.523106"}
{"text": "you have more than a few variables and classes -- you would require an .enormous number of observations ( records ) to estimate these .probabilities .Naive Bayes classification gets around this problem by .not requiring that you have lots of observations for each possible . combination of the variables .", "label": "", "metadata": {}, "score": "54.535034"}
{"text": "The resulting collection of document windows is then treated as a document corpus -- thus the name pseudo - corpus .In a preferred embodiment of the present invention , the pseudo - corpus is fed into a text engine known in the art for further processing .", "label": "", "metadata": {}, "score": "54.596287"}
{"text": "The method of .claim 1 , further comprising the step of computing a reduced rank approximation based on the selected singular values and the selected singular vectors , the reduced rank approximation providing an index for use during information retrieval .", "label": "", "metadata": {}, "score": "54.59721"}
{"text": "An extended cosine formula is preferred in this circumstance .The extended cosine procedure is nearly identical to the composite wavelet energy except that the normalized dot product is used to operate on the vectors to be compared , thus emphasizing the pattern of usage and de - emphasizing some information about frequency of usage .", "label": "", "metadata": {}, "score": "54.601093"}
{"text": "Some iterative optimization techniques are much faster than others .When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods .", "label": "", "metadata": {}, "score": "54.618988"}
{"text": "[ 4 ] Budanitsky , A. and Hirst , G. 2006 .Evaluating WordNet - based measures of lexical semantic relatedness .Computational Linguistics 32 ( Mar. 2006 ) , 13 - 47 .[5 ] Connolly , D. and Lundy , H. D. 1999 .", "label": "", "metadata": {}, "score": "54.786522"}
{"text": "The report discusses a variety of ways of evaluating the effectiveness . of text categorization systems .This is a very detailed study of machine learning in Automated Text .Classification , different algorithms and issue pertaining to document .representation , classifier construction and classifier evaluation .", "label": "", "metadata": {}, "score": "54.878277"}
{"text": "show that SVM , kNN and LLSF significantly outperform .NNet and NB when the number of positive training . instances per category are small ( less than ten ) , and that all .the methods perform comparably when the categories are . sufficiently common ( over 300 instances ) .", "label": "", "metadata": {}, "score": "54.881477"}
{"text": "i.e. locating major thematic breaks .However , there may be information relevant for particular queries in the neglected channels .Thus , in certain implementations of the present invention , many more PCA channels might be kept to provide additional information as may be required by the particular user .", "label": "", "metadata": {}, "score": "54.939804"}
{"text": "The model is a modification of Li and Hirst 's semantic relatedness model [ 16 ] and takes the following form : . where w 1 and w 2 are any two words in the dictionary .Stem ( w 1 ) and Stem ( w 2 ) are the word stems of w 1 and w 2 .", "label": "", "metadata": {}, "score": "54.953667"}
{"text": "Finding good text entry methods for keypads with very few keys remains a significant challenge for HCI researchers , although progress continues .Some recent advances include a 5-key watch - top interface from Dunlop [ 7 ] and Wobbrock et al . 's EdgeWrite [ 31 ] , as well as other methods [ e.g. , 1 , 8].", "label": "", "metadata": {}, "score": "55.035683"}
{"text": "in the passage , like human minor knowledge acquisition .LSA is .intuitively sensible , with a three - fourths gain in total comprehension . vocabulary inferred from knowledge about words not in the passage or . paragraph .Human children have a rapid growth of vocabulary and .", "label": "", "metadata": {}, "score": "55.077652"}
{"text": "3.5 Cross - Validation .In order to evaluate our models , we must reserve a portion of the annotated data for the test set .As we already mentioned , if the test set is too small , then our evaluation may not be accurate .", "label": "", "metadata": {}, "score": "55.088844"}
{"text": "First , a set of x - values ( representing bins of relative likelihood for [ l ] ) was generated by taking 101 evenly spaced points in the range [ 0,0\u00b706 ] , i.e. the range over which the posterior probability of [ l ] has support .", "label": "", "metadata": {}, "score": "55.161407"}
{"text": "At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification .In 1 , we saw several examples of corpora where documents have been labeled with categories .", "label": "", "metadata": {}, "score": "55.16945"}
{"text": "The ratio of actual word frequency to expected word frequency is used to pinpoint words of \" greatest topicality \" and produces a topicality measure for each term .[ Bookstein , A. et.al . , 1995 ] \" SID \" actually uses the reciprocal of a ratio related to Bookstein 's to assign topicality to terms .", "label": "", "metadata": {}, "score": "55.22339"}
{"text": "But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \" fly , \" knowing that the previous word is \" a \" will allow us to determine that it is functioning as a noun , not a verb .", "label": "", "metadata": {}, "score": "55.224506"}
{"text": "Trace Center , University of Wisconsin - Madison , 177 - 178 .[16 ] Li , J. and Hirst , G. 2005 .Semantic knowledge in word completion .In Proceedings of the ACM SIGACCESS Conference on Computers and Accessibility ( Baltimore , MD , October 9 - 12 , 2005 ) .", "label": "", "metadata": {}, "score": "55.333694"}
{"text": "On the other hand , if the input values have a wide variety of labels , then there are many labels with a \" medium \" frequency , where neither P(l ) nor log 2P(l ) is small , so the entropy is high .", "label": "", "metadata": {}, "score": "55.398415"}
{"text": "import math def entropy ( labels ) : .Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump .", "label": "", "metadata": {}, "score": "55.40783"}
{"text": "We also evaluate LDA - SP 's effectiveness at filtering improper applications of inference rules , where we show substantial improvement over Pantel et al . 's system ( Pantel et al . , 2007 ) . by Hui Lin , Jeff Bilmes - IN THE 49TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS : HUMAN LANGUAGE TECHNOLOGIES ( ACL - HLT , 2011 . \" ...", "label": "", "metadata": {}, "score": "55.447712"}
{"text": "Experiment I exploits this reasoning by explicitly generating ' matched corpora ' with the same number of documents and amount of data in each document , according to the null hypothesis .The variance in the real corpus was then compared to the variance in the generated corpora .", "label": "", "metadata": {}, "score": "55.45897"}
{"text": "This paper reports a controlled study with statistical significant . tests on five text categorization methods and their robustness in . dealing with different situations .This paper focuses on a comparative evaluation of a wide - range of text .categorization methods , including previously published results on the .", "label": "", "metadata": {}, "score": "55.53546"}
{"text": "For example , this selection can be done by weighting the number of singular values selected from any given topic .This weighting can be made by the relevance of the topic , or the size of the topic , or by some other metric of topical importance .", "label": "", "metadata": {}, "score": "55.540382"}
{"text": "Thus , the contrast between a nearly - null result here and the multiple positive results in Lee and Davis ' work did not arise because of insufficient data .Rather , it must have arisen from some other kind of intrinsic difference .", "label": "", "metadata": {}, "score": "55.56733"}
{"text": "You can use it on the raw and/or processed data to give an estimate of how aggressively you should aim to prune features ( or unprune them as the case may be ) .A paper describing it can be found here : .", "label": "", "metadata": {}, "score": "55.600952"}
{"text": "event tracking system over different ... .SUMMARY .This paper explores the use of Support Vector Machines SVMs for .learning text classifiers from examples It analyzes the particular .properties of learning with text data and identifies why SVMs are . appropriate for this task Empirical results support the theoretical . findings SVMs achieve substantial improvements over the currently best .", "label": "", "metadata": {}, "score": "55.6043"}
{"text": "This is due to an under - studied bias effect .that shrinks weights for classes with few training examples .ANother .systematic problem with Naive Bayes is that features are assumed to be . independent .As a result , even when the words are dependent , each word . contributes evidence individually .", "label": "", "metadata": {}, "score": "55.623486"}
{"text": "Table 1 reports the ten most frequent COMBINATIONS of speakers present in a document .For example , if only the target child and his / her parent were listed in the document header , then the combination of speakers would be ' Target , Parent ' .", "label": "", "metadata": {}, "score": "55.624634"}
{"text": "These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True .The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these .", "label": "", "metadata": {}, "score": "55.624866"}
{"text": "Of course , we do n't usually build naive Bayes classifiers that contain two identical features .However , we do build classifiers that contain features which are dependent on one another .For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature .", "label": "", "metadata": {}, "score": "55.631668"}
{"text": "determining via the computer a set of singular vectors based on the selected singular values , wherein the singular vectors provide an index for use during information retrieval .The method of . claim 10 , further comprising the step of computing a reduced rank approximation based on the selected singular values and the selected singular vectors , the reduced rank approximation providing an index for use during information retrieval .", "label": "", "metadata": {}, "score": "55.63668"}
{"text": "report provides classification statistics on the decision tree .Problems : .The drawback of this algorithm is that large number of gini indices . have to be computed at each node of the decision tree .In order to . decide which attribute is to be split at each node , the gini indices . have to be computed for all the attributes and for each successive . pair of values for all patterns which have not been classified .", "label": "", "metadata": {}, "score": "55.65175"}
{"text": "Many visualization systems have been built to help the information analyst sift though massive quantities of expository language text found in an electronic format in computer databases and the like .These types of systems have been critically important to identify key documents for intensive analysis .", "label": "", "metadata": {}, "score": "55.652695"}
{"text": "This is exactly what the Maximum Entropy classifier does as well .In particular , for each joint - feature , the Maximum Entropy model calculates the \" empirical frequency \" of that feature - i.e. , the frequency with which it occurs in the training set .", "label": "", "metadata": {}, "score": "55.698036"}
{"text": "Infants ' sensitivity to the sound patterns of native language words .Journal of Memory and Language32 , 402 - 420 .Kirchhoff K. , & Schimmel S. , ( Year : 2005 ) .Statistical properties of infant - directed versus adult - directed speech : Insights from speech recognition .", "label": "", "metadata": {}, "score": "55.709602"}
{"text": "computing via the computer the reduced rank approximation based on the grouping of the singular values , wherein the reduced rank approximation provides an index for use during information retrieval .The method of . claim 19 , wherein the grouping step comprises estimating singular values corresponding to at least one of the probabilistic distributions on parts and selecting actual singular values of matrix A that correspond to the estimated singular values .", "label": "", "metadata": {}, "score": "55.72493"}
{"text": "2.1 Sentence Segmentation .Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence .", "label": "", "metadata": {}, "score": "55.764038"}
{"text": "The simplest approach is hard thresholding : replacing small wavelet coefficients by zeros .This gives the greatest compression and speed - up , but is not necessarily the most effective denoising method .More complex denoising approaches have been developed ( such as the SURE algorithm of Donaho ) and shown effective in many cases .", "label": "", "metadata": {}, "score": "55.775864"}
{"text": "General Terms Measurement , Performance , Design , Experimentation , Human Factors , Standardization , Languages , Theory , Verification .Keywords Mobile devices , text entry , keypad , disambiguation , prediction .INTRODUCTION .Handheld mobile devices vary widely in their text entry interfaces .", "label": "", "metadata": {}, "score": "55.792076"}
{"text": "Unfortunately , most real - world problems involve non - separable .data for which there does not exist a hyperplane that successfully . separates the positive from the negative examples .One solution to the .inseparability problem is to map the data into a higher - dimensional . space and define a separating hyperplane there .", "label": "", "metadata": {}, "score": "55.794067"}
{"text": "In step 220 , the singular values of the topic are estimated .This prediction is represented as a subroutine , exemplary steps of which are described hereinafter with reference to .FIG .3 .In step 230 , the estimated singular values of the topic are compared to the actual singular values of the term - by - document matrix A to determine the actual singular values that correspond to the topic .", "label": "", "metadata": {}, "score": "55.79457"}
{"text": "In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : .Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male .", "label": "", "metadata": {}, "score": "55.861984"}
{"text": "[ 31 ] Wobbrock , J. O. , Myers , B. A. , and Kembel , J. A. 2003 .EdgeWrite : A stylus - based text entry method designed for high accuracy and stability of motion .In Proceedings of the Annual ACM Symposium on User Interface Software and Technology ( Vancouver , Canada , November 2 - 5 , 2003 ) .", "label": "", "metadata": {}, "score": "55.92063"}
{"text": "During testing , participants entered a set of twenty text phrases using either a standard frequency - based predictive disambiguation method or our context - aware method .Participants were then asked to enter another set of twenty text phrases in a second session using the other method .", "label": "", "metadata": {}, "score": "55.981934"}
{"text": "The coefficients of the characteristic equation can be calculated explicitly .Alternatively , the coefficients can be calculated more efficiently using a recursion method .It is also possible to calculate the coefficients of the characteristic equation in a manner that probabilistically weights the contribution from documents of varying length .", "label": "", "metadata": {}, "score": "55.99321"}
{"text": "In predictive disambiguation mode ( when a sequence of key strokes is entered but not committed ) , the function keys were NEXT and SPACE .After words were committed , they changed to DEL and DONE , as illustrated in Figures 4 and 5 .", "label": "", "metadata": {}, "score": "56.079273"}
{"text": "A characteristic polynomial can be said to represent a matrix , or parameters thereof , in a polynomial form .Exemplary embodiments of step 310 will be described in more detail hereinafter with reference to .FIGS .4 - 6 .", "label": "", "metadata": {}, "score": "56.100445"}
{"text": "In particular , the identity of the previous word is included as a feature .It is clear that exploiting contextual features improves the performance of our part - of - speech tagger .For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \" large \" or the word \" gubernatorial \" .", "label": "", "metadata": {}, "score": "56.118652"}
{"text": "Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low .In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2", "label": "", "metadata": {}, "score": "56.13867"}
{"text": "Accordingly , it is an object of the present invention to provide a method for automatically determining the semantic structure of an electronically formatted natural language based document .It is then a further object of the present invention to utilize spectral analysis of the digital signal as a method of characterizing the document .", "label": "", "metadata": {}, "score": "56.145325"}
{"text": "\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions .Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation .", "label": "", "metadata": {}, "score": "56.1574"}
{"text": "The results of Experiment II were generally in accord with the null hypothesis that child-and adult - directed speech do not differ on this dimension .More precisely , the frequency of glides is slightly higher in the aggregate input to children , owing to high - frequency glide - containing items like you / your and what's / what .", "label": "", "metadata": {}, "score": "56.178017"}
{"text": "This would constitute strong evidence against the hypothesis that glides are more frequent in child input than in adult input ( since in the subsamples , they were actually more frequent in adult input in 99\u00b79 percent of all trials ) .", "label": "", "metadata": {}, "score": "56.18549"}
{"text": "This article assesses burstiness using the relative frequency counts method , rather than the co - occurrence interval method .The primary rationale is that it is simpler to collect relative frequency distributions than co - occurrence interval distributions ; arguably it is also simpler to avoid explicating certain mathematical aspects of the Altmann et al .", "label": "", "metadata": {}, "score": "56.186207"}
{"text": "The present invention utilizes spectral analysis of a waveform or digital signal created from written words contained in an electronically formatted natural language document as a method for providing document characterization .As practiced by the present invention , the digital signal retains the order of the words within the document .", "label": "", "metadata": {}, "score": "56.190216"}
{"text": "classes can be differentiated from one another by searching for . similarities between the data provided .The K - Nearest Neighbor is suitable for data streams .KNN does not . build a classifier in advance .When a new sample arrives , KNN finds .", "label": "", "metadata": {}, "score": "56.21348"}
{"text": "Presumably , this kind of non - stationarity arises from multiple factors , including authors ' preferences for particular words , as well as the fact that documents are about one or more topics , and words are more likely to recur if they are associated with the same topic .", "label": "", "metadata": {}, "score": "56.236633"}
{"text": "The corpus data is divided into two sets : the development set , and the test set .The development set is often further subdivided into a training set and a dev - test set .Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set .", "label": "", "metadata": {}, "score": "56.274326"}
{"text": "Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns .Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models .", "label": "", "metadata": {}, "score": "56.306763"}
{"text": "The corpus was POS - tagged with the TreeTagger using this tagset , and lemmatized using the Morph - it ! lexicon , more information available here .PMID : 23046894 Owner : NLM Status : Publisher .Abstract / OtherAbstract : .", "label": "", "metadata": {}, "score": "56.31275"}
{"text": "It also mitigates the problem with polysemy , since components of polysemous words that point in the \" right \" direction are added to the components of words that share a similar meaning .Conversely , components that point in other directions tend to either simply cancel out , or , at worst , to be smaller than components in the directions corresponding to the intended sense .", "label": "", "metadata": {}, "score": "56.373245"}
{"text": "These visual \" Waves \" thus provide the user with the information present in a written outline .Further , the surface representation is more flexible than a standard outline or tree because instead of requiring each sub - section to be strictly contained in one and only one higher level section , subsections may be \" fuzzily \" contained in a section or more than one section .", "label": "", "metadata": {}, "score": "56.37564"}
{"text": "def rte_features ( rtepair ) : . return features .Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \" Recognizing Text Entailment \" Feature Extractor .The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference .", "label": "", "metadata": {}, "score": "56.4505"}
{"text": "For each triangle , a centroid vector is created .A theme similarity parameter may then be used to merge triangles .The merging stops when further merges would fall outside the parameter range specified .The resulting merged triangles may then be associated with themes .", "label": "", "metadata": {}, "score": "56.468964"}
{"text": "For instance , if an estimated singular value is selected , there could be several actual singular vectors that correspond to actual singular values that are close to the estimated singular value .In this case , one , some , or even all of the candidate actual singular vectors can be selected .", "label": "", "metadata": {}, "score": "56.5698"}
{"text": "The result is that chance differences will be inflated by the resampling procedure .In short , the present results are somewhat inconclusive ; glides appear to be genuinely more frequent in child input than in adult input , but at the sensitivity of the present method , no other manner differences emerged as significant .", "label": "", "metadata": {}, "score": "56.585876"}
{"text": "5.3 Non - Binary Features .We have assumed here that each feature is binary , i.e. that each input either has a feature or does not .Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \" color - is - red \" .", "label": "", "metadata": {}, "score": "56.61418"}
{"text": "[29 ] Stocky , T. , Faaborg , A. , and Lieberman , H. 2004 .A commonsense approach to predictive text entry .In Extended Abstracts of the ACM Conference on Human factors in Computing Systems ( Vienna , Austria , April 24 - 29 , 2004 ) .", "label": "", "metadata": {}, "score": "56.618885"}
{"text": "The final set of features includes around 20.000 features , which is actually a 90 % decrease , but not enough for intended accuracy of test - prediction .I am using LibSVM and SVM - light in turn for training and prediction ( both linear and RBF kernel ) and also Python and Bash in general .", "label": "", "metadata": {}, "score": "56.699802"}
{"text": "Using these data points , the problem is to find a surface ( \u03b1 , \u03b2 , \u03b3 ) that contains the greatest number of data points in the positive plane of this surface .Solving the Coefficient Optimization Problem .With a transformed but equivalent problem , finding the best coefficients for linearly combining estimated validities is much easier .", "label": "", "metadata": {}, "score": "56.72087"}
{"text": "Goldrick M. , & Larson M. , ( Year : 2010 ) .Constraints on the acquisition of variation In C.Fougeron , B.Kuhnert , M.D'Imperio & N.Vallee ( eds ) , Laboratory phonology 10 : Variation , phonetic detail and phonological representation , 285 - 310Berlin : Mouton de Gruyter .", "label": "", "metadata": {}, "score": "56.7454"}
{"text": "This partitioning method can allow retaining any combination of singular values from each desired partition , the partition being desired if loss of coverage of the topic corresponding to that partition is undesirable .A number of methods can be employed to select the singular vectors that correspond to a particular estimated singular value .", "label": "", "metadata": {}, "score": "56.76983"}
{"text": "6.3 Generative vs Conditional Classifiers .An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer .The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair .", "label": "", "metadata": {}, "score": "56.785694"}
{"text": "Lower error rates were also observed for the new context - aware method .Furthermore , there are minimal added attention costs resulting from the context - aware method .A method for finding the best linear combination of frequency , semantic relatedness , and POS models for the KSPC metric was also introduced and used in our implementation .", "label": "", "metadata": {}, "score": "56.79641"}
{"text": "KSPC is keystrokes per character , defined as the number of keystrokes required per character of text entered , averaged and normalized over the entire corpus .Note that since the simulation used a 10-fold cross validation design , each entry is the average of the results from ten separate runs using the same parameters ( number of keys and performance metric ) , but each has different training and testing sets .", "label": "", "metadata": {}, "score": "56.899696"}
{"text": "One reason is that in an earlier version of this article , no additional phonological transcriptions were supplied ; including these high- and medium - frequency types did not appreciably change the pattern of results .Relative frequency counts and data filtering .", "label": "", "metadata": {}, "score": "56.92465"}
{"text": "The method of . claim 10 , wherein the selecting step comprises matching the estimated singular values corresponding to at least one of a plurality of probabilistic distributions on parts with the actual singular values of matrix A. .The method of .", "label": "", "metadata": {}, "score": "56.9905"}
{"text": "The names classifier that we have built generates about 100 errors on the dev - test corpus : .Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders .", "label": "", "metadata": {}, "score": "57.06036"}
{"text": "No words are omitted as the document is partitioned into windows .The ith window overlaps the ( I-1)st window by a fixed number of words .The last window is usually incomplete and the first window may also be incomplete if the windowing starts at a location other than the first word of the document .", "label": "", "metadata": {}, "score": "57.081985"}
{"text": "To remove redundancy a subset of j 's which differ by multiples of 2 k would be computed .The main advantage of the redundancy , which is most commonly used in edge detection , is the accurate location of features with sharp edges .", "label": "", "metadata": {}, "score": "57.160698"}
{"text": "In other words , f 2 is an exact copy of f 1 , and contains no new information .When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose .", "label": "", "metadata": {}, "score": "57.167244"}
{"text": "Therefore a large language model is problematic and of limited practical use .However , our language model requires limited memory and provides real - time processing capabilities , resulting in good disambiguation performance .After running simulations that showed expected performance improvements with our method , a prototype system was implemented on a Dell Pocket PC for practical study .", "label": "", "metadata": {}, "score": "57.193962"}
{"text": "When an item is more frequent in one document , and less frequent in another , it follows mathematically that the average interval between occurrences must be shorter in the former , and longer in the latter .On analogy with the all - or - nothing firing patterns of neurons , it is said that the item is ' bursting ' in the former case , and ' lulling ' in the latter case .", "label": "", "metadata": {}, "score": "57.224266"}
{"text": "Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable .A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets .", "label": "", "metadata": {}, "score": "57.284294"}
{"text": "Can infants map meaning to newly segmented words ?Statistical segmentation and word learning .Psychological Science18 , 254 - 60.17444923 .Heller J. , & Pierrehumbert J. B ..( Year : 2011 ) .Word burstiness improves models of word reduction in spontaneous speech .", "label": "", "metadata": {}, "score": "57.291084"}
{"text": "Annual Conference of the Cognitive Science Society , 2208 - 213 .Austin , TX : Cognitive Science Society .Goldowsky B. N. , & Newport E. L. , ( Year : 1993 ) .Modeling the effects of processing limitations on the acquisition of morphology : The less is more hypothesis In J.Mead ( ed . ) , Proceedings of the 11th West Coast Conference on Formal Linguistics .", "label": "", "metadata": {}, "score": "57.324596"}
{"text": "The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier .But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier .", "label": "", "metadata": {}, "score": "57.392296"}
{"text": "Squares Fit mapping are the top - performing classifiers , while the .Rocchio approaches had rela ... .SUMMARY .We report text categorization accuracy for different types of features . and different types of feature weights .The comparison of these .", "label": "", "metadata": {}, "score": "57.41306"}
{"text": "Latent Semantic Analysis is a model of language learning , based on the . exposure to texts .It predicts to which extent semantic similarities .between words are learned from reading texts .variety of topics on this issue .Thanks , . leader - ga .", "label": "", "metadata": {}, "score": "57.44021"}
{"text": "The ' true ' frequency of most linguistic items of interest can not be reliably estimated .High - frequency items are over - represented in small and medium - sized corpora ; low - frequency items are under - represented in corpora of all sizes ( Baayen , 2001 ) .", "label": "", "metadata": {}, "score": "57.457626"}
{"text": "These vectors may be thought of as .points in an m - dimensional space .In theory , a simple way to build a . binary classifier is to construct a hyperplane ( i.e. , a plane in a . space with more than three dimensions ) separating class members .", "label": "", "metadata": {}, "score": "57.51361"}
{"text": "Unfortunately , the number of possible tag sequences is quite large .Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence .In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) .", "label": "", "metadata": {}, "score": "57.518265"}
{"text": "However , the effect is often lessened due to words having a predominant sense throughout a corpus ( i.e. not all meanings are equally likely ) .Sidebar .Table of Contents .Corpora .The resources below are large corpora build by downloading text from the web .", "label": "", "metadata": {}, "score": "57.518574"}
{"text": "This is the case for binary classification .I need at least 90 % at both cases and can not figure how to increase it : via optimizing training parameters or via optimizing feature selection ?I have read articles about feature selection in text classification and what I found is that three different methods are used , which have actually a clear correlation among each other .", "label": "", "metadata": {}, "score": "57.583717"}
{"text": "Researchers from Microsoft recently proposed an n - gram language model for text entry with soft keyboards [ 12].However , the model is based on characters , and therefore has limited applicability to word - level text entry systems using dictionary - based prediction .", "label": "", "metadata": {}, "score": "57.626984"}
{"text": "The inventive rank reduction method can ensure this retention of topical coverage by using information identifying the singular values that correspond to certain topics .For example , when selecting which singular values to retain , this information can be used to ensure that at least one singular value is retained corresponding to each desired topic .", "label": "", "metadata": {}, "score": "57.63777"}
{"text": "claim 1 , further comprising the step of identifying a probabilistic matrix model , the model comprising at least one of a plurality of probabilities representing the number of occurrences of the terms within the documents .The method of .claim 1 , wherein the selecting step further comprises matching the estimated singular values corresponding to one or more topics with the actual singular values of matrix A. .", "label": "", "metadata": {}, "score": "57.69496"}
{"text": "Methodological prescriptions .Aside from the specific theoretical point that child input is not that different from adult input ( in terms of segmental frequencies ) , this article aims to contribute to the field by raising awareness about the sampling issues that arise in language distributions .", "label": "", "metadata": {}, "score": "57.71229"}
{"text": "The general process by which child and adult input samples were obtained consists of several steps .First , raw corpus files were downloaded from the corpora repositories .Next , they were preprocessed to yield input files , consisting of input utterances .", "label": "", "metadata": {}, "score": "57.722366"}
{"text": "Significance was assessed as follows .For each manner \u03d5 , the value p \u03d5 was defined as total number of 1s for manner \u03d5 , divided by the total number of trials ( 10,000 ) .This value p \u03d5 represents the p -value for the one - tailed hypothesis that manner \u03d5 is more frequent in the child corpus than in the adult corpus .", "label": "", "metadata": {}, "score": "57.8356"}
{"text": "ACM Press , New York , NY , 147 - 155 .[26 ] Sandnes , F. E. , Thorkildssen , H. W. , Arvei , A. and Buverud , J. O. 2003 .Techniques for fast and easy text - entry with three - keys .", "label": "", "metadata": {}, "score": "57.886177"}
{"text": "Participants were compensated $ 10 upon completing the experiment .Apparatus .The experiment studied text entry performance using dictionary - based predictive disambiguation with a very small keypad design ( three character keys + two function keys ) .The three - key design was chosen because of the increased interest in small mobile devices with only a few character keys , such as those in Figure 3 .", "label": "", "metadata": {}, "score": "57.894085"}
{"text": "A special concern arising from the bursty distribution of segments is the increased vulnerability to Type I errors ( false positives ) .Of course , in any specific case there may indeed be genuine variation of the type the researcher is interested in .", "label": "", "metadata": {}, "score": "57.89466"}
{"text": "Forms that lacked an entry were omitted ; the five most frequent unlisted forms for each corpus are given in Table 3 .While disfluencies and untranscribable elements constitute the bulk of unlisted tokens , many genuine word - forms were also omitted .", "label": "", "metadata": {}, "score": "57.89984"}
{"text": "Note .Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative .Retrain the classifier with these new features , and test its accuracy .", "label": "", "metadata": {}, "score": "57.941772"}
{"text": "A social summary : amount of input by speaker role .CHILDES files contain speaker role information , which made it possible to collect statistics on which talkers said what , and how often .The talker roles listed in CHILDES exhibit a Zipfian distribution in which a few speaker roles occur many times ( e.g. Mother , Target_Child ) and many roles occur just a few times ( e.g. Environment , Toy , Camera_Operator ) .", "label": "", "metadata": {}, "score": "57.965416"}
{"text": "Document / sample .Informally , a sample of speech refers to a collection of utterances that were uttered together in temporal succession , for example a 15-minute conversation between two people .This study uses the CHILDES ( MacWhinney , 2000 ) and Buckeye ( Pitt , Johnson , Hume , Kiesling & Raymond , 2005 ) corpora to select samples representing child and adult input , respectively .", "label": "", "metadata": {}, "score": "57.971188"}
{"text": "UIST ' 01 .ACM Press , New York , NY , 111 - 120 .[ 18 ] MacKenzie , I. S. , and Soukoreff , R. W. 2002 .Text entry for mobile computing : Models and methods , theory and practice .", "label": "", "metadata": {}, "score": "57.990116"}
{"text": "In particular , the glide asymmetry seems to mainly be driven by the fact that you / your and what / what 's are more frequent in child input than adult input .Readers who would like to compare the present results with those of Lee and Davis ( 2010 ) might have noted that the present study studied child input , while that study focused specifically on infant - directed speech .", "label": "", "metadata": {}, "score": "58.123978"}
{"text": "Words are then compared by taking the cosine of the angle between the two vectors ( or the dot product between the normalizations of the two vectors ) formed by any two rows .Values close to 1 represent very similar words while values close to 0 represent very dissimilar words .", "label": "", "metadata": {}, "score": "58.141884"}
{"text": "Such rank reduction is not lossless .Making some of the singular values equal to zero reduces the rank of a matrix and invariably removes some information .When using the conventional LSI techniques , one example of a loss that can be introduced is a loss of topical coverage .", "label": "", "metadata": {}, "score": "58.16713"}
{"text": "Then in step 650 , the remaining coefficients up to c t ( l ) are computed using the recursion illustrated in Equation 5 below .c .n .l . ) j .n .n .j . ) j .", "label": "", "metadata": {}, "score": "58.172592"}
{"text": "Have you considered altering your bag - of - words representation to use , for example , word pairs or n - grams instead ?You may find that you have more dimensions to begin with but that they condense down a lot further and contain more useful information .", "label": "", "metadata": {}, "score": "58.18129"}
{"text": "In a preferred embodiment , the true value may also be approximated by taking the dominant PCA components .This approach dramatically enhances computational efficiency .In the preferred embodiment , only those PCA components with singular values greater than about 1/100 times the maximum singular value are retained .", "label": "", "metadata": {}, "score": "58.266"}
{"text": "classification trees and with neural network classifiers .They have . also exhibited high accuracy and speed when applied to large . databases .The problem with multinomial Naive Bayes is that when one class has .more training examples than another , Naive Bayes selects poor weights .", "label": "", "metadata": {}, "score": "58.32852"}
{"text": "Frequency approach of bag - of - words ( BOW ) .Information Gain ( IG ) .X^2 Statistic ( CHI ) .The first method is already the one I use , but I use it very simply and need guidance for a better use of it in order to obtain high enough accuracy .", "label": "", "metadata": {}, "score": "58.34095"}
{"text": "a .j .c .n .j .l . )The recursion Equation 5 can compute the value of the coefficient c n+1 ( l ) using the value of the n th and lower coefficients .Thus , c 1 ( l ) is computed using c 0 ( l ) , c 2 ( l ) is computed using c 0 ( l ) and c 1 ( l ) , and so on up to c t ( l ) .", "label": "", "metadata": {}, "score": "58.426582"}
{"text": "The corresponding performances of the normal predictive disambiguation method , when enhanced with either the semantic relatedness or POS models , are 68.5 % and 1.2027 , and 71.2 % and 1.1856 respectively for DA and KSPC metrics for 3-Key keypad design .", "label": "", "metadata": {}, "score": "58.459892"}
{"text": "One skilled in the art will recognize efficient techniques for accomplishing this task .One such technique is to sum the n th powers of the eigenvalues of M. The trace of a square matrix is defined to be the sum of the elements on the main diagonal of the matrix .", "label": "", "metadata": {}, "score": "58.55282"}
{"text": "return features .Example 1.2 ( code_gender_features_overfitting .py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features .The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus .", "label": "", "metadata": {}, "score": "58.575775"}
{"text": "Absolute ( left ) and relative ( right ) amount of input by speaker role .The # and % columns indicate the role code .Documents columns give the number or percentage of documents in which the role appears .Utterances and words give the number and percentage of utterances and words contributed by each speaker role .", "label": "", "metadata": {}, "score": "58.602108"}
{"text": "ACM Press , New York , NY , 1163 - 1166 .[ 30 ] Viterbi , A. J. 1967 .Error bounds for convolutional codes and asymptotically optimal decoding algorithm .IEEE Transactions on Information Theory 13 ( Apr. 1967 ) .", "label": "", "metadata": {}, "score": "58.64632"}
{"text": "Based on my experience the ultimate limitation of SVM accuracy depends on the positive and negative \" features \" .You can do a grid search ( or in the case of linear svm you can just search for the best cost value ) to find the optimal parameters for maximum accuracy , but in the end you are limited by the separability of your feature - sets .", "label": "", "metadata": {}, "score": "58.677494"}
{"text": "The user can also select a text location of interest for \" Topic Island \" generation or retrieval .Any location on the \" Wave \" visualization will have a specific multi - resolution level and energy level .By selecting a given point , and thereby specifying a multi - resolution level and energy level , the user then defines a cut off value of energy which may be used to partition the document .", "label": "", "metadata": {}, "score": "58.793625"}
{"text": "The present invention is thus a method for identifying the sub - topic structure of a document and visualizing those sub - topics .The invention is carried out as a series of instructions provided as a code for a programmable computer .", "label": "", "metadata": {}, "score": "58.79898"}
{"text": "However , even with eight character keys , the semantic relatedness and POS models still managed to improve DA and KSPC .USABILITY EXPERIMENT .To test the empirical performance of context - aware dictionary - based predictive disambiguation on small mobile devices with very few keys , usability testing was conducted using a PDA implementation of the method and a three - key keypad design .", "label": "", "metadata": {}, "score": "58.81301"}
{"text": "During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk .", "label": "", "metadata": {}, "score": "58.856796"}
{"text": "In an exemplary embodiment , determining the number of singular values to retain from each topical partition can comprise taking the largest singular values from each topic partition in a quantity such that each topic is represented by a substantially equal number of singular values .", "label": "", "metadata": {}, "score": "58.918266"}
{"text": "Computer Interaction 17 .Taylor & Francis Group , London , UK , 147 - 198 .[19 ] MacKenzie , I. S. and Soukoreff , R. W. 2003 .Phrase Sets for Evaluating Text Entry Techniques .In Extended Abstracts of the ACM Conference on Human Factors in Computing Systems ( Fort Lauderdale , FL , April 5 - 10 , 2003 ) .", "label": "", "metadata": {}, "score": "58.92002"}
{"text": "9 Further Reading .Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python .For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) .", "label": "", "metadata": {}, "score": "58.944645"}
{"text": "n .j .n .n .j . ) j .j .a .j .c .n .j .The recursion Equation 4 can compute the value of the coefficient c n+1 using the value of the n th and lower coefficients .", "label": "", "metadata": {}, "score": "58.961407"}
{"text": "l .l .p .i . lp .i . )i .j .l .l .p .i .p .j .i .j .In step 520 , the sequence a n is computed according to the formula illustrated in Equation 3 below , where n ranges from one to t ( inclusive of both one and t ) .", "label": "", "metadata": {}, "score": "58.96598"}
{"text": "Contributions of prosodic and distributional features of caregivers ' speech in early word learning In S.Ohlsson & R.Catrambone ( eds ) , Proceedings of the 32nd Annual Cognitive Science Conference , Portland , Oregon , 1822 - 27 .Austin , TX : Cognitive Science Society .", "label": "", "metadata": {}, "score": "58.96924"}
{"text": "Make use of this fact to build a consecutive classifier for labeling dialog acts .Be sure to consider what features might be useful .See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas .", "label": "", "metadata": {}, "score": "58.975597"}
{"text": "Thanks a lot , and if you need any additional info for help , just let me know .@larsmans : Frequency Threshold : I am looking for the occurrences of unique words in examples , such that if a word is occurring in different examples frequently enough , it is included in the feature set as a unique feature .", "label": "", "metadata": {}, "score": "59.087387"}
{"text": "This means that , similar to baby names , most words will be under - represented in some samples ( relative to their expected rate of mention under stationarity ) , and then in others be comparatively over - represented .Just as different names exhibit differing degrees of faddiness , the authors found that words vary considerably in the extent of burstiness .", "label": "", "metadata": {}, "score": "59.09884"}
{"text": "The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) .Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets .", "label": "", "metadata": {}, "score": "59.15617"}
{"text": "I am using python and bash scripts .I have had a quick search on singular value decomposition , principal component analysis and specifically LDA , but I need time in order to understand how to use them . - clancularius Nov 30 ' 12 at 12:14 .", "label": "", "metadata": {}, "score": "59.159145"}
{"text": "In other words , the stationary , ergodic assumption makes for a wonderful null hypothesis ; the relevant research question is when it matters that the null hypothesis is incorrect .A known failing of the null hypothesis : burstiness .It is well known that once a word has occurred in a document , the likelihood of it occurring again ( and again ) is far greater than expected under stationarity ( Baayen , 2001 ) , a property that may be referred to as BURSTINESS .", "label": "", "metadata": {}, "score": "59.177242"}
{"text": "In this manner , the present invention allows the user to quickly identify changes in the theme in the document narration , define meaningful subdocuments , enhance queries of the document , and provide visual summaries of the topic evolution within the document without necessarily reading the document .", "label": "", "metadata": {}, "score": "59.177505"}
{"text": "for classes with strong word dependencies is larger than for classes .with weak word dependencies .K - Nearest Neighbor .The goal of this clustering method is to simply separate the data . based on the assumed similarities between various classes .", "label": "", "metadata": {}, "score": "59.26319"}
{"text": "The results of this recursion are the t partial characteristic coefficients used by method 220 in .FIG .3 .From step 540 , the method 310 B proceeds to step 320 ( .FIG .3 ) .FIG .", "label": "", "metadata": {}, "score": "59.33487"}
{"text": "Each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space .For example , the occurrence of \" chair \" in a document containing \" The Chair of the Board \" and in a separate document containing \" the chair maker \" are considered the same .", "label": "", "metadata": {}, "score": "59.365486"}
{"text": "your reduction already removed necessary information .SVM is quire good in handling a lot of dimensions .did you try bigger feature sets ?what is the sample size you use in training ? if you can not train with more features , try to train the second most frequent 20.000 to verify there is no information left there .", "label": "", "metadata": {}, "score": "59.391617"}
{"text": "Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model .In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] .", "label": "", "metadata": {}, "score": "59.46772"}
{"text": "Summary Statistics - to deselect attributes that contain to many .values to provide any useful insight to the exploration engine .Underlying Algorithms : .Information Gain splitting criteria .Shannon information theory and statistical significance tests .The Data Used : .", "label": "", "metadata": {}, "score": "59.52903"}
{"text": "d k and c k are filters .Then the following identities , called the two - scale relations , hold : # # EQU3 # # and similarly for \u03c6 .That is , if the scaling function coefficients are known at index m-1 , then the wavelet and scaling function coefficients at index m can be determined .", "label": "", "metadata": {}, "score": "59.554634"}
{"text": "Two - tailed paired t -tests were used to evaluate whether differences in means were statistically significant .The associated p values are given in the right - hand column of Table 3 .On average , the participants achieved text entry speeds of 7.31 wpm using standard disambiguation .", "label": "", "metadata": {}, "score": "59.554947"}
{"text": "[ 12 ] Goodman , J. , Venolia , G. , Steury , K. , and Parker , C. 2002 .Language modeling for soft keyboards .In Proceedings of the Eighteenth National Conference on Artificial Intelligence ( Edmonton , Canada , July 28 - August 1 , 2002 ) . AAAI-02 .", "label": "", "metadata": {}, "score": "59.619694"}
{"text": "This partitioning can be performed using a technique for estimating the singular values that correspond to each topic .The estimated singular values approximate the singular values obtained from the SVD to identify a mapping between the SVD singular values and the topics .", "label": "", "metadata": {}, "score": "59.670815"}
{"text": "In step 350 , the selected root is multiplied by the number of documents within the topic .If the information on the documents is from a statistical model rather than actual documents , this number of documents can be the expected number of documents within the topic .", "label": "", "metadata": {}, "score": "59.75872"}
{"text": "l .i .j .Equation 10 below provides a format for elements of the M matrix when the term - by - document matrix elements are the logarithm of the frequency of the term in a document .In Equation 10 , j i corresponds to the number of times the i th term is chosen .", "label": "", "metadata": {}, "score": "59.771027"}
{"text": "5.4 The Naivete of Independence .The reason that naive Bayes classifiers are called \" naive \" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) .In particular , almost all real - world problems contain features with varying degrees of dependence on one another .", "label": "", "metadata": {}, "score": "59.777763"}
{"text": "In other words , the result of the SVD is the decomposition of a matrix into its component singular vectors such that the sum of the rank one matrices given by the outer products of the singular vectors multiplied by their corresponding singular values is the original matrix .", "label": "", "metadata": {}, "score": "59.810535"}
{"text": "Mathematically , a topic can be considered a probability distribution over all terms .For example , the term \" hexagon \" is perhaps more probabilistically likely to be related to a topic of a mathematical nature than it would be to a topic of a historical nature .", "label": "", "metadata": {}, "score": "59.85195"}
{"text": "At the task of producing generic DUC - style summaries , HIERSUM yields state - of - the - art ROUGE performance and in pairwise user evaluation strongly outperforms Toutanova et al .( 2007 ) 's state - of - the - art discriminative system .", "label": "", "metadata": {}, "score": "59.864"}
{"text": "We will learn more about the naive Bayes classifier later in the chapter .For now , let 's just test it out on some names that did not appear in its training data : .Observe that these character names from The Matrix are correctly classified .", "label": "", "metadata": {}, "score": "59.872658"}
{"text": "The method of claim 1 wherein said wavelet transform is selected from the group comprising a fast wavelet transform , a redundant wavelet transform , a non - orthogonal wavelet transform , a local cosine transform , and a local sine transform .", "label": "", "metadata": {}, "score": "60.053978"}
{"text": "There is a positive correlation between the semantic similarity of two words ( as measured by LSA ) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns .", "label": "", "metadata": {}, "score": "60.055878"}
{"text": "To test our context - aware predictive disambiguation method , computer simulations were conducted .We compared the performance of our method against a standard ( unmodified ) predictive disambiguation method , similar to the methods implemented on many current cell phones .", "label": "", "metadata": {}, "score": "60.06454"}
{"text": "An algorithm for suffix stripping .[ 24 ] Rabiner , L. R. and Schafer , R. W. 1976 .Digital techniques for computer voice response : Implementations and applications .Proceedings of the IEEE 64 , 4 ( April 1976 ) .", "label": "", "metadata": {}, "score": "60.074677"}
{"text": "Based on the new information , it sounds as though you are on the right track and 84%+ accuracy ( F1 or BEP - precision and recall based for multi - class problems ) is generally considered very good for most datasets .", "label": "", "metadata": {}, "score": "60.078125"}
{"text": "A philosophically similar point was raised in Tomasello and Stahl ( 2004 ) .That study , which was primarily concerned with infrequent constructions , demonstrated empirically that rare phenomena are systematically under - represented , even in quite large longitudinal samples .", "label": "", "metadata": {}, "score": "60.12565"}
{"text": "Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data .Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input .", "label": "", "metadata": {}, "score": "60.13755"}
{"text": "Please let me know if these are the kind of papers that you will consider .Thanks for being patient with me .SUMMARY .This paper reports a controlled study with statistical significance . tests on five text categorization methods : the Support .", "label": "", "metadata": {}, "score": "60.252823"}
{"text": "I am currently working on a project , a simple sentiment analyzer such that there will be 2 and 3 classes in separate cases .I am using a corpus that is pretty rich in the means of unique words ( around 200.000 ) .", "label": "", "metadata": {}, "score": "60.267506"}
{"text": "Let this row vector be called .Likewise , the only part of that contributes to is the column , .These are not the eigenvectors , but depend on all the eigenvectors .It turns out that when you select the largest singular values , and their corresponding singular vectors from and , you get the rank approximation to with the smallest error ( Frobenius norm ) .", "label": "", "metadata": {}, "score": "60.321594"}
{"text": "Springer - Verlag , Berlin , 342 - 346 .[ 8 ] Evreinova , T. , Evreinov , G. , and Raisamo , R. 2004 .Four - key text entry for physically challenged people .In Adjunct Proceedings of the 8th ERCIM Workshop \" User Interfaces For All \" ( Vienna , Austria , June 28 - 29 , 2004 ) .", "label": "", "metadata": {}, "score": "60.33628"}
{"text": "3 is a logical flow diagram depicting a method for estimating the singular values of a topic according to an exemplary embodiment of the invention .FIG .4 is a logical flow diagram depicting a method for generating characteristic coefficients to estimate singular values corresponding to a topic according to an exemplary embodiment of the invention .", "label": "", "metadata": {}, "score": "60.38965"}
{"text": "We can systematically evaluate the classifier on a much larger quantity of unseen data : .Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : .This listing shows that the names in the training set that end in \" a \" are female 33 times more often than they are male , but names that end in \" k \" are male 32 times more often than they are female .", "label": "", "metadata": {}, "score": "60.404438"}
{"text": "If term densities are not taken into account this information is lost and the fewer categories you have the more impact this loss with have .On a similar note , it is not always prudent to only retain terms that have high frequencies , as they may not actually be providing any useful information .", "label": "", "metadata": {}, "score": "60.413765"}
{"text": "I need info especially to setup an interface ( python oriented , open - source ) between feature space dimension reduction methods ( LDA , LSI , moVMF etc . ) and clustering methods ( k - means , hierarchical etc . ) .", "label": "", "metadata": {}, "score": "60.537323"}
{"text": "The matrix product contains all these dot products .Element ( which is equal to element ) contains the dot product ( ) .Likewise , the matrix contains the dot products between all the document vectors , giving their correlation over the terms : .", "label": "", "metadata": {}, "score": "60.612457"}
{"text": "This is a straightforward consequence of the following facts : ( i ) there are far more child documents than adult documents , and ( ii ) the child documents are more heterogeneous in length than the adult ones .In other words , this difference plausibly derives from the amount of data available , rather than intrinsic differences between the two language varieties .", "label": "", "metadata": {}, "score": "60.674683"}
{"text": "Hearst creates a smoothed token gap sequence that corresponds to the narrative order of the text .Merged paragraphs may also form a narrative based signal .While all of these methods have advantages for IR , there still exists a need for an improved method of automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure .", "label": "", "metadata": {}, "score": "60.68129"}
{"text": "However , the invention is not limited to the order of the steps described if such order or sequence does not alter the functionality of the present invention .That is , it is recognized that some steps can be performed before , after , or in parallel with other steps without departing from the scope and spirit of the present invention .", "label": "", "metadata": {}, "score": "60.80341"}
{"text": "Such alternative embodiments are within the scope and spirit of the present invention .In step 270 , the k singular values of A that are to be retained in the reduced rank approximation are selected .The numbers of singular values to retain from each topical partition that were determined in step 260 guide the selection of the particular k singular values .", "label": "", "metadata": {}, "score": "60.935722"}
{"text": "The primary location may be found using a coarser multi - resolution level ( a higher value for k ) while the minor discussion would be located using a finer multi - resolution level(a lower value for k ) .Thus , \" fuzzily \" located refers to the phenomenon where discussions of a single topic are scattered throughout a document .", "label": "", "metadata": {}, "score": "60.969315"}
{"text": "learning tasks Furthermore they are fully automatic eliminating the .need for manual parameter tuning .Sincerely , . leader - ga .Request for Answer Clarification by mcsemorgan - ga on 17 Aug 2003 08:11 PDT .In the advantages and disadvantages part , you made research on Vector Space Model but what I needed is Support Vector Machine .", "label": "", "metadata": {}, "score": "61.003883"}
{"text": "But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set .An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded .", "label": "", "metadata": {}, "score": "61.049873"}
{"text": "Method and apparatus for electronically extracting application specific multidimensional information from documents selected from a set of documents electronically extracted from a library of electronically searchable documents .O. Troyanskaya , M. Cantor , G. Sherlock , P. Brown , T. Hastie , R. Tibshirani , D. Botstein , R. B. Altman , Missing value estimation methods for DNA microarrays , Bioinformatics 17 ( 6 ) ( 2001 ) 520 - 525 , evaluation Studies .", "label": "", "metadata": {}, "score": "61.13361"}
{"text": "The inputs are the keystroke encoding ks i of word w i and sequence H i of context words .H i contains all the input words preceding w i and in the same sentence .The output is a list of English words w 1 , w 2 , ... w m ordered so that the most probable list is given by ks i and H i .", "label": "", "metadata": {}, "score": "61.181236"}
{"text": "Link Analysis PageRank [ 22 ] and HITS [ 9 ] are two popular algorithms for link analysis between web pages and they have been success ... . byJie Tang , Limin Yao , Dewei Chen - SIAM International Conference Data Mining , 2009 . \" ...", "label": "", "metadata": {}, "score": "61.211716"}
{"text": "The result from step 360 is one of the estimated singular values for the given topic .Next , decision step 370 tests if any additional roots remain to be operated upon .If so , the method 220 branches back to step 340 where another root is selected to be operated on by steps 350 and 360 .", "label": "", "metadata": {}, "score": "61.231102"}
{"text": "To reduce the number of POS tags in the algorithm and minimize computation , we used 19 closely related POS tags from the British National Corpus ( BNC ) tag set .These were used for tagging the BNC itself .The POS validity of a word , based on words preceding it , is defined as follows : .", "label": "", "metadata": {}, "score": "61.23717"}
{"text": "One skilled in the art will appreciate that references to terms and documents are non - limiting examples .The inventive method for indexing terms and documents can just as well address indexing of genes within individuals ; atoms within molecules ; elements within sets ; or any general parts within collections .", "label": "", "metadata": {}, "score": "61.243042"}
{"text": "In other words , for the index i selected in step 410 , the determinant computed in step 450 is the i th characteristic coefficient , written as c i .Next , decision step 460 determines whether any additional indexes remain to be operated upon .", "label": "", "metadata": {}, "score": "61.320503"}
{"text": "The median amount of data per document in the present study is about the same as the amount of data per interaction in Lee and Davis ' study ; thus the dataset used here is about thirty times the size of the one in Lee and Davis .", "label": "", "metadata": {}, "score": "61.357483"}
{"text": "One used a standard QWERTY keyboard and another used predictive disambiguation text entry with a reduced keyboard .Results showed that although predictive disambiguation text entry required greater mental load , keyboard designs that put multiple characters on a single key still offered advantages , such as minimizing the size of the device as well as the physical movement required by motor - impaired users .", "label": "", "metadata": {}, "score": "61.429512"}
{"text": "Bayes Theorem to estimate the posterior probabilities of all . qualifications .For each . instance of the example language a classification with the highest .posterier probability is chosen . as the prediction .Example : .Suppose your data consist of fruits , described by their color and . shape .", "label": "", "metadata": {}, "score": "61.45292"}
{"text": "The DT algorithm is .well - poised for analyzing very large databases because it does not . require loading all the data in machine main memory simultaneously .PolyAnalyst takes a full advantage of this feature by implementing . incremental DT learning with the help of the OLE DB for Data Mining . mechanism .", "label": "", "metadata": {}, "score": "61.49185"}
{"text": "FIG .3 , according to another alternative exemplary embodiment .The method 310 C can be considered a generalization of method 310 B. While the method of 310 B computes the partial characteristic coefficients when all of the documents are the same length , method 310 C can compute the coefficients when the document lengths comprise a finite probability distribution .", "label": "", "metadata": {}, "score": "61.530975"}
{"text": "Therefore , where w i is the user desired word , the greatest probability among \u03b4 i ( t 1 ) to \u03b4 i ( t n ) is chosen to represent the POS validity for w i .This follows the definition of word POS validity given above .", "label": "", "metadata": {}, "score": "61.5415"}
{"text": "For example , if you have a . series of mRNA expression level measurements for each of a large . number of genes , the SVM can learn to answer questions such as , ' ' Does .the given gene belong to functional class X ? ' ' where X is some . category such as ' ' ribosomal genes ' ' or ' ' sugar and carbohydrate . transporters . ' '", "label": "", "metadata": {}, "score": "61.54517"}
{"text": "Thus , the stationary , ergodic aspect of the null hypothesis was disconfirmed ( Experiment I ) ; but the unitary property was shown to be approximately correct ( Experiment II ) .The interesting exception was glides .Although the effect was rather subtle , the results of Experiment II suggested that glides are more frequent in child - directed speech than in adult - directed speech .", "label": "", "metadata": {}, "score": "61.624954"}
{"text": "1 Supervised Classification .Classification is the task of choosing the correct class label for a given input .In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance .", "label": "", "metadata": {}, "score": "61.641464"}
{"text": "1 .These separate multi - resolution levels are then combined as illustrated in FIG .2 . using a color shade or gray - scale to indicate the energy level .As illustrated in FIG .3 , this visualization may then be extended to a 3-D colored or grey scale surface plot .", "label": "", "metadata": {}, "score": "61.947136"}
{"text": "The x -axis shows k , the number of documents per subsample .The y -axis represents log odds of the p \u03d5 -value ( a log odds transform was done in order to represent the full dynamic range of p \u03d5 .", "label": "", "metadata": {}, "score": "61.98912"}
{"text": "Patented systems , such as those developed by Ameritech and Tegic [ 5 , 9 ] , all employ NLP methods for disambiguating user input .A common drawback is that the models deployed are usually built at the character - level , since a word - level language model simply would not fit in the available memory of a mobile device .", "label": "", "metadata": {}, "score": "62.021976"}
{"text": "Two of these values are calculated using an MDS projection on the centoids for the collection of thematic chunks .These values are used to determine the placement of the thematic chunk in the x - y plane .The multi - resolution level is then used to determine the placement of the thematic chunk in the z plane .", "label": "", "metadata": {}, "score": "62.04072"}
{"text": "A controlled . study using three classifiers , kNN , LLSF and WORD , was conducted to . examine the impact of configuration variations in five versions of .Reuters on the observed performance of classifiers .Analysis and empirical evidence suggest that the evaluation results on .", "label": "", "metadata": {}, "score": "62.055553"}
{"text": "The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \" in parallel . \"5 Naive Bayes Classifiers .In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value .", "label": "", "metadata": {}, "score": "62.079155"}
{"text": "( That is , utterances by the target listener are included in the social summary of the corpus , but they are excluded from the target listener 's input . )The speakers were classified by their relationship to the target child , hereafter referred to as ' role ' .", "label": "", "metadata": {}, "score": "62.170662"}
{"text": "The aggregate numerical frequency differences in glides between child and adult input appears to have been caused by individual lexical items that are more frequent in child input and happen to contain glides in English ( you / your , what / what 's , want ) .", "label": "", "metadata": {}, "score": "62.287468"}
{"text": "However , the number of tokens is no larger than a typical word level bi - gram model .The sizes of the semantic and syntactic language models were approximately 28 MB and 700 KB , respectively , which are practical for current mobile devices .", "label": "", "metadata": {}, "score": "62.37652"}
{"text": "It contains data for four words : hard , interest , line , and serve .Choose one of these four words , and load the corresponding data : .Using this dataset , build a classifier that predicts the correct sense tag for a given instance .", "label": "", "metadata": {}, "score": "62.443386"}
{"text": "Several methods have been used to visualize theme breaks found in electronically formatted text .Salton 's \" tour \" is a graph with links and nodes .[ Salton , 1994 ] Heart has developed a system called \" TileBars \" which allows the user to define specific topics of interest and then produces a linear color block map to show where chunks of the document are likely to contain these topics .", "label": "", "metadata": {}, "score": "62.479866"}
{"text": "[ 1 ] Baljko , M. and Tam , A. 2006 .Indirect text entry using one or two keys .In Proceedings of the ACM Conference on Computers & Accessibility ( Portland , OR , October 23 - 25 , 2006 ) .", "label": "", "metadata": {}, "score": "62.48528"}
{"text": "FIG .3 is a 3-D rendering of the smoothed plot of FIG .2 where the z - axis is formed using energy intensity .FIG .4 depicts a graph of the energy levels from three different multi - resolution levels .", "label": "", "metadata": {}, "score": "62.57047"}
{"text": "Similarities between paragraphs are calculated using a cosine measurement ( normalized dot product ) and are used to create a text relationship map .In the text relationship map , nodes are the paragraphs and links are the paragraph similarities .All groups of three mutually related ( based on the similarity measure ) paragraphs are identified and merged .", "label": "", "metadata": {}, "score": "62.634605"}
{"text": "Humans draw conclusions from missing data .Reducing the . dimensionality of representation is useful when the representation .matches the data .The data should not be perfectly regenerated .The . similarity of dimensionality reduction is the cosine between vectors .", "label": "", "metadata": {}, "score": "62.798683"}
{"text": "However , there could be two different ways in which adults use more words that contain glides .One way is that there is a small number of glide - containing word types that happen to be far more frequent in child input than in adult input .", "label": "", "metadata": {}, "score": "62.8308"}
{"text": "The method of .claim 16 , wherein the step of generating characteristic coefficients comprises computing a recursion such that each coefficient is based on those coefficients already computed .The method of . claim 17 , wherein the step of generating characteristic coefficients further comprises computing a probabilistically weighted average of coefficients based on the probability of collection size , the weighted averaging allowing the method to function with collections of non - uniform sizes .", "label": "", "metadata": {}, "score": "62.83902"}
{"text": "The first reason is that the six manners are not independent , so if glides are disproportionately more frequent in adult input , it can only be the case that some combination of other manners must be less frequent .The second reason is that the significance test was not sampling from the entire population , but only resampling from the available sample .", "label": "", "metadata": {}, "score": "62.8664"}
{"text": "FIG .1 is a series of plots of one minus the normalized composite wavelet energy at 3 fixed multi - resolution levels of an exemplary narrative .FIG .2 is a grey scale plot combining nine plots as created in FIG .", "label": "", "metadata": {}, "score": "62.869762"}
{"text": "( b )If so , what causes the difference ?The present study will offer a different perspective on these questions than the one offered by Lee and Davis ; a theme of this article will be that aspects of the sampling and analysis process may dramatically affect the nature of the results a researcher obtains .", "label": "", "metadata": {}, "score": "62.887993"}
{"text": "Regarding your use of document frequency , are you merely using the probability / percentage of documents that contain a term or are you using the term densities found within the documents ?If category one has only 10 douments and they each contain a term once , then category one is indeed associated with the document .", "label": "", "metadata": {}, "score": "62.982086"}
{"text": "claim 6 , wherein the step of generating characteristic coefficients comprises computing a recursion such that each coefficient is based on those coefficients already computed .The method of . claim 8 , wherein the step of generating characteristic coefficients further comprises computing a probabilistically weighted average of coefficients based on the probability of document length , the weighted averaging allowing the method to function with documents of non - uniform lengths .", "label": "", "metadata": {}, "score": "63.052734"}
{"text": "For example , 20 words may be assigned as a token - sequence , which may then be described as a pseudo - sentence , and 6 token sequences may then be assigned as a block , which may then be described as a pseudo paragraph .", "label": "", "metadata": {}, "score": "63.111946"}
{"text": "The CHILDES project was one of the first crowd - sourcing projects applied to linguistic data .Brian MacWhinney solicited other child language researchers to share the transcriptions they had collected in the course of their research .In the course of the project , the CHAT coding conventions were established , and to the extent that it was feasible , corpora were adjusted to conform to those conventions .", "label": "", "metadata": {}, "score": "63.188774"}
{"text": "Trained on a corpus of noisy Twitter conversations , our method discovers dialogue acts by clustering raw utterances .Because it accounts for the sequential behaviour of these acts , the learned model can provide insight into the shape of communication in a new medium .", "label": "", "metadata": {}, "score": "63.220238"}
{"text": "Similarly , Equation 11 below provides a general , non - simplified , format for elements of the M matrix .In Equation 11 , j i corresponds to the number of times the i th term is chosen .M . ij .", "label": "", "metadata": {}, "score": "63.22232"}
{"text": "3 , according to an alternative exemplary embodiment .Recursion refers to generating partial characteristic coefficients such that each coefficient is based on those coefficients already computed .While the resultant coefficients of method 310 B may be equivalent to those generated by the exemplary method 310 A described with reference to .", "label": "", "metadata": {}, "score": "63.308224"}
{"text": "It is argued here that the null hypothesis should be that adult and child input do not differ in segment class frequencies .Rather than consider all imaginable classes of segments , the present article focuses on consonantal manner classes to achieve greater empirical coherence ; the general findings about variability in segmental frequency distributions presumably generalize straightforwardly to other classes , such as vowel height and consonant place .", "label": "", "metadata": {}, "score": "63.340485"}
{"text": "Two adjacent blocks form a window .By shifting each window over by one token sequence , a comparison may be made for the next pair of adjacent windows .The cosine calculation for each window is centered over the gap between the blocks .", "label": "", "metadata": {}, "score": "63.347637"}
{"text": "2 .In step 310 , a quantity t of partial characteristic coefficients c 1 through c t are generated , where t is the number of terms in the current topic .These t values are used in the calculation of partial coefficients of the terms in a specially formed characteristic polynomial detailed below in Equation 1 .", "label": "", "metadata": {}, "score": "63.36302"}
{"text": "Jurafsky D. , & Martin J. H. , ( Year : 2009 ) .Speech and language processing : An introduction to natural language processing , speech recognition , and computational linguistics , 2nd edn .Upper Saddle River , NJ : Prentice - Hall .", "label": "", "metadata": {}, "score": "63.372776"}
{"text": "Results show that the meaning similarities are . close to that of humans , LSA 's rate of knowledge acquisition . approximates that of humans , and LSA depends on the dimensionality .LSA can be use to test theories of human cognition .", "label": "", "metadata": {}, "score": "63.418365"}
{"text": "We propose the first unsupervised approach to the problem of modeling dialogue acts in an open domain .Trained on a corpus of noisy Twitter conversations , our method discovers dialogue acts by clustering raw utterances .Because it accounts for the sequential behaviour of these acts , the learned mode ... \" .", "label": "", "metadata": {}, "score": "63.47034"}
{"text": "( 2005 ) for further details .Preprocessing I : isolating the input .In the Buckeye , a sample is made up of several files .For each sample , there is exactly one file with the extension.txt ; this file is a close orthographic transcription of the speaker 's speech .", "label": "", "metadata": {}, "score": "63.478954"}
{"text": "Figure 6 .Learning effects on text entry speeds and error rates for frequency - based and context - aware predictive disambiguation methods .With every ten short phrases grouped into a period , learning effects for text entry speeds and error rates for users of both frequency - based and context - aware predictive disambiguation methods are plotted in Figure 6 .", "label": "", "metadata": {}, "score": "63.50354"}
{"text": "Pitt M. , , Johnson K. , , Hume E. , , Kiesling S. , & Raymond W. , ( Year : 2005 ) .The Buckeye corpus of conversational speech : Labeling conventions and a test of transcriber reliability .Speech Communication45 , 90 - 95 .", "label": "", "metadata": {}, "score": "63.531082"}
{"text": "LSA links information retrieval and human semantic memory .Latent .Semantic Indexing ( LSI ) , like LSA , was tested against pre - examined . documents .Direct comparisons were muddied by preprocessing words .LSA .does synonym tests , since most near neighbors are related by the . cosine .", "label": "", "metadata": {}, "score": "63.53711"}
{"text": "-Yavar May 14 ' 13 at 5:49 .I would recommend dimensionality reduction instead of feature selection .Consider either singular value decomposition , principal component analysis , or even better considering it 's tailored for bag - of - words representations , Latent Dirichlet Allocation .", "label": "", "metadata": {}, "score": "63.61467"}
{"text": "4 is a flow chart depicting a method 310 A for generating partial characteristic coefficients to estimate the singular values corresponding to a topic as referred to in step 310 of .FIG .3 , according to an exemplary embodiment .", "label": "", "metadata": {}, "score": "63.61544"}
{"text": "These counts were entered into a tab - separated spreadsheet file , along with additional information such as a unique listener identifier and the listener age at document collection ( if identifiable ) , with each row representing one document .This spreadsheet formed the basis for the experiments reported below , and was read in as a data frame by R. .", "label": "", "metadata": {}, "score": "63.63155"}
{"text": "The proportion of speech that is child - directed varies enormously across children , and it varies from document to document within - child , and it varies even within a single document ; moreover , the documents themselves vary enormously in size .", "label": "", "metadata": {}, "score": "63.666115"}
{"text": "claim 6 , wherein the step of generating characteristic coefficients comprises , for each coefficient : . forming a vector representing probabilities of the terms in the documents ; . forming a matrix B with copies of the vector as its columns ; and .", "label": "", "metadata": {}, "score": "63.692543"}
{"text": "It should be noted that for any given matrix generation technique , only the matrix M need be changed .Thus , method 220 can be generalized for use with different forms of the term - by - document matrix .Several alternative example formulations of the M matrix are addressed hereinafter .", "label": "", "metadata": {}, "score": "63.70176"}
{"text": "The method of Selective Latent Semantic Indexing presented in this summary is for illustrative purposes only .Various aspects of the present invention can be more clearly understood and appreciated from a review of the following detailed description of the disclosed embodiments and by reference to the drawings and any claims that follow .", "label": "", "metadata": {}, "score": "63.715797"}
{"text": "They explicitly indicated that many of the segmental differences they found derived from lexical items that were associated with the target toys ( 2010 : 779 - 83 , 785 - 87 ) .Since many of the manner - class frequency differences they found were driven specifically by toy - associated lexical items , there is no reason to expect that these differences would generalize from their samples .", "label": "", "metadata": {}, "score": "63.75271"}
{"text": "Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand .", "label": "", "metadata": {}, "score": "63.797966"}
{"text": ".1000 ) were conducted .The actual density distribution was estimated likewise , except using the actual relative frequencies instead of the generated ones .The actual distribution and median expected distribution ( with confidence intervals ) are plotted in Figure 1 .", "label": "", "metadata": {}, "score": "63.917248"}
{"text": "The three character keys were arranged as per a previous optimized design [ 10].The other two keys allowed cycling through candidate words and basic editing .Four functions were implemented : .DONE : When a text phrase is complete , DONE is pressed to clear the text box and begin another phrase .", "label": "", "metadata": {}, "score": "63.941902"}
{"text": "These effects imply that listeners dynamically adjust their expectations of upcoming linguistic material in a way that can not be explained by the null hypothesis .In summary , it seems to be an inherent property of words that they are more or less bursty .", "label": "", "metadata": {}, "score": "63.96676"}
{"text": "In fact , the performance of the context - aware method always exceeded the performance of the standard method in each individual run of the ten - fold cross validation simulations .It is worth noting that the improvements are most pronounced for small keypad designs , but apply to standard 12-key mobile phones as well ( with no changes required to the physical keypad layout or to the way text is entered ) .", "label": "", "metadata": {}, "score": "63.991432"}
{"text": "Unfortunately I 've also not worked with Turkish datasets or the python language .I answered you in my question above .Please take a look .Thanks for your answer btw . - clancularius Mar 18 ' 13 at 14:17 .", "label": "", "metadata": {}, "score": "64.14305"}
{"text": "Terminology .The input .For the purposes of this article , ' the input ' is defined as the set of utterances a listener hears that were not produced by that listener .For methodological and theoretical reasons , the present study does not distinguish between input that was directed to the listener and other input .", "label": "", "metadata": {}, "score": "64.17028"}
{"text": "Table 4 shows the five words that contribute most to the observed asymmetry , as well as the five words that anti - contribute the most to the observed asymmetry , for two manners , glides and nasal .The total asymmetry is also shown .", "label": "", "metadata": {}, "score": "64.218254"}
{"text": "EXPERIMENT II : SEGMENTAL DISTRIBUTIONS IN CHILD AND ADULT INPUT .Since Experiment II focuses on consonantal manner classes , relative frequency is calculated with respect to consonants only .Figure 2 uses a violin plot to compare the distribution of relative frequencies of each consonantal manner class in the child and adult input corpora described in Experiment I .", "label": "", "metadata": {}, "score": "64.32542"}
{"text": "Because of the relatively small size of this corpus , the predictive disambiguation performance might further improve if larger text corpora were used in the future .Simulation Setup .The simulation used a ten - fold cross validation design .The corpus was randomly divided into ten equally sized data sets .", "label": "", "metadata": {}, "score": "64.36795"}
{"text": "To address this possibility , a series of mixed - effects linear regressions was carried out .GENERAL DISCUSSION .Summary of key findings .The goal of this article was to investigate the amount and causes of frequency variation for manner classes in the input to children and the input to adults .", "label": "", "metadata": {}, "score": "64.38136"}
{"text": "Experiment II showed the English segments are approximately unitary : the natural background variation in segmental frequencies that arises within a single language variety is much larger than numerical differences across varieties .Variation in segmental frequencies seems to be driven by variation in discourse topic ; topic - associated words cause bursts / lulls in local segmental frequencies .", "label": "", "metadata": {}, "score": "64.44289"}
{"text": "Observe that the total frequency of manner \u03d5 may be expressed as the sum of frequencies contributed by each of the words that contain \u03d5. Crucially , the above definitions can be made variety - specific by calculating frequencies with respect to particular registers , notated here with a subscript ( child or adult ) .", "label": "", "metadata": {}, "score": "64.57105"}
{"text": "A language variety is a set of utterances that share properties of interest .In the present article , the relevant property is whether the target listener is a child or an adult .Note that the terms ADS / IDS / CDS are avoided here , since it was infeasible ( and arguably undesirable ) to eliminate utterances from the input that were not directed toward the listener .", "label": "", "metadata": {}, "score": "64.59686"}
{"text": "A support vector machine is a supervised learning algorithm developed .over the past decade by Vapnik and others ( Vapnik , Statistical .Learning Theory , 1998 ) .The algorithm addresses the general problem of .learning to discriminate between positive and negative members of a .", "label": "", "metadata": {}, "score": "64.69544"}
{"text": "Deciding whether an email is spam or not .Deciding what the topic of a news article is , from a fixed list of topic areas such as \" sports , \" \" technology , \" and \" politics . \"Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution .", "label": "", "metadata": {}, "score": "64.70694"}
{"text": "When we speak , the sounds that we produce are a function of the words we choose , and we normally choose words to convey a meaning .Thus , the relative frequency of sounds in speech is driven by the relative frequency of the meanings we express .", "label": "", "metadata": {}, "score": "64.792465"}
{"text": "Reducing this redundancy should enhance computational efficiency .The composite wavelet energy is calculated by taking the sum of squares across all channels ( index m ) for a fixed location ( index j ) and fixed multiresolution level ( index k ) .", "label": "", "metadata": {}, "score": "64.829544"}
{"text": "7 ) .Search engine server 710 can form one or more term - by - document matrices A. Such a matrix A can represent the occurrence of terms in documents stored on the other servers attached to Internet 720 .By examining the matrix A , search engine server 710 can respond to search queries .", "label": "", "metadata": {}, "score": "64.89806"}
{"text": "In contrast , when a new toy is introduced to a mother who is having a conversation with an adult , she is less likely to name the toy repeatedly and invoke other words associated with it ; presumably she simply continues the conversation she is already having .", "label": "", "metadata": {}, "score": "64.98599"}
{"text": "The method of claim 6 wherein the document is partitioned according to the semantic structure of the document at multiple levels to produce an outline of the document .The method of claim 6 wherein the document is partitioned according to the semantic structure of the document at multiple levels to produce a fuzzy outline of the document .", "label": "", "metadata": {}, "score": "64.9884"}
{"text": "To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams .Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset .The label True indicates that the entailment holds , and False , that it fails to hold .", "label": "", "metadata": {}, "score": "65.05464"}
{"text": "how to access the quality of each .The paper surveys the machine learning text classification algorithms .and measure their efficiency to predict the best classification .method .This is a thesis written by one of the doctorate candidates of the .", "label": "", "metadata": {}, "score": "65.09519"}
{"text": "[ 9 ] Flinchem , P. E. , Grover , D. , Grunbock , C. , King , T. M. , and Kushler , A. C. 2001 .Reduced Keyboard Disambiguating System .US Patent Number 6307548 .[ 10 ] Gong , J. and Tarasewich , P. 2005 .", "label": "", "metadata": {}, "score": "65.18852"}
{"text": "There are two types of compression commonly used , often simultaneously .Both are lossy -- some information is lost in the compression procedure .In a truncation type scheme , wavelet coefficients less than a specified cutoff value are replaced by zeros .", "label": "", "metadata": {}, "score": "65.250015"}
{"text": "This generalization can be re - applied in various application realms , for example , in genetics or bioinformatics the usage can be ( gene , individual , expression , population model ) .An example from the realm of chemistry can be ( atom , molecule , classification of chemical , molecular model ) .", "label": "", "metadata": {}, "score": "65.39068"}
{"text": "The results of the indexed iterations are the t partial characteristic coefficients used by method 220 in .FIG .3 .The method 310 A then proceeds to step 320 ( .FIG .3 ) .FIG .5 is a flow chart depicting a method 310 B for recursively generating partial characteristic coefficients to estimate the singular values corresponding to a topic where the document lengths are uniform , as referred to in step 310 of .", "label": "", "metadata": {}, "score": "65.43631"}
{"text": "deWaC : a 1.7 billion word corpus constructed from the Web limiting the crawl to the .de domain and using medium - frequency words from the SudDeutsche Zeitung corpus and basic German vocabulary lists as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger using this tagset , more information available here . sdewac a 0.88 billion word corpus derived from deWaC , duplicate sentences and some noise have been removed .", "label": "", "metadata": {}, "score": "65.53517"}
{"text": "These varied applications of the inventive method are within the scope and spirit of the invention as the invention is not to be limited by any use herein of exemplary language from the realm of term and document indexing or otherwise .", "label": "", "metadata": {}, "score": "65.54603"}
{"text": "[ 15 ] described a technique with a reduced computer keyboard containing more than one letter on a single key .The technique determined which letter was the most likely using both an English dictionary and letter trigram statistics .The authors also used a genetic algorithm to alter the keyboard layout to minimize word ambiguity .", "label": "", "metadata": {}, "score": "65.550415"}
{"text": "The decision to focus on /l/ rather than any other segment was somewhat arbitrary ; the only real basis for selection was that it is somewhere in the middle of the frequency spectrum for English segments .Nothing hinges on the particular choice of /l/ ; the relevant fact is that if the null hypothesis is false for any item , then it is false in general .", "label": "", "metadata": {}, "score": "65.59815"}
{"text": "Much of the SVM 's power comes from its criterion for selecting a . separating plane when many candidates planes exist : the SVM chooses .the plane that maintains a maximum margin from any point in the .training set .", "label": "", "metadata": {}, "score": "65.62648"}
{"text": "AGAIN , As far as I can assume you may already have knowledge of the algorithms , therefore instead of Repeating the information or writing my own review , I tried hard to find the type of papers that can help you with your research .", "label": "", "metadata": {}, "score": "65.633804"}
{"text": "Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) .Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) .The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \u00d7 Precision \u00d7 Recall ) / ( Precision + Recall ) .", "label": "", "metadata": {}, "score": "65.636795"}
{"text": "The method of .claim 11 , wherein the step of estimating singular values corresponding to a probabilistic distribution on parts comprises the steps of : . generating characteristic coefficients ; . forming a characteristic polynomial ; and . solving for the roots of the characteristic polynomial , wherein multiplying the roots by the number of collections related to the probabilistic distribution on parts and then taking a square - root yields the estimated singular values .", "label": "", "metadata": {}, "score": "65.65352"}
{"text": "It is logically imaginable that a language would consist of distinct varieties , each of which was unigram ; it is also logically imaginable that a language might consist of a single variety that is not unigram .In fact , this article will argue that this latter is the most insightful characterization of the true state of affairs for English segmental frequency distributions .", "label": "", "metadata": {}, "score": "65.65584"}
{"text": "But how did we know where to start looking , which aspects of form to associate with which aspects of meaning ?The goal of this chapter is to answer the following questions : .How can we identify particular features of language data that are salient for classifying it ?", "label": "", "metadata": {}, "score": "65.67638"}
{"text": "Use of this language is merely exemplary and is not intended to limit the information retrieval applications of the inventive methods to the realm of documents made up of words or terms .One of ordinary skill in the art will appreciate , without departure from the scope or spirit of the invention , the generalization of these terms to other information retrieval applications .", "label": "", "metadata": {}, "score": "65.74753"}
{"text": "KNN is a good choice when simplicity and accuracy are the predominant . issues .KNN can be superior when a resident , trained and tested .classifiers has a short useful lifespan , such as in the case with the .", "label": "", "metadata": {}, "score": "65.75846"}
{"text": "I have researched several articles and compiled a summary of the advantages and disadvantages of the models .I am unable to find any article that specifically relates to the comparison of each model and than prove for various situations , which is the best for each .", "label": "", "metadata": {}, "score": "65.840225"}
{"text": "Stationary means that the probability of events is constant , rather than varying with time .For example , we normally believe that a coin does not become biased toward heads over time .Ergodic means that all sources are equivalent , i.e. the statistical properties are the same whether one obtains samples by flipping one coin 100 times , ten coins ten times , or 100 coins one time each .", "label": "", "metadata": {}, "score": "65.90855"}
{"text": "claim 22 , wherein the step of generating characteristic coefficients comprises computing a recursion such that each subsequent coefficient is based on at least one coefficient already computed .The method of .claim 24 , wherein the step of generating characteristic coefficients further comprises computing a probabilistically weighted average of coefficients based on the probability of sample length , the weighted averaging allowing the method to function with samples of non - uniform lengths .", "label": "", "metadata": {}, "score": "65.91699"}
{"text": "Visualization may be dramatically enhanced by then allowing the user to rotate the orientation angle .This dynamic surface shows at a glance the entire thematic complexity of the article at all the multi - resolution levels including major sections of topics , subsections , and transition paragraphs .", "label": "", "metadata": {}, "score": "66.02718"}
{"text": "It is natural to ask whether this general pattern also applies to other articulatory dimensions of contrast .Manner of articulation is an especially important phonological dimension , since manner is correlated with sonority and syllabification .To my knowledge , very few studies have directly investigated frequency of manner - of - articulation classes in the input .", "label": "", "metadata": {}, "score": "66.210526"}
{"text": "We describe a user study with 32 participants entering text on a keypad with letters arranged on three keys .Entry speed was 9.6 % faster , and error rates 21.2 % lower , compared with standard disambiguation , as found on mobile phones .", "label": "", "metadata": {}, "score": "66.248535"}
{"text": "Problems in previously published .experiments are analyzed , and the results of flawed experiments are .excluded from the cross - method evaluation .As . a result , eleven out of the fourteen methods are remained .A . k - nearest neighbor ( kNN ) classifier was chosen for .", "label": "", "metadata": {}, "score": "66.26659"}
{"text": "Gender Identification .In 4 we saw that male and female names have some distinctive characteristics .Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male .", "label": "", "metadata": {}, "score": "66.32296"}
{"text": "US Patent Number 6005495 .[ 6 ] Dominowska , E. , Roy , D. , and Patel , R. 2002 .An adaptive context - sensitive communication aid .In Proceedings of the 17th Annual Technology and Persons with Disabilities Conference ( Northridge , CA , 2002 ) .", "label": "", "metadata": {}, "score": "66.49277"}
{"text": "claim 22 , wherein the step of generating characteristic coefficients comprises , for each coefficient : . forming a vector representing probabilities of the parts ; . forming a matrix B with copies of the vector as its columns ; and .", "label": "", "metadata": {}, "score": "66.547516"}
{"text": "English .PukWaC : the same as ukWaC , but with a further layer of annotation added , i.e. a full dependency parse .The parsing was performed with the MaltParser .Some useful information about the dependency relations used in PukWaC can be found on pp .", "label": "", "metadata": {}, "score": "66.60997"}
{"text": "KNN does not rely on prior probabilities , and it is . computationally efficient .The main computation is the sorting of the .training documents in order to find out the K nearest neighbors for .the test document .K - Nearest Neighbor is useful when their are less than 20 attributes . per instance , there is lots of training data , training is very fast , .", "label": "", "metadata": {}, "score": "66.61192"}
{"text": "ukWaC : a 2 billion word corpus constructed from the Web limiting the crawl to the .uk domain and using medium - frequency words from the BNC as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger .", "label": "", "metadata": {}, "score": "66.659515"}
{"text": "Its computational complexity is O(N ) , which is slightly faster than the fast fourier transform .The filters c k and d k are called low- and high - pass filters , respectively .This refers to the part of the frequency spectrum that they are biased towards- low or high frequencies .", "label": "", "metadata": {}, "score": "66.6671"}
{"text": "As discussed above , fast algorithms exist for computing the wavelet transform .The algorithm is based on the two - scale relation ( 1 ) and is of similar complexity , O(N ) , as the fast fourier transform , where N is the number of elements in the vector or signal .", "label": "", "metadata": {}, "score": "66.70781"}
{"text": "The current system does not yet learn text patterns based on specific user history .This could be another interesting avenue for potential improvements .In addition , the method needs to be tested with character sets other than English , as word dictionaries in different languages may provide different results than those presented here .", "label": "", "metadata": {}, "score": "66.864845"}
{"text": "In step 240 , the identification of actual singular values of matrix A that correspond to the estimated singular values for the topic are used to establish a topical partitioning of the singular values of matrix A. In other words , actual singular values that correspond to the estimated singular values for the topic can be identified and grouped to partition the selected topic from other topics represented in the matrix A. As the steps 220 - 240 are performed for multiple topics , the singular values of the term - by - document matrix A are partitioned into subsets of singular values that correspond to the topics covered by the documents .", "label": "", "metadata": {}, "score": "66.975876"}
{"text": "DA .KSPC .Corpus .We used the BNC Baby corpus [ 3 ] for these simulations .This is a four million word sampling of the entire British National Corpus ( BNC ) .BNC Baby was chosen because of its XML formatting and because it is entirely POS tagged .", "label": "", "metadata": {}, "score": "67.00293"}
{"text": "In step 530 , a coefficient of index zero , written as c 0 , is initialized to the value of one .This coefficient forms the base of the recursion performed in step 540 .Then , in step 540 , the remaining coefficients up to c t are computed using the recursion illustrated in Equation 4 below .", "label": "", "metadata": {}, "score": "67.22227"}
{"text": "While the above themes have been explained in detail for illustrative purposes , the present invention should in no way be limited to those precise schemes .Many other wavelets and corresponding subband coding schemes have been generated in recent years , and the use of these schemes in the method of the present invention is fully contemplated by the present invention .", "label": "", "metadata": {}, "score": "67.24722"}
{"text": "Each combination of labels and features that receives its own parameter is called a joint - feature .Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values .Note .", "label": "", "metadata": {}, "score": "67.33229"}
{"text": "French .frWaC : a 1.6 billion word corpus constructed from the Web limiting the crawl to the .fr domain and using medium - frequency words from the Le Monde Diplomatique corpus and basic French vocabulary lists as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger , more information available here .", "label": "", "metadata": {}, "score": "67.40833"}
{"text": "Algorithms using text co - occurrence information were proposed for finding the semantic relatedness between English word pairs .Various Part - of - Speech ( POS ) models are widely used by researchers in Natural Language Processing ( NLP ) .", "label": "", "metadata": {}, "score": "67.467545"}
{"text": "journal/2001WordsSenses .pdf .SUMMARY .This paper is a comparative study of text categorization methods .Fourteen methods are investigated , based on .previously published results and newly obtained results from .additional experiments .Corpus biases in commonly .", "label": "", "metadata": {}, "score": "67.4917"}
{"text": "These potential differences are the ones of interest .However , just because the medians are visually different on the violin plot , it does not follow that the distributions themselves have different means .This is because , for all six manners , the relative frequency distributions heavily overlap between the child and adult corpora .", "label": "", "metadata": {}, "score": "67.52771"}
{"text": "The role of frequency in segmental acquisition .One reason to focus on variation in segmental frequencies is that absolute segmental frequency appears to matter for segmental acquisition .Analogously , Beckman et al .( 2003 ) showed that in Japanese , dorsal stops were more frequent than coronal stops , and Japanese - learning children produced dorsal stops more rapidly and/or more accurately than coronal stops , while English - learning children exhibited the opposite pattern .", "label": "", "metadata": {}, "score": "67.72452"}
{"text": "How much variation is there ?( ii )What contributes to it ? and ( iii )How much does it matter for language acquisition , if at all ?Even if the empirical focus is rather narrow , it is to be hoped that the article is of general interest , since the methodological points of this study are likely to generalize to other domains .", "label": "", "metadata": {}, "score": "67.787506"}
{"text": "This work is inspired by a corpus of 1.3 million Twitter conversations , which will be made publicly available .This huge amount of data , available only because Twitter blurs the line between chatting and publishing , highlights the need to be able to adapt quickly to a new medium . by Alan Ritter , Oren Etzioni - In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics ( ACL , 2010 . \" ...", "label": "", "metadata": {}, "score": "67.82069"}
{"text": "For example , consider a classifier that determines the correct word sense for each occurrence of the word bank .If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 .", "label": "", "metadata": {}, "score": "67.825485"}
{"text": "It 's common to start with a \" kitchen sink \" approach , including all the features that you can think of , and then checking to see which features actually are helpful .We take this approach for name gender features in 1.2 .", "label": "", "metadata": {}, "score": "67.83969"}
{"text": "From step 270 , the method 130 proceeds to step 140 ( .FIG .1 ) .FIG .3 is a flow chart depicting a method 220 for calculating the expected singular values corresponding to a topic according to an exemplary embodiment , as referred to in step 220 of .", "label": "", "metadata": {}, "score": "67.9082"}
{"text": "Power Point presentation of five text classifiers ( Read the . conclusion ) .Hope this will help you .I will try to post more resources if I come .across any .Please clarify if you have any questions regarding this . research or if you need more help .", "label": "", "metadata": {}, "score": "68.51434"}
{"text": "Therefore , disclosure of a particular set of program code instructions or detailed hardware devices is not considered necessary for an adequate understanding of how to make and use the invention .The inventive functionality of the claimed computer implemented processes will be explained in more detail in the following description in conjunction with the remaining figures illustrating other logical process flows .", "label": "", "metadata": {}, "score": "68.64775"}
{"text": "Thus , a violin plot conveys whether a distribution is unimodal or not , while this information is not available from a boxplot .One fact that is immediately apparent from visually inspecting Figure 2 is that there appear to be some child / adult differences .", "label": "", "metadata": {}, "score": "68.6791"}
{"text": "Both products have the same non - zero eigenvalues , given by the non - zero entries of , or equally , by the non - zero entries of .Now the decomposition looks like this : .The values are called the singular values , and and the left and right singular vectors .", "label": "", "metadata": {}, "score": "68.68851"}
{"text": "10 Exercises .Find out what type and quantity of annotated data is required for developing such systems .Why do you think a large amount of data is required ?Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set .", "label": "", "metadata": {}, "score": "68.72951"}
{"text": "Naive Bayes classification gets around this problem by not requiring .that you have lots of observations for each possible combination of .the variables .Rather , the variables are assumed to be independent of . one another and , therefore the probability that a fruit that is red , . round , firm , 3 \" in diameter , etc . will be an apple can be calculated .", "label": "", "metadata": {}, "score": "68.74426"}
{"text": "difficult to interpret and leading to considerable confusions in the . literature .Using the results evaluated on the other versions of .Reuters which exclude the unlabelled documents , the performance of .twelve methods are compared directly or indirectly .", "label": "", "metadata": {}, "score": "68.76901"}
{"text": "These findings are referred to as the Semantic Proximity Effect .[ 6 ] .When participants made mistakes in recalling studied items , these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list .", "label": "", "metadata": {}, "score": "68.77496"}
{"text": "Science277 , 684 - 86.9235890 .Lam C. , & Kitamura C. , ( Year : 2010 ) .Maternal interactions with a hearing and hearing - impaired twin : Similarities and differences in speech input , interaction quality , and word production .", "label": "", "metadata": {}, "score": "68.96658"}
{"text": "Such signals can include modulated electrical , optical , microwave , radiofrequency , ultrasonic , or electromagnetic energy , among other energy forms .Search engine server 710 can scan information made available on other servers on the Internet ( these other servers are not shown in .", "label": "", "metadata": {}, "score": "68.96904"}
{"text": "Beside the question of what combination of speakers was present , it is equally of interest how much input a given role contributes ( independent of what other speakers are present ) ; raw and percentage counts are reported in Table 2 .", "label": "", "metadata": {}, "score": "68.98363"}
{"text": "[ 25 ] Rau , H. and Skiena , S. S. 2004 .Dialing for documents : An experiment in information theory .In Proceedings of the 17th Annual ACM Symposium on User Interface Software and Technology ( Santa Fe , NM , October 24 - 27 , 2004 ) .", "label": "", "metadata": {}, "score": "69.04019"}
{"text": "Because of the small key set , deletion operated on a word basis and was only allowed in editing mode ( when keystroke sequences were committed ) .NEXT : In disambiguation mode , NEXT cycles through candidate words if a keystroke sequence is ambiguous . \" _ _ \" ( SPACE ) : Pressing SPACE commits the currently disambiguated word .", "label": "", "metadata": {}, "score": "69.07816"}
{"text": "All of these methods showed significant improvement .( up to 71 % reduction in weighted error rates ) .over the performance of the original kNN algorithm on .TDT benchmark collections , making kNN among the . top - performing systems in the recent TDT3 official evaluation .", "label": "", "metadata": {}, "score": "69.09544"}
{"text": "[26 ] reviews existing text entry methods on devices with only a few keys ( usually 3 to 4 ) .Such devices are particularly useful for users with special needs ( e.g. , those with motor or visual impairments ) .", "label": "", "metadata": {}, "score": "69.389915"}
{"text": "As mentioned previously , any other type of matrix formulation technique could be substituted for this step by simply changing the way M is constructed .Several alternative example formulations of the M matrix are addressed hereinafter .In step 630 , the sequence an is computed where n ranges from one to t ( inclusive of both one and t ) .", "label": "", "metadata": {}, "score": "69.40796"}
{"text": "Top five words contributing to the asymmetry between CDS and ADS in the relative frequency of glides ( columns 2 - 3 ) and nasals ( columns 4 - 5 ) are shown in the top five data rows .The five words that anti - contribute the most to the total asymmetry are shown in the bottom five data rows .", "label": "", "metadata": {}, "score": "69.48842"}
{"text": "For example , in the sentence \" The dog was really sick and barked all night \" , the word \" dog \" is a good contextual word for disambiguating \" barked \" .To capture this relationship , a word level n -gram model of length six is necessary ; however , this exceeds the memory limitations of most mobile devices .", "label": "", "metadata": {}, "score": "69.494415"}
{"text": "Improving dictionary - based disambiguation text entry method accuracy .In Extended Abstracts of the ACM Conference on Human Factors in Computing Systems ( San Jose , CA , April 28 - May 3 , 2007 ) .CHI ' 07 .", "label": "", "metadata": {}, "score": "69.55165"}
{"text": "Do you know how to optimize the implementation of term - frequency method ? - clancularius Nov 29 ' 12 at 6:47 .I 'm sure this is way too late to be of use to the poster , but perhaps it will be useful to someone else .", "label": "", "metadata": {}, "score": "69.71579"}
{"text": "claim 1 , wherein the selecting step further comprises selecting a plurality of actual singular values of matrix A that each correspond to at least one of the estimated singular values , the actual singular values corresponding to at least one of the topics .", "label": "", "metadata": {}, "score": "69.78596"}
{"text": "An elevation or ( x - y ) location can be specified from graphical user input to perform certain functions .For example , the user can specify the elevation , or energy level , used in selection of text breaks by GUI on the \" Waves \" visualization .", "label": "", "metadata": {}, "score": "70.18971"}
{"text": "Words that deviate the least from the null hypothesis tend to subserve topic - general , core functions of English such as syntactically obligatory marking .Even within the same class of words , some words are more bursty ( e.g. Obama ) , and others less so ( e.g. John ) .", "label": "", "metadata": {}, "score": "70.2204"}
{"text": "C ( Stem ( w 2 ) ) is the number of times the stem of w 2 occurs in the training corpus .C ( Stem ( w 1 ) , Stem ( w 2 ) ) is the number of times the stems of w 1 and w 2 occur in the same defined contexts in the training corpus .", "label": "", "metadata": {}, "score": "70.239136"}
{"text": "[ .j .j .i .l . ] log .j .i . log .j .j .l .j .j .t . )p .j .p .t .j .", "label": "", "metadata": {}, "score": "70.3611"}
{"text": "For instance , their citation translation equivalents in Russian are /t\u0197/ ' you .I NFORMAL .N OM ' , /tvo - j/ ' you .I NFORMAL -A DJ .N OM.MASC ' , /\u00e7to-/ ' what - N OM ' , /xot - it\u02b2/ ' want - I NF ' ; the only glide in these items comes from the inflectional marker in /tvoj/. Thus , the mild preponderance of glides in the input to English - learning children is likely a statistical accident , rather than reflecting tailoring of caregivers .", "label": "", "metadata": {}, "score": "70.39293"}
{"text": "Further description is omitted , as readers of this journal are likely to be familiar with CHILDES .The Buckeye corpus .The Buckeye corpus was collected with the intention of collecting a representative sample of the variation in speech from native talkers of a typical Midwestern town .", "label": "", "metadata": {}, "score": "70.403915"}
{"text": "From a more practical standpoint , there are many other requirements on the function to ensure that the resulting transform is useful .However , the requirements are quite variable depending on the application and the data \u0192(x ) that it will be applied to .", "label": "", "metadata": {}, "score": "70.409805"}
{"text": "This invention was made with Government support under Contract DE - AC06 - 76RL0 1830 awarded by the U.S. Department of Energy .The Government has certain rights in the invention .FIELD OF THE INVENTION .The present invention relates generally to a method for automatically partitioning an unstructured electronically formatted natural language document into its sub - topic structure and specifies a device that may be used to graphically display and interact with the sub - topic structure of the document .", "label": "", "metadata": {}, "score": "70.50256"}
{"text": "In step 670 , the partial characteristic coefficients for each length ( generated at each pass of step 650 ) can be combined together as a weighted sum according to the expression set forth in Equation 6 below .c . i .", "label": "", "metadata": {}, "score": "70.52468"}
{"text": "For each ' violin ' , the white dot and thick internal lines represent the median and 25th/75th percentiles , and the total height of the violin represents the range of the data after outliers are trimmed - just as with a boxplot .", "label": "", "metadata": {}, "score": "70.53113"}
{"text": "As used herein , the visualization of sub - topic structure includes an energy surface device called \" Waves \" and a topographical surface called \" Topic Islands \" .Also as described herein , this approach to sub - topic structure is called \" topic - o - graphy \" .", "label": "", "metadata": {}, "score": "70.670906"}
{"text": "From step 650 , decision step 660 determines if all possible lengths have been used for calculating a set of coefficients .If possible lengths remain , the method 310 C loops back to step 610 where another possible length is selected for use in steps 620 , 630 , 640 and 650 .", "label": "", "metadata": {}, "score": "70.83597"}
{"text": "None of the other manners reaches significance as k is varied , although a clear linear trend is apparent in all cases .One point this suggests is that , while the current data do not actually demonstrate a significant difference for any manner besides glides , there may indeed be small differences in the relative frequency of most manners of articulation between child and adult input .", "label": "", "metadata": {}, "score": "70.93784"}
{"text": "It is intended that all such aspects , systems , methods , features , advantages , and objects are included within this description , are within the scope of the present invention , and are protected by any accompanying claims .BRIEF DESCRIPTION OF THE DRAWINGS .", "label": "", "metadata": {}, "score": "71.19917"}
{"text": "While a preferred embodiment of the present invention has been shown and described , it will be apparent to those skilled in the art that many changes and modifications may be made without departing from the invention in its broader aspects .", "label": "", "metadata": {}, "score": "71.25583"}
{"text": "The improvement will be more pronounced on very small keyboards ( e.g. , 3 or 4 keys ) since the word lists are often large .Devices with very few buttons are often used for text entry in scenarios including text entry for disabled users , text entry with game controllers , and text entry with wearable computing devices .", "label": "", "metadata": {}, "score": "71.35249"}
{"text": "A pervasive theme in contemporary language research is that frequency matters .It has become clear that to properly validate our theories , we must have a detailed understanding of the input , including the frequency relations it contains .This article focuses on the frequency of consonantal manner - of - articulation classes ( stop , liquid , nasal , etc . ) in English .", "label": "", "metadata": {}, "score": "71.4155"}
{"text": "The next section takes up the question of why glides seem to be more frequent in child input than in adult input .WHY ARE GLIDES MORE FREQUENT IN CHILD INPUT THAN IN ADULT INPUT ?There is a trivial sense in which the answer to this question can only be that when adults are speaking to children , they use more words that contain glides .", "label": "", "metadata": {}, "score": "71.5148"}
{"text": "It was not feasible to supply phonological transcriptions for items whose frequency was thirty or less , and which were unlisted in the pronouncing dictionary , owing to the excessively large number of such items ( 19,000 ) .In the absence of phonological transcriptions , it is not possible to completely rule out the hypothesis that the non - inclusion of these low - frequency items may have meaningfully altered the results .", "label": "", "metadata": {}, "score": "71.631546"}
{"text": "The subject matter of the present invention is particularly pointed out and distinctly claimed in the concluding portion of this specification .However , both the organization and method of operation , together with further advantages and objects thereof , may best be understood by reference to the following description taken in connection with accompanying drawings wherein like reference characters refer to like elements .", "label": "", "metadata": {}, "score": "71.872314"}
{"text": "In step 440 , the matrix transpose of B is multiplied by matrix B to form a Gram matrix , written as B T B. The transpose of a first matrix is a second matrix , produced by turning the rows of the first matrix into the columns of the second matrix as to reflect the matrix about the diagonal that runs from the top left to bottom right of the matrix .", "label": "", "metadata": {}, "score": "72.20961"}
{"text": "The invention can be provided as a computer program which can include a machine - readable medium having stored thereon instructions which can be used to program a computer ( or other electronic devices ) to perform a process according to the invention .", "label": "", "metadata": {}, "score": "72.34182"}
{"text": "5.5 The Cause of Double - Counting .The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined .", "label": "", "metadata": {}, "score": "72.47639"}
{"text": "that is red and round , which type of fruit is it most likely to be , . based on the observed data sample ?In future , classify red and round . fruit as that type of fruit . \"A difficulty arises when you have more than a few variables and . classes -- you would require an enormous number of observations .", "label": "", "metadata": {}, "score": "72.48465"}
{"text": "Figure 2 .Devices with very few keys or buttons .The Dunlop [ 7 ] watch interface is shown in the lower left .Semantic Relatedness of Word Pairs .N -gram models have been used by text entry or natural language processing systems to capture relationships between different words [ 20].", "label": "", "metadata": {}, "score": "72.61036"}
{"text": "The author supplied a phonological transcription for items with a frequency greater than thirty if they constituted a content word ( somethin , d'you , Tyrese ) rather than an interjection or sound routine ( peekaboo , num , whoopsie ) .", "label": "", "metadata": {}, "score": "72.660645"}
{"text": "kiew.cs.uni-dortmund .de:8001/mlnet/ instances/81d91eaa - da14013dc6 .Sorry !I had an emergency situation and could n't reply .Please refer .to the reference papers and you can have a very good idea of the . advantages and disadvantages related to SVM .", "label": "", "metadata": {}, "score": "72.66935"}
{"text": "Therefore , the optimization algorithm simply collects the data points and progressively varies \u03b8 and \u03c6 from 0 \u00b0 to 90 \u00b0 in steps of 1 \u00b0 .The parameters \u03b1 , \u03b2 , and \u03b3 are chosen based on the best angles reported ; that is , those that place the most data points in the positive plane .", "label": "", "metadata": {}, "score": "72.69909"}
{"text": "@larsmans This is already what I ask for .As I explained above , I am looking for a better \" feature selection \" method , which you advise me to do .Yes , I need to select my features to obtain better accuracy results , but how ?", "label": "", "metadata": {}, "score": "72.904015"}
{"text": "Equivalently , /l/ is bursting and lulling , rather than occurring randomly according to its expected frequency .Presumably , this property derives from the fact that some documents contain bursts of words that contain /l/ ( e.g. when parents are discussing lemonade , lollipops , etc . ) .", "label": "", "metadata": {}, "score": "73.09454"}
{"text": "As will be apparent to those skilled in the art , other filters could also be used ; the optimal filter being dependant on the particular user needs .In the preferred embodiment of the present invention , a dilation factor of 2 is used .", "label": "", "metadata": {}, "score": "73.11548"}
{"text": "29 of CRPIT , Australian Computer Society , 2004 , pp .123 - 129 .Y. Fu , T. Bauer , J. Mostafa , M. Palakal , S. Mukhopadhyay , Concept extraction and association from cancer literature , in : R. H. L. Chiang , E.-P. Lim ( Eds . ) , Fourth ACM CIKM International Workshop on Web Information and Data Management ( WIDM 2002 ) , SAIC Headquaters , McLean , Virginia , USA , Nov. 8 , 2002 , ACM ; 2002 ; pp .", "label": "", "metadata": {}, "score": "73.20712"}
{"text": "Language - specific and language - universal aspects of lingual obstruent productions in Japanese - acquiring children .Journal of the Phonetic Society of Japan7 , 18 - 28 .Feldman N. H. , , Griffiths T. L. , & Morgan J. L. , ( Year : 2009 ) .", "label": "", "metadata": {}, "score": "73.22548"}
{"text": "the price of gas is high .Results and Discussion .Table 3 shows the average text entry speed ( in words per minute , WPM ) for the frequency - based and context - based predictive disambiguation methods , along with the average number of errors per word ( both assuming an average word length of five characters ) .", "label": "", "metadata": {}, "score": "73.280945"}
{"text": "instances/81d91e93-df4da3c279 .Thanks for the clarifications .I am still trying to find the relevant .documents or papers .Sincerely , . leader - ga .Request for Answer Clarification by mcsemorgan - ga on 06Aug 2003 00:21 PDT .", "label": "", "metadata": {}, "score": "73.356125"}
{"text": "Experimental results on the DUC2001 and DUC2002 datasets demonstrate the good effectiveness of our proposed summarization models .The results also demonstrate that the ClusterCMRW model is more robust than the ClusterHITS model , with respect to different cluster numbers .Categories and Subject Descriptors : . ...", "label": "", "metadata": {}, "score": "73.46891"}
{"text": "Another example used in IR is an algorithm for finding sub - topic structure in expository text that uses a moving window approach .[ Multi - Paragraph Segmentation Of Expository Text , Marti A. Hearst , ACL ' 94 , Las Cruces , NM].", "label": "", "metadata": {}, "score": "73.642136"}
{"text": "PDAs often use a stylus with a virtual keyboard .While some mobile phones have integrated full keyboards , most employ a 12-key keypad ( Figure 1 ) .The result is ambiguous when used for text entry because multiple letters reside on each key .", "label": "", "metadata": {}, "score": "73.744644"}
{"text": "- stefan Dec 2 ' 12 at 16:30 .4 Answers 4 .As Bee points out and you are already aware , the use of SVM as a classifier is wasted if you have already lost the information in the stages prior to classification .", "label": "", "metadata": {}, "score": "73.86099"}
{"text": "If not , then the method 130 branches back to step 210 to partition another topic .If yes , then the method 130 branches to step 260 .In step 260 , the number of singular values to retain from each topical partition of the singular values of A is determined .", "label": "", "metadata": {}, "score": "73.87394"}
{"text": "The most significant improvement was gained with the three - key keypad using the context - aware predictive disambiguation method .The DA value improved 4.24 % from the original 67.58 % to 71.82 % , and the KSPC value improved 0.0335 , dropping from 1.2124 to 1.1789 .", "label": "", "metadata": {}, "score": "73.92442"}
{"text": "However , to more quickly test our concepts , we chose a Pocket PC device with a virtual keypad design as an initial test platform .While we feel this is suitable for initial testing , we will move to other devices as research continues .", "label": "", "metadata": {}, "score": "73.95309"}
{"text": "Watch - top text - entry : Can phone - style predictive text - entry work with only 5 buttons ?In Proceedings of the 6th International Conference on Human Computer Interaction with Mobile Devices and Services ( Glasgow , Scotland , September 13 - 16 , 2004 ) .", "label": "", "metadata": {}, "score": "74.28105"}
{"text": "claim 20 , wherein the selecting step comprises matching the estimated singular values corresponding to at least one of the probabilistic distributions on parts with the actual singular values of matrix A. .The method of .claim 20 , wherein the step of estimating singular values corresponding to a probabilistic distributions on parts comprises the steps of : . generating characteristic coefficients ; . forming a special characteristic polynomial ; and . solving for the roots of the characteristic polynomial , wherein multiplying the roots by the number of samples related to corresponding probabilistic distributions on parts and then taking a square - root yields the estimated singular values .", "label": "", "metadata": {}, "score": "74.36742"}
{"text": "As an example , the corpus contains a huge sequel to Tolkien 's The Lord of the Rings written by a Russian author ( Nick Perumov ) .A related case , albeit not strictly in the same category as online speech production , comes from baby names .", "label": "", "metadata": {}, "score": "74.73294"}
{"text": "In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \" automotive \" label .But it then considers the effect of each feature .In this example , the input document contains the word \" dark , \" which is a weak indicator for murder mysteries , but it also contains the word \" football , \" which is a strong indicator for sports documents .", "label": "", "metadata": {}, "score": "74.73859"}
{"text": "For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are .", "label": "", "metadata": {}, "score": "74.858955"}
{"text": "Lee and Davis conducted a series of laboratory play sessions in English and Korean in which toys were introduced to mother - child and mother - experimenter dyads ( only the English data will be considered here ) .Sampling the 250 syllables of mother speech after the introduction of each of four toys ( 2010 : 773 ) , the experimenters analyzed various segmental frequency distributions and found significant differences on every dimension investigated ( p. 775 ) .", "label": "", "metadata": {}, "score": "74.87225"}
{"text": "The testing system was implemented on a Dell Axim X30 Pocket PC using Embedded Visual C++ ( EVC++ ) .The participants were instructed to hold the PDA with one hand , and press virtual \" keys \" using the index finger of their other ( dominant ) hand .", "label": "", "metadata": {}, "score": "74.913124"}
{"text": "WaCkypedia_EN : a 2009 dump of the English Wikipedia ( about 800 million tokens ) , in the same format as PukWaC , including POS / lemma information , as well as a full dependency parse ( parsing performed with the MaltParser ) .", "label": "", "metadata": {}, "score": "75.12336"}
{"text": "The topic of conversation varied within and across talkers , and generally concerned local events of interest , such as sports and politics .For each speaker , two or three segments of a few minutes ' duration each were chosen for transcription .", "label": "", "metadata": {}, "score": "75.14471"}
{"text": "Important Disclaimer : Answers and comments provided on Google Answers are general information , and are not intended to substitute for informed professional medical , psychiatric , psychological , tax , legal , investment , accounting , or other professional advice .", "label": "", "metadata": {}, "score": "75.29323"}
{"text": "I 'm afraid I ca n't help much more at the moment as I 'm nearing the deadline for my PhD thesis ... which ironically is based on streamlining , interfacing and standardising the stages used in Text Categorisation !", "label": "", "metadata": {}, "score": "75.65733"}
{"text": "For example , the next section discusses in more detail this article 's claim that the local fluctuations that children experience in segmental frequencies appear to be more or less the same as the ones that adults experience .How many varieties of a language are there ?", "label": "", "metadata": {}, "score": "75.68175"}
{"text": "Kuhl P. K. , , Andruski J. E. , , Chistovich I. A. , , Chistovich L. A. , , Kozhevnikova E. V. , , Ryskina V. , , Stolyarova E. I. , , Sundberg U .. & Lacerda F. , ( Year : 1997 ) .", "label": "", "metadata": {}, "score": "75.73566"}
{"text": "Polysemy is the phenomenon where the same word has multiple meanings .So a search may retrieve irrelevant documents containing the desired words in the wrong meaning .For example , a botanist and a computer scientist looking for the word \" tree \" probably desire different sets of documents .", "label": "", "metadata": {}, "score": "75.759445"}
{"text": "5 is a logical flow diagram depicting a method for recursively generating characteristic coefficients to estimate the singular values corresponding to a topic where the document lengths are uniform according to an exemplary embodiment of the invention .FIG .6 is a logical flow diagram depicting a method for recursively generating characteristic coefficients to estimate the singular values corresponding to a topic where the document lengths are non - uniform according to an exemplary embodiment of the invention .", "label": "", "metadata": {}, "score": "75.76169"}
{"text": "Addison Wesley , Cambridge , Mass. , 1949].A document frequency filter is used to omit words that occur too frequently to usefully discriminate the topics between pseudo - corpus windows .Some infrequent words may also be eliminated .", "label": "", "metadata": {}, "score": "75.985855"}
{"text": "With this fact in hand , it is time to turn to the other aspect of the null hypothesis : Are English segmental frequency distributions unitary ?In particular , is the segmental frequency distribution to which children are exposed different from the one adults hear ?", "label": "", "metadata": {}, "score": "76.090454"}
{"text": "Stirling numbers are sets of numbers well established in the field of combinatoric mathematics and known to one of ordinary skill on the art .M . ij .t .l .t .l .i .j .t .", "label": "", "metadata": {}, "score": "76.194664"}
{"text": "NIK ' 03 .Tapir Academic Publishers , 205 - 216 .[ 27 ] Shannon , C. E. 1951 Prediction and entropy of printed English .Bell System Technical Journal 30 .John Wiley & Sons , Hoboken , NJ , 50 - 64 .", "label": "", "metadata": {}, "score": "76.435165"}
{"text": "About 65 percent of the interactions documented included only a parent , the child , and the investigator .Since the investigator was there to record the child 's natural environment rather than interact with the child 's family , it can be inferred that most of the utterances spoken by the parent and investigator were directed toward the child , rather than toward each other .", "label": "", "metadata": {}, "score": "76.5995"}
{"text": "The answer likely lies in the nature of the samples .One factor that is not likely to explain the disparity in study conclusions is the raw amount of data .The present study analyzed 492 interactions with child listeners and 150 with adult listeners , with the child samples drawn from a wide variety of social situations , occurring primarily in a child 's home .", "label": "", "metadata": {}, "score": "76.60392"}
{"text": "Use of this method can allow the search engine server 710 to reduce the complexity of its indexing matrix A so that keyword search queries may be serviced more efficiently .Method 100 can be used by search engine server 710 to reduce the rank of the indexing matrix without loss of the ability to respond to keyword search queries on particular topics .", "label": "", "metadata": {}, "score": "77.127266"}
{"text": "True negatives are irrelevant items that we correctly identified as irrelevant .False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant .False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant .", "label": "", "metadata": {}, "score": "77.90317"}
{"text": "That is , they suggest that adult input and child input are two distinct varieties of English , because the statistical differences they found in their samples reflect differences between these two varieties as a whole .Lee and Davis ' ( 2010 ) study addresses questions of broad theoretical interest : ( a )", "label": "", "metadata": {}, "score": "78.226364"}
{"text": "As the topics of conversation are ever - changing , so are the words we use to discuss them , and the sounds they contain .We are all immersed in an ocean of variation , whose global trends may be measured in the aggregate , but whose action is often washed out by the evanescent ebbs and flows of ordinary conversation .", "label": "", "metadata": {}, "score": "78.43379"}
{"text": "Approximation algorithms for performing summarization are also proposed and empirical experiments are conducted to demonstrate the effectiveness of our proposed framework . byPeng Li , Jing Jiang , Shanghai Jiao - In : Proceedings of the 48th ACL . \" ...", "label": "", "metadata": {}, "score": "78.515625"}
{"text": "p .i .p .j . )l .i .j .M . ij .i .j .p .i . )l .p .j . )l .p .i .p .", "label": "", "metadata": {}, "score": "78.71367"}
{"text": "Description .RELATED APPLICATION .The complete disclosure of the above - identified priority application is hereby fully incorporated herein by reference .TECHNICAL FIELD .The present invention is generally directed to computer - based information retrieval systems .More particularly , the present invention relates to avoiding the loss of topical coverage and increasing information retrieval performance when using reduced rank models in computer - based information retrieval systems .", "label": "", "metadata": {}, "score": "78.81712"}
{"text": "i need your answer very urgent .Request for Answer Clarification by mcsemorgan - ga on 23 Aug 2003 12:41 PDT .SVM ADVANTAGES Reduced computational complexity due to discrete kernel function Faster training times Good classification accuracy , Fast to classify new instances DISSADVANTAGES There is currently no on - line learning version of the SVM .", "label": "", "metadata": {}, "score": "79.71552"}
{"text": "Hello again : I am sorry .Yes , I did a mistake .Instead of posting the results for SVM , I accidentely posted the results for this model .I have started doing research on SVM and will post my results anytime tonight or tommorow before afternoon .", "label": "", "metadata": {}, "score": "79.94159"}
{"text": "To appreciate the operation of the present invention , it is useful to review some of the mathematical theory behind the wavelet transform .The continuous wavelet transform of a function f(x ) is defined as # # EQU1 # # where \u03c8(x ) is the wavelet .", "label": "", "metadata": {}, "score": "80.0349"}
{"text": "Thirty - two students ( 22 male , 10 female ; mean age 26.1 yrs ) voluntarily participated in the experiment .Twenty - six were graduate students and the rest were undergraduates .As only three used dictionary - based predictive disambiguation text entry methods on a regular basis , participants as a whole were considered novices for the purpose of this study .", "label": "", "metadata": {}, "score": "80.41426"}
{"text": "In Proceedings of the ACM Conference on Human Factors in Computing Systems ( Portland , OR , April 2 - 7 , 2005 ) .CHI ' 05 .ACM Press , New York , NY , 211 - 220 .[", "label": "", "metadata": {}, "score": "81.0726"}
{"text": "Could you try to list them item by item ? ?Hello mcsemorgan - ga : Thanks for the clarification .I will start working on your clarification tommorow 28th July ( EST ) .I will keep you informed of the progress .", "label": "", "metadata": {}, "score": "81.08687"}
{"text": "Such a keyword search query is typical of Internet search engine requests .These keyword searches can be issued to the search engine server 710 through the Internet 720 .The searches can originate from user systems 760 that are connected to the Internet 720 by communication links 750 .", "label": "", "metadata": {}, "score": "81.49785"}
{"text": "Testing program using a 3-key constrained keypad design in predictive disambiguation mode .Table 2 Example Testing Phrases .you want to eat your cake .every apple from every tree .the plug does not fit the socket .", "label": "", "metadata": {}, "score": "81.56982"}
{"text": "Request for Answer Clarification by mcsemorgan - ga on 28 Jul 2003 08:59 PDT .I will pay you more , if the answer is nice ! !Could you also tell me how they compare in performance(I mean how can you proof which method in which condition would be best ; we can accord this result to apply each method ) ! !", "label": "", "metadata": {}, "score": "81.57828"}
{"text": "In Proceedings of the First International Symposium on Handheld and Ubiquitous Computing ( Karlsruhe , Germany , September 27 - 29 , 1999 ) .HUC ' 99 .Springer - Verlag , Berlin , 289 - 300 .[ 22 ] Phone Key Pads .", "label": "", "metadata": {}, "score": "82.05443"}
{"text": "Positive numbers indicate the manner is more frequent in CDS than in ADS ; negative numbers indicate the opposite .Subject : categorization Category : Computers Asked by : mcsemorgan - ga List Price : $ 100.00 .Posted : 05 Jul 2003 04:20 PDT Expires : 04 Aug 2003 04:20 PDT Question ID : 225316 .", "label": "", "metadata": {}, "score": "82.5113"}
{"text": "New York : ACM .Improved Word List Ordering for Text Entry on Ambiguous Keypads . 1 Google Inc. 1600 Amphitheatre Pkwy . 2 Sawyer Business School , Suffolk University 8 Ashburton Place Boston , MA , USA 02108 1 - 617 - 994 - 6841 tarase@suffolk.edu .", "label": "", "metadata": {}, "score": "82.89054"}
{"text": "the performance scores of other methods were .normalized using the score of kNN .This provides a common basis for a . global observation on methods whose results . are only available on individual collections .Widrow - Hoff , k - nearest .", "label": "", "metadata": {}, "score": "83.300865"}
{"text": "Thanks for your comments mcsemorgan - ga .I wish you a very good luck in the future .I will also like to thank ragingacadam - ga for pointing out to his answer .I hope it helps mcsemorgan - ga .", "label": "", "metadata": {}, "score": "83.4141"}
{"text": "Could you give me more answers that I wanted before tuesday ? ?I need it very urgent , otherwise I have to looking for other peopele .Hello mcsemorgan - ga : My research revealed some additional papers that you may like .", "label": "", "metadata": {}, "score": "84.609665"}
{"text": "Specifically , they show that some names ( e.g. John ) show a relatively stable frequency across all decades for which data is available , while other names ( e.g. Kayla ) undergo a rapid rise and an equally rapid fall in popularity .", "label": "", "metadata": {}, "score": "84.62246"}
{"text": "A search engine server 710 is connected to the Internet 720 over one or more communication links 715 .The communication link 715 also can include a signal path that is optical , fiber optic , wired , wireless , wire - line , waveguided , satellite - based , synchronous , asynchronous , or isochronous to name a few possibilities , for example .", "label": "", "metadata": {}, "score": "85.27804"}
{"text": "TheManWithNoName Mar 19 ' 13 at 20:56 .I appreciate your effort and thank you .I will have a look at the paper you included and try to make a use of it . - clancularius Mar 21 ' 13 at 8:28 . @TheManWithNoName : Great Answer !", "label": "", "metadata": {}, "score": "85.49361"}
{"text": "This article used the orthographic transcripts .It should be noted that the Buckeye corpus is considerably less heterogeneous than the CHILDES corpus ; in comparison to the remarkable diversity of social situations sampled in CHILDES , every document in the Buckeye consists of a talker speaking one - on - one with a researcher .", "label": "", "metadata": {}, "score": "85.86525"}
{"text": "t .c . t .c . t .c . t .t .In step 330 , the roots of the polynomial are computed .One of ordinary skill in the art will appreciate various methods for computing the roots of a polynomial .", "label": "", "metadata": {}, "score": "87.24034"}
{"text": "I think you have work hard on it , but they are not something that I needed .I know something that I wanted are difficult to search .Hope you can still working on it , and give me the answer as soon as possible .", "label": "", "metadata": {}, "score": "88.31691"}
{"text": "Title : Journal of child language Volume : - ISSN : 1469 - 7602 ISO Abbreviation : J Child Lang Publication Date : 2012 Oct .Date Detail : .Created Date : 2012 - 10 - 10 Completed Date : - Revised Date : - .", "label": "", "metadata": {}, "score": "88.647934"}
{"text": "claim 1 , wherein the step of estimating singular values corresponding to a topic comprises the steps of : . generating characteristic coefficients ; . forming a special characteristic polynomial based on the characteristic coefficients ; and . solving for the roots of the characteristic polynomial , wherein multiplying the roots by the number of documents related to the topic and then taking a square - root yields the estimated singular values .", "label": "", "metadata": {}, "score": "90.3486"}
{"text": "Challenge 3 , Pair 81 ( False ) .T :According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart .H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory .", "label": "", "metadata": {}, "score": "90.58974"}
{"text": "Request for Answer Clarification by mcsemorgan - ga on 27 Jul 2003 14:42 PDT .Thanks for your research , they are quite detailed .But could you try to answer my question ? ?Actually , I have read many references that you researched before .", "label": "", "metadata": {}, "score": "90.95687"}
{"text": "Left violins of each pair indicate adult - directed speech ; right violins indicate child - directed speech .See text for further details on interpretation .NOTE :Role code and roles columns indicate speaker roles ; document count column indicates how many documents had that exact combination of roles ; the cumulative percentage of all documents is given in the remaining column .", "label": "", "metadata": {}, "score": "92.08736"}
{"text": "ACM Press , New York , NY , 61 - 70 .", "label": "", "metadata": {}, "score": "96.83118"}
{"text": "Nlm Unique ID : 0425743 Medline TA : J Child Lang Country : - .Other Details : .Languages : ENG Pagination : 1 - 32 Citation Subset : - .Journal Information Journal ID ( nlm - ta ): J Child Lang Journal ID ( iso - abbrev ): J Child Lang Journal ID ( publisher - id ): JCL ISSN :", "label": "", "metadata": {}, "score": "98.71947"}
{"text": "t .l .t .l .t .l .t .t .t . k . ) t . k .S .l .t . k . ) t .l .i .j .", "label": "", "metadata": {}, "score": "100.538376"}
{"text": "T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism .", "label": "", "metadata": {}, "score": "104.23642"}
{"text": "Sincerely , leader - ga .Request for Answer Clarification by mcsemorgan - ga on 17 Aug 2003 10:52 PDT .Hope you can post the answer as soon as possible , because I need the answer very urgent .Request for Answer Clarification by mcsemorgan - ga on 20 Aug 2003 03:33 PDT .", "label": "", "metadata": {}, "score": "106.76978"}
