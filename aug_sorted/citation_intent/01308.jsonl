{"text": "Preferably , assigning probabilities to terminals of the context - free grammars includes normalizing the probabilities of the terminals from the N - gram language model in each of the context - free grammars as a function of the terminals in the corresponding context - free grammar .", "label": "", "metadata": {}, "score": "29.156357"}
{"text": "We describe an algorithm for determining maximum - likelihood estimates of the parameters of these models .The language models which we present differ from previous models based on stochastic context - free grammars in that they are highly lexical .In particular , they include the familiar n - gram models as a natural subclass .", "label": "", "metadata": {}, "score": "29.312948"}
{"text": "The uniform model does not capture the empirical word distribution underneath a context - free grammar non - terminal .A better alternative is to inherit existing domain - independent word tri - gram probabilities .These probabilities need to be appropriately normalized in the same probability space .", "label": "", "metadata": {}, "score": "31.200287"}
{"text": "While such a mapping may be easy to specify , the proof of its injectivity remains a problem .Recently , Dowell and Eddy have re - addressed this problem [ 1 ] in the framework of stochastic context free grammars ( SCFGs ) .", "label": "", "metadata": {}, "score": "32.779068"}
{"text": "A classical n - gram model is used to capture the local relations between words , while a stochastic grammatical model is considered to represent the long - term relations between syntactical structures .In order to dene this grammatical model , which will be used on large - vocabulary complex tasks , a category - based SCFG and a probabilistic model of word distribution in the categories have been proposed .", "label": "", "metadata": {}, "score": "34.21199"}
{"text": "For ( non - length - dependent ) SCFGs it is well known how this can be achieved [ 11 , 12 ] : .If we are training from full - parse trees the relative frequencies with which the rules occur ( among all rules with the same left - hand side ) are a maximum - likelihood estimator .", "label": "", "metadata": {}, "score": "35.248466"}
{"text": "Approximating them with easily describable functions seems possible , but would potentially be difficult to automate and would not be covered by the considerations in Section 3 .Thus we decided to use uniform distributions here .Combining the above considerations length - dependent stochastic context - free grammars can be defined : .", "label": "", "metadata": {}, "score": "35.357925"}
{"text": "( Note : there is an erratum on p. 592 , Fig . 7 .You can skip section 4 , as we 'll just discuss Eisner 's method for calculating expectations . )Lari and Young , 1990 .Applications of stochastic context - free grammars using the Inside - Outside algorithm .", "label": "", "metadata": {}, "score": "37.082024"}
{"text": "Notably , this probabilistic method is capable of producing accurate ( prediction ) results , where its worst - case time and space requirements are equal to those of common RNA folding algorithms for single sequences .As we will see , significant differences with respect to the overall quality of generated sample sets and the resulting predictive accuracy are typically implied .", "label": "", "metadata": {}, "score": "37.377922"}
{"text": "A common thermody - namic model for computing the free energy of a given RNA secondary structure , as well as stochastic context - free grammars and generating functions are used to derive the desired results .These results include asymptotics for the expected free energy and for the corresponding variance of a random RNA secondary structure .", "label": "", "metadata": {}, "score": "39.207314"}
{"text": "They come with well - documented algorithms for sequence alignment , structure prediction , parameterization by supervised learning from various kinds of training data , and calculation of posterior probabilities [ 20 ] .Discussion of these algorithms is facilitated by a well - developed and widely - understood probabilistic vocabulary .", "label": "", "metadata": {}, "score": "39.404015"}
{"text": "We would like to thank the anonymous referees of the previous revisions for their helpful suggestions .References .Nussinov , R. ; Pieczenik , G. ; R'Griggs , J. ; Kleitmann , D.J. Algorithms for loop matchings .SIAM J. Appl .", "label": "", "metadata": {}, "score": "39.735676"}
{"text": "And even if the most likely annotation sequence is returned by the Viterbi algorithm , its computed probability is too small when there are further derivations of this annotation sequence .As Dowell and Eddy have shown , this happens in practice and the effects are severe .", "label": "", "metadata": {}, "score": "39.794945"}
{"text": "These probability values collectively form the N - gram language model .Some aspects of the invention described below can be applied to building a standard statistical N - gram model .As is also well known in the art , a language model can also comprise a context - free grammar .", "label": "", "metadata": {}, "score": "40.1223"}
{"text": "In this case , the probability of a certain annotation sequence is split up into the probabilities of its multiple derivations .In Figure 1 , this is exemplified by the two derivations on the left that both represent the annotation sequence ( ( ( ( .... ) ) ) ) .", "label": "", "metadata": {}, "score": "40.54595"}
{"text": "At this point , it should be noted that the method illustrated in .FIG .7 is not limited to a unified language model , or even an N - gram language model , but rather , can be helpful in forming language models of any type used in a language processing system where the model is based on a task - independent corpus .", "label": "", "metadata": {}, "score": "40.87381"}
{"text": "But in order to expand those subsequent symbols , according to Paeseler 's algorithm , the best probability must be known , otherwise if a better probability appears , the parsing must be redone .This means potentially an exponential amount of work , which is highly undesirable .", "label": "", "metadata": {}, "score": "41.053253"}
{"text": "A N - gram model is built having the non - terminal tokens .A second plurality of context - free grammars is obtained for at least some of the same non - terminals representing the same semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "41.77866"}
{"text": "The same holds for the changes to the training algorithms explained at the beginning of the following section .Estimating the Rule Probabilities .When given a set of training data of either full - parse trees of the grammar or words from the language generated by it , we want to train the grammar according to the maximum likelihood principle , i.e. , choose rule probabilities such that .", "label": "", "metadata": {}, "score": "42.02793"}
{"text": "It is therefore desirable to constrain such algorithms , by pre - processing the sequences and using this first pass to limit the range of structures and/or alignments that can be considered .Results .We demonstrate how flexible classes of constraint can be imposed , greatly reducing the computational costs while maintaining a high quality of structural homology prediction .", "label": "", "metadata": {}, "score": "42.373466"}
{"text": "An N - gram model is then built at step 168 using an N - gram algorithm , the N - gram model having the non - terminal tokens embedded therein .At step 170 , a second plurality of context - free grammars is obtained suitable for the selected application .", "label": "", "metadata": {}, "score": "42.781296"}
{"text": "For application developers , a CFG is also often highly labor - intensive to create .A second form of a language model is an N - gram model .Because the N - gram can be trained with a large amount of data , the n - word dependency can often accommodate both syntactic and semantic shallow structure seamlessly .", "label": "", "metadata": {}, "score": "42.832764"}
{"text": "To counter this the authors of [ 13 ] translated the concept of transition and emission probabilities from hidden Markov models to SCFGs .The emission probability then is the probability that a given b is a placeholder , e.g. u independent of the rule which introduced the b . G1 and G2 have been taken from [ 13 ] , G1 being the simplest grammar in the comparison and G2 , which originates from [ 3 ] , being the grammar which achieved the best results .", "label": "", "metadata": {}, "score": "42.91746"}
{"text": "As above , choose some appropriate single - sequence SCFG / pair HMM that models RNA folding / primary sequence alignment .Compute the maximum traceback path - likelihood from all subsequences / cutpoints using the CYK - KYC / Viterbi - forward - backward algorithm for this grammar .", "label": "", "metadata": {}, "score": "43.085396"}
{"text": "Thus for this grammar each parse tree will have \" probability \" 1 and each word will have a \" probability \" equal to its degree of ambiguity .Both cases can be countered by adding the proper probability distributions to the model .", "label": "", "metadata": {}, "score": "43.22603"}
{"text": "Using the first two scoring schemes , we obtain the Viterbi and the Inside algorithm .Using the other two , we obtain an algorithm for counting the number of derivations for the input string , and an algorithm for base pair maximization .", "label": "", "metadata": {}, "score": "43.23077"}
{"text": "In particular , they include the familiar n - gram models as a natural subclass .The motivation for considering this class is to estimate the contribution which grammar can make to reducing the relative entropy of natural language . \"Abstract : \" In this paper we present a new class of language models .", "label": "", "metadata": {}, "score": "43.42171"}
{"text": "Since each of these factors is described by an unrestricted probability distribution on its defining domain , Theorem 1 from [ 11 ] applies , stating that relative frequencies are a maximum likelihood estimator for their probabilities .Theorem 4 .Then the inside - outside algorithm will converge to a set of rule probabilities that ( locally or globally ) maximises the likelihood of observing .", "label": "", "metadata": {}, "score": "43.546623"}
{"text": "For simplicity , we chose probabilities independent of certain bases .In SCFG design , often also non - canonical base pairings are allowed with a low probability .For grammar G 1 , the derivations shown in Figure 1 have probabilities of 5.24\u00b710 -14 , 2.1\u00b710 -15 , 4.19\u00b710 -16 and 4.19\u00b710 -16 ( from left to right ) .", "label": "", "metadata": {}, "score": "43.58722"}
{"text": "Sampling structures from sample sequences .Dowell and Eddy suggested a testing procedure that relies on a comparison of the results from the Viterbi and the Inside algorithms , where the latter is conditioned on the most likely annotation sequence s returned by the Viterbi run .", "label": "", "metadata": {}, "score": "43.81475"}
{"text": "This algorithm roughly works as follows : .Summing up these probabilities for all subwords it gets expected numbers of rule occurrences from which expected relative frequencies can be computed as in the case of given parse trees .By using these expected relative frequencies as a new estimate for the rule probabilities and iterating the procedure we are guaranteed to converge to a set of rule probabilities that gives a ( local ) maximum likelihood .", "label": "", "metadata": {}, "score": "43.817513"}
{"text": "Another goal of the present invention is to provide a chart parser that correctly computes hypothesis probabilities in an efficient manner for CFGs and stochastic unification grammars , by interleaving the search for explanations of symbols from both the top down and from the bottom up .", "label": "", "metadata": {}, "score": "43.85558"}
{"text": "Therefore , probabilities of the terminals from the N - gram language model need to be appropriately normalized in the same probability space as the terminals present in the corresponding context - free grammar .i in W. The likelihood of W under the segmentation T is therefore .", "label": "", "metadata": {}, "score": "43.865322"}
{"text": "This can be a source of error , since misaligned bases can add noise that swamps the covariation signal .The most recent of these methods allows for some uncertainty in the alignment [ 7 ] .More generally , one can view the alignment and structure prediction as a combined problem , to be solved simultaneously .", "label": "", "metadata": {}, "score": "43.934223"}
{"text": "Because of this efficiency , the algorithm applies to multiple grammar levels of a spoken language system .In accordance with the invention , a method is disclosed for combining unification grammars with rule and observation probabilities to enable a processor to recognize a speech signal input for several levels of a language model .", "label": "", "metadata": {}, "score": "44.00859"}
{"text": "In a first aspect , a task dependent unified language model for a selected application is created from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .", "label": "", "metadata": {}, "score": "44.311615"}
{"text": "The concepts and methods presented in Sections 2 - 4 can immediately be applied to any other application where SCFGs are used as a model , e.g. , natural language processing .From our limited insight in that field it appears possible that length - dependent grammars can successfully be applied there as well .", "label": "", "metadata": {}, "score": "44.503265"}
{"text": "Dowell and Eddy suggest to run the test also for a sample of suboptimal annotation sequences for x .Since the minimizing Viterbi run gives us the least probable derivation tree , we may have a higher chance to find an ambiguous one ( if present ) than in the maximizing run .", "label": "", "metadata": {}, "score": "44.753567"}
{"text": "For example , a bi - gram ( or 2-gram ) language model considers the previous word as having an influence on the next word .where w is a word of interest : .Also , the probability of a word sequence is determined based on the multiplication of the probability of each word given its history .", "label": "", "metadata": {}, "score": "44.83287"}
{"text": "Note 1 .For probabilistic Earley parsing ( [ 10 ] ) we have to regard that the length of the generated subword will only be known in the completion step .Thus for LSCFGs we have to multiply in the rule probability ( and the factor p \u03b1 , l ) in this step instead of the prediction step , as is usually done .", "label": "", "metadata": {}, "score": "44.921257"}
{"text": "This is prohibitively expensive at the time of writing , except for fairly short sequences , which has motivated the development of various constrained versions of these algorithms [ 9 - 11 , 13 , 21 ] .The purpose of this paper is to report our progress on general pairwise constrained versions of Sankoff 's algorithm ( or , more precisely , constrained versions of some related dynamic programming algorithms for SCFGs ) .", "label": "", "metadata": {}, "score": "44.941963"}
{"text": "A . i . )P .n .A . i .Theorem 3 .Then P is a maximum - likelihood estimator for the length - dependent rule - probabilities on G , R and Q .Proof .P .", "label": "", "metadata": {}, "score": "44.999687"}
{"text": "Determining the Most Probable Derivation .In order to find the most probable derivation for a given primary structure we decided to employ a probabilistic Earley parser , since it allows to use the grammars unmodified while the commonly used CYK algorithm requires the grammars to be transformed into Chomsky normal form .", "label": "", "metadata": {}, "score": "45.364433"}
{"text": "Stolcke , A. An Efficient Probabilistic Context - Free Parsing Algorithm that Computes Prefix Probabilities .Comput .Linguist .[ Google Scholar ] .Prescher , D. A Tutorial on the Expectation - Maximization Algorithm Including Maximum - Likelihood Estimation and EM Training of Probabilistic Context - Free Grammars .", "label": "", "metadata": {}, "score": "45.41675"}
{"text": "A heuristic algorithm for performing multiple alignment - and - folding of RNA sequences with a pairwise SCFG by progressive single - linkage clustering runs as follows .Start by making pairwise alignments ( with predicted secondary structures ) for all pairs of input sequences .", "label": "", "metadata": {}, "score": "45.682083"}
{"text": "The technique described in the following comprises two steps .First , we remove the syntactic ambiguity of the grammar and reduce a possibly existent semantic ambiguity to fl - ambiguity .Then we use a parser generator to check the transformed grammar for fl - ambiguity .", "label": "", "metadata": {}, "score": "45.706047"}
{"text": "An ( ambiguous ) stochastic context - free grammar ( SCFG ) that generates the primary structures is chosen such that the derivation trees for a given primary structure uniquely correspond to the possible secondary structures .The probabilities of this grammar are then trained either from molecules with known secondary structures or by expectation maximization ( [ 3 ] ) .", "label": "", "metadata": {}, "score": "46.031586"}
{"text": "This technique narrows the task - independent corpus , but can identify yet more examples of task specific sentences , phrases , etc . .FIG .7 illustrates a method 200 for creating a language model for a selected application from a task - independent corpus in the manner discussed above .", "label": "", "metadata": {}, "score": "46.195686"}
{"text": "Select all subsequences / cutpoints with posterior probability above some threshold ( or select e.g. the top 10 percentile of the posterior probabability distribution ) .Ensure that each of these subsequences / cutpoints is on a valid traceback path , e.g. by running the CYK - KYC / Viterbi - forward - backward algorithm to find the maximum - likelihood traceback path from any given subsequence/ cutpoint .", "label": "", "metadata": {}, "score": "46.262108"}
{"text": "Observations and Dicussion .We did the training and prediction using a length - dependent version of the Earley - style - parser from [ 16 ] .The results of the benchmarks are listed in Table 1 .Looking at these results it is immediately apparent that the predictions from G1 are too bad to be of any use either without or with lengths .", "label": "", "metadata": {}, "score": "46.42777"}
{"text": "( b ) Complete .In implementing the above algorithm , the processor reads a vector of grammars representing any number of grammar levels .Looking for the moment at FIG .2 , an example of grammar levels is shown .", "label": "", "metadata": {}, "score": "46.55919"}
{"text": "This iterative process can be repeated as necessary until the errors are corrected and a suitable N - gram model has been obtained .As discussed above , the task - independent corpus is a general corpus and in fact it is likely that most of the corpus is unrelated to the task or application that the developer is interested in .", "label": "", "metadata": {}, "score": "46.826454"}
{"text": "Score calculation includes the detection of these cycles and propagation of the best scores to the next frame .The algorithm creates no more states than a nonprobilistic chart parser , and remains linear for regular grammars and cubic in the worst case for CFGs .", "label": "", "metadata": {}, "score": "47.208885"}
{"text": "Combinatorial Pattern Matching : 11th Annual Symposium ( Edited by : Giancarlo R , Sankoff D ) .Springer - Verlag Heidelberg 2000 , 1848 : 46 - 59 .View Article .Dowell RD , Eddy SR : Evaluation of several lightweight stochastic context - free grammars for RNA secondary structure prediction .", "label": "", "metadata": {}, "score": "47.35977"}
{"text": "In most cases , stochastic grammars are used for the latter alternative applying the maximum likelihood principle for determining a grammar 's probabilities .In this paper , building on such a stochastic model , we will analyze the expected minimum free energy of an RNA molecule according to Turner 's energy rules .", "label": "", "metadata": {}, "score": "47.40734"}
{"text": "Stochastic context free grammars .Stochastic context free grammars associate a ( nonzero ) probability with each production , such that the probabilities for all alternative productions emerging from the same nonterminal symbol add up to 1 .As a string is derived , probabilities of the involved rules multiply .", "label": "", "metadata": {}, "score": "47.486275"}
{"text": "For example , the non - ambiguous Wuchty algorithm ( RNAsubopt , [ 2 ] ) requires four tables for storing intermediate results , while the ambiguous Zuker - Stiegler recurrences ( Mfold , [ 7 ] ) require only two .", "label": "", "metadata": {}, "score": "47.61384"}
{"text": "These models provide good recognition results when perplexity can be minimized , but preclude any direct support for spoken language systems by eliminating the semantic level .Language models have traditionally proven valuable in natural language systems , but only during the past decade have computationally - oriented , declarative grammar formalisms become widely available .", "label": "", "metadata": {}, "score": "47.81462"}
{"text": "Crucially , SCFGs are sufficiently general to express virtually all of the features offered by other scoring schemes [ 19 ] .We also acknowledge the appeal of free energy - based scoring schemes , which have the advantage that the parameters can be determined experimentally .", "label": "", "metadata": {}, "score": "47.912605"}
{"text": "The probability of parsing this input is the product of all the rule probabilitiesthat are given in portion C of FIG .4 .The trace seen in Portion D of FIG .4 shows the behaviour of the algorithm with these rule probabilities with respect to the input .", "label": "", "metadata": {}, "score": "47.936565"}
{"text": "However , due to the respective problems associated with both methods , new statistics based approaches towards RNA structure prediction have become increasingly appreciated .For instance , over the last years , several statistical sampling methods and clustering techniques have been invented that are based on the computation of partition functions ( PFs ) and base pair probabilities according to thermodynamic models .", "label": "", "metadata": {}, "score": "48.258827"}
{"text": "Each of the identified word occurrences is replaced with corresponding non - terminal tokens .A N - gram model is then built having the non - terminal tokens .A third aspect is a method for creating a language model for a selected application from a task - independent corpus .", "label": "", "metadata": {}, "score": "48.425743"}
{"text": "Write a second program that reads the data file , inputs text strings one per line , and produces as output the unigram , bigram , or trigram model probability for the string .Feel free to exploit Pedersen 's N - gram Statistics Package .", "label": "", "metadata": {}, "score": "48.56475"}
{"text": "The latter two strategies have been implemented in the stemloc package described below .Empirically , the stochastic strategy appears to be less reliable than the deterministic strategies ( although in theory the stochastic strategy will eventually find the globally optimal alignment given sufficiently many random repetitions , which may be a useful property ) .", "label": "", "metadata": {}, "score": "48.7956"}
{"text": "The multiple alignments produced by this algorithm lack well - defined probabilistic scores unless the pair SCFG is conditionally normalized .It is also straightforward to retrieve the N best non - overlapping alignments by repeatedly applying an incremental Waterman - Eggert - style mask to the alignment envelope [ 24 ] .", "label": "", "metadata": {}, "score": "48.802166"}
{"text": "Finally , experiments using the Penn Treebank corpus improved by 30 % the test set perplexity with regard to the classical n - gram models .One of the fundamental problems in computational structural biology is the prediction of RNA secondary structures from a single sequence .", "label": "", "metadata": {}, "score": "48.817528"}
{"text": "7 wherein the same reference numerals have been used to identify similar steps .However , method 220 can be used to create an N - gram language model having the non - terminal tokens of the context - free grammars .", "label": "", "metadata": {}, "score": "49.021217"}
{"text": "As this problem inherits undecidability ( as we show here ) from the namely problem for context free languages , there is no complete algorithmic solution to the problem of ambiguity checking .Results .We explain frequently observed sources of ambiguity , and show how to avoid them .", "label": "", "metadata": {}, "score": "49.08178"}
{"text": "In the present paper we will however confine ourselves with grouping the lengths together in finitely many intervals , a rule having the same probability for lengths that are in the same interval .This allows for the probabilities to be stored as a vector and be retrieved in the algorithms without further computation .", "label": "", "metadata": {}, "score": "49.182457"}
{"text": "Previous constrained versions of Sankoff - like algorithms , such as the programs DYNALIGN [ 11 ] and FOLDALIGN [ 10 ] , have been restricted to \" banding \" the algorithm e.g. by constraining the maximum insertion / deletion distance between the two sequences or the maximum separation between paired bases .", "label": "", "metadata": {}, "score": "49.274765"}
{"text": "Before considering the algorithm of the present invention , two definitions are needed .First , one defines a stochastic unification grammar which is based on the definition of stochastic context - free grammar and is described by the generalization that the symbols are not restricted to atomic symbols but can be feature - value pairs or feature sets .", "label": "", "metadata": {}, "score": "49.305305"}
{"text": "While this result rules out an automated proof procedure for arbitrary grammars used in SCFG modeling , there might still be the possibility to design such a procedure for a restricted class of grammars , say all grammars which describe RNA secondary structures .", "label": "", "metadata": {}, "score": "49.33309"}
{"text": "A powerful , general dynamic programming algorithm for simultaneously aligning and predicting the structure of multiple RNA sequences was developed by David Sankoff [ 14 ] .The energy - based folding of Zuker et al [ 15 ] and recent approaches based on Stochastic Context - Free Grammars ( SCFGs ) [ 9 , 13 , 16 - 20 ] are both closely related to Sankoff 's algorithm .", "label": "", "metadata": {}, "score": "49.374184"}
{"text": "Performing this test on a large number of inputs x should give a good hint whether ambiguity is present .Of course , enumerating the annotation sequences for all possible derivation trees creates voluminous output , and the automated check for duplicates requires some careful programming .", "label": "", "metadata": {}, "score": "49.409336"}
{"text": "It has the property that any combined alignment and structure prediction for two RNA sequences has a single , unambiguous parse tree .In our investigations , this unambiguity appeared to improve the accuracy of alignment and structure prediction substantially ; see also writings on this topic by Giegerich [ 34 ] and Dowell , Eddy et al [ 35 ] .", "label": "", "metadata": {}, "score": "49.744705"}
{"text": "This allows us to define a thermodynamic matcher , which uses the minimum free energy as a scoring scheme and focuses only on a specific realm of secondary structures .Here , for every new RNA family , a new grammar must be devised .", "label": "", "metadata": {}, "score": "49.819706"}
{"text": "The unified language model has the potential of overcoming the weaknesses of both the word N - gram & CFG language models .However , there is no clear way to leverage domain - independent training corpus or domain - independent language models , including the unified language models , for domain specific applications .", "label": "", "metadata": {}, "score": "49.905342"}
{"text": "First and second N - gram language models are built from the word phrases and the task - independent corpus , respectively .The first N - gram language model and the second N - gram language model are combined to form a third N - gram language model .", "label": "", "metadata": {}, "score": "49.958202"}
{"text": "In fact , the stochastic model for RNA secondary structures presented in this work has , for example , been used as the basis of a new algorithm for the ( nonuniform ) generation of random RNA secondary structures .INDEX TERMS .", "label": "", "metadata": {}, "score": "50.027718"}
{"text": "Inside each context - free grammar , the standard probabilistic context - free grammar can be used .However , without real data pertaining to the specific task or application , an estimate for each of the terminal probabilities can not be easily determined .", "label": "", "metadata": {}, "score": "50.302044"}
{"text": "Comput .Linguist .[ Google Scholar ] .Dowell , R.D. ; Eddy , S.R. Evaluation of several lightweight stochastic context - free grammars for RNA secondary structure prediction .BMC Bioinforma .[ Google Scholar ] .Nebel , M.E. Identifying Good Predictions of RNA Secondary Structure .", "label": "", "metadata": {}, "score": "50.305534"}
{"text": "Efficient implementation of the algorithm depends on use of some details set out in Earley 's work .Furthermore , inherent throughout the program is a compute function .When finding probabilities for terminal symbols , it is up to the compute step to note cycles and efficiently find the probability of a given state .", "label": "", "metadata": {}, "score": "50.4428"}
{"text": "More flexibly , we can limit the recursion to a single alignment , a single structure , or a broadly - specified set of alignments or structures ( Figure 5 ) .Applications such as alignment of two known structures [ 13 , 32 ] , alignment of an unstructured sequence to a known structure [ 33 ] or structure prediction from a known alignment [ 9 ] all reduce to simple application of the appropriate constraints .", "label": "", "metadata": {}, "score": "50.75815"}
{"text": "For example , we have only tested the pairwise alignment functionality ; full evaluation / optimisation of the multiple alignment algorithm remains .Rather than using the CYK algorithm , one could use the Inside - Outside algorithm with a decision - theoretic dynamic programming step to maximize expected performance [ 38 , 39 ] .", "label": "", "metadata": {}, "score": "50.829315"}
{"text": "We introduce , for the special case of stochastic context free grammars and RNA structure modeling , an automated partial procedure for proving non - ambiguity .It is used to demonstrate non - ambiguity for several relevant grammars .Conclusion .", "label": "", "metadata": {}, "score": "50.925987"}
{"text": "Formal Definitions .We assume the reader is familiar with basic terms of context - free grammars .An introduction can be found in ( [ 8 ] ) .Definition 1 .Words are generated as for usual context - free grammars , the product of the probabilities of the production rules used in a parse tree \u0394 provides its probability P ( \u0394 ) .", "label": "", "metadata": {}, "score": "50.931343"}
{"text": "Additionally , we wish to be able to parameterize the model automatically from training data .Without constraints , the above tasks are addressed by the resource - intensive CYK and Inside - Outside algorithms ; here , we present constrained versions of these algorithms that work in reduced space and time ( the exact complexity depends nontrivially on the constraints ) .", "label": "", "metadata": {}, "score": "51.03164"}
{"text": "This design decision involves a close trade - off between CPU and memory usage .Initially , we tested various combinations of generic containers with O ( N ) -storage and O ( N log N ) access - times , such as balanced search trees [ 22 ] .", "label": "", "metadata": {}, "score": "51.036606"}
{"text": "In other cases , ambiguity can cause a DP algorithm to return an \" optimal \" answer which is plainly wrong .Previous work .The phenomenon of ambiguity has been formalized and studied in [ 3 ] in a quite general framework of dynamic programming over sequence data .", "label": "", "metadata": {}, "score": "51.282955"}
{"text": "Furthermore , we show that the extended model is suited to improve the quality of predictions of RNA secondary structures .The extended model may also be applied to other fields where stochastic context - free grammars are used like natural language processing .", "label": "", "metadata": {}, "score": "51.289375"}
{"text": "Further possible constraints .The constraints given here allow the independent imposition of alignment or fold constraints .One can imagine further , even more general constraints .This constraint is employed by the FOLDALIGN program .It is not expressible as a combination of independent alignment and fold constraints , and has not been implemented for the present work , though it would be relatively straightforward to combine it with the other constraints described here [ 10 ] .", "label": "", "metadata": {}, "score": "51.315033"}
{"text": "A language model provides a method or means of specifying which sequences of words in the vocabulary are possible , or in general provides information about the likelihood of various word sequences .Speech recognition is often considered to be a form of top - down language processing .", "label": "", "metadata": {}, "score": "51.329338"}
{"text": "wm ) is represented as follows : .The .P .w . wm . )i . m .P .w .i .H .i . )N - gram model is obtained by applying an N - gram algorithm to a corpus ( a collection of phrases , sentences , sentence fragments , paragraphs , etc ) of textual training data .", "label": "", "metadata": {}, "score": "51.39679"}
{"text": "These algorithms are available at the accompanying website [ 8 ] , where readers are welcome to practice their insight on ambiguity matters .In the following , we write G ( \u03c3 , x ) for running the CYK parser based on grammar G with scoring scheme \u03c3 on input x .", "label": "", "metadata": {}, "score": "51.524612"}
{"text": "As we stated in Section 2 we implemented length - dependency such that we grouped the lengths into intervals , the rule probabilities changing only from one interval to the other but not within them .Since the influence a change in length has on the probabilities most likely depends on the relative change rather than the absolute one , we decided to make the intervals longer as the subwords considered get longer .", "label": "", "metadata": {}, "score": "51.57319"}
{"text": "If desired , the second language model can be weighted based on whether the identified text is believed to be accurate .The weighting can be based on the amount of text identified in the task - independent corpus , the number of queries used , etc . .", "label": "", "metadata": {}, "score": "51.643494"}
{"text": "Thus the most probable secondary structure ( derivation tree ) is computed as prediction ( [ 3 ] ) .Many other approaches as well as extensions and modifications of the ones mentioned above have been suggested over the years .A recent overview can be found in [ 4 ] .", "label": "", "metadata": {}, "score": "51.75572"}
{"text": "We will present this extension formally in Section 2 .In Sections 3 and 4 we show that existing training algorithms can easily be adapted to the new model without significant losses in performance .We have compared the prediction quality of the modified model with the conventional one for different grammars and sets of RNA .", "label": "", "metadata": {}, "score": "51.957993"}
{"text": "It is not mathematically deep , but rather a tedious exercise , and the likelihood to produce errors or oversights is high .By showing that a one - to - one mapping between parse trees of G and R exists , it is possible to prove the non - ambiguity of G .", "label": "", "metadata": {}, "score": "52.016827"}
{"text": "By different interpretations of the operations H , o and P , different scoring schemes can be plugged in .The recurrences may also be \" conditioned \" by annotating the symbol sequence x with a given annotation sequence s [ 1 ] .", "label": "", "metadata": {}, "score": "52.06791"}
{"text": "Both frequencies were computed over the complete set ( instead of calculating individual scores for each molecule and taking the average of these ) .Data .In [ 13 ] Dowell and Eddy compared the prediction quality of several different grammars as well as some commonly used programs that predict RNA secondary structures by minimizing free energy .", "label": "", "metadata": {}, "score": "52.128998"}
{"text": "It is also convenient to introduce some notation for ungapped sequences at this stage .Table 1 .A stochastic context - free grammar for generating pairwise alignments of RNA structures .The parse tree and the sequence likelihood .The grammar is a probabilistic model for deriving sequences X , Y from a single nonterminal .", "label": "", "metadata": {}, "score": "52.32224"}
{"text": "6 illustrates a method 180 for creating a unified language model for a selected application from a task - independent corpus that includes a large number of phrases that may be of different context .Simple parsing of the task - independent corpus with context - free grammars for the task - dependent application may cause errors , which will then propagate to the N - gram model upon application of an N - gram algorithm .", "label": "", "metadata": {}, "score": "52.431602"}
{"text": "PubMed .Lari K , Young SJ : The Estimation of Stochastic Context - Free Grammars Using the Inside - Outside Algorithm .Computer Speech and Language 1990 , 4 : 35 - 56 .View Article .McCaskill JS : The Equilibrium Partition Function and Base Pair Binding Probabilities for RNA Secondary Structure .", "label": "", "metadata": {}, "score": "52.438408"}
{"text": "The parser generator MSTA used as a partial proof method is available at [ 11 ] .Appendix : Ambiguity in DP is undecidable .Dynamic programming is a very general programming technique , and its scope is not precisely circumscribed .", "label": "", "metadata": {}, "score": "52.458763"}
{"text": "In this grammar we can derive from each symbol A i exactly those words that can be derived from A in the original grammar and have length i .This reflects the distinction between choosing the symbols on the right - hand side and distributing the length amongst them introduced by our model .", "label": "", "metadata": {}, "score": "52.47843"}
{"text": "The tested equation therefore holds if and only if the annotation sequence s has exactly one derivation tree .If there are more than one , the Inside algorithm will return a higher probability than the Viterbi run , which indicates ambiguity of s ( and hence G ) .", "label": "", "metadata": {}, "score": "52.512764"}
{"text": "The following set of intervals yielded the best or close to the best results for all 4 grammars : .Since all structures in the benchmark sets are shorter than 500 bases the probabilities of the last interval did not influence the predictions .", "label": "", "metadata": {}, "score": "52.545258"}
{"text": "It should be noted , however , that the frame counter may be incremented at other times during the execution of the algorithm depending on how one wants to keep track of the frames .At this point , either the states are complete or are reported as observations to the next higher grammar level .", "label": "", "metadata": {}, "score": "52.654102"}
{"text": "Simply speaking , a DP problem is given by a grammar G and a scoring scheme \u03c3 ( not necessarily stochastic ) , as was exemplified in Section Testing for ambiguity .Theorem 1 Semantic ambiguity in dynamic programming is formally undecidable .", "label": "", "metadata": {}, "score": "52.804653"}
{"text": "Methods .We begin our description of the envelope method with an explanatory note regarding our decision to present these constraints in terms of SCFGs , rather than other scoring schemes such as those based solely on energies [ 15 ] or on energy / information - theoretic hybrids [ 11 ] .", "label": "", "metadata": {}, "score": "52.994736"}
{"text": "The algorithms described here can reproduce nearly all such banding constraints and , further , can take advantage of more flexible sequence - tailored constraints .Specifically , the fold envelopes determine the subsequences of X and Y that can be considered by the algorithm , while the alignment envelope determines the permissible cutpoints in the pairwise alignment of X and Y .", "label": "", "metadata": {}, "score": "52.99498"}
{"text": "These results indicate that LSCFGs are indeed capable of giving better predictions than classic SCFGs .However further experiments will be needed to confirm these initial results on other data sets and determine good choices for grammar and length intervals .Possible Other Applications .", "label": "", "metadata": {}, "score": "53.22623"}
{"text": "5112 - 5120 , 1994 .[ 30 ] N. Chomsky and M.P. Sch\u00fctzenberger , \" The Algebraic Theory of Context - Free Languages , \" Computer Programming and Formal Systems , P. Braffort and D. Hirschberg , eds . , pp .", "label": "", "metadata": {}, "score": "53.24132"}
{"text": "Define language model uses linear interpolation to combine evidence from models at the word level and the character level .( The previous assignments provide you with implementations of these component models .Clever , huh ? )Train the coefficients of the interpolated model using EM .", "label": "", "metadata": {}, "score": "53.369267"}
{"text": "As mentioned in the Background section , the application developer should be provided with an efficient method in which an appropriate language model 16 can be created for the selected application .In some applications , a standard N - gram language model will work and any improvements in developing such a model will be valuable .", "label": "", "metadata": {}, "score": "53.369953"}
{"text": "View Article PubMed .Holmes I , Rubin GM : Pairwise RNA structure comparison using stochastic context - free grammars .Pac Symp Biocomput 2002 , 163 - 174 .Sankoff D : Simultaneous solution of the RNA folding , alignment , and protosequence problems .", "label": "", "metadata": {}, "score": "53.45289"}
{"text": "In addition to the applications , extending the concept of context - free grammars also gives rise to interesting questions in the field of formal language theory .The most obvious of these questions is if adding in length - dependencies changes the class of languages that can be generated .", "label": "", "metadata": {}, "score": "53.554527"}
{"text": "It is now possible to combine independent structural and alignment constraints of unprecedented general flexibility in Pair SCFG alignment algorithms .We outline several applications to the bioinformatics of RNA sequence and structure , including Waterman - Eggert N - best alignments and progressive multiple alignment .", "label": "", "metadata": {}, "score": "53.561203"}
{"text": "On these three sets we again did the training and prediction for the grammars G2 and G3 , them being the most likely candidates for future use .The results are listed in Table 2 .For each of the sets G3 with lengths performed best , backing our assumption .", "label": "", "metadata": {}, "score": "53.69152"}
{"text": "( For some reason this , and not the original Baker paper , is the standard reference .The derivation of the update equations is not as simple as it could be , I think . )Chiang , 2003 , Mildly context sensitive grammars for estimating maximum entropy models .", "label": "", "metadata": {}, "score": "53.771027"}
{"text": "View Article PubMed .Giegerich R : Explaining and Controlling Ambiguity in Dynamic Programming .Proc .Combinatorial Pattern Matching , Springer LNCS 1848 2000 , 46 - 59 .Chomsky N : Three Models for the Description of Language .IRE Transactions on information theory 1956 , 2 : 113 - 124 .", "label": "", "metadata": {}, "score": "53.97438"}
{"text": "It saves time and memory space by expanding symbols only once .The present invention extends Earley 's basic CFG parsing algorithm to combine rule and observation probabilities with the use of unification grammars .This retains the Earley algorithm complexity while extending the results to spoken input recognition : linear for regular grammars , quadratic for unambiguous CFGs , and cubic for general CFGs .", "label": "", "metadata": {}, "score": "53.997173"}
{"text": "Instead , they suggest a testing approach to check for the presence of ambiguity , which , of course , can not prove its absence .Formalization of ambiguity .We formalize the problem at hand in two steps , going from context free grammars ( CFGs ) to stochastic context free grammars , and then differentiating between syntactic and semantic ambiguity .", "label": "", "metadata": {}, "score": "54.11577"}
{"text": "Abstract : \" In this paper we present a new class of language models .This class derives from link grammar , a context - free formalism for the description of natural language .We describe an algorithm for determining maximum - likelihood estimates of the parameters of these models .", "label": "", "metadata": {}, "score": "54.165077"}
{"text": "Similarity between the query and segments of the task - independent corpus can be computed using cosine similarity measure .These are generally well - known techniques in the field of information retrieval .Alternatively , the query can include Boolean logic ( \" and \" , \" or \" , etc . ) as may be desired to combine word phrases .", "label": "", "metadata": {}, "score": "54.32052"}
{"text": "Suppose that A is the alignment according to RFAM , and B is the alignment predicted by stemloc .Suppose that S is the published structure , and T is the structure predicted by stemloc .These performance indicators are averaged over all 22 pairwise alignments and plotted for the three test regimes in Figure 8 ( alignment sensitivity ) , Figure 9 ( alignment specificity ) , Figure 10 ( basepair sensitivity ) and Figure 11 ( basepair specificity ) .", "label": "", "metadata": {}, "score": "54.330177"}
{"text": "This continues until all of the input has advanced up to sentence grammar level 0 and advanced S over several time frames to cover all of the input data .At this point , the processor has completed its parse and outputs its hypothesis of the spoken input .", "label": "", "metadata": {}, "score": "54.541298"}
{"text": "The resource usages of the test regimes are plotted in Figure 12 ( user - mode running time ) and Figure 13 ( memory usage ) .The resource usage of the constrained algorithms is substantially reduced when the envelopes are smaller ( i.e. at lower N ) .", "label": "", "metadata": {}, "score": "54.561047"}
{"text": "The exponential growth factors of the resulting asymptotics are compared to the corresponding experimentally obtained value as given by Giegerich et al . .This article focuses on the analytical analysis of the free energy in a realistic model for RNA secondary structures .", "label": "", "metadata": {}, "score": "54.721077"}
{"text": "Since the generated parser can only read a limited number of input characters ahead ( k ) , the parser generator is not able to construct a deterministic parser for this situation and reports a conflict .However , we can circumvent this problem by extending the alphabet of the annotation sequence by an additional character ( say , ' :') for unpaired bases in left bulges 1 : .", "label": "", "metadata": {}, "score": "54.94854"}
{"text": "Also , the search engine 114 accesses the language model 16 .The language model 16 is a unified language model or a word N - gram or a context - free grammar that is used in identifying the most likely word represented by the input speech .", "label": "", "metadata": {}, "score": "54.96136"}
{"text": "View Article PubMed .Sakakibara Y , Brown M , Hughey R , Mian IS , Sj\u00f6lander K , Underwood RC , Haussler D : Stochastic Context - Free Grammars for tRNA Modeling .Nucleic Acids Research 1994 , 22 : 5112 - 5120 .", "label": "", "metadata": {}, "score": "55.004803"}
{"text": "Experience from a larger example .The parser generator test works quite well for the small grammars we presented so far .However , there exist cases where , due to the finite lookahead of the generated parser , the parser generator reports conflicts while the grammar is in fact non - ambiguous .", "label": "", "metadata": {}, "score": "55.038067"}
{"text": "Accordingly , it is intended that the invention be limited only in terms of the appended claims .Trehan , R. et al . , A paralle chart parser for the committed choice non deterministic logic languages , IEEE Proceedings of the Fifth International Conference and Symposium : Logic Programming , Seattle , WA , Aug. 1988 , 212 232 , vol .", "label": "", "metadata": {}, "score": "55.104553"}
{"text": "The design of the constrained algorithms is discussed using concepts from object - oriented programming : the dynamic programming matrix can be viewed as a sparsely populated container , whereas the main loop that fills the matrix is a complex iterator [ 22 ] .", "label": "", "metadata": {}, "score": "55.20213"}
{"text": "Hence , v is injective if and only if G is non - ambiguous .Could we formally decide the semantic ambiguity of an arbitrary DP problem , we could do so for the problem given by G and \u03c3 , and hence , ambiguity of context free languages would be decidable .", "label": "", "metadata": {}, "score": "55.203163"}
{"text": "Abstract .A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .The unified language model includes a plurality of context - free grammars having non - terminals and a hybrid N - gram model having at least some of the same non - terminals embedded therein .", "label": "", "metadata": {}, "score": "55.31736"}
{"text": "Conclusions .We introduced an extension to the concept of stochastic context - free grammars that allows the probabilities of the productions to depend on the length of the generated subword .Furthermore we showed that existing algorithms that work on stochastic context - free grammars like training algorithms or determining the most likely parse - tree can easily be adapted to the new concept without significantly affecting their run - time or memory consumption .", "label": "", "metadata": {}, "score": "55.406353"}
{"text": "94 - 102 ) is one of the most efficient parsing algorithms for written sentence input and can operate in linear time for regular grammars .It was one of the first parsing methods that used a central data structure , known as a chart , for storing all intermediate results during the parsing process of a sentence .", "label": "", "metadata": {}, "score": "55.48179"}
{"text": "When the grammar is not LR ( k ) , the generator will not be able to create a deterministic parser and reports this situations in form of \" shift - reduce \" and \" reduce - reduce\"-conflicts to the user .", "label": "", "metadata": {}, "score": "55.654175"}
{"text": "The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .A word language model is built from the corpus .Probabilities of terminals of at least some of the context - free grammars are normalized and assigned as a function of corresponding probabilities obtained for the same terminals from the word language model .", "label": "", "metadata": {}, "score": "55.752815"}
{"text": "This can simply be done by adding them to the items as an additional parameter .The modifications of scanner and predictor are straightforward .Since choosing the most probable sequence of steps for each partial derivation will lead to the most probable derivation overall , the completer maximises the overall probability by choosing the most probable alternative , whenever there are multiple possibilities for generating a subword .", "label": "", "metadata": {}, "score": "55.80737"}
{"text": "Due to the variability and ambiguity of a spoken input signal , modified algorithms were created to improve on Earley 's algorithm and to adapt it to spoken language recognition .An example of such a modified algorithm is shown in A. Paeseler , ' Modification of Earley 's Algorithm for Speech Recognition ' , Proc . of NATO ASI , Bad Windsheim , 1987 .", "label": "", "metadata": {}, "score": "55.847164"}
{"text": "For a more detailed introduction of probabilistic Earley parsing as well as a proof of correctness and hints on efficient implementation see ( [ 10 ] ) .Application .In order to see if adding length - dependency actually improves the quality of the predictions of RNA secondary structures from stochastic context - free grammars , we used length - dependent and traditional versions of four different grammars to predict two sets of RNA molecules for which the correct secondary structure is already known .", "label": "", "metadata": {}, "score": "55.855095"}
{"text": "n .A .A .P .n .A .A .For the second type note that there is at most 1 way to distribute length 0 among the symbols of \u03b1 ensuring that if A \u03b1 , 0 occurs in .", "label": "", "metadata": {}, "score": "55.883026"}
{"text": "Degree of ambiguity and consequences for testing .Dowell and Eddy showed that semantic ambiguity produces sometimes mildly , sometimes drastically false results .In one experiment , they showed that the CYK algorithm for the semantically ambiguous grammar G 1 does not give the optimal secondary structure for about 20 % of a sample set of 2455 sequences .", "label": "", "metadata": {}, "score": "55.98821"}
{"text": "One such defect involves the calculation of probabilities .For context - free grammars , a nonterminal symbol may rewrite to another nonterminal symbol without having to go through a terminal symbol .Probabilities can therefore occur from many directions in the grammar .", "label": "", "metadata": {}, "score": "56.031185"}
{"text": "The important difference is that positional information is turned into symbolic information .After this modification , the parser generator runs smoothly through the grammar , which proves its non - ambiguity .Conclusion .In this work , we have presented testing methods and a partial proof procedure to analyze the semantic ambiguity of SCFGs .", "label": "", "metadata": {}, "score": "56.13256"}
{"text": "However , in an SCFG framework , all scores are information - theoretic and so there is no conflict of units .Despite these arguments , many people continue to find calories preferable to bits as a unit of score .For such readers , we note that the system of constraints described here is entirely applicable to the general score - attributed grammar .", "label": "", "metadata": {}, "score": "56.16315"}
{"text": "The order in which the Outside iterator visits nonterminals is topologically forward - sorted with respect to the grammar 's transition - rule graph ( i.e. the reverse of the order used by the Inside iterator ) .Calculate ( m , n ) and add to .", "label": "", "metadata": {}, "score": "56.163223"}
{"text": "To create a general purpose language model , such as an N - gram language model , a task - independent corpus of training data can be used and applied as discussed above to an N - gram algorithm .Task - independent corpora are readily available and can comprise compilations of magazines , newspapers , etc . , to name just a few .", "label": "", "metadata": {}, "score": "56.17377"}
{"text": "Wild , S. An Earley - style Parser for Solving the RNA - RNA Interaction Problem . B.Sc .Thesis , Kaiserslautern , Germany , 2010 .[ Google Scholar ] .Weinberg , F. Position - and - Length - Dependent Context - free Grammars .", "label": "", "metadata": {}, "score": "56.29522"}
{"text": "The processor then loops down to the bottom level , advances as many states as possible and then returns to parse .This shows a mutually recursive relationship between hypothesize and parse .rh o.+(\u03c3.sub.\u03bf -\u03c3 ) ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "56.37275"}
{"text": "This solution is wrong if several \" different \" solutions represent the same real - world object .Dowell and Eddy experimented with two ambiguous SCFGs , and showed that the quality of results may range from just slightly wrong to totally useless .", "label": "", "metadata": {}, "score": "56.434708"}
{"text": "This parser must be deterministic , and , in contrast to our CYK parsers , it only exists for non - ambiguous grammars .There are many such generators available ; we will focus on the class of LR ( k ) grammars [ 10 ] and their parser generators .", "label": "", "metadata": {}, "score": "56.535217"}
{"text": "We can combine these various strategies into a generalized constraint on base - pairs , alignment - columns or both .Here and are sets of permissible co - ordinates for structurally discrete subsequences in X and Y .Fold - related features ( basepairs and unpaired residues ) can be included or excluded by this set , and so we refer to it as a fold envelope [ 13 ] .", "label": "", "metadata": {}, "score": "56.551243"}
{"text": "The parser may compute the overall probability of a given string , summing up probabilities over all its derivations , in which case it is called the Inside algorithm .Or , the parser can return the most likely derivation of the input string , in which case it is known as the Viterbi algorithm .", "label": "", "metadata": {}, "score": "56.614403"}
{"text": "Proof .We show that for a given CFG G there exists a DP problem and an associated canonical model such that the DP algorithm is semantically am biguous if and only if the grammar is fl - ambiguous .Given an algorithm to decide ambiguity for DP problems , we could hence decide ambiguity for context free grammars , which is impossible .", "label": "", "metadata": {}, "score": "56.72083"}
{"text": "The parser operates frame synchronously to provide top - down hypotheses and to incorporate observation probabilities as they become available .A chart parser is disclosed which incorporates rule and observation probabilities with stochastic unification grammars .The parser operates frame synchronously to provide top - down hypotheses and to incorporate observation probabilities as they become available .", "label": "", "metadata": {}, "score": "56.730797"}
{"text": "and G3 both extend G1 based on the observation that a secondary structure will be more stable if it contains longer runs of immediately nested base pairs .They differ in the approach taken to get this into the model .G4 has been taken from [ 15 ] .", "label": "", "metadata": {}, "score": "56.845398"}
{"text": "It is not quite the same problem , however .In striving for avoidance of ambiguity , we want to get rid of the bad type and retain the good .Ambiguity is not a problem with a dynamic programming ( DP ) algorithm that returns a single , optimal score , together with a solution that achieves this score , and does not make assertions about other solutions in the search space .", "label": "", "metadata": {}, "score": "56.853477"}
{"text": "storing the N - gram model and a second plurality of context - free grammars comprising at least some of the same non - terminals representing the same semantic or syntactic concepts in the memo , each of the context - free grammars of the second plurality being more appropriate for use in the selected application .", "label": "", "metadata": {}, "score": "57.02953"}
{"text": "911 - 940 , 1999 .[ 27 ] Y. Sakakibara , M. Brown , R. Hughey , I.S. Mian , K. Sj\u00f6lander , R.C. Underwood , and D. Haussler , \" Stochastic Context - Free Grammars for tRNA modeling , \" Nucleic Acids Research , vol .", "label": "", "metadata": {}, "score": "57.059982"}
{"text": "METHOD : .Repeat the following two steps until no new states can be added : .( a ) Predict .( b ) Complete .Hypothesize .Scan .Repeat the following two steps until no new states can be added : .", "label": "", "metadata": {}, "score": "57.078003"}
{"text": "View Article .Zuker M , Stiegler P : Optimal Computer Folding of Large RNA Sequences using Thermodynamics and Auxiliary Information .Nucleic Acids Research 1981 , 9 : 133 - 148 .View Article PubMed .Chomsky N , Sch\u00fctzenberger MP : The algebraic theory of context - free languages .", "label": "", "metadata": {}, "score": "57.114872"}
{"text": "B .i . )P .B .i . )B .A . and .i . q . else . , where w \u0394 denotes the word generated by \u0394. Then we find .L .T .", "label": "", "metadata": {}, "score": "57.24431"}
{"text": "Loosely coupled systems , such as bottom - up systems or word lattice parsers , have produced nominal results , primarily due to time alignment problems .Top - down constraints from CFG 's have been integrated with speech using the Cocke - Younger - Kasami ( CYK ) algorithm , but this algorithm has bad average time complexity ( cubic ) .", "label": "", "metadata": {}, "score": "57.27381"}
{"text": "The system of . claim 2 having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the set of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .", "label": "", "metadata": {}, "score": "57.314285"}
{"text": "The present invention assigns probabilities to the hypotheses that it next wants to explore .It also employs a beam pruning technique , well - known in the art , and a delayed commitment in the score caluclation to determine the most probable correct response ( speech recognition ) .", "label": "", "metadata": {}, "score": "57.420082"}
{"text": "The CYK algorithm .The entries of the DP matrix , ( i , j , k , l ) , represent the maximum likelihood of any parse tree for ( X ij , Y kl ) .The Outside and KYC algorithms .", "label": "", "metadata": {}, "score": "57.596977"}
{"text": "l . .]P .s .u . t .i ._ . )Here represents the special end - of - sentence word .Three different methods are used to calculate the likelihood of a word given history inside a context - free grammar non - terminal .", "label": "", "metadata": {}, "score": "57.651554"}
{"text": "Thereafter , we make some observations with respect to the potential of testing procedures .Three simple cases .Ambiguity does not sneak into our grammars by chance and non - awareness .There are two competing goals in grammar design , and both may foster ambiguity .", "label": "", "metadata": {}, "score": "57.834507"}
{"text": "View Article PubMed .Altschul SF : Amino Acid Substitution Matrices from an Information Theoretic Perspective .Journal of Molecular Biology 1991 , 219 : 555 - 565 .View Article PubMed .Chomsky N : Three Models for the Description of Language .", "label": "", "metadata": {}, "score": "57.836094"}
{"text": "At step 164 , the task - independent corpus is parsed with the plurality of context - free grammars obtained in step 162 in order to identify word occurrences in the task - independent corpus of each of the semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "57.83936"}
{"text": "As described above , commonly the context - free grammars are written by a developer having at least some knowledge of what phrases may be used in the selected application for each of the semantic or syntactic concepts , but the extent of knowledge about such phrases is not complete .", "label": "", "metadata": {}, "score": "58.155876"}
{"text": "Language modeling has become an essential element in high performance , speaker - independent , continuous speech systems .Until recently , speech recognition systems have primarily used Finite State Automatons ( FSAs ) as the language model .These models offer efficient processing , easily accommodate observation probabilities , and permit simple training techniques to produce transition probabilities .", "label": "", "metadata": {}, "score": "58.17465"}
{"text": "This is demonstrated by the fact that the four derivation trees of Figure 1 all belong to the same symbol sequence .We now need to refine this notion of ambiguity .In modeling with SCFGs , derivations do not merely produce strings , but they represent objects of interest themselves .", "label": "", "metadata": {}, "score": "58.21856"}
{"text": "To reach this goal , we analyze the number of secondary structures and shapes compatible with an RNA sequence of length n under the assumption that base pairing is allowed between arbitrary pairs of bases analytically and compare their exponential growths .", "label": "", "metadata": {}, "score": "58.229774"}
{"text": "As different applications are developed for language processing , task - dependent ( domain dependent ) language models may be more appropriate , due to their increased specificity , which can also make the language models more accurate than a larger , general purpose language model .", "label": "", "metadata": {}, "score": "58.402245"}
{"text": "Unification grammars have allowed the close integration of syntax , semantics , and pragmatics .These grammars are especially significant for spoken language systems because syntactic , semantic , and pragmatic constraints must be applied simultaneously during processing .Discourse and domain contraints can then limit the number of hypotheses to consider at lower levels , thereby greatly improving performance .", "label": "", "metadata": {}, "score": "58.41475"}
{"text": "Unfortunately it always takes N 3 time , rather than linear time , even when processing a regular grammar .Additionally , the CYK algorithm is exhaustive ; it systematically expands everything whether it will be needed or not .Such an algorithm uses a great deal of processing time and memory space .", "label": "", "metadata": {}, "score": "58.436485"}
{"text": "Since it is easier to control the envelope construction parameter N than to control the envelope sizes directly , the following section will report performance indicators as a direct function of N , rather than as a function of the strongly - correlated but widely - varying envelope sizes .", "label": "", "metadata": {}, "score": "58.650772"}
{"text": "The system for parsing of claim 20 , wherein said language model incorporates context - free grammars .A method for parsing a spoken sentence having a plurality of words , comprising the steps of : .( a ) inputting a desired spoken input composed of a plurality of grammar levels ; .", "label": "", "metadata": {}, "score": "58.68137"}
{"text": "S .l .m . w .j .X .w .j .i .The item is considered to represent the partial derivation .X .w .j .i .Then the transitive closure with respect to the following operations is computed : .", "label": "", "metadata": {}, "score": "58.88456"}
{"text": "For an arbitrary context free grammar G , we can construct a DP problem where L ( G ) serves as the canonical model , and show that the context free grammar G is ambiguous if and only if the DP problem is semantically ambiguous .", "label": "", "metadata": {}, "score": "59.076443"}
{"text": "Furthermore the processor will output a score of for best sentence explanation at grammar level 0 ( FIG .2 ) , which is the sentence level grammar .After reading the vector of grammars , in the preferred embodiment , the processor will then input an ending frame indicator n. Although this is not required , it makes the algorithm more complete .", "label": "", "metadata": {}, "score": "59.080055"}
{"text": "Testing procedures .Brute force testing .Checking for duplicates in G ( Dotbracket , x ) .We can simply enumerate the dot - bracket representation of all structures exhaustively for a given input string and check for any repeats .", "label": "", "metadata": {}, "score": "59.12151"}
{"text": "If the grammar is ambiguous , this will be detected with the first application where it occurs .Proving non - ambiguity .Proving the absence of ambiguity in a grammar is of course better than any test procedure .Semantic ambiguity in dynamic programming is unde - cidable .", "label": "", "metadata": {}, "score": "59.172684"}
{"text": "BACKGROUND OF THE INVENTION .Field of the Invention .This invention relates to spoken language interfaces , and more particularly to a spoken language processor containing a chart parser that incorporates rule and observation 10 probabilities with stochastic unification grammars .", "label": "", "metadata": {}, "score": "59.23143"}
{"text": "Larger grammars allow a more sophisticated distinction of cases , hence providing a more fine - tuned model .However , if the underlying \" distinct \" cases lead to the same annotation sequence , then the grammar is ambiguous .This case is witnessed by grammar G 2 , where along with the introduction of base pair specific rules , another source of ambiguity is introduced .", "label": "", "metadata": {}, "score": "59.25505"}
{"text": "Nevertheless , while the CFG provides us with a deeper structure , it is still inappropriate for robust spoken language processing since the grammar is almost always incomplete .A CFG - based system is only good when you know what sentences to speak , which diminishes the value and usability of the system .", "label": "", "metadata": {}, "score": "59.270027"}
{"text": "The latter is the rule rather than the exception .Moreover , if derivations emerging from T are also ambiguous , the degrees of ambiguity multiply .Studying sources of ambiguity helps to better understand the nature of the error .Depending on the grammar , certain types of RNA structures may have their probability split up over a large number of derivations , while others are unaffected .", "label": "", "metadata": {}, "score": "59.31051"}
{"text": "A second aspect is a method for creating a task dependent unified language model for a selected application from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .", "label": "", "metadata": {}, "score": "59.37847"}
{"text": "3 labeled B discloses a word lattice , which is the input to the system ( in this example ) showing the beginning and ending frames of each of the word hypotheses .The lattice contains a plurality of lines which show the logarithmic probability of seeing a particular word over a particular span of data and gives such probability as a negative number .", "label": "", "metadata": {}, "score": "59.526814"}
{"text": "A system for recognizing a spoken sentence representing a plurality of words , comprising : . a processing means ; . a grammar coupled to said processing means for defining sentences in terms of elements of a language model ; . a lexicon for defining elements of the grammar in terms of symbols ; . a parser coupled to said grammar for combining words into partial sentences , for generating sets of states and for determining completed states ; . a predictor coupled to said grammar and said processing means for predicting the symbols of valid next elements generated by said parser ; . a completer for explaining the results from the parser ; and .", "label": "", "metadata": {}, "score": "59.56984"}
{"text": "Its successful construction is the proof of non - ambiguity ; for applying our SCFG , we need the original grammar and its CYK parser .MSTA accepts input files in the widely used yacc format .The following shows the input file for grammar G 5 : .", "label": "", "metadata": {}, "score": "59.606445"}
{"text": "However , to extend energy - based methods to two or more sequences , one must incorporate substitution scores .These are information - theoretic in nature and so are measured in bits , rather than kilocalories - per - mole [ 28 ] .", "label": "", "metadata": {}, "score": "59.61011"}
{"text": "These probability distributions are later used in executing a Viterbi or similar type of processing technique .Upon receiving the code words from the feature extraction module 106 , the tree search engine 114 accesses information stored in the acoustic model 112 .", "label": "", "metadata": {}, "score": "59.683918"}
{"text": "Take the union of all subsequences / cutpoints on these traceback paths to obtain the required envelope .As above , choose some appropriate single - sequence SCFG / pair HMM that models RNA folding / primary sequence alignment .Sample some number of RNA structures /pairwise alignments using the Inside / Forward algorithm with stochastic traceback .", "label": "", "metadata": {}, "score": "60.067177"}
{"text": "At the end of this article , it is discussed how our results could be used to help on identifying good predictions of RNA secondary structure .In this article we present a method to generate random objects from a large variety of combinatorial classes according to a given distribution .", "label": "", "metadata": {}, "score": "60.143326"}
{"text": "The parser now has two complete verb symbols and it extends , or looks , for states that need a verb that starts at either frame 2 or frame 3 and finds an S that corresponds to both back in state 8 and state 4 .", "label": "", "metadata": {}, "score": "60.156006"}
{"text": "Preferably , the N - gram model having the non - terminal tokens and the second plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts are stored on a computer readable medium accessible by the speech recognizer 100 .", "label": "", "metadata": {}, "score": "60.183525"}
{"text": "To do this we first need to define the length of a rule application formally : .Definition 2 .We will use this idea , but in order to ensure that our modified versions of the training algorithms provide consistent grammars - like the original versions do for SCFGs - we need some technical additions to this idea .", "label": "", "metadata": {}, "score": "60.256706"}
{"text": "Calculate ( i , j , k , l ) and store .The reduced - space dynamic programming matrix that was developed above for the constrained Inside algorithm can be re - used for the constrained Outside algorithm .Implementation .", "label": "", "metadata": {}, "score": "60.28783"}
{"text": "Generally , another aspect of the present invention includes using the context - free grammars for the task - dependent application to form phrases , sentences or sentence fragments that can then be used as queries in an information retrieval system .", "label": "", "metadata": {}, "score": "60.404278"}
{"text": "Furthermore , statistical modeling of RNA evolution continues to play a fundamental role in the phylogenetic classification of new forms of life .These biological motives have driven a demand for RNA sequence analysis tools that are faster , slimmer and more scaleable .", "label": "", "metadata": {}, "score": "60.755882"}
{"text": "The rule probabilities are functions of five scalar probability parameters ( stemExtend , stemGap , bifurcate , loopExtend and loopGap ) and four arrays of probability parameters ( baseIndel[4 ] , baseSubstitution[16 ] , basepairIndel[16 ] and basepairSubstitution [ 256 ] ) .", "label": "", "metadata": {}, "score": "60.811363"}
{"text": "( g ) predicting initial and final probabilities for a current frame for each start symbol of grammar ; .( h ) predicting a valid next nonterminal symbol to thereby create at least one state from its corresponding at least one rule according to said at least one grammar ; .", "label": "", "metadata": {}, "score": "60.817894"}
{"text": "An ending frame n. .OUTPUT : .A matrix of state sets E , 1 rows and n columns . , the best score for S of G o .METHOD : .Make E l , o empty for all l. .", "label": "", "metadata": {}, "score": "60.861134"}
{"text": "Semantic ambiguity is not a problem with the Inside algorithm , as a probability sum over all derivations is computed anyway .With the Viterbi algorithm , we can certainly obtain the most likely derivation , but we do not know whether it represents the most likely annotation sequence .", "label": "", "metadata": {}, "score": "60.890778"}
{"text": "The A / D converter 104 converts the analog speech signal into a sequence of digital signals , which is provided to the feature extraction module 106 .In one embodiment , the feature extraction module 106 is a conventional array processor that performs spectral analysis on the digital signals and computes a magnitude value for each frequency band of a frequency spectrum .", "label": "", "metadata": {}, "score": "60.923367"}
{"text": "In the embodiment illustrated , the language model 16 can be an N - gram language model or a unified language model comprising a context - free grammar specifying semantic or syntactic concepts with non - terminals and a hybrid N - gram model having non - terminals embedded therein .", "label": "", "metadata": {}, "score": "61.072506"}
{"text": "In particular , at step 182 , a plurality of context - free grammars is obtained .For example , a task - dependent application may require modeling the day of the week as a semantic concept in the N - gram model .", "label": "", "metadata": {}, "score": "61.077744"}
{"text": "However it has been shown that this co - transcriptional folding has an effect on the resulting secondary structures ( e.g. , [ 6 , 7 ] ) .Since the simulation algorithms have the downside of being computationally expensive , it is desirable to add the effects of co - transcriptional folding into the traditional algorithms .", "label": "", "metadata": {}, "score": "61.077953"}
{"text": "Waterman MS , Eggert M : A new algorithm for best subsequence alignments with application to tRNA - rRNA comparisons .Journal of Molecular Biology 1987 , 197 : 723 - 725 .View Article PubMed .Higgins DG , Sharp PM : Fast and Sensitive Multiple Sequence Alignments on a Microcomputer .", "label": "", "metadata": {}, "score": "61.16135"}
{"text": "( s ) parsing said start symbols according to the spoken input and grammars to produce observations of said symbols ; and .( t ) explaining the input based on the results of said step of parsing .The method for recognizing spoken sentences of claim 3 , further comprising the steps of : .", "label": "", "metadata": {}, "score": "61.16803"}
{"text": "The variable may be used elsewhere to denote the occurence of the same feature set .The preferred embodiment of the present invention is as follows : .INPUT : .A vector of grammars , G , G o , . . .", "label": "", "metadata": {}, "score": "61.18721"}
{"text": "This is especially important for left recursive rules .Subtracting the beginning probability from the ending probability , yields \u03b7 . sigma .-\u03c3.sub.\u03bf ) ] _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "61.19809"}
{"text": "We then compared these predicted structures to the structures from the database , computing two commonly used criteria to measure the quality : .Sensitivity ( also called recall ) : The relative frequency of correctly predicted base pairs among base pairs that appear in the correct structure .", "label": "", "metadata": {}, "score": "61.221016"}
{"text": "The system of . claim 8 and having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .", "label": "", "metadata": {}, "score": "61.295147"}
{"text": "A grammar for pairwise RNA alignment and structure prediction .After some empirical experimentation , we developed the grammar of Tables 2 , 3 , 4 for the stemloc program .The grammar is split over three tables due to its considerable number of rules .", "label": "", "metadata": {}, "score": "61.298355"}
{"text": "Structure counting for sample sequences .An even stronger test is possible when we have a reference grammar R available that generates the same language and is known to be semantically non - ambiguous .Grammar G will produce counts that are larger than those of R if and only if G allows ambiguous derivations for x .", "label": "", "metadata": {}, "score": "61.301804"}
{"text": "When the third N - gram language model 252 is built having non - terminals , the word phrases or synthetic data at block 242 typically will also include the non - terminals as well .When the context - free grammars are used to generate synthetic data , probabilities for the word phrases formed with the non - terminals and the terminals of the non - terminals can be chosen as desired ; for instance , each can be assigned equal probability .", "label": "", "metadata": {}, "score": "61.34832"}
{"text": "Durbin R , Eddy S , Krogh A , Mitchison G : Biological Sequence Analysis : Probabilistic Models of Proteins and Nucleic Acids Cambridge , UK : Cambridge University Press 1998 .View Article .Hofacker IL , Bernhart SH , Stadler PF : Alignment of RNA base pairing probability matrices .", "label": "", "metadata": {}, "score": "61.428696"}
{"text": "The canonical model plays an essential role .It is the mathematical formalization of the real - world domain we want to study , and \" canonical \" means one - to - one correspondence .Any formal proof can only deal with the formalization of the real - world domain , and when the one - to - one correspondence does not hold , all proofs of ( non- ) ambiguity would be meaningless for the real world .", "label": "", "metadata": {}, "score": "61.467995"}
{"text": "For instance , by way of example , one set of context - free grammars of a larger plurality of context - free grammars for a software application or task concerning scheduling meetings or sending electronic mail may comprise : . etc . .", "label": "", "metadata": {}, "score": "61.533512"}
{"text": "Lefebvre F : A Grammar - Based Unification of Several Alignment and Folding Algorithms .Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology ( Edited by : States DJ , Agarwal P , Gaasterland T , Hunter L , Smith RF , Menlo Park ) .", "label": "", "metadata": {}, "score": "61.55503"}
{"text": "Aside from this consideration and the restrictions implied by Definition 5 ( consistency of a set of intervals ) there is no obvious criterion that helps with deciding on a set of intervals .Thus we created several different sets ranging from approximately 10 to approximately 100 intervals , evaluating a subset of the prediction data with each of them .", "label": "", "metadata": {}, "score": "61.608635"}
{"text": "The efficiency of the parser makes it applicable to multiple levels of a spoken language system ( e.g. , sentence , word , phoneme , and phone levels ) .( n ) parsing terminal symbols from the current grammar level as start symbols for the next lower grammar level unless at the lowest grammar level ; .", "label": "", "metadata": {}, "score": "62.008884"}
{"text": "By definition , an LR ( k ) grammar is non - ambiguous , and for a given k it is decidable whether a grammar is LR ( k ) .This decision can be assigned to a parser generator .Given the grammar and the lookahead k , a parser generator tries to construct a parser that uses k symbols of lookahead .", "label": "", "metadata": {}, "score": "62.014496"}
{"text": "In this manner , the size of the task - independent corpus is reduced prior to parsing so that method 180 may execute more quickly .Appropriate context - free grammars can then be determined and included in the plurality of context - free grammars at step 182 .", "label": "", "metadata": {}, "score": "62.03727"}
{"text": "Task - dependent corpora , on the other hand , are typically not available .These corpora must be laboriously compiled , and even then , may not be very complete .A broad aspect of the invention includes a method for creating a task or domain dependent unified language model for a selected application from a task - independent corpus .", "label": "", "metadata": {}, "score": "62.124306"}
{"text": "3A - C show an example demonstrating frame synchronous parsing using probabilities employed by the present invention ; .FIGS .4A - D show an example showing a typical left recursive rule showing rule probabilities correctly computed by the present invention ; and .", "label": "", "metadata": {}, "score": "62.16929"}
{"text": "Simply setting some intermediate probabilities to zero is not sufficient to accelerate the Inside algorithm .We also need to redesign the iteration to avoid visiting zero - probability subsequence - pairs ( X ij , Y kl ) .This is achieved by pre - indexing the fold envelopes , and the alignment envelope so that we can quickly locate valid co - ordinates ( i , j , k , l ) .", "label": "", "metadata": {}, "score": "62.19166"}
{"text": "Results demonstrating the program 's efficient resource usage are presented .The stemloc program also implements various familiar extensions to pairwise alignment , including local alignment [ 23 ] , Waterman - Eggert N -best suboptimal alignments [ 24 ] and progressive multiple alignment [ 25 ] .", "label": "", "metadata": {}, "score": "62.192604"}
{"text": "Rivas E , Eddy SR : Secondary structure alone is generally not statistically significant for the detection of noncoding RNAs .Bioinformatics 2000 , 16 ( 7 ) : 583 - 605 .View Article PubMed .Coventry A , Kleitman DJ , Berger B : MSARI : Multiple sequence alignments for statistical detection of RNA secondary structure .", "label": "", "metadata": {}, "score": "62.195827"}
{"text": "If the processor is not on the bottom level , then it will predict the terminal symbols at the next lower level and proceed to parse .The processor will scan observations from a lower grammar level into the current grammar level .", "label": "", "metadata": {}, "score": "62.264313"}
{"text": "The lower ceiling for N in the first two tests was imposed by resource limitations .A range of different values for the parameter N was used to test the above three strategies .As N was increased over the range , the size of the corresponding fold or alignment envelopes was found to be strongly correlated ( Figures 6 , 7 ) .", "label": "", "metadata": {}, "score": "62.34479"}
{"text": "Essentially , the N - gram language model 142 ( also known as a hybrid N - gram model ) of the unified language model 140 includes an augmented vocabulary having words and at least some of the non - terminals .", "label": "", "metadata": {}, "score": "62.37601"}
{"text": "The identified text of the task - independent corpus is more relevant to the selected task or application ; therefore , a language model derived from the identified text may be more specific than a language model based on the complete task - independent corpus .", "label": "", "metadata": {}, "score": "62.423664"}
{"text": "As with the Inside algorithm , we sum contributions to ( i , j , k , l ) from various matching production rules .In contrast to the Inside algorithm , the nonterminal V that indexes ( ... ) must now be matched on the right - hand - side , not the left - hand - side , of these production rules .", "label": "", "metadata": {}, "score": "62.42765"}
{"text": "The problem for N - gram models is that a lot of data is needed and the model may not be specific enough for the desired application .Since a word - based N - gram model is limited to n - word dependency , it can not include longer - distance constraints in the language whereas CFG can .", "label": "", "metadata": {}, "score": "62.446293"}
{"text": "Each of the context - free grammars include words or terminals present in the task - independent corpus to form the semantic or syntactic concepts .The task - independent corpus with the plurality of context - free grammars is parsed to identify word occurrences of each of the semantic or syntactic concepts and phrases .", "label": "", "metadata": {}, "score": "62.461437"}
{"text": "In his thesis [ 14 ] , Bj\u00f6rn Voss introduced a new grammar that promises to handle dangling bases of multiloop components in a non - ambiguous way .With 28 nonterminal symbols and 79 rules , the grammar is quite large .", "label": "", "metadata": {}, "score": "62.56265"}
{"text": "i . ) j .n .p .j . )P .n .A . i .P .i . w .T .i .R .n .w . else .Lemma 1 .Proof .", "label": "", "metadata": {}, "score": "62.641594"}
{"text": "The performance of stemloc with these envelopes on each of the pairwise test alignments is given in Table 5 .Discussion .The algorithms presented here include constrained versions of Pair - SCFG dynamic programming algorithms that run in significantly reduced space and time .", "label": "", "metadata": {}, "score": "62.687675"}
{"text": "The N - gram language model 142 will be used to first predict words and non - terminals .Then , if a non - terminal has been predicted , the plurality of context - free grammars 144 is used to predict terminals as a function of the non - terminals .", "label": "", "metadata": {}, "score": "62.696297"}
{"text": "[ September 9 ] Topic Area 2 .Extend the previous assignment so your language model estimates use smoothed probabilities .Use any smoothing method you like except add - K. Feel free to exploit Dan Melamed 's Simple Good - Turing smoothing software .", "label": "", "metadata": {}, "score": "62.91809"}
{"text": "Both types of envelope are illustrated in Figure 2 .The fold and alignment envelopes satisfy the following set relations .If equality holds in all three cases , then we recover the unconstrained Inside algorithm .Note that the co - ordinates ( i , j , k , l ) for the cutpoints and subsequences lie between residues of X and Y .", "label": "", "metadata": {}, "score": "62.937897"}
{"text": "Explanations of each of those different functions are given below .Further assume all symbols are at level 1 unless otherwise indicated .If B 1 and B 2 are atomic symbols for context - free grammars , then they are unified by default .", "label": "", "metadata": {}, "score": "63.002277"}
{"text": "Note that all co - ordinates ( i , j , k , l , m , n ) lie on the grid - lines between nucleotides .Imposing constraints .The high time and memory cost of the Inside and related algorithms motivate the development of slimmer , faster versions .", "label": "", "metadata": {}, "score": "63.04308"}
{"text": "Take for example .Proof .Every dot - bracket string describes exactly one possible secondary structure .Then , for an RNA sequence x compatible with z , using the corresponding productions there are different derivations in G which represent the same secondary structure z .", "label": "", "metadata": {}, "score": "63.048683"}
{"text": "( p ) scanning observations from said next lower grammar level into waiting states of said current grammar level ; .( q ) repeating steps ( j ) through ( p ) until no new states can be completed ; .", "label": "", "metadata": {}, "score": "63.087086"}
{"text": "This configuration wastes some space , but has the important advantage of constant access time for given indices ( U , i , j , k , l ) .These are also precomputed and stored .Similar precomputed sets and ranks are stored for .", "label": "", "metadata": {}, "score": "63.13739"}
{"text": "Since the algorithms are implemented for any SCFG , it is straightforward to modify the program to experiment with grammars that model such phenomena .An example of a grammar that models the latter type of mutation ( whole - substructure indels ) , and is also fully derived from an evolutionary rate - based model , is presented in a companion paper [ 36 ] .", "label": "", "metadata": {}, "score": "63.159225"}
{"text": "Our first approach with the parser generator succeeded , except for one small part of the grammar for which it reports a con - flict .Figure 2 shows two example derivations where this conflict occurs .Two example derivations .Two example derivations of a grammar taken from [ 14 ] .", "label": "", "metadata": {}, "score": "63.199852"}
{"text": "Still , we shall refer to some DP algorithms that are not based on SCFGs , where our treatment also applies .SCFGs for RNA secondary structure analysis .We will further exemplify the above using the grammars G 1 to G 6 studied by Dowell and Eddy : . Dowell and Eddy showed that grammars G 1 and G 2 are semantically ambiguous , while G 3 to G 6 passed a partial test for non - ambiguity .", "label": "", "metadata": {}, "score": "63.236298"}
{"text": "( n ) scanning observations from said next lower grammar level into waiting states of said current grammar level ; .( i ) repeating steps ( h ) through ( n ) until no new states can be completed ; .", "label": "", "metadata": {}, "score": "63.29239"}
{"text": "The tree in portion B reflects the desired outcome of this parse .The tree shows that to recognize this input the parser has to use a left recursive rule two times and the non - left recursive S choice , which is S goes to C , once at the bottom .", "label": "", "metadata": {}, "score": "63.299015"}
{"text": "This sum can be performed efficiently by the Inside algorithm , as will be described below .Dynamic programming algorithms for Pair SCFGs .The following section describes the constrained and unconstrained dynamic programming ( DP ) algorithms used for Pair SCFGs .", "label": "", "metadata": {}, "score": "63.31152"}
{"text": "Every time we develop a new grammar , the dragon of ambiguity raises its head , but with the weapons presented here , we can be confident to defeat it .Methods .Our way to describe various tests by combining a grammar with varying scoring schemes is derived from the algebraic dynamic programming method , described in detail in [ 6 ] .", "label": "", "metadata": {}, "score": "63.326996"}
{"text": "Calculate Q B ( m , n ) and add to Q 1 . Calculate ( i , j , k , l ) and store .Alternative designs for the algorithm are possible , and indeed different circumstances may affect the choice of optimal design ( e.g. depending on which envelopes are most constrained ) .", "label": "", "metadata": {}, "score": "63.34584"}
{"text": "For grammars that are rather distinct , the proof is as messy as the de - novo proof .Mechanical proof of non - ambiguity .We now present a mechanical technique that is a partial proof procedure for the case of modeling RNA structure with SCFGs : If it succeeds , it proofs non - ambiguity , if it fails , we do not know .", "label": "", "metadata": {}, "score": "63.365955"}
{"text": "Because the chart parser parses symbols only once , it can specifically treat left recursive rules .To add the probabilities correctly , it is very important that the predict function add the rule probabilitiy onto the final probability of the state , not to add it onto the initial probability of the state .", "label": "", "metadata": {}, "score": "63.524094"}
{"text": "View Article .Zuker M , Stiegler P : Optimal Computer Folding of Large RNA Sequences Using Thermodynamics and Auxiliary Information .Nucleic Acids Research 1981 , 9 : 133 - 148 .View Article PubMed .Eddy SR , Durbin R : RNA Sequence Analysis Using Covariance Models .", "label": "", "metadata": {}, "score": "63.54151"}
{"text": "In this section , we first review some sources of ambiguity and suggest three ways to deal with it : ambiguity avoidance , testing for ambiguity , and , best of all when successful , a mechanical proof of absence .Sources of ambiguity , and how to avoid them .", "label": "", "metadata": {}, "score": "63.55587"}
{"text": "( t ) after step ( k ) , incrementing a frame counter .The method for parsing of claim 25 , wherein said state sets are 1 rows corresponding to the number of grammar levels and N columns corresponding to the number of input frames of speech .", "label": "", "metadata": {}, "score": "63.728462"}
{"text": "CITATION .Markus E. Nebel , Anika Scheid , \" Analysis of the Free Energy in a Stochastic RNA Secondary Structure Model \" , IEEE / ACM Transactions on Computational Biology and Bioinformatics , vol.8 , no .6 , pp .", "label": "", "metadata": {}, "score": "63.77855"}
{"text": "[ 10 ] M. Zuker , D.H. Mathews , and D.H. Turner , \" Algorithms and Thermodynamics for RNA Secondary Structure Prediction : A Practical Guide , \" RNA Biochemistry and Biotechnology , J. Barciszewski and B.F.C. Clark , eds . , pp .", "label": "", "metadata": {}, "score": "63.9509"}
{"text": "The word phrases can include some or all of the various combinations and permutations defined by the associated context - free grammars where the non - terminal tokens include multiple words .At step 206 , at least one query is formulated for an information retrieval system using at least one of the generated word phrases .", "label": "", "metadata": {}, "score": "63.95863"}
{"text": "Note that the subtree rooted at any internal W -labeled node describes a sub - process that generates some pair of subsequences ( X ij , Y kl ) starting from nonterminal W .We will refer to this subsequence - pair ( X ij , Y kl ) as the inside sequence - pair of W .", "label": "", "metadata": {}, "score": "63.96454"}
{"text": "The four grammars G 3 - G 6 passed the Dowell - Eddy test in [ 1 ] , and in the next section we shall prove their non - ambiguity .In this sense , we can state that this test has already worked quite well in practice .", "label": "", "metadata": {}, "score": "63.96791"}
{"text": "Applications .For our experiments , we used the MSTA parser generator of the COCOM compiler construction toolkit [ 11 ] .MSTA is capable of generating LR ( k ) parsers for arbitrary k. Note that compiler writers prefer other parser generators like yacc [ 12 ] and bison [ 13 ] , which for efficiency reasons only implement LR ( 1 ) parsers .", "label": "", "metadata": {}, "score": "64.03281"}
{"text": "Our case , however , is easy .When RNA secondary structure is our domain of study , base pair sets or the familiar dot - bracket strings can serve as a canonical model , as they uniquely represent secondary structures .", "label": "", "metadata": {}, "score": "64.12842"}
{"text": "Semantic ambiguity in dynamic programming .Our treatment here extends to all dynamic programming algorithms that fall into the class known as algebraic dynamic programming ( ADP ) [ 6 ] .However , some definitions must be refined , as the ADP approach uses so - called yield grammars rather than ( S)CFGs .", "label": "", "metadata": {}, "score": "64.13902"}
{"text": "The method for recognizing spoken sentences of claim 1 , wherein said state sets are 1 rows corresponding to the number of grammar levels and N columns correspondong to the number of input frames of speech .The method for recognizing spoken sentences of claim 1 , wherein said grammar is a stochastic unification grammar .", "label": "", "metadata": {}, "score": "64.2963"}
{"text": "Thes constraints lead to significant reductions in requirements for processor and memory usage , which will increase the length of RNA sequences that can be analyzed on mainstream computer hardware .These algorithms have been used to implement stemloc , a fast , efficient software tool for multiple RNA sequence alignment implementing numerous extra features such as local alignment , Waterman - Eggert N -best suboptimal alignment and progressive multiple alignment .", "label": "", "metadata": {}, "score": "64.36934"}
{"text": "I . i . a .i .A . i .w .P .n .A . i . w . ) by induction on i .From this the result follows by Lemma 1 .For the first kind the construction of the bijection in Lemma 1 guarantees that .", "label": "", "metadata": {}, "score": "64.44317"}
{"text": "FIG .5 illustrates a first method 160 for creating or building a language model .The method 160 includes a step 162 for obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "64.52899"}
{"text": "expected ) number of occurrences for each rule globally we determine separate counts for each length interval and use these to compute separate ( expected ) relative frequencies .This works since each occurrence ( resp .probability of occurrence ) is tied to a specific subword and thus associated a length .", "label": "", "metadata": {}, "score": "64.62028"}
{"text": "Hand - made proof of non - ambiguity .A hand - made de - novo proof of the non - ambiguity of a new grammar G requires an inductive argument on the number of parses corresponding to the same annotation sequence .", "label": "", "metadata": {}, "score": "64.63164"}
{"text": "Thus this case is implicitly included in the following definitions and proofs .Definition 6 . r .A .R .P .r . )f .r .T . ) s .A .R .f .", "label": "", "metadata": {}, "score": "64.64549"}
{"text": "However , each of the context - free grammars of the second plurality is more appropriate for the selected application .Referring back to the proper name example provided above , the second plurality of context - free grammars could include a CFG : .", "label": "", "metadata": {}, "score": "64.67295"}
{"text": "Most computational prediction methods based on free energy minimization compute a number of suboptimal foldings and we have to identify the native structures among all these possible secondary structures .For this reason , much effort has been made to develop approaches for identifying good predictions of RNA secondary structure .", "label": "", "metadata": {}, "score": "64.71938"}
{"text": "Mathews DH , Turner DH : Dynalign : an algorithm for finding the secondary structure common to two RNA sequences .Journal of Molecular Biology 2002 , 317 ( 2 ) : 191 - 203 .View Article PubMed .Perriquet O , Touzet H , Dauchet M : Finding the common structure shared by two homologous RNAs .", "label": "", "metadata": {}, "score": "64.74321"}
{"text": "Word phrases are generated from the plurality of context - free grammars .The context - free grammars are used for formulating an information retrieval query from at least one of the word phrases .The task - independent corpus is queried based on the query formulated and text in the task - independent corpus is identified based on the query .", "label": "", "metadata": {}, "score": "64.78689"}
{"text": "But this test is much more thorough than our previous one , as the entire structure space of each tested x is analyzed .For example , a sequence of length 30 has an expected number of 175550 feasible structures [ 3 ] .", "label": "", "metadata": {}, "score": "64.8211"}
{"text": "n .A . i .P . a .i . w .T .i . w . else .The second restriction obviously only applies if lengths are to be grouped into intervals as we will do in this paper .", "label": "", "metadata": {}, "score": "64.90837"}
{"text": "The phrases associated with these grammars would not normally be spoken in the selected application .Thus , the extent or size of the plurality of context - free grammars is less during speech recognition , corresponding to less required storage space in the computer 50 than was used for parsing the task - independent corpus .", "label": "", "metadata": {}, "score": "64.93771"}
{"text": "[ 12 ] D. Sankoff , J.B. Kruskal , S. Mainville , and R.J. Cedergren , \" Fast Algorithms to Determine RNA Secondary Structures Containing Multiple Loops , \" Time Warps , String Edits , and Macromolecules : The Theory and Practice of Sequence Comparison , ch .", "label": "", "metadata": {}, "score": "64.98766"}
{"text": "Memory is the most prohibitively expensive resource demand of the Inside algorithm .In its simplest form , the algorithm stores the intermediate probabilities ( i , j , k , l ) using a five - dimensional array indexed by U , i , j , k and l .", "label": "", "metadata": {}, "score": "65.01146"}
{"text": "In order to do so we took the tRNA database from [ 14 ] , filtered out all sequences with unidentified bases and split the remaining data into a training set of 1285 sequences and a benchmark set of 1284 sequences .", "label": "", "metadata": {}, "score": "65.055855"}
{"text": "HMM grammars have reference vectors symbols corresponding to reference vectors as their terminal symbols .The processor then scores these reference vectors and gets probabilities for the frame and returns those as observations to the next higher level .Those observations are scanned into that next higher level .", "label": "", "metadata": {}, "score": "65.06094"}
{"text": "Since the processor is at a terminal symbol , it can then perform this algorithm because all states have been predicted and completed and there is no more information available to enable it to predict or complete any more states .The processor will predict at level 1 + 1 , which is the next lower level .", "label": "", "metadata": {}, "score": "65.086205"}
{"text": "We want to point out that the non - ambiguity proofs for the grammars studied here do not solve the problem of ambiguity for modeling of RNA secondary structures once and for all .New scientific interests and research questions will always demand new grammars .", "label": "", "metadata": {}, "score": "65.1159"}
{"text": "Whereas the structural signal encoded in a single RNA gene is rather weak and may be barely ( if at all ) distinguishable from the secondary structure of a random sequence [ 6 ] , the covariation signal increases with every additional sequence considered .", "label": "", "metadata": {}, "score": "65.23771"}
{"text": "With natural language systems , N is traditionally words ; however with speech systems , N is equal to frames , which are the fundamental time units used in speech recognition .The algorithm was significant in that it provided a time synchronous algorithm for speech recognition which improved accuracy because the processor did not have to be concerned about how the words fit together .", "label": "", "metadata": {}, "score": "65.24449"}
{"text": "The bifurcation rules ( Q B ( m , n ) ) account for conserved multiloop structures in the RNA , i.e. one homology between substructures X i m and Y kn and another between substructures X mj and Y nl ( Figure 3 ) .", "label": "", "metadata": {}, "score": "65.297295"}
{"text": "Testing for ambiguity .Performing a test for semantic ambiguity allows us to obtain more confidence in the grammar , although testing can not prove non - ambiguity , but only ambiguity .Algorithmic arsenal for ambiguity testing .First , we create several variants of the Inside and Viterbi algorithms , which are our algorithmic arsenal for testing .", "label": "", "metadata": {}, "score": "65.327835"}
{"text": "( s ) between steps ( d ) and ( e ) , reading an ending frame indicator ; and .( t ) after step ( n ) , incrementing a frame counter .The method for parsing of claim 25 , further comprising the steps of : .", "label": "", "metadata": {}, "score": "65.4312"}
{"text": "[ Google Scholar ] .Zuker , M. ; Stiegler , P. Optimal computer folding of large RNA sequences using thermodynamics and auxiliary information .Nucleic Acids Res .[ Google Scholar ] .Knudsen , B. ; Hein , J. RNA secondary structure prediction using stochastic context - free grammars and evolutionary history .", "label": "", "metadata": {}, "score": "65.442"}
{"text": "Commonly , a plurality of context - free grammars comprising non - terminal tokens representing various semantic or syntactic concepts are used .For instance , other semantic or syntactic concepts include geographical places , regions , titles , dates , times , currency amounts , and percentage amounts to name a few .", "label": "", "metadata": {}, "score": "65.46148"}
{"text": "Table 1 .Results of mechanical proof procedure .Number of shift - reduce ( SR ) and reduce - reduce ( RR ) conflicts when feeding example grammars G1 to G8 into parser generator MSTA .A 0/0 entry indicates a successful proof of non - ambiguity .", "label": "", "metadata": {}, "score": "65.46292"}
{"text": "Furthermore , the pairwise alignments were binned according to sequence identity , providing four alternative parameterisations ; the bin ranges were 30 - 40 % , 50 - 60 % , 70 - 80 % and 90 - 100 % . stemloc allows the user to re - estimate all parameters from their own personal training set of trusted alignments .", "label": "", "metadata": {}, "score": "65.607376"}
{"text": "Q .h . )P . word .w .u . t .i .l .u . t .i .l .The normalization is performed the same as in Equation ( 7 ) .Multiple segmentations may be available for W due to the ambiguity of natural language .", "label": "", "metadata": {}, "score": "65.616455"}
{"text": "( q ) parsing said start symbols according to the spoken input and grammars to produce observations of said symbols ; and .( r ) explaining the input based on the results of said step of parsing .The method for parsing of claim 25 , wherein a complete state is a state which fully explains a segment of the spoken input .", "label": "", "metadata": {}, "score": "65.62724"}
{"text": "View Article PubMed .Gorodkin J , Heyer LJ , Stormo GD : Finding the most significant common sequence and structure motifs in a set of RNA sequences .Nucleic Acids Research 1997 , 25 ( 18 ) : 3724 - 3732 .", "label": "", "metadata": {}, "score": "65.81493"}
{"text": "Returning again to FIG .2 , the processor looks at the inputted grammar and sees that there is a symbol , say S , that corresponds to a sentence .The processor proceeds to the parse function , does the prediction and completion , and finds out that it needs some words which are terminal symbols in the sentence grammar level .", "label": "", "metadata": {}, "score": "65.89387"}
{"text": "3 , an example of frame synchronous parsing using probabilities is demonstrated .In the portion labeled A , a simple grammar is given which consists of one rule , namely sentence rewrites to a noun and a verb .One noun \" boys \" and two verbs \" sleep \" and \" leap \" are given .", "label": "", "metadata": {}, "score": "66.02665"}
{"text": "As above , an extra nonterminal symbol is required to achieve non - ambiguity .Sometimes it is tempting to add a special case by using \u03b5 .The general case of \u03b5 -rules may be more tricky to handle .In general , all context free languages can be described without \u03b5 -rules , except possibly one for the axiom symbol .", "label": "", "metadata": {}, "score": "66.2551"}
{"text": "Computer Applications in the Biosciences 1990 , 6 ( 4 ) : 309 - 318 .PubMed .Klein R , Eddy SR : Noncoding RNA gene detection using comparative sequence analysis .BMC Bioinformatics 2003 ., 4 ( 44 ) : .", "label": "", "metadata": {}, "score": "66.40443"}
{"text": "At the bottom of the trace , it is seen that the one - third arises the correct number of times ( 3 ) , the 0.4 arises twice , and the 0.6 arises once , which accurately reflect the number of times the left and nonleft recursive rule were applied .", "label": "", "metadata": {}, "score": "66.559685"}
{"text": "It should simply be noted that the language model created from the identified text according to the present technique works better with information retrieval techniques that identify more relevant text of the task - independent corpus .The text identified in the task - independent corpus based on the query is indicated at step 210 .", "label": "", "metadata": {}, "score": "66.74167"}
{"text": "( Only tangentially related .Implicitly uses the idea of semirings to generalize Inside - Outside and conditional random fields to a large class of grammar formalisms beyond CFG . )Paul Smith , \" Statistics , Machine Learning and Data Mining \" , Monday , October 25 , 2004 , 4:00 pm , MATH 3206 , Abstract is at the Mathematics Graduate Minicourse Series page , along with links to relevant papers .", "label": "", "metadata": {}, "score": "66.765785"}
{"text": "Step 190 is similar to Step 170 and includes obtaining a second set of context - free grammars suited for the selected application .Used during language processing such as speech recognition , the N - gram model having the non - terminal tokens and the plurality of context - free grammars associated with the task - dependent application is stored on a computer readable medium accessible by the speech recognition module 100 .", "label": "", "metadata": {}, "score": "66.85692"}
{"text": "Aho A , Ullman J : The Theory of Parsing , Translation and Compiling Englewood Cliffs , NJ : Prentice - Hall 1973 .[ I and II ] .Giegerich R , Meyer C , Steffen P : A Discipline of Dy namic Programming over Sequence Data .", "label": "", "metadata": {}, "score": "66.89514"}
{"text": "Runtime .The considerations in Note 1 lead to the assumption that both versions should take about the same time on a given grammar .This was confirmed during our experiments , with none of the versions being consistently faster , i.e. , if there is a difference it was overshadowed by effects like system load .", "label": "", "metadata": {}, "score": "67.57642"}
{"text": "claim 7 and having instructions further comprising : . building a second N - gram language model from the word phases from the plurality of context - free grammars ; and .combining the first - mentioned N - gram language model and the second N - gram language model to form a third N - gram language model .", "label": "", "metadata": {}, "score": "67.60032"}
{"text": "P .n .A . i .P .n .w . ) , where each of the \u03b2 contains at most one symbol B i contributing a factor of b i along with an arbitrary number of other symbols ( terminal or with smaller indices ) contributing factors of 0 or 1 by the induction hypothesis .", "label": "", "metadata": {}, "score": "67.6212"}
{"text": "Just - in - time testing .While testing can not guarantee the non - ambiguity of the grammar , we can convert the previous idea to a test that ensures for each application run that the results are not affected by ambiguity .", "label": "", "metadata": {}, "score": "67.62471"}
{"text": "As discussed above , the task - independent corpus is a compilation of sentences , phrases , etc . that is not directed at any one particular application , but rather , generally shows , through a wide variety of examples , how words are ordered in a language .", "label": "", "metadata": {}, "score": "67.70813"}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a language model , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . generating word phases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phases ; . querying a task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and .", "label": "", "metadata": {}, "score": "67.80265"}
{"text": "The actual words , also called atoms , are defined elsewhere , such as in a knowledge base .The grammar and lexicon , taken together , can be compiled without reference to a particular domain .The result is to define a spoken language reference which can be a fairly complex subset of a language .", "label": "", "metadata": {}, "score": "67.9107"}
{"text": "It has been the goal of recent research to make machine understanding of spoken language possible through a tight coupling between speech and natural language systems .The difficulty posed by this coupling lies in trying to integrate statistical speech information with natural language grammars .", "label": "", "metadata": {}, "score": "67.99792"}
{"text": "5 is a graph showing the effect of chart parsing by the present invention on pruning .The graph considers time versus logarithmic probability of hypotheses .Each of the dots represent the same hypothesis symbol at times t i and t k .", "label": "", "metadata": {}, "score": "68.03852"}
{"text": "It remains open whether specifically for the class of grammars that describe RNA secondary structure , this problem is decidable .We have proposed several tests , and a partial , mechanical proof procedure .We mechanically proved that the six grammars that passed Dowell and Eddy 's test for non - ambiguity are actually non - ambiguous .", "label": "", "metadata": {}, "score": "68.078384"}
{"text": "The Multi state models multiloops , using a bifurcation to LMulti and RMulti .Tables 2 , 3 , 4 also refer to probabilistic parameters used by the models .All of the above parameters were automatically estimated from training data by the dart software .", "label": "", "metadata": {}, "score": "68.173874"}
{"text": "PhD thesis University of Bielefeld 2004 .Copyright .\u00a9 Reeder et al .2005 .This article is published under license to BioMed Central Ltd.Affiliated with .Abstract .Background .Pairwise stochastic context - free grammars ( Pair SCFGs ) are powerful tools for evolutionary analysis of RNA , including simultaneous RNA sequence alignment and secondary structure prediction , but the associated algorithms are intensive in both CPU and memory usage .", "label": "", "metadata": {}, "score": "68.20187"}
{"text": "Memory is the limiting factor in pairwise RNA alignment , and the primary motivation for constraints .For example , without constraints , alignment of two 16S ribosomal subunits using the stemloc grammar would take approximately 500 terabytes .( Using fold envelope constraints with structures fully specified , it can be done in under 5 gigabytes . )", "label": "", "metadata": {}, "score": "68.26339"}
{"text": "To add , there must be given a state that differs from a current state in the state set .It does not evaluate \u03c1 or \u03c3 , but instead marks the ending in the existing state set , thereby noting it has been added until the processor can later look up the probability and find the maximum .", "label": "", "metadata": {}, "score": "68.344376"}
{"text": "The explanation of the difference in effect lies with the degree of ambiguity .The degree of ambiguity of a given annotation sequence is the number of its derivations , i.e. a degree of 1 means that this annotation sequence is not ambiguous .", "label": "", "metadata": {}, "score": "68.457954"}
{"text": "One of the newly created states needs the terminal \" sleep \" and one needs terminal \" leap \" , both of which have the same beginning and end probabilities of -0.03 .There it sees both \" sleep \" and \" leap \" , but \" sleep \" begins at frame 2 and \" leap \" begins at frame 3 .", "label": "", "metadata": {}, "score": "68.55164"}
{"text": "( c ) inputting a lexicon having entries for defining terminal symbols of said at least one grammar in terms of linguistic , syntactic or semantic features ; .( d ) generating a matrix of state sets ; .( e ) initializing said state sets ; .", "label": "", "metadata": {}, "score": "68.78305"}
{"text": "For each start symbol of grammar , the processor predicts the current frame 's initial and final probability as 0.0 .The processor then parses , given the start frame , the state set , and the level .In the parse algorithm , the processor inputs the matrix of state sets , a level 1 , a frame index i , and outputs an extra state for the next input frame request i+1 .", "label": "", "metadata": {}, "score": "68.947296"}
{"text": "2005 , 33 , W605-W610 .[ Google Scholar ] .Boyle , J. ; Robillard , G.T. ; Kim , S. Sequential Folding of Transfer RNA .J. Mol .Biol .[ Google Scholar ] .Meyer , I. ; Miklos , I. Co - transcriptional folding is encoded within RNA genes .", "label": "", "metadata": {}, "score": "68.96153"}
{"text": "At this point , the processor initializes all of the state sets to empty at all levels of the first frame and sets the initial probability to 0.0 which is a logarithmic probability .The processor sets the level to 0 ( starts at the sentence grammar level ) , and sets the frame to 0 .", "label": "", "metadata": {}, "score": "68.96724"}
{"text": "At step 208 , the task - independent corpus is queried based on the query formulated .The particular information retrieval technique used to generate and execute the query against the task - independent corpus is not critical to this feature of the present invention .", "label": "", "metadata": {}, "score": "69.12636"}
{"text": "We are also considering ways of elaborating the grammar to include basepair stacking terms .These and other improvements we hope to address in future work .Conclusion .RNA sequence analysis has generated considerable interest over recent years , as many new roles for RNA in the cell have come to light .", "label": "", "metadata": {}, "score": "69.16561"}
{"text": "The method for parsing of claim 25 , wherein said grammar incorporates stochastic unification grammars .The method of parsing of claim 25 , wherein said probability score for said completed state is the probability for completing states in the state set using already complete states in the state sets .", "label": "", "metadata": {}, "score": "69.25668"}
{"text": "To hypothesize , the processor takes a terminal symbol b from a state needed to advance a given state or a set of states .For all terminals \" b \" at a given level 1 , the processor computes a new probability based on the ending probability \u03c1 of the state , \u03c1 ' .", "label": "", "metadata": {}, "score": "69.293655"}
{"text": "Concerning the other grammars we note that adding length - dependency significantly improved the results on tRNA while they became worse on the mixed set .A possible explanation for these results could be that the correct parameters for the folding are different for different types of RNA .", "label": "", "metadata": {}, "score": "69.36479"}
{"text": "Cells of this inner array are further sub - indexed by nonterminal U using a standard fixed - length array , yielding ( i , j , k , l ) .This particular configuration is efficient when the alignment envelope is densely populated and the fold envelopes are sparsely populated .", "label": "", "metadata": {}, "score": "69.40102"}
{"text": "93 - 120 , Addison - Wesley , 1983 .37 , pp .14719 - 14735 , 1998 .[ 25 ] D.H. Mathews , J. Sabina , M. Zuker , and D.H. Turner , \" Expanded Sequence Dependence of Thermodynamic Parameters Improves Prediction of RNA Secondary Structure , \" J. Molecular Biology , vol .", "label": "", "metadata": {}, "score": "69.40738"}
{"text": "The Outside algorithm , together with the Inside , can be used to recover posterior probabilities of given basepairs / columns , which can be used as alignment reliability indicators or as update counts in Expectation Maximization parameter training [ 20 , 26 ] .", "label": "", "metadata": {}, "score": "69.48453"}
{"text": "Affiliated with .Abstract .Background .Ambiguity is a problem in biosequence analysis that arises in various analysis tasks solved via dynamic programming , and in particular , in the modeling of families of RNA secondary structures with stochastic context free grammars .", "label": "", "metadata": {}, "score": "69.54487"}
{"text": "Background .The ambiguity problem in biosequence analysis .Biosequence analysis problems are typically optimization problems - we seek the best alignment of two protein sequences under a similarity score , or the most stable secondary structure of an RNA molecule under a thermodynamic model .", "label": "", "metadata": {}, "score": "69.62988"}
{"text": "Referring to .FIG .4 , a unified language model 140 includes a combination of an N - gram language model 142 and a plurality of context - free grammars 144 .Specifically , the N - gram language model 142 includes at least some of the same non - terminals of the plurality of context - free grammars 144 embedded therein such that in addition to predicting words , the N - gram language model 142 also can predict non - terminals . where ( h 1 , h 2 , . . .", "label": "", "metadata": {}, "score": "69.69868"}
{"text": "Thus we have to distinguish ( in the grammar ) between terminal symbols representing unpaired bases and terminal symbols being part of a base pair .In the above grammars the former are denoted by the symbol b while the latter are denoted by pairs a , \u00e2 .", "label": "", "metadata": {}, "score": "69.73791"}
{"text": "In one embodiment , the acoustic model 112 includes a senone tree associated with each Markov state in a Hidden Markov Model .The Hidden Markov models represent , in one illustrative embodiment , phonemes .The tree search engine 114 also accesses the lexicon stored in module 110 .", "label": "", "metadata": {}, "score": "69.79378"}
{"text": "View Article PubMed .Holmes I : A probabilistic model for the evolution of RNA structure .BMC Bioinformatics 2004 . , 5 ( 166 ) : .Griffiths - Jones S , Bateman A , Marshall M , Khanna A , Eddy SR : Rfam : an RNA family database .", "label": "", "metadata": {}, "score": "69.93594"}
{"text": "When it has done all the predicting and completing it can do , it sees that it needs a verb indicated by v. There are two verb rules , so it predicts them .One is \" sleep \" and the other is \" leap \" , both beginning at frame 2 and having initial and final probabilities of -0.04 .", "label": "", "metadata": {}, "score": "70.19261"}
{"text": "The filled cells in the rectangular grid show the aligned nucleotides .Note that the co - ordinates ( i , j , k , l ) lie on the grid - lines between the nucleotides .Bifurcation rules allow a subsequence - pair ( X ij , Y kl ) to be composed from two adjoining subsequence - pairs ( X i m , Y kn ) and ( X nj , Y ni ) .", "label": "", "metadata": {}, "score": "70.29642"}
{"text": "A single derivation exists in G for a compatible RNA sequence x , and hence , G is semantically non - ambiguous .Non - ambiguity proof .As stated above , this question is undecidable in general .However , compiler technology provides a partial proof procedure : If a deterministic parser can be generated for a grammar , then it is non - ambiguous [ 5 ] .", "label": "", "metadata": {}, "score": "70.3645"}
{"text": "Both forms of language processing can benefit from a language model .One common technique of classifying is to use a formal grammar .The formal grammar defines the sequence of words that the application will allow .One particular type of grammar is known as a \" context - free grammar \" ( CFG ) , which allows a language to be specified based on language structure or semantically .", "label": "", "metadata": {}, "score": "70.390305"}
{"text": "The feature extraction module 106 divides the digital signal received from the A / D converter 104 into frames that include a plurality of digital samples .Each frame is approximately 10 milliseconds in duration .The frames are then encoded by the feature extraction module 106 into a feature vector reflecting the spectral characteristics for a plurality of frequency bands .", "label": "", "metadata": {}, "score": "70.40073"}
{"text": "Each such derivation can uniquely be represented as a derivation tree , and if the same terminal string has two different derivation trees , the grammar is called ambiguous .Our first example is Dowell and Eddy 's grammar G 1 [ 1 ] to describe RNA secondary structures : .", "label": "", "metadata": {}, "score": "70.414246"}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a unified language model for a selected application , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . building a word language model from a corpus ; and .", "label": "", "metadata": {}, "score": "70.44544"}
{"text": "Table 1 summarizes the results for grammars G 1 to G 6 .For G 1 and G 2 , the results only show that both grammars are not LR ( 1 ) , LR ( 2 ) or LR ( 3 ) .", "label": "", "metadata": {}, "score": "70.4949"}
{"text": "Here , o expects a dot - bracket string as its left argument , a function as its right argument , and applies the latter to the former .( ) \" for the symbol sequence \" agu \" .( ) \" , \" ... \" , \" ... \" , etc . ] , where the duplicate entries result from the ambiguity of G 1 .", "label": "", "metadata": {}, "score": "70.58664"}
{"text": "The present invention then enables the chart processor to predict all start symbols and parse for all input frames .The parse function requires that the processor alternately repeat a predict function and a complete function until no new states can be added .", "label": "", "metadata": {}, "score": "70.63114"}
{"text": "View Article PubMed .Holmes I , Durbin R : Dynamic programming alignment accuracy .Journal of Computational Biology 1998 , 5 ( 3 ) : 493 - 504 .View Article PubMed .Do CB , Brudno M , Batzoglou S : PROBCONS : Probabilistic Consistency - based Multiple Alignment of Amino Acid Sequences . , in press .", "label": "", "metadata": {}, "score": "70.64334"}
{"text": "We construct a scoring scheme \u03c3 for grammar G such that G ( \u03c3 , x ) computes all derivation trees for x .For each production \u03c0 we use a unique tree label T \u03c0 .By construction , G ( \u03c3 , x ) constructs the list of all derivation trees for x .", "label": "", "metadata": {}, "score": "70.65313"}
{"text": "These include the homology modeling program INFERNAL [ 37 ] and the de novo structure prediction program PFOLD [ 8 ] .Declarations .Acknowledgements .The author thanks Sean Eddy for inspiring discussions and three anonymous reviewers for their helpful suggestions .", "label": "", "metadata": {}, "score": "70.66256"}
{"text": "By the term syntactic ambiguity we denote the fact that typically an RNA sequence has many secondary structures , i.e. annotation sequences , hence many derivations .Figure 1 shows two example annotation sequences of the same RNA sequence .Semantic ambiguity exists when there are , for some sequence , several derivations that represent the same annotation sequence , and hence , the same secondary structure .", "label": "", "metadata": {}, "score": "70.804695"}
{"text": "View Article .Chomsky N : On Certain Formal Properties of Grammars .Information and Control 1959 , 2 : 137 - 167 .View Article .Holmes I : Studies in probabilistic sequence alignment and evolution .PhD thesis The Sanger Centre 1998 .", "label": "", "metadata": {}, "score": "71.282"}
{"text": "Ambiguity reduction .Paired bases can always also be unpaired - this creates the syntactic ( good ) ambiguity .Used in concert , they create the \" good \" ambiguity that allows us to parse \" CAAAG \" either as \" ( ... ) \" or as \" ..... \" .", "label": "", "metadata": {}, "score": "71.32884"}
{"text": "If desired , the identified text can be extracted , copied or otherwise stored separate from the task - independent corpus as an aid in isolating relevant text and providing easier processing .FIG .9 is a block diagram illustrating another aspect of the present invention .", "label": "", "metadata": {}, "score": "71.65181"}
{"text": "Results .To investigate the comparative resource usage of the various different kinds of constraint that can be applied using fold and alignment envelopes , stemloc was tested on 22 pairwise alignments taken from version 6.1 of RFAM [ 37 ] , spanning 7 different families of functional noncoding RNA .", "label": "", "metadata": {}, "score": "71.67938"}
{"text": "Other pairs are less stable and hence less common ) , thus folding the molecule to a complex three - dimensional layout called the tertiary structure .As determining the tertiary structure is computationally complex , it has proven convenient to first search for the secondary structure , for which only a subset of the hydrogen bonds is considered , so that the molecule can be modeled as a planar graph .", "label": "", "metadata": {}, "score": "71.785095"}
{"text": "The cycle produces some terminal symbols that are hypothesized at the next lower grammar level .The processor hypothesizes the terminal symbols from this level as start symbols at the next lower grammar level .It returns a set of observations which are scanned into the waiting states .", "label": "", "metadata": {}, "score": "71.9044"}
{"text": "[ Google Scholar ] .Andersen , E.S. Prediction and design of DNA and RNA structures .New Biotechnol .[ Google Scholar ] .Xayaphoummine , A. ; Bucher , T. ; Isambert , H. Kinefold web server for RNA / DNA folding path and structure prediction including pseudoknots and knots .", "label": "", "metadata": {}, "score": "72.00063"}
{"text": "Dowell R , Eddy SR : Evaluation of several lightweight stochastic context - free grammars for RNA secondary structure prediction .BMC Bioinformatics 2004 , 5 : 71 .View Article PubMed .Wuchty S , Fontana W , Hofacker I , Schuster P : Complete Suboptimal Folding of RNA and the Stability of Secondary Structures .", "label": "", "metadata": {}, "score": "72.04579"}
{"text": "ambiguous .In Table 1 we also report on the number of conflicts found by the parser generator for increasing values of k .While the nature of these conflicts is not relevant for us , the table shows that various behaviors are possible .", "label": "", "metadata": {}, "score": "72.09703"}
{"text": "For example when starting transcription at the marked end the structure from Figure 1 would be denoted by the word .The oldest and most commonly used method for computing the secondary structure is to determine the structure with minimum free energy .", "label": "", "metadata": {}, "score": "72.194786"}
{"text": "View Article PubMed .Austern MH : Generic Programming and the STL : Using and Extending the C++ Standard Template Library Addison - Wesley 1999 .Smith TF , Waterman MS : Identification of Common Molecular Subsequences .Journal of Molecular Biology 1981 , 147 : 195 - 197 .", "label": "", "metadata": {}, "score": "72.26758"}
{"text": "In the case of an N - gram language model or a hybrid N - gram language model , step 212 will commonly require use of an N - gram algorithm .FIG .8 illustrates a method 220 similar to the method 200 of .", "label": "", "metadata": {}, "score": "72.26978"}
{"text": "5 is a graph showing the effect of chart parsing on pruning as employed by the present invention .DETAILED DESCRIPTION OF A PREFERRED EMBODIMENT .The present invention discloses a method which makes use of an algorithm ( discussed below ) which includes a parsing subalgorithm to affect the central data structure of a spoken language processor .", "label": "", "metadata": {}, "score": "72.3119"}
{"text": "This is what the processor needs to see to indicate it has seen B 3 . \u03b5 is a string of 0 or more terminals and nonterminals In this case the ending frame is also the current frame because the processor has not processed anything .", "label": "", "metadata": {}, "score": "72.33665"}
{"text": "The two noun hypotheses ( \" boys \" from frame 0 to frame 2 and \" boys \" from frame 0 to frame 3 ) are non - intersecting hypotheses since they have different stop times , and therefore remain separate .", "label": "", "metadata": {}, "score": "72.3947"}
{"text": "The following gives , as we will show afterwards , a sufficient condition that they do .Definition 5 .Intuitively to satisfy this condition we may not group lengths i and j together if there is a rule that can lead to a word of length i but not to one of length j ( or vice versa ) .", "label": "", "metadata": {}, "score": "72.48532"}
{"text": "In RNA normal form , the emission rules ( Q 2 ) account for homologous base - pairings between residues ( X i , X j ) and ( Y k , Y l ) , or unpaired residues at X i , X j , Y k or Y l .", "label": "", "metadata": {}, "score": "72.53239"}
{"text": "T .i . w .A .R .w .T .i . otherwise .and .i .P .i . )f .i .T . )T .Proof .We show that .", "label": "", "metadata": {}, "score": "72.66237"}
{"text": "Step 162 represents obtaining context - free grammars having non - terminal tokens to represent the semantic or syntactic concepts in the task - independent corpus , the non - terminal tokens having terminals present in the task - independent corpus .", "label": "", "metadata": {}, "score": "72.9452"}
{"text": "These fold envelopes ( triangular grids ) and alignment envelope ( rectangular grid ) limit the subsequences ( black dots ) and cutpoints ( short diagonal lines ) to those consistent with a given alignment and consensus secondary structure ( shown ) .", "label": "", "metadata": {}, "score": "72.961136"}
{"text": "The bottom line represents the best plus some predetermined log probability amount , or threshold .With other algorithms , if the probability of a symbol drops below that threshold at any point , it is discarded .Furthermore , when the best probability symbol completes , the parser will then associate the lower probability symbols with their starting states .", "label": "", "metadata": {}, "score": "73.01453"}
{"text": "View Article PubMed .Knudsen B , Hein J : RNA secondary structure prediction using stochastic context - free grammars and evolutionary history .Bioinformatics 1999 , 15 ( 6 ) : 446 - 454 .View Article PubMed .Rivas E , Eddy SR : Noncoding RNA gene detection using comparative sequence analysis .", "label": "", "metadata": {}, "score": "73.078125"}
{"text": "As they all emerge from the same terminal string acaggaaacuguacggugcaaccg , this grammar is ambiguous .Four derivation trees .Four derivation trees for RNA sequence \" acaggaaacuguacggugcaaccg \" , two ( left ) representing the annotation sequence ( ( ( ( .... ) ) ) ) .", "label": "", "metadata": {}, "score": "73.12727"}
{"text": "The system for recognizing a spoken sentence of claim 11 , further comprising a knowledge base for supplying symbols , wherein said predictor is coupled to said knowledge base .The system for recognizing a spoken sentence of claim 11 , wherein said language model incorporates stochastic unification grammars .", "label": "", "metadata": {}, "score": "73.202354"}
{"text": "The good one is that there are many solutions to choose from .The bad one is that our algorithm may find the same solution several times , or even worse , it may study seemingly different solutions , which in fact represent the same object of interest .", "label": "", "metadata": {}, "score": "73.27493"}
{"text": "Mandal M , Boese B , Barrick JE , Winkler WC , Breaker RR : Riboswitches Control Fundamental Biochemical Pathways in Bacillus subtilis and Other Bacteria .Cell 2003 , 113 : 577 - 586 .View Article PubMed .Sijen T , Plasterk RH : Transposon silencing in the Caenorhabditis elegans germ line by natural RNAi .", "label": "", "metadata": {}, "score": "73.30107"}
{"text": "These are suitable to describe the pseudoknot - free secondary structure of RNA .When considering pseudoknots , context - sensitive grammars are needed .Context free grammars .Starting with the axiom symbol , by successive replacement of nonterminal symbols by right - hand sides of corresponding productions , we can derive a set of terminal strings .", "label": "", "metadata": {}, "score": "73.602745"}
{"text": "A fourth aspect is a method for creating a language model for a selected application from a task - independent corpus .The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .", "label": "", "metadata": {}, "score": "73.70145"}
{"text": "Thus , grammar G 1 is syntactically as well as semantically ambiguous .Semantic ambiguity is the \" bad \" , syntactic ambiguity the \" good \" type of ambiguity in SCFG modeling and dynamic programming that was mentioned above .On the pure formal language level , they can not be distinguished - both are manifest as fl - ambiguity .", "label": "", "metadata": {}, "score": "74.22948"}
{"text": "In a network environment , program modules depicted relative to the personal computer 50 , or portions thereof , can be stored in the remote memory storage devices .As appreciated by those skilled in the art , the network connections shown are exemplary and other means of establishing a communications link between the computers can be used .", "label": "", "metadata": {}, "score": "74.277374"}
{"text": "Select the highest - scoring of the pairwise ( marked - to - unmarked ) alignments .Use this alignment to merge the unmarked sequence into the seed alignment , and mark this sequence as newly aligned .Return the seed alignment .", "label": "", "metadata": {}, "score": "74.40586"}
{"text": "Prior to a detailed discussion of the present invention , an overview of an operating environment may be helpful .FIG .2 and the related discussion provide a brief , general description of a suitable computing environment in which the invention can be implemented .", "label": "", "metadata": {}, "score": "74.71582"}
{"text": "Once it has a complete noun from frame 0 to frame 2 , it checks if there are any symbols that ended at zero that needed a noun that it can complete and finds there was one starting with S in the very first state .", "label": "", "metadata": {}, "score": "74.79053"}
{"text": "Thus , the feature extraction module 106 provides , at its output the feature vectors ( or code words ) for each spoken utterance .The feature extraction module 106 provides the feature vectors ( or code words ) at a rate of one feature vector or ( code word ) approximately every 10 milliseconds .", "label": "", "metadata": {}, "score": "74.85461"}
{"text": "Each internal node is labeled with a nonterminal ( Stem or Loop ) ; additionally , the subsequences ( X ij , Y kl ) generated by each internal node are shown .The parse tree determines both the structure and alignment of the two sequences .", "label": "", "metadata": {}, "score": "74.86025"}
{"text": "In addition , those skilled in the art will appreciate that the invention can be practiced with other computer system configurations , including hand - held devices , multiprocessor systems , microprocessor - based or programmable consumer electronics , network PCs , minicomputers , mainframe computers , and the like .", "label": "", "metadata": {}, "score": "74.91397"}
{"text": "( j ) generating a probability score for each said completed state ; .( k ) repeating steps ( h ) to ( j ) until no new states can be created ; .( l ) parsing terminal symbols from the current grammar level as start symbols for the next lower grammar level unless at the lowest grammar level ; .", "label": "", "metadata": {}, "score": "74.94897"}
{"text": "The particular RNA normal form described in this section is chosen for ease of presentation .The implementation in the dart library uses the slightly more restrictive form for Pair SCFGs defined in an earlier paper [ 13 ] .For presentational purposes , we will generally omit all - gap columns from the pairwise alignment and the grammar .", "label": "", "metadata": {}, "score": "75.010025"}
{"text": "His abstract is at the Logic and AI Seminar Series , along with links to relevant papers .See Noah 's slides plus a very nice handout containing mathematical details .( Note : you might need texpoint , the tool that lets you put latex equations into Powerpoint slides , to view the formulas properly . )", "label": "", "metadata": {}, "score": "75.14099"}
{"text": "This highest - scoring pair is called the seed alignment .While some sequences remain unmarked : .For each newly - marked sequence : .Align the marked sequence , with a fold envelope constrained by its predicted structure , to each unmarked sequence in turn .", "label": "", "metadata": {}, "score": "75.289566"}
{"text": "In the embodiment illustrated in .FIG .9 , block 240 represents the context - free grammars obtained ( for example , authored by the developer ) for the selected task or application .The context - free grammars are used to generate synthetic data or word phrases 242 in a manner similar to step 204 of methods 200 and 220 .", "label": "", "metadata": {}, "score": "75.33168"}
{"text": "2 .Furthermore , the tree search engine 114 is implemented in processing unit 51 ( which can include one or more processors ) or can be performed by a dedicated speech recognition processor employed by the personal computer 50 .In the embodiment illustrated , during speech recognition , speech is provided as an input into the system 100 in the form of an audible voice signal by the user to the microphone 92 .", "label": "", "metadata": {}, "score": "75.340775"}
{"text": "At step 186 , each of the identified word occurrences for non - terminals representing concepts which are of interest to the target application is replaced with the corresponding non - terminal token as defined by the corresponding context - free grammar .", "label": "", "metadata": {}, "score": "75.43871"}
{"text": "The construction of these rules guarantees , that every derivation of a multiloop must lead to at least two closed substructures .One of these derivations is shown on the left side of Figure 2 .Therefore , a derivation of a multiloop can by no means conflict with a derivation of a left bulge , which must include a single closed substructure .", "label": "", "metadata": {}, "score": "75.55247"}
{"text": "The dot above that refers to the place in the rule or how far one has progressed through the rule which , at this point , is none .The numbers at the end of the rule represent the logarithmic probabilities of the initial and final probabilities , respectively .", "label": "", "metadata": {}, "score": "75.59482"}
{"text": "Step 224 then includes replacing each of the identified word occurrences with corresponding non - terminal tokens for selected non - terminals ( i.e. excluding the non - terminals which may have been introduced to prevent mistakes during parsing ) .Step 212 would then include building an N - gram model having non - terminal tokens .", "label": "", "metadata": {}, "score": "75.787964"}
{"text": "Notation .To implement SCFG dynamic programming algorithms efficiently for RNA , it is convenient to define a simplified ( but universal ) template for grammars , similar in principle to \" Chomsky normal form \" [ 29 , 30 ] .", "label": "", "metadata": {}, "score": "75.89381"}
{"text": "A more compact representation of a secondary structure is the widely used dot - bracket notation , as shown at the bottom of Figure 1 .In the following , we will use the term annotation sequence for the dot - bracket string representing one secondary structure of the underlying RNA sequence .", "label": "", "metadata": {}, "score": "75.99402"}
{"text": "i 2 . . .u t .A CFG state constrains the possible words that can follow the history .u t .i 1 u t .i 2 . . .u t .The likelihood of observing u t .", "label": "", "metadata": {}, "score": "76.09451"}
{"text": "FIG .2 and accessible by the processing unit 51 or another suitable processor .In addition , the lexicon storage module 110 , the acoustic model 112 , and the language model 16 are also preferably stored in any of the memory devices shown in .", "label": "", "metadata": {}, "score": "76.19667"}
{"text": "This option is illustrated in dashed lines for block 264 and arrows 266 and 268 .Of course , if this option is chosen the identified text 210 would not be provided directly to the N - gram algorithm 248 , but rather to block 264 .", "label": "", "metadata": {}, "score": "76.3233"}
{"text": "DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram showing a speech recognition processor which employs the present invention ; .FIG .2 is a stack diagram demonstrating a possible grammar level structure and positioning as used by the present invention ; .", "label": "", "metadata": {}, "score": "76.5642"}
{"text": "Generally , program modules include routine programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .Tasks performed by the programs and modules are described below and with the aid of block diagrams and flow charts .", "label": "", "metadata": {}, "score": "76.768074"}
{"text": "Several non - ambiguous reference grammars for RNA are known - the critical part here is to assure that our grammar G to be tested describes the same language as R .Both grammars must impose the same restrictions on loop sizes , lonely base pairs , etc .", "label": "", "metadata": {}, "score": "76.88898"}
{"text": "There must be no transition - termination path from V or W to \u03b5 , i.e. neither V or W can have completely empty inside sequence - pairs ( see next section for a formal definition of the \" inside sequence - pair \" ) .", "label": "", "metadata": {}, "score": "77.21523"}
{"text": "Grammar G 3 is LR ( 5 ) and G 4 to G 6 are LR ( 1 ) .Therefore , we have proved mechanically that the four \" good \" grammars studied by Dowell and Eddy are definitely non - ambiguous .", "label": "", "metadata": {}, "score": "77.22206"}
{"text": "The present invention relates to language modeling .More particularly , the present invention relates to creating a language model for a language processing system .Accurate speech recognition requires more than just an acoustic model to select the correct word spoken by the user .", "label": "", "metadata": {}, "score": "77.52852"}
{"text": ", each class of similar secondary structures is represented by one shape and the native structures can be found among the top shape representatives .In this article , we derive some interesting results answering enumeration problems for abstract shapes and secondary structures of RNA .", "label": "", "metadata": {}, "score": "77.67287"}
{"text": "Ambiguity can have many sources .Here , we present three common situations that lead us to write ambiguous rules , but can be easily avoided .The price for non - ambiguity is the new nonterminal symbol L , more parameters in the training set , and possibly another DP table in the implementation .", "label": "", "metadata": {}, "score": "77.68176"}
{"text": "The system for parsing of claim 20 , wherein the chart comprises states and state sets , said states to be manipulated by said parser and said predictor .The system for parsing of claim 20 , further comprising a knowledge base coupled to said predictor for supplying symbols and appropriate operating data .", "label": "", "metadata": {}, "score": "77.698044"}
{"text": "Due to co - transcriptional folding one would expect that the probability of two bases being paired depends on how far the bases are apart , and the probability of a part of the molecule forming a specific motif depends on how large the part of the molecule is .", "label": "", "metadata": {}, "score": "77.73656"}
{"text": "Eventually the sequence will contain only terminals from \u03a8. This process generates a parse tree , rooted at node S , in which internal nodes are labeled with nonterminals and leaf nodes with terminals , with children of each node ordered left - to - right ( Figure 1 ) .", "label": "", "metadata": {}, "score": "77.74689"}
{"text": "All - gap columns are not very interesting to a sequence analyst , and only arise in our formalism because all emission rules have the same form .Table 1 is an example of an RNA normal form grammar with two nonterminals , Stem and Loop .", "label": "", "metadata": {}, "score": "77.83003"}
{"text": "Authors ' contributions .IH designed , programmed , tested and documented the algorithms .Authors ' Affiliations .Department of Bioengineering , University of California .References .Eddy SR : Noncoding RNA genes .Current Opinion in Genetics and Development 1999 , 9 ( 6 ) : 695 - 699 .", "label": "", "metadata": {}, "score": "77.88832"}
{"text": "the size of a motif ) is just the size of the subword that results from the rule application introducing the base pair as first and last symbol ( resp .starting building of the motif ) , assuming such a rule application exists .", "label": "", "metadata": {}, "score": "78.177505"}
{"text": "The alignment envelope containing the N best primary sequence alignments , with the unconstrained fold envelopes ( stemloc options : ' --nalign N --nfold -1 ' ) .This is the red curve in Figures 8 , 9 , 10 , 11 , 12 , 13 .", "label": "", "metadata": {}, "score": "78.23511"}
{"text": "FIGS .5 - 8 are flow charts for different aspects of the present invention .FIG .9 is a block diagram of another aspect of the present invention .DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS .FIG .1 generally illustrates a language processing system 10 that receives a language input 12 and processes the language input 12 to provide a language output 14 .", "label": "", "metadata": {}, "score": "78.27577"}
{"text": "For RNA secondary structures , there is an obvious choice , our annotation sequences in the widely used dot - bracket notation ( cf .Figure 1 ) .Each secondary structure ( excluding pseudoknots ) is uniquely represented by such a string .", "label": "", "metadata": {}, "score": "78.38486"}
{"text": "FIG .4 is an example of a typical left recursive rule showing rule probabilities , using a treatment of conjunctions and disjunctions such as \" a or b or d \" .As seen in portion A of FIG .4 , the example shown has the four terminal symbols : a , b , d and \" or \" .", "label": "", "metadata": {}, "score": "78.547356"}
{"text": "The Outside algorithm calculates intermediate probabilities of the form . representing the sum - over - probabilities of all partial parse trees rooted at S and ending in V without having yet generated sequences X ij and Y kl .Then , for example , the posterior probability that some node V in the parse tree has inside sequence - pair ( X ij , Y kl ) is .", "label": "", "metadata": {}, "score": "78.7246"}
{"text": "Below that is word grammar level 1 , and below that is phoneme grammar level 2 .The next lower level shown is phone grammar level 3 .Each lower level contains narrower and narrower portions of the inputted data until the lowest level or reference frame grammar level 1 is reached .", "label": "", "metadata": {}, "score": "78.99341"}
{"text": "Topics : .Mehryar Mohri and Michael Riley ( 2002 ) .Tutorial on Weighted Finite - State Transducers in Speech Recognition , ( Part 1 ) , International Conference on Spoken Language Processing 2002 ( ICSLP ' 02 ) , Denver , Colorado , September 2002 .", "label": "", "metadata": {}, "score": "79.145996"}
{"text": "A formal language is a subset of the set of all strings over a finite alphabet .Formal languages are typically described by formal grammars .In general , a formal grammar consists of an alphabet , a set of nonterminal symbols , and a set of production rules .", "label": "", "metadata": {}, "score": "79.333954"}
{"text": "Biol .[ Google Scholar ] .Harrison , M.A. Introduction to Formal Language Theory ; Addison - Wesley : Boston , MA , USA , 1978 .[ Google Scholar ] .Durbin , R. ; Eddy , S.R. ; Krogh , A. ; Mitchison , G. Biological Sequence Analysis ; Cambridge University Press : Cambridge , UK , 1998 .", "label": "", "metadata": {}, "score": "79.400116"}
{"text": "The system for recognizing a spoken sentence of claim 11 , further comprising a means for generating a chart , wherein the chart is accessed by said parser , said predictor , and said completer for storing intermediate results .The system for recognizing a spoken sentence of claim 12 , wherein the chart comprises states and state sets , said states to be manipulated by said parser and said predictor .", "label": "", "metadata": {}, "score": "79.496925"}
{"text": "Conclusion .A program , Stemloc , that implements these algorithms for efficient RNA sequence alignment and structure prediction is available under the GNU General Public License .Background .As our acquaintance with RNA 's diverse functional repertoire develops [ 1 - 5 ] , so does demand for faster and more accurate tools for RNA sequence analysis .", "label": "", "metadata": {}, "score": "79.78308"}
{"text": "Three main conclusions can be drawn from these data .First , allowing the search to consider more than a single alignment greatly improves structure prediction ( the red curve ) .Second , constraining the alignment search while exhaustively scanning fold space ( the red curve ) outperforms constraining the fold search while exhaustively scanning alignment space ( the green curve ) .", "label": "", "metadata": {}, "score": "79.87273"}
{"text": "Their training set consists of 139 each large and small subunit rRNAs , the benchmark dataset contains 225 RNase Ps , 81 SRPs and 97 tmRNAs .Since it contains different types of RNA we will refer to this set as the mixed set for the remainder of this article .", "label": "", "metadata": {}, "score": "80.00226"}
{"text": "Figure 1 .Example of an RNA secondary structure .Letters represent bases , the colored band marks the phosphordiester bonds , short edges mark hydrogen bonds .( The different colors only serve to identify the corresponding parts in the formal language represantation below . )", "label": "", "metadata": {}, "score": "80.17294"}
{"text": "etc .This type of grammar does not require an in - depth knowledge of formal sentence structure or linguistics , but rather , a knowledge of what words , phrases , sentences or sentence fragments are used in a particular application or task .", "label": "", "metadata": {}, "score": "80.556885"}
{"text": "For example , the fundamental time unit may be 20 milliseconds .This means that every 20 milliseconds the processor will characterize speech data with a vector of length 18 or so , floating point features of various characteristics of the speech signal and will match those characteristics to the expected data or symbols which correspond to words .", "label": "", "metadata": {}, "score": "80.63524"}
{"text": "As technology advances and speech and handwriting recognition is provided in more applications , the application developer must be provided with an efficient method in which an appropriate language model can be created for the selected application .SUMMARY OF THE INVENTION .", "label": "", "metadata": {}, "score": "80.80951"}
{"text": "For example , a person 's average speech input is 4 - 5 seconds long , which corresponds to 400 - 500 frames .When cubed , those 400 - 500 frames yield 64,000,000 - 125,000,000 processing steps to recognize an input .", "label": "", "metadata": {}, "score": "81.3378"}
{"text": "The central nonterminal of the grammar is CL , which splits up into closed structures like hairpin loops , bulges , and multiloops .Due to the necessity to handle dangling bases in a non - ambiguous way , the rules for multiloops are the most complicated of this grammar .", "label": "", "metadata": {}, "score": "81.39506"}
{"text": "l .c .l . where c \u03b1 , l is the number of different assignments of lengths to the symbols of \u03b1 that satisfy : .Terminals are always assigned a length of 1 .Words are generated as for usual context - free grammars .", "label": "", "metadata": {}, "score": "81.440155"}
{"text": "N -best folds , all alignments .The unconstrained alignment envelope , with the fold envelopes containing the N best single - sequence structure predictions ( stemloc options : ' --nalign -1 --nfold N ' ) .This is the green curve in Figures 8 , 9 , 10 , 11 , 12 , 13 .", "label": "", "metadata": {}, "score": "81.46939"}
{"text": "Then the processor predicts and completes grammar rules for the words which are composed of phonemes and when it finds a set of phonemes which are terminal symbols , it then calls itself at grammar level 2 which has phoneme symbols as start symbols .", "label": "", "metadata": {}, "score": "81.83188"}
{"text": "The language processing system 10 processes the spoken language and provides as an output , recognized words typically in the form of a textual output .During processing , the speech recognition system or module 10 can access a language model 16 in order to determine which words have been spoken .", "label": "", "metadata": {}, "score": "82.1112"}
{"text": "claim 6 and having instructions further comprising : . storing the identified text of the task independent corpus separate from the task independent corpus .The system of .claim 9 wherein building the second N - gram language model includes using only the identified text .", "label": "", "metadata": {}, "score": "82.17575"}
{"text": "Keywords : . stochastic context - free grammar ; length - dependency ; RNA secondary structure prediction .Introduction .Single - stranded RNA molecules consist of a sequence of nucleotides connected by phosphordiester bonds .Nucleotides only differ by the bases involved , them being adenine , cytosine , guanine and uracil .", "label": "", "metadata": {}, "score": "82.177185"}
{"text": "The only difference is that \" complete \" deals with nonterminal symbols and \" scan \" deals with terminal symbols .First the processor makes the state set at level 1 , frame i+1 , empty .Its probability is \u03c1 , that is the ending probability of that state plus the ending probability of the completed state minus the initial probability of the completed state .", "label": "", "metadata": {}, "score": "82.24124"}
{"text": "The alignment envelope containing the 100 best primary sequence alignments , with the fold envelopes containing the N best single - sequence structure predictions ( stemloc options : ' --nalign 100 --nfold N ' ) .This is the blue curve in Figures 8 , 9 , 10 , 11 , 12 , 13 .", "label": "", "metadata": {}, "score": "82.25331"}
{"text": "The system of .claim 6 wherein the language model is an N - gram language model .The system of .claim 7 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and . wherein building the N - gram language model comprises building a N - gram model having the non - terminal tokens .", "label": "", "metadata": {}, "score": "82.2799"}
{"text": "Example of an RNA secondary structure .Letters represent bases , the colored band marks the phosphordiester bonds , short edges mark hydrogen bonds .( The different colors only serve to identify the corresponding parts in the formal language represantation below . )", "label": "", "metadata": {}, "score": "82.511055"}
{"text": "As appreciated by those skilled in the art , the language model 16 can be used in other language processing systems besides the speech recognition system discussed above .For instance , language models of the type described above can be used in handwriting recognition , Optical Character Recognition ( OCR ) , spell - checkers , language translation , input of Chinese or Japanese characters using standard PC keyboard , or input of English words using a telephone keypad .", "label": "", "metadata": {}, "score": "82.606544"}
{"text": "The expressive power of a grammar type depends on these laws .In 1956 , Noam Chomsky introduced a hierarchy of formal grammars that ranks grammar types by their expressive power , the Chomsky hierarchy [ 4 ] .It consists of four levels : regular grammars , context - free grammars , context - sensitive grammars , and unrestricted grammars .", "label": "", "metadata": {}, "score": "82.65767"}
{"text": "Selecting appropriate fold and alignment envelopes .This section offers a non - exhaustive list of possible strategies for choosing appropriate fold/ alignment envelopes .( Italicized terms apply to fold envelopes , and bold terms to alignment envelopes . )Choose some appropriately simplified grammar , such as a single - sequence SCFG/ pair HMM that models RNA folding /primary sequence alignment .", "label": "", "metadata": {}, "score": "82.71831"}
{"text": "1 , an input device 10 receives input from a user and transmits the input along connecting element 12 to processor 14 .Processor 14 contains a central data structure , known as a chart 24 , not shown , where the algorithm is implemented .", "label": "", "metadata": {}, "score": "82.968735"}
{"text": "We supply the Inside matrix , I , as an input to the Outside algorithm ( they are usually calculated at the same time anyway ) .In terms of the underlying iteration , the key difference between the Inside and Outside algorithms is as follows .", "label": "", "metadata": {}, "score": "83.02965"}
{"text": "Consider the following annotation sequence : .Here , the string \" ( ( ... ( ( \" appears two times in the annotation sequence .The first appearance denotes a left bulge , the second the beginning of a multiloop .", "label": "", "metadata": {}, "score": "83.20385"}
{"text": "The nonterminal L is referred to as the left - hand side ( LHS ) of the production rule and the symbol sequence R as the right hand side ( RHS ) .The allowable forms for production rules include terminations , transitions , bifurcations and emissions .", "label": "", "metadata": {}, "score": "83.89609"}
{"text": "Top - down language processing begins with the largest unit of language to be recognized , such as a sentence , and processes it by classifying it into smaller units , such as phrases , which in turn , are classified into yet smaller units , such as words .", "label": "", "metadata": {}, "score": "84.03224"}
{"text": "Block 248 illustrates application of an N - gram algorithm to obtain the second N - gram language model 250 .A third N - gram language model 252 is formed by combining the first N - gram language model 246 and the second N - gram language model 250 .", "label": "", "metadata": {}, "score": "84.033005"}
{"text": "There exists no program that can determine for an arbitrary grammar G whether or not G is fl - ambiguous .Here , the problem is to decide whether a given SCFG is se - mantically ambiguous .It is not surprising that this problem is not easier : .", "label": "", "metadata": {}, "score": "84.087975"}
{"text": "Although described herein where the speech recognition system 100 uses HMM modeling and senone trees , it should be understood that this is but one illustrative embodiment .As appreciated by those skilled in the art , the speech recognition system 100 can take many forms and all that is required is that it uses the language model 16 and provides as an output the text spoken by the user .", "label": "", "metadata": {}, "score": "84.1051"}
{"text": "Syntactic versus semantic ambiguity .Above , we introduced the formal language - theoretic notion of ambiguity : if the same symbol sequence has two or more different derivation trees , the grammar is called ambiguous .For clarity , we will refer to it as fl - ambiguity .", "label": "", "metadata": {}, "score": "84.226456"}
{"text": "FIG .1 is a block diagram of a language processing system .FIG .2 is a block diagram of an exemplary computing environment .FIG .3 is a block diagram of an exemplary speech recognition system .FIG .", "label": "", "metadata": {}, "score": "84.5181"}
{"text": "The lexicon contains definitions of the terminal symbols of the grammar .These terminal grammar symbols are preferably word classification descriptors , such as verb , noun , and article , with syntactic and semantic information .The terms of the lexicon are assigned features , such as tense , plurality , or definiteness .", "label": "", "metadata": {}, "score": "84.65625"}
{"text": "The operator - overloading features of C++ are utilized in full , so that the syntax of initializing a grammar object involves very few function calls and is essentially declarative .dart source code releases can be downloaded under the terms of the GNU Public License , from the following URL ( which also gives access to the latest development code in the CVS repository ) .", "label": "", "metadata": {}, "score": "84.66467"}
{"text": "claim 1 wherein the plurality of context - free grammars include at least one context - free grammar having a non - terminal token for a phrase that can be mistaken for one of the desired task dependent semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "84.78229"}
{"text": "Schedule of Topics .This is the schedule of topics for an advanced seminar in natural language processing , focused on the foundations of cutting - edge methods in NLP .Foundations , as used here , are the things that the cutting - edge research papers assume you already understand .", "label": "", "metadata": {}, "score": "84.96622"}
{"text": "claim 15 wherein the word language model comprises an N - gram language model and wherein the corpus comprises a task independent corpus .The system of .claim 16 and having instructions further comprising : . generating word phrases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phrases ; . querying the task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and . wherein building a N - gram language model includes using the identified text .", "label": "", "metadata": {}, "score": "85.278114"}
{"text": "The system for recognizing a spoken sentence of claim 11 , wherein said processing means includes an input means for recording spoken words and an acoustic device for tranforming spoken words into a medium readable by said processing means .The system for recognizing a spoken sentence of claim 11 , wherein said processing means is coupled to a translating means adapted to receive spoken input and transform said input into a medium readable by said processing means .", "label": "", "metadata": {}, "score": "85.761154"}
{"text": "all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .Applying Length - Dependent Stochastic Context - Free Grammars to RNA Secondary Structure Prediction .", "label": "", "metadata": {}, "score": "86.24138"}
{"text": "All authors cooperated closely in writing the manuscript .Authors ' Affiliations .InternationaI NRW Graduate School of Bioinformatics and Genome Research , Center of Biotechnology ( CeBiTec ) , Bielefeld University .Practical Computer Science , Faculty of Technology , Bielefeld University .", "label": "", "metadata": {}, "score": "86.83513"}
{"text": "View Article PubMed .Ambros V : The functions of animal microRNAs .Nature 2004 , 431 ( 7006 ) : 350 - 355 .View Article PubMed .Baulcombe D : RNA silencing in plants .Nature 2004 , 431 ( 7006 ) : 356 - 363 .", "label": "", "metadata": {}, "score": "87.18288"}
{"text": "The method of claim 3 , wherein said probability score for said completed state is the probability for completing states in the state set using already complete states in the state sets .The method of claim 3 , wherein said score is calculated by summing the ending probability of the active state with the difference between the ending and initial probabilities of the complete state , wherein the active state is the state requiring the symbol which the complete state defines .", "label": "", "metadata": {}, "score": "87.30026"}
{"text": "P .w . )T .S .W . )P .W .T . )Although the present invention has been described with reference to preferred embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .", "label": "", "metadata": {}, "score": "87.36865"}
{"text": "CROSS - REFERENCE TO RELATED APPLICATION .The present application is a continuation of and claims priority of U.S. patent application Ser .No .09/585,298 , filed Jun. 1 , 2000 , the content of which is hereby incorporated by reference in its entirety .", "label": "", "metadata": {}, "score": "87.51068"}
{"text": "( We introduce the name \" KYC \" as a simple reversal of \" CYK \" , reflecting the fact that KYC is to CYK as Outside is to Inside , i.e. the \" reverse \" version of the algorithm . )", "label": "", "metadata": {}, "score": "87.81122"}
{"text": "The \" output device \" may be a screen , another processor , an audio speaker , a robotic arm , etc .In the implementation of the preferred emmbodiment of the present invention , the input is spoken , the input device is a microphone , the output and output device involve a processor response written to a screen .", "label": "", "metadata": {}, "score": "88.26781"}
{"text": "One dart program in particular , stemloc , is an efficient general - purpose RNA multiple - sequence alignment program that can be flexibly controlled by the user from the Unix command line , including re - estimation of parameters from training data as well as a broad range of alignment functions .", "label": "", "metadata": {}, "score": "88.895065"}
{"text": "Option 2 .Come up with ( do not look up ! ) an EM algorithm for estimating the parameters of this model .Implement your algorithm , train on a large sample of English , and compare the probabilities of \" colorless green ideas sleep furiously \" versus \" furiously sleep ideas green colorless \" .", "label": "", "metadata": {}, "score": "89.34924"}
{"text": "Since the parser has probabilites of -0.09 on state 13 and 0.06 on state 14 , it chooses the best one , which is -0.06 , and traces back through the parse state to find \" boys leap \" .This phrase will then be outputted as the speech recognition processor 's best explanation for the spoken input given .", "label": "", "metadata": {}, "score": "89.44552"}
{"text": "The EMBL accession numbers and co - ordinates of all sequences are listed in Table 5 .The table shows the performance of stemloc using the 1000-best fold envelope and the 100-best alignment envelope .The following three test regimes were used , each representing a different combination of fold and alignment envelopes : .", "label": "", "metadata": {}, "score": "89.74997"}
{"text": "Declarations .Acknowledgements .We are grateful to Jens Reeder for discussion and the anonymous referees for very detailed and helpful comments .Work of JR is funded by DFG Graduate College 635 Bioinformatik .Authors ' contributions .RG suggested the topic and contributed the undecid - ability proof .", "label": "", "metadata": {}, "score": "90.776"}
{"text": "Additional readings \" are optional links pointing either to material you should already know ( but might want to review ) or to related material you might be interested in .Some topic areas may take longer than expected , so keep an eye on the class mailing list or e - mail me for \" official \" dates .", "label": "", "metadata": {}, "score": "91.30349"}
{"text": "Amsterdam : North - Holland 1963 , 118 - 161 .View Article .Knuth D : On the Translation of Languages from Left to Right .Information and Control 1965 , 8 ( 6 ) : 607 - 639 .View Article .", "label": "", "metadata": {}, "score": "91.611176"}
{"text": "A number of program modules can be stored on the hard disk , magnetic disk 59 , optical disk 61 , ROM 54 or RAM 55 , including an operating system 65 , one or more application programs 66 , other program modules 67 , and program data 68 .", "label": "", "metadata": {}, "score": "92.22648"}
{"text": "Other input devices ( not shown ) can include a joystick , game pad , satellite dish , scanner , or the like .A monitor 77 or other type of display device is also connected to the system bus 53 via an interface , such as a video adapter 78 .", "label": "", "metadata": {}, "score": "93.2719"}
{"text": "T . ) s .A .R .f .s .T . )A . r .A .R . q .Q .i . q .P .r .i . ) j . q .", "label": "", "metadata": {}, "score": "95.17688"}
{"text": "FIG .3 .It should be noted that the entire system 100 , or part of speech recognition system 100 , can be implemented in the environment illustrated in .FIG .2 .For example , microphone 92 can preferably be provided as an input device to the computer 50 , through an appropriate interface , and through the A / D converter 104 .", "label": "", "metadata": {}, "score": "95.3506"}
{"text": "In a distributed computing environment , program modules can be located in both local and remote memory storage devices .With reference to .The system bus 53 can be any of several types of bus structures including a memory bus or memory controller , a peripheral bus , and a local bus using any of a variety of bus architectures .", "label": "", "metadata": {}, "score": "95.77249"}
{"text": "Let be the \" ungapped RNA alphabet \" , i.e. the set of four possible nucleotides in RNA .Let be the \" gapped RNA alphabet \" , i.e. the ungapped RNA alphabet \u03a9 plus the gap symbol .We write \u03a8 - symbols by vertically stacking pairs of \u03a9'-symbols , like this or this .", "label": "", "metadata": {}, "score": "95.8235"}
{"text": "When the parser has predicted and completed as much as it can , it has a set of terminal symbols that it needs to see before it can go on and the set consists of one element which is \" boys \" .", "label": "", "metadata": {}, "score": "95.99846"}
{"text": "u t .i 1 u t .i 2 . . .u t .i k ] from the context - free grammar non - terminal t i .In the case when t i itself is a word ( \u016b t .", "label": "", "metadata": {}, "score": "96.892815"}
{"text": "Many thanks to Bill Byrne , David Chiang , Bonnie Dorr , Jason Eisner , Christof Monz , David Smith , Noah Smith , and undoubtedly others I 'm forgetting , for discussions about the syllabus .Responsibility for the outcome is , of course , completely indeterminate .", "label": "", "metadata": {}, "score": "97.472206"}
{"text": "claim 9 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and .", "label": "", "metadata": {}, "score": "97.79328"}
{"text": "A basic input / output system 56 ( BIOS ) , containing the basic routine that helps to transfer information between elements within the personal computer 50 , such as during start - up , is stored in ROM 54 .The hard disk drive 57 , magnetic disk drive 58 , and optical disk drive 60 are connected to the system bus 53 by a hard disk drive interface 62 , magnetic disk drive interface 63 , and an optical drive interface 64 , respectively .", "label": "", "metadata": {}, "score": "98.17335"}
{"text": "r .j .T . ) s .A .R .j . q .f .s .j .T . ) s .A .R .j . q .f .s .j .", "label": "", "metadata": {}, "score": "98.79229"}
{"text": "i .A .B .A . i .A .i .R .n .A . i .B .R .n .P .n .w . )b .i .P .n .", "label": "", "metadata": {}, "score": "100.462135"}
{"text": "P .n .A .P .n . a . a .i .A . i .A .i .R .n .A .i .R .n .P .n .A . i .", "label": "", "metadata": {}, "score": "100.48503"}
{"text": "W .T . )i . m .P .t .i .t .i .t .i .i . m .P .u . t .i ._ .t .i . )", "label": "", "metadata": {}, "score": "102.984055"}
{"text": "The personal computer 50 can operate in a networked environment using logic connections to one or more remote computers , such as a remote computer 79 .FIG .2 .The logic connections depicted in .FIG .2 include a local area network ( LAN ) 81 and a wide area network ( WAN ) 82 .", "label": "", "metadata": {}, "score": "103.07179"}
{"text": "Table 3 describes the connectivity of bulges and Table 4 handles emissions ( basepaired , unpaired , aligned or gapped ) .The starting nonterminal is Start .The nonterminals representing higher - level units of RNA structure are Loop , Stem , LBulge , RBulge and LRBulge .", "label": "", "metadata": {}, "score": "103.08264"}
{"text": "A . k . ) w . k . ) w .j . )T .j . k .j .k . w .j . )i .j .P .n .S .S . i . )", "label": "", "metadata": {}, "score": "104.32439"}
{"text": "L .T .P .A .I . q .Q .L .T .P .A . q .A .R .i .p .i . )f .A . i .T . )", "label": "", "metadata": {}, "score": "105.00272"}
{"text": "Processor 14 communicates with processor memory 16 via connecting element 18 .After processor 14 has completed the algorithm and has identified the input from input device 10 , processor 14 transmits an output to output device 22 via connecting element 20 .", "label": "", "metadata": {}, "score": "106.05533"}
{"text": "P .u . t .i .l .h . )P . word .u . t .i .l .u . t .i .l .u . t .i .l .w .", "label": "", "metadata": {}, "score": "108.51727"}
{"text": "I .n .S .A . i .A .I . i .A . i .A .R .i .R .n .S .S . i . i .A . i .", "label": "", "metadata": {}, "score": "108.64714"}
{"text": "A .R .i .A . i . w .A . i . w .w . k .A . i . k . k . ) w . k . )A .R .w .", "label": "", "metadata": {}, "score": "110.0417"}
{"text": "[ Google Scholar ] .\u00a9 2011 by the authors ; licensee MDPI , Basel , Switzerland .A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .", "label": "", "metadata": {}, "score": "110.38019"}
{"text": "P .u . t .i ._ .t .i . )[ .l .u ._ .t .i .P .u . t .l .u . t .i . u . t .", "label": "", "metadata": {}, "score": "111.149536"}
{"text": "However , the task - independent corpus might contain references to a person called \" Joe Friday \" .In this manner , during parsing of the task - independent corpus , instances of days of the week will be identified separate from instances where \" Friday \" is the last name of an individual .", "label": "", "metadata": {}, "score": "113.5124"}
{"text": "Author to whom correspondence should be addressed ; Tel . : +49 - 631 - 205 - 3979 ; Fax : +49 - 631 - 205 - 2573 .Received : 12 October 2011 / Accepted : 20 October 2011 / Published : 21 October 2011 .", "label": "", "metadata": {}, "score": "119.78885"}
{"text": "For instance , the task - independent corpus includes various instances of how proper names are used .For example , the task - independent corpus could have sentences like : \" Bill Clinton was present at the meeting \" and \" John Smith went to lunch at the conference \" .", "label": "", "metadata": {}, "score": "121.10419"}
{"text": "When used in a LAN networking environment , the personal computer 50 is connected to the local area network 81 through a network interface or adapter 83 .When used in a WAN networking environment , the personal computer 50 typically includes a modem 84 or other means for establishing communications over the wide area network 82 , such as the Internet .", "label": "", "metadata": {}, "score": "121.23961"}
{"text": "\u00a9 Holmes .This article is published under license to BioMed Central Ltd.", "label": "", "metadata": {}, "score": "136.88666"}
