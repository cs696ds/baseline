{"text": "Two key approaches have been developed , Minimum Message Length ( MML ) [ Wallace and Boulton , 1968 ] and Minimum Description Length ( MDL )[ Rissanen , 1983 ] ] .Both approaches admit to probabilistic interpretations .Given prior probabilities for both theories and data , minimization of the MML encoding closely approximates maximization of posterior probability [ Wallace and Freeman , 1987 ] .", "label": "", "metadata": {}, "score": "46.5264"}
{"text": "The clustering method is designed to operate when the correspondences between nodes are unknown and must be inferred as part of the learning process .We adopt a minimum description length approach to the problem of fitting the mixture model to data .", "label": "", "metadata": {}, "score": "49.797153"}
{"text": "From this perspective , models are compared on their ability to compress a ... \" .The Minimum Description Length ( MDL ) principle is an information theoretic approach to inductive inference that originated in algorithmic coding theory .In this approach , data are viewed as codes to be compressed by the model .", "label": "", "metadata": {}, "score": "50.15284"}
{"text": "R. Baxter .Minimum Message Length Inductive Inference - Theory and Application .PhD thesis , Dept .Computer Science , Monash University , Dec. 1996 .G. Farr & C. S. Wallace .The Complexity of Strict Minimum Message Length Inference .", "label": "", "metadata": {}, "score": "50.455204"}
{"text": "He used MML to come to the conclusion that they really are rough circles rather than more elaborate geometries that had been proposed , and still are proposed regularly .Georgeff and Wallace ( 1983 , 1984 ) described a minimum message length criterion for fitting one or more ( straight ) line segments through a set of data points .", "label": "", "metadata": {}, "score": "50.66363"}
{"text": "Some penalty for the complexity of H is needed to give teeth to the so - called \" law of diminishing returns \" .The minimum message length ( MML ) criterion is to consider a two - part message ( remember [ Bayes ] ): .", "label": "", "metadata": {}, "score": "51.341843"}
{"text": "Dec ' 1996 The authors credit Itoh 's work ( 1990 ) as being \" of particular relevance \" to their paper .L. J. Fitzgibbon , L. Allison & D. L. Dowe .Minimum message length grouping of ordered data .", "label": "", "metadata": {}, "score": "51.743954"}
{"text": "Determining the causal structure of a domain is a key task in the area of Data Mining and Knowledge Discovery .The algorithm proposed by Wallace et al .[ 15 ] has demonstrated its strong ability in discovering Linear Causal Models from given data sets .", "label": "", "metadata": {}, "score": "54.03012"}
{"text": "The name ' minimum message length ' is after Shannon 's mathematical theory of communication .The first part of the two - part message can be considered to be a \" header \" , as in data compression or data communication .", "label": "", "metadata": {}, "score": "54.450523"}
{"text": "We develop a search - based algorithm for learning hierarchical latent class models from data .The algorithm is evaluated using both synthetic and real - world data . \" ...Abstract - This paper poses the problem of tree - clustering as that of fitting a mixture of tree unions to a set of sample trees .", "label": "", "metadata": {}, "score": "55.160255"}
{"text": "In MDL , it is the selection of the model that is the focus of atte ... . \" ...The Minimum Description Length ( MDL ) principle is an information theoretic approach to inductive inference that originated in algorithmic coding theory .", "label": "", "metadata": {}, "score": "55.169456"}
{"text": "The goal of model selection is to identify the model , from a set of candidate models , that permits the shortest description length ( code ) of the data .It represents an elegant solution to the model selection problem .", "label": "", "metadata": {}, "score": "56.931213"}
{"text": "In this paper , a more efficient and precise MML encoding scheme is proposed to describe the model structure and the nodes in a Linear Causal Model .The estimation of different parameters is also derived .Empirical results show that the new algorithm outperformed the previous MML - based algorithm in terms of both speed and precision . eng .", "label": "", "metadata": {}, "score": "57.217575"}
{"text": "The tree - unions and the mixing proportions are sought so as to minimize the description length criterion .This is the sum of the negative logarithm of the Bernoulli distribution , and a message - length criterion that encodes both the complexity of the uniontrees and the number of mixture components .", "label": "", "metadata": {}, "score": "57.228683"}
{"text": "Some background : Chris Wallace published the first paper on Minimum Message Length ( MML ) in Wallace & Boulton ( 1968 ) .Wallace & Dowe ( 1999a ) was once the Computer J ( OUP ) 's most downloaded article - and currently remains as Chris Wallace 's most cited co - authored work by a researcher still active in the area .", "label": "", "metadata": {}, "score": "57.329838"}
{"text": "The proposed method also avoids another drawback of EM for mixture fitting : the possibility of convergence toward a singular estimate at the boundary of the parameter space .The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models ; instead , we seamlessly integrate estimation and model selection in a single algorithm .", "label": "", "metadata": {}, "score": "57.66774"}
{"text": "Clustering is a common unsupervised learning technique used to discover group structure in a set of data .While there exist many algorithms for clustering , the important issue of feature selection , that is , what attributes of the data should be used by the clustering algorithms , is rarely touched upon .", "label": "", "metadata": {}, "score": "57.91668"}
{"text": "We also show that this can be useful for other problems in multivariate analysis , such as discriminant analysis and multivariate density estimation .We give examples from medical diagnosis , mineeld detection , cluster recovery from noisy data , and spatial density estimation .", "label": "", "metadata": {}, "score": "58.001366"}
{"text": "( Farr 1999 p.43 ) .Note that [ Strict MML ] ( SMML ) ( Wallace & Boulton 1975 , Farr 1999 p.49 ) does not make the simplifying approximations of MML , however the mathematical and algorithmic consequences can be severe ( Farr & Wallace 1997 ) .", "label": "", "metadata": {}, "score": "58.114212"}
{"text": "We investigate three types of similarity metrics : queueing delay measured by sandwich probes , delay variance measured by packet pairs , and loss rate measured also by packet pairs .Unlike previous work which first assumes the network topology is a binary tree and then tries to generalize to a non - binary tree , we provide a framework which directly deals with general logical tree topologies .", "label": "", "metadata": {}, "score": "58.245747"}
{"text": "An alternative is described below .MML .Attempts to minimise the discrepancy between given data , D , and values implied by a hypothesis , H , almost always results in over - fitting , i.e. a too complex hypothesis ( model , parameter estimate , ... ) .", "label": "", "metadata": {}, "score": "58.253838"}
{"text": "The method can be applied to both unweighted and weighted trees .We illustrate the utility of the resulting algorithm on the problem of classifying 2D shapes using a shock graph representation .Index Terms - Structural learning , tree clustering , mixture modelinq , minimum description length , model codes , shock graphs . ... lights some of the differences in perspective .", "label": "", "metadata": {}, "score": "58.812"}
{"text": "The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models ; instead , we seamlessly integrate estimation and model selection in a single algorithm .Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm ; in this paper , we illustrate it with experiments involving Gaussian mixtures .", "label": "", "metadata": {}, "score": "58.936333"}
{"text": "The MML derivations above generalise the particular forms of Wallace and Boulton ( 1968 ) for the binomial , multinomial and normal distributions .This material is based on talks given by C. S. Wallace c1988 , on Wallace & Freeman ( 1987 ) , R. Baxter 's PhD thesis ( 1996 ) , and G. Farr ( 1999 ) .", "label": "", "metadata": {}, "score": "60.34529"}
{"text": "Index Terms\u00d0Finite mixtures , unsupervised learning , model selection , minimum message length criterion , Bayesian methods , expectation - maximization algorithm , clustering . ... teria .Despite their formal appeal , we thinkthat MCMC - based techniques are still fa ... . by Gilles Celeux , Merrilee Hurn , Christian P. Robert - Journal of the American Statistical Association , 1999 . \" ...", "label": "", "metadata": {}, "score": "60.570343"}
{"text": "Whereas the Bayesian Information Criterion performed the best of the ICs , the bootstrap likelihood ratio test proved to be a very consistent indicator of classes across all of the models considered . ...Jedidi , Jagpal , and DeSarbo ( 1997 ) found that among commonly used model selection criteria , the BIC picked the correct model most consistently in the finite mixture structure equation model .", "label": "", "metadata": {}, "score": "60.611595"}
{"text": "Minimum message length ( MML ) inference takes the hypothesis , H , into account with a two - part message : .A complex hypothesis with many segments might fit D very well , .Related problems are : .Use of basis functions other than straight lines , e.g. .", "label": "", "metadata": {}, "score": "60.721733"}
{"text": "The evidence from the study presented herein appears to support the potential desirability of doing so .This casts some doubt upon the utility of the universal prior employed by MDL and the default prior usually employed with MML , at least with respect to their use for maximizing predictive accuracy .", "label": "", "metadata": {}, "score": "61.700134"}
{"text": "Another important problem in clustering is the determination of the number of clusters , which clearly impacts and is influenced by the feature selection issue .In this paper , we propose the concept of feature saliency and introduce an expectation - maximization ( EM ) algorithm to estimate it , in the context of mixture - based clustering .", "label": "", "metadata": {}, "score": "61.848686"}
{"text": "These experiments testify for the good performance of our approach .Index Terms\u00d0Finite mixtures , unsupervised learning , model selection , minimum message length criterion , Bayesian methods , expectation - maximization algorithm , clustering . ... close to the optimal value .", "label": "", "metadata": {}, "score": "62.07283"}
{"text": "Forster and Sober ( [ 1994 ] ) advocate the use of Akaike 's Information Criterion ( AIC ) , a non - Bayesian formalisation of the notion of simplicity .This forms an important part of their wider attack on Bayesianism in the philosophy of science .", "label": "", "metadata": {}, "score": "62.350883"}
{"text": "The criterion and algorithm are then extended to simultaneously estimate the feature saliencies and the number of clusters .INDEX TERMS .Feature selection , clustering , unsupervised learning , mixture models , minimum message length , EM algorithm .CITATION .", "label": "", "metadata": {}, "score": "62.35378"}
{"text": "An Invariant Bayes Method for Point Estimation .Classification Soc .Bulletin , 3 , pp.11 - 34 , 1975 .C. S. Wallace & P. R. Freeman .Estimation and Inference by Compact Coding .J. Royal Stat .Soc . , 49 ( 3 ) , pp.240 - 265 , 1987 .", "label": "", "metadata": {}, "score": "62.782806"}
{"text": "Notes .W. D. Fisher .On Grouping for Maximum Homegeneity .Jrnl .Am .Stat .Soc .53 pp.789 - 798 , 1958 Gave an O(s.n 2 ) -time algorithm for finding the optimum ( minimum sum of squared errors ) segmentations of a sequence of n points into .", "label": "", "metadata": {}, "score": "63.20869"}
{"text": "However , as the prior distribution does not distinguish between the different components , the posterior mixture distribution is symmetric and thus standard estimators such as posterior means can not be used .Since this is also true for most non - symmetric priors , we propose alternatives for Bayesian inference for permutation invariant posteriors , including a cluster ... . \" ...", "label": "", "metadata": {}, "score": "63.3543"}
{"text": "Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type .However , there is little ... \" .Cluster analysis is the automated search for groups of related observations in a data set .", "label": "", "metadata": {}, "score": "63.473343"}
{"text": "( NB .Needs Java on ) .Draw shapes in the window ( hold the left mouse button down ) .The faster and slower buttons vary the sampling rate .Closed or open curves can be drawn .The MDL button fits a polygon through the last shape drawn using the MDL fitting method .", "label": "", "metadata": {}, "score": "63.523155"}
{"text": "We interpret the violation of this assumption as an indication of the presence of latent variables and show how latent variables can be detected .Latent variable dis ... \" .The naive Bayes model makes the often unrealistic assumption that feature variables are mutually independent given the class variable .", "label": "", "metadata": {}, "score": "63.54315"}
{"text": "Without any cooperation from the internal routers , topology estimation can be formulated as hierarchical clustering of the leaf nodes based on pair - wise correlations as similarit ... \" .In this paper we address the problem of topology discovery in unicast logical tree networks using endto - end measurements .", "label": "", "metadata": {}, "score": "63.90918"}
{"text": "The distribution of observed tree nodes in ea ... \" .Abstract - This paper poses the problem of tree - clustering as that of fitting a mixture of tree unions to a set of sample trees .The treeunions are structures from which the individual data samples belonging to a cluster can be obtained by edit operations .", "label": "", "metadata": {}, "score": "64.09339"}
{"text": "A hierarchical algorithm to estimate the topology is developed in a similar manner by finding the best partitions of the leaf nodes .Our simulations show that the algorithm is more robust than binary - tree based methods .The three types of similarity metrics are also evaluated under various network load conditions using ns-2 . ... m in which a higher model order generally results in a higher likelihood .", "label": "", "metadata": {}, "score": "64.54523"}
{"text": "Standard Markov chain Monte Carlo techniques usually have difficulties with well - separated modes such as occur here ; the Markov chain Monte Carlo sampler stays within a neighbourhood of a local mode and fails to visit other equally important modes .", "label": "", "metadata": {}, "score": "64.69435"}
{"text": "Moffitt 's theory is prototypical of other developmental taxono - mies that have been proposed in such diverse areas as developmental psychopathology ( Schulenberg , . ... acting multiple components .Presaging this use of normal mixture m .. by Karen L. Nylund , Tihomir Asparouhov , Muth\u00e9n Muth\u00e9n , Bengt O. Muth\u00e9n , 2007 . \" ...", "label": "", "metadata": {}, "score": "65.608665"}
{"text": "An Algorithm for the Piecewise Linear Approximation of Planar Curves .Proc .IEEE Symp . on Information Theory ( ISIT ) , pp.62 , 1990 .S. Banerjee , W. Niblack & M. Flickner .A Minimum Description Length Polygon Approximation method .", "label": "", "metadata": {}, "score": "66.112526"}
{"text": "Further , the within - class parameter estimates obtained from these mod - els are largely uninterpretable .Significant predictive relationships may be obscured or spurious relationships identified .The implications of these results for applied research are highlighted , and future directions for quantitative developments are suggested .", "label": "", "metadata": {}, "score": "66.30152"}
{"text": "The use of a prior , P(H ) , is considered to be controversial in classical statistics .Notes .The idea of using compression to guide inference seems to have started in the 1960s .R. J. Solomonoff .A Formal Theory of Inductive Inference , I and II .", "label": "", "metadata": {}, "score": "66.6897"}
{"text": "Three approaches to the Quantitaive Definition of Information .Problems of Information and Transmission 1 ( 1 ) pp1 - 7 , 1965 .G. J. Chaitin .On the Length of Programs for Computing Finite Binary Sequences .JACM 13 ( 4 ) pp547 - 569 , Oct ' 1966 .", "label": "", "metadata": {}, "score": "67.10903"}
{"text": "Additionally , we offer improvements to an existing algorithm for dimension estimation , based on k - nearest neighbor graphs , and offer an algorithm for adapting any dimension estimation algorithm to operate locally .Finally , we illustrate the uses of local dimension estimation with data sets consisting of multiple manifolds , including applications such as diagnosing anomalies in router networks and image segmentation .", "label": "", "metadata": {}, "score": "67.21433"}
{"text": "Point - sets : we are given an unordered set of points to be described by one or more straight lines etc .. e.g. .Segmentation : fitting piece - wise constant functions .Approximation by piece - wise straight lines or other basis curves etc . .", "label": "", "metadata": {}, "score": "67.33476"}
{"text": "TR#32 , Dept .Computer Science , Monash University , Australia 3800 , March 1983 .[HTML ] Fuller version of Georgeff and Wallace ( 1984 ) .Includes PFSA 's .M. P. Georgeff & C. S. Wallace .A General Selection Criterion for Inductive Inference .", "label": "", "metadata": {}, "score": "68.726776"}
{"text": "16 ( 6)sMML has been widely used in unsupervised learning of mixture models [ 12 , 40 , 41 , 45].The incompl ...Inference .This page : Inference , model class , model , parameter estimation & accuracy , over - fitting , noise , MML , compression .", "label": "", "metadata": {}, "score": "68.854935"}
{"text": "The two approaches differ in that MDL employs a universal prior , which Rissanen [ 1983 ] explicitly justifies in terms of Occam 's razor , while MML allows the specification of distinct appropriate priors for each induction task .However , in practice , a default prior is usually employed for MML , one that appears to also derive its justification from Occam 's razor .", "label": "", "metadata": {}, "score": "68.90481"}
{"text": "These models have been adopted en - thusiastically by applied psychological researchers in part because they provide a more dynamic analysis of repeated measures data than do many traditional tech - niques .However , these methods are not ideally suited for testing theories that posit the existence of qualita - tively different developmental pathways , that is , theo - ries in which distinct developmental pathways are thought to hold within subpopulations .", "label": "", "metadata": {}, "score": "69.745735"}
{"text": "An Information Measure for Classification .CACM 11 ( 2 ) pp185 - 194 , Aug ' 1968 This seems to be the first application of the principle to a real inference problem resulting in a practical and useful computer program .", "label": "", "metadata": {}, "score": "70.09942"}
{"text": "e.g. The normal distribution N(\u03bc , \u03c3 ) is a model and N(0,1 ) is a model instance .Parameter Estimation .e.g. Hypothesis Complexity .Over - fitting .Over - fitting often appears as selecting a too complex model for the data .", "label": "", "metadata": {}, "score": "70.260086"}
{"text": "n. To perform dimension reduction , one first needs to know the intrinsic dimensionality of the manifold supporting the data .When the intrinsic dimension is assumed constant over the data set , several algorithms [ 2 - 5 ] have been proposed to estimate the dimension ... . \" ...", "label": "", "metadata": {}, "score": "70.35493"}
{"text": "This would almost certainly be a ridiculous thing to do .That small amount of data is probably better described by a straight line or by a quadratic with any minor variations explained as \" noise \" and experimental error .Parameter estimation provides another manifestation of over - fitting : stating a parameter value too accurately is also over - fitting .", "label": "", "metadata": {}, "score": "70.77165"}
{"text": "Latent variable discovery is interesting , especially for medical applications , because it can lead to better understanding of application domains .It can also improve classification accuracy and boost user confidence in classification models . ... nd mixture sequence , and the feature vectors are again assigned into the tree nodes for estimation of transformation parameters .", "label": "", "metadata": {}, "score": "70.811035"}
{"text": "See also ... .M. P. Georgeff & C. S. Wallace .A General Selection Criterion for Inductive Inference .TR#44 , Dept .Computer Science ( later School of Comp .Sci . and Software Eng . ) , Monash University , Australia 3800 , June 1984 .", "label": "", "metadata": {}, "score": "71.059814"}
{"text": "We also show that MML provides answers to many of Forster 's objections to Bayesianism .Hence an important part of the attack on Bayesianism fails . by Chris Fraley , Adrian E. Raftery - JOURNAL OF THE AMERICAN STATISTICAL ASSOCIATION , 2000 . \" ...", "label": "", "metadata": {}, "score": "71.12209"}
{"text": "Underlying such a model is the assumption that the observed variables are mutually independent given the class variable .A serious problem with the use of latent class models , known as local dependence , is that this assumption is often untrue .", "label": "", "metadata": {}, "score": "71.74363"}
{"text": "by Kevin M. Carter , Alfred O. Hero Iii , Raviv Raich - in Proc .IEEE Statistical Signal Processing Workshop , 2007 . \" ...Many algorithms have been proposed for estimating the intrinsic dimension of high dimensional data .A phenomenon common to all of them is a negative bias , perceived to be the result of undersampling .", "label": "", "metadata": {}, "score": "71.95712"}
{"text": "The method has been applied to data arising from different fields of interest .The important issue of consistency was however left open .In this paper , we settle this issue in affirmative .Introduction .Recent . \" ...Growth mixture models are often used to determine if subgroups exist within the population that follow qualitatively distinct developmental trajectories .", "label": "", "metadata": {}, "score": "72.40003"}
{"text": "An application example of NML in cognitive modeling is also provided . by Volkan Cevher , Student Member , James H. Mcclellan - IEEE Trans . on Signal Processing , 2005 . \" ...Traditionally in target tracking , much emphasis is put on the motion model that realistically represents the target 's movements .", "label": "", "metadata": {}, "score": "72.43471"}
{"text": "Latent class models are used for cluster analysis of categorical data .Underlying such a model is the assumption that the observed variables are mutually independent given the class variable .A serious problem with the use of latent class models , known as local dependence , is that this assumption is ... \" .", "label": "", "metadata": {}, "score": "72.96873"}
{"text": "However , there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis , such as \\How many clusters are there ? \" , \" Which clustering method should be used ? \" and \\How should outliers be handled ?", "label": "", "metadata": {}, "score": "73.19111"}
{"text": "Int .Workshop on Algorithmic Learning Theory ( ALT2000 ) , H. Arimura , S. Jain & A. Sharma ( eds ) , pp.56 - 70 , Sydney , Springer Verlag , LNCS , Dec ' 2000 .Uses an MML cost criterion to give a stopping condition to Fisher 's ( 1958 ) algorithm and hence find an optimal segmentation of multivariate data .", "label": "", "metadata": {}, "score": "73.23027"}
{"text": "Fac .Sci . ) fetched Sunday , 14-Feb-2016 04:10:26 EST .Curve and Polygon Fitting .We want \" optimal \" to mean \" most probable \" given the data , .i.e. having the largest posterior probability .", "label": "", "metadata": {}, "score": "73.71483"}
{"text": "This article presents the results of a simulation study that examines the performance of likelihood - based tests and the traditionally used Information Criterion ( ICs ) used for determining the number of classes in mixture modeling .We look at the performance of these tests and indexes for 3 types of mixture models : latent class analysis ( LCA ) , a factor mixture model ( FMA ) , and a growth mixture models ( GMM ) .", "label": "", "metadata": {}, "score": "74.606544"}
{"text": "Model .e.g. It has four parameters a , b , c & d which can be estimated to fit the model to some given data .It is usually the case that a model has a fixed number of parameters ( e.g. four above ) , but this can become blurred if hierarchical parameters or dependent parameters crop up .", "label": "", "metadata": {}, "score": "75.34337"}
{"text": "Polygon Fitting : The sequence of points is considered to be closed , circular , .L .A l l i s o n .The data might , for example , come from digitising the boundary of an object in an image .", "label": "", "metadata": {}, "score": "76.98343"}
{"text": "1154 - 1166 , September 2004 , doi:10.1109/TPAMI.2004.71 Linear causal model discovery using the MML criterion .Li , Gang , Dai , Honghua and Tu , Yiqing 2002 , Linear causal model discovery using the MML criterion , in Proceedings of the 2002 IEEE International Conference on Data Mining , IEEE Computer Society , Los Alamitos , Calif. , pp .", "label": "", "metadata": {}, "score": "77.25072"}
{"text": ".. re of the prior knowledge [ 16].The intuitive choice of the prior is usually the uniform prior on the natural space of the parameter .The resulting reference prior is not integrable ( and hence , is improper ) ... . \" ...", "label": "", "metadata": {}, "score": "77.42236"}
{"text": "Notes .This section follows Farr ( p17 ... , 1999 ) .G. Farr .Information Theory and MML Inference .School of Computer Science and Software Engineering , Monash University , 1999 .( or as otherwise indicated ) , Faculty of Information Technology ( Clayton ) , .", "label": "", "metadata": {}, "score": "77.53273"}
{"text": "It is an explanation of , a hypothesis about , the process that generated the data .It can be a good explanation or a bad explanation .Naturally we prefer good explanations .Model Class .The class includes constants , straight lines , quadratics , cubics , etc . and note that these have increasing numbers of parameters ; they grow more complex .", "label": "", "metadata": {}, "score": "77.94237"}
{"text": "If h(\u03b8 ) is the prior probability density function of \u03b8 , the probability , and message length , of the interval are approximated by .s .s ) nits .always assuming that h(\u03b8 ) does not vary much over the interval .", "label": "", "metadata": {}, "score": "77.993"}
{"text": "In the recent years , efficient Markov chain Monte Carlo method for the computation of the posterior distribution has been developed .The method has been app ... \" .A Dirichlet mixture of normal densities is a useful choice for a prior distribution on densities in the problem of Bayesian density estimation .", "label": "", "metadata": {}, "score": "78.258766"}
{"text": "G. Farr .Information Theory and MML Inference .School of Computer Science and Software Engineering , 1999 .( or as otherwise indicated ) , Faculty of Information Technology ( Clayton ) , .Monash University , .Australia 3800 ( 6/'05 was .", "label": "", "metadata": {}, "score": "78.28665"}
{"text": "Many algorithms have been proposed for estimating the intrinsic dimension of high dimensional data .A phenomenon common to all of them is a negative bias , perceived to be the result of undersampling .We propose improved methods for estimating intrinsic dimension , taking manifold boundaries into consideration .", "label": "", "metadata": {}, "score": "78.487"}
{"text": "Fac .Sci . , ' 68-'71 was .Department of Information Science , .Fac .Sci . ) fetched Thursday , 11-Feb-2016 13:24:32 EST .The advent of formal definitions of the simplicity of a theory has important implications for model selection .", "label": "", "metadata": {}, "score": "79.46796"}
{"text": "Noise or measurement error in the data is assumed to be due to some random process , but it can be modelled .e.g. We might assume that if a point \" really \" comes from a certain line segment then the error perpendicular to the line is modelled by a normal distribution , say .", "label": "", "metadata": {}, "score": "79.47188"}
{"text": "Growth mixture models are often used to determine if subgroups exist within the population that follow qualitatively distinct developmental trajectories .However , statistical theory developed for finite normal mixture models suggests that latent trajectory classes can be estimated even in the absence of population heterogeneity if the distribution of the repeated measures is nonnormal .", "label": "", "metadata": {}, "score": "79.81879"}
{"text": "The specification of mixture posterior distributions means that the presence of k ! modes is known immediately .Standard Markov chain Monte Carlo techniques usually have difficult ... \" .This paper deals with both exploration and interpretation problems related to posterior distributions for mixture models .", "label": "", "metadata": {}, "score": "79.961044"}
{"text": "J. D. Patrick & C. S. Wallace .Stone Circle Geometries : An Information Theory Approach .In Archaeoastronomy in the Old World , . D. Heggie ( ed ) , C.U.P. , 1982 .C. S. Wallace & M. P. Georgeff .", "label": "", "metadata": {}, "score": "80.07169"}
{"text": "This includes dealing with the challenging Neyman - Scott - like situation where , for some players and teams , there are few games per player or few games between different groups of players .Our enhanced modelling will be for a range of games and sports - including advantages such as , e.g. , first move ( as in chess ) , home ground and location , surface ( as in tennis ) , etc .", "label": "", "metadata": {}, "score": "80.240005"}
{"text": "i.e. inferences , should be stated .We should infer a parameter estimate ( usually ) close to the maximum likelihood estimate , .i.e. close to where .Quantities can easily be converted to bits later .MML .A parameter estimate , \u03b8 , can only be stated to finite accuracy .", "label": "", "metadata": {}, "score": "81.062256"}
{"text": "The adjective \u00aaunsupervised\u00ba is justified by two properties of the algorithm : 1 ) it is capable of selecting the number of components and 2 ) unlike the standard expectation - maximization ... \" .Abstract\u00d0This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data .", "label": "", "metadata": {}, "score": "81.09744"}
{"text": "Maximization of these factors is not necessarily directly linked with maximizing predictive accuracy .Rating systems go back at least as far as Harkness ( 1949 ) and the better - known Elo ( 1961 ) system for rating chess players .", "label": "", "metadata": {}, "score": "81.53914"}
{"text": "Abstract\u00d0This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data .The adjective \u00aaunsupervised\u00ba is justified by two properties of the algorithm : 1 ) it is capable of selecting the number of components and 2 ) unlike the standard expectation - maximization ... \" .", "label": "", "metadata": {}, "score": "81.76402"}
{"text": "Did not have a criterion for stopping , .i.e. choosing the .best 's ' .J. D. Patrick .An Information Measure Comparative Analysis of Megalithic Geometries .( i.e. stone - circles ) .Ph .D Thesis , Monash University , 1979 It is a good read .", "label": "", "metadata": {}, "score": "82.08447"}
{"text": "We show that AIC is inadequate for many statistical problems where MML performs well .Whereas MML is always defined , AIC can be undefined .Whereas MML is not known ever to be statistically inconsistent , AIC can be .Even when defined and consistent , AIC performs worse than MML on small sample sizes .", "label": "", "metadata": {}, "score": "82.28531"}
{"text": "The adjective \u00aaunsupervised\u00ba is justified by two properties of the algorithm : 1 ) it is capable of selecting the number of components and 2 ) unlike the standard expectation - maximization ( EM ) algorithm , it does not require careful initialization .", "label": "", "metadata": {}, "score": "82.33909"}
{"text": "This is exemplified by showing that target frequencies , which may be unrelated to the target motion , can also be used to improve the tracking performance .In order to include the frequency variable , a new array steering vector is presented for the direction - of - arrival ( DOA ) estimation problems .", "label": "", "metadata": {}, "score": "82.51783"}
{"text": "The second part of the message transmits the data given the first part .The receiver has not seen the data , x , and does not know any estimate based on the data unless told by the transmitter , so we must use the average over the interval ( \u03b8 - s/2 , \u03b8+s/2 ) .", "label": "", "metadata": {}, "score": "83.39839"}
{"text": "We also refine how quickly ratings can change depending upon the strength of the player .The successful candidate will have an undergraduate degree and will be at least semi - literate in at least one of mathematics and information theory , or at least interested in both areas .", "label": "", "metadata": {}, "score": "83.71649"}
{"text": "Traditionally in target tracking , much emphasis is put on the motion model that realistically represents the target 's movements .In this paper , we first present the classical constant velocity model and then introduce a new model that incorporates an acceleration component along the heading direction of the target .", "label": "", "metadata": {}, "score": "84.25296"}
{"text": "They made the simplifying assumptions that .( i ) the polygon 's vertices ( knots ) are a subset of the given data points , .( ii ) the first data point is a vertex , and thereby achieved an O(n 2 ) -time algorithm . -", "label": "", "metadata": {}, "score": "84.42328"}
{"text": "Dowe has a forthcoming ( 2011 ) piece on MML to appear in the forthcoming Handbook of Philosophy of Statistics , Elsevier .Tools . by Mario A. T. Figueiredo , Senior Member , Anil K. Jain - IEEE Transactions on pattern analysis and machine intelligence , 2002 . \" ...", "label": "", "metadata": {}, "score": "84.5321"}
{"text": "the integral of t 2 over ( -s/2 , s/2 ) .A number of simplifying assumptions have been made along the way ; beware if their preconditions do not hold !The simplifications lead to more tractable mathematics .Multiple Parameters .", "label": "", "metadata": {}, "score": "84.919655"}
{"text": "Despite mixture models ' usefulness in practice , one unresolved issue in the application of mixture models is that there is not one commonly accepted statistical indicator for deci ... \" .Mixture modeling is a widely applied data analysis technique used to identify unobserved heterogeneity in a population .", "label": "", "metadata": {}, "score": "87.232376"}
{"text": "D : data , an observation , D : X , often x : X .Maximum Likelihood .e.g. Binomial Distribution ( Bernouilli Trials ) .( 1-p ) # tail .Take the -ve log because ( it 's easier and ) maximising the likelihood is equivalent to minimising the negative log likelihood : . log 2 ( p ) -#tail . log 2 ( 1-p ) .", "label": "", "metadata": {}, "score": "88.01996"}
{"text": "i.e. average over x in the .data - space X .( NB .The 'd 's should be curly but this is HTML not XML . )The Fisher information shows how sensitive the likelihood is to the . parameter \u03b8 .", "label": "", "metadata": {}, "score": "89.55121"}
{"text": "A : Think of a hypothesis as a card .A theory is a house made of hypotheses .( From rec.humor.funny , attributed to Marilyn vos Savant . )Introduction . to fit some data .It is argued that although these distinctions are sometimes of practical convenience , that 's all : They are all really one and the same process of inference .", "label": "", "metadata": {}, "score": "90.746925"}
{"text": "e.g. Normal Distribution .Given N data points , the maximum likelihood estimators for the parameters of a normal distribution , \u03bc ' , \u03c3 ' are given by : .--the sample mean .--the sample variance .Note that \u03c3 ' 2 is biased , e.g. if there are just two data values it is implicitly assumed that they lie on opposite sides of the mean which plainly is not necessarily the case , .", "label": "", "metadata": {}, "score": "91.42648"}
{"text": "p .So the maximum - likelihood inference for the bias of the coin is # head / N. .To sow some seeds of doubt , note that if the coin is thrown just once , the estimate for p must be either 0.0 or 1.0 , which seems rather silly , although one could argue that such a small number of trials is itself rather silly .", "label": "", "metadata": {}, "score": "91.74234"}
{"text": "Tech . , ' 89 was .Department of Computer Science , .Fac .Sci . , ' 68-'71 was .Department of Information Science , .Fac .Sci . ) fetched Friday , 12-Feb-2016 12:20:57 EST .", "label": "", "metadata": {}, "score": "91.90334"}
{"text": "If you believe that your rights have been infringed by this repository , please contact drosupport@deakin.edu.au .Versions .Version .Filter Type .Every reasonable effort has been made to ensure that permission has been obtained for items included in DRO .", "label": "", "metadata": {}, "score": "92.51575"}
{"text": "F(x , \u03b8 ) .The Fisher information is now defined to be the determinant of F(\u03b8 ) . -ln(h \u03b8 ) .+ ( 1/2 ) ln(F \u03b8 ) .+ ( n/2 ) ( 1 + ln k n ) nits .", "label": "", "metadata": {}, "score": "94.49774"}
{"text": "Socio Economic Objective . 0Not Applicable .HERDC Research category .E1 Full written paper - refereed .ERA Research output type .E Conference publication .Copyright notice .\u00a9This material is presented to ensure timely dissemination of scholarly and technical work .", "label": "", "metadata": {}, "score": "95.401764"}
{"text": "Fisher information , two - part message , accuracy of parameter ( inference ) , multiple parameters .For one continuous - valued parameter , \u03b8 , the Fisher information is defined to be : . hypothesis , ... ) \u03b8 .", "label": "", "metadata": {}, "score": "96.55544"}
{"text": "If a coin is tossed three times and comes up heads once we surely have much less information about any bias ( \u03b8 ) than if the coin is tossed 300 times and comes up heads 100 times .Finite accuracy amounts to stating that \u03b8 lies in an interval ( \u03b8 - s/2 , \u03b8+s/2 ) ; note that the width , s , depends on \u03b8 in general .", "label": "", "metadata": {}, "score": "100.40251"}
{"text": "Monash University , .Australia 3800 ( 6/'05 was .School of Computer Science and Software Engineering , .Fac .Info .Tech . , .Monash University , was .Department of Computer Science , .Fac .Comp .", "label": "", "metadata": {}, "score": "112.9488"}
{"text": "All persons copying this information are expected to adhere to the terms and constraints invoked by each author 's copyright .In most cases , these works may not be reposted without the explicit permission of the copyright holder .Unless expressly stated otherwise , the copyright for items in DRO is owned by the author , with all rights reserved .", "label": "", "metadata": {}, "score": "113.21919"}
{"text": "Fac .Info .Tech . , .Monash University , was .Department of Computer Science , .Fac .Comp .& Info .Tech . , ' 89 was .Department of Computer Science , .Fac .Sci . , ' 68-'71 was .", "label": "", "metadata": {}, "score": "114.8058"}
{"text": "Theta : variously the parameter - space , model class etc . .theta or H : variously a particular parameter value , hypothesis , model , etc .P(theta ) or P(H ) : prior probability of parameter value , hypothesis etc . .", "label": "", "metadata": {}, "score": "116.80081"}
{"text": "It lands ' heads ' once and ' tails ' twice .What do we infer ?e.g. I toss a coin 30 times .It lands ' heads ' 10 times and ' tails ' 20 times .We are probably justified in starting to suspect a bias , perhaps 0.2 .", "label": "", "metadata": {}, "score": "129.16307"}
{"text": "Australia 3800 ( 6/'05 was .School of Computer Science and Software Engineering , .Fac .Info .Tech . , .Monash University , was .Department of Computer Science , .Fac .Comp .& Info .Tech . , ' 89 was .", "label": "", "metadata": {}, "score": "130.86569"}
