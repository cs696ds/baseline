{"text": "This paper describes a method for conducting evaluations of Treebank and non - Treebank parsers alike against the English language U. Penn Treebank ( Marcus et al . , 1993 ) using a metric that focuses on the accuracy of relatively non - controversial aspects of parse structure .", "label": "", "metadata": {}, "score": "50.815247"}
{"text": "We hope that this method may find wider acceptance and be useful in establishing a generally applicable framework for evaluation in natural language parsing .We employ this method in an evaluation of NLPWin ( Heidorn , 2000 ) , a parser developed at Microsoft Research without reference to the Penn Treebank , and , for comparison , the well - known statistical Treebank parser of Charniak ( 2000 ) .", "label": "", "metadata": {}, "score": "57.08786"}
{"text": "This paper discusses the implementation of crucial aspects of this new annotation scheme .It incorporates a more consistent treatment of a wide range of gramma ... \" .The Penn Treebank has recently implemented a new syntactic annotation scheme , designed to highlight aspects of predicate - argument structure .", "label": "", "metadata": {}, "score": "59.550797"}
{"text": "The corpus is annotated with 31 part - of - speech tags .The treebank annotation scheme is based on Stanford Typed Dependencies ( de Marneffe et al . , 2006 ; de Marneffe and Manning , 2008 ) .Previous releases : .", "label": "", "metadata": {}, "score": "60.522568"}
{"text": "The development data was 200 sentences of labeled biomedical oncology text ( BIO , the ONCO portion of the Penn Biomedical Treebank ) , as well as 200 K unlabeled sentences ( Kulick et al ., 2004 ) .The two test domains were a collection of medline chem- istry abstracts ( pchem , the CYP portion of the Penn Biomedical Treebank ) and the Child Language Data Exchange System corpus ( CHILDES ) ( MacWhin- ney , 2000 ; Brown , 1973 ) .", "label": "", "metadata": {}, "score": "60.64943"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .French .French TreeBank ( FTB , Abeill\u00e9 et al ; 2003 )Le Monde , December 2007 version , 28-tag tagset ( CC tagset , Crabb\u00e9 and Candito , 2008 ) .", "label": "", "metadata": {}, "score": "60.78285"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .French .French TreeBank ( FTB , Abeill\u00e9 et al ; 2003 )Le Monde , December 2007 version , 28-tag tagset ( CC tagset , Crabb\u00e9 and Candito , 2008 ) .", "label": "", "metadata": {}, "score": "60.78285"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .French .French TreeBank ( FTB , Abeill\u00e9 et al ; 2003 )Le Monde , December 2007 version , 28-tag tagset ( CC tagset , Crabb\u00e9 and Candito , 2008 ) .", "label": "", "metadata": {}, "score": "60.78285"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .French .French TreeBank ( FTB , Abeill\u00e9 et al ; 2003 )Le Monde , December 2007 version , 28-tag tagset ( CC tagset , Crabb\u00e9 and Candito , 2008 ) .", "label": "", "metadata": {}, "score": "60.78285"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : . Brants ( 2000 ) reports 96.7 % token accuracy and 85.5 % unknown word accuracy on a 10-fold cross - validation of the Penn WSJ corpus .", "label": "", "metadata": {}, "score": "61.02005"}
{"text": "Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : . Brants ( 2000 ) reports 96.7 % token accuracy and 85.5 % unknown word accuracy on a 10-fold cross - validation of the Penn WSJ corpus .", "label": "", "metadata": {}, "score": "61.02005"}
{"text": "We have designed a Lexical - Functional Grammar ( LFG ) ( Kaplan and Bresnan , 1982 ) f - structure annotation algorithm to automatically annotate this treebank with f - structure information approximating to basic predicate - argument or dependency structures ( Cahill et al . , 2002c , 2004a ) .", "label": "", "metadata": {}, "score": "61.7323"}
{"text": "Contents .Performance measure : per token accuracy .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )English .Penn Treebank Wall Street Journal ( WSJ ) release 3 ( LDC99T42 ) .", "label": "", "metadata": {}, "score": "62.28292"}
{"text": "In this paper , we review our experience with constructing one such large annotated corpus -- the Penn Treebank , a corpus 1 consisting of over 4.5 million words of American English .During the first three - year phase of the Penn Treebank Project ( 1989 - 1992 ) , this corpus has been annotated for part - of - speech ( POS ) information .", "label": "", "metadata": {}, "score": "62.367615"}
{"text": "This indicates that most of the loss comes from missing these edges .The primary problem for nouns is the difference between structures in each domain . tion guidelines for the Penn Treebank flattened noun phrases to simplify annotation ( Marcus et al . , 1993 ) , so there is no complex structure to NPs . K\u00a8 ubler ( 2006 ) showed that it is difficult to compare the Penn Treebank to other treebanks with more com- plexnounstructures , suchasBIO.ConsidertheWSJ phrase \" the New York State Insurance Department \" .", "label": "", "metadata": {}, "score": "62.85664"}
{"text": "However , the first release of the treebank , containing a seed data set of 225 sentences , was in Fall 2011 and the process of treebank development was published in the journal article Linguistic Issues in Language Technology , 7(18):1 - 10 , January 2012 .", "label": "", "metadata": {}, "score": "63.3254"}
{"text": "These materials are available to members of the Linguistic Data Consortium ; for details , see Section 5.1 . \" ...Because meaningful sentences are composed of meaningful words , any system that hopes to process natural languages as people do must have information about words and their meanings .", "label": "", "metadata": {}, "score": "63.68682"}
{"text": "Brants ( 2000 ) reports 96.7 % token accuracy and 85.5 % unknown word accuracy on a 10-fold cross - validation of the Penn WSJ corpus .The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .", "label": "", "metadata": {}, "score": "64.54263"}
{"text": "Participants in CONLL 2008 task and other tasks involving a split of the Penn Treebank between training ( sections 2 - 21 ) from the other sections , e.g. , 23 is for testing and 24 is for development .Since NOMLEX - PLUS and COMNOM ( see Those Other NomBank Dictionaries ) involve information derived from corpus annotation , these users need to finesse how they use these resources .", "label": "", "metadata": {}, "score": "66.7808"}
{"text": "So in many ways , we are carving out new ground .Each of these phenomena is interesting in that an argument of a noun may occur outside of the NP headed by that noun .On December 17 , 2007 , we released NomBank.1.0 covering all the \" markable \" nouns in the Penn Treebank II Wall Street Journal corpus .", "label": "", "metadata": {}, "score": "67.27338"}
{"text": "Spoustov\u00e1 , Drahom\u00edra \" Johanka \" , Jan Haji\u010d , Jan Raab and Miroslav Spousta .Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 .NomBank is an annotation project at New York University that is related to the PropBank project at the University of Colorado .", "label": "", "metadata": {}, "score": "67.29059"}
{"text": "As a side effect of the annotation process , we are producing a number of other resources including various dictionaries , as well as PropBank style lexical entries called frame files .These resources help the user label the various arguments and adjuncts of the head nouns with roles ( sets of argument labels for each sense of each noun ) .", "label": "", "metadata": {}, "score": "68.010376"}
{"text": "The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .EMNLP 2002 ' ' .EMNLP 2002 ' ' .\" TALN'11 \" . \"PACLIC 2009 \" .", "label": "", "metadata": {}, "score": "68.04758"}
{"text": "# Rada , Mili , Bicknell , & Blett ... . \" ...Does knowledge of language consist of mentally - represented rules ?Rumelhart and McClelland have described a connectionist ( parallel distributed processing ) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms , both regular ( walk / walked ) ... \" .", "label": "", "metadata": {}, "score": "68.265"}
{"text": "There is a growing consensus that significant , rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora .", "label": "", "metadata": {}, "score": "69.29039"}
{"text": "INTRODUCTION During the first phase of the The Penn Treebank project [ 10 ] , ending in December 1992 , 4.5 million words of text were tagged for part - of - speech , with about two - thirds of this material also annotated with a skeletal syntactic bracketing .", "label": "", "metadata": {}, "score": "70.35387"}
{"text": "Acknowledgments .References .De Marneffe , Marie - Catherine , Bill MacCartney , and Christopher D. Manning .Generating typed dependency parses from phrase structure parses .In Proceedings of the 5th International Conference on Language Resources and Evaluation ( LREC ) .", "label": "", "metadata": {}, "score": "70.48366"}
{"text": "97 - 146 ) is a dependency - based syntactically annotated corpus .The treebank consists of 6000 sentences ( 151,671 tokens ) of written text in CoNLL - format and is developed through a bootstrapping procedure involving the open source data - driven dependency parser MaltParser ( Nivre et al . , 2006 ) , and manual validation of the annotation .", "label": "", "metadata": {}, "score": "70.68127"}
{"text": "Because meaningful sentences are composed of meaningful words , any system that hopes to process natural languages as people do must have information about words and their meanings .This information is traditionally provided through dictionaries , and machine - readable dictionaries are now widely available .", "label": "", "metadata": {}, "score": "71.29554"}
{"text": "Performance measure : per token accuracy .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )English .Penn Treebank Wall Street Journal ( WSJ ) release 3 ( LDC99T42 ) .", "label": "", "metadata": {}, "score": "71.34345"}
{"text": "Performance measure : per token accuracy .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )English .Penn Treebank Wall Street Journal ( WSJ ) release 3 ( LDC99T42 ) .", "label": "", "metadata": {}, "score": "71.34345"}
{"text": "Performance measure : per token accuracy .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )English .Penn Treebank Wall Street Journal ( WSJ ) release 3 ( LDC99T42 ) .", "label": "", "metadata": {}, "score": "71.34345"}
{"text": "WordNet 1 provides a more effective combination of traditional lexicographic information and modern computing .WordNet is an online lexical database designed for use under program control .English nouns , verbs , adjectives , and adverbs are organized into sets of synonyms , each representing a lexicalized concept .", "label": "", "metadata": {}, "score": "72.205505"}
{"text": "Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .", "label": "", "metadata": {}, "score": "72.55768"}
{"text": "Traditionally , rich , constraint - based grammatical resources have been hand - coded .Scaling such resources beyond toy fragments to unrestricted , real text is knowledge - intensive , timeconsuming and expensive .The work reported in this thesis is part of a larger project to automate as much as possible the construction of wide - coverage , deep , constraint - based grammatical resources from treebanks .", "label": "", "metadata": {}, "score": "73.44028"}
{"text": "For example , our clus- tering algorithm grouped first names in one group and measurements in another .We then added the cluster membership as a lexical feature to the parser .None of the resulting features helped adaptation .3.2Diversity Training diversity may be an effective source for adaptation .", "label": "", "metadata": {}, "score": "73.748344"}
{"text": "The major technical innova- tion is the use of a \" maximum - entropy - inspired \" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events .We also present some partial results showing the effects of different conditioning information , including a surprising 2 % improvement due to guessing the lexical head 's pre - terminal before guessing the lexical head . \" ... this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .", "label": "", "metadata": {}, "score": "73.88254"}
{"text": "We have also ported our grammar induction methodology to German and the TIGER treebank resource ( Cahill et al . , 2003a ) .We have developed a method for treebank - based , wide - coverage , deep , constraintbased grammar acquisition .", "label": "", "metadata": {}, "score": "73.96977"}
{"text": "Lawrence Erlbaum .M. Marcus , B. Santorini , and M. Marcinkiewicz .Building a large annotated corpus of English : the Penn Treebank .Computational Linguistics , 19(2):313 - 330 .Ryan McDonald , Kevin Lerman , and Fernando Pereira .", "label": "", "metadata": {}, "score": "74.04263"}
{"text": "Integrated annotation for biomedical information ex- traction .In Proc . of the Human Language Technol- ogy Conference and the Annual Meeting of the North American Chapter of the Association for Computa- tional Linguistics ( HLT / NAACL ) .B. MacWhinney .", "label": "", "metadata": {}, "score": "74.28654"}
{"text": "We believe that our approach successfully addresses the knowledge - acquisition bottleneck ( familiar from rule - based approaches to Al and NLP ) in wide - coverage , constraint - based grammar development .Our approach can provide an attractive , wide - coverage , multilingual , deep , constraint - based grammar acquisition paradigm .", "label": "", "metadata": {}, "score": "74.442955"}
{"text": "Text - mining methods in particular can help disease modeling by mapping named - entities mentions to terminologies and clustering semantically related terms .EHR corpora , however , exhibit specific statistical and linguistic characteristics when compared with corpora in the biomedical literature domain .", "label": "", "metadata": {}, "score": "74.548676"}
{"text": "Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging .or words ending in ous .This information is derived automatically from the training corpus .", "label": "", "metadata": {}, "score": "74.85075"}
{"text": ".. ncordance is a textual corpus and a lexicon combined so that every substantive word in the text is linked to its appropriate sense in the lexicon .However , this semantic concordance is still too small to provide representative samples of conte ... .", "label": "", "metadata": {}, "score": "75.07111"}
{"text": "Thus , within a longitudinal patient record , one expects to observe heavy redundancy .In this paper , we ask three research questions : ( i ) How can redundancy be quantified in large - scale text corpora ?( ii )", "label": "", "metadata": {}, "score": "75.17371"}
{"text": "Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections . by Mitchell Marcus , Grace Kim , Mary Ann Marcinkiewicz , Robert Macintyre , Ann Bies , Mark Ferguson , Karen Katz , Britta Schasberger - In ARPA Human Language Technology Workshop , 1994 . \" ...", "label": "", "metadata": {}, "score": "75.39109"}
{"text": "We observe redundancy levels of about 30 % and non - standard distribution of both words and concepts .We measure the impact of redundancy on two standard text - mining applications : collocation identification and topic modeling .We compare the results of these methods on synthetic data with controlled levels of redundancy and observe significant performance variation .", "label": "", "metadata": {}, "score": "75.43675"}
{"text": "This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation ( Nivre et al . , 2007 ) .The goal was to adapt a parser trained on a single source domain to a new target domain us- ing only unlabeled data .", "label": "", "metadata": {}, "score": "75.66524"}
{"text": "We have designed and implemented two PCFG - based probabilistic parsing architectures for parsing unseen text into f - structures : the pipeline and the integrated model .Both architectures parse raw text into basic , but possibly incomplete , predicate - argument structures ( \" proto f - structures \" ) with long distance dependencies ( LDDs ) unresolved ( Cahill et al . , 2002c ) .", "label": "", "metadata": {}, "score": "76.221985"}
{"text": "hand - retagged using the Penn Treebank tagset .The level of syntactic analysis annotated during this phase of this project was an extended and somewhat modified form of the skeletal analysis which ha ... . . .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )", "label": "", "metadata": {}, "score": "76.347244"}
{"text": "In this paper , we present a sim- ple rule - based part of speech tagger which automatically acquires its rules and tags with accuracy coinparable ... \" .Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule- based methods .", "label": "", "metadata": {}, "score": "76.7852"}
{"text": "The UPDT has sequentially been split into 10 parts , of which segments 1 - 8 are used for training ( 80 % ) , 9 for development ( 10 % ) , and 10 for test ( 10 % ) sets .", "label": "", "metadata": {}, "score": "77.44395"}
{"text": "However , our results were obtained without adap- tation .Given our position in the ranking , this sug- gests that no team was able to significantly improve performance on either test domain beyond that of a state - of - the - art parser .", "label": "", "metadata": {}, "score": "77.98812"}
{"text": "The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .Manning , Christopher D. 2011 .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "78.06871"}
{"text": "The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .Manning , Christopher D. 2011 .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "78.06871"}
{"text": "The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .Manning , Christopher D. 2011 .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "78.06871"}
{"text": "The distributed GENiA tagger is trained on a mixed training corpus and gets 96.94 % on WSJ , and 98.26 % on GENiA biomedical English .Manning , Christopher D. 2011 .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "78.06871"}
{"text": "Spoustov\u00e1 , Drahom\u00edra \" Johanka \" , Jan Haji\u010d , Jan Raab and Miroslav Spousta .Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 .Tools . by Mitchell P. Marcus , Beatrice Santorini , Mary Ann Marcinkiewicz - COMPUTATIONAL LINGUISTICS , 1993 . \" ...", "label": "", "metadata": {}, "score": "78.795074"}
{"text": "We divided the available WSJ data into a train and test set , trained a parser on the train set and compared errors on the test set and BIO .Accuracy dropped from 90 % on WSJ to 84 % on BIO .", "label": "", "metadata": {}, "score": "78.81377"}
{"text": "cation of yet another word , this would have to be built into the decision tree as well .12 Eric Brill Transormation - Based Error - Driven Learning based learning are available and can be used ... . by Philip Resnik - In Proceedings of the 14th International Joint Conference on Artificial Intelligence ( IJCAI-95 , 1995 . \" ...", "label": "", "metadata": {}, "score": "79.47972"}
{"text": "The tagger then acquires patches to improve its perfor ... . \" ...This article presents a measure of semantic similarityinanis - a taxonomy based on the notion of shared information content .Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge - counting approach .", "label": "", "metadata": {}, "score": "79.96872"}
{"text": "We present a detailed case study of this learni ... \" .this paper , we will describe a simple rule - based approach to automated learning of linguistic knowledge .This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance .", "label": "", "metadata": {}, "score": "80.31265"}
{"text": "Spoustov\u00e1 , Drahom\u00edra \" Johanka \" , Jan Haji\u010d , Jan Raab and Miroslav Spousta .Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 .Uppsala Persian Dependency Treebank : UPDT .", "label": "", "metadata": {}, "score": "80.35429"}
{"text": "But how does the observed EHR redundancy affect text mining ?Does such redundancy introduce a bias that distorts learned models ?Or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus ?( iii )How can one mitigate the impact of redundancy on text mining ?", "label": "", "metadata": {}, "score": "80.66905"}
{"text": "For example , we are using PropBank 's frame file for the verb \" decide \" in our annotation of the noun \" decision \" .However , our coordination goes far beyond that .We began this project firmly on the shoulders of Catherine Macleod 's Nomlex project and related work on support verbs .", "label": "", "metadata": {}, "score": "81.027435"}
{"text": "The splits of data for this . task . were not standardized early on ( unlike for parsing ) and early work uses various data splits defined by counts of tokens or by sections .Most work from 2002 on adopts the following data splits , introduced by Collins ( 2002 ) : .", "label": "", "metadata": {}, "score": "81.02898"}
{"text": "The CoNLL In Proc .Lawrence Saul and Fernando Pereira . gate and mixed - order markov models for statistical language modeling .In EMNLP . Aggre-1055 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .", "label": "", "metadata": {}, "score": "81.4857"}
{"text": "Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" One of the main challenges in natural language processing ( NLP ) is to correct for biases in the manually annotated data available to system engineers .", "label": "", "metadata": {}, "score": "81.55908"}
{"text": "This article presents a measure of semantic similarityinanis - a taxonomy based on the notion of shared information content .Experimental evaluation against a benchmark set of human similarity judgments demonstrates that the measure performs better than the traditional edge - counting approach .", "label": "", "metadata": {}, "score": "82.13385"}
{"text": "We tried a number of criteria to weigh sentences without suc- cess , including sentence length and number of verbs .Next , we trained a discriminative model on the pro- vided unlabeled data to predict the domain of each sentence based on POS n - grams in the sentence .", "label": "", "metadata": {}, "score": "82.33899"}
{"text": "Many studies have indeed shown that cross - domain learned corpora yield poor language models [ 35].The field of domain adaptation attempts to compensate for the poor quality of cross - domain data , by adding carefully picked text from other domains [ 36,37 ] or other statistical mitigation techniques .", "label": "", "metadata": {}, "score": "82.81952"}
{"text": "aFor text mining , preprocessing the EHR corpus with fingerprinting yields significantly better results .Conclusions Before applying text - mining techniques , one must pay careful attention to the structure of the analyzed corpora .While the importance of data cleaning has been known for low - level text characteristics ( e.g. , encoding and spelling ) , high - level and difficult - to - quantify corpus characteristics , such as naturally occurring redundancy , can also hurt text mining .", "label": "", "metadata": {}, "score": "82.8559"}
{"text": "Our suspicions are supported by the observation that no team was able to im- prove target domain performance substan- tially over a state of the art baseline . 1 Introduction Dependency parsing , an important NLP task , can be done with high levels of accuracy .", "label": "", "metadata": {}, "score": "82.95003"}
{"text": "To address conflicting annota-1053 .Page 4 . tions , we added slack variables to the MIRA learn- ing algorithm ( Crammer et al . , 2006 ) used to train the parsers , without success .We measured diversity by comparing the parses of each model .", "label": "", "metadata": {}, "score": "83.73703"}
{"text": "That document , entitled Those Other NomBank Dictionaries can be viewed here or downloaded as part of one of the archive ( tgz or zip ) files .Alternatively , click here for a quick users guide that assumes some familiarity with PropBank .", "label": "", "metadata": {}, "score": "83.75192"}
{"text": "In Conference on Natural Language Learning ( CoNLL ) .J. Nivre , J. Hall , S. K\u00a8 ubler , R. McDonald , J. Nils- son , S. Riedel , and D. Yuret .2007 shared task on dependency parsing . of the CoNLL 2007 Shared Task .", "label": "", "metadata": {}, "score": "83.78415"}
{"text": "To confirm that nouns were problem- atic , we modified a first - order parser ( no second or- der features ) by adding a feature indicating correct noun - noun edges , forcing the parser to predict these edges correctly .", "label": "", "metadata": {}, "score": "84.139755"}
{"text": "First , there may be room for adaptation with our domains if a common annotation scheme is used .Second , we have stressed that typical adaptation , modifying a model trained on the source domain , will fail but there may be unsupervised parsing techniques that improve performance after adaptation , such as a rule based NP parser for BIO based on knowledge of the annotations .", "label": "", "metadata": {}, "score": "84.37607"}
{"text": "Since these annotations are new with respect to the WSJ guidelines , it is impossi- ble to parse these without injecting knowledge of the annotation guidelines.3 common , comprising 33 % of BIO and 30 % of WSJ tokens , the most popular POS tag by far .", "label": "", "metadata": {}, "score": "84.46904"}
{"text": "Portland , Oregon .Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 . , Yoshimasa .and Jun'ichi Tsujii .Revision as of 15:46 , 10 December 2012 .", "label": "", "metadata": {}, "score": "84.57741"}
{"text": "Uncontrolled Keywords : .Lexical - Functional Grammar ; LFG ; Treebank Tool Suite ; TTS ; constraint - based grammatical resources", "label": "", "metadata": {}, "score": "85.021225"}
{"text": "In addition , NomBank will contribute to the 2008 CONLL task and will create a small amount of additional annotation in connection therewith .Documentation : NomBank Specifications can be viewed here , but are also included in the nombank tgz and zip archives .", "label": "", "metadata": {}, "score": "85.23921"}
{"text": "For the most common 1While only 8 teams participated in the closed track with us , our score beat all of the teams in the open track .Page 2 .The parser was trained on the provided WSJ data .Digits are less than 4 % of the tokens in BIO .", "label": "", "metadata": {}, "score": "85.27779"}
{"text": "171 - -189 .Springer .Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL 2007 ) ' ' , pages 760 - 767 .Semi - supervised condensed nearest neighbor for part - of - speech tagging .", "label": "", "metadata": {}, "score": "85.67861"}
{"text": "171 - -189 .Springer .Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL 2007 ) ' ' , pages 760 - 767 .Semi - supervised condensed nearest neighbor for part - of - speech tagging .", "label": "", "metadata": {}, "score": "85.67861"}
{"text": "171 - -189 .Springer .Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL 2007 ) ' ' , pages 760 - 767 .Semi - supervised condensed nearest neighbor for part - of - speech tagging .", "label": "", "metadata": {}, "score": "85.67861"}
{"text": "For other uses , e.g. , gigabytes of non - wsj data , there is no particular reason to stick to the training or clean versions of these resources .To clarify , it is illegal for CONLL 2008 participants to use normal NOMLEX - PLUS or COMNOM ( to be distributed by the LDC ) .", "label": "", "metadata": {}, "score": "86.82337"}
{"text": "171 - -189 .Springer .S\u00f8gaard , Anders .Semi - supervised condensed nearest neighbor for part - of - speech tagging .The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .", "label": "", "metadata": {}, "score": "87.083305"}
{"text": "171 - -189 .Springer .S\u00f8gaard , Anders .Semi - supervised condensed nearest neighbor for part - of - speech tagging .The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .", "label": "", "metadata": {}, "score": "87.083305"}
{"text": "171 - -189 .Springer .S\u00f8gaard , Anders .Semi - supervised condensed nearest neighbor for part - of - speech tagging .The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .", "label": "", "metadata": {}, "score": "87.083305"}
{"text": "171 - -189 .Springer .S\u00f8gaard , Anders .Semi - supervised condensed nearest neighbor for part - of - speech tagging .The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .", "label": "", "metadata": {}, "score": "87.083305"}
{"text": "Since the guidelines differ , we observe no corresponding structure in the WSJ .It is telling that the parser labels this BIO example by attaching ev- ery token to the final proper noun \" P1 - 1 \" , exactly as the WSJ guidelines indicate .", "label": "", "metadata": {}, "score": "87.294106"}
{"text": "Both words were unknown only 5 % of the time in BIO , while one of the words being un- known was more common , reflecting 27 % of deci- sions .Upon further investigation , the majority of unknown words were nouns , which indicates that unknown word errors were caused by the problems discussed above .", "label": "", "metadata": {}, "score": "87.802055"}
{"text": "( 2006 ) , which achieved top results in the 2006 We were given around CoNLL - X shared task .Preliminary experiments in- dicated that the edge labeler was fairly robust to do- main adaptation , lowering accuracy by 3 % in the de- velopment domain as opposed to 2 % in the source , so we focused on unlabeled dependency parsing .", "label": "", "metadata": {}, "score": "87.923706"}
{"text": "EMNLP 2002 ' ' .\" TALN'11 \" . \"PACLIC 2009 \" .Proceedings of the 4th International Conference on Language Resources and Evaluation ( LREC'04 ) ' ' .Lisbon , Portugal .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "88.02578"}
{"text": "Our er- ror analysis suggests that the primary cause of loss from adaptation is from differences in the annotation guidelines themselves .Therefore , significant im- provements can not be made without specific knowl- edgeofthetargetdomain'sannotationstandards .No amount of source training data can help if no rele- vant structure exists in the data .", "label": "", "metadata": {}, "score": "88.043915"}
{"text": "Proceedings of the 4th International Conference on Language Resources and Evaluation ( LREC'04 ) ' ' .Lisbon , Portugal . \" 6th Applied Natural Language Processing Conference \" .\"LREC 2008 \" .EMNLP 2002 ' ' .\" TALN'11 \" . \"", "label": "", "metadata": {}, "score": "88.395584"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .", "label": "", "metadata": {}, "score": "88.509766"}
{"text": "Spoustov\u00e1 , Drahom\u00edra \" Johanka \" , Jan Haji\u010d , Jan Raab and Miroslav Spousta .Semi - supervised Training for the Averaged Perceptron POS Tagger .Proceedings of the 12 EACL , pages 763 - 771 .Eric Ringger , Robert C. Moore , Eugene Charniak , Lucy Vanderwende , and Hisami Suzuki May 2004 .", "label": "", "metadata": {}, "score": "88.693954"}
{"text": "To date , the best result achieved by our own Penn - II induced grammars is a dependency f - score of 80.33 % against the PARC 700 , an improvement of 0.73 % over the best handcrafted grammar of ( Kaplan et al .", "label": "", "metadata": {}, "score": "88.74497"}
{"text": "Nouns are far more The annota- 2We measured these drops on several other dependency parsers and found similar results .ery token is headed by \" Department \" .In contrast , a similar BIO phrase has a very different structure , pursuant to the BIO guidelines .", "label": "", "metadata": {}, "score": "88.91043"}
{"text": "References Shai Ben - David , John Blitzer , Koby Crammer , and Fer- nando Pereira .Analysis of representations for domain adaptation .In NIPS .Leo Breiman .Learning , 24(2):123 - 140 .Bagging predictors .Machine R. Brown .", "label": "", "metadata": {}, "score": "88.914665"}
{"text": "The cause for this is clear when the annotation guide- lines are considered .The proper nouns in WSJ are names of companies , people and places , while in BIO they are names of genes , proteins and chemi- cals .", "label": "", "metadata": {}, "score": "89.10401"}
{"text": "An additional 35,000 nouns were not looked at because those nouns never take arguments .This release completes NomBank.1.0 .For previous releases , we looked at about 21,000 instances tagged as likely errors .For this release , we looked at an additional 13,000 instances .", "label": "", "metadata": {}, "score": "89.15655"}
{"text": "We began with the first ap- proach and removed a large number of features that we believed transfered poorly , such as most features for noun - noun edges .We obtained a small improve- ment in BIO performance on limited data only .", "label": "", "metadata": {}, "score": "89.76529"}
{"text": "An additional document entitled \" Those Other NomBank Dictionaries \" is also included in order to provide clearer descriptions of the supplemental dictionaries included with NomBank .Nombank can be downloaded in the form of the attached tgz or zip file .", "label": "", "metadata": {}, "score": "90.0562"}
{"text": "In what follows , we provide an er- ror analysis that attributes domain loss for this task to a difference in annotation guidelines between do- mains .We then overview our attempts to improve adaptation .While we were able to show limited adaptation on reduced training data or with first- order features , no modifications improved parsing with all the training data and second - order features .", "label": "", "metadata": {}, "score": "90.33865"}
{"text": "Instead , we scaled each feature 's value by a factor proportional to its frequency in the target do- main and trained the parser on these scaled feature values .We obtained small improvements on small amounts of training data .Target Focused Learning 4 Future Directions Given our pessimistic analysis and the long list of failed methods , one may wonder if parser adapta- tion is possible at all .", "label": "", "metadata": {}, "score": "90.608185"}
{"text": "We found certain scaling techniques obtained tiny improvements on the target domain that , while significant compared to competition results , are not statistically significant .We also attempted a sim- ilar approach on the feature level .A very predic- tive source domain feature is not useful if it does not appear in the target domain .", "label": "", "metadata": {}, "score": "90.90627"}
{"text": "171 - -189 .Springer .Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL 2007 ) ' ' , pages 760 - 767 .Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics ( ACL 2007 ) ' ' , pages 760 - 767 .", "label": "", "metadata": {}, "score": "91.59881"}
{"text": "Harvard University Press .Koby Crammer , Ofer Dekel , Joseph Keshet , Shai Shalev-Shwartz , and Yoram Singer .Online passive- aggressive algorithms .Journal of Machine Learning Research , 7:551 - 585 , Mar. R. Johansson and P. Nugues .", "label": "", "metadata": {}, "score": "91.839325"}
{"text": "\" TALN'11 \" . \"PACLIC 2009 \" .Proceedings of the 4th International Conference on Language Resources and Evaluation ( LREC'04 ) ' ' .Lisbon , Portugal .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "91.84979"}
{"text": "In addition to a change in the annotation guide- lines for NPs , we observed an important difference in the distribution of POS tags .NN tags were almost twice as likely in the BIO domain ( 14 % in WSJ and 25 % in BIO ) .", "label": "", "metadata": {}, "score": "92.6224"}
{"text": "( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )The splits of data for this .data set . were not standardized early on ( unlike for parsing ) and early work uses various data splits defined by counts of tokens or by sections .", "label": "", "metadata": {}, "score": "92.967834"}
{"text": "Seraji , Mojgan .Morphosyntactic Corpora and Tools for Persian .Doctoral dissertation , Uppsala University .Studia Linguistica Upsaliensia 16 .[ pdf ] Conference : EMNLP - CoNLL 2007 , Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning , June 28 - 30 , 2007 , Prague , Czech Republic .", "label": "", "metadata": {}, "score": "94.17168"}
{"text": "Stanford Typed Dependencies Representation .In Proceedings of the COLING'08 Workshop on Cross - Framework and Cross - Domain Parser Evaluation .Nivre J. , Hall J. , and Nilsson J. 2006 .Maltparser : A data - driven parser - generator for dependency parsing .", "label": "", "metadata": {}, "score": "94.91545"}
{"text": "Proceedings of the 4th International Conference on Language Resources and Evaluation ( LREC'04 ) ' ' .Lisbon , Portugal . . .+ .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "95.72275"}
{"text": "Portland , Oregon .Proceedings of HLT - NAACL 2003 ' ' , pages 252 - 259 .Proceedings of the Fifteenth Conference on Computational Natural Language Learning ' ' , pp 238 - 246 , 2011 .Revision as of 15:46 , 10 December 2012 .", "label": "", "metadata": {}, "score": "96.74414"}
{"text": "Portland , Oregon .Proceedings of HLT - NAACL 2003 ' ' , pages 252 - 259 .Proceedings of the Fifteenth Conference on Computational Natural Language Learning ' ' , pp 238 - 246 , 2011 .Revision as of 15:46 , 10 December 2012 .", "label": "", "metadata": {}, "score": "96.74414"}
{"text": "Printed / Distributed with the permission of ELRA .This paper was published within the proceedings of the LREC'2004 Conference .\u00a9 2004 ELRA - European Language Resources Association .All rights reserved .( The convention is for this to be measured on all tokens , including punctuation tokens and other unambiguous tokens . )", "label": "", "metadata": {}, "score": "97.14789"}
{"text": "Introduction Evaluating semantic relatedness using network representations is a problem with a long history in arti#cial intelligence and psychology , dating back to the spreading activation approach of Quillian # 1968 # and Collins and Loftus # 1975#.Semantic similarity represents a special case of semantic relatedness : for example , cars and gasoline would seem to be more closely related than , say , cars and bicycles , but the latter pair are certainly more similar .", "label": "", "metadata": {}, "score": "97.5322"}
{"text": "3 Adaptation Approaches We survey the main approaches we explored for this task .While some of these approaches provided a modest performance boost to a simple parser ( lim- ited data and first - order features ) , no method added any performance to our best parser ( all data and second - order features ) .", "label": "", "metadata": {}, "score": "98.01965"}
{"text": "An example is the above BIO NP , in which the phrase \" glutathione transferase P1 - 1 \" is an appositive indicating which \" enzyme \" is meant .However , since there are no commas , the parser thinks \" P1 - 1 \" is the head .", "label": "", "metadata": {}, "score": "98.15268"}
{"text": "For ex- ample , trained on in - domain data , nouns that occur more often tend to be heads .However , none of these features transfered between domains .A final type of feature we added was based on the behavior of nouns , adjectives and verbs in each domain .", "label": "", "metadata": {}, "score": "98.28187"}
{"text": "Lisbon , Portugal .Proceedings of the 4th International Conference on Language Resources and Evaluation ( LREC'04 ) ' ' .Lisbon , Portugal .Part - of - Speech Tagging from 97 % to 100 % : Is It Time for Some Linguistics ?", "label": "", "metadata": {}, "score": "98.436195"}
{"text": "This decision effectively removes NNP from the BIO domain and renders all features that depend on the NNP tag ineffective .In our above BIO NP example , all nouns are labeled NN , whereas the WSJ example contains NNP tags .", "label": "", "metadata": {}, "score": "98.75334"}
{"text": "Please note that we are in the process of determining how to distribute some additional files that require licenses from the LDC .We will update this website with instructions for obtaining these as soon as we can .Previous Releases : We have removed preliminary releases of NomBank from our website .", "label": "", "metadata": {}, "score": "100.26109"}
{"text": "We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge , as well as conjunctions with edge types .This failed to improve BIO accuracy since these features were less reliable at test time .", "label": "", "metadata": {}, "score": "100.44487"}
{"text": "In Proc . of the 16th Nordic Conference on Computational Linguistics ( NODALIDA ) .Extended Sandra K\u00a8 ubler . schemes influence parsing results ? or how not to com- pare apples and oranges .In RANLP .How do treebank annotation 1054 .", "label": "", "metadata": {}, "score": "103.3168"}
{"text": "While we believe this is not enough diversity , it was not feasible to repeat our experiment with a large number of parsers . 3.3Another approach to adaptation is to favor training examples that are similar to the target .We first mod- ified the weight given by the parser to each training sentence based on the similarity of the sentence to target domain sentences .", "label": "", "metadata": {}, "score": "103.82224"}
{"text": "We selected with replacement 2000 training examples from the training data and trained three parsers .Each parser then tagged the remain- ing 13 K sentences , yielding 39 K parsed sentences .We then shuffled these sentences and trained a final parser .", "label": "", "metadata": {}, "score": "104.021515"}
{"text": "These fine grained tags provide more information than coarse tags ; experiments that removed fine grained tags 1052 .Page 3 . hurt WSJ performance but did not affect BIO .Finally , we examined the effect of unknown words .Not surprisingly , the most significant dif- ferences in error rates concerned dependencies be- tween words of which one or both were unknown to the parser .", "label": "", "metadata": {}, "score": "104.16365"}
{"text": "The 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies ( ACL - HLT ) .Portland , Oregon .Proceedings of HLT - NAACL 2003 ' ' , pages 252 - 259 .Proceedings of HLT - NAACL 2003 ' ' , pages 252 - 259 .", "label": "", "metadata": {}, "score": "104.168304"}
{"text": "NomBan k was supported under Grants N66001 - 001 - 1 - 8917 and N66001 - 04 - 1 - 8920 from the Space and Naval Warfare Systems Center San Diego and National Science Foundation grant CNI-0551615 .The reports and papers of the NomBank project do not necessarily reflect the position or the policy of the U.S. Government .", "label": "", "metadata": {}, "score": "104.18598"}
{"text": "5 Acknowledgments We thank Joel Wallenberg and Nikhil Dinesh for their informative and helpful linguistic expertise , Kevin Lerman for his edge labeler code , and Koby Crammer for helpful conversations .Dredze is sup- ported by a NDSEG fellowship ; Ganchev and Taluk- dar by NSF ITR EIA-0205448 ; and Blitzer by DARPA under Contract No . NBCHD03001 .", "label": "", "metadata": {}, "score": "104.19032"}
{"text": "Further experiments showed that any decrease in training data hurt parser perfor- mance .It would seem that the parser has no dif- ficulty learning important training sentences in the presence of unimportant training examples .A related idea focused on words , weighing highly tokens that appeared frequently in the target domain .", "label": "", "metadata": {}, "score": "104.92727"}
{"text": "Another problem concerns appositives .For ex- ample , the phrase \" Howard Mosher , president and chief executive officer , \" has \" Mosher \" as the head of \" Howard \" and of the appositive NP delimited by commas .", "label": "", "metadata": {}, "score": "105.87007"}
{"text": "Proceedings of the CoNLL Shared Task Session of EMNLP - CoNLL 2007 , pp .1051 - 1055 , Prague , June 2007 .c ?upenn.edu 2L2F - INESC - ID Lisboa / IST , Rua Alves Redol 9 , 1000 - 029 , Lisboa , Portugal javg@l2f.inesc-id.pt Abstract We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .", "label": "", "metadata": {}, "score": "108.81331"}
{"text": "These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .", "label": "", "metadata": {}, "score": "109.739075"}
{"text": "NN , NNP - NNP - NNP , NN - IN - NN , and IN - NN - NN .However , when we examine the coarse POS tags , which do not distinguish between nouns , these dif- ferences disappear .", "label": "", "metadata": {}, "score": "112.54846"}
{"text": "Adaptation techniques focus on the former since it is impossible to determine the lat- ter without knowledge of the labeling function .In parsing adaptation , the former corresponds to a dif- ference between the features seen in each domain , such as new words in the target domain .", "label": "", "metadata": {}, "score": "113.60448"}
