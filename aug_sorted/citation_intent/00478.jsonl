{"text": "We show , contrary to a widely held belief that discriminative classifiers are almost always to be preferred , that there can often be two distinct regimes of performance as the training set size is increased , one in which each algorithm does better .", "label": "", "metadata": {}, "score": "20.035902"}
{"text": "It is shown that the rates of convergence of classifiers depend on two parameters : the complexity of the class of candidate sets and the margin parameter .The main result of the paper concerns optimal aggregation of classifiers : we suggest a classifier that automatically adapts both to the complexity and to the margin , and attains the optimal fast rates , up to a logarithmic factor .", "label": "", "metadata": {}, "score": "23.185314"}
{"text": "The more accurate each base classifier and the more diverse different base classifiers , the better the classification performance of the ensemble learning .However , these two factors are conflicting ; that is , with the increase in average accuracy , the average diversity inevitably declines , and vice versa .", "label": "", "metadata": {}, "score": "23.50758"}
{"text": "To exactly model the distribution of the observation vector , all possible combinations of values in the feature space have to be considered , resulting in a large number of parameters and requiring accordingly large numbers of training elements .As demonstrated by Kasteren et al .", "label": "", "metadata": {}, "score": "23.70543"}
{"text": "Consequently , the proposed learning algorithm is compact with a smaller number of weak classifiers compared to the conventional learning algorithms but is capable of producing a strong classifier with the same performance .As a result , a faster and more accurate detector can be constructed for object detection .", "label": "", "metadata": {}, "score": "24.021786"}
{"text": "The changing weights of sample vectors in the first phase , as well as increasing the size of classifier pool , can be easily recast as a moving classification goal , described in [ 3 ] .This is an additional reason why we only update a single weak classifier , rather than all of them , since , otherwise , the drift would be proportional to the number of classifiers added , significantly penalizing convergence rate .", "label": "", "metadata": {}, "score": "24.811718"}
{"text": "Results have shown that the proposed learning algorithm demonstrated better classification performance than the conventional learning algorithm .It can be stated that the proposed learning algorithm is a suitable approach to training classifiers with LPR in object detection and recognition tasks .", "label": "", "metadata": {}, "score": "25.313204"}
{"text": "For these classifiers , no function is applied to their output , since they are focused to directly recognize the activities , taking the sensor features as input .The tree - based classifier is modeled by the C 4.5 algorithm [ 45 ] , a widely employed algorithm to generate decision trees .", "label": "", "metadata": {}, "score": "26.099003"}
{"text": "During the experimentation these feature representations were used standalone and combined .Combining the feature representations was done by concatenating the feature matrices .As can be noticed in Table 2 , datasets suffer from a severe class imbalance problem due to the nature of the data .", "label": "", "metadata": {}, "score": "27.85677"}
{"text": "The learning algorithm is applied to the original gene datasets as well as each newly obtained dataset containing only the selected genes , and in each case the final overall accuracy is measured .Figure 1 summarizes the learning accuracy of decision tree classifier on different feature sets .", "label": "", "metadata": {}, "score": "27.997337"}
{"text": "To avoid the weakness in filtering methods , wrapper methods wrap around a particular learning algorithm that can assess the selected feature subsets in terms of the estimated classification errors and then build the final classifier [ 20 ] .Wrapper methods use a learning machine to measure the quality of the subsets of features .", "label": "", "metadata": {}, "score": "28.533422"}
{"text": "It is therefore essential to develop general approaches and robust methods that are able to overcome the limitation of the small number of training instances and reduce the influence of uncertainties so as to produce reliable classification results .The motivation for this study is to utilize robust ensemble methods that are less sensitive to the selection of genes and are capable of removing the uncertainties of gene expression data .", "label": "", "metadata": {}, "score": "28.657303"}
{"text": "Statnikov et al .[ 25 ] systematically assessed these strategies by performing experiments and found that OAA often produces better classification accuracy .In the present study , we use OAA as a baseline coding strategy .We also note that this decomposition can further damage the equilibrium of training instances .", "label": "", "metadata": {}, "score": "28.716122"}
{"text": "That is , the base learners utilized in a robust ensemble classifier should be of high classification accuracy and avoid making coincident misclassification errors which in turn necessitate the diverse learners .Thus , a sample misclassified by a base learner will be corrected by others , so the fused outputs are more accurate than that of the best individual classifier [ 37 ] .", "label": "", "metadata": {}, "score": "29.237097"}
{"text": "Instead of determining the threshold with a set of real numbers , these classifiers typically employ a lookup table as means of determining the classification boundaries .An example of a classifier using a lookup table can be implemented as shown in Figure 2 .", "label": "", "metadata": {}, "score": "29.620693"}
{"text": "As affirmed previously in Section 2 , the advantage of LPR based method we propose here is its fast computation due to a small number of candidates of weak classifiers .For example , the Haar - like feature based method selects a large number of weak classifiers from a significantly larger set of candidates of weak classifiers to construct a strong classifier .", "label": "", "metadata": {}, "score": "29.704618"}
{"text": "In particular , we introduce an optimal selection of weak classifiers minimizing the cost function and derive the reinforced predictions based on a judicial confidence estimate to determine the classification results .However , the weak classifier of the proposed method produces a real value which reflects the confidence level of the prediction .", "label": "", "metadata": {}, "score": "29.909962"}
{"text": "However , a necessary and sufficient condition for an ensemble to outperform its individual members is that the base classifiers should be accurate and diverse [ 18 ] .An accurate classifier is one that has an error rate of better than randomly guessing classes for new unseen samples .", "label": "", "metadata": {}, "score": "30.182522"}
{"text": "In short , the classifier produces the decision results by finding the designated prediction corresponding to each feature value .The LPR based method uses not only finite - integer numbers as feature values but also a small number of pixel locations within an image patch as candidates of weak classifiers .", "label": "", "metadata": {}, "score": "30.348663"}
{"text": "In summary , we have selected the best weak classifier with the smallest error rate for constructing a strong classifier and derived a reinforced prediction of weak classifier in the Adaboost learning . , and ( 4 ) update the sample weights individually depending on the confidence level of reinforced prediction .", "label": "", "metadata": {}, "score": "30.79456"}
{"text": "On the other hand , by using SVM , we obtained a unique classification model derived from unique best training in each experiment ; therefore , the results in Tables 1 and 2 are the same with the use of SVMs .", "label": "", "metadata": {}, "score": "30.895267"}
{"text": "Adding to this the fact that in most real - time applications the exact form of the data distribution is not known , we have decided to compare algorithm performance using experimental data .Possible Extensions .In this section , we discuss several possible extensions of our algorithm .", "label": "", "metadata": {}, "score": "31.104567"}
{"text": "In all ensemble experiments a classification tree [ 16 ] was exploited as the base learner because it is sensitive to the changes in its training data .In order to provide a fair comparison , for all utilized ensemble techniques 100 trees are trained to constitute the corresponding ensemble classifiers .", "label": "", "metadata": {}, "score": "31.209122"}
{"text": "( e ) .( f ) .Output : Ranked feature list R. .Wrapper methods can noticeably reduce the number of features and significantly improve the classification accuracy [ 22 ] .However , wrapper methods have the drawback of having a high computational load .", "label": "", "metadata": {}, "score": "31.653221"}
{"text": "We provide here as a starting point two plausible explanations supported by theory and a simulation experiment ( in Additional File 2 ) .We note that prior research has established that linear decision functions capture very well the underlying distributions in microarray classification tasks [ 15 , 16 ] .", "label": "", "metadata": {}, "score": "31.757252"}
{"text": "The exact values of these parameters depend on both the complexity of the classification function and the number of genes in a microarray dataset .Therefore , in general , it is advisable to optimize these parameters by nested cross - validation that accounts for the variability of the random forest model ( e.g. , the selected parameter configuration is the one that performs best on average over multiple validation sample sets ) .", "label": "", "metadata": {}, "score": "31.86515"}
{"text": "On the contrary , the proposed learning algorithm allows the sample weights to be updated individually depending on the confidence level of each weak classifier prediction , thereby reducing the number of weak classifier iterations for convergence .Experimental classification performance on human face and license plate images confirm that the proposed method requires smaller number of weak classifiers than the conventional learning algorithm , resulting in higher learning and faster classification rates .", "label": "", "metadata": {}, "score": "31.922293"}
{"text": "These ensemble techniques have the advantage to alleviate the small sample size problem by averaging and incorporating over multiple classification models to reduce the potential for overfitting the training data [ 16 ] .In this way the training data set may be used in a more efficient way , which is critical to many bioinformatics applications with small sample size .", "label": "", "metadata": {}, "score": "31.95366"}
{"text": "In this case , the entire sample weights are updated by the same rate regardless of the confidence levels of predictions .Consequently , it may lead to slow convergence requiring many rounds of iterations for constructing weak classifiers .However , in our reinforced AdaBoost learning method , the weak classifier .", "label": "", "metadata": {}, "score": "32.409374"}
{"text": "Given that the number of optimal genes varies from dataset to dataset , and that SVMs are known to be fairly insensitive to a very large number of irrelevant genes , such application of SVMs likely biases down their performance .Second , a one - versus - one SVM algorithm was applied for the multicategory classification tasks , while it is has been shown that in microarray gene expression domain this method is inferior to other multicategory SVM methods , such as one - versus - rest [ 1 , 6 ] .", "label": "", "metadata": {}, "score": "32.433575"}
{"text": "In this paper a model for automatically learning bias is investigated .The central assumption of the model is that the learner is embedded within an environment of related learning tasks .Within such an environment the learner can sample from multiple tasks , and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment .", "label": "", "metadata": {}, "score": "32.502136"}
{"text": "In the case of AdaBoost learning , the constructed classifier is guided with reward in the direction of the fastest route toward the final convergence [ 29 ] .In particular , the classifier requires a cumulative reward feedback on the result of the classification .", "label": "", "metadata": {}, "score": "32.673244"}
{"text": "The decision threshold is adjusted by using the following default equation [ 17 ] : .Ensemble Learning Framework Based on Feature Subspace and Counter Voting Integration Rule for Classifying Imbalanced Multiclass Cancer Microarray Data .Ensemble learning often provides a framework to generate multiple weak classifiers and aggregates these by using an integration rule to become a strong classifier .", "label": "", "metadata": {}, "score": "32.726788"}
{"text": "In practice , it appeared to be difficult to define a single measure of diversity and even more difficult to relate that measure to the ensemble performance in a neat and expressive dependency .Here , to investigate the ability of the proposed ICA - based RotBoost ensemble to build accurate and diverse base learners efficiently , the pairwise diversity measure is utilized [ 40 ] .", "label": "", "metadata": {}, "score": "32.81072"}
{"text": "( 5 ) can be reformulated as an instance of semidefinite programming ( SDP ) [ 35 ] and the global minimum of eq .( 5 ) can be efficiently computed .( 4 ) is .Other learning classifiers .", "label": "", "metadata": {}, "score": "32.9154"}
{"text": "The classification performance of our proposed algorithms is restricted by many factors , including the size of feature space , the size of feature subspace , and the number of base classifiers ; the size of feature subspace is the most significant factor .", "label": "", "metadata": {}, "score": "32.952187"}
{"text": "Specifically , if the generative linear decision function is not orthogonal to the coordinate axes , then a decision tree of infinite size is required to represent this function without error [ 17 ] .SVMs on the other hand use linear classifiers and thus can model such functions naturally , using a small number of free parameters ( i.e. , bounded by the available sample size ) .", "label": "", "metadata": {}, "score": "33.44281"}
{"text": "These assumptions lead to two distinct models , which are often confused .[ 9 ] [ 10 ] .When dealing with continuous data , a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution .", "label": "", "metadata": {}, "score": "33.534843"}
{"text": "If we compare the results shown in Table 1 and Table 2 , we found that the results obtained by using SVMs are the same in both tables , but the results of using other classifiers are different .In each experiment , with the use of other classifiers , there are multiple classification models , derived from the best trainings with different feature numbers .", "label": "", "metadata": {}, "score": "33.56727"}
{"text": "First , it provides a better probabilistic model of the data , which can better identify where the data concentrate in .-dimensional space .Second , it can find a not necessarily orthogonal basis , which may reconstruct the data better than PCA in the presence of noise .", "label": "", "metadata": {}, "score": "34.41288"}
{"text": "Next , we built a classification model with the best parameters on the original training set and applied this model to the original testing set .Details about the \" nested cross - validation \" procedure can be found in [ 19 , 20 ] .", "label": "", "metadata": {}, "score": "34.455803"}
{"text": "The learning classifiers , listed in Table 1 , are used for the training data and the testing data consisting of the feature sets chosen by SVMRFE and GLGS .In each experiment , 80 % samples are randomly chosen for training , and the remaining 20 % samples are tested .", "label": "", "metadata": {}, "score": "34.468388"}
{"text": "We consider these results to be related to a weighted combination of number of classes , class imbalance ratio , and class overlapping , as explained by previous studies [ 19 , 50 , 51 ] .( iii ) Both THR and RUS correction technologies help SVM - OAA classifier promote classification performance on those sensitive datasets .", "label": "", "metadata": {}, "score": "34.63723"}
{"text": "Following transformation , the axes are rotated optimally .Despite the conventional approach of choosing some directions for good discriminate capability , the rotation mainly contributes to the generation of diversity among the classifiers without weakening the individual classifiers .Thus , an acceptable trade - off between diversity and accuracy can be maintained simultaneously .", "label": "", "metadata": {}, "score": "34.740532"}
{"text": "Feature selection .To address the \" curse of dimensionality \" problem , three strategies have been proposed : filtering , wrapper and embedded methods .Filtering methods select subset features independently from the learning classifiers and do not incorporate learning .", "label": "", "metadata": {}, "score": "34.754433"}
{"text": "Japkowicz , N. ; Stephen , S. The class imbalance problem : A systematic study .Intell .Data Anal .[ Google Scholar ] .Dem\u0161ar , J. Statistical comparisons of classifiers over multiple data sets .J. Mach .Learn .", "label": "", "metadata": {}, "score": "34.87361"}
{"text": "When an additional classifier is created , a new . is created and initialized with zeroes .:The number of iterations current weak classifier have been trained .Using established convergence bounds of Pegasos algorithm , can be used as a simplest form of cutoff parameter , that is , each additional weak classifier is trained on a fixed number of samples .", "label": "", "metadata": {}, "score": "34.884296"}
{"text": "Yet , the combination of these features may have a combination effect that does not necessarily follow from the individual performances of the features in that group .One of the consequences of the filtering methods is that we may end up with many highly correlated features ; yet , any highly redundant information will worsen the classification and prediction performance .", "label": "", "metadata": {}, "score": "34.95113"}
{"text": "This allows creation of the classifiers that compensate for the errors introduced by the previous ones , and eventual convergence to the true distribution of the samples .After the weak classifier , boosting weights are adjusted , using vector . as an input .", "label": "", "metadata": {}, "score": "34.985092"}
{"text": "Using this conversion , the discriminative models can be integrated into hybrid structural - connectionist models via a statistical framework [ 39 ] , obtaining at each time slice the emission probabilities needed for the HMMs with better discriminating properties and without any hypothesis on the statistical distribution of the data .", "label": "", "metadata": {}, "score": "35.002518"}
{"text": "Previous approaches have shown how simple binary sensors have solid potential for solving the ADL recognition problem in the home [ 13 ] , and can be applied in human - centric problems such as health and elder care [ 14 - 16 ] .", "label": "", "metadata": {}, "score": "35.043564"}
{"text": "The raw data streams generated by the sensor networks can either be used directly or preprocessed into a different representation form .In order to augment the features space and to obtain further evaluation of our models , in this work we have experimented with different feature representations , originally proposed by Kasteren et al .", "label": "", "metadata": {}, "score": "35.098156"}
{"text": "On the other hand , one of the drawbacks of this hybrid approach is that we have to use fully labeled datasets to train the classifiers .The hybrid HMM model training is done by an iterative Expectation - Maximization algorithm , as proposed by Reference [ 39 ] .", "label": "", "metadata": {}, "score": "35.21337"}
{"text": "The choice of the classifiers included in the comparison is based on the activity recognition study presented by Bao et . al .[ 44 ] .These discriminative models are : an MLP , an SVM , a tree - based classifier , a rule - based classifier and an instance - based classifier .", "label": "", "metadata": {}, "score": "35.228943"}
{"text": "Gene selection is then performed on the training set , and the goodness of selected genes is assessed from the unseen test set [ 31 ] .However , due to the small number of instances in gene microarray datasets , such an approach can lead to unreliable results .", "label": "", "metadata": {}, "score": "35.55863"}
{"text": "Specifically , we show how the hybrid system obtained by using an SVM to estimate the emission probabilities of an HMM outperforms other well known sequential pattern recognition approaches .By incorporating the time modelling abilities of the HMM to the discriminative skills of the classifier we obtain an efficient scheme that is able to deal with the diverse statistical challenges presented in recognizing human activities and overcome the weakness of HMMs as effective classifiers .", "label": "", "metadata": {}, "score": "35.863903"}
{"text": "Our approach in Adaboost algorithm is to minimize the upper bound of training error .For this purpose , we employ an exponential criterion and Newton 's method in optimization .We use the exponential criterion as a cost function for its differentiability to set the upper bound on the misclassification rate for optimizing the model of classifiers .", "label": "", "metadata": {}, "score": "35.868378"}
{"text": "This is coincident with the observation previously given that the diversity usually conflicts with the accuracy of the base learners .However our experiments indicate that the ICA - based RotBoost ensemble could establish a proper balance between the diversity and the accuracy of the constituted base learners .", "label": "", "metadata": {}, "score": "36.202065"}
{"text": "In step ( 1 ) , a classifier is trained using RPPA and drug sensitivity data .A single classifier is generated per each drug which means the number of classifiers is same as the number of drugs .Based on the result of the prediction , the classifier can recommend a set of drugs that is more likely to have Low sensitivity .", "label": "", "metadata": {}, "score": "36.211258"}
{"text": "For example , the goal of margin maximization and a convex objective function is based on the hinge loss .In multi - classification , the training time of SVMs scales at least linearly in the number of classes .By contrast , LMNN has no explicit dependence on the number of classes [ 34 ] .", "label": "", "metadata": {}, "score": "36.266808"}
{"text": "There are largely two kinds of transformation methods , that is , PCA and ICA .PCA projects the data into a new space spanned by the principal components [ 26 ] .In contrast to PCA , ICA decomposes an input dataset into components so that each component is statistically as independent from the others as possible .", "label": "", "metadata": {}, "score": "36.314743"}
{"text": "There exist several algorithms to deal with such a problem that have well - established convergence bound .Amongst them , methods using stochastic gradient descent ( or , in case of hinge - loss function , subgradient descent ) are most prominent .", "label": "", "metadata": {}, "score": "36.32184"}
{"text": "By doing so , no matter how the number of rounds increases , the number of combined classifiers for constructing a strong classifier can be fixed , while the performance of the strong classifier improves . is the LPR feature value .", "label": "", "metadata": {}, "score": "36.607735"}
{"text": "Specifically , support vector machine was used as base classifier , and a novel voting rule called counter voting was presented for making a final decision .Experimental results on eight skewed multiclass cancer microarray datasets indicate that unlike many traditional classification approaches , our methods are insensitive to class imbalance .", "label": "", "metadata": {}, "score": "36.613884"}
{"text": "In the AdaBoost learning , the constructed strong classifier can be optimized by updating the weights of the samples that indicate their importance for the classification .On each round , the weight of the correctly classified samples is decreased , whereas the weight of the misclassified samples is increased .", "label": "", "metadata": {}, "score": "36.697624"}
{"text": "The experiment shows that the choice of RF parameters creates large variation in the classifier performance whereas the choice of the main SVM parameter has only minor effects on the error .In practical analysis of microarrays this means that finding the RFs with optimal error for the dataset may involve extensive model selection which in turn opens up the possibility for overfitting given the small sample sizes in validation datasets .", "label": "", "metadata": {}, "score": "36.76561"}
{"text": "In learning extremely imbalanced data , the overall classification accuracy is considered not an appropriate measure of performance .A trivial classifier that predicted every instance as the majority class could achieve very high accuracy .Since in our case rare classes are of interest , we evaluate the models using F - Measure , a measure that considers the correct classification of each class equally important .", "label": "", "metadata": {}, "score": "36.81517"}
{"text": "A theoretical analysis of the sparsification method reveals its close affinity to kernel PCA , and a data - dependent loss bound is presented , quantifying the generalization performance of the KRLS algorithm .We demonstrate the performance and scaling properties of KRLS and compare it to a stateof -the - art Support Vector Regression algorithm , using both synthetic and real data .", "label": "", "metadata": {}, "score": "36.96027"}
{"text": "The parameters \u03b3 and r were set to default value 1 .Random forest classifiers .Random forests ( RF ) is a classification algorithm that uses an ensemble of unpruned decision trees , each of which is built on a bootstrap sample of the training data using a randomly selected subset of variables [ 2 ] .", "label": "", "metadata": {}, "score": "37.078606"}
{"text": "Because of the impact of different uncertainties together with the lack of labeled training samples , the conventional machine learning techniques face complicated challenges to develop reliable classification models .Quite often selecting only a few genes can discriminate a majority of training instances correctly [ 14 ] .", "label": "", "metadata": {}, "score": "37.134903"}
{"text": "The total error rate of a certain pixel location is calculated by adding its bin error rates as shown in Algorithm 1 .Figure 4 : Selection of weak classifier for each round in AdaBoost learning .At first , LPR images are extracted from the training images .", "label": "", "metadata": {}, "score": "37.14528"}
{"text": "In this paper , we dealt with multiclass imbalanced classification problem , as encountered in cancer DNA microarray , by using ensemble learning .We utilized one - against - all coding strategy to transform multiclass to multiple binary classes , each of them carrying out feature subspace , which is an evolving version of random subspace that generates multiple diverse training subsets .", "label": "", "metadata": {}, "score": "37.14777"}
{"text": "This Viterbi procedure uses the class priors estimated from the relative frequencies of each class in the training data .Experimental Setup and Results .To properly evaluate the presented approach , it has been tested on a real domain using real datasets .", "label": "", "metadata": {}, "score": "37.283684"}
{"text": "Section 3 explains the proposed learning algorithm in detail .Experimental results are provided in Section 4 .Finally , conclusions are drawn in Section 5 .Local Pattern Representations and Design of Classifier .Selection of feature sets is obviously very important for detecting the intended objects from background , but it is particularly crucial in learning based classifiers .", "label": "", "metadata": {}, "score": "37.286945"}
{"text": "So there was no consistent relationship between the classification accuracy and . was the optimum choice to establish a proper balance between the overall classification accuracy and the diversity of the based learners for most examined gene datasets .To find the most promising transformation method and preserve both diversity and accuracy of the base classifiers , we conduct experiments with two well - known transforms , that is , PCA and ICA .", "label": "", "metadata": {}, "score": "37.29164"}
{"text": "Based on the above explanation , we employed SVMRFE and GLGS algorithms for feature selection in our experimental study .Learning classifiers .Support vector machines .SVM [ 23 ] has been widely used in classification .It constructs an optimal hyperplane decision function in feature space that is mapped from the original input space by using kernels , briefly introduced as follows : .", "label": "", "metadata": {}, "score": "37.350742"}
{"text": "Statistical comparison among classifiers .When comparing two classifiers , it is important to assess whether the observed difference in classification performance is statistically significant or simply due to chance .We assessed significance of differences in classification performance in individual datasets or in all datasets on average using a non - parametric permutation test [ 29 ] based on the theory of [ 30 ] .", "label": "", "metadata": {}, "score": "37.371407"}
{"text": "New samples are classified according to the side of the hyperplane they belong to [ 22 ] .Many extensions of the basic SVM algorithm can handle multicategory data .The \" one - versus - rest \" SVM works better for multi - class microarray data [ 1 , 6 ] , so we adopted this method for the analysis of multicategory datasets in the present study .", "label": "", "metadata": {}, "score": "37.488487"}
{"text": "Our proposed algorithms outperform other classification algorithms , including several subtle multiclass imbalance classification algorithms [ 19 , 41 , 42 ] , in terms of all evaluation criteria for most datasets and especially on the sensitive ones .During the experiments , we observed an interesting phenomenon : EnSVM - OAA(RUS ) generally has more stable performance than its partner , although EnSVM - OAA(THR ) produces slightly better recognition results on several datasets .", "label": "", "metadata": {}, "score": "37.488655"}
{"text": "There is no dataset where RFs outperform SVMs with statistically significant difference .The number of genes selected on average across 10 cross - validation training sets is provided in Table 3 .We note that in the present comparison we focus exclusively on classification performance and do not incorporate number of selected genes in the comparison metrics because there is no well - defined trade - off between number of selected genes and classification performance in the datasets studied .", "label": "", "metadata": {}, "score": "37.519005"}
{"text": "However , we have observed that these methods are not sufficiently effective in classifying high - dimensional data .Therefore , we used a modified random subspace method [ 38 ] , and proposed an FSS generation strategy , which is described below .", "label": "", "metadata": {}, "score": "37.565964"}
{"text": "It organizes all binary classifiers into one hierarchical structure ( see Figure 1(d ) ) and makes a decision for test samples from root to leaf , which is helpful for decreasing time complexity of the testing process .To our knowledge , no previous work has considered the effect of class imbalance on these coding strategies , although some have indicated that it is , in fact , harmful [ 33 , 34 ] .", "label": "", "metadata": {}, "score": "37.57183"}
{"text": "It should be noted that we have followed the recommendations from Brush et al .[ 41 ] , in the experimentation process and in the presentation of the results for the activity recognition domain .This section is organized as follows .", "label": "", "metadata": {}, "score": "37.68104"}
{"text": "In this manner , the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model .[ 3 ] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below .", "label": "", "metadata": {}, "score": "37.74934"}
{"text": "In our experimentation , the generative / discriminative combination has been proved to outperform as much to the HMM as to the discriminative model employed ( either the MLP or the SVM ) .Besides , it is also remarkable that , when dealing with binary sensor features , the activity recognition algorithm based on SVM generalizes better than the MLP approach , in both hybrid and sliding window configurations .", "label": "", "metadata": {}, "score": "37.77629"}
{"text": "Finally , a novel voting rule based on counter voting was presented , which made the final decision in ensemble learning .We evaluated the proposed method by using eight multiclass cancer DNA microarray datasets that have different numbers of classes , genes , and samples , as well as class imbalance ratios .", "label": "", "metadata": {}, "score": "37.783794"}
{"text": "The proposed method achieved the highest averaged generalization ability compared to its counterparts and denoted an acceptable level of diversity among the learners for majority of the analyzed benchmark datasets .J. Khan , J. S. Wei , M. Ringn\u00e9r et al . , \" Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks , \" Nature Medicine , vol .", "label": "", "metadata": {}, "score": "38.07202"}
{"text": "Indeed , this kind of sensors is considered one of the most promising technologies to solve key problems in the ubiquitous computing domain , due to their suitability to supply constant supervision and their inherent non - intrusive characteristics .In different studies , several models have been used to recognize ADL from sensor streams , such as Bayesian Networks [ 14 ] , Conditional Random Field [ 18 ] or Evolving Classifiers [ 19 ] .", "label": "", "metadata": {}, "score": "38.100643"}
{"text": "Also unlike [ 1 ] , we only train the last weak classifier of the set rather than all of them .The reason for this is that fixing parameters of all classifiers previously added increases the accuracy of the error rate estimation for the training of additional classifier , as well as significantly decreasing the amount of calculations needed for a single update .", "label": "", "metadata": {}, "score": "38.237225"}
{"text": "In terms of generalization accuracy , ICA - based RotBoost ensemble in conjunction with fast correlation - based filter demonstrated superior average performance over all considered ensemble classifiers and is therefore recommended as an efficient classification technique for the prediction of new gene microarray class labels .", "label": "", "metadata": {}, "score": "38.295116"}
{"text": "We examine the algorithms both in the settings when no gene selection is performed and when several popular gene selection methods are used .To make our evaluation more relevant to practitioners , we focus not only on diagnostic datasets that are in general known to have strong predictive signals , but also include several outcome prediction datasets where the signals are weaker and larger gene sets are often required for optimal prediction .", "label": "", "metadata": {}, "score": "38.31636"}
{"text": "The numbers of the first row and the first column represent the LPR feature values and the candidates of the weak classifier , respectively .The Adaboost learning with LPR selects the best pixel location having minimum error rate .So , each pixel location can be a candidate of a weak classifier .", "label": "", "metadata": {}, "score": "38.331955"}
{"text": "Recent work , however , suggests that random forest classifiers may outperform support vector machines in this domain .Results .In the present paper we identify methodological biases of prior work comparing random forests and support vector machines and conduct a new rigorous evaluation of the two algorithms that corrects these limitations .", "label": "", "metadata": {}, "score": "38.358128"}
{"text": "set to 1 on both stages .Our algorithm uses three parameters : . and . , corresponding to the regularization parameters in the SGD algorithm , and additional parameter .that defines the length each additional classifier is trained and may be defined in several ways .", "label": "", "metadata": {}, "score": "38.419872"}
{"text": "It is also necessary to point out that all these experiments have been accomplished on the best discriminative genes already selected by FCBF algorithm .It is well known that no algorithm can hold a general advantage in terms of generalization capability over another one across all possible classification tasks .", "label": "", "metadata": {}, "score": "38.533264"}
{"text": "The rest of this paper is organized as follows .In Section 2 , the methods referred to in this study are introduced in detail .Section 3 briefly describes the datasets that were used .Section 4 introduces performance evaluation metrics and experimental settings .", "label": "", "metadata": {}, "score": "38.54135"}
{"text": "Learning of an LPR Based Classifier .Algorithm 1 shows the proposed Adaboost learning procedure of the LPR based classifiers .Prior to starting with the learning process , training samples for learning of classifiers are prepared .Positive samples consist of the patch images of target objects , and negative samples consist of the patch images from the background containing nontarget objects .", "label": "", "metadata": {}, "score": "38.620544"}
{"text": "There are two broad categories for feature selection algorithms , filter model or wrapper [ 28 ] .The filter model relies on general characteristics of the training data to choose best features without involving any learning algorithm .The wrapper models , on the contrary , depends on feature addition or deletion to compose subset features and uses an evaluation function with a predetermined learning algorithm to estimate the subset features .", "label": "", "metadata": {}, "score": "38.73101"}
{"text": "This will also allow the algorithm to adapt better to the case of changing distribution .The other way is to increase the parameter .depending on . , or to choose a different cutoff algorithm altogether .Also , to increase adaptability to changing input conditions , it is possible to change the calculation of the learning rate of the Pegasos algorithm , for example , stopping its decay on a certain threshold .", "label": "", "metadata": {}, "score": "38.748814"}
{"text": "When implementing an LPR based weak classifier , a lookup table is employed in order to produce the classification results by finding the designated predictions corresponding to the LPR feature values .The rest of the paper is organized as follows .", "label": "", "metadata": {}, "score": "38.7864"}
{"text": "As previously mentioned , two different discriminative models hybridized with HMMs are used in this work ( HMM / MLP and HMM / SVM ) .Apart from a generative model ( represented as a standard HMM ) , several well known classifiers are included in the comparison using a sliding window mechanism .", "label": "", "metadata": {}, "score": "38.79058"}
{"text": "It is also important to note that while parameters . and .This is shown in our Section 3 , where our algorithm is running on the same set of parameters for datasets with different variable distributions , and often outperforming even the kernel - using algorithms with ideal kernel settings .", "label": "", "metadata": {}, "score": "38.800877"}
{"text": "Using the above similarity , we separate the boosting process into two phases : the training of each successful weak classifier , and adjusting the boosting weights that combine weak classifiers into a strong one .Both phases can then be cast as a primal SVM problem , same as ( 4 ) , with the difference that in phase 1 ( weak classifier training ) each sample is weighted according to the result provided by the current strong classifier .", "label": "", "metadata": {}, "score": "38.840187"}
{"text": "With the further increase of the feature subspace dimension , the classification performance drops rapidly , which indicates that selecting a feature subspace with 10 to 30 dimensions can maximize the balanced relationship between accuracy and diversity of base classifiers .This result can be easily explained by the following : extracting a too - small subgroup of feature genes can negatively affect the performance of each base classifier , whereas using too many feature genes can negatively affect diversity among base classifiers .", "label": "", "metadata": {}, "score": "38.862076"}
{"text": "FSS generation strategy uses hierarchical clustering , which uses Pearson correlation coefficient ( PCC ) as a similarity measure to delete redundant genes and signal - to - noise ratio ( SNR ) feature selection method [ 6 ] to remove noisy genes .", "label": "", "metadata": {}, "score": "39.00504"}
{"text": "The experimental results help guide the construction of the optimal classification model .Conclusions .In this paper , we attempted to address multiclass imbalanced classification problem in tumor DNA microarray data by using ensemble learning .The empirical results show that our proposed classification algorithms outperform many traditional classification approaches and yield more balanced and robust classification results .", "label": "", "metadata": {}, "score": "39.00729"}
{"text": "In this paper , we attempted to address the multiclass imbalance classification problem of cancer microarray data by using ensemble learning .Ensemble learning has been used to improve the accuracy of feature gene selection [ 26 ] and cancer classification [ 27 - 29 ] .", "label": "", "metadata": {}, "score": "39.194946"}
{"text": "We compare our algorithm to other online training algorithms , and we show , that for most cases with unknown kernel parameters , our algorithm outperforms other algorithms both in runtime and convergence speed .Introduction .Online Training Methods .During the recent years , there has been an increasing amount of interest in the area of online learning methods .", "label": "", "metadata": {}, "score": "39.33053"}
{"text": "Classification can be considered as nonparametric estimation of sets , where the risk is defined by means of a specific distance between sets associated with misclassification error .It is shown that the rates of convergence of classifiers depend on two parameters : the complexity of the class of cand ... \" .", "label": "", "metadata": {}, "score": "39.384254"}
{"text": "We call these algorithms as EnSVM - OAA(THR ) and EnSVM - OAA(RUS ) .Figure 3 shows that if one classification task is binary , counter voting turns into majority voting .Counter voting , rather than majority voting or weighted voting , is used to classify multiclass data because generating feature space on each binary - class is more accurate than directly generating feature space on multiple classes .", "label": "", "metadata": {}, "score": "39.517517"}
{"text": "In our experiments , hybrid models outperform other classical activity recognition methods , showing that the combination of generative and discriminative models can result in a significant increase in recognition performance .This paper is organized as follows .Section 2 gives an overview of the type of data used in this study .", "label": "", "metadata": {}, "score": "39.53961"}
{"text": "View Article .Hall M , Frank E , Holmes G , Pfahringer B , Reutemann P , H I : The WEKA Data Mining Software : An Update .SIGKDD Explorations 2009 . , 11 ( 1 ) : .Ng A , Jordan M : On discriminative vs. generative classifiers : A comparison of logistic regression and naive bayes .", "label": "", "metadata": {}, "score": "39.577335"}
{"text": "View Article .Xing E , Ng A , Jordan M , Russell S : Distance metric learning with application to clustering with side - information .Proc NIPS 2003 .Domeniconi C , Gunopulos D : Adaptive nearest neighbor classification using support vector machines .", "label": "", "metadata": {}, "score": "39.627075"}
{"text": "10.1016/j.patcog.2004.05.012 View Article .Fayyad UM , Irani KB : Multi - Interval Discretization of Continuous - Valued Attributes for Classification Learning .Proceedings of the 13th International Joint Conference on Artificial Intelligence 1993 , 1022 - 1027 .Cover TM , Thomas JA : Elements of Information Theory .", "label": "", "metadata": {}, "score": "39.698357"}
{"text": "This in turn improves classification accuracy with predominant selected features .-correlation .Using the sorted feature list , redundant features are eliminated one by one in a descending order .The remaining feature subset thus contains the predominant features with zero redundant features in terms of .", "label": "", "metadata": {}, "score": "39.73456"}
{"text": "Cross - validation design .We used 10-fold cross - validation to estimate the performance of the classification algorithms .In order to optimize algorithm parameters , we used another \" nested \" loop of cross - validation by further splitting each of the 10 original training sets into smaller training sets and validation sets .", "label": "", "metadata": {}, "score": "39.77379"}
{"text": "However , in both cases the recognition power of the discriminative models increases when they are combined with the ability of HMM to deal with temporal patterns .Conclusions .In this paper we have proposed two new approaches to recognize ADLs from home environments using a network of binary sensors .", "label": "", "metadata": {}, "score": "39.8783"}
{"text": "In this paper we demonstrate that , ignoring computational constraints , it is possible to release synthetic databases that are useful for accurately answering large classes of queries while preserving differential privacy .Specifically , we give a mechanism that privately releases synthetic data usefu ... \" .", "label": "", "metadata": {}, "score": "39.885593"}
{"text": "Rotation Forest is an ensemble classification approach which is built with a set of decision trees .For each tree , the bootstrap samples extracted from the original training set are adopted to construct a new training set .Then the feature set of the new training set is randomly split into some subsets , which are transformed individually .", "label": "", "metadata": {}, "score": "39.964348"}
{"text": "While the algorithm described in [ 3 ] allows truncation of the kernel expansion coefficients , this is only applicable to the SGD algorithms with constant learning rate , and it results in an accuracy penalty .The second difference is the update of vector . , which allows change in the weights different from the exponential decay of [ 3 ] .", "label": "", "metadata": {}, "score": "40.14668"}
{"text": "It can be noticed how both hybrid models significantly outperform the other approaches .The SVM hybrid model is the scheme that offers the best performance for this domain , significantly outperforming all other approaches .Both significance tests ( Student t - test and Wilcoxon signed - ranks test ) reveal how there are significant differences in the performance between hybrid MLP and hybrid SVM approaches .", "label": "", "metadata": {}, "score": "40.183304"}
{"text": "Conclusion .Regarding feature selection , SVMRFE outperformed GLGS in classification .As for the learning classifiers , when classification models derived from the best training were compared , SVMs performed the best with respect to the expected testing accuracy .However , the distance metric learning LMNN outperformed SVMs and other classifiers on evaluating the best testing .", "label": "", "metadata": {}, "score": "40.22467"}
{"text": "Furthermore , the work presented here further demonstrates that accurate ADL recognition can be achieved by a set of simple and cheap state - change sensors installed in a wireless network .In terms of future work , further extensions of the hybrid models are feasible , being possible to employ different classifiers as the discriminative layer of our approach .", "label": "", "metadata": {}, "score": "40.451836"}
{"text": "Both the weak classifiers and selectors were updated each iteration .The limited number of selectors and features allowed for simplified online algorithm .Their algorithm , however , had two potential problems , one being that a limited number of selectors limited flexibility of the classifier , the second being that the effect constant updates have on the error rate of the weak classifiers was never addressed .", "label": "", "metadata": {}, "score": "40.45686"}
{"text": "This experiment shows the ability of the algorithms to completely relearn a distribution .In case of the dataset with the changing distribution , the distribution at the last iteration is used for testing .and the cutoff parameter .Pegasos and Norma used parameter . , and either a linear kernel or a Gaussian RBF kernel with . , which is the same value of .", "label": "", "metadata": {}, "score": "40.45822"}
{"text": "Using full set of genes .The performance results of classification prior to gene selection are shown in Figure 1 and Table 1 .In total , SVMs nominally ( that is , not necessarily statistically significantly ) outperform RFs in 15 datasets , RFs nominally outperform SVMs in 4 datasets , and in 3 datasets algorithms perform the same .", "label": "", "metadata": {}, "score": "40.64174"}
{"text": "RCI is an entropy - based measure that quantifies how much the uncertainty of a decision problem is reduced by a classifier relative to classifying using only the prior probabilities of each class .We note that both AUC and RCI are more discriminative than the accuracy metric ( also known as proportion of correct classifications ) and are not sensitive to unbalanced distributions [ 7 - 10 ] .", "label": "", "metadata": {}, "score": "40.662098"}
{"text": "Transformation Methods .As it was already mentioned , the purpose of rotation - based ensemble classifiers such as Rotation Forest and RotBoost is to increase the individual classifier performance and the diversity within the ensemble .Thus , a full feature set is obtained with all the transformed features for each considered tree in the ensemble .", "label": "", "metadata": {}, "score": "40.68608"}
{"text": "To be more specific , in a typical microarray dataset , there are thousands of gene features .Then if RotBoost ensemble classifier is applied to classify such dataset directly , a rotation matrix with thousands of dimensions is required for each tree , which greatly increases the computational complexity .", "label": "", "metadata": {}, "score": "40.7003"}
{"text": "See subsection \" Statistical comparison among classifiers \" for the description of statistical test employed to compute reported p - values .P - values shown with boldface denote statistically significant differences between classification methods at the 0.05 \u03b1 level .According to the results in Figure 2 and Table 2 , in 17 datasets SVMs nominally outperform RFs , in 3 datasets RFs nominally outperform SVMs , and in 2 datasets algorithms perform the same .", "label": "", "metadata": {}, "score": "40.78524"}
{"text": "Specifically , we give a mechanism that privately releases synthetic data useful for answering a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries .", "label": "", "metadata": {}, "score": "40.84411"}
{"text": "The key feature of this problem is that there is n ... \" .We investigate the following problem : Given a set of documents of a particular topic or class # , and a large set # of mixed documents that contains documents from class # and other types of documents , identify the documents from class # in # .", "label": "", "metadata": {}, "score": "40.897728"}
{"text": "Best testing accuracy and standard errors ( mean \u00b1 standard error , % ) with classification models derived from best training , with the use of GLGS and SVMRFE feature selection algorithms and seven learning classifiers .By using each feature selection algorithm on each data set , the best result as well as the classifier is highlighted in bold .", "label": "", "metadata": {}, "score": "40.89877"}
{"text": "In supervised global distance metric learning , the representative work formulates distance metric learning as a constrained convex programming problem [ 27 ] .In local adaptive distance metric learning , many researchers presented approaches to learn an appropriate distance metric to improve a KNN classifier [ 28 - 32 ] .", "label": "", "metadata": {}, "score": "41.04665"}
{"text": "Van Kasteren , T.L.M. ; Englebienne , G. ; Kr\u00f6se , B.J. An activity monitoring system for elderly care using generative and discriminative models .Pers .Ubiquitous Comput .[ Google Scholar ] .Atallah , L. ; Yang , G.Z. The use of pervasive sensing for behaviour profiling - A survey .", "label": "", "metadata": {}, "score": "41.094494"}
{"text": "As the last step of an iteration , we calculate the cutoff parameter for adding new classifier .If the preset threshold is reached ( in this case , the number of training iterations .reaches ., the value of . is increased , and a new classifier is initialized with zero weights .", "label": "", "metadata": {}, "score": "41.136215"}
{"text": "Keywords : . activity recognition ; hidden Markov model ; hybrid schemes ; wireless sensor networks .Introduction .Population aging is currently having a significant impact on health care systems [ 1 ] .Improvements in medical care are resulting in increased survival into old age , thus cognitive impairments and problems associated with aging will increase [ 2 ] .", "label": "", "metadata": {}, "score": "41.224052"}
{"text": "It may take several years before the precise reasons of differences in empirical error are thoroughly understood , and in the meantime the empirical advantages and disadvantages of methods should be noted first by practitioners .Data analysts should also be aware of a limitation of RFs imposed by its embedded random gene selection .", "label": "", "metadata": {}, "score": "41.296204"}
{"text": "Having captured the spot intensities , the obtained intensities undergo a normalization preprocessing stage to remove systematic errors within the data [ 2 ] .Early application of microarrays to the study of human disease conditions rapidly revealed their potential as a medical diagnostic tool [ 3 , 4 ] .", "label": "", "metadata": {}, "score": "41.32492"}
{"text": "We first segment the data by the class , and then compute the mean and variance of in each class .Let be the mean of the values in associated with class c , and let be the variance of the values in associated with class c .", "label": "", "metadata": {}, "score": "41.37231"}
{"text": "For comparison of the performance of the proposed algorithm against the conventional method , the detector based on the conventional algorithm in [ 17 - 19 , 25 ] was implemented and compared by applying all test conditions of the proposed algorithm stated previously .", "label": "", "metadata": {}, "score": "41.37683"}
{"text": "The benefits arising from using ANNs or SVMs as emission probability estimators are : .They provide discriminant - based learning , suppressing incorrect classification .They do not need to treat features as independent .There is no need of any particular assumptions about the independence of input features and statistical distributions .", "label": "", "metadata": {}, "score": "41.473434"}
{"text": "A detailed description of the steps to select the weak classifiers as listed in Algorithm 1 is given below .First , histograms of LPR from positive samples and negative samples are generated as shown in Algorithm 1 .A histogram is generated by taking an accumulative weight of samples , according to the LPR feature values of the same pixel location .", "label": "", "metadata": {}, "score": "41.484383"}
{"text": "Several approaches to the solution exist , such as interior point [ 6 ] methods and the decomposition methods , such as SMO [ 7 ] .Also , recently , interest in gradient - based methods to solving primal SVM problem has risen drastically .", "label": "", "metadata": {}, "score": "41.600876"}
{"text": "Next , objective function .The loss function is then used as a weight for training the weights of the latest weak classifier .with the Pegasos algorithm .It can be seen that , similar to AdaBoost family of boosting algorithms , our algorithm increases the weights of samples misclassified by the current strong classifier , and decreases the weights of samples classified correctly .", "label": "", "metadata": {}, "score": "41.673744"}
{"text": "Specifying a SVMs classifier requires two parameters , that is , the kernel function and the regularization parameter .Table 4 : Mean classification accuracy of each classification method against 8 different gene datasets .Table 4 summarizes the mean classification accuracy of each classification method on the considered datasets .", "label": "", "metadata": {}, "score": "41.70201"}
{"text": "where is the probability of class generating the term .This event model is especially popular for classifying short texts .It has the benefit of explicitly modelling the absence of terms .Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one .", "label": "", "metadata": {}, "score": "41.857452"}
{"text": "Hence , the amount of change of the updated weight of each sample depends on the confidence level of prediction .In this case , the weight of the correctly classified sample with high confidence is significantly reduced to be ignored in the next round , whereas the weight of the misclassified sample with high confidence is significantly increased to be concentrated in the next round .", "label": "", "metadata": {}, "score": "41.891605"}
{"text": "Assuming that the 1st pixel location is the best pixel location with the smallest error rate , it can be selected as a weak classifier for that round in AdaBoost learning .The selected weak classifier is described as a lookup table which is generated by ( 8 ) , and the real values in the lookup table represent the confidence level of prediction .", "label": "", "metadata": {}, "score": "41.902367"}
{"text": "In Bagging , each base classifier is constructed on a bootstrap sample of the original training data , that is , a random sample of instances drawn with replacement and having the same size as the original training data .Ensemble classification is achieved by means of majority voting , where an unlabeled unseen data is assigned the class with the highest number of votes among the individual classifiers ' predictions [ 21 ] .", "label": "", "metadata": {}, "score": "41.977818"}
{"text": "Feature Subspace Generation Technology .The performance of ensemble learning is related to two factors : accuracy and diversity of base classifiers [ 35 ] .The generalization error of ensemble learning . are averages of generalization errors and diversities , respectively .", "label": "", "metadata": {}, "score": "41.98113"}
{"text": "Notations .:An input sample vector .A single vector is provided for each iteration of the algorithm .Sample vectors are assumed to be extended with an additional constant element representing bias term .:The label provided for .", "label": "", "metadata": {}, "score": "41.984234"}
{"text": "For this purpose , Haar - like features have been widely used for object detection .Haar - like feature represents a comparison of attributes such as intensity or gradient between subregions in a kernel .However , since the method depends on the absolute values of the attributes , it may not be robust against illumination changes or motion blur .", "label": "", "metadata": {}, "score": "41.996185"}
{"text": "[ 11 ] .Despite the fact that the far - reaching independence assumptions are often inaccurate , the naive Bayes classifier has several properties that make it surprisingly useful in practice .In particular , the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one - dimensional distribution .", "label": "", "metadata": {}, "score": "42.018295"}
{"text": "Consider the problem of classifying documents by their content , for example into spam and non - spam e - mails .Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the ( independent ) probability that the i - th word of a given document occurs in a document from class C can be written as .", "label": "", "metadata": {}, "score": "42.026264"}
{"text": "Our method uses Pegasos , Stochastic Gradient Descent-(SGD- ) based SVM training method introduced in [ 4 ] to produce both a set of weak classifiers and boosting weights for combining them into strong classifier .Using this kind of training algorithm allows us to utilize solid theoretical background and well - defined convergence rate of SGD algorithms , as well as increased accuracy of classifier produced by boosting .", "label": "", "metadata": {}, "score": "42.037064"}
{"text": "If the confidence level of prediction is a positive number , the result of classification is positive ; otherwise , it will be negative .A large absolute number implies that the prediction is highly confident .The real numbers obtained in this sense can be considered as \" reinforcement \" in the AdaBoost learning for the following reasons .", "label": "", "metadata": {}, "score": "42.076397"}
{"text": "With the characteristics of multiclass problem taken into consideration and referring to the idea of majority voting , we propose a novel integration rule called counter voting .For each decomposed binary - class branch in OAA , one counter is assigned , which indicates the proportion of test sample .", "label": "", "metadata": {}, "score": "42.087185"}
{"text": "( i ) SVM with various coding strategies exhibits quite similar classification performance in terms of Acc , . -mean evaluation metrics .Compared with its three competitors , SVM - OAA does not show sufficient superiority , although it simplifies transformation by decomposing each multiclass problem to the least binary - class problems .", "label": "", "metadata": {}, "score": "42.163536"}
{"text": "Alternatively , because electronic noise is generated from the electronic instrument and is usually randomly distributed in the spectra , Chen et al [ 3 ] designed a wavelet - based de - noising that applies wavelet transformation and removes a certain amount of lower value wavelet coefficients .", "label": "", "metadata": {}, "score": "42.209118"}
{"text": "To assess the efficiency of the RotBoost , other nonensemble / ensemble techniques including Decision Trees , Support Vector Machines , Rotation Forest , AdaBoost , and Bagging are also deployed .Experimental results have revealed that the combination of the fast correlation - based feature selection method with ICA - based RotBoost ensemble is highly effective for gene classification .", "label": "", "metadata": {}, "score": "42.26751"}
{"text": "In most cases , when dealing with kernels , the task of learning a support vector machine is cast as a constrained quadratic programming problem .Methods that deal with such problem usually need access to all labeled samples at once and require about .", "label": "", "metadata": {}, "score": "42.290108"}
{"text": "Subsequently , the training time can be shortened over the conventional method as it is clearly shown for a field test described in the following section .Field Test .To evaluate the object detection performance based on the proposed learning algorithm , tests were conducted to detect the location of target objects in still images .", "label": "", "metadata": {}, "score": "42.306038"}
{"text": "is the number of clusters .Obviously , redundant genes can emerge in the same clusters .For this , we use the SNR feature selection method [ 6 ] to select differentially expressed genes in each cluster , with the computational formula listed as follows : . are their standard deviations , respectively .", "label": "", "metadata": {}, "score": "42.324997"}
{"text": "where a superscript means the sequence number of the selected weak classifier and a subscript refers to the index of candidates .This method requires the selection of a large number of weak classifiers , . , the proposed method calculates the combination operation of the weak classifiers in advance at the repetitively selected pixels .", "label": "", "metadata": {}, "score": "42.392197"}
{"text": "being the number of added classifiers , and each classifier producing output . , so the convergence rate slowly decreases as additional classifiers are added .To combat this , certain classifiers with lows weights may be removed from the pool .", "label": "", "metadata": {}, "score": "42.42872"}
{"text": "Nevertheless , temporal probabilistic models provide a good framework to handle the uncertainty caused by these issues .Specifically , the hidden Markov model ( HMM ) has been successfully applied in many sequential data modeling problems , and has been shown to perform well in this domain [ 8 ] .", "label": "", "metadata": {}, "score": "42.479073"}
{"text": "We also show that it is not possible to release even simple classes of queries ( such as intervals and their generalizations ) over continuous domains with worst - case utility guarantees while preserving differential privacy .In response to this , we consider a relaxation of the utility guarantee and give a privacy preserving polynomial time algorithm that for any halfspace query will provide an answer that is accurate for some small perturbation of the query .", "label": "", "metadata": {}, "score": "42.55512"}
{"text": "The next step is to use discrete wavelet transform to eliminate the electrical noise .By applying a wavelet transform , the original signal is decomposed into multi - level wavelet coefficients .By setting up a threshold value , given percentiles of lower value coefficients are removed .", "label": "", "metadata": {}, "score": "42.66749"}
{"text": "In order to design a weak classifier for the 1st pixel location , the histograms of both the positive samples and the negative samples are compared by searching for larger accumulated weights bin by bin .The decision of classification for one bin is made depending on the side having a larger accumulated weight .", "label": "", "metadata": {}, "score": "42.709446"}
{"text": "Although SVM is more robust to class imbalance than many other machine learning methods because its classification hyperplane only associates with a few support vectors , it can still be , more or less , affected by skewed class distribution .Previous studies [ 40 , 41 ] have found that the classification hyperplane can be pushed toward the minority class if the classification data is skewed ( see Figure 2(a ) ) .", "label": "", "metadata": {}, "score": "42.81057"}
{"text": "In addition to microarray classification , SVMRFE has been widely used in other high - throughput biological data analysis including a proteomics study [ 16 ] and non - bioinformatics areas involving feature selection and pattern classification situations [ 21 ] .", "label": "", "metadata": {}, "score": "42.825306"}
{"text": "While naive Bayes often fails to produce a good estimate for the correct class probabilities , [ 12 ] this may not be a requirement for many applications .For example , the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class .", "label": "", "metadata": {}, "score": "42.8766"}
{"text": "Unfortunately , most existing work has only considered binary - class imbalance and ignored the multiclass problem , that is , identifying multiple imbalanced tumor types or several skewed subtypes of a special tumor .Applying traditional supervised learning algorithms that solve minimum classification errors will provide inaccurate classification results .", "label": "", "metadata": {}, "score": "42.886482"}
{"text": "The Relief family methods are especially attractive because they may be applied in all situations , have low bias , include interaction among features , and may capture local dependencies that other methods miss [ 35 ] .The CFS method is based on test theory concepts and relies on a set of heuristics to assess the adequacy of subsets of features .", "label": "", "metadata": {}, "score": "42.923443"}
{"text": "Inza I , Sierra B , Blanco R , Larranaga P : Gene selection by sequential search wrapper approaches in microarray cancer class prediction .Journal of Intelligent and Fuzzy Systems 2002 , 12 ( 1 ) : 25 - 33 .", "label": "", "metadata": {}, "score": "43.00022"}
{"text": "2012 , Article ID 320698 , 7 pages , 2012 .View at Publisher \u00b7 View at Google Scholar . A. C. Lorena , A. C. P. L. F. De Carvalho , and J. M. P. Gama , \" A review on the combination of binary classifiers in multiclass problems , \" Artificial Intelligence Review , vol .", "label": "", "metadata": {}, "score": "43.005657"}
{"text": "That study did not compare feature selection and classifiers for MALDI - MS data .\" The curse of dimensionality \" in MS data requires a powerful feature selection algorithm to choose the discriminative feature subset .While distance metric learning has drawn many researchers ' attention , researchers recognize that different classifiers yield different results .", "label": "", "metadata": {}, "score": "43.095818"}
{"text": "Some studies have been reported on the application of microarray gene expression data analysis for molecular cancer classification [ 5 , 6 ] .Usually , microarray classification process comprised of two successive steps , that is , feature selection and classification .", "label": "", "metadata": {}, "score": "43.141838"}
{"text": "Outline of the Paper .The remainder of this article is organized as follows .In Section 2 , we give the description of our algorithm .In Section 3 , we compare it to several existing online training methods in terms of computational cost and flexibility .", "label": "", "metadata": {}, "score": "43.268944"}
{"text": "The drawbacks of our algorithm include the fact that it is not as efficient in case of linearly separable problems , and that it inherits some of the sensibility to the rapid changes in target function from Pegasos .In the future , we plan to study the application of our algorithm to various image and signal processing tasks , in partiular to object tracking problem to compare its effectiveness to methods based on various online AdaBoost modifications , like the ones described in [ 1 ] .", "label": "", "metadata": {}, "score": "43.310158"}
{"text": "Phoungphol et al .[42 ] used ramp loss function to construct a more robust and cost - sensitive support vector machine ( Ramp - MCSVM ) and used it to classify multiclass imbalanced biomedical data .Decision threshold adjustment based on support vector machine ( SVM - THR ) directly pushes classification hyperplane toward the majority class .", "label": "", "metadata": {}, "score": "43.39359"}
{"text": "Here , we utilized 8 publicly available benchmark datasets [ 32 ] .A brief overview of these datasets is summarized in Table 1 .Preprocessing is an important step for handling gene expression data .This includes two steps : filling missing values and normalization .", "label": "", "metadata": {}, "score": "43.55922"}
{"text": "Then , we present the results and discuss the outcome .Datasets .Five fully labeled datasets were employed to validate the proposed approach , generated using five different sensor networks .The activities or labels considered were not the same for every dataset .", "label": "", "metadata": {}, "score": "43.588234"}
{"text": "Normalization is performed so that every gene expression has mean equal to 0 and variance equal to 1 .In summary , the 8 datasets had between 2 and 5 distinct diagnostic categories , 60 - 253 instances , and 2000 - 24481 genes .", "label": "", "metadata": {}, "score": "43.588444"}
{"text": "View Article PubMed .Vapnik VN : Statistical learning theory New York , Wiley 1998 .Fan RE , Chen PH , Lin CJ : Working set selection using second order information for training support vector machines .Journal of Machine Learning Research 2005 , 6 : 1918 .", "label": "", "metadata": {}, "score": "43.621475"}
{"text": "Peng J , Heisterkamp D , Dai H : Adaptive kernel metric nearest neighbor classification .Proc International Conference on Pattern Recognition 2002 .Goldberger J , Roweis S , Hinton G , Salakhutdinov R : Neighbourhood components analysis .Proc NIPS 2005 .", "label": "", "metadata": {}, "score": "43.62177"}
{"text": "t .j .y .t .i . ) a .i .j .Hybrid Generative / Discriminative Modeling .The hybrid HMM approach is a combination of an HMM that models the temporal characteristics of the sequential data and a static classifier that outputs a posterior probability for each label , taking as input the features in the sensor feature space .", "label": "", "metadata": {}, "score": "43.639328"}
{"text": "L. Kuncheva and J. Rodriguez , \" An experimental study on rotation forest ensembles , \" in Multiple Classifier Systems , vol .4472 of Lecture Notes on Computer Science , pp .459 - 468 , 2007 .View at Google Scholar .", "label": "", "metadata": {}, "score": "43.64617"}
{"text": "Computer Vision and Pattern Recognition 2005 , 1 : 539 - 546 .Weinberger K , Blitzer J , Saul L : Distance metric learning for large margin nearest neighbor classification .Proc NIPS 2006 , 1475 - 1482 .Vandenberghe L , Boyd SP : Semidefinite programming .", "label": "", "metadata": {}, "score": "43.700005"}
{"text": "When the number of features becomes very large , the filter model is usually chosen due to its computational efficiency .Here , we utilize fast correlation - based filter ( FCBF ) as previous experiments [ 30 ] suggest that FCBF is an efficient and fast feature selection algorithm for classification of high dimensional data .", "label": "", "metadata": {}, "score": "43.709156"}
{"text": "To overcome the limitation of sample size , we plan to investigate more about discriminative parameter learning and effective feature selection for Bayesian network classifier as future works .Declarations .Acknowledgements .This article has been published as part of Proteome Science Volume 10 Supplement 1 , 2012 : Selected articles from the IEEE International Conference on Bioinformatics and Biomedicine 2011 : Proteome Science .", "label": "", "metadata": {}, "score": "43.725784"}
{"text": "Experimental Results .The graphs for the estimated error rate are shown on Figure 3 , while the resulting error rate on the test datasets is shown in Table 1 .It can be seen that for linearly separable problems our algorithms performs on par with the Pegagos algorithm , with slight increase of the error rate possibly due to the overfitting .", "label": "", "metadata": {}, "score": "43.78588"}
{"text": "i . g .n . can be expressed in the same form as objective function .of SVM : . where . is a vector that consists of outputs provided by a set of .weak classifiers , .That means that an SVM training algorithm can be applied for training weights . , with the same guarantees for convergence rate and generalization error bound shown in [ 5 ] .", "label": "", "metadata": {}, "score": "43.813313"}
{"text": "Similarly to RFE and RFVS , we perform backward elimination by discarding 0.2 proportion of genes at each iteration .Backward elimination procedure based on univariate ranking of genes with Kruskal - Wallis one - way non - parametric ANOVA [ 1 ] ( denoted as \" KW \" ) : This procedure is applied similarly to the S2N method except for it uses different univariate ranking of genes .", "label": "", "metadata": {}, "score": "43.830734"}
{"text": "The neural networks we used in this work are Multi - Layer Perceptrons ( MLP ) trained with the error back - propagation algorithm in order to maximize the relative entropy criterion .The support vector classifiers are widely used in diverse disciplines due to their high accuracy and ability to handle non - linear problems .", "label": "", "metadata": {}, "score": "43.851013"}
{"text": "At each stage , those negative training sets that were classified as \" background \" were discarded in the following stage and replaced by the other background images that have not been used in previous stages .After preparing the training sets , the MCT features were extracted from all the training sets and the detectors were trained according to the proposed learning algorithm .", "label": "", "metadata": {}, "score": "43.871353"}
{"text": "Next , we designed an improved random subspace generation approach called feature subspace ( FSS ) to produce a large number of accurate and diverse training subsets .We then introduced one of two correction technologies , namely , either decision threshold adjustment ( THR ) [", "label": "", "metadata": {}, "score": "43.895187"}
{"text": "In short , Segal showed that there exist some data distributions where maximal unpruned trees used in the random forests do not achieve as good performance as the trees with smaller number of splits and/or smaller node size .Thus , application of random forests in general requires careful tuning of the relevant classifier parameters .", "label": "", "metadata": {}, "score": "43.898384"}
{"text": "That is , .[ 4 ] .With a multinomial event model , samples ( feature vectors ) represent the frequencies with which certain events have been generated by a multinomial where is the probability that event i occurs ( or K such multinomials in the multiclass case ) .", "label": "", "metadata": {}, "score": "43.91242"}
{"text": "Three of these datasets have been broadly employed in previous studies [ 32 , 33 ] and are publicly available for download from Reference [ 34 ] .The datasets were obtained using similar sensor systems in different houses .The layout of the different home settings differs strongly , as well as the sensors configuration .", "label": "", "metadata": {}, "score": "43.91243"}
{"text": "Here , we only present algorithm with our modifications for weighting sample , for detailed analysis please refer to the original article [ 4 ] .On each iteration , the Pegasos algorithm is given iteration number .( regulating how \" soft \" the resulting margin is , i.e. , whether the priority is given to low error rate over training set or larger margin between classes ) , sample vector . , used in the original Pegasos algorithm , is taken to be 1 , reducing Pegasos to an SGD algorithm with an additional projection step .", "label": "", "metadata": {}, "score": "43.98723"}
{"text": "We describe and analyze a simple and effective two - step online boosting algorithm that allows us to utilize highly effective gradient descent - based methods developed for online SVM training without the need to fine - tune the kernel parameters , and we show its efficiency by several experiments .", "label": "", "metadata": {}, "score": "44.082592"}
{"text": "Figure 3 shows the box - plots of 50 best testing accuracy values for each learning classifier with the feature selection methods of GLGS and SVMRFE , respectively .Table 2 lists the mean value and the standard error of the best testing accuracy with the classification models derived from the best training in each experiment .", "label": "", "metadata": {}, "score": "44.13011"}
{"text": "The other parameters follow the initial settings in Section 4 .The average results of 10 random runs for EnSVM - OAA(THR ) and EnSVM - OAA(RUS ) are reported in Figures 4 and 5 , respectively .Figure 5 : Performance comparison for EnSVM - OAA(RUS ) algorithm based on different sizes of feature subspace on the eight imbalanced multiclass cancer microarray datasets .", "label": "", "metadata": {}, "score": "44.174557"}
{"text": "We show that our theoretical convergence bounds are similar to those of earlier algorithms , while allowing for greater flexibility .Our approach may also easily incorporate additional nonlinearity in form of Mercer kernels , although our experiments show that this is not necessary for most situations .", "label": "", "metadata": {}, "score": "44.214172"}
{"text": ": Activities of daily living are good indicators of elderly health status , and activity recognition in smart environments is a well - known problem that has been previously addressed by several studies .The output scores of the discriminative models , after processing , are used as observation probabilities of the hybrid approach .", "label": "", "metadata": {}, "score": "44.31439"}
{"text": "The circle points denote positive samples and the asterisk points represent negative examples , respectively .Class imbalance correction technologies of SVM can be roughly divided into three categories : sampling [ 30 , 40 ] , weighting [ 41 , 42 ] , and decision threshold adjustment [ 17 ] , that is , threshold moving .", "label": "", "metadata": {}, "score": "44.31846"}
{"text": "Menke J , Martinez TR : Using permutations instead of student 's t distribution for p - values in paired - difference algorithm comparisons .Proceedings of 2004 IEEE International Joint Conference on Neural Networks 2004 , 2 : 1331 - 1335 .", "label": "", "metadata": {}, "score": "44.351345"}
{"text": "309 - 318 , 2013 .View at Google Scholar .W. J. Lin and J. J. Chen , \" Class - imbalanced classifiers for high - dimensional data , \" Briefings in Bioinformatics , vol .14 , no . 1 , pp .", "label": "", "metadata": {}, "score": "44.426544"}
{"text": "Proc International Joint Conference on Artificial Intelligence 2003 .Zhang K , Tang M , Kwok JT : Applying neighborhood consistency for fast clustering and kernel density estimation .Proc Computer Vision and Pattern Recognition 2005 , 1001 - 1007 .Chopra S , Hadsell R , LeCun Y : Learning a similarity metric discriminatively , with application to face verification .", "label": "", "metadata": {}, "score": "44.445366"}
{"text": "As alluded in Section 4.2 , the proposed learning algorithm requires smaller number of rounds than the conventional learning algorithm for the training error to become zero .Also , in some cases of license plate detection , a number of pixel locations smaller than the maximum possible were selected at stages 4 and 5 , resulting in the detection of speed improvement .", "label": "", "metadata": {}, "score": "44.45654"}
{"text": "On the other hand , classifier ensembles have received increasing attention in various applications .Here , we address the gene classification issue using RotBoost ensemble methodology .This method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "44.531944"}
{"text": "Introduction .In proteome research , high - throughput mass spectrometry ( MS ) establishes an effective framework for biomedical diagnosis and protein identification [ 1 ] .A mass spectrum data sample includes a sequence of mass / charge ( m / z ) ratios .", "label": "", "metadata": {}, "score": "44.53613"}
{"text": "For the assessment to be reliable , the experiments for computing execution times were repeated ten times and the total execution time and their averages were calculated .Each column of the table represents the number of combined pixels used for the strong classifier and all the tested classifiers were trained through the same 1000 rounds .", "label": "", "metadata": {}, "score": "44.549343"}
{"text": "A standard HMM is employed to capture the temporal dynamics , but instead of directly using the sensor features to define an observation distribution , we trained the HMM employing the posterior probabilities obtained by the discriminative model selected ( either an ANN or an SVM ) .", "label": "", "metadata": {}, "score": "44.563698"}
{"text": "In many practical applications , parameter estimation for naive Bayes models uses the method of maximum likelihood ; in other words , one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods .Despite their naive design and apparently oversimplified assumptions , naive Bayes classifiers have worked quite well in many complex real - world situations .", "label": "", "metadata": {}, "score": "44.60987"}
{"text": "The authors concluded that GLGS outperforms SVMRFE in microarray data analysis [ 17 ] , a finding that our previous work corroborates in that we found that GLGS also effectively classified microarray data [ 18 ] .To reach a more definitive understanding of how methods compare , we evaluated two methods of feature selection as well as popular learning classifiers in an experimental study on MALDI - MS data .", "label": "", "metadata": {}, "score": "44.665844"}
{"text": "Here , for the sake of comparison , we experiment with both PCA and ICA transformation methods and will report on their efficiency for our gene microarray classification task later on .Gene Selection .Available training data sets for classification of cancer types generally have a fairly small sample size compared to the number of genes involved .", "label": "", "metadata": {}, "score": "44.703453"}
{"text": "A major problem in machine learning is that of inductive bias : how to choose a learner 's hypothesis space so that it is large enough to contain a solution to the problem being learnt , yet small enough to ensure reliable generalization from reasonably - sized training sets .", "label": "", "metadata": {}, "score": "44.84495"}
{"text": "As it can be seen , the number of selected genes for each processed gene dataset is different and depends on the choice of a feature selection algorithm .It should be noted that both mRMR and GSNR algorithms provide an ordered list of the initial genes ( features ) according to the genes importance and discrimination power .", "label": "", "metadata": {}, "score": "44.90699"}
{"text": "Among the different schemes evaluated , the SVM / HMM hybrid approach obtains a significant and notable better performance .We consider that SVM based approaches have great potential and further uses in this human activity recognition problem .However , it must be noticed that hybridizing these schemes implies a more complex system ; hence , when integrating into a real home monitoring solution , it should be considered whether performance should take priority over efficiency .", "label": "", "metadata": {}, "score": "44.957436"}
{"text": "proposed a distance metric learning for Large Margin Nearest Neighbor classification ( LMNN ) .Specifically , the Mahanalobis distance is optimized with the goal that the k - nearest neighbors always belong to the same class while examples from different classes are separated by a large margin [ 34 ] .", "label": "", "metadata": {}, "score": "44.988403"}
{"text": "Gene selection methods .Even though both SVM and RF classifiers are fairly insensitive to very large number of irrelevant genes , we applied the following widely used gene selection methods in order to further improve classification performance : .Random forest - based backward elimination procedure RFVS [ 5 ] : The RFVS procedure involves iteratively fitting RFs ( on the training data ) , and at each iteration building a random forest after discarding genes with the smallest importance values .", "label": "", "metadata": {}, "score": "45.17313"}
{"text": ".. lexity of function classes using the VC - dimension ( Vapnik & Chervonenkis , 1971 ) of the function class .For a class of function and a finite set , let be the restriction of to ( that is , the set of all possible -valued functions on the domain that can be obtained from the class ) .", "label": "", "metadata": {}, "score": "45.224106"}
{"text": "It increases instances of minority class [ 40 ] or decreases examples of majority class [ 30 ] to mediate the skewed scaling relation .The former is called oversampling and the latter is called undersampling .Weighting [ 41 ] , which is also known as cost - sensitive learning , assigns different penalty factors for the samples of positive and negative classes .", "label": "", "metadata": {}, "score": "45.32513"}
{"text": "[5 ] Still , a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches , such as boosted trees or random forests .[ 6 ] .An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification .", "label": "", "metadata": {}, "score": "45.370853"}
{"text": "Method .Bayesian networks .In this paper , we assume that all variables are discrete .The first component G is a network structure where each node represents a variable in X .For each variable X i , a set of parent variables is denoted by .", "label": "", "metadata": {}, "score": "45.47837"}
{"text": "A new classifier is added if .: Objective function of a . 's weak classifier .: Objective function of a strong classifier .: A loss function for the combined strong classifier .In this work we use hinge - loss function ( see ( 1 ) ) .", "label": "", "metadata": {}, "score": "45.552574"}
{"text": "Therefore , a classifier with these types of features can be implemented simply by selecting a threshold boundary in terms of metric distance .However , the LPR has discrete integer - valued attributes as feature values .Each integer value of features represents its own independent pattern and the feature value does not have metric distance characteristic [ 26 ] .", "label": "", "metadata": {}, "score": "45.58006"}
{"text": "Therefore , the proposed update method can be called \" reinforcement learning \" that can guide the classifier toward the fastest route toward convergence . is an indicator function that takes 1 if the argument is true and 0 otherwise .Finally , it is possible to construct a final strong classifier combining pixel classifiers .", "label": "", "metadata": {}, "score": "45.595345"}
{"text": "Experiments carried out over the \" OrdonezA \" dataset show a clearly better F - measure performance for the hybrid schemes .In this case , the increase in performance for such hybrid approaches is statistically significant in all cases .On the other hand , although the HMM / SVM model outperforms the HMM hybridized with an MLP , the differences are not significant .", "label": "", "metadata": {}, "score": "45.7526"}
{"text": "Among the available labelled data , training and test subsets are chosen using the cross - validation mechanism .Assign an initial nonzero value to transition probabilities of the HMM .The training data are employed to train the corresponding classifier ( either the MLP or the SVM ) .", "label": "", "metadata": {}, "score": "45.79698"}
{"text": "Average testing under each dimension .Figure 1 shows the average testing accuracy by using the seven classifiers for the feature sets chosen by GLGS and SVMRFE , with the feature numbers from 5 to 100 .Regarding feature selection , SVMRFE is superior to GLGS in the testing of each type of MS data .", "label": "", "metadata": {}, "score": "45.880634"}
{"text": "Generative models provide an explicit representation of dependencies by specifying the factorization of the joint probability of the hidden and observable variables .The HMM is defined by two dependence assumptions , represented by the directed arrows in Figure 2 .In our domain each binary sensor observation is modeled as an independent Bernoulli distribution , giving : . p .", "label": "", "metadata": {}, "score": "45.884132"}
{"text": "377 - 388 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. G. Dietterich and G. Bariki , \" Solving multiclass learning problems via error - correcting output codes , \" Journal of Artificial Intelligence Research , vol .", "label": "", "metadata": {}, "score": "45.92411"}
{"text": "Considering that the 8 gene microarray datasets include different characteristics in terms of number of samples , genes , classes and the type of the cancer to which these data is related to ; overall , ICA - based RotBoost classifier seems to be more effective than PCA - based RotBoost .", "label": "", "metadata": {}, "score": "45.955406"}
{"text": "To achieve this , it is assumed that drug sensitivity can be predicted by using quantitative patterns of protein expression which represents molecular characteristics of individual patients [ 1 , 2 ] .More precisely , as medicinal effect is closely relevant to cancer signaling transduction pathways , proteomic profiling can provide important pathophysiologic cues regarding responses to chemotherapies [ 3 , 4 ] .", "label": "", "metadata": {}, "score": "46.01654"}
{"text": "Table 3 : Classification results obtained by RotBoost ensemble learning against typical 8 gene datasets in terms of PCA / ICA transformation methods .As it can be noted from Table 3 , in 5 cases out of 8 , ICA - based RotBoost learners could outperform their PCA - based counterparts in terms of higher classifications accuracies and lower standard deviations .", "label": "", "metadata": {}, "score": "46.04786"}
{"text": "Boosting algorithms that employ linear combination of weak classifiers to form confidence function for strong classifier were shown to be closely related to the primal formulation for support vector machines [ 10 ] .As in case of SVM , many boosting methods were designed for offline setting , where all of the training examples are given a priori , and share the same set of problems when dealing with larger datasets .", "label": "", "metadata": {}, "score": "46.199238"}
{"text": "Table 3 .Number of genes selected for each microarray dataset and gene selection method .Average number of genes selected over 10 cross - validation training sets .Discussion .The results presented in this paper illustrate that SVMs offer classification performance advantages compared to RFs in diagnostic and prognostic classification tasks based on microarray gene expression data .", "label": "", "metadata": {}, "score": "46.21833"}
{"text": "x .x .t .x .t .x .T .The Hybrid HMM Approach .In our ADL recognition problem , the goal is to identify which activities took place given a sequence of sensor data .Therefore , we want to find the likeliest sequence of activities y 1 : T that best explains the sequence of observations x 1 : T .", "label": "", "metadata": {}, "score": "46.226448"}
{"text": "A single group was retained as the validation set for testing the trained classifier , and the remaining four groups were used as the training set .Thus , the cross validation process can be repeated five times with each validation set .", "label": "", "metadata": {}, "score": "46.235542"}
{"text": "K. Kira and L. Rendell , \" A practical approach to feature selection , \" in Proceedings of the 9th International Workshop on Machine Learning , pp .249 - 256 , 1992 .C. Ding and H. Peng , \" Minimum redundancy feature selection from microarray gene expression data , \" in Proceedings of the 2nd IEEE Computational Systems Bioinformatics , pp .", "label": "", "metadata": {}, "score": "46.236786"}
{"text": "In comparison with the SVMRFE method , the GLGS feature selection method delivered a comparable and/or better performance in classifying microarray data ; however , our experimental results showed that it does not perform as well as SVMRFE in classifying MALDI - MS data .", "label": "", "metadata": {}, "score": "46.246643"}
{"text": "The quest for high performance classifiers with microarray gene expression and other \" omics \" data is ongoing .Random forests have appealing theoretical and practical characteristics , however our experiments show that currently they do not exhibit \" best of class \" performance .", "label": "", "metadata": {}, "score": "46.294098"}
{"text": "References .T. Puelma , R. A. Gutierrez , and A. Soto , \" Discriminative local subspaces in gene expression data for effective gene function prediction , \" Bioinformatics , vol .28 , no .17 , pp .2256 - 2264 , 2012 .", "label": "", "metadata": {}, "score": "46.31241"}
{"text": "We perform several experiments , aiming to compare generalization error and convergence rates over different datasets , as well as the ability of the algorithm to adapt to the distribution with the changing parameters ( flexibility ) .We use several artificial datasets with known distributions and separation properties , and a Forest Covertype dataset ( separating class 5 from other classes ) , originally used in [ 12 ] , and also used for comparison of convergence speed in [ 4 ] .", "label": "", "metadata": {}, "score": "46.493416"}
{"text": "Table 1 .Expected testing accuracy and standard errors ( mean \u00b1 standard error , % ) with classification models derived from best training , with the use of GLGS and SVMRFE feature selection algorithms and seven learning classifiers .Following the use of each feature selection algorithm on each data set , the best result as well as the classifier is highlighted in bold .", "label": "", "metadata": {}, "score": "46.49828"}
{"text": "s .i . g .n . :Boosting coefficients , used for combining several weak classifiers into a stronger one .With the bias term , the number of elements in .:A combined strong classifier .Using this notation , the algorithm is initialized with the following data : .", "label": "", "metadata": {}, "score": "46.50965"}
{"text": "The decoding rule of OAO is majority voting ; that is , the test instance is designated to the class with the most votes .ECOC proposed by Dietterich and Bariki [ 31 ] uses error correcting codes to denote .classes of a multiclass problem .", "label": "", "metadata": {}, "score": "46.572716"}
{"text": "[ 13 ] .This prior probability distribution might be based on our knowledge of frequencies in the larger population , or on frequency in the training set .However , given the sample the evidence is a constant and thus scales both posteriors equally .", "label": "", "metadata": {}, "score": "46.59035"}
{"text": "It is noted that when the number of combined pixels is too low to construct the strong classifier , such as for 20 pixels , the testing error rate decreases up to round 40 but increases beyond round 40 .It is evident for the case of small number of pixels that overfitting occurs in larger number of rounds .", "label": "", "metadata": {}, "score": "46.639526"}
{"text": "Table 1 lists the mean value and the standard error of the expected testing accuracy with the classification models derived from the best training .By comparing the box - plots on the left sub - figures and on the right sub - figures in Figure 1 and comparing the results shown in Table 1 , we concluded that the SVMRFE outperformed GLGS and SVM classifiers showed remarkable advantages over other classifiers .", "label": "", "metadata": {}, "score": "46.64003"}
{"text": "1205 - 1224 , 2004 .View at Google Scholar .T. Li , C. Zhang , and M. Ogihara , \" A comparative study of feature selection and multiclass classfication methods for tissue classification based on gene expression , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "46.732758"}
{"text": "Duong , T. ; Phung , D. ; Bui , H. ; Venkatesh , S. Efficient duration and hierarchical modeling for human activity recognition .Artif .Intell .[ Google Scholar ] .Patterson , D.J. ; Fox , D. ; Kautz , H. ; Philipose , M. Fine - Grained Activity Recognition by Aggregating Abstract Object Usage .", "label": "", "metadata": {}, "score": "46.754402"}
{"text": "In the second step , the best weak classifier is selected for each round .In the third step , a pixel classifier of a single pixel location is generated by constructing a weak classifier at pixel location x selected in the second step .", "label": "", "metadata": {}, "score": "46.77544"}
{"text": "In this work , we addressed RotBoost ensemble classification method to cope with gene microarray classification problems .This ensemble classifier method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "46.782234"}
{"text": "The left half of the figure shows the results of the detector based on the conventional method , and the other half shows the results of the detector based on the proposed method .The sensitivity parameters in ( 15 ) for both detectors were set to zero .", "label": "", "metadata": {}, "score": "46.87153"}
{"text": "Sparsity of the solution is achieved by a sequential sparsification process that admits into the kernel representation a new input sample only if its feature space image can not be suffciently well approximated by combining the images of previously admitted samples .", "label": "", "metadata": {}, "score": "46.952435"}
{"text": "The main objective of this paper is to compare the methods of feature selection and different learning classifiers when applied to MALDI - MS data and to provide a subsequent reference for the analysis of MS proteomics data .Results .We compared a well - known method of feature selection , Support Vector Machine Recursive Feature Elimination ( SVMRFE ) , and a recently developed method , Gradient based Leave - one - out Gene Selection ( GLGS ) that effectively performs microarray data analysis .", "label": "", "metadata": {}, "score": "46.965725"}
{"text": "Saul LK , Roweis ST : Think globally , fit locally : Unsupervised learning of low dimensional manifolds .Journal of Machine Learning Research 2003 , 4 : 119 - 155 .View Article .Belkin M , Niyogi P : Laplacian eigenmaps for dimensionality reduction and data representation .", "label": "", "metadata": {}, "score": "46.972305"}
{"text": "In the last few years substantial interest has developed within the bioinformatics community in the random forest algorithm [ 2 ] for classification of microarray and other high - dimensional molecular data [ 3 - 5 ] .Recent work [ 5 ] reported an empirical evaluation of random forests in the cancer microarray gene expression domain and concluded that random forest classifiers have predictive performance comparable to that of the best performing alternatives ( including SVMs ) for classification of microarray gene expression data .", "label": "", "metadata": {}, "score": "47.091576"}
{"text": "( ii )Some datasets are sensitive to class imbalance but others are not , as shown by the difference between Acc and .-mean value means that the corresponding classifier is significantly affected by imbalanced class distribution , which was observed in several datasets used in the study , including Brain_Tumor1 , 11_Tumors , and 14_Tumors .", "label": "", "metadata": {}, "score": "47.16774"}
{"text": "AdaBoost [ 8 ] is an algorithm that iteratively adds weighted weak classifiers .Each additional classifier is selected from the pool of available classifiers to minimize the weighted error rate over training samples .This error rate was also used to calculate the weight of .", "label": "", "metadata": {}, "score": "47.181274"}
{"text": "Let ( Xi , Yi ) . by Jonathan Baxter - Journal of Artificial Intelligence Research , 2000 . \" ...A major problem in machine learning is that of inductive bias : how to choose a learner 's hypothesis space so that it is large enough to contain a solution to the problem being learnt , yet small enough to ensure reliable generalization from reasonably - sized training sets .", "label": "", "metadata": {}, "score": "47.255142"}
{"text": "Later , a loss term was added to account for noisy data .The most widely used loss function for SVM training is hinge - loss : . m . a .x . where . is called a margin parameter , . is a label of a sample , and .", "label": "", "metadata": {}, "score": "47.324493"}
{"text": "Section 4 describes the experimental setting and experimental results obtained .Finally , Section 5 presents our conclusions and future work .Binary Sensor Data .In this paper we have employed datasets generated by a set of simple state - change sensors installed in five different environments .", "label": "", "metadata": {}, "score": "47.51513"}
{"text": "39 - 48 , 2010 .View at Google Scholar \u00b7 View at Scopus . A. Statnikov , C. F. Aliferis , I. Tsamardinos , D. Hardin , and S. Levy , \" A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "47.527557"}
{"text": "Algorithm Description .Overview of SVM and Pegasos Algorithm .Support vector machines are a useful classification tool , developed by Cortes and Vapnik [ 11 ] , that attempt to construct a hyperplane separating the data that has the largest distance to any point in any class ( see Figure 1 ) .", "label": "", "metadata": {}, "score": "47.568558"}
{"text": "Dietterich TG : Ensemble methods in machine learning .Proceedings of the First International Workshop on Multiple Classifier Systems New York , NY , Springer - Verlag 2000 , 1 - 15 .View Article .Segal MR : Machine Learning Benchmarks and Random Forest Regression .", "label": "", "metadata": {}, "score": "47.59095"}
{"text": "A particularly popular approach is to combine HMMs with ANNs .Rynkiewicz applied a hybrid HMM / ANN scheme to predict time series data , obtaining a model that gave a much better segmentation of the series [ 21 ] .Models based on a hybrid ANN framework have been also widely used on various recognition tasks , namely : speech recognition [ 22 , 23 ] , handwritten text recognition [ 24 ] , sentence recognition [ 25 ] and digit recognition [ 26 ] .", "label": "", "metadata": {}, "score": "47.61001"}
{"text": "Both matrices y ij and \u03b7 ij are fixed during training .The transform is used to compute squared distance as .The first term penalizes large distances between each input and its target neighbors and the second term penalizes small distances between each input and all other inputs that do not share the same label .", "label": "", "metadata": {}, "score": "47.61148"}
{"text": "The local approach is to learn the distance metric in a local setting , i.e. , only to meet local pairwise constraints .Unsupervised distance metric learning is also called manifold learning .Its main idea is to learn an underlying low - dimensional manifold whereby the geometric relationships between most of the observed data are preserved .", "label": "", "metadata": {}, "score": "47.628906"}
{"text": "This paper proposes two new approaches to recognize ADLs from binary sensor streams based on hybrid HMM schemes ( combined with either ANN or SVM ) .We evaluate and compare the activity recognition performance of these models on multiple fully annotated real world datasets : three well known datasets generated by Kasteren et al . , and two new datasets ( \" OrdonezA \" and \" OrdonezB \" ) that we introduce in this paper .", "label": "", "metadata": {}, "score": "47.643875"}
{"text": "View at Google Scholar .R. Blagus and L. Lusa , \" Evaluation of SMOTE for high - dimensional class - imbalanced microarray data , \" in Proceedings of the 11th International Conference on Machine Learning and Applications , pp .89 - 94 , Boca Raton , Fla , USA , 2012 .", "label": "", "metadata": {}, "score": "47.663124"}
{"text": "We also obtain rates of convergence in L\u00e9vy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates . ...Proof . ...he sample may be found , which , however , leads to very poor generalization . \" ...", "label": "", "metadata": {}, "score": "47.663906"}
{"text": "The number of current iteration .: Regularization parameter from Pegasos algorithm used for training weak classifiers .: Regularization parameter used for training strong classifier .:The number of weak classifiers in the current pool .This number is incremented each time a new classifier is added .", "label": "", "metadata": {}, "score": "47.727318"}
{"text": "The k - Nearest Neighbor ( k - NN ) algorithm [ 47 ] generates the instance - based classifier .Tables 4 , 5 , 6 , 7 and 8 show the average F - Measure values for the five different datasets evaluated ( \" KasterenA \" , \" KasterenB \" , \" KasterenC \" , \" OrdonezA \" and \" OrdonezB \" respectively ) .", "label": "", "metadata": {}, "score": "47.777138"}
{"text": "Given a collection of labeled samples L and unlabeled samples U , start by training a naive Bayes classifier on L .Until convergence , do : .Predict class probabilities for all examples x in .Re- train the model based on the probabilities ( not the labels ) predicted in the previous step .", "label": "", "metadata": {}, "score": "47.89224"}
{"text": "[ 15 ] , which refines the optimum feature set by using the Support Vector Machine ( SVM ) .The idea of SVMRFE is that the orientation of the separating hyper - plane found by the SVM can be used to select informative features .", "label": "", "metadata": {}, "score": "47.945892"}
{"text": "View at Google Scholar .G.-Z. Li , H.-H. Meng , and J. Ni , \" Embedded gene selection for imbalanced microarray data analysis , \" in Proceedings of the 3rd International Multi - Symposiums on Computer and Computational Sciences ( IMSCCS ' 08 ) , pp .", "label": "", "metadata": {}, "score": "47.975887"}
{"text": "t . , can be transformed into emission probabilities required by HMMs , .p .x .t .y .t . ) , by applying Bayes rule : . p .x .t .y .t . )", "label": "", "metadata": {}, "score": "48.11461"}
{"text": "On the contrary , when the number of pixels increases , the processing speed becomes slower and the error rate decreases .Therefore , in order to configure a cascade classifier as shown in [ 17 - 19 ] , the strong classifier consisting of a small number of pixels was placed at earlier stage and a large number of pixels at later stage .", "label": "", "metadata": {}, "score": "48.118053"}
{"text": "# document , which makes traditional machine learning techniques inapplicable , as they all need labeled documents of both classes .We call this problem partially supervised classification .In this paper , we show that this problem can be posed as a constrained optimization problem and that under appropriate conditions , solutions to the constrained optimization problem will give good solutions to the partially supervised classification problem .", "label": "", "metadata": {}, "score": "48.180218"}
{"text": "In the methods section , the basic concept of Bayesian network and Bayesian network classifier are reviewed , and we give a detailed account of the proposed ANBC .In the results section , we present the experimental result comparing to other classification algorithms .", "label": "", "metadata": {}, "score": "48.18351"}
{"text": "Expected testing performance under best training .Besides comparing the average testing accuracy under each feature dimension from 5 to 100 , we also compared the testing accuracy with the use of the classification models that are based on the best training .", "label": "", "metadata": {}, "score": "48.198853"}
{"text": "Then the probability that a given document D contains all of the words , given a class C , is .( This technique of \" log - likelihood ratios \" is a common technique in statistics .In the case of two mutually exclusive alternatives ( such as this example ) , the conversion of a log - likelihood ratio to a probability takes the form of a sigmoid curve : see logit for details . )", "label": "", "metadata": {}, "score": "48.295918"}
{"text": "To deal with the feature selection in microarray data classification , Tang et al . also proposed two gene selection methods : leave - one - out calculation sequential forward selection ( LOOCSFS ) and GLGS that is based on the calculation of the leave - one - out cross - validation error of LS - SVM [ 17 ] .", "label": "", "metadata": {}, "score": "48.340984"}
{"text": "^ Narasimha Murty , M. ; Susheela Devi , V. ( 2011 ) .Pattern Recognition : An Algorithmic Approach .ISBN 0857294946 .^ John , George H. ; Langley , Pat ( 1995 ) .Estimating Continuous Distributions in Bayesian Classifiers .", "label": "", "metadata": {}, "score": "48.34874"}
{"text": "This is the event model typically used for document classification , with events representing the occurrence of a word in a single document ( see bag of words assumption ) .The likelihood of observing a histogram x is given by .", "label": "", "metadata": {}, "score": "48.406704"}
{"text": "Proceedings of the Conference on Autonomous Agents and Multiagent Systems ( AAMAS 2007 ) , Honolulu , HI , USA , 14 - 18 May 2007 ; pp .235:1 - 235:8 .Ordo\u00f1ez , F.J. ; Iglesias , J.A. ; de Toledo , P. ; Ledezma , A. ; Sanchis , A. Online activity recognition using evolving classifiers .", "label": "", "metadata": {}, "score": "48.43932"}
{"text": "MLP can be trained to approximate the posterior probabilities of states when each unit of the output layer is associated with a specific state of the model [ 37 ] .p .y .x . exp .s .y . )", "label": "", "metadata": {}, "score": "48.458534"}
{"text": "Same as ( 1 ) , but 10 % of the labels are switched , simulating salt - and - pepper noise .( 3 ) Bayes - separable data ( Bayesian ) .This dataset is generated as described in [ 3 ] , that is , in such a way so that data is clearly separable using ideal Bayesian classifier for known class distributions .", "label": "", "metadata": {}, "score": "48.460785"}
{"text": "More precisely , the augmented edges of TAN are restricted to tree structure but the augmented edges of ANBC are not necessary to be tree structure ( i.e. Some node may not have an augmented edge in ANBC ) .Once the structure is constructed and the parameters are estimated with training data , we can classify an instance into a class label that maximizes the posterior given by .", "label": "", "metadata": {}, "score": "48.48406"}
{"text": "263 - 286 , 1995 .View at Google Scholar .B. Kijsirikul and N. Ussivakul , \" Multiclass support vector machines using adaptive directed acyclic graph , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 02 ) , pp .", "label": "", "metadata": {}, "score": "48.48802"}
{"text": "11 , pp .1341 - 1347 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "48.50567"}
{"text": "Results and Discussions .The experimental results of 12 classification algorithms on 8 datasets are reported in Tables 3 , 4 and 5 , where the best result in each dataset is highlighted in bold , the second best is underlined , and the worst is italicized .", "label": "", "metadata": {}, "score": "48.537945"}
{"text": "Declarations .Acknowledgements .The authors wish to thank ICASA ( Institute for Complex Additive Systems Analysis , a division of New Mexico Tech ) for the support of this study .This work was also supported by the Mississippi Functional Genomics Network ( DHHS / NIH / NCRR Grant # 2P20RR016476 - 04 ) .", "label": "", "metadata": {}, "score": "48.609123"}
{"text": "17 , pp .4963 - 4967 , 2002 .View at Google Scholar \u00b7 View at Scopus .X. Wang , M. J. Hessner , Y. Wu , N. Pati , and S. Ghosh , \" Quantitatative quality control in microarray experiments and the application in data filtering , normalization and false positive rate prediction , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "48.655617"}
{"text": "M. P. S. Brown , W. N. Grundy , D. Lin et al . , \" Knowledge - based analysis of microarray gene expression data by using support vector machines , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "48.682983"}
{"text": "A parameter is defined as .P .B .X .i .x .i . k .X .i .i .j . )i .j .k . where .i .j .i .", "label": "", "metadata": {}, "score": "48.691902"}
{"text": "This in turn leads to less computational cost in experiments .From Table 2 , it is obvious that FCBF achieves the highest level of dimensionality reduction by selecting the least number of discriminative genes .This is consistent with the theoretical analysis about FCBF 's ability to identify and ignore redundant features .", "label": "", "metadata": {}, "score": "48.726833"}
{"text": "n .In our work , we also incorporate a bias term . by substituting .with .In this case , .To simplify notation , we assume that all input vectors have been extended in such a way , and simply use . . . .", "label": "", "metadata": {}, "score": "48.770912"}
{"text": "39 - 50 , September 2004 .View at Scopus .P. Phoungphol , Y. Zhang , and Y. Zhao , \" Robust multiclass classification for learning from imbalanced biomedical data , \" Tsinghua Science and Technology , vol .17 , no .", "label": "", "metadata": {}, "score": "48.81543"}
{"text": "In the testing of the breast cancer data set , KNNC performs the best , followed by SVM classifiers with linear kernel and rbf kernel .In the testing of the liver disease data set , SVM classifiers outperformed other classifiers .", "label": "", "metadata": {}, "score": "48.853638"}
{"text": "We also perform a Wilcoxon significance test because Student t - test has shown a high probability of Type I errors when applied to repetitive random sampling or cross / leave - one - out validation [ 43 ] .Results .", "label": "", "metadata": {}, "score": "48.861023"}
{"text": "II53-II60 , Washington , DC , USA , June 2004 .View at Scopus .T. Mita , T. Kaneko , B. Stenger , and O. Hori , \" Discriminative feature co - occurrence selection for object detection , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "48.901176"}
{"text": "Best testing accuracy with classification models derived from best training .In each sub - figure , the results shown in column 1 to column 7 are obtained by using KNNC , NBC , NMSC , UDC , SVM_linear , SVM_rbf , and LMNN classifiers , respectively .", "label": "", "metadata": {}, "score": "48.952766"}
{"text": "In our experiments , we choose four representative feature selection algorithms , that is , ReliefF , correlation - based filter selection ( CFS ) , minimum redundancy maximum relevance ( mRMR ) , and general signal to noise ratio ( GSNR ) in comparison with FCBF .", "label": "", "metadata": {}, "score": "48.974747"}
{"text": "The most popular ensemble methods utilize a base classification algorithm to differently permutated training sets .Examples of these techniques include AdaBoost , Bagging , Random Subspace , Random Forest , and Rotation Forest [ 19 ] .AdaBoost has become a very popular choice for its simplicity and adaptability [ 20 ] .", "label": "", "metadata": {}, "score": "49.00639"}
{"text": "12 , pp .6745 - 6750 , 1999 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Blanco , P. Larra\u00f1aga , I. Inza , and B. Sierra , \" Gene selection for cancer classification using wrapper approaches , \" International Journal of Pattern Recognition and Artificial Intelligence , vol .", "label": "", "metadata": {}, "score": "49.019547"}
{"text": "View at Google Scholar .R. Akbani , S. Kwek , and N. Japkowicz , \" Applying support vector machines to imbalanced datasets , \" in Proceedings of the 15th European Conference on Machine Learning ( ECML ' 04 ) , vol .", "label": "", "metadata": {}, "score": "49.049446"}
{"text": "The mRMR criterion computes both the redundancy between features and the relevance of each feature .Redundancy is computed by the mutual information ( MI ) between pairs of features whereas relevance is measured by the MI between each feature and the class labels .", "label": "", "metadata": {}, "score": "49.201126"}
{"text": "Exp ... . by Maria - florina Balcan , Alina Beygelzimer , John Langford - In ICML , 2006 . \" ...We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise .", "label": "", "metadata": {}, "score": "49.2201"}
{"text": "t .y .t . ) has to be estimated for each state y t of the Markov chain , that is , the probability of the observed sensor features x t given the hypothesized state y t of the model .", "label": "", "metadata": {}, "score": "49.35081"}
{"text": "1165 - 1170 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Kim , H. S. Yoon , and J. H. Kim , \" User recognition based on continuous monitoring and tracking , \" in Proceedings of the 6th ACM / IEEE International Conference on Human - Robot Interaction ( HRI ' 11 ) , pp .", "label": "", "metadata": {}, "score": "49.414417"}
{"text": "The next crucial step is to extract features from the spectra and then form the initial complete feature set .The simplest way is to extract every data point as a discriminative feature and generate a huge feature set including more than 15,000 features [ 4 , 5 ] .", "label": "", "metadata": {}, "score": "49.41532"}
{"text": "Conclusion .The primary contribution of the present work is that we conducted the most comprehensive comparative benchmarking of random forests and support vector machines to date , using 22 diagnostic and outcome prediction datasets .Our hypothesis that in previously reported work , research design limitations may have biased the comparison of classifiers in favour of random forests , was verified .", "label": "", "metadata": {}, "score": "49.466553"}
{"text": "10 , pp .906 - 914 , 2000 .View at Google Scholar \u00b7 View at Scopus .N. Friedman , M. Linial , I. Nachman , and D. Pe'er , \" Using Bayesian networks to analyze expression data , \" in Proceedings of the 4th Annual International Conference on Computational Molecular Biology ( RECOMB ' 00 ) , pp .", "label": "", "metadata": {}, "score": "49.51725"}
{"text": "We focus on discriminative structure learning for ANBC since it is shown that a good discriminative structure is sufficient to generate good discriminative classifier in the comparative research [ 11 ] .Indeed , BNC with discriminative structures and generative parameters outperforms BNC with not only discriminative structures and discriminative parameters but also generative structures and either discriminative or generative parameters in their experimental results .", "label": "", "metadata": {}, "score": "49.55429"}
{"text": "For discriminative structure learning of ANBC , local classification rate ( LCR ) is used to score augmented edges , and greedy search algorithm is used to find the discriminative structure that maximizes classification rate ( CR ) .Once a classifier is trained by RPPA and drug sensitivity using cancer patient samples , the classifier is able to predict the drug sensitivity given RPPA information from a patient .", "label": "", "metadata": {}, "score": "49.566616"}
{"text": "In [ 11 ] , they use the greedy method , hill climbing search , to find the structure that has local optimum CR in updating ( adding or deleting augmented edge ) the structure iteratively .However , CR based scoring and searching approach is computationally expensive than other method due to the exponential searching space ( ( n -1 ) n -2 ) as training and testing of updated structure is repeated in every iterations .", "label": "", "metadata": {}, "score": "49.61532"}
{"text": "i .A structure of Bayesian network defines a unique joint probability distribution over X given by the product of local distributions as .P .B .X .X .n . )i .n .P .B .", "label": "", "metadata": {}, "score": "49.648357"}
{"text": "This assumption , however , is not realistic especially in biological domain because the interactive dependencies between cancer - related proteins in signaling pathways may exist .To overcome this limitation of NBC , how to involve the relationship between attributes for improving the classification performance has been the issue of Bayesian network classifier study during the past years .", "label": "", "metadata": {}, "score": "49.71273"}
{"text": "P - values shown with boldface denote statistically significant differences between classification methods at the 0.05 \u03b1 level .Using gene selection .Six classification performance estimates have been produced for each classifier and dataset ( 5 estimates corresponding to various gene selection methods and one estimate corresponding to using no gene selection ) .", "label": "", "metadata": {}, "score": "49.73214"}
{"text": "5 , pp .631 - 643 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Bertoni , R. Folgieri , and G. Valentini , \" Classification of DNA microarray data with Random Projection Ensembles of Polynomial SVMs , \" in Proceedings of the 18th", "label": "", "metadata": {}, "score": "49.7427"}
{"text": "Pattern Recognition 2008 , 41 ( 1 ) : 56 - 66 .View Article .Rivals I , Personnaz L : MLPs ( Mono - Layer Polynomials and Multi - Layer Perceptrons ) for nonlinear modeling .Journal of Machine Learning Research 2003 , 3 : 1383 - 1398 .", "label": "", "metadata": {}, "score": "49.766388"}
{"text": "Pernkopf F , Bilmes J : Discriminative versus generative parameter and structure learning of Bayesian Network Classifiers .International Conference on Machine Learning 2005 , 657 - 664 .Pernkopf F : Bayesian network classifiers versus selective k - NN classifier .", "label": "", "metadata": {}, "score": "49.85401"}
{"text": "To provide a proper temporal format , the timeline is discretized into a set of time slices : measurements of the binary sensors taken at intervals that are regularly spaced with a predetermined time granularity \u0394 t .Sensor events for each time slice t are denoted as .", "label": "", "metadata": {}, "score": "49.94245"}
{"text": "Copyright .\u00a9 Statnikov et al .2008 .This article is published under license to BioMed Central Ltd. \" ...We compare discriminative and generative learning as typified by logistic regression and naive Bayes .We show , contrary to a widely held belief that discriminative classifiers are almost always to be preferred , that there can often be two distinct regimes of performance as the training set size is i ... \" .", "label": "", "metadata": {}, "score": "49.944912"}
{"text": "A detecting window having the same size as its training image can determine whether an image within a window contains a target object or not .Hence , the detecting window slides across the image in raster scan format and classifies each subregion .", "label": "", "metadata": {}, "score": "49.9942"}
{"text": "is much less than the kernel expansion terms , it still grows with additional samples , which may lead to loss of effectiveness and overfitting .There are several possible extensions that may allow to avoid this .One way is to remove classifiers .", "label": "", "metadata": {}, "score": "50.125736"}
{"text": "Data sets and experiments .The following three mass spectrometry data sets have been tested in our experiment : .High resolution time - of - flight ( TOF ) mass spectrometry ( MS ) proteomics data set from surface - enhanced laser / desorption ionization ( SELDI ) ProteinChip arrays on 121 ovarian cancer cases and 95 controls .", "label": "", "metadata": {}, "score": "50.136227"}
{"text": "The link between the two can be seen by observing that the decision function for naive Bayes ( in the binary case ) can be rewritten as \" predict class if the odds of exceed those of \" .Expressing this in log - space gives : .", "label": "", "metadata": {}, "score": "50.13842"}
{"text": "Efron B , Tibshirani R : Improvements on cross - validation : the .632 + bootstrap method .Journal of the American Statistical Association 1997 , 92 : 548 - 560 .View Article .Hastie T , Tibshirani R , Friedman JH : The elements of statistical learning : data mining , inference , and prediction New York , Springer 2001 .", "label": "", "metadata": {}, "score": "50.21858"}
{"text": "Computational Statistics & Data Analysis 2007 , 52 ( 1 ) : 211 - 220 .View Article .Li L , et al .: Applications of the GA / KNN method to SELDI proteomics data .Bioinformatics 2004 , 20 : 1638 - 1640 .", "label": "", "metadata": {}, "score": "50.225567"}
{"text": "Indeed , to verify the efficiency of the proposed method on gene - related data , 8 publically available gene microarray benchmark datasets are analyzed .To this end , we accomplish a comparative study of RotBoost efficiency against several other ensemble and single classifier systems including AdaBoost , Bagging , Rotation Forest single tree , and support vector machines ( SVMs ) .", "label": "", "metadata": {}, "score": "50.340515"}
{"text": "This research was supported by Seoul R&BD Program ( WR080951 ) .References .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. S. Kim , D. H. Yeom , and Y. H. Joo , \" Fast and robust algorithm of tracking multiple moving objects for intelligent video surveillance systems , \" IEEE Transactions on Consumer Electronics , vol .", "label": "", "metadata": {}, "score": "50.360435"}
{"text": "Corchado , J.M. ; Bajo , J. ; Tapia , D.I. ; Abraham , A. Using heterogeneous wireless sensor networks in a telemonitoring system for healthcare .Trans .Inf .Tech .Biomed .[ Google Scholar ] .Herv\u00e1s , R. ; Bravo , J. ; Fontecha , J. Awareness marks : Adaptive services through user interactions with augmented objects .", "label": "", "metadata": {}, "score": "50.44446"}
{"text": "-mean metrics , which are used to evaluate the balance level of classification results .Thus , the correction technologies are useless when the classification tasks are robust to class imbalance .( iv )In contrast with SVM - OAA , the ensemble version EnSVM - OAA helps to slightly improve the overall classification accuracy Acc , with possible sacrifice of two other evaluation metrics on most datasets , which means that classification accuracies between majority and minority classes are further increased .", "label": "", "metadata": {}, "score": "50.542786"}
{"text": "Introduction .Previous studies have shown that gene microarray data analysis is a powerful and revolutionary tool for biological and medical researches by allowing the simultaneous monitoring of the expression levels of tens of thousands of genes [ 1 ] .This is done by measuring the signal intensity of fluorescing molecules attached to DNA species that are bound to complementary strands of DNA localized to the surface of the microarray .", "label": "", "metadata": {}, "score": "50.544765"}
{"text": "It is worth to note that the feature selection is then carried out using only the training samples .Finally , the test error ( classification accuracy ) is estimated on the unseen test samples using ( 3 ) .Table 3 presents the RotBoost mean classification accuracy against the considered 8 gene datasets when transformation matrix is chosen to be either PCA or ICA where the values following \" \u00b1 \" denote the related standard deviations .", "label": "", "metadata": {}, "score": "50.55137"}
{"text": "To estimate the parameters for a feature 's distribution , one must assume a distribution or generate nonparametric models for the features from the training set .[ 8 ] .The assumptions on distributions of features are called the event model of the Naive Bayes classifier .", "label": "", "metadata": {}, "score": "50.665367"}
{"text": "Main method that is used as a basis for our algorithm is Pegasos [ 4 ] , which is a modified SGD method with an added projection step , although more generic algorithms such as NORMA [ 3 ] can be easily used in its place .", "label": "", "metadata": {}, "score": "50.669483"}
{"text": "For design of LPR based classifiers , techniques such as template matching [ 20 ] , support vector machine [ 21 ] , linear programming [ 22 ] , or AdaBoost learning have been used .AdaBoost algorithm is a well - known classifier combination method to construct a strong classifier with weak classifiers [ 23 , 24 ] .", "label": "", "metadata": {}, "score": "50.70179"}
{"text": "Figure 3 : Experimental results .For the Covertype dataset , linear classifiers work best and approach the error rates indicated in the paper [ 12 ] , with Pegasos and our algorithm giving virtually the same results .Conclusion and Future Work .", "label": "", "metadata": {}, "score": "50.725273"}
{"text": "The final values of Acc , . , which indicates the number of base classifiers in each OAA branch , is also empirically assigned as 100 .To equitably compare the performance of various methods , we used the same common parameters .", "label": "", "metadata": {}, "score": "50.79651"}
{"text": "Microarray technology allows large - scale and parallel measurements for expression of around thousands , perhaps even tens of thousands , of genes .Among these applications , cancer classification , which has been the subject of extensive research all around the world , is most promising [ 10 ] .", "label": "", "metadata": {}, "score": "50.816322"}
{"text": "Since naive Bayes is also a linear model for the two \" discrete \" event models , it can be reparametrised as a linear function .Obtaining the probabilities is then a matter of applying the logistic function to , or in the multiclass case , the softmax function .", "label": "", "metadata": {}, "score": "50.827938"}
{"text": "606 - 615 , 2005 .View at Google Scholar \u00b7 View at Scopus .N. Japkowicz and S. Stephen , \" The class imbalance problem : a systematic study , \" Intelligent Data Analysis , vol .6 , no .", "label": "", "metadata": {}, "score": "50.85548"}
{"text": "The Newton 's optimization of the function of exponential criterion .In the next section , we discuss our two main contributions in this paper .That is , how the best weak classifier for constructing a strong classifier is selected and how we derive a reinforced prediction of .", "label": "", "metadata": {}, "score": "50.886658"}
{"text": "Chen X , Zeng X , van Alphen D : Multi - class feature selection for texture classification .Pattern Recognition Letters 2006 , 27 : 1685 - 1691 .View Article .Science 1999 , 286 : 531 - 537 .", "label": "", "metadata": {}, "score": "50.906536"}
{"text": "View Article .Heijden F , Duin RPW , Ridder D , Tax DMJ : Classification , parameter estimation and State estimation - an engineering approach using Matlab John Wiley & Sons 2004 .ISBN 0470090138 , .Pusztai , et al .", "label": "", "metadata": {}, "score": "50.955414"}
{"text": "In the experiments , ANBC with proposed score function is compared to well - known classification algorithms such as Support vector machine , Logistic regression , and Random forest .We also compare to Bayesian network classifiers , TAN and NBC with generative parameters .", "label": "", "metadata": {}, "score": "51.10831"}
{"text": "Classification performance of SVMs and RFs with gene selection .The performance is estimated using area under ROC curve ( AUC ) for binary classification tasks and relative classifier information ( RCI ) for multicategory tasks .Table 2 .Comparison of classification performance of SVMs and RFs with gene selection .", "label": "", "metadata": {}, "score": "51.11199"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .C.-H. Yang , L.-Y. Chuang , and C.-H. Yang , \" IG - GA : a hybrid filter / wrapper method for feature selection of microarray data , \" Journal of Medical and Biological Engineering , vol .", "label": "", "metadata": {}, "score": "51.120583"}
{"text": "15 , pp .2429 - 2437 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .I. Kononenko , \" Estimating attributes : analysis and extensions of RELIEF , \" in Proceedings of the European Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "51.141106"}
{"text": "S316-S322 , 2001 .View at Google Scholar \u00b7 View at Scopus .S. J. Joseph , K. R. Robbins , W. Zhang , and R. Rekaya , \" Comparison of two output - coding strategies for multi - class tumor classification using gene expression data and latent variable model as binary classifier , \" Cancer Informatics , vol .", "label": "", "metadata": {}, "score": "51.16605"}
{"text": "A detailed explanation related to this is described in Section 3 .[ 27 , 28 ] .Now , we have to find a way to optimize the exponential criterion for Adaboost learning .In this paper , we use the Newton 's method for optimizing the exponential criterion [ 28 ] since the characteristic of an exponential criterion is monotonic and smooth .", "label": "", "metadata": {}, "score": "51.174805"}
{"text": "The description in Table 2 gives four baseline statistical components , where TP and FN denote the number of positive examples which are accurately and falsely predicted , respectively , and TN and FP represent the number of negative samples that are predicted accurately and wrongly , respectively .", "label": "", "metadata": {}, "score": "51.309914"}
{"text": "The peak is identified where the first derivative is changing from a positive to a negative .In our mass spectrum experiment , the peak detection method proposed by Coombes et al [ 19 ] is performed on a mean spectrum rather than individual spectra .", "label": "", "metadata": {}, "score": "51.348026"}
{"text": "To avoid the curse of dimensionality problem , gene selection plays a crucial role in DNA microarray analysis .Another important reason to reduce dimensionality is to help biologists to identify the underlying mechanism that relates gene expression to diseases .Indeed , the microarray data is associated with various uncertainties such as microarray data , gathering process which include fabrication , hybridization and image processing .", "label": "", "metadata": {}, "score": "51.35685"}
{"text": "Then , we present how to create the hybrid recognition system through the effective combination of the HMM with discriminative classifiers .Hidden Markov Model .A standard HMM is a generative probabilistic model defined in terms of an observable variable x t and a hidden variable y t at each discrete time instant .", "label": "", "metadata": {}, "score": "51.3596"}
{"text": "The GSNR is a measure of the ratio between intergroup and intragroup variations .Higher GSNR values indicate higher discrimination power for the gene .GSNR selects .In order to reduce the computational complexity of the problem at hand and select the most informative genes , we run all 5 feature selection algorithms against each dataset and obtain the number of selected features for each algorithm .", "label": "", "metadata": {}, "score": "51.386894"}
{"text": "Comput .Sci .[ Google Scholar ] .Van Kasteren , T.L.M. ; Noulas , A. ; Englebienne , G. ; Kr\u00f6 se , B.J. Accurate Activity Recognition in a Home Setting .Proceedings of the Conference on Autonomous Agents and Multiagent Systems ( AAMAS 2007 ) , Seoul , South Korea , 21 - 24 September 2007 ; pp . 1 - 9 .", "label": "", "metadata": {}, "score": "51.434746"}
{"text": "We show that A2 achieves an exponential improvement ... \" .We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise .The algorithm , A2 ( for Agnostic Active ) , relies only upon the assumption that the samples are drawn i.i.d . from a fixed distribution .", "label": "", "metadata": {}, "score": "51.44345"}
{"text": "For linear SVMs , . is a simple inner product of input and weight vectors , while for SVMs using kernel trick for nonlinear classification : .Several methods exist for solving both primal and dual formulations of the SVM optimization problem .", "label": "", "metadata": {}, "score": "51.526237"}
{"text": "To cope with these challenges and to develop a more robust and accurate learning method , ensemble learning methodology is utilized .The datasets are first preprocessed , and then to reduce the computational complexity and select the most informative genes , FCBF is applied to these datasets .", "label": "", "metadata": {}, "score": "51.53192"}
{"text": "Ph .D.Thesis , Technischen Universit\u00e4t Berlin , School of Computer Science 1999 .Furey TS , Cristianini N , Duffy N , Bednarski DW , Schummer M , Haussler D : Support vector machine classification and validation of cancer tissue samples using microarray expression data .", "label": "", "metadata": {}, "score": "51.655514"}
{"text": "A reinforced AdaBoost learning algorithm is proposed for object detection with local pattern representations .In implementing Adaboost learning , the proposed algorithm employs an exponential criterion as a cost function and Newton 's method for its optimization .In particular , we introduce an optimal selection of weak classifiers minimizing the cost function and derive the reinforced predictions based on a judicial confidence estimate to determine the classification results .", "label": "", "metadata": {}, "score": "51.768883"}
{"text": "Therefore , to apply SVM to multiclass problems , it should be reconfigured for multiple binary - class problems by using a coding strategy [ 21 ] .Previous studies have presented several well - known coding strategies , including one - against - all ( OAA ) , one - against - one ( OAO ) , decision directed acyclic graph ( DDAG ) , and error correcting output codes ( ECOC ) .", "label": "", "metadata": {}, "score": "51.825172"}
{"text": "The hybrid models achieve the best F - Measure value , but the difference with other models is considered to be not statistically significant in some cases .Regarding the type of representation employed , in general terms , the best results are obtained using the \" LastSensor \" configuration as standalone and the \" ChangePoint + LastSensor \" concatenation as combined representation .", "label": "", "metadata": {}, "score": "51.86825"}
{"text": "Statnikov A , Tsamardinos I , Dosbayev Y , Aliferis CF : GEMS : a system for automated cancer diagnosis and biomarker discovery from microarray gene expression data .Int J Med Inform 2005 , 74 : 491 - 503 .View Article PubMed .", "label": "", "metadata": {}, "score": "51.899395"}
{"text": "[ 2 ] .In the multivariate Bernoulli event model , features are independent booleans ( binary variables ) describing inputs .Like the multinomial model , this model is popular for document classification tasks , [ 9 ] where binary term occurrence features are used rather than term frequencies .", "label": "", "metadata": {}, "score": "51.96614"}
{"text": "In our opinion , it is caused by the difference between microarray data and MS data .Microarray data have a huge number of variables .It has a complicated correlation / interaction among genes as well as high redundancy .MALDI - MS data consist of mass / charge ratio values , after peak detection , correlation / interaction among peaks are generally not as complicated and much less redundancy exists .", "label": "", "metadata": {}, "score": "52.02357"}
{"text": "Fifth , no statistical comparison among classifiers has been performed .Furthermore , .632 + bootstrap is currently not developed for performance metrics other than proportion of correct classifications .We hypothesize that these apparent methodological biases of prior work have compromised its conclusions and the question of whether random forests indeed outperform SVMs for classification of microarray gene expression data is not convincingly answered .", "label": "", "metadata": {}, "score": "52.053238"}
{"text": "The code matrix of OAA is presented in Figure 1(a ) .In practical applications , OAA assigns the class label with the highest decision output value to the test instance .Unlike OAA , OAO trains . binary classifiers and assigns each one by using only two original classes and simply ignoring the others .", "label": "", "metadata": {}, "score": "52.10865"}
{"text": "This training algorithm is an instance of the more general expectation - maximization algorithm ( EM ) : the prediction step inside the loop is the E -step of EM , while the re - training of naive Bayes is the M -step .", "label": "", "metadata": {}, "score": "52.169598"}
{"text": "Figure 3 : The frame diagram of the ensemble learning algorithms based on feature subspace and counter voting rule for classifying imbalanced multiclass cancer microarray data .Datasets .Performance Evaluation Metrics and Experimental Settings .When one classification task is skewed , the overall classification accuracy Acc is no longer an appropriate evaluation metric for estimating the quality of a classifier .", "label": "", "metadata": {}, "score": "52.17914"}
{"text": "In each sub - figure , the results shown in column 1 to column 7 are obtained by using KNNC , NBC , NMSC , UDC , SVM_linear , SVM_rbf , and LMNN classifiers , respectively .Best testing performance under best training .", "label": "", "metadata": {}, "score": "52.18149"}
{"text": "12 , pp .2037 - 2041 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "52.18719"}
{"text": "In RPPA , sample lysates are immobilized in series of dilutions to generate dilution curves for quantitative measurements being able to use only small amount ( nanoliter ) of sample while other protein arrays immobilize antibodies .After primary and secondary antibodies are probed , signal is detected by Qdot assays .", "label": "", "metadata": {}, "score": "52.20387"}
{"text": "10.1186/1477 - 5956 - 9 - 53 PubMed Central PubMed View Article .Duda RO , Hart PE : Pattern Classification and Scene Analysis .John Wiley & Sons Inc ; 1973 .Friedman N , Geiger D , Goldszmidt M : Bayesian Network Classifiers .", "label": "", "metadata": {}, "score": "52.226532"}
{"text": "View at Scopus . H. L. Yu , S. Gao , B. Qin , et al . , \" Multiclass microarray data classification based on confidence evaluation , \" Genetics and Molecular Research , vol .11 , no . 2 , pp .", "label": "", "metadata": {}, "score": "52.253452"}
{"text": "DNA microarray technology can measure the activities of tens of thousands of genes simultaneously , which provides an efficient way to diagnose cancer at the molecular level .Although this strategy has attracted significant research attention , most studies neglect an important problem , namely , that most DNA microarray datasets are skewed , which causes traditional learning algorithms to produce inaccurate results .", "label": "", "metadata": {}, "score": "52.253563"}
{"text": "1257 - 1269 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Zhang , W. Jia , X. He , and Q. Wu , \" Learning - based license plate detection using global and local features , \" in Proceedings of the IEEE International Conference on Pattern Recognition ( ICPR ' 06 ) , vol .", "label": "", "metadata": {}, "score": "52.265293"}
{"text": "Besides , in our case , the implementation of the SVM has to reduce our multiclass problem into multiple binary classification problems .As previously explained , in a classic hidden Markov modeling approach , the emission probability density .p .", "label": "", "metadata": {}, "score": "52.286777"}
{"text": "Our data also underlines the importance of sound research design in benchmarking and comparison of bioinformatics algorithms .Conclusion .We found that both on average and in the majority of microarray datasets , random forests are outperformed by support vector machines both in the settings when no gene selection is performed and when several popular gene selection methods are used .", "label": "", "metadata": {}, "score": "52.305473"}
{"text": "t .x .y .x .t .x .t .N .x .t .N . )T . is defined for each time slice .In the employed data representation , each time interval strictly corresponds to a single data instance .", "label": "", "metadata": {}, "score": "52.32771"}
{"text": "Classification performance evaluation metrics .We used two classification performance metrics .For binary tasks , we used the area under the ROC curve ( AUC ) which was computed from continuous outputs of the classifiers ( distances from separating hyperplane for SVMs and outcome probabilities for RFs ) [ 8 ] .", "label": "", "metadata": {}, "score": "52.39315"}
{"text": "Haar - like features represent differences of intensity or gradient in specific regions and may have infinite real number of feature values .In contrast , LPR represents various forms of spatial relative relationship between a specific pixel and its neighboring pixels and has a finite number of feature values .", "label": "", "metadata": {}, "score": "52.39421"}
{"text": "i .j .Accuracy is calculated by a ratio of the number of correct predictions to the total number of samples in leave - one - out estimation .In addition , for reasonable comparison , feature selection is applied for all classification methods because some of methods may not produce a good result in high dimension data and also all 55 proteins may be not related to drug sensitivity directly .", "label": "", "metadata": {}, "score": "52.449913"}
{"text": "View Article .Sindhwani V , Bhattacharyya P , Rakshit S : Information Theoretic Feature Crediting in Multiclass Support Vector Machines .Proceedings of the First SIAM International Conference on Data Mining 2001 .Harrell FE Jr. , Lee KL , Mark DB : Multivariable prognostic models : issues in developing models , evaluating assumptions and adequacy , and measuring and reducing errors .", "label": "", "metadata": {}, "score": "52.55744"}
{"text": "Figure 2 illustrates these histograms against the typical Lung cancer dataset .Figure 2 : Kappa error diagrams for the Lung dataset using different ensemble algorithms .We perform these experiments on all 8 gene datasets , and overall the ICA and PCA RotBoost methods perform best in terms of accuracy with average classification accuracy about 98.0 % and 96.3 % , respectively .", "label": "", "metadata": {}, "score": "52.57479"}
{"text": "Experimental Results .This section describes how we evaluated the performance of the proposed learning algorithm .In order to validate the effectiveness of the learning algorithm proposed in this paper , the proposed learning algorithm is compared in terms of accuracy and learning speed with a conventional learning algorithm using the same features [ 17 - 19 , 25 ] .", "label": "", "metadata": {}, "score": "52.592564"}
{"text": "For performance evaluation , we measured image classification performance over face and license plate database using the MCT feature often used in the LPR based methods .Also , we have conducted an experiment to detect objects in field images with the trained object detector .", "label": "", "metadata": {}, "score": "52.610435"}
{"text": "View at Google Scholar .J. A. Blackard , \" Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables , \" Computers and Electronics in Agriculture , vol .24 , no . 3 , pp .", "label": "", "metadata": {}, "score": "52.691647"}
{"text": "We refer to this method as \" RFVS1 .\" Even though RFE was originally introduced as a method for binary classification problems , it can be trivially extended to multiclass case by using binary SVM models in \" one - versus - rest \" fashion ( e.g. , see [ 27 ] ) .", "label": "", "metadata": {}, "score": "52.718594"}
{"text": "In these experiments , we use MCT with a .Database .For training of the classifier and performance assessment of our proposed algorithm , we used a dataset which consists of human face images and vehicle license plate images .To obtain the face training images , we took the following steps .", "label": "", "metadata": {}, "score": "52.823715"}
{"text": "The average performance of SVMs is 0.775 AUC and 0.860 RCI in binary and multicategory classification tasks , respectively .The average performance of RFs in the same tasks is 0.742 AUC and 0.803 RCI .Classification performance of SVMs and RFs without gene selection .", "label": "", "metadata": {}, "score": "52.871834"}
{"text": "In particular , several methods for online boosting and online support vector machine ( SVM ) training has been proposed .However , those methods have several limitations .Furthermore , kernel - using SVM usually have significant storage and computational requirements due to the large amount of kernel expansion terms .", "label": "", "metadata": {}, "score": "52.95717"}
{"text": "5 , pp . 1651 - 1686 , 1998 .View at Google Scholar \u00b7 View at Scopus .G. Ratsch , B. Scholkopf , S. Mika , and K.-R. Muller , \" SVM and boosting : one class , \" Tech .", "label": "", "metadata": {}, "score": "53.015137"}
{"text": "The numbers in rows of \" Rounds \" and \" Pixels \" mean the required number of rounds for the training error to become zero and the final number of combined pixels in the classifier , respectively .The number in the row of \" Dataset \" means the number of datasets for training of the classifier at each stage .", "label": "", "metadata": {}, "score": "53.063927"}
{"text": "-score , and Acc , which are described in ( 14 ) , ( 15 ) , and ( 17 ) , respectively , as evaluation metrics .We empirically performed threefold cross - validation [ 16 ] to evaluate classification performance .", "label": "", "metadata": {}, "score": "53.100864"}
{"text": "Q . exp .s .i . ) where s y is the y th output value before applying the softmax function .Regarding the SVM , the transformation of the model 's class distances to probabilities is done by applying a sigmoid function : . p .", "label": "", "metadata": {}, "score": "53.15685"}
{"text": "Abstractly , naive Bayes is a conditional probability model : given a problem instance to be classified , represented by a vector representing some n features ( independent variables ) , it assigns to this instance probabilities .The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values , then basing such a model on probability tables is infeasible .", "label": "", "metadata": {}, "score": "53.1721"}
{"text": "Generally speaking , support vector machine ( SVM ) is the best choice for classifying cancer microarray data because of its advantages , such as its high generalization capability , absence of local minima , and adaptability for high - dimension and small sample data [ 20 ] .", "label": "", "metadata": {}, "score": "53.225616"}
{"text": "Stadermann et al . , presented an acoustic model combining SVMs and HMMs that obtained an improvement of the word error rate compared with baseline acoustic models [ 27 ] .Ganapathiraju et al . , also employed an implementation of a hybrid SVM / HMM system for speech recognition , where the SVMs were trained on segment level data with one - state HMMs [ 28 ] .", "label": "", "metadata": {}, "score": "53.237377"}
{"text": "The resulting model is denoted as a hybrid HMM approach , where the temporal characteristics of the data are modeled by HMM state transitions and a machine learning scheme is used to model HMM state distributions .An important advantage of such hybrid models is that existing methods for HMM design , training and recognition can be employed without significant modifications , since the hybrid HMM behaves essentially as a conventional HMM .", "label": "", "metadata": {}, "score": "53.255627"}
{"text": "Table 2 .Accuracy of sensitivity prediction for 24 drugs with 20 selected features .Conclusion .In this paper , we introduce the personalized medicine with RPPA and drug sensitivity .The goal of personalized medicine is to provide the optimal therapy to patients who have Different biological profile regarding the target cancer .", "label": "", "metadata": {}, "score": "53.304375"}
{"text": "[ Google Scholar ] .Marukatat , S. ; Artires , T. ; Gallinari , P. ; Dorizzi , B. Sentence Recognition through Hybrid Neuro - Markovian Modeling .Proceedings of the International Conference on Document Analysis and Recognition ( ICDAR ) , Seattle , WA , USA , 10 - 13 September 2001 ; pp .", "label": "", "metadata": {}, "score": "53.30472"}
{"text": "By using spline interpolation , we resample the data and confine the interval to a unified size .Before re - sampling , the sample spectrum has little variation from the true spectrum .The data is re - sampled to a standard discrete data that could be analyzed in a frequency domain .", "label": "", "metadata": {}, "score": "53.31691"}
{"text": "22 , no .6 , pp .583 - 600 , 2000 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Friedman , T. Hastie , and R. Tibshirani , \" Additive logistic regression : a statistical view of boosting , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "53.3776"}
{"text": "We call the space that merely contains the . denotes the kernel function .Some previous studies have found that the radial basis kernel function ( RBF ) generally produces better classification accuracy than many other kernel functions [ 20 , 30 ] .", "label": "", "metadata": {}, "score": "53.43737"}
{"text": "For example , in Figure 4 , consider that there are .while LPR feature values range from 0 to 510 .The histogram of all 576 pixel locations accumulates the sample weights that fall into each of the bins from positive samples and negative samples .", "label": "", "metadata": {}, "score": "53.462166"}
{"text": "Figure 12 : ( a ) Examples of detection results for MIT + CMU DB .( b ) Examples of detection results for BioID DB .( c ) Examples of detection results for license plate DB .Conclusions .In this paper , we proposed a reinforced learning algorithm for image classification with LPR .", "label": "", "metadata": {}, "score": "53.52001"}
{"text": "To classify MALDI MS data , peak detection , feature selection , and classifier are generally important to obtain the final results .To compare public peak detection algorithms , Yang et al .[ 9 ] recently conducted an experimental study using five single spectrum based peak detection algorithms including Cromwell [ 10 ] , CWT [ 11 ] , PROcess [ 12 ] , LMS [ 13 ] , and LIMPIC [", "label": "", "metadata": {}, "score": "53.61263"}
{"text": "Methods .Microarray datasets and classification tasks .Gene expression microarray datasets used in the present work are described in Table 4 .All 22 datasets span the domain of cancer ; 14 datasets correspond to diagnostic tasks ( and denoted with prefix \" Dx \" ) and 8 are concerned with clinical outcome prediction ( and denoted with \" Px \" ) .", "label": "", "metadata": {}, "score": "53.90982"}
{"text": "It is spam if ( i.e. , ) , otherwise it is not spam .^ Caruana , R. ; Niculescu - Mizil , A. ( 2006 ) .An empirical comparison of supervised learning algorithms .Proc . 23rdInternational Conference on Machine Learning .", "label": "", "metadata": {}, "score": "53.913414"}
{"text": "Artificial Intelligence in Medicine 2010 , 49 ( 3):177 - 185 .10.1016/j.artmed.2010.04.001 PubMed View Article .Wang X , Dong Y , Jiwani A , Zou Y , Pastor J , Kuro - O M , Habib A , Ruan M , Boothman D , Yang C : Improved protein arrays for quantitative systems analysis of the dynamics of signaling pathway interactions .", "label": "", "metadata": {}, "score": "53.965393"}
{"text": "Therefore , it can be said that these results indicate that the proposed learning algorithm is more efficient compared to the conventional learning algorithm .The face detector has been tested on two commonly used databases .One is the CMU + MIT database [ 30 ] which has 107 images with 450 visible frontal faces excluding hand - drawn images , cartoons , and images containing small faces .", "label": "", "metadata": {}, "score": "53.99785"}
{"text": "The datasets contain 50 - 308 samples and 2,000 - 24,188 variables ( genes ) after data preparatory steps described in [ 1 ] .Similarly , all prognostic datasets were obtained from the links given in the primary study for each dataset .", "label": "", "metadata": {}, "score": "54.028862"}
{"text": "For .Table 5 summarizes the kappa - error values for typical Lung cancer dataset with FCBF gene selection method in terms of the centroids of different ensembles .From this table it is clear that the ICA - based RotBoost provides the highest pairwise accuracy , and the second best accuracy is achieved by PCA - based RotBoost .", "label": "", "metadata": {}, "score": "54.123184"}
{"text": "Vapnik VN : Statistical learning theory John Wiley and Sons , New York 1998 .Tenenbaum J , Silva V , Langford JC : A global geometric framework for nonlinear dimensionality reduction .Science 2000 , 290 : 2319 - 2323 .", "label": "", "metadata": {}, "score": "54.226456"}
{"text": "As most learning algorithms of that time , AdaBoost was designed for offline ( batch ) training , with the error rate estimated over all available samples .There are , therefore , several difficulties involved in employing boosting as an online training algorithm , several of which were mentioned in [ 1 ] .", "label": "", "metadata": {}, "score": "54.327396"}
{"text": "p .y .t .x .t .p .y .t . ) can be directly used as emission probabilities in the addressed scheme since , during recognition , the scaling factor .p .x .t . is a constant for all states .", "label": "", "metadata": {}, "score": "54.373127"}
{"text": "View at Scopus .G. J. Gordon , R. V. Jensen , L.-L. Hsiao et al . , \" Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma , \" Cancer Research , vol .", "label": "", "metadata": {}, "score": "54.41736"}
{"text": "Figure 9(b ) shows a comparison of performance of the proposed and conventional algorithms for testing error .Through the experimental results , it can be claimed that the error rate of the proposed algorithm converged to zero faster than that of the conventional algorithm .", "label": "", "metadata": {}, "score": "54.42666"}
{"text": "The microarray data measures the expressions of tens of thousands of genes , producing a feature vector that is high in dimensionality and that contains much irrelevant information .This dimensionality degrades classification performance .Moreover , datasets typically contain few samples for training ( e.g. , lung dataset [ 12 ] contains 12535 genes and only 181 samples ) , leading to the curse of dimensionality problem .", "label": "", "metadata": {}, "score": "54.49128"}
{"text": "23 - 28 , 2010 .View at Google Scholar \u00b7 View at Scopus .L. Yu and H. Liu , \" Efficient feature selection via analysis of relevance and redundancy , \" Journal of Machine Learning Research , vol .5 , no .", "label": "", "metadata": {}, "score": "54.52911"}
{"text": "N .i .j .k . . . .After the parameters are estimated , then these parameters .i .j .i .j . q . k .r .Augmented naive Bayes classifier .To solve the limitation of NBC , Friedman et al .", "label": "", "metadata": {}, "score": "54.58738"}
{"text": "Figure 11(b ) shows the results of the face detectors for the BioID database .As shown in Figure 11(b ) , the proposed method has a higher detection rate than the conventional method with an identical number of false positives .", "label": "", "metadata": {}, "score": "54.615143"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" Experiments with a new boosting algorithm , \" in Proceedings of the International Conference on Machine Learning ( ICML ' 96 ) , pp .", "label": "", "metadata": {}, "score": "54.676155"}
{"text": "S. Shalev - Shwartz , Y. Singer , and N. Srebro , \" Pegasos : primal estimated sub - GrAdient sOlver for SVM , \" in Proceedings of the 24th ACM International Conference on Machine Learning ( ICML ' 07 ) , pp .", "label": "", "metadata": {}, "score": "54.684837"}
{"text": "This is problematic because it will wipe out all information in the other probabilities when they are multiplied .Therefore , it is often desirable to incorporate a small - sample correction , called pseudocount , in all probability estimates such that no probability is ever set to be exactly zero .", "label": "", "metadata": {}, "score": "54.786896"}
{"text": "View Article PubMed .Breiman L : Random forests .Machine Learning 2001 , 45 : 5 - 32 .View Article .Wu B , Abbott T , Fishman D , McMurray W , Mor G , Stone K , Ward D , Williams K , Zhao H : Comparison of statistical methods for classification of ovarian cancer using mass spectrometry data .", "label": "", "metadata": {}, "score": "54.795174"}
{"text": "Related Work .Our algorithm is closely related to algorithms used for SVM training and boosting .These algorithms , taken together , can be separated into several classes : .Support Vector Machines .The support vector machines are a class of linear or kernel - based binary classifiers that attempt to maximize the minimal distance ( margin ) between each member of the class and separating surface .", "label": "", "metadata": {}, "score": "54.80346"}
{"text": "Tapia , E.M. ; Intille , S.S. ; Larson , K. Activity recognition in the home using simple and ubiquitous sensors .Pervasive Comput .[ Google Scholar ] .Wilson , D. ; Atkeson , C. Simultaneous Tracking and Activity Recognition ( STAR )", "label": "", "metadata": {}, "score": "54.924297"}
{"text": "64 , no . 1 , pp .137 - 149 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. L. Nutt , D. R. Mani , R. A. Betensky et al . , \" Gene expression - based classification of malignant gliomas correlates better with survival than histological classification , \" Cancer Research , vol .", "label": "", "metadata": {}, "score": "54.932705"}
{"text": "x .A . exp .s .y .B . ) where s y denotes the SVM output representing class y.The sigmoid function parameters A and B are estimated using the algorithm from Reference [ 38 ] .Hence , the output values of the classifiers are estimates of the probability distribution over states conditioned on the input : . g .", "label": "", "metadata": {}, "score": "54.95328"}
{"text": "MS data is generated with chemical noise through matrix or ion overloading , and the noise usually shows up as a baseline along the spectrum .Baseline correction computes the local minimum value , draws a baseline represented as the background noise , and subtracts the baseline from the spectrum .", "label": "", "metadata": {}, "score": "55.105072"}
{"text": "Table 1 .Comparison of classification performance of SVMs and RFs without gene selection .The performance is estimated using area under ROC curve ( AUC ) for binary classification tasks and relative classifier information ( RCI ) for multicategory tasks .", "label": "", "metadata": {}, "score": "55.15789"}
{"text": "Artif .Intell .[ Google Scholar ] .Rigoll , G. ; Neukirchen , C. A New Approach to Hybrid HMM / ANN Speech Recognition Using Mutual Information Neural Networks .[ Google Scholar ] .Bengio , Y. ; Lecun , Y. ; Nohl , C. ; Burges , C. LeRec : A NN / HMM hybrid for on - line handwriting recognition .", "label": "", "metadata": {}, "score": "55.216045"}
{"text": "504 - 507 .Markov , K. ; Nakamura , S. Using hybrid HMM / BN acoustic models : Design and implementation issues .IEICE Trans .Inf .Syst .2006 , E89-D , 981 - 988 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "55.23522"}
{"text": "Experimental setup .To evaluate the performance of Different methods , we measure the prediction accuracy on average using leave - one - out estimation Since the structure is randomly updated in searching , 5 times leave - one - out are performed in ANBC .", "label": "", "metadata": {}, "score": "55.434223"}
{"text": "F - Measure can be calculated from the precision and recall scores as follows : .The models were validated splitting the original data into a test and training set using a \" leave one day out \" approach , retaining one full day of sensor readings for testing and using the remaining sub - samples as training data .", "label": "", "metadata": {}, "score": "55.444126"}
{"text": "262 - 267 , 2000 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. S. Furey , N. Cristianini , N. Duffy , D. W. Bednarski , M. Schummer , and D. Haussler , \" Support vector machine classification and validation of cancer tissue samples using microarray expression data , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "55.566956"}
{"text": "The sensor streams have been employed using three different representations : .Raw : The raw sensor representation uses the sensor data in the same way it was received from the sensors network .The value is 1 when the sensor is active and 0 otherwise ( see Figure 4(a ) ) .", "label": "", "metadata": {}, "score": "55.59146"}
{"text": "185 - 208 , MIT Press , Cambridge , Mass , USA , 1999 .View at Google Scholar . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "55.74243"}
{"text": "Williams B , Cornett S , Dawant B , Crecelius A , Bodenheimer B , Caprioli R : An algorithm for baseline correction of MALDI mass spectra .Proceedings of the 43rd annual Southeast regional conference , March 18 - 20 , 2005 , Kennesaw , Georgia 2005 .", "label": "", "metadata": {}, "score": "55.825462"}
{"text": "The discussion so far has derived the independent feature model , that is , the naive Bayes probability model .The naive Bayes classifier combines this model with a decision rule .One common rule is to pick the hypothesis that is most probable ; this is known as the maximum a posteriori or MAP decision rule .", "label": "", "metadata": {}, "score": "55.84832"}
{"text": "The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 9 - 319 ) contains supplementary material , which is available to authorized users .Background .Gene expression microarrays are becoming increasingly promising for clinical decision support in the form of diagnosis and prediction of clinical outcomes of cancer and other complex diseases .", "label": "", "metadata": {}, "score": "55.87617"}
{"text": "619 - 628 , 2012 .View at Google Scholar .S. A. Armstrong , J. E. Staunton , L. B. Silverman et al . , \" MLL translocations specify a distinct gene expression profile that distinguishes a unique leukemia , \" Nature Genetics , vol .", "label": "", "metadata": {}, "score": "55.90886"}
{"text": "24 , pp .13790 - 13795 , 2001 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Ramaswamy , P. Tamayo , R. Rifkin et al . , \" Multiclass cancer diagnosis using tumor gene expression signatures , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "55.960045"}
{"text": "[ 3 ] .In the statistics and computer science literature , Naive Bayes models are known under a variety of names , including simple Bayes and independence Bayes .[ 1 ] : 482 .Naive Bayes is a simple technique for constructing classifiers : models that assign class labels to problem instances , represented as vectors of feature values , where the class labels are drawn from some finite set .", "label": "", "metadata": {}, "score": "55.987144"}
{"text": "Class imbalance occurs when examples from one class outnumber those of the other class , which results in great underestimation of the classification performance of the minority , thereby further affecting the evaluation precision of the overall classification performance .In other words , developing a clinical tumor diagnostic system is meaningless if class imbalance is not considered .", "label": "", "metadata": {}, "score": "56.010456"}
{"text": "10 , pp .1104 - 1125 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . U.Alon , N. Barka , D. A. Notterman et al . , \" Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "56.042114"}
{"text": "Figure 1 : An example of the feature extraction of an image region in a vehicle license plate using MCT .Typical feature extraction methods widely used in pattern recognition produce continuous real - valued features such as differences of intensity , magnitudes of edge , or directions of edge .", "label": "", "metadata": {}, "score": "56.07223"}
{"text": "The results for datasets \" KasterenA \" and \" KasterenB \" are quite similar .For dataset \" KasterenC \" , the hybrid MLP / HMM approach outperforms the other models , but the differences in this case can not be considered to be significant either .", "label": "", "metadata": {}, "score": "56.11625"}
{"text": "BMC Genomics 2006 , 7 : 278 .View Article PubMed .Hammer B , Gersmann K : A Note on the Universal Approximation Capability of Support Vector Machines .Neural Processing Letters 2003 , 17 : 43 - 53 .View Article .", "label": "", "metadata": {}, "score": "56.163303"}
{"text": "For cancer patients with different drug sensitivity , the proteomic profiling reveals important pathophysiologic information which can be used to predict chemotherapy responses .Results .The goal of this paper is to present a framework for personalized medicine using both RPPA and drug sensitivity ( drug resistance or intolerance ) .", "label": "", "metadata": {}, "score": "56.24656"}
{"text": "Indeed , from individual accuracy values , we observe that for all the datasets except SRBCT , FCBF can highly increase the overall gene classification accuracy .On the other hand , CFS method achieves the second best classification accuracy , and both relief and GSNR accomplish more than 90 % average accuracy .", "label": "", "metadata": {}, "score": "56.254753"}
{"text": "Boosting is a meta - algorithm for supervised learning that combines several weak classifiers , that is , classifiers that can label examples only slightly better than random guessing , into a single strong classifier with arbitrary classification accuracy .One of the most successful and well known examples is AdaBoost [ 8 ] and its variants , like LPBoost .", "label": "", "metadata": {}, "score": "56.26529"}
{"text": "Bishop , CM .Neural Networks for Pattern Recognition ; Oxford University Press : Oxford , UK , 1995 .[ Google Scholar ] .Platt , J. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods .Adv .", "label": "", "metadata": {}, "score": "56.303364"}
{"text": "( 1 ) High - dimensional linearly separable data ( Linear ) .A random hyperplane is created in 50-dimensional space .Data points are generated randomly to both positive and negative sides of the hyperplane .Data points too close to the hyperplane are filtered out .", "label": "", "metadata": {}, "score": "56.32967"}
{"text": "As in ( 4 ) , but the parameters of a distribution are changed slightly each iteration , simulating target movement .This experiment estimates the ability of the algorithms to adapt to gradual changes in the data distribution .( 5 ) Bayes - separable data with switching distribution ( Switching ) .", "label": "", "metadata": {}, "score": "56.341164"}
{"text": "26 , pp .15149 - 15154 , 2001 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. \u00d6zg\u00fcr , L. \u00d6zg\u00fcr , and T. G\u00fcng\u00f6r , \" Text categorization with class - based and corpus - based keyword selection , \" Lecture Notes in Computer Science , vol .", "label": "", "metadata": {}, "score": "56.408073"}
{"text": "However , Wireless Sensor Networks ( WSN ) are considered one of the most promising technologies for enabling health monitoring at home due to their suitability to supply constant supervision , flexibility , low cost and rapid deployment [ 10 , 11 ] .", "label": "", "metadata": {}, "score": "56.52156"}
{"text": "In addition , RPPA offers more accurate pathophysiologic information in a signaling pathway with posttranslational modifications ( e.g. phosphorylation ) not obtainable by gene microarray and protein - protein interactions .Naive Bayes Classifiers ( NBC ) [ 9 ] ( Figure 2(A ) ) competitively works with state - of - the art classifiers in many complex real - world applications .", "label": "", "metadata": {}, "score": "56.60978"}
{"text": "Section 6 summarizes the main contributions of this paper .Methods .Coding Strategies for Transforming Multiclass into Multiple Binary Classes .Coding strategies are often used to transform multiclass into multiple binary - classes [ 21 ] .OAA , OAO , and ECOC can be described by a code matrix .", "label": "", "metadata": {}, "score": "56.624397"}
{"text": "s .i . g .n .s .i . g .n .There are several key differences between our algorithms and SVM using Mercer kernels as described in [ 3 , 4 ] .The first and possibly most important one is that the number of weak classifiers .", "label": "", "metadata": {}, "score": "56.74194"}
{"text": "103 - 111 , Springer , New York , NY , USA , 2002 .View at Google Scholar Affiliated with .Affiliated with .Affiliated with .Abstract .Introduction .In the classification of Mass Spectrometry ( MS ) proteomics data , peak detection , feature selection , and learning classifiers are critical to classification accuracy .", "label": "", "metadata": {}, "score": "56.810898"}
{"text": "[ Google Scholar ] .Espana - Boquera , S. ; Castro - Bleda , M.J. ; Gorbe - Moya , J. ; Zamora - Martinez , F. Improving offline handwritten text recognition with hybrid HMM / ANN models .IEEE Trans .", "label": "", "metadata": {}, "score": "56.81176"}
{"text": "We now determine the probability distribution for the sex of the sample . where and are the parameters of normal distribution which have been previously determined from the training set .Note that a value greater than 1 is OK here - it is a probability density rather than a probability , because height is a continuous variable .", "label": "", "metadata": {}, "score": "56.92531"}
{"text": "237 - 285 , 1996 .View at Google Scholar \u00b7 View at Scopus .K. J. Kirchberg , O. Jesorsky , and R. W. Frischholz , \" Genetic model optimization for hausdorff distance - based face localization , \" in Biometric Authentication , vol .", "label": "", "metadata": {}, "score": "56.983406"}
{"text": "44 - 51 .Huynh , T. ; Schiele , B. Towards Less Supervision in Activity Recognition from Wearable Sensors .Proceedings of the 2006 10th IEEE International Symposium on Wearable Computers , Montreux , Switzerland , 11 - 14 October 2006 ; pp .", "label": "", "metadata": {}, "score": "57.019684"}
{"text": "RotBoost offers a potential computational advantage over AdaBoost in that it has the ability to execute in parallel .In fact , each subensemble classifier formed by AdaBoost can be learned independently of the other ones .Pseudocode 1 illustrates this algorithm . . .", "label": "", "metadata": {}, "score": "57.041325"}
{"text": "Hilario M , et al .: Processing and classification of protein mass spectra .Mass Spectrom Rev 2006 , 25 : 409 - 449 .View Article PubMed .Shin H , Markey M : A machine learning perspective on the development of clinical decision support systems utilizing mass spectra of blood samples .", "label": "", "metadata": {}, "score": "57.069244"}
{"text": "View at Scopus .B. Jun and D. Kim , \" Robust face detection using local gradient patterns and evidence accumulation , \" Pattern Recognition , vol .45 , no . 9 , pp .3304 - 3316 , 2012 .", "label": "", "metadata": {}, "score": "57.110695"}
{"text": "41 - 47 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. I. Su , J. B. Welsh , L. M. Sapinoso et al . , \" Molecular classification of human carcinomas by use of gene expression signatures , \" Cancer Research , vol .", "label": "", "metadata": {}, "score": "57.18661"}
{"text": "C . )Hence , the classifier is defined as .a .r .g . m . a .x .c .C .p .C .c . )i .n .p .X .", "label": "", "metadata": {}, "score": "57.24798"}
{"text": "Bagging also presents the second best diversity but low classification accuracy .Table 5 : Kappa error diagram for Lung dataset ( the centroids of ensembles ) .When outlining the kappa - error diagrams , the diagrams of different ensembles are greatly overlapping , and the distances between the centroids are small .", "label": "", "metadata": {}, "score": "57.279335"}
{"text": "Coombes K , et al .: Improved peak detection and quantification of mass spectrometry data acquired from surface - enhanced laser desorption and ionization by denoising spectra with the undecimated discrete wavelet transform .Proteomics 2005 , 5 ( 16 ) : 4107 - 4117 .", "label": "", "metadata": {}, "score": "57.386074"}
{"text": "BMC Bioinformatics 2006 , 7 : 3 .View Article PubMed .Rifkin R , Mukherjee S , Tamayo P , Ramaswamy S , Yeang CH , Angelo M , Reich M , Poggio T , Lander ES , Golub TR , Mesirov JP : An analytical method for multi - class molecular cancer classification .", "label": "", "metadata": {}, "score": "57.40104"}
{"text": "In practical applications , the decision threshold adjustment function should be subtly designed by considering real distribution of instances .Table 3 : Accuracy of various classification methods on eight datasets , where bold represents the best result , underline denotes the second best , and italic labels the worst one in each column , respectively .", "label": "", "metadata": {}, "score": "57.411522"}
{"text": "337 - 407 , 2000 .View at Google Scholar \u00b7 View at Scopus .L. P. Kaelbling , M. L. Littman , and A. W. Moore , \" Reinforcement learning : a survey , \" Journal of Artificial Intelligence Research , vol .", "label": "", "metadata": {}, "score": "57.46425"}
{"text": "We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis - aligned rectangles of constant dimension over discrete domains . \" ...Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers .", "label": "", "metadata": {}, "score": "57.54715"}
{"text": "Preprocessing MALDI - MS data .Mass spectrum data has high dimensionality within a small sample size .Both chemical and electrical noises are involved in the signal , and the redundancy of the spectra , different reference points , and unaligned feature points increase the computational intensity and decrease the classification accuracy .", "label": "", "metadata": {}, "score": "57.62526"}
{"text": "Support vector machine classifiers .Extensive applications literature in text categorization , image recognition and other fields also shows the excellent empirical performance of this classifier in many more domains .The underlying idea of SVM classifiers is to calculate a maximal margin hyperplane separating two classes of the data .", "label": "", "metadata": {}, "score": "57.66487"}
{"text": "In step 1 , each classifier is trained by RPPA and sensitivity of corresponding drug .In step 2 and 3 , patient 's RPPA is tested in each classifier , and the sensitivity of each drug is predicted .As a final step , only the drugs predicted to have low sensitivity are recommended to the patient .", "label": "", "metadata": {}, "score": "57.76966"}
{"text": "Distance metric learning .Depending on the availability of training examples , the algorithms of distance metric learning can be divided into two categories : supervised distance metric learning and unsupervised distance metric learning .With the given class labels for training samples , supervised distance metric learning can be divided into global distance metric learning and local distance metric learning .", "label": "", "metadata": {}, "score": "57.796078"}
{"text": "View Article PubMed .Guyon I , Weston J , Barnhill S , Vapnik VN : Gene selection for cancer classification using support vector machines .Machine Learning 2002 , 46 ( 1 - 3 ) : 389 - 422 .View Article .", "label": "", "metadata": {}, "score": "57.95508"}
{"text": "Affiliated with .Abstract .Background .Cancer diagnosis and clinical outcome prediction are among the most important emerging applications of gene expression microarray technology with several molecular signatures on their way toward clinical deployment .Use of the most accurate classification algorithms available for microarray gene expression data is a critical ingredient in order to develop the best possible molecular signatures for patient care .", "label": "", "metadata": {}, "score": "58.154564"}
{"text": "Introduction .In these applications , detecting objects in image frames is a crucial step in developing practical systems .One commonly employed method among most object detection techniques is learning based detection approach .A typical implementation of learning based object detection methods involves a pretrained detection window capable of detecting the presence of any target object within its frame as it sweeps over the entire image .", "label": "", "metadata": {}, "score": "58.16359"}
{"text": "Baseline correction and normalization .Chemical contamination introduces the baseline effect and changes the true protein distribution .To minimize chemical noise , the baseline is subtracted from the spectrum .To obtain the baseline , the local minima are computed by assigning a shifting window size of 30 and a step size of 30 .", "label": "", "metadata": {}, "score": "58.243683"}
{"text": "R News 2002 , 2 : 18 - 22 .Guyon I , Weston J , Barnhill S , Vapnik V : Gene selection for cancer classification using support vector machines .Machine Learning 2002 , 46 : 389 - 422 .", "label": "", "metadata": {}, "score": "58.262173"}
{"text": "These additional edges are called augmented edge .The idea is that if a strong dependency between X 1 and X 2 exists , the directed edge is added between X 1 and X 2 ( Figure 2(B ) ) .The maximum number of edges added to relax the independent assumption between variables is n - 1 , but the augmented edges of TAN are limited to construct tree - like Bayesian network .", "label": "", "metadata": {}, "score": "58.375046"}
{"text": "r .g . m .i .n .In the linear case , function .can be represented as . , where . denote an inner product , and the equation becomes . a .r .g . m .", "label": "", "metadata": {}, "score": "58.38718"}
{"text": "Then , RotBoost was employed on the selected genes .Here , we experimented with 2 different transformation matrixes , that is , PCA and ICA towards RotBoost implementation .To assess the efficiency of RotBoost algorithm different ensemble / nonensemble techniques including Rotation Forest , AdaBoost , Bagging single tree , and SVMs were also deployed .", "label": "", "metadata": {}, "score": "58.533195"}
{"text": "In stage 4 and stage 5 , it became difficult to fill the required number of negative samples because most of the negative samples were removed in the previous stages .Therefore , only a small number of negative samples from the set originally numbered in billions were available for the training .", "label": "", "metadata": {}, "score": "58.579636"}
{"text": "Each edge between attributes is evaluated by a modified CR .We call the proposed score function Local Classification Rate ( LCR ) as the score measures how each augmented edge is likely to contribute the increase of classification rate when only the edge is added in NBC .", "label": "", "metadata": {}, "score": "58.62201"}
{"text": "Experiments .Description .We compare our algorithm to both Pegasos [ 4 ] and Norma [ 3 ] , implemented on MATLAB for both the linear and the kernel - based case .The experiments are being run on AMD Phenom X4", "label": "", "metadata": {}, "score": "58.62867"}
{"text": "x .i .C .c . )So , in NBC , it is assumed that each protein is conditionally independent to other protein and dependent to only the drug sensitivity .However , this assumption is unrealistic since the selected proteins of RPPA could have the biological interactions in the signaling pathway affecting the efficacy of the drug .", "label": "", "metadata": {}, "score": "58.654778"}
{"text": "In ECOC , hamming distance is applied as decoding strategy .In particular , when using an exhaustive code to construct the code matrix of ECOC , it can generate more binary classifiers than OAA and OAO .The size of ECOC is .", "label": "", "metadata": {}, "score": "58.663795"}
{"text": "After smoothing , the baseline is subtracted from all spectra .To compare sample spectra , we need to normalize the spectra using its total ion current to represent the data in a systematic scale .Peak detection and qualification .The final feature acquisition of MS data is to obtain the peak position and its magnitude .", "label": "", "metadata": {}, "score": "58.666252"}
{"text": "Subsequently , in the second step , the result of the first step is compared to the mean intensity and the intensity of each location in the kernel .In the third step , the intensity values are converted into a binary string whose values are equal to 1 if the intensity of the pixel is greater than the mean intensity and is equal to 0 otherwise .", "label": "", "metadata": {}, "score": "58.71953"}
{"text": "Using Bayes ' theorem , the conditional probability can be decomposed as .In practice , there is interest only in the numerator of that fraction , because the denominator does not depend on and the values of the features are given , so that the denominator is effectively constant .", "label": "", "metadata": {}, "score": "58.73751"}
{"text": "Figure 9 shows a comparison of learning algorithms composed of the proposed algorithm and the conventional algorithm used in [ 17 - 19 , 25 ] .The number of combined pixels for the strong classifier was 320 .Red lines and blue lines represent the results on face DB and license plate DB , respectively .", "label": "", "metadata": {}, "score": "58.767677"}
{"text": "and the results for which a significant difference with RotBoost was found are marked with a bullet or an open circle next to them .A bullet next to a result indicates that RotBoost is significantly better than the corresponding method .", "label": "", "metadata": {}, "score": "58.85658"}
{"text": "y .t .x .t .p .x .t .p .y .t . )The state priors p ( y t ) can be estimated from the relative frequencies of each state from the training data .", "label": "", "metadata": {}, "score": "58.891197"}
{"text": "Petricoin E , et al .: Use of proteomics patterns in serum to identify ovarian cancer .The Lancet 2002 , 359 : 572 - 577 .View Article .Coombes K , et al .: Pre - processing mass spectrometry data .", "label": "", "metadata": {}, "score": "58.965134"}
{"text": "12953_2012_357_MOESM1_ESM.pdf Additional file 1 : Accuracy of sensitivity prediction for 24 drugs with 10 and 30 selected features .The file includes two tables for classification accuracy in 10 and 30 selected features .( PDF 131 KB ) .Competing interests .", "label": "", "metadata": {}, "score": "58.986706"}
{"text": "The novelty we claim here is that we introduced a process of selecting the weak classifier minimizing the cost function .In particular , we derived a calculation of the reinforced predictions based on confidence level in a lookup table to determine the classification results for LPR based image classifier .", "label": "", "metadata": {}, "score": "59.06093"}
{"text": "99 , no .10 , pp .6562 - 6566 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Affiliated with .Affiliated with .Abstract .Background .The goal of personalized medicine is to provide patients optimal drug screening and treatment based on individual genomic or proteomic profiles .", "label": "", "metadata": {}, "score": "59.27717"}
{"text": "20 , pp .7388 - 7393 , 2001 .View at Google Scholar \u00b7 View at Scopus .J. Khan , J. S. Wei , M. Ringn\u00e9r , et al . , \" Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks , \" Nature Medicine , vol .", "label": "", "metadata": {}, "score": "59.33322"}
{"text": "60 - 66 , Vietri sul Mare , Italy , 2008 .K.-J. Kim and S.-B. Cho , \" An evolutionary algorithm approach to optimal ensemble classifiers for DNA microarray data analysis , \" IEEE Transactions on Evolutionary Computation , vol .", "label": "", "metadata": {}, "score": "59.56138"}
{"text": "Each point represents a data set ( 24 drugs ) where the y and x coordinate of a point is the accuracy rate according to ANBC and counterpart respectively .The red points above the diagonal line represent the drug whose sensitivity is predicted better in ANBC ( vertical axis ) than counterpart ( horizontal axis ) .", "label": "", "metadata": {}, "score": "59.858665"}
{"text": "e continuous w.r.t . \u03bd . 2 During the course of the proof , we will need several capacity concepts of function sets . by Yaakov Engel , Shie Mannor , Ron Meir - IEEE Transactions on Signal Processing , 2003 . \" ...", "label": "", "metadata": {}, "score": "59.91149"}
{"text": "19 - 37 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C.-H. Yeang , S. Ramaswamy , P. Tamayo et al . , \" Molecular classification of multiple tumor types , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "59.93499"}
{"text": "Figure 11(a ) shows the results of the face detectors for the MIT + CMU database .Both the proposed and conventional methods produced the detection rates over 80 % while maintaining less than 10 false positives , but the proposed method showed much improved performance .", "label": "", "metadata": {}, "score": "59.952602"}
{"text": "View Article PubMed .Lee JW , Lee JB , Park M , Song SH : An extensive comparison of recent classification tools applied to microarray data .Computational Statistics & Data Analysis 2005 , 48 : 869 - 885 .View Article .", "label": "", "metadata": {}, "score": "60.168278"}
{"text": "J. Yoon and D. Kim , \" Frontal face classifier using AdaBoost with MCT features , \" in Proceedings of the International Conference on Control , Automation , Robotics and Vision ( ICARCV ' 10 ) , pp .2084 - 2087 , Singapore , December 2010 .", "label": "", "metadata": {}, "score": "60.232834"}
{"text": "Our future work will consider the extension of correction strategies and classification approaches to deal with this problem and will also explore some efficient solutions with several other coding strategies .Acknowledgments .This work is partially supported by the National Natural Science Foundation of China under Grant no .", "label": "", "metadata": {}, "score": "60.265984"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the margin : a new explanation for the effectiveness of voting methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "60.30873"}
{"text": "1373 - 1390 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Cho and H. Won , \" Machine learning in DNA microarray analysis for cancer classification , \" in Proceedings of the 1st Asia - Pacific Bioinformatics Conference on Bioinformatics , pp .", "label": "", "metadata": {}, "score": "60.548702"}
{"text": "6 , pp .673 - 679 , 2001 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Bhaskar , D. C. Hoyle , and S. Singh , \" Machine learning in bioinformatics : a brief survey and recommendations for practitioners , \" Computers in Biology and Medicine , vol .", "label": "", "metadata": {}, "score": "60.571785"}
{"text": "This article has been published as part of BMC Genomics Volume 10 Supplement 1 , 2009 : The 2008 International Conference on Bioinformatics & Computational Biology ( BIOCOMP'08 ) .Competing interests .The authors declare that they have no competing interests .", "label": "", "metadata": {}, "score": "60.60863"}
{"text": "Spectra re - sampling and wavelet de - noising .Mass spectrum data presents in a discrete format along intervals that are not equal in the whole spectrum .For high - resolution data , the high - frequency noise and redundant data points harm the quality of the dataset .", "label": "", "metadata": {}, "score": "60.612457"}
{"text": "Support Vector Machine Recursive Feature Elimination ( SVMRFE ) [ 15 ] is a very popular method for feature selection based on the backward feature elimination that recursively removes the least ranking feature .Originally proposed for microarray data analysis , it has been widely used for feature selection in different areas including MS data analysis [ 16 ] .", "label": "", "metadata": {}, "score": "60.64422"}
{"text": "t .n .y .t .i . )i .n . )i .n . )This is given by a conditional probability table A , where individual transition probabilities are denoted as follows : . p .", "label": "", "metadata": {}, "score": "60.652367"}
{"text": "The process of convergence is illustrated in Figure 2 for the case of two - dimensional dataset , where the data is classified according to whether it is inside unit circle about origin .The parameters used for this illustration , .", "label": "", "metadata": {}, "score": "60.723938"}
{"text": "Sensors that need to be worn on the body may be considered intrusive by the user , and sensors that are easy to install can increase the acceptance of the system .The WSNs deployed in our different home environments were focused to measure equivalent things : passive infrared sensors to detect motion in a specific area ; reed switches for open / close states of doors and cupboards , and float sensors to measure the toilet being flushed .", "label": "", "metadata": {}, "score": "60.879684"}
{"text": "MCT may be an example , which is defined as an order set of binary comparison of pixel intensities between neighboring pixels in the kernel and their mean intensity .MCT at a specific point . square - shaped kernel from a license plate image .", "label": "", "metadata": {}, "score": "60.932404"}
{"text": "proposed a Tree - Augmented Naive Bayesian classifier ( TAN ) by adding edges into the structure of NBC .Augmented edges in TAN are restricted to tree structure and learning structure algorithm is based on the conditional mutual information between two variables given a class variable .", "label": "", "metadata": {}, "score": "61.032852"}
{"text": "Worth mentioning is that , although LMNN has the best performance in testing the ovarian cancer data set , it did not fare well on the breast cancer and liver disease data sets , given the average from the feature dimension from 5 to 100 .", "label": "", "metadata": {}, "score": "61.104088"}
{"text": "Authors ' contributions .Dong - Chul Kim and Jean Gao contribute the computational algorithm design and the manuscript writing .Xiaoyu Wang carried out the biological experiment for the RPPM data generation .Chin - Rang Yang was responsible for the overall project layout and direction .", "label": "", "metadata": {}, "score": "61.16889"}
{"text": "Blood 2009 , 113 : 154 - 164 .10.1182/blood-2007 - 10 - 119438 PubMed Central PubMed View Article .Cain JW , Hauptschein RS , Stewart JK , Bagci T , Sahagian GG , Jay DG : Identification of CD44 as a Surface Biomarker for Drug Resistance by Surface Proteome Signature Technology .", "label": "", "metadata": {}, "score": "61.18954"}
{"text": "151 - 158 , Stockholm , Sweden , May 1994 .B. Fr\u00f6ba and A. Ernst , \" Face detection with the modified census transform , \" in Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition ( FGR ' 04 ) , pp .", "label": "", "metadata": {}, "score": "61.198853"}
{"text": "[ Google Scholar ] .Milenkovic , A. ; Otto , C. ; Jovanov , E. Wireless sensor networks for personal health monitoring : Issues and an implementation .Comput .Commun .[ Google Scholar ] .Garc\u00eda - V\u00e1zquez , J.P. ; Rodr\u00edguez , M.D. ; Andrade , A.G. ; Bravo , J. Supporting the strategies to improve elders ' medication compliance by providing ambient aids .", "label": "", "metadata": {}, "score": "61.280655"}
{"text": "Cancer 2004 , 100 : 1814 - 1822 .View Article PubMed .Ressom HW , Varghese RS , Drake SK , Hortin GL , Abdel - Hamid M , Loffredo CA , Goldman R : Peak selection from MALDI - TOF mass spectra using ant colony optimization .", "label": "", "metadata": {}, "score": "61.334904"}
{"text": "After the conversion , the MCT with a . square - shaped kernel can have integer feature values ranging from 0 to 510 .Between MCT and other LPRs such as LBP , LGP , and LSP , their differences depend only on the type of kernel used and the attributes compared .", "label": "", "metadata": {}, "score": "61.36626"}
{"text": "Proceedings of 7th European Symposium on Artificial Neural Networks ( ESANN 1999 ) , Bruges , Belgium , 21 - 23 April 1999 ; pp .455 - 462 .Bengio , Y. A connectionist approach to speech recognition .Int .", "label": "", "metadata": {}, "score": "61.56311"}
{"text": "329 - 334 .[ Google Scholar ] .Cook , D.J. ; Schmitter - Edgecombe , M. Assessing the quality of activities in a smart environment .Methods Inf .Med .[ Google Scholar ] .de Ipi\u00f1a , D.L. ; de Sarralde , I.D. ; Garc\u00eda - Zubia , J. An ambient assisted living platform integrating RFID Data - on - Tag care annotations and twitter .", "label": "", "metadata": {}, "score": "61.623837"}
{"text": "In the activity recognition domain , hybrid approaches have been also successfully employed .In our recent work [ 30 ] , we showed that an ANN could be hybridized with HMMs to deal with the activity recognition problem in a home setting .", "label": "", "metadata": {}, "score": "61.642014"}
{"text": "Bioinformatics 2000 , 16 : 906 - 914 .View Article PubMed .Du P , Kibbe WA , Lin SM : Improved peak detection in mass spectrum by incorporating continuous wavelet transform - based pattern matching .Bioinformatics 2006 , 22 : 2059 - 2065 .", "label": "", "metadata": {}, "score": "61.647385"}
{"text": "The proteomic profiling is implemented by measuring the expression level of selected proteins which could be related to signaling pathways of the target cancer .To quantitatively measure the systemic responses of proteins in pathways , RPPA is used in conjunction with the quantum dots ( Qdot ) nano - technology .", "label": "", "metadata": {}, "score": "61.65516"}
{"text": "Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers .Such combinations could be implemented by neural networks or by voting methods of combining the classifiers , such as boosting and bagging .", "label": "", "metadata": {}, "score": "61.72518"}
{"text": "Over all , ANBC outperformed support vector machine classification with three Different kernels , logistic regression , and random forest algorithm in all feature sets ( 10 , 20 , and 30 features ) .ANBC outperforms NBC in 10 and 20 selected features but not 30 features .", "label": "", "metadata": {}, "score": "61.837807"}
{"text": "Our Kernel - RLS ( KRLS ) algorithm performs linear regression in the feature space induced by a Mercer kernel , and can therefore be used to recursively construct the minimum mean squared -error regressor .Spars ... \" .We present a non - linear kernel - based version of the Recursive Least Squares ( RLS ) algorithm .", "label": "", "metadata": {}, "score": "61.905453"}
{"text": "The sensitivity of each drug is measured with 43 cell lines on average .As a preprocessing , the drug sensitivity is discretized into 2 states ( High or Low ) by K - means clustering algorithm in which the maximum and minimum values of drug sensitivity are used for initial centroid .", "label": "", "metadata": {}, "score": "62.022476"}
{"text": "In Section 2 , the framework of RotBoost is described in detail .In Section 3 , the experimental results and corresponding discussions are presented .Section 4 concludes the paper .Materials and Methods .The Description of RotBoost Ensemble Classification .", "label": "", "metadata": {}, "score": "62.05886"}
{"text": "[ Google Scholar ] .Rabiner , L.R. A tutorial on hidden markov models and selected applications in speech recognition .Proc .IEEE 1989 , 77 , 257 - 286 .[ Google Scholar ] .Sch\u00f6lkopf , B. ; Burges , C. ; Smola , A. Advances in Kernel Methods : Support Vector Learning ; MIT Press : Cambridge , MA , USA , 1998 .", "label": "", "metadata": {}, "score": "62.098038"}
{"text": "View Article PubMed .Ling CX , Huang J , Zhang H : AUC : a statistically consistent and more discriminating measure than accuracy .Proceedings of the Eighteenth International Joint Conference of Artificial Intelligence ( IJCAI ) 2003 .Fawcett T : ROC Graphs : Notes and Practical Considerations for Researchers .", "label": "", "metadata": {}, "score": "62.155506"}
{"text": "For 10 or less false positives , the proposed method exhibited detection rate at 98.8 % while the conventional method was at 98.2 % .From Figure 11(c ) , the detection rate of the license plate detectors with 5 false positives was 99.7 % for the proposed method while that of the conventional method was 98.6 % .", "label": "", "metadata": {}, "score": "62.226383"}
{"text": ":A data - analytic strategy for protein biomarker discovery : profiling of high - dimensional proteomic data for cancer detection .Biostatistics 2003 , 4 : 449 - 463 .View Article PubMed .Mantini D , et al .: LIMPIC : a computational method for the separation of protein MALDI - TOF - MS signals from noise .", "label": "", "metadata": {}, "score": "62.49287"}
{"text": "Figure 10 shows the cascade structure of the detector .The number in parentheses refers to the maximum number of combined pixel locations .At the earlier stage of the detector most of the search area is examined coarsely but quickly using the selected small number of combined pixel classifiers .", "label": "", "metadata": {}, "score": "62.634426"}
{"text": "Ubiquitous Comput .[ Google Scholar ] .Jafari , R. ; Encarnacao , A. ; Zahoory , A. ; Dabiri , F. ; Noshadi , H. ; Sarrafzadeh , M. Wireless Sensor Networks for Health Monitoring .Proceedings of the Second Annual International Conference on the Mobile and Ubiquitous Systems : Networking and Services ( MobiQuitous 2005 ) , San Diego , CA , USA , 17 - 21 July 2005 ; pp .", "label": "", "metadata": {}, "score": "62.65172"}
{"text": "The total number of rounds for Adaboost training was 1000 .Figure 7 shows the results of training error .Red lines represent the results on face DB , and blue lines represent the results on license plate DB .Dotted lines and solid lines represent the results when the numbers of combined pixels are 20 and 40 , respectively .", "label": "", "metadata": {}, "score": "62.777985"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. Ahonen , A. Hadid , and M. Pietik\u00e4inen , \" Face description with local binary patterns : application to face recognition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "62.913795"}
{"text": "Figure 11 shows the ROCs of the face and license plate detectors for the proposed and conventional learning algorithms .The horizontal axis represents false positives representing the number of mistaken results in all tested images .The vertical axis represents detection rate calculated by the number of detected target objects divided by the total number of target objects in all the tested images .", "label": "", "metadata": {}, "score": "62.945686"}
{"text": "( A )In NBC , all the attributes are conditionally independent given the class variable .( B )In ANBC , each attribute have at most one other attribute as an additional parent but augmented edges of ANBC are not necessary to constitute the tree structure which means that any attribute can have only class variable as a single parent .", "label": "", "metadata": {}, "score": "62.987484"}
{"text": "The test was applied with 100,000 permutations and two - sided p - values were computed as described in [ 29 ] .Declarations .Acknowledgements .The work was in part supported by grant 2R56LM007948 - 04A1 .Electronic supplementary material .", "label": "", "metadata": {}, "score": "63.325893"}
{"text": "Journal of the American Statistical Association 2002 , 97 : 77 - 88 .View Article .Dupuy A , Simon RM :Critical review of published microarray studies for cancer outcome and guidelines on statistical analysis and reporting .J Natl Cancer Inst 2007 , 99 : 147 - 157 .", "label": "", "metadata": {}, "score": "63.389286"}
{"text": "To find discriminative structure , we propose a new method based on local classification rate ( LCR ) to score augmented edges and greedy search algorithm to find the ANBC structure that has the highest classification rate .In the experiments , the proposed ANBC for personalized medicine is compared to state - of - the - art classifiers including NBC and TAN in lung cancer data .", "label": "", "metadata": {}, "score": "63.602867"}
{"text": "We propose a new score function LCR for learning discriminative structure of Bayesian network classifier .All augmented edges are scored by LCR that is based on the difference between CR before and after a single edge is augmented .In other words , the score represents how the edge augmented in NBC is likely to increase the classification rate in ANBC .", "label": "", "metadata": {}, "score": "63.60901"}
{"text": "Cross - Validation Test .To evaluate the accuracy performance of the classifier , a 5-fold cross - validation test in terms of error rate was performed with 100,000 positive samples ( faces , license plates ) and 150,000 negative samples ( backgrounds ) .", "label": "", "metadata": {}, "score": "63.615135"}
{"text": "All authors have read and approved the final manuscript .Authors ' Affiliations .Department of Computer Science , New Mexico Tech .Institute for Complex Additive Systems Analysis , New Mexico Tech .Biostatistics Epidemiology Research Design Core , Center for Clinical and Translational Sciences , The University of Texas Health Science Center at Houston .", "label": "", "metadata": {}, "score": "63.635525"}
{"text": "Department of Computer Science and Engineering , The University of Texas at Arlington .Simmons Comprehensive Cancer Center , The University of Texas Southwestern Medical Center .References .Wistuba II , Gelovani JG , Jacoby JJ , Davis SE , Herbst RS : Methodological and practical challenges for personalized cancer therapies .", "label": "", "metadata": {}, "score": "63.644333"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .V. N. Vapnik , The Nature of Statistical Learning Theory , Springer , New York , NY , USA , 1995 .S. Boyd and L. Vandenberghe , Convex Optimization , Cambridge University Press , New York , NY , USA , 2004 .", "label": "", "metadata": {}, "score": "63.685337"}
{"text": "A .N .B .C .i .j .x . m .x .n . m . )For ANBC , the number of all possible augmented edges are ( n - 1 ) ( n - 2 ) .", "label": "", "metadata": {}, "score": "63.702152"}
{"text": "MCR-09 - 0237 PubMed Central PubMed View Article .Liotta LA , Espina V , Mehta AI , Calvert V , Rosenblatt K , Geho D , Munson PJ , Young L , Wulfkuhle J , Petri - coin EF III : Protein microarrays : Meeting analytical challenges for clinical applications .", "label": "", "metadata": {}, "score": "63.765392"}
{"text": "Mach .Intell .[ Google Scholar ] .Bourlard , H.A. ; Morgan , N. Connectionist Speech Recognition : A Hybrid Approach ; Kluwer Academic Publishers : Norwell , MA , USA , 1993 .[ Google Scholar ] .Brush , A. ; Krumm , J. ; Scott , J. Activity Recognition Research : The Good , the Bad , and the Future .", "label": "", "metadata": {}, "score": "64.271515"}
{"text": "1 , pp .47 - 56 , September 2006 .N. Oza and S. Russell , \" Online bagging and boosting , \" in Artificial Intelligence and Statistics Pages , pp .105 - 112 , Morgan Kaufmann , Boston , Mass , USA , 2001 .", "label": "", "metadata": {}, "score": "64.68405"}
{"text": "Figure 8 : ( a )Testing error according to the number of pixel locations on face DB .( b )Testing error according to the number of pixel locations on license plate ( LP ) DB .Table 1 shows the execution time of classification and the number of misclassified samples for 200,000 test samples on a MATLAB platform .", "label": "", "metadata": {}, "score": "64.69266"}
{"text": "m .c . m . where .m . is the class label predicted by .B .N .C .x . m .x .n . m . ) and c m is the correct class label ( the state of the class variable C of the m th instance ) .", "label": "", "metadata": {}, "score": "64.69542"}
{"text": "6 , pp .673 - 679 , 2001 .View at Google Scholar . A. Bhattacharjee , W. G. Richards , J. Staunton et al . , \" Classification of human lung carcinomas by mRNA expression profiling reveals distinct adenocarcinoma subclasses , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "64.8153"}
{"text": "C. Zhang and J. Zhang [ 19 ] proposed a novel ensemble classifier generation method RotBoost through combining Rotation Forest and AdaBoost .In this new ensemble method , the base classifier in Rotation Forest algorithm is replaced with AdaBoost .The experimental results show that RotBoost performs better than either Rotation Forest or AdaBoost when using some non - microarray gene - related data sets from the UCI repository .", "label": "", "metadata": {}, "score": "64.926605"}
{"text": "Ubiquitous Comput .[ Google Scholar ] .Turaga , P. ; Chellappa , R. ; Subrahmanian , V.S. ; Udrea , O. Machine recognition of human activities : A survey .IEEE Trans .Circuit .Syst .Video Technol .", "label": "", "metadata": {}, "score": "65.171074"}
{"text": "To generate the ROC curve , the experiment was repeated by adjusting the sensitivity parameter .Figure 11 : ( a ) Performance of face detector on MIT - CMU DB .( b ) Performance of face detector on BioID DB .", "label": "", "metadata": {}, "score": "65.20115"}
{"text": "i .X .i . )Bayesian networks classifier .Bayesian Network Classifier ( BNC ) is a probabilistic classifier based on Bayes ' theorem .Naive Bayes classifier .In Naive Bayes Classifier ( NBC ) , the posterior probability is defined as .", "label": "", "metadata": {}, "score": "65.24118"}
{"text": "Li X , Gentleman R , Lu X , Shi Q , Lglehart JD , Harris L , Miron A : SELDI - TOF mass spectrometry protein data .Bioinformatics and Computational Biology Solutions Using R and Bioconductor Springer 2005 , 91 - 109 .", "label": "", "metadata": {}, "score": "65.3878"}
{"text": "In this paper we proposed a framework for personalized medicine where a patient is profiled by RPPA and drug sensitivity is predicted by ANBC and LCR .Electronic supplementary material .Background .In this paper , we present a framework for personalized cancer medicine with RPPA and drug sensitivity .", "label": "", "metadata": {}, "score": "65.577576"}
{"text": "APBC 2004 , 191 - 200 .Tang EK , Suganthan PN , Yao X : Gene selection algorithms for microarray data based on least squares support vector machine .BMC Bioinformatics 2006 , 7 : 95 .View Article PubMed .", "label": "", "metadata": {}, "score": "65.83397"}
{"text": "1602 - 1607 , 2003 .View at Google Scholar \u00b7 View at Scopus .S. Ghorai , A. Mukherjee , S. Sengupta , and P. K. Dutta , \" Cancer classification from gene expression data by NPPC ensemble , \" IEEE / ACM Transactions on Computational Biology and Bioinformatics , vol . 8 , no . 3 , pp .", "label": "", "metadata": {}, "score": "65.92382"}
{"text": "C . )i . m .p .X .i .X .i .\\ .C .C . where .X .i .\\ .C . denotes the parent set of variable X i except the class variable C .", "label": "", "metadata": {}, "score": "66.04075"}
{"text": "[ Google Scholar ] .Pynoos , J. Neglected Areas in Gerontology : Housing Adaptation .Presentation at the Annual Scientific Meeting of the Gerontological Society of America , Boston , 20 - 24 November 2002 .World Health Organization ( WHO ) .", "label": "", "metadata": {}, "score": "66.07037"}
{"text": "ANBC also has better accuracy than TAN in most of the drugs except four drugs ( Figure 3(g ) ) .Figure 4 shows the accuracy of each classifier using Different feature sets .The performance of each method is similar to Table 2 .", "label": "", "metadata": {}, "score": "66.12344"}
{"text": "In ANBC and NBC , the prediction accuracy slightly increases when they have larger number of features while the performance of TAN and SVM is independent of the number of features .In LR and RF , the accuracy is decreased with more features .", "label": "", "metadata": {}, "score": "66.12542"}
{"text": "^ .i .j .k .N .i .j .k .N .i .j .X .i .i .j . , and .N .i .j .k .r .", "label": "", "metadata": {}, "score": "66.16096"}
{"text": "We employed the state - of - the - art implementation of RF available in the R package randomForest [ 24 ] .This implementation is based on the original Fortran code authored by Leo Breiman , the inventor of RFs .", "label": "", "metadata": {}, "score": "66.1676"}
{"text": "To select proteins ( features ) in NBC , TAN , and ANBC , we used Mutual Information between attribute and class variable .The number of features to be selected is predefined as 10 , 20 , and 30 .Experimental results .", "label": "", "metadata": {}, "score": "66.310265"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. D. Pham , C. Wells , and D. I. Grane , \" Analysis of microarray gene expression data , \" Current Bioinformatics , vol .1 , no . 1 , pp .", "label": "", "metadata": {}, "score": "66.71159"}
{"text": "[ Google Scholar ] .Cohen , W.W. Fast Effective Rule Induction .Proceedings of the Twelfth International Conference on Machine Learning , Tahoe City , CA , USA , 9 - 12 July 1995 ; pp .115 - 123 .", "label": "", "metadata": {}, "score": "66.7605"}
{"text": "Figure 8 shows the results on testing error for face DB and license plate DB , respectively .Through the experimental results , the testing error rate is closed to zero when the number of combined pixels becomes larger at the same round .", "label": "", "metadata": {}, "score": "67.02698"}
{"text": "The primal form of SVM problem is a minimization problem with added regularization term .Formally , for a reproducing kernel Hilbert space .with kernel . , and a set of vectors .with corresponding labels ., find function .", "label": "", "metadata": {}, "score": "67.2679"}
{"text": "Mass spectrum data mining usually contains four steps : preprocessing , feature extraction or peak detection , feature selection and classification .Sometimes preprocessing and peak detection are merged as preprocessing .The main task in preprocessing is to purify the data and systematically represent the data for the following steps .", "label": "", "metadata": {}, "score": "67.32128"}
{"text": "t .i . , indicating whether sensor i fired at least once between time t and time t + \u0394 t , with .x .t .i .In a home setting with N state - change sensors , a binary observation vector .", "label": "", "metadata": {}, "score": "67.63677"}
{"text": "Cosi , P. Hybrid HMM - NN Architectures for Connected Digit Recognition .Proceedings of IEEE -INNS - ENNS International Joint Conference on the Neural Networks , Como , Italy , 24 - 27 July 2000 ; Volume 5 , p. 5085 .", "label": "", "metadata": {}, "score": "67.676636"}
{"text": "The background images were collected by randomly selecting 150,000 frames from an arbitrary image set that does not contain any face or any license plate .The size of a background image is the same as the face images or the license plate images .", "label": "", "metadata": {}, "score": "67.79329"}
{"text": "t .x .t .p .y .t .x .t .with g yt denoting the output representing state yt .The a posteriori probability estimates from the output , .p .y .t .", "label": "", "metadata": {}, "score": "67.959854"}
{"text": "For the parameter estimation , only maximum likelihood parameters are used for NBC , TAN , and ANBC since we only compare the structure leaning methods rather than discriminative parameter learning methods .To avoid zero conditional probability in logarithm of likelihood when we calculate the joint probability , we set .", "label": "", "metadata": {}, "score": "68.13428"}
{"text": ", 14 : .Copyright .\u00a9 Kim et al ; licensee BioMed Central Ltd. 2012 .This article is published under license to BioMed Central Ltd. Reinforced AdaBoost Learning for Object Detection with Local Pattern Representations .Received 21 August 2013 ; Accepted 29 September 2013 .", "label": "", "metadata": {}, "score": "68.34874"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. L. Yu , J. Ni , and J. Zhao , \" ACOSampling : an ant colony optimization - based undersampling method for classifying imbalanced DNA microarray data , \" Neurocomputing , vol .", "label": "", "metadata": {}, "score": "68.65985"}
{"text": "10.1016/S1535 - 6108(03)00086 - 2 PubMed View Article .Spurrier B , Ramalingam S , Nishizuka S : Reverse - phase protein lysate microarrays for cell signaling analysis .Nature Protocols 2008 , 3 ( 11):1796 - 1808 .10.1038/nprot.2008.179 PubMed View Article .", "label": "", "metadata": {}, "score": "68.74535"}
{"text": "All license plates were cropped manually and scaled and aligned to .pixel frame .As in the case with facial images , a total of 100,000 training license plate images were generated by randomly translating and rotating each selected frame within 5 pixels and 10 degrees , respectively .", "label": "", "metadata": {}, "score": "68.905106"}
{"text": "For the training error rate as shown in Figure 9(a ) , the proposed algorithm has better performance since its training error rate converges to zero quicker with both databases .According to the results , the training error rate for the face DB converges to zero with the proposed algorithm at round 31 and the conventional algorithm at round 48 .", "label": "", "metadata": {}, "score": "68.98449"}
{"text": "Ambient Assisted Living and Home Care - 4th International Workshop , WAAL 2012 , Vitoria - Gasteiz , Spain , 3 - 5 December 2012 ; pp .98 - 105 .Lester , J. ; Choudhury , T. ; Borriello , G. A Practical Approach to Recognizing Physical Activities .", "label": "", "metadata": {}, "score": "69.16245"}
{"text": "Acknowledgments .References .Zola , I.K. Living at Home : The Convergence of Aging and Disability .In Staying Put - Adapting the Places Instead of the People ; Lanspery , S. , Hyde , J. , Eds . ; Baywood Publishing : Amityville , NY , USA , 1997 ; pp .", "label": "", "metadata": {}, "score": "69.23493"}
{"text": "Because variable X i can have only a single X j as a parent except class variable , only the variable that maximizes .L .C .R .X .i .X .i .\\ .C . is selected as the parent of X i .", "label": "", "metadata": {}, "score": "69.50791"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Saeed , A. Al - Hamadi , and M. Heuer , \" Speaker tracking using multi - modal fusion framework , \" in Proceedings of the International conference on Image and Signal Processing ( ICISP ' 12 ) , pp .", "label": "", "metadata": {}, "score": "69.62474"}
{"text": "1102 - 1105 , Hong Kong , China , August 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Zabih and J. Woodfill , \" Non - parametric local transforms for computing visual correspondence , \" in Proceedings of the European Conference on Computer Vision ( ECCV ' 94 ) , vol .", "label": "", "metadata": {}, "score": "69.84408"}
{"text": "As the burden of healthcare on society increases , the need for finding more effective ways of providing care and support to the disabled and elderly at home becomes more predominant .Monitoring human activities of daily living ( ADL ) , in order to assess the cognitive and physical wellbeing of elderly , is considered a main aspect in building intelligent and pervasive environments [ 6 ] .", "label": "", "metadata": {}, "score": "70.08317"}
{"text": "All training face images were scaled and aligned to .pixels .From each selected frame , additional training frames were generated by randomly translating the frame within 8 pixels and rotating within 10 degrees to a total of 100,000 .For the license plate training images , a digital video camera captured images from a static location on a street and a parking lot .", "label": "", "metadata": {}, "score": "70.1246"}
{"text": "i . , and X i takes the state x ik that is the k th state of .x .i .x .i .r .i . where r i is the number of possible states of X i .", "label": "", "metadata": {}, "score": "70.24557"}
{"text": "The detector consists of a cascade structure of five stages , and the maximum number of pixel locations for each stage is 20 , 40 , 80 , 160 , and 320 , respectively .Test images are classified sequentially from stage 1 to stage 5 .", "label": "", "metadata": {}, "score": "70.56384"}
{"text": "The training error rate for the license plate DB converges to zero with 20 pixels at round 57 and 40 pixels at round 38 .The training error rate of 80 , 160 , or 320 is the same as the training error rate of 40 .", "label": "", "metadata": {}, "score": "70.753845"}
{"text": "Appl .[ Google Scholar ] .Van Kasteren , T. Activity recognition for health monitoring elderly using temporal probabilistic models .Ph.D. thesis , University of Amsterdam , Amsterdam , The Netherlands , 27 April 2011 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "71.373276"}
{"text": "Proceedings of the 8th International Conference on Spoken Language Processing ( ICSLP ) , Jeju Island , Korea , 4 - 8 October 2004 .Ganapathiraju , A. ; Hamaker , J. ; Picone , J. Hybrid SVM / HMM Architectures for Speech Recognition .", "label": "", "metadata": {}, "score": "71.77996"}
{"text": "This interval length is considered long enough to be discriminative and short enough to provide good accuracy labelling results , since with larger time slices the shorter activities would not survive the discretization process .After segmentation , there were a total of 33,120 time slices for \" KasterenA \" dataset , 17,280 time slices for \" KasterenB \" , 24,480 time slices for \" KasterenC \" , 20,160 time slices for \" OrdonezA \" and 30,240 time slices for \" OrdonezB \" dataset .", "label": "", "metadata": {}, "score": "71.96585"}
{"text": "View Article PubMed .Yang C , He Z , Yu W : Comparison of public peak detection algorithms for MALDI mass spectrometry data analysis .BMC Bioinformatics 2009 , 10 : 4 .View Article PubMed .Furey T , et al .", "label": "", "metadata": {}, "score": "72.10976"}
{"text": "Authors ' contributions .Conceived and designed the experiments : AS , LW , CFA .Performed the experiments : AS .Analyzed the results of experiments : AS , LW , CFA .Wrote the paper : AS , CFA .", "label": "", "metadata": {}, "score": "72.25064"}
{"text": "Department of Biology Science , The University of Southern Mississippi .References .Petricoin E , Liotta L : Mass spectrometry - based diagnostic : the upcoming revolution in disease detection .Clin Chem 2003 , 49 : 533 - 534 .", "label": "", "metadata": {}, "score": "72.40936"}
{"text": "C .X .X .n .Z .p .C . )i .n .p .X .i .C . ) where .p .C . )i .n .p .X .", "label": "", "metadata": {}, "score": "72.57774"}
{"text": "[ Google Scholar ] .Bao , L. ; Intille , S.S. Activity Recognition from User - Annotated Acceleration Data .In Pervasive Computing ; Springer : Berlin / Heidelberg , Germany , 2004 ; pp . 1 - 17 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "72.90539"}
{"text": "( PDF 287 KB ) .1471 - 2105 - 9 - 319-S2.pdf Additional file 2 : Simulation experiment demonstrating sensitivity of random forests to input parameters .( PDF 14 KB ) .1471 - 2105 - 9 - 319-S3.pdf Additional file 3 : Complete information about microarray datasets used in the study .", "label": "", "metadata": {}, "score": "73.09594"}
{"text": "Notice that the dataset collection used in this work contains all datasets from the prior comparison [ 5 ] .Breast cancer 5-year metastasis - free survival , metastasis within 5 years , germline BRCA1 mutation .Px - Yeoh .", "label": "", "metadata": {}, "score": "73.17051"}
{"text": "The license plate detector has been tested on 248 real field images that contain 287 visible license plates under various weather conditions .The scale parameter and shifting parameter for both detectors were 1.15 and 0.1 , respectively .In other words , an image pyramid was generated while a test image was downsampled by a factor of 1.15 until the width or height of the test image gets smaller than the size of the detecting window .", "label": "", "metadata": {}, "score": "73.25418"}
{"text": "In the \" Ordonez \" datasets \" Drink \" labels are not present , nevertheless four additional activities are included , namely : \" Lunch \" , \" Snack \" , \" Spare time \" , \" Grooming \" .Table 2 shows the number of separate instances per activity in each dataset .", "label": "", "metadata": {}, "score": "75.585434"}
{"text": "PubMed View Article .Mueller C , Liotta L , Espina V : Reverse phase protein microarrays advance to use in clinical trials .Molecular Oncology 2010 , 4 ( 6):461 - 481 .10.1016/j.molonc.2010.09.003 PubMed Central PubMed View Article .", "label": "", "metadata": {}, "score": "75.99905"}
{"text": "Authors ' Affiliations .Department of Biomedical Informatics , Vanderbilt University .Department of Biostatistics , Vanderbilt University .Department of Cancer Biology , Vanderbilt University .Department of Computer Science , Vanderbilt University .References .Statnikov A , Aliferis CF , Tsamardinos I , Hardin D , Levy S : A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis .", "label": "", "metadata": {}, "score": "76.134674"}
{"text": "This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .Abstract .The gene microarray analysis and classification have demonstrated an effective way for the effective diagnosis of diseases and cancers .", "label": "", "metadata": {}, "score": "76.15578"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. Levi and Y. Weiss , \" Learning object detection from a small number of examples : the importance of good features , \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ' 04 ) , vol .", "label": "", "metadata": {}, "score": "76.29695"}
{"text": "Mach .Learn .[ Google Scholar ] .\u00a9 2013 by the authors ; licensee MDPI , Basel , Switzerland .Naive Bayes has been studied extensively since the 1950s .With appropriate preprocessing , it is competitive in this domain with more advanced methods including support vector machines .", "label": "", "metadata": {}, "score": "76.3019"}
{"text": "The CR is defined as .C .R .S . m .S .I .B .N .C .x . m .x .n . m .c . m .I . m .", "label": "", "metadata": {}, "score": "76.34642"}
{"text": "Experiments .Lung cancer data .In this section , lung cancer data is used to gauge the performance of proposed personalized medicine system with a new score function LCR for learning discriminate structure of ANBC .RPPA for lung cancer consists of 55 antibodies ( Table 1 ) , 75 cell lines .", "label": "", "metadata": {}, "score": "77.464134"}
{"text": "X .i . and q i is the number of possible parent configuration given .X .i .The parameter \u03b8 ijk denotes the probability that the state of X i is x ik given \u03c0 ij as the state of .", "label": "", "metadata": {}, "score": "78.051865"}
{"text": "When compared with Rotation Forest , the statistically significant difference is favorable in 6 datasets , although the Rotation Forest could surpass the RotBoost when working on Breast and Ovarian datasets .Indeed , RotBoost is seen to outperform AdaBoost in most cases even though the advantage of RotBoost is not significant in 1 dataset and tie is occurred on the remaining 2 datasets .", "label": "", "metadata": {}, "score": "78.19126"}
{"text": "The target objects of these tests were defined as faces or license plates .The positive training sets for each stage consisted of 100,000 face images and 130,000 license plate images , respectively .The negative training sets were composed of 150,000 background images for the face detector and 210,000 background images for the license plate detector .", "label": "", "metadata": {}, "score": "78.37172"}
{"text": "In Table 2 , ANBC achieved 100 % accuracy in four drugs , Docetaxel , Gemcitabine , Orexin , and Paclitaxel .Logistic regression shows the lowest accuracy , 64.61 % on average , and SVM with Radial basis function kernel has the lowest accuracy , 17.78 % in Cyclopamine .", "label": "", "metadata": {}, "score": "78.74628"}
{"text": "The diagonal of the matrix contains the true positives ( TP ) , while the sum of a row gives us the total of true labels ( TT ) and the sum of a column gives us the total of inferred labels ( TI ) .", "label": "", "metadata": {}, "score": "78.915054"}
{"text": "Figures 7 and 8 show the comparison of the performances according to the number of pixel locations combined and used in the strong classifier .The numbers of pixels that were compared were 20 , 40 , 80 , 160 , and 320 .", "label": "", "metadata": {}, "score": "78.980865"}
{"text": "Eleventh Conf . on Uncertainty in Artificial Intelligence .Morgan Kaufmann .pp .338 - 345 .Online Boosting Algorithm Based on Two - Phase SVM Training . 1 Department of Information Processing , Tokyo Institute of Technology , Tokyo 152 - 8550 , Japan 2 Imaging Science and Engineering Laboratory , Tokyo Institute of Technology , Tokyo 152 - 8550 , Japan .", "label": "", "metadata": {}, "score": "79.71327"}
{"text": "[ 38 ] for peak selection using ant colony optimization .The spectra were binned with bin size of 100 ppm , and the dimension was reduced from 136,000 m / z values to 23846 m / z bins .Since the two liver diseases have similar symptoms but different treatments , our effort is focused on the classification of these two different diseases , or the identification of HCC and cirrhosis .", "label": "", "metadata": {}, "score": "80.23355"}
{"text": "N .B .C .x . m .x .n . m .c . m . where ANBC ij is a ANBC where the single directed edge from j to i ( E ij ) is augmented in the structure of NBC .", "label": "", "metadata": {}, "score": "80.55165"}
{"text": "[34 ] , eight different ADLs were included as labels , namely : \" Leaving \" , \" Toileting \" , \" Showering \" , \" Sleeping \" , \" Breakfast \" , \" Dinner \" , \" Drink \" .", "label": "", "metadata": {}, "score": "82.06081"}
{"text": "L .C .R .i .j .S . m .S .I .A .N .B .C .i .j .x . m .x .n . m .c . m .", "label": "", "metadata": {}, "score": "82.14922"}
{"text": "For example , a fruit may be considered to be an apple if it is red , round , and about 10 cm in diameter .A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple , regardless of any possible correlations between the color , roundness and diameter features .", "label": "", "metadata": {}, "score": "82.22131"}
{"text": "View Article PubMed .Copyright .\u00a9 Liu et al .2009 .This article is published under license to BioMed Central Ltd. all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .", "label": "", "metadata": {}, "score": "84.88615"}
{"text": "That is , the value is 1 when a sensor state changes from zero to one or vice versa , and 0 otherwise ( see Figure 4(b ) ) .LastSensor : The last sensor representation indicates which sensor fired last .", "label": "", "metadata": {}, "score": "85.59492"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Recognition of Multiple Imbalanced Cancer Types Based on DNA Microarray Data Using Ensemble Classifiers .Received 7 April 2013 ; Revised 8 July 2013 ; Accepted 17 July 2013 .", "label": "", "metadata": {}, "score": "88.03752"}
{"text": "i .j .k .N .i .j .k .N .i .j .k .N .i .j .N .i .j .N .i .j .k .", "label": "", "metadata": {}, "score": "89.82544"}
{"text": "..The covering nu ...An Efficient Ensemble Learning Method for Gene Microarray Classification .Department of Computer Engineering , Islamic Azad University , Dezful Branch , Dezful 313 , Iran .Received 30 April 2013 ; Accepted 12 July 2013 .", "label": "", "metadata": {}, "score": "91.97493"}
{"text": "Department of Health and Human Services , National Human Genome Research Institute , National Institutes of Health ( NIH ) , U.S .Conjugate and Medicinal Chemistry Laboratory , Division of Nuclear Medicine and Molecular Imaging and Center for Advanced Medical Imaging , Department of Radiology , Brigham and Women 's Hospital and Harvard Medical School .", "label": "", "metadata": {}, "score": "97.49783"}
{"text": "429 - 449 , 2002 .View at Google Scholar", "label": "", "metadata": {}, "score": "98.676056"}
{"text": "Fco .Computer Science Department , University Carlos III of Madrid , Legan\u00e9s , Madrid 28911 , Spain .Author to whom correspondence should be addressed ; Tel . : +34 - 916 - 249 - 424 .Received : 27 February 2013 ; in revised form : 18 April 2013 / Accepted : 22 April 2013 / Published : 24 April 2013 .", "label": "", "metadata": {}, "score": "105.36888"}
{"text": "Copyright \u00a9 2012 Vsevolod Yugov and Itsuo Kumazawa .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "110.377174"}
{"text": "Copyright \u00a9 2013 Hualong Yu et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "113.206856"}
{"text": "Copyright \u00a9 2013 Younghyun Lee et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "123.21092"}
