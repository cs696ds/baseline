{"text": ", 2013b ) .The current work is guided by the hypothesis that as sounds evolve in a multi - dimensional feature space , regularities among features are tracked , and deviations from these regularities are \" flagged \" as salient .", "label": "", "metadata": {}, "score": "31.277508"}
{"text": "View at Scopus . A. Batliner , F. Burkhardt , M. van Ballegooy , and E. N\u00f6th , \" A taxonomy of applications that utilize emotional awareness , \" in Proceedings of the 1stInternational Language Technologies Conference ( IS - LTC ' 06 ) , pp .", "label": "", "metadata": {}, "score": "34.63743"}
{"text": "A variety of different computational accounts have been proposed to explain sensitivity to conditional statistical information ( for discussion , see Frank et al . , 2010 ) .The most successful of these models - clustering models - search for and store clusters of statistically coherent elements ( e.g. , Perruchet and Vinter , 1998 ; Orban et al . , 2008 ) .", "label": "", "metadata": {}, "score": "35.451744"}
{"text": "Overview .In Section 2 , we present the database and the annotations performed , as well as the mapping onto main classes used in this paper .In Section 4 , we describe the acoustic and linguistic features used in this study , as well as the classifier chosen for this task .", "label": "", "metadata": {}, "score": "37.401703"}
{"text": "325 - 328 , Philadelphia , Pa , USA , March 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Vlasenko , B. Schuller , A. Wendemuth , and G. Rigoll , \" Combining frame and turn - level information for robust recognition ofemotions within speech , \" in Proceedings of the 8th Annual Conference of the International Speech Communication Association ( Interspeech ' 07 ) , vol .", "label": "", "metadata": {}, "score": "37.754486"}
{"text": "For the highest transparency , we utilise the open source openSMILE feature extraction and choose the basic set used in the only official challenge on emotion recognition from speech to the present day ( cf . \"Classifier Sub - Challenge \" [ 33 ] ) .", "label": "", "metadata": {}, "score": "38.071156"}
{"text": "25 - 36 , 2005 .S. Kopp , T. Stocksmeier , and D. Gibbon , \" Incremental multimodal feedback for conversational agents , \" in Proceedings of the Intelligent Virtual Agents , vol .4722 of Lecture Notes in Computer Science , pp .", "label": "", "metadata": {}, "score": "39.24475"}
{"text": "25 - 36 , 2005 .S. Kopp , T. Stocksmeier , and D. Gibbon , \" Incremental multimodal feedback for conversational agents , \" in Proceedings of the Intelligent Virtual Agents , vol .4722 of Lecture Notes in Computer Science , pp .", "label": "", "metadata": {}, "score": "39.24475"}
{"text": "Earlier work argued a close correspondence between timbre perception and spectro - temporal details of sound events ( Patil et al ., 2012 ) .Here , we follow the same premise and first augment our feature space directly with the channels of the spectrogram .", "label": "", "metadata": {}, "score": "39.3432"}
{"text": "As we consider acoustic and linguistic information , the two information streams need to be integrated .In this respect , all experiments found in the literature use static classification techniques [ 17 , 37 , 49 ] : an early fusion is usually the best choice for preserving all information prior to the final decision .", "label": "", "metadata": {}, "score": "39.51017"}
{"text": "The first row in each expression represents our results ( with SVM classifier ) .The other row shows the results of Lucey et al . , as reported in [ 13 ] .Our proposed neutral - independent approach allows us to have a frame - based decision of the facial expressions .", "label": "", "metadata": {}, "score": "39.544857"}
{"text": "C. A. Thompson , M. H. G\u00f6ker , and P. Langley , \" A personalized system for conversational recommendations , \" Journal of Artificial Intelligence Research , vol .21 , pp .393 - 428 , 2004 .View at Google Scholar \u00b7 View at Scopus . S. Young , M. Ga\u0161i\u0107 , S. Keizer et al . , \" The Hidden Information State model : a practical framework for POMDP - based spoken dialogue management , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "39.86079"}
{"text": "C. A. Thompson , M. H. G\u00f6ker , and P. Langley , \" A personalized system for conversational recommendations , \" Journal of Artificial Intelligence Research , vol .21 , pp .393 - 428 , 2004 .View at Google Scholar \u00b7 View at Scopus . S. Young , M. Ga\u0161i\u0107 , S. Keizer et al . , \" The Hidden Information State model : a practical framework for POMDP - based spoken dialogue management , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "39.86079"}
{"text": "118 - 127 , Springer , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Viola and M. Jones , \" Rapid object detection using a boosted cascade of simple features , \" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp .", "label": "", "metadata": {}, "score": "39.916416"}
{"text": "Borji , A. , Sihite , D. N. , and Itti , L. ( 2013a ) .Quantitative analysis of human - model agreement in visual saliency modeling : a comparative study .IEEE Trans .Image Process .doi : 10.1109/TIP.2012.2210727 .", "label": "", "metadata": {}, "score": "40.023117"}
{"text": "This is motivated by the fact that an ASR engine would also provide this information .Classifiers .Also a selection of ensemble techniques has been applied , as Boosting , Bagging , Multiboosting , and Stacking with and without confidences [ 47 ] .", "label": "", "metadata": {}, "score": "40.287746"}
{"text": "In addition to determine the effect of longer - term temporal modelling afforded by the hidden Markov models , a one - state HMM system , identical in all other ways to the proposed system , was implemented and its performance is also included in Table 9 .", "label": "", "metadata": {}, "score": "40.44963"}
{"text": "Here , we follow a similar experimental approach by probing stimulus - related attentional perception using single sound clips , and asking listeners whether they heard a salient event .This paradigm allows us to construct structured full - factorial experiments that can map interactions between features with high statistical power .", "label": "", "metadata": {}, "score": "40.477276"}
{"text": "6 , no . 2 , pp . 79 - 89 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Saeed , A. Al - Hamadi , R. Niese , and M. Elzobi , \" Effective geometric features for human emotion recognition , \" in Proceedings of the 11th IEEE International Conference on Signal Processing ( ICSP ' 12 ) , pp .", "label": "", "metadata": {}, "score": "40.70736"}
{"text": "Discussion .Results from our perceptual experiments reveal an intricate auditory saliency space that is multidimensional and highly interconnected .Some of the observed interactions are not unique to the current study ; but have been reported in other contexts of detection , classification and discrimination tasks ( Melara and Marks , 1990 ; Moore , 2003 ; Allen and Oxenham , 2013 ) .", "label": "", "metadata": {}, "score": "40.890785"}
{"text": "In this study , a feature set is employed that shall best cover the described gained knowledge .We therefore stick to the findings in [ 32 ] by choosing the most common and at the same time promising feature types and functionals covering prosodic , spectral , and voice quality features .", "label": "", "metadata": {}, "score": "41.05011"}
{"text": "\" There are several prior models of this kind of information integration , primarily models of long - term memory that combine information across prior instances to identify commonalities ( e.g. , Hintzman , 1984 ; McClelland and Rumelhart , 1985 ) .", "label": "", "metadata": {}, "score": "41.205727"}
{"text": "2 , pp .1297 - 1304 , October 2003 .View at Scopus .S. Koelstra , M. Pantic , and I. Patras , \" A dynamic texture - based approach to recognition of facial actions and their temporal models , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "41.28938"}
{"text": "The present studies replicate and extend prior work by Thiessen and Saffran ( 2003 ) demonstrating that sensitivity to conditional statistical information in speech is early developing , and appears to emerge - at least in English - learning infants - before the development of the trochaic bias .", "label": "", "metadata": {}, "score": "41.438034"}
{"text": "Then , we truncate the out - of - range components to either 0 or 1 .Point Distribution Model ( PDM ) .The extracted features from the previous section rely on the facial point location , where state - of - the - art facial point detectors still do not provide manually annotated accuracy , especially when the face is not in the neutral expression .", "label": "", "metadata": {}, "score": "41.49714"}
{"text": "786 - 791 , October 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Castillo , A. Rivera , and O. Chae , \" Facial expression recognition based on local sign directional pattern , \" in Proceedings of the 19th", "label": "", "metadata": {}, "score": "41.952854"}
{"text": "We compared our results with those of Lucey et al .approach , which relied on features extracted from 68 fiducial points , taken into consideration their prior knowledge of person - specific neutral expression .The comparison was carried out on CK+ database .", "label": "", "metadata": {}, "score": "42.069595"}
{"text": "As a result , we could not find any significant difference among the three feature sets .The filled - pause feature did not give significant improvement either , but there might be a possibility to obtain more improvement by combining the filled - pause feature with other multimodal features .", "label": "", "metadata": {}, "score": "42.30232"}
{"text": "I. H. Witten and E. Frank , Data Mining : Practical Machine Learning Tools and Techniques , Morgan Kaufmann , San Francisco , Calif , USA , 2nd edition , 2005 .T. Athanaselis , S. Bakamidis , I. Dologlou , R. Cowie , E. Douglas - Cowie , and C. Cox , \" ASR for emotional speech : clarifying the issues and enhancing performance , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "42.34835"}
{"text": "For example , the k - nearest neighbor rule assigns the class represented the most in the closest k neighbors to the test sample .Experimental Results .To assess the reliability of our approach , we compared our results with those of Lucey et al .", "label": "", "metadata": {}, "score": "42.71688"}
{"text": "Then , we evaluated our approach on Binghamton University 3D dynamic Facial Expression Database ( BU-4DFE ) [ 41 ] .Next , we provided a method to synthesize facial points with several uncertainties that are supposed to simulate the facial point detector errors .", "label": "", "metadata": {}, "score": "42.840397"}
{"text": "Evidence that learners are extracting clusters of statistically coherent elements can be seen even for non - linguistic stimuli ( e.g. , Fiser and Aslin , 2005 ) , suggesting that this extraction is a domain - general aspect of conditional statistical learning .", "label": "", "metadata": {}, "score": "42.870476"}
{"text": ", 2009 ; Johnson and Tyler , 2010 ; Kudo et al . , 2011 ) .Indeed , to our knowledge the infants in this experiment are younger than any prior group of infants in a behavioral word segmentation experiment .", "label": "", "metadata": {}, "score": "43.04541"}
{"text": "Finally , detailed results and comparisons are reported .In this proposed approach , we provide a frame - level facial expression recognition .Besides its usefulness to analyze single images , it could be exploited in the facial expression recognition using spatiotemporal methods .", "label": "", "metadata": {}, "score": "43.060783"}
{"text": "Automatic extraction of the useful features and incremental discrimination are issues for future works .This paper is organized as follows .Preparation of the experimental data is described in Section 2 .Then the audio features and visual features are introduced in Sections 3 and 4 , respectively .", "label": "", "metadata": {}, "score": "43.191338"}
{"text": "The elements of the feature vector are employed from the features examined above ( details of the features will be described in Section 5.2 ) .We used libSVM [ 37 ] with a linear kernel for the experiments .The simple pairwise method was employed for multiclass discrimination .", "label": "", "metadata": {}, "score": "43.343384"}
{"text": "View at Scopus .K. Dupuis and K. Pichora - Fuller , \" Use of lexical and affective prosodic cues to emotion by younger and older adults , \" in Proceedings of the 8th Annual Conference of the International Speech Communication Association ( Interspeech ' 07 ) , vol .", "label": "", "metadata": {}, "score": "44.224747"}
{"text": "39 , no .5 , pp .514 - 531 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. Komatani , S. Ueno , T. Kawahara , and H. G. Okuno , \" Flexible guidance generation using user model in spoken dialogue systems , \" in Proceedings of the COLING , pp .", "label": "", "metadata": {}, "score": "44.247375"}
{"text": "39 , no .5 , pp .514 - 531 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. Komatani , S. Ueno , T. Kawahara , and H. G. Okuno , \" Flexible guidance generation using user model in spoken dialogue systems , \" in Proceedings of the COLING , pp .", "label": "", "metadata": {}, "score": "44.247375"}
{"text": "Wu S , Falk TH , Chan W - Y : Automatic speech emotion recognition using modulation spectral features .Speech Communication 2011 , 53 : 768 - 785 .10.1016/j.specom.2010.08.013 View Article .Dumouchel P , Dehak N , Attabi Y , Dehak R , Boufaden N : Cepstral and long - term features for emotion recognition , in Proceedings of the 10th Annual Conference of the International Speech Communication Association ( INTERSPEECH-2009 ) .", "label": "", "metadata": {}, "score": "44.469814"}
{"text": "One thing 's for certain , the prior knowledge about the distribution of point localization error would help in selecting more robust geometrical features .Studying this issue is behind the scope of this work , since all state - of - the - art point detectors are not providing these error distributions [ 38 , 44 - 46 ] .", "label": "", "metadata": {}, "score": "44.489906"}
{"text": "The nature of sound as a time - evolving entity can not be captured by spatial processing .There have been attempts to remedy this problem by changes to the procedure of computing saliency after feature extraction , but the methodologies used are still adaptations from vision mechanisms ( Kaya and Elhilali , 2012 ; Cottrell and Tsuchida , 2012 ) .", "label": "", "metadata": {}, "score": "44.50917"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Schuller , R. Jim\u00e9nez Villar , G. Rigoll , and M. Lang , \" Meta - classifiers in acoustic and linguistic feature fusion - based affect recognition , \" in Proceedings of the IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ' 05 ) , vol .", "label": "", "metadata": {}, "score": "44.595497"}
{"text": "For example , exposure to a set of words allows infants to discover the phonological regularities that characterize those words ( Chambers et al . , 2003 ; Saffran and Thiessen , 2003 ; Thiessen and Saffran , 2007 ; Thiessen and Yee , 2010 ) .", "label": "", "metadata": {}, "score": "44.600853"}
{"text": "The ability to learn the relation between lexical stress and word position on the basis of a set of exemplars following a particular prosodic pattern is an example of distributional statistical learning .These processes are typically studied and modeled in isolation ( e.g. , Perruchet and Vinter , 1998 ; Frank et al . , 2010 ; Thiessen and Pavlik , 2012 ) .", "label": "", "metadata": {}, "score": "44.772995"}
{"text": "This work is an extension to our previous paper [ 33 ] .Here we further enhance the approach performance by employing a point distribution model ( PDM ) to avoid shape distortions , which could be caused by noisy detection .", "label": "", "metadata": {}, "score": "44.791656"}
{"text": "However , in this paper , we can only deal with performance measures such as classification rates as criteria .Note that we will not deal with data , use cases , or applications without speech .If we take into account more than one modality we always have to align the unit of one modality with the unit(s ) found in the other modality / modalities .", "label": "", "metadata": {}, "score": "44.83203"}
{"text": "In another example in 3D facial data , Niese et al .[14 ] extracted dynamic and geometrical features from facial points and specific regions associated with the 3D face model of each subject .These points are initially annotated or detected on the neutral state image and tracked over the remaining sequence .", "label": "", "metadata": {}, "score": "45.19294"}
{"text": "The aim is to compute features from the speech input which can be relevant for the detection of emotion in the user 's voice .We extracted the most representative selection from the list of 60 features shown in Table 1 .", "label": "", "metadata": {}, "score": "45.26481"}
{"text": "11 , pp .1940 - 1954 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Zhao and M. Pietik\u00e4inen , \" Dynamic texture recognition using local binary patterns with an application to facial expressions , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "45.28534"}
{"text": "Strategies for social robots to initiate interaction , \" in Proceedings of the 4th ACM / IEEE International Conference on Human - Robot Interaction ( HRI ' 09 ) , pp .109 - 116 , March 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "45.391624"}
{"text": "Strategies for social robots to initiate interaction , \" in Proceedings of the 4th ACM / IEEE International Conference on Human - Robot Interaction ( HRI ' 09 ) , pp .109 - 116 , March 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "45.391624"}
{"text": "M. F. Valstar and M. Pantic , \" Combined support vector machines and hidden Markov models for modeling facial action temporal dynamics , \" in Human - Computer Interaction , M. Lew , N. Sebe , T. Huang , and E. Bakker , Eds . , vol .", "label": "", "metadata": {}, "score": "45.40316"}
{"text": "View at Scopus .J. Begole , N. E. Matsakis , and J. C. Tang , \" Lilsys : sensing unavailability , \" in Proceedings of the Computer Supported Cooperative Work ( CSCW ' 04 ) , pp .511 - 514 , November 2004 .", "label": "", "metadata": {}, "score": "45.44422"}
{"text": "View at Scopus .J. Begole , N. E. Matsakis , and J. C. Tang , \" Lilsys : sensing unavailability , \" in Proceedings of the Computer Supported Cooperative Work ( CSCW ' 04 ) , pp .511 - 514 , November 2004 .", "label": "", "metadata": {}, "score": "45.44422"}
{"text": "View at Google Scholar .M. Schr\u00f6der , R. Cowie , D. Heylen , M. Pantic , C. Pelachaud , and B. Schuller , \" Towards responsive sensitive artificial listeners , \" in Proceedings of the 4th International Workshop on Human - Computer Conversation , Bellagio , Italy , 2008 .", "label": "", "metadata": {}, "score": "45.552383"}
{"text": "If we look at the attempt towards a taxonomy of applications in [ 52 ] , most important for segmentation might be the difference between online and offline applications .We mentioned in the beginning that for online applications such as SmartKom [ 11 ] or Semaine [ 12 ] , incremental processing will be mandatory because of time constraints .", "label": "", "metadata": {}, "score": "45.715164"}
{"text": "The corpora further feature detailed speaker meta - data , orthographic transcript , phonemic transcript , and segmentation and multiple annotation tracks .Both are given with distinct definitions of test , development , and training partitions , incorporating speaker independence as needed in most real - life settings .", "label": "", "metadata": {}, "score": "45.748764"}
{"text": "In this paper we explore a combination of both approaches .Short - term temporal information is captured by the front - end , and longer - term temporal information is modelled by the back - end .Section 5.2 reports the results of a listening test carried out to validate the use of linear approximations of glottal parameter contours , conducted in a similar manner to a previous listening test carried out to validate the use of linear approximations to pitch contours [ 21 ] .", "label": "", "metadata": {}, "score": "45.777763"}
{"text": "P. Viola and M. Jones , \" Rapid object detection using a boosted cascade of simple features , \" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp .I511-I518 , December 2001 .", "label": "", "metadata": {}, "score": "45.838642"}
{"text": "P. Viola and M. Jones , \" Rapid object detection using a boosted cascade of simple features , \" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition , pp .I511-I518 , December 2001 .", "label": "", "metadata": {}, "score": "45.838642"}
{"text": "Additionally , we have enhanced the UAH system to deal with the mental - state information .In order to do so , we have improved the dialogue manager to take this information into account to compute and adapt the system responses .", "label": "", "metadata": {}, "score": "45.882595"}
{"text": "View Article .Sindlar M , Dastani M , Meyer JJ : Mental State Ascription Using Dynamic Logic .In Proceedings of the 19th European Conference on Artificial Intelligence .Lisbon , Portugal ; 2010:561 - 566 .Oztop E , Wolpert D , Kawato M : Mental state inference using visual control parameters .", "label": "", "metadata": {}, "score": "45.890854"}
{"text": "2613 - 2616 , 2012 . A. Saeed , A. Al - Hamadi , and R. Niese , \" Neutral - independent geometric features for facial expression recognition , \" in Proceedings of the 12th International Conference on Intelligent Systems Design and Applications ( ISDA ' 12 ) , pp .", "label": "", "metadata": {}, "score": "45.93466"}
{"text": "6 , pp .915 - 928 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. Wu , M. S. Bartlett , and J. R. Movellan , \" Facial expression recognition using Gabor motion energy filters , \" in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops ( CVPRW ' 10 ) , pp .", "label": "", "metadata": {}, "score": "45.93525"}
{"text": "We previously evaluated four alternatives : a multinomial naive Bayes classifier , a n -gram based classifier , a classifier based on grammatical inference techniques , and a classifier based on neural networks [ 46 , 47 ] .The accuracy results obtained with these classifiers were respectively 88.5 , 51.2 , 75.7 and 97.5 % .", "label": "", "metadata": {}, "score": "46.0288"}
{"text": "Acknowledgments .References .Akatsuka , K. , Wasaka , T. , Nakata , H. , Kida , T. , and Kakigi , R. ( 2007 ) .The effect of stimulus probability on the somatosensory mismatch field .Exp .Brain Res .", "label": "", "metadata": {}, "score": "46.035263"}
{"text": "[ .Z .t . )Z .t . )Z .t . .]F . [ . 1 . ]H .[ . 0 . ]The number of regularity streams ( each represented with a separate Kalman ) to initialize for each feature is determined by k - means clustering of the first 125 ms of feature values .", "label": "", "metadata": {}, "score": "46.076836"}
{"text": "( ii )The researchers in geometrical facial features synthesize facial points with the help of aforementioned information .As a result of this step , we overcome the issue of reimplementing not available state - of - the - art facial point detectors .", "label": "", "metadata": {}, "score": "46.083385"}
{"text": "195 - 200 , 2000 . D. Litman and K. Forbes - Riley , \" Spoken tutorial dialogue and the feeling of another 's knowing , \" in Proceedings of the SIGDIAL , 2009 .M. Goto , K. Itou , and S. Hayamizu , \" A realtime filled pause detection system : toward spontaneous speech dialogue , \" in Proceedings of the Eurospeech , pp .", "label": "", "metadata": {}, "score": "46.108833"}
{"text": "195 - 200 , 2000 . D. Litman and K. Forbes - Riley , \" Spoken tutorial dialogue and the feeling of another 's knowing , \" in Proceedings of the SIGDIAL , 2009 .M. Goto , K. Itou , and S. Hayamizu , \" A realtime filled pause detection system : toward spontaneous speech dialogue , \" in Proceedings of the Eurospeech , pp .", "label": "", "metadata": {}, "score": "46.108833"}
{"text": "As in Figure 4 , the weight and directionality of interactions in this figure are inferred from the coefficients of the fitted model , and are limited by the levels of sound features tested in the human experiments .The probabilistic saliency output of the model can function as a discrete deviance detection mechanism by mapping the saliency scores to a binary classification .", "label": "", "metadata": {}, "score": "46.375256"}
{"text": "Z. Zeng , M. Pantic , G. I. Roisman , and T. S. Huang , \" A survey of affect recognition methods : audio , visual , and spontaneous expressions , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "46.44586"}
{"text": "On the other hand , although the error correction rates were also improved in absolute values by using the mental - state system , this relationship was not significant in the t test .Both results are explained by the fact that we have not designed a specific strategy to improve the recognition or understanding processes and decrease the error rate , but rather our proposal for adaptation to the user mental state overcomes these problems during the dialogue once they are produced .", "label": "", "metadata": {}, "score": "46.469498"}
{"text": "View at Scopus .Z.-J. Chuang and C.-H. Wu , \" Emotion recognition using acoustic features and textual content , \" in Proceedings of the IEEE International Conference on Multimedia and Expo ( ICME ' 04 ) , vol .1 , pp .", "label": "", "metadata": {}, "score": "46.54593"}
{"text": "433 - 449 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Littlewort , J. Whitehill , T. Wu et al . , \" The computer expression recognition toolbox ( CERT ) , \" in Proceedings of the IEEE International Conference on Automatic Face and Gesture Recognition and Workshops ( FG ' 11 ) , pp .", "label": "", "metadata": {}, "score": "46.58757"}
{"text": "Obviously , these corresponding approaches work with image sequences starting usually with neutral expression .By contrast , the second category does not require prior knowledge of the considered person 's neutral expression .For example , Littlewort et al .[", "label": "", "metadata": {}, "score": "46.591564"}
{"text": "We normalize contribution of each feature and non - linearly model integration interactions with constrained logistic regression , using the stimuli used in our experimental paradigm with their corresponding ground truth about the timings of salient sounds ( i.e. , timing of foreground tokens ) .", "label": "", "metadata": {}, "score": "46.707016"}
{"text": "Volume 4 .IEEE :Piscataway ; 2007:941 - 944 .Vlasenko B , Schuller B , Wendemuth A , Rigoll G : Frame vs. turn - level : emotion recognition from speech considering static and dynamic processing , in Affective Computing and Intelligent Interaction .", "label": "", "metadata": {}, "score": "46.85502"}
{"text": "Many features have been proposed for speech - based emotion recognition , and a majority of them are frame based or statistics estimated from frame - based features .Temporal information is typically modelled on a per utterance basis , with either functionals of frame - based features or a suitable back - end .", "label": "", "metadata": {}, "score": "46.931835"}
{"text": "Biol .9:e1002911 . doi : 10.1371/journal.pcbi.1002911 .McAdams , S. , Winsberg , S. , Donnadieu , S. , Soete , G. D. , and Krimphoff , J. ( 1995 ) .Perceptual scaling of synthesized musical timbres : common dimensions , specificities , and latent subject classes .", "label": "", "metadata": {}, "score": "46.97896"}
{"text": "Following this , we extract two geometrical feature types from the projected points .Finally , we classify each normalized feature vector to one of the basic facial expressions , where we employed two machine learning algorithms for the facial expression recognition .", "label": "", "metadata": {}, "score": "47.028015"}
{"text": "In the current study , we take a more fundamental approach to modeling auditory attention ; providing the first examination of the space of auditory saliency spanning pitch , intensity and timbre ; and shedding light on complex interactions among these features .", "label": "", "metadata": {}, "score": "47.227882"}
{"text": "The Benjamini - Hochberg procedure ( Benjamini and Hochberg , 1995 ) is used to iteratively validate the significance levels for multiple comparisons shown in Tables 1 , 2 .Although the backgrounds in the trials are not identical , there is a possibility that subjects learn the backgrounds over time because of the limited set of background tokens .", "label": "", "metadata": {}, "score": "47.2335"}
{"text": "The role of temporal regularity in auditory segregation .Hearing Res .doi : 10.1016/j.heares.2011.06.001 .Arnaud , E. , Memin , E. , and Cernuschi - Frias , B. ( 2005 ) .Conditional filters for image sequence - based tracking - application to point tracking .", "label": "", "metadata": {}, "score": "47.25484"}
{"text": "Visual elements that differ along modalities of color , intensity , orientation , size and depth ( among others ) are shown to affect visual search ( Wolfe and Horowitz , 2004 ) , and bias eye fixations in natural scenes ( Masciocchi et al . , 2009 ) .", "label": "", "metadata": {}, "score": "47.544334"}
{"text": "Witten IH , Frank E : Data Mining : Practical Machine Learning Tools and Techniques .Morgan Kaufmann , San Francisco ; 2005 .Griol D , Hurtado LF , Segarra E , Sanchis E : A statistical approach to spoken dialog systems design and evaluation .", "label": "", "metadata": {}, "score": "47.6671"}
{"text": "The current work argues for a strong link between tracking statistics of an auditory scene and elicitation of deviance signals that flag salient sounds as aberrant events that would be attention grabbing .This process builds strongly on the notion of predictive inference , and frames the analysis of auditory scenes and selecting events of interest via predictive interpretations of the underlying events in the scene .", "label": "", "metadata": {}, "score": "47.694183"}
{"text": "Adaptations of the visual saliency map have been introduced by considering the time - frequency spectrogram of an audio signal as an \" auditory image \" upon which saliency mechanisms can operate ( Kayser et al . , 2005 ) .This architecture has also been extended to extract attributes better suited for the auditory domain such as a pitch ( Duangudom and Anderson , 2007 ; Kalinli and Narayanan , 2007 ) .", "label": "", "metadata": {}, "score": "47.777527"}
{"text": "One linguistically relevant application of this sensitivity to distributional information is category learning .Sensitivity to variability is similarly informative for category learning ; when exposed to distributions with high variability , learners accept a wider range of exemplars as members of the category ( e.g. , Clayards et al . , 2008 ) .", "label": "", "metadata": {}, "score": "47.801407"}
{"text": "It can be modulated by \" bottom - up \" sensory - driven factors , \" top - down \" task - specific goals , expectations , and learned schemas ; as well as \" lateral - based \" behavioral history and reward ( Awh et al .", "label": "", "metadata": {}, "score": "47.836464"}
{"text": "Morrison D , Wang R , Silva LCD : Ensemble methods for spoken emotion recognition in call - centers .Speech Commun 2007 , 49 ( 2):98 - 112 .10.1016/j.specom.2006.11.004 View Article .Batliner A , Steidl S , Schuller B , Seppi D , Vogt T , Wagner J , Devillers L , Vidrascu L , Aharonson V , Kessous L , Amir N : Whodunnit -- searching for the most important feature types signalling emotion - related user states in speech .", "label": "", "metadata": {}, "score": "47.924683"}
{"text": "The most commonly used acoustic and prosodic features tend to be those based on cepstral coefficients , pitch , intensity and speech rate .The standard speech production model ( source - filter model ) [ 12 ] , widely used in speech processing literature , is the model that underpins most low - level feature extraction algorithms .", "label": "", "metadata": {}, "score": "48.112274"}
{"text": "10.1016/j.specom.2008.04.001 View Article .Griol D , Hurtado LF , Sanchis E , Segarra E : Managing Unseen Situations in a Stochastic Dialog Model .In Proceedings of AAAI Workshop on Statistical and Empirical Approaches for Spoken Dialogue Systems .", "label": "", "metadata": {}, "score": "48.141434"}
{"text": "In the present study , we aim at realistic conditions - apart from the last step to use fully automatic ASR .In [ 23 ] we could show that - depending on the recording conditions and the feature set used - ASR errors do not always deteriorate emotion recognition .", "label": "", "metadata": {}, "score": "48.28711"}
{"text": "The authors declare that there is no conflict of interests regarding the publication of this paper .Acknowledgment .This work is part of the project done within the Transregional Collaborative Research Centre SFB / TRR 62 Companion - Technology for Cognitive Technical Systems funded by the German Research Foundation ( DFG ) .", "label": "", "metadata": {}, "score": "48.385826"}
{"text": "View at Scopus . K. Laskowski , J. Edhund , and M. Heldner , \" Incremental learning and forgetting in stochastic turn - taking models , \" in Proceedings of the Interspeech , pp .2069 - 2072 , 2011 .K. Laskowski and E. Shriberg , \" Corpusindependent history compression for stochastic turn - taking models , \" in Proceedings of the ICASSP , pp .", "label": "", "metadata": {}, "score": "48.402733"}
{"text": "View at Scopus . K. Laskowski , J. Edhund , and M. Heldner , \" Incremental learning and forgetting in stochastic turn - taking models , \" in Proceedings of the Interspeech , pp .2069 - 2072 , 2011 .K. Laskowski and E. Shriberg , \" Corpusindependent history compression for stochastic turn - taking models , \" in Proceedings of the ICASSP , pp .", "label": "", "metadata": {}, "score": "48.402733"}
{"text": "This simulator carries out the functions of the ASR ( Automatic Speech Recognition ) and NLU modules .An additional error simulator module is used to perform error generation and the addition of ASR confidence scores [ 55 ] .The number of errors that are introduced in the recognized sentence can be modified to adapt the error simulator module to the operation of any ASR and NLU modules .", "label": "", "metadata": {}, "score": "48.41941"}
{"text": "Previously , a modified feature warping technique was proposed as a means of speaker normalisation [ 50 ] and was applied to all features in experiments reported in this paper unless otherwise stated .GMM - based classification system .In order to facilitate comparisons to help determine the significance of temporal information contained in parameter contours as opposed to the statistical distributions of parameter values , another classification system based on Gaussian mixture models ( GMMs ) was set up .", "label": "", "metadata": {}, "score": "48.440346"}
{"text": "Multiplexed and robust representations of sound features in auditory cortex .J. Neurosci .doi : 10.1523/JNEUROSCI.2074 - 11.2011 .Winkler , I. , Denham , S. L. , and Nelken , I. ( 2009 ) .Modeling the auditory scene : predictive regularity representations and perceptual objects .", "label": "", "metadata": {}, "score": "48.461727"}
{"text": "89 - 103 , Springer , Berlin , Germany , 2012 .View at Google Scholar . M.-C.Hwang , L. T. Ha , N.-H. Kim , C.-S. Park , and S.-J. Ko , \" Person identification system for future digital TV with intelligence , \" IEEE Transactions on Consumer Electronics , vol .", "label": "", "metadata": {}, "score": "48.48095"}
{"text": "However , as these are very rare cases , we can not model them reliably for automatic processing and have to map them onto R(est ) .Furthermore , in this paper we will not deal with the problem of automatically obtaining EC given a set of features .", "label": "", "metadata": {}, "score": "48.608486"}
{"text": "Duration and stress conditions .Hansen [ 59 ] , Ververidis and Kotropoulos [ 60 ] , Morrison et al .[ 61 ] and Batliner et al .[ 62 ] .The second step of the emotion recognition process is feature normalization , with which the features extracted in the previous phase are normalized around the user neutral speaking style .", "label": "", "metadata": {}, "score": "48.619144"}
{"text": "The author considers that a mental state involves beliefs , intentions and expectations .Dragoni [ 17 ] followed this vision to formalize the consequences of an utterance or series of dialogue acts on the mental state of the hearer in a multi - context framework .", "label": "", "metadata": {}, "score": "48.718636"}
{"text": "From a theoretical point of view , this might be easier to accomplish ; from a practical point of view , it will be a matter of performance , of ease of handling , and - perhaps most important - of the weight a modality has in specific scenarios .", "label": "", "metadata": {}, "score": "48.833885"}
{"text": "Our criteria are \" external \" and objective and are not based on intuitive notions of an \" emotional \" unit of analysis as in the studies by [ 28 - 30 ] .Moreover , using syntactically motivated units makes processing in an end - to - end system more straightforward and adequate .", "label": "", "metadata": {}, "score": "48.8468"}
{"text": "Two databases were used to evaluate our approach .We achieved an average 83 % ( frame - level ) recognition rate on CK+ database which as well as Lucey et al .approach ; however , they used 68 facial points along with prior knowledge of person - specific neutral expression .", "label": "", "metadata": {}, "score": "48.88815"}
{"text": "Finally , each computed feature is further binned using 200 ms windows , such that the mean of the window is assigned to every sample in the window .Figure 3 .Schematic of the computational saliency model .The model is structured along three stages .", "label": "", "metadata": {}, "score": "48.968727"}
{"text": "Baltru\u0161aitis et al .[5 ] proposed a dynamic system with three levels of inference on progressively longer time scales to understand the human mental states from facial expressions and upper - body gestures , where they employed both DBN and HMM .", "label": "", "metadata": {}, "score": "49.016922"}
{"text": "within salient event duration . otherwise .The optimization is performed simultaneously on all features ; with clips from all experiments ( and their correspondent ground truths ) incorporated as training data .For analyses where each experiment is trained separately , each feature is also optimized separately to reduce noise .", "label": "", "metadata": {}, "score": "49.022823"}
{"text": "3 , no .6 , pp .32 - 43 , 1996 .View at Google Scholar .L. Ferrer , E. Shriberg , and A. Stolcke , \" A prosody - based approach to end - of - utterance detection that does not require speech recognition , \" in Proceedings of the IEEE International Conference on Accoustics , Speech , and Signal Processing , pp .", "label": "", "metadata": {}, "score": "49.2006"}
{"text": "3 , no .6 , pp .32 - 43 , 1996 .View at Google Scholar .L. Ferrer , E. Shriberg , and A. Stolcke , \" A prosody - based approach to end - of - utterance detection that does not require speech recognition , \" in Proceedings of the IEEE International Conference on Accoustics , Speech , and Signal Processing , pp .", "label": "", "metadata": {}, "score": "49.2006"}
{"text": "Lourens et al .[ 23 ] also carried out mental - state recognition from motor movements following the mirror neuron system perspective .In the research described so far , affective information is not explicitly considered although it can sometimes be represented using a number of formalisms .", "label": "", "metadata": {}, "score": "49.28665"}
{"text": "Litman DJ , Forbes - Riley K : Recognizing student emotions and attitudes on the basis of utterances in spoken tutoring dialogues with both human and computer tutors .Speech Commun 2006 , 48 ( 5):559 - 590 .10.1016/j.specom.2005.09.008 View Article .", "label": "", "metadata": {}, "score": "49.294987"}
{"text": "And the images ( g)-(l ) are samples from BU-4DFE .Feature Extraction .Two feature sets are extracted from the selected facial points .Figure 2(b ) shows the eight facial points within a box returned by the employed face detector .", "label": "", "metadata": {}, "score": "49.307434"}
{"text": "On this account , infants initially rely on language - universal cues - such as sensitivity to conditional statistical information - to segment words from fluent speech .Once they have identified and stored a set of word forms , they can identify the acoustic features that are consistent across them ( e.g. , Lew - Williams and Saffran , 2012 ) .", "label": "", "metadata": {}, "score": "49.35883"}
{"text": "An alternative to the proposed method would be to directly estimate the mental state from the voice signal , the dialogue features and the semantics of the user input in a single step .However , we have considered several phases that differentiate the emotion and intentions recognizers to provide a more modular architecture , in which different emotion and intention recognizers could be plugged - in .", "label": "", "metadata": {}, "score": "49.42612"}
{"text": "An additional remark on this experiment , the state - of - the - art facial point detector approach reported 2 - 5 % error for the eight selected facial points .This error was measured on a challenge database , which includes non - frontal face and occluded facial parts .", "label": "", "metadata": {}, "score": "49.491814"}
{"text": "All those features are the input into an individual SVM classifier for smaller facial action units ( AUs ) .Finally , they built a multivariate logistic regression classifier ( MLR ) on top of the output of AU classifiers to recognize the human facial expressions .", "label": "", "metadata": {}, "score": "49.534206"}
{"text": "pp .2149 - 2152 .Pantic M , Rothkrantz LJM : Toward an affect - sensitive multimodal human - computer interaction .Proc IEEE 2003 , 91 : 1370 - 1390 .10.1109/JPROC.2003.817122 View Article .Ververidis D , Kotropoulos C : Emotional speech recognition : resources , features , and methods .", "label": "", "metadata": {}, "score": "49.560898"}
{"text": "Planet S , Iriondo I , Socor\u00f3 J , Monzo C , Adell J : GTM - URL contribution to the INTERSPEECH 2009 Emotion Challenge , in Proceedings of the 10th Annual Conference of the International Speech Communication Association ( INTERSPEECH-2009 ) .", "label": "", "metadata": {}, "score": "49.586952"}
{"text": "GMM - based systems were used to model distributions without taking into account temporal contours .These experiments also compared the S - I representation to the MP representation for all three parameters and the results are summarised in Table 10 .", "label": "", "metadata": {}, "score": "49.59372"}
{"text": "218 - 226 , 2007 .View at Publisher \u00b7 View at Google Scholar .P. Ekman and W. Friesen , Facial Action Coding System : A Technique for the Measurments of Facial Movements , Consulting Psychologists Press , 1978 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "49.599316"}
{"text": "However , these methods are error prone and can not provide hand - annotation accuracy .As a sample of the first category , Lucey et al .[ 13 ] manually labeled 68 facial points in keyframes within each image sequence then used a gradient descent Active Appearance Model ( AAM ) to fit these points in the remaining frames .", "label": "", "metadata": {}, "score": "49.654232"}
{"text": "Zhao J.,Ngo N.,McKendrick R.,Turk - Browne N. B ..( Year : 2011 ) .Mutual interference between statistical summary perception and statistical learning .Psychol .Sci.22 , 1212 - 121910.1177/095679761141930421852450 .Top : an excerpt of the iambic familiarization stream used in Experiment 1B ; capitalized syllables represent stress .", "label": "", "metadata": {}, "score": "49.74685"}
{"text": "10.1016/j.specom.2006.04.003 View Article .Vidrascu L , Devillers L : Five emotion classes detection in real - world call center data : the use of various types of paralinguistic features , in Proceedings of International Workshop on Paralinguistic Speech - 2007 .", "label": "", "metadata": {}, "score": "49.800545"}
{"text": "The work also has the advantage of not requiring explicit separation of speech into utterances .As part of the parameterisation process , this paper has taken a second look at the traditional source - filter model widely used in speech processing tasks and , in particular , the assumption about the vocal excitation that is inherent in common feature extraction procedures .", "label": "", "metadata": {}, "score": "49.800735"}
{"text": "O. Lemon and l. Konstas , \" User simulations for context - sensitive speech recognition in spoken dialogue systems , \" in Proceedings of the EACL , pp .505 - 513 , 2009 .M. Rickert , M. E. Foster , M. Giuliani , G. Panin , T. By , and A. Knoll , \" Integrating language , vision and action for human robot dialog systems , \" in Universal Access in Human - Computer Interaction .", "label": "", "metadata": {}, "score": "49.840424"}
{"text": "O. Lemon and l. Konstas , \" User simulations for context - sensitive speech recognition in spoken dialogue systems , \" in Proceedings of the EACL , pp .505 - 513 , 2009 .M. Rickert , M. E. Foster , M. Giuliani , G. Panin , T. By , and A. Knoll , \" Integrating language , vision and action for human robot dialog systems , \" in Universal Access in Human - Computer Interaction .", "label": "", "metadata": {}, "score": "49.840424"}
{"text": "However , there has been no proposal of an explicit tie between this framework and bottom - up attention in complex natural soundscapes .In this work , we aim to bridge this gap by asking whether the predictive - coding theory can provide an explanation for auditory saliency .", "label": "", "metadata": {}, "score": "49.889767"}
{"text": "635 - 640 , October 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Zhu , C. De Silva , and C. Ko , \" Using moment invariants and hmm in facial expression recognition , \" in Proceedings of the 4th IEEE Southwest Symposium on Image Analysis and Interpretation , pp .", "label": "", "metadata": {}, "score": "49.929718"}
{"text": "Unlike Lucey et al .'s approach [ 13 ] which used 68 facial points , we use just eight facial points .These points have shown to perform well in the facial expression recognition [ 15 , 33 ] .Moreover , these points represent corner and edge points that can be efficiently detected and tracked [ 38 ] .", "label": "", "metadata": {}, "score": "49.943344"}
{"text": "10.1016/j.ijhcs.2009.12.003 View Article .Gnjatovic M , R\u00f6sner D : Adaptive dialogue management in the NIMITEK prototype system .Lect Notes Comput Sci 2008 , 5078 : 14 - 25 .10.1007/978 - 3 - 540 - 69369 - 7_3 View Article .", "label": "", "metadata": {}, "score": "49.979614"}
{"text": "( Year : 1988 ) .A precursor to language acquisition in young infants .Cognition29 , 143 - 17810.1016/0010 - 0277(88)90035 - 23168420 .Mersad K.,Nazzi T ..( Year : 2011 ) .Transitional probabilities and positional frequency phonotactics in a hierarchical model of speech segmentation .", "label": "", "metadata": {}, "score": "50.056976"}
{"text": ", 2012 ; Lieder et al . , 2013 ) .The premise is based on the notion that the \" Bayesian brain \" continuously makes likelihood inferences about its sensory input , conceivably by generating predictions about upcoming stimuli ( Friston , 2010 ) .", "label": "", "metadata": {}, "score": "50.074062"}
{"text": "For example , some proposals have suggested that the earliest tools for word segmentation are prosodic cues ( e.g. , Johnson and Jusczyk , 2001 ; Nazzi et al . , 2006 ) .Indeed , German - learning infants have been found to use lexical stress as a cue to word segmentation by 6 months ( H\u00f6hle et al . , 2009 ) , younger than any prior demonstrations that infants were able to use conditional statistical information to segment fluent speech .", "label": "", "metadata": {}, "score": "50.132988"}
{"text": "The probabilistic output of the saliency model leads to a detection curve in ROC space by setting a threshold to distinguish true and false detections .We can infer from the curves that the saliency scores of the control trials are most easily separable than the saliency scores of the test trials for Experiment III , and that the performance of the model is closest to humans for Experiment II .", "label": "", "metadata": {}, "score": "50.186428"}
{"text": "Discussion .The fact that infants show a different pattern of preference after listening to the trochaic and iambic familiarization streams indicates that they segmented different items from the two streams .Further , it replicates prior work with 7-month - olds demonstrating that infants - even 5-month - old infants - can learn to use lexical stress as a cue to word segmentation upon exposure to lexical forms that consistently exemplify a stress pattern ( Thiessen and Saffran , 2007 ) .", "label": "", "metadata": {}, "score": "50.202667"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Wendemuth and S. Biundo , \" A companion technology for cognitive technical systems , \" in Cognitive Behavioural Systems , A. Esposito , A. Esposito , A. Vinciarelli , R. Hoffmann , and V. Mller , Eds . , vol .", "label": "", "metadata": {}, "score": "50.245407"}
{"text": "This suggests that a system , such as the HMM - based one , can be used to model all the contours by simply concatenating the contour descriptors ( either slope , initial value or midpoint value or some combination of them ) to form the feature vector .", "label": "", "metadata": {}, "score": "50.265903"}
{"text": "View Article .Miesenberger K , Klaus J , Zagler W , Karshmer A : Computers helping people with special needs .In Proceedings of 12th International Conference on Computers Helping People with Special Needs ( ICCHP 2010 ) .Lecture Notes on Computer Science 4061 ; 2010 .", "label": "", "metadata": {}, "score": "50.272934"}
{"text": "\u00c1balos N , Espejo G , L\u00f3pez - C\u00f3zar R , Callejas Z , Griol D : A Multimodal Dialogue System for an Ambient Intelligent Application in Home Environments .Volume 6231 .Lectures Notes in Artificial Intelligence ; 2010:484 - 491 .", "label": "", "metadata": {}, "score": "50.28325"}
{"text": "However , this is only the case because our processing is sort of trivial ; we are able to map mixed cases onto one class because we have previously performed MV and adopted threshold criteria ( Section 2 ) .Taking into account other dimensions or mixtures of annotations [ 29 ] would have required a more sophisticated clustering strategy and would not have been feasible for our data , due to the severe sparse data problem .", "label": "", "metadata": {}, "score": "50.32023"}
{"text": "For example , Sindlar et al .[21 ] used dynamic logic to model ascription of beliefs , goals or plans on grounds of observed actions to interpret other agents ' actions .Oztop et al .[ 22 ] developed a computational model of mental - state inference that used the circuitry that underlay motor control .", "label": "", "metadata": {}, "score": "50.33634"}
{"text": "From our perspective , sensitivity to conditional statistical information is one of a small set of language - universal cues that help infants extract a set of lexical items from the input ( for discussion , see Thiessen and Erickson , in press ) .", "label": "", "metadata": {}, "score": "50.385864"}
{"text": "B. Schuller , N. K\u00f6hler , R. M\u00fcller , and G. Rigoll , \" Recognition of interest in human conversational speech , \" in Proceedings of the 9th International Conference on Spoken Language Processing ( ICSLP ' 06 ) , vol .", "label": "", "metadata": {}, "score": "50.393875"}
{"text": "Barra R , Montero JM , Macias - Guarasa J , D'Haro LF , San - Segundo R , Cordoba R : Prosodic and segmental rubrics in emotion identification , in Proceedings of the 2006 IEEE .Speech and Signal Processing , vol . 1 ( IEEE , Piscataway , 2006 ) : International Conference on Acoustics ; 2006:1 .", "label": "", "metadata": {}, "score": "50.470467"}
{"text": "vol 2 2nd edn ; 2003:1 - 4 .Nogueiras A , Moreno A , Bonafonte A , Mari\u00f1o J : Speech emotion recognition using hidden Markov models , in Proceedings of EUROSPEECH-2001 .Scandinavia : EUROSPEECH ; 2001:2679 - 2682 .", "label": "", "metadata": {}, "score": "50.499084"}
{"text": "40 ] .Bui et al .[ 41 ] based their model on Partially Observable Markov Decision Processes [ 42 ] that adapt the dialogue strategy to the user actions and emotional states , which are the output of an emotion recognition module .", "label": "", "metadata": {}, "score": "50.515137"}
{"text": "The significance of the results discussed in \" Evaluation results \" section was computed using the SPSS software with a significance level of 95 % a .Table 3 .Evaluation measures based on the interaction parameters gathered from the dialogues of simulated and recruited users .", "label": "", "metadata": {}, "score": "50.520897"}
{"text": "The unique contribution of the present study is the use of word - based annotations and the subsequent mapping onto different types of higher units , to investigate promising possibilities of segmenting emotional episodes .However , word - based annotation is very time - consuming and thus expensive .", "label": "", "metadata": {}, "score": "50.54608"}
{"text": "The main focus has been on prosodic features in the past , in particular pitch , durations , and intensity [ 31 ] .Comparably small feature sets ( 10 - 100 ) were first utilised .In only a few studies , low - level feature modelling on a frame level was pursued , usually by Hidden Markov Models ( HMMs ) or Gaussian Mixture Models ( GMMs ) .", "label": "", "metadata": {}, "score": "50.54847"}
{"text": "This would , in turn , reveal the best performance possible given the system setup and indicate what information is utilised .The back - end was fixed as a two - state HMM with a 6-mixture GMM modelling each state and various combinations of descriptors ( slope , initial and midpoint values ) of the different source - filter model parameter contours ( Figure 4 ) used as features and the UAR determined for each setup .", "label": "", "metadata": {}, "score": "50.58052"}
{"text": "Then , the filled pause likelihood is calculated as .Filled pause features are thought to affect the choice of label by the evaluators .We extracted the filled pauses contained in the user 's utterance using Goto 's method and used the length of filled pause ( .", "label": "", "metadata": {}, "score": "50.58208"}
{"text": "Int J Comput Sci 2007 , 2 ( 4):285 - 293 .Acosta JC , Ward NG : Responding to user emotional state by adding emotional coloring to utterances .In Proceedings of 10th Annual Conference of the International Speech Communication Association ( Interspeech 09 ) .", "label": "", "metadata": {}, "score": "50.613144"}
{"text": "Analysis of model results .( A )The time instance where the maximum likelihood of saliency was detected for foreground tokens in the scene .Trials in which the maximum saliency was found outside the duration of the foreground are not included .", "label": "", "metadata": {}, "score": "50.629887"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Sandbach , S. Zafeiriou , and M. Pantic , \" Local normal binary patterns for 3d facial action unit detection , \" in Proceedings of the 19thIEEE International Conference on Image Processing ( ICIP ' 12 ) , pp .", "label": "", "metadata": {}, "score": "50.64699"}
{"text": "Three experiments are discussed in Section 3 , where a comprehensive evaluation of the performance of our method , including a comparison with a state - of - the - art method , is provided .Finally , the conclusion and future perspectives are given in Section 4 .", "label": "", "metadata": {}, "score": "50.660004"}
{"text": "This is not only a clever move to push classification performance , it simply has grown out from the problem of class assignment in emotion processing : there is no simple and unequivocal ground truth .We have elaborated on the use of prototypes and their impact on classification performance in [ 5 , 6 ] .", "label": "", "metadata": {}, "score": "50.72171"}
{"text": "Again , only errors that modify the values of the attributes are considered .Error correction rate ( % ECR ) is the percentage of corrected errors , computed as n CE/ ( n CE + n NCE ) .High - level dialogue features .", "label": "", "metadata": {}, "score": "50.768364"}
{"text": "Preliminary experiments with chunks of different granularity , that is , length , showed that using our longer turns actually results in suboptimal classification performance , while the chunking procedure presented below , which was used for the experiments dealt with in this paper , results in better performance .", "label": "", "metadata": {}, "score": "50.78475"}
{"text": "Pattern Anal .Mach .Intell . doi : 10.1109/TPAMI.2012.147 .Lieder , F. , Daunizeau , J. , Garrido , M. I. , Friston , K. J. , and Stephan , K. E. ( 2013 ) .Modelling trial - by - trial changes in the mismatch negativity .", "label": "", "metadata": {}, "score": "50.815414"}
{"text": "Res .doi : 10.1007/BF00419633 .Melara , R. D. , and Marks , L. E. ( 1990 ) .Perceptual primacy of dimensions : support for a model of dimensional interaction .J. Exp .Psychol .Hum .", "label": "", "metadata": {}, "score": "50.82421"}
{"text": "The list was identical to that used in Thiessen and Saffran ( 2007 ) .All of the words in this list were different from the four words that occurred in the familiarization stream .Procedure .Results .We compared listening times to words and part - words for infants exposed to both the trochaic language and the iambic language .", "label": "", "metadata": {}, "score": "50.83809"}
{"text": "Furthermore , results from Exp .III confirm that detection of tokens in the beginning of each trial is low throughout the experiment ( Figure 2B ) , refuting the possibility of meta - learning .Figure 2 .Behavioral results .", "label": "", "metadata": {}, "score": "50.90157"}
{"text": "As can be observed in the table , this flexibility has a bigger impact in the case of the S2 scenarios as the users must convey more information to the system .Also , recruited users seemed to benefit in a greater extent from the flexibility of the mental - state system than simulated users .", "label": "", "metadata": {}, "score": "50.93105"}
{"text": "10.1007/s10472 - 008 - 9100-y MathSciNet View Article MATH .Jonker CM , Treur J : A dynamic perspective on an agent 's mental states and interaction with its environment .In Proceedings of the ACM first international joint conference on Autonomous agents and multiagent systems , Bologna .", "label": "", "metadata": {}, "score": "50.93616"}
{"text": "Second , the parameter learning is carried out by discriminative frequency estimation , whereby the likelihood information and the prediction error are considered .Thus , a combination of generative and discriminative learning is employed .This method is known to work well in highly correlated spaces ( as in our case ) , to converge quickly , and not to suffer from overfitting .", "label": "", "metadata": {}, "score": "50.94616"}
{"text": "The frequencies of the four main classes on the chunk level are given in Table 4 .Our word - based labelling makes it possible to try out different types and sizes of chunks .The other way round would be to attribute the same label to a word that the chunk it belongs to has been annotated with .", "label": "", "metadata": {}, "score": "51.085487"}
{"text": "( B )The effect of the time factor reveals a temporal build - up observed in human detection of saliency .Interaction of time with pitch and intensity are shown .The significance levels corresponding to these plots can be found in Table 2 .", "label": "", "metadata": {}, "score": "51.134125"}
{"text": "M. Valstar , M. Pantic , and I. Patras , \" Motion history for facial action detection in video , \" in Proceedings of the IEEE International Conference on Systems , Man and Cybernetics ( SMC ' 04 ) , vol .", "label": "", "metadata": {}, "score": "51.163643"}
{"text": "Thereby , we are avoiding segmentation errors for EC , as we do for SC , in both cases assuming a 100 % correct segmentation ; this can be considered as an upper bound for classification performance .Note that in preliminary experiments , we found out that Hidden Markov Models trained on EC obtained on the training set lead to a segmentation of the test data that achieves a classification performance comparable to the SC approach .", "label": "", "metadata": {}, "score": "51.174988"}
{"text": "Additionally , we will investigate whether infants at this young age prioritize conditional statistical information over lexical stress as a cue to word segmentation , consistent with the statistical bootstrapping account .In Experiment 2 , we will investigate whether infants in this age range are capable of learning to use lexical stress as a cue to word segmentation .", "label": "", "metadata": {}, "score": "51.19178"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Pantic and I. Patras , \" Dynamics of facial expression : recognition of facial actions and their temporal segments from face profile image sequences , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B : Cybernetics , vol .", "label": "", "metadata": {}, "score": "51.195095"}
{"text": "The time trend that emerged in the model results for Experiment III .We observe a similar trend as in Figure 2B .( C )We show that as saliency increases , the model produces higher saliency scores .This is along the same lines with human results .", "label": "", "metadata": {}, "score": "51.211304"}
{"text": "Using this corpus we will be able to evaluate the impact of the adapted dialogue management strategies , not only on the performance of the interaction and the subjective experience of the user , but also on the emotional state of the user .", "label": "", "metadata": {}, "score": "51.22354"}
{"text": "The confidence score is provided during the recognition and understanding processes and can be increased by means of confirmation turns .2 : The concept or attribute is activated with a confidence score that is lower than the given threshold .We propose the use of a classification process to predict the user intention following the previous equation .", "label": "", "metadata": {}, "score": "51.23417"}
{"text": "Meanwhile , Maatman et al .[14 ] and Kopp et al .[ 15 ] studied the natural behavior of the agent while the user is speaking .These virtual agents need to generate nonverbal outputs to the conversation partner , and these outputs directly affect the naturalness of the dialog .", "label": "", "metadata": {}, "score": "51.241913"}
{"text": "All results are above chance level ( 25 % correct ) .The chunk - based figures might be more relevant if we have applications in mind , the word - based figures are more balanced .We can see that the early fusion of acoustic and linguistic features pays off , always yielding higher WA and UA .", "label": "", "metadata": {}, "score": "51.416473"}
{"text": "Goto , M. , Hashiguchi , H. , Nishimura , T. , and Oka , R. ( 2003 ) . \"Rwc music database : music genre database and musical instrument sound database , \" in Proceedings of the 4th International Conference on Music Information Retrieval ( ISMIR 2003 ) ( Baltimore , MD ) , 229 - 230 .", "label": "", "metadata": {}, "score": "51.501762"}
{"text": "If the pauses at those lower syntactic boundaries given in Table 2 ( s2 , d2 , v1 , and v2 ) are at least 500 ms long , we insert a chunk boundary as well . s3 and s2 delimit syntactically \" well - formed \" clauses containing a verb ; p3 characterises not - well - formed units , functioning like clauses but without a verb .", "label": "", "metadata": {}, "score": "51.51668"}
{"text": "As such , this information could serve to bias subsequent segmentation of novel words .For this hypothesis to be correct , two conditions must be met that would allow infants to learn a lexical stress pattern by 6 months of age ( H\u00f6hle et al . , 2009 ) .", "label": "", "metadata": {}, "score": "51.522675"}
{"text": "Each sample is given as .Machine Learning Algorithms .To solve the facial expression recognition from its representing feature vector , we employed two machine learning algorithms .In the experimental results ( Section 3 ) , we reported the recognition rates that stem from both algorithms .", "label": "", "metadata": {}, "score": "51.540367"}
{"text": "4441 of Lecture Notes in Computer Science / Lecture Notes in Artificial Intelligence , pp .43 - 56 , Springer , Berlin , Germany , 2007 .View at Google Scholar .M. Streit , A. Batliner , and T. Portele , \" Emotions analysis and emotion - handling subdialogues , \" in SmartKom : Foundations of Multimodal Dialogue Systems , W. Wahlster , Ed . , pp .", "label": "", "metadata": {}, "score": "51.575157"}
{"text": "Figure 6 shows the distribution of synthesized facial points generated from a sample of neutral expression .We used the facial points at the apex of each facial expression , which stem from gradient descent AAM fitting , as a ground truth for ( 13 ) .", "label": "", "metadata": {}, "score": "51.672062"}
{"text": "Work is currently underway on collecting an Australian speech corpus with emotional speech , and the use of the proposed contour - based features will be validated on those data in the future .Further , the choice of using linear approximations to model temporal information within voiced speech segments , while the simplest , may not be optimal and is an avenue for future work .", "label": "", "metadata": {}, "score": "51.678345"}
{"text": "2 , pp .137 - 140 , Hong Kong , 2003 .View at Scopus .S. Lee , A. Potamianos , and S. Narayanan , \" Acoustics of children 's speech : developmental changes of temporal and spectral parameters , \" Journal of the Acoustical Society of America , vol .", "label": "", "metadata": {}, "score": "51.708626"}
{"text": "73 - 84 , 2010 .View at Google Scholar .O. Bu\u00df and D. Schlangen , \" Modelling subutterance phenomena in spoken dialogue systems , \" in Proceedings of Semdial , pp .33 - 41 , 2010 .S. E. Hudson , J. Fogarty , C. G. Atkeson et al . , \" Predicting human interruptibility with sensors : a Wizard of Oz feasibility study , \" in Proceedings of the CHI New Horizons : Human Factors in Computing Systems , pp .", "label": "", "metadata": {}, "score": "51.8323"}
{"text": "73 - 84 , 2010 .View at Google Scholar .O. Bu\u00df and D. Schlangen , \" Modelling subutterance phenomena in spoken dialogue systems , \" in Proceedings of Semdial , pp .33 - 41 , 2010 .S. E. Hudson , J. Fogarty , C. G. Atkeson et al . , \" Predicting human interruptibility with sensors : a Wizard of Oz feasibility study , \" in Proceedings of the CHI New Horizons : Human Factors in Computing Systems , pp .", "label": "", "metadata": {}, "score": "51.8323"}
{"text": "Each facial expression sequence is represented by only one frame , which carries the apex of the expression , and due to the lack of training samples , we and Lucey et al .employed a leave - one - out subject cross - validation ( LOOCV ) strategy .", "label": "", "metadata": {}, "score": "51.875435"}
{"text": "Instead of explicitly building an approach to recognize the facial expressions from their corresponding AUs , one can use directly geometry and appearance features for expression recognition , where those features implicitly encode the aforementioned AUs .By exploring the state - of - the - art approaches for human facial expression recognition , we can sort them into two categories .", "label": "", "metadata": {}, "score": "51.93551"}
{"text": "Table 11 also includes the UAR obtained when the standard parameter contour feature set was used by the system .Table 11 .Top 5 UARs obtained on the Aibo corpus and the corresponding feature set utilised .i .b .", "label": "", "metadata": {}, "score": "51.935997"}
{"text": "Soc .Psychol .View Article .Schuller B , Steidl S , Batliner A : The INTERSPEECH 2009 Emotion Challenge , in INTERSPEECH-2009 .Brighton : ISCA ; 2009:312 - 315 .McAulay R , Quatieri T : Speech analysis / synthesis based on a sinusoidal representation .", "label": "", "metadata": {}, "score": "51.969597"}
{"text": "Pattern Recogn .Lett . doi : 10.1016/j.patrec.2013.11.010 .Kondo , H. M. , Kitagawa , N. , Kitamura , M. S. , Koizumi , A. , Nomura , M. , and Kashino , M. ( 2012 ) .Separability and commonality of auditory and visual bistable perception .", "label": "", "metadata": {}, "score": "52.04294"}
{"text": "IEEE :Piscataway ; 2008:3985 - 3988 .View Article .Steidl S : Automatic Classification of Emotion - Related User States in Spontaneous Children 's Speech .Berlin : Logos Verlag ; 2009 .Banse R , Scherer K : Acoustic profiles in vocal emotion expression .", "label": "", "metadata": {}, "score": "52.067978"}
{"text": ", 2012 ) .The computational framework presented in this study follows the same predictive coding premise to model mechanisms of bottom - up auditory attention .Kalman filtering is a natural fit for modeling such behavior .It provides an online tool for tracking evolution of states of a dynamical system that reflect past behavior and expected trajectory of the system .", "label": "", "metadata": {}, "score": "52.105568"}
{"text": "23 , no . 1 , pp .39 - 46 , 1994 .View at Google Scholar .C. Shan , S. Gong , and P. W. McOwan , \" Beyond facial expressions : learning human emotion from body gestures , \" in Proceedings of the British Machine Vision Conference ( BMVC ' 07 ) , 2007 .", "label": "", "metadata": {}, "score": "52.14443"}
{"text": "Because items with high conditional probabilities are more likely to be real words in the language than groupings with low conditional probabilities , sensitivity to conditional statistical information helps to guide infants toward discovering a set of lexical items .These lexical items , in turn , are likely to follow the predominant phonological characteristics of the lexical forms in the native language ( e.g. , Swingley , 2005 ) .", "label": "", "metadata": {}, "score": "52.170982"}
{"text": "2237 - 2240 , Antwerp , Belgium , August 2007 .View at Scopus . A. Batliner , S. Steidl , B. Schuller , et al . , \" Combining efforts for improving automatic classification of emotional user states , \" in Proceedings of the 1st", "label": "", "metadata": {}, "score": "52.29612"}
{"text": "Linguistic Features .Spoken or written text also carries information on the underlying affective state [ 34 - 36 ] .This is usually reflected in the usage of certain words or grammatical alterations - which means in turn , in the usage of specific higher semantic and pragmatic entities .", "label": "", "metadata": {}, "score": "52.448364"}
{"text": "In Table 8 , we display classification results for cross - unit evaluation , that is , we use different units for the training and for testing partitions .This could only be done for acoustic features because of the unbalanced distribution of linguistic features in the different units .", "label": "", "metadata": {}, "score": "52.470184"}
{"text": "( Year : 1994 ) .Domain - general abilities applied to domain - specific tasks : sensitivity to probabilities in perception , cognition and language .Lingua92 , 105 - 14010.1016/0024 - 3841(94)90339 - 5 .Kirkham N. Z.,Slemmer J. A.,Johnson S. P ..", "label": "", "metadata": {}, "score": "52.488132"}
{"text": "R. Sato , R. Higashinaka , M. Tamoto , M. Nakano , and K. Aikawa , \" Learning decision trees to determine turn - taking by spoken dialogue systems , \" in Proceedings of the ICSLP , pp .861 - 864 , 2002 .", "label": "", "metadata": {}, "score": "52.661488"}
{"text": "R. Sato , R. Higashinaka , M. Tamoto , M. Nakano , and K. Aikawa , \" Learning decision trees to determine turn - taking by spoken dialogue systems , \" in Proceedings of the ICSLP , pp .861 - 864 , 2002 .", "label": "", "metadata": {}, "score": "52.661488"}
{"text": "IEEE : Piscataway ; 2005:147 - 151 .Lugger M , Yang B : An incremental analysis of different feature groups in speaker independent emotion recognition , in Proceedings of the 16th International Congress of Phonetic Sciences , Saarbruecken , August 2007 .", "label": "", "metadata": {}, "score": "52.736378"}
{"text": "When we add the neutral expression as a new class to the expression classifier in the frame - based case , the recognition rate drops to 73.6 % due to additional higher confusions between the subtle expressions with the neutral one .", "label": "", "metadata": {}, "score": "52.752304"}
{"text": "In particular , the model tests the hypothesis that perception tracks the evolution of sound events in a multidimensional feature space , and flags any deviation from background statistics as salient .Predictions from the model corroborate the relationship between bottom - up auditory attention and statistical inference , and argues for a potential role of predictive coding as mechanism for saliency detection in acoustic scenes .", "label": "", "metadata": {}, "score": "52.786453"}
{"text": "[19 ] used time - series kernels to analyze the spatiotemporal process of the facial points , where the points ' movements in 3D space are classified with kernels derived from time - warping similarity measures .Some approaches utilized the texture dynamics for the facial expression recognition [ 20 - 22 ] .", "label": "", "metadata": {}, "score": "52.812336"}
{"text": "In Logic , Language and Computation .Volume 1 .Edited by : Seligman J , Westerstahl D. CSLI Publications , Stanford , CA ; 1996 .Jokinen K , Mc Tear MF : Spoken Dialogue Systems .Morgan and Claypool Publishers , San Rafael , CA ; 2010 .", "label": "", "metadata": {}, "score": "52.864273"}
{"text": "Cabral J , Renals S , Richmond K , Yamagishi J : Glottal spectral separation for parametric speech synthesis , in Proceedings of INTERSPEECH-2008 .Brisbane : ICSA ; 2008:1829 - 1832 .Frohlich M , Michaelis D , Strube HW : SIM - simultaneous inverse filtering and matching of a glottal flow model for acoustic speech signals .", "label": "", "metadata": {}, "score": "52.907707"}
{"text": "[ 16 ] who utilized the motion history inside the face image .Zhu et al .[17 ] used hidden Markov model ( HMM ) along with moment invariants to do facial expression recognition .Zhang and Ji [ 18 ] used dynamic based network to model the temporal behavior of the facial expressions .", "label": "", "metadata": {}, "score": "52.966698"}
{"text": "150 - 174 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . S. Hara , N. Kitaoka , and K. Takeda , \" Estimation method of user satisfaction using n - gram - based dialog history model for spoken dialog system , \" in Proceedings of the LREC , pp .", "label": "", "metadata": {}, "score": "53.047928"}
{"text": "150 - 174 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . S. Hara , N. Kitaoka , and K. Takeda , \" Estimation method of user satisfaction using n - gram - based dialog history model for spoken dialog system , \" in Proceedings of the LREC , pp .", "label": "", "metadata": {}, "score": "53.047928"}
{"text": "10.1016/S0167 - 6393(03)00099 - 2 View Article .Huang R , Ma C : Toward a speaker - independent real - time affect detection system , in 18thInternational Conference on Pattern Recognition ICPR 2006 .Volume 1 .New York : IEEE ; 2006:1204 - 1207 .", "label": "", "metadata": {}, "score": "53.156548"}
{"text": "As in Section 3.1 , we represented each expression sequence by one apex frame , used LOOCV strategy , and employed both machine learning algorithms : SVM and kNN .The recognition results are summarized in a confusion matrix , as shown in Table 4 .", "label": "", "metadata": {}, "score": "53.18744"}
{"text": "This experiment is carried out to set up a method for investigating the performance of geometry - based facial expression recognition approaches in fully automatic frame - based scenarios , where the facial point detector is applied on each frame .This method helps to neutralize the analysis of geometry - based approaches from the performance facial point detectors .", "label": "", "metadata": {}, "score": "53.235928"}
{"text": "This method was chosen since all the estimated parameter contours , particularly the pitch contour , can be directly incorporated without any further processing : . s .t . k .N .V . kf .t .t .", "label": "", "metadata": {}, "score": "53.23656"}
{"text": "These concepts have often been linked to studies of auditory attention , though the causal relationship between attention and representations of regularity is still a matter of debate ( Sussman et al . , 2007 ) .The underlying mechanisms eliciting this negativity have been attributed to a potential role of memory ( Naatanen et al . , 1978 ; Garagnani and Pulvermuller , 2011 ) or caused by neural habituation to repeated stimulation ( May and Tiitinen , 2010 ) .", "label": "", "metadata": {}, "score": "53.263847"}
{"text": "At least for the time being , it seems to us that speech , if available , is advantageous over the other modalities to start with .This is , of course , an assumption that has to be validated or falsified .", "label": "", "metadata": {}, "score": "53.285896"}
{"text": "( iii )The process of geometrical feature extraction should be optimized with respect to the known distributions of point localization errors .Conclusions and Future Work .Several approaches were proposed to do facial expression recognition .These approaches can be grouped into two main categories : geometry and appearance based .", "label": "", "metadata": {}, "score": "53.36329"}
{"text": "987 - 995 , 2007 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. M. Maatman , J. Gratch , and S. Marsella , \" Natural behavior of a listening agent , \" in Proceedings of the Intelligent Virtual Agents , vol .", "label": "", "metadata": {}, "score": "53.407253"}
{"text": "987 - 995 , 2007 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. M. Maatman , J. Gratch , and S. Marsella , \" Natural behavior of a listening agent , \" in Proceedings of the Intelligent Virtual Agents , vol .", "label": "", "metadata": {}, "score": "53.407253"}
{"text": "The setup we used was not ideal for varying orientation or quantifying aeroelastic bending .The data in this paper derive entirely from single feathers , because this is a comparatively simple experimental paradigm .Adding a second feather to the experiments presented here would introduced several additional degrees of freedom to the geometry .", "label": "", "metadata": {}, "score": "53.448433"}
{"text": "Moreover , the performance of the combined system compares very well with the human classification performance .Also , the automatic ( GMM - based ) system that did not make use of any temporal information had an overall classification accuracy of 59.0 % .", "label": "", "metadata": {}, "score": "53.449047"}
{"text": "Detection rates are computed for every possible threshold in the range [ 0 , 1 ] with a step size of 0.001 .The resulting ROC curves of the model ( with weights from training all experimental stimuli simultaneously ) are shown in Figure 6D , along with each subject 's performance as mapped onto the ROC space . saliency model ( Kayser et al . , 2005 ) are : I : 0.91 , II : 0.78 , III : 0.52 ( scores correspond to maximum amplitude of the saliency map , parallel to our definition of the saliency score in this study ) .", "label": "", "metadata": {}, "score": "53.454285"}
{"text": "To the extent that 5-month - olds are familiar with any lexical items , they have likely already begun to identify some acoustic regularities across those forms .As these results demonstrate , exposure to lexical forms that show a consistent acoustic pattern allows infants to use that consistent information as a cue to subsequent word segmentation , a cue that infants did not rely upon in the absence of such exposure .", "label": "", "metadata": {}, "score": "53.45791"}
{"text": "In contrast , if it is a chunk with more than one word , individual words belonging to the very same SC can be attributed to different classes but the combined threshold for the whole SC overrides such differences .Features and Classifiers .", "label": "", "metadata": {}, "score": "53.5541"}
{"text": "4 , pp .437 - 444 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Schuller , A. Batliner , S. Steidl , and D. Seppi , \" Does affect affect automatic recognition of children 's speech ? \" in Proceedings of the 1st", "label": "", "metadata": {}, "score": "53.55459"}
{"text": "Moreover , the 5-month - olds in this experiment are demonstrating sensitivity to conditional statistical information at a younger age than any prior experiment has found sensitivity to language - specific acoustic cues to segmentation , such as lexical stress patterns .", "label": "", "metadata": {}, "score": "53.776474"}
{"text": "Research with both infants and adults is consistent with this prediction .For example , infants accept words from the synthesized speech in English utterances after exposure to a stream of synthesized speech ( Saffran , 2001 ) .Similarly , infants and adults learn labels for novel objects more easily when provided the opportunity to segment the labels from fluent speech ( Graf Estes et al . , 2007 ; Mirman et al . , 2008 ) .", "label": "", "metadata": {}, "score": "53.831802"}
{"text": "Similarly , another appearance based approach suffers from alike confusions with the neutral expression [ 28 ] .Binghamton University 3D Facial Expression Database ( BU-4DFE ) [ 41 ] .To assess the effectiveness of our approach , we evaluated it on a second database ( BU-4DFE database ) to recognize the six basic facial expressions , plus neutral .", "label": "", "metadata": {}, "score": "53.845848"}
{"text": "Cases without an MV were mapped onto R(est ) as well .The confusion matrices and the subsequent one- and two - dimensional plots based on Nonmetric Multidimensional Scaling ( NMDS ) in [ 14 ] , both for labelling correspondences and for confusion matrices based on automatic classification , corroborate these mappings .", "label": "", "metadata": {}, "score": "53.899216"}
{"text": "In Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue .Antwerp , Belgium ; 2007:124 - 131 .Griol D , Callejas Z , L\u00f3pez - C\u00f3zar R : A comparison between dialog corpora acquired with real and simulated users .", "label": "", "metadata": {}, "score": "53.940445"}
{"text": "The paper closes with concluding remarks in Section 7 .Database and Annotation .The general frame for our FAU Aibo Emotion Corpus is human - robot communication , children 's speech , and the elicitation and subsequent recognition of emotional user states .", "label": "", "metadata": {}, "score": "53.983727"}
{"text": "12 , pp .1760 - 1774 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Blomberg and D. Elenius , \" Collection and recognition of children 's speech in the PF - Star project , \" in Proceedings of the Swedish Phonetics Conference ( Fonetik ' 00 ) , pp .", "label": "", "metadata": {}, "score": "54.04229"}
{"text": "While the first open comparative challenges in the field of paralinguistics targeted more \" conventional \" phenomena such as emotion , age , and gender , there still exists a multiplicity of not yet covered , but highly relevant speaker states and traits .", "label": "", "metadata": {}, "score": "54.074097"}
{"text": "Burkhardt F , van Ballegooy M , Engelbrecht KP , Polzehl T , Stegmann J : Emotion detection in dialog systems -- usecases , strategies and challenges .In Proceedings of International Conference on Affective Computing and Intelligent Interaction ( ACII 09 ) .", "label": "", "metadata": {}, "score": "54.0754"}
{"text": "Graf Estes K.,Alibali M. W.,Evans J. L.,Saffran J. R ..( Year : 2007 ) .Can infants map meaning to newly segmented words ?Statistical segmentation and word learning .Psychol .Sci.18 , 254 - 26010.1111/j.1467 - 9280.2007.01885 .", "label": "", "metadata": {}, "score": "54.107147"}
{"text": "The challenge faced by infants is comparable to the task faced by adults attempting to identify words spoken in a foreign language .Nevertheless , infants succeed in this task before they have amassed a large lexicon of familiar word forms ( e.g. , Jusczyk and Aslin , 1995 ; Bortfeld et al . , 2005 ) .", "label": "", "metadata": {}, "score": "54.187553"}
{"text": "In other words , each facial expression is concluded by comparing features from face image with those of the same face at the neutral expression [ 13 - 15 ] .These approaches do have limitations such as the human intervention to define the neutral expression of the considered person .", "label": "", "metadata": {}, "score": "54.23492"}
{"text": "Further , they are able to learn a trochaic bias through exposure to a set of words that follow a consistent trochaic pattern .This is also consistent with the hypothesis that segmenting word forms via a domain - general process such as statistical learning is potential mechanism by which infants can develop language - specific acoustic biases ( e.g. , Thiessen and Saffran , 2007 ; Thiessen and Erickson , in press ) .", "label": "", "metadata": {}, "score": "54.275654"}
{"text": "However , a standard feature set will be given per corpus that may be used .Participants may report on results obtained on the development set , but have only a limited number of five trials to upload their results on the test set , whose labels are unknown to them .", "label": "", "metadata": {}, "score": "54.294804"}
{"text": "10.1016/j.specom.2008.04.004 View Article .Callejas Z , L\u00f3pez - C\u00f3zar R : Improving acceptability assessment for the labeling of affective speech corpora .In Proceedings of 10 Annual Conference of the International Speech Communication Association ( Interspeech 09 ) .", "label": "", "metadata": {}, "score": "54.35103"}
{"text": "As expected , the recognition rate is adversely affected by increasing the facial point localization error .The next step in our research is to generalize the facial expression recognition to non - frontal face poses .Combining geometry and appearance - based approaches deserves also more attention .", "label": "", "metadata": {}, "score": "54.357735"}
{"text": "Our proposed approach follows the second category idea , by not benefiting from prior knowledge about person - specific neutral state or any temporal information .We infer the facial expressions from features utilizing the location of just 8 facial points inside a bounding rectangle around the detected face .", "label": "", "metadata": {}, "score": "54.383854"}
{"text": "Such a system was constructed and its performance was evaluated on the LDC corpus .The classification accuracies obtained are reported in Table 8 .This system uses the S - I description for pitch contours and the midpoint value ( MP ) description for glottal and vocal parameter contours , i.e. the feature vector corresponding to the i th voiced segment is given by .", "label": "", "metadata": {}, "score": "54.434"}
{"text": "III .In one case , the effect of pitch on perceived saliency is found to depend on the length of build - up ( Figure 2B ) .The complete high - level interactions can be found in Table 2 , corroborating the importance of timing of events for auditory saliency .", "label": "", "metadata": {}, "score": "54.434372"}
{"text": "The off - the - shelf facial point detectors do not provide hand - annotated accuracy for localizing facial parts in images .And to neutralize the recognition rate analysis from using a specific facial point detector , we provide a method to synthesize facial expression data with different uncertainties matching the errors from the detection of the used facial points .", "label": "", "metadata": {}, "score": "54.441193"}
{"text": "The results are obtained with the help of SVM classifier .It is not applicable to just classify images into the six basic expressions without automatically recognizing the neutral expression , which is a pitfall of the approaches that use annotated prior knowledge of person - specific neutral expression .", "label": "", "metadata": {}, "score": "54.442623"}
{"text": "Tao J , Kang Y : Features importance analysis for emotional speech classification , in Affective Computing and Intelligent Interaction .Springer Berlin , Heidelberg , 2005 : ed . by J Tao et al ; 2005:449 - 457 .Rui S , Moore E : JF Torres , Investigating glottal parameters for differentiating emotional categories with similar prosodics , in IEEE International Conference on Acoustics , Speech and Signal Processing , ICASSP 2009 .", "label": "", "metadata": {}, "score": "54.482204"}
{"text": "Also , the annotated corpus , augmented with new dialogues , will offer us the possibility to employ stochastic approaches for optimized dialogue strategies tailored to the user mental states .Moreover , we are interested in studying how to evaluate and optimize the proposed mental - state simulator .", "label": "", "metadata": {}, "score": "54.486122"}
{"text": "The higher units \" syntactic chunk \" and \" emotion / ememe chunk \" introduced in this study are , in our opinion , representative for two different types of most promising units .However , a great variety of different thresholds or mapping procedures can be imagined .", "label": "", "metadata": {}, "score": "54.49299"}
{"text": "L\u00f3pez - C\u00f3zar R , Callejas Z , McTear MF : Testing the performance of spoken dialogue systems by means of an artificially simulated user .Artif Intell Rev 2006 , 26 ( 4):291 - 323 .10.1007/s10462 - 007 - 9059 - 9 View Article .", "label": "", "metadata": {}, "score": "54.519028"}
{"text": "The accuracies reported are the means of the seven trials .Classification experiments were performed using the HMM - based system using features based on pitch contour , glottal parameter contours and formant contours individually .In both descriptions the length of each voiced segment was appended as outlined in Section 6.1 .", "label": "", "metadata": {}, "score": "54.53009"}
{"text": "The structure of the proposed facial expression recognition approach is pictured in Figure 1 .First , a human face is detected inside the input image .To cope with deficiencies in facial point localization , we project the facial points onto facial point subspaces with the help of a trained PDM .", "label": "", "metadata": {}, "score": "54.581596"}
{"text": "These cues are likely to work together in natural languages , but an open developmental question is which is available to infants earlier in development .In this series of experiments , we will examine the hypothesis that sensitivity to conditional structure is available from an earlier age , and that statistical learning helps infants discover the predominant prosodic structure of words in their native language .", "label": "", "metadata": {}, "score": "54.60511"}
{"text": "However , Reddig ( Reddig , 1978 ) did produce strong harmonics from snipe rectrices in this manner , contra to the whistle hypothesis .One cause of the discrepancy may be the fast Fourier transform ( FFT ) window size of 1024 samples that van Casteren and colleagues ( van Casteren et al . , 2010 ) used to analyze their data .", "label": "", "metadata": {}, "score": "54.61551"}
{"text": "x20636696 .Weber C.,Hahne A.,Friedrich M.,Friederici A. D ..( Year : 2005 ) .Reduced stress pattern discrimination in 5-month - olds as a marker of risk for later language impairment : neurophysiological evidence .Brain Res .Cogn .", "label": "", "metadata": {}, "score": "54.64962"}
{"text": "The \" Evaluation methodology \" section describes the methodology used to evaluate the proposal , whereas in \" Evaluation results\"we discuss the evaluation results obtained by comparing the initial UAH system with an enhanced version of if that adapts its behaviour to the perceived user mental state .", "label": "", "metadata": {}, "score": "54.649933"}
{"text": "These results support our prediction that 5-month - olds should rely on conditional statistical information over lexical stress , as do 7-month - olds infants ( Thiessen and Saffran , 2003 ) .This is consistent with proposal that use of statistical cues to segment speech develops earlier than use of acoustic cues such as lexical stress .", "label": "", "metadata": {}, "score": "54.69673"}
{"text": "M. Garrett , T. Bever , and J. Fodor , \" The active use of grammar in speech perception , \" Perception and Psychophysics , vol .1 , pp .30 - 32 , 1966 .View at Google Scholar Abstract .", "label": "", "metadata": {}, "score": "54.714066"}
{"text": "Regularities within each feature dimension are then tracked used a Kalman - filter to make predictive inferences about deviations from ongoing statistics in that corresponding feature .Detected deviants are boosted according to interaction weights learned using the experimental stimuli , then integrated across feature dimensions to yield an overall saliency estimate of the entire auditory scene .", "label": "", "metadata": {}, "score": "54.877663"}
{"text": "The same paradigm is used in each experiment , with different modalities of stimuli ( musical tones , bird sounds , speech ) .Short sound clips containing temporally overlapping tokens of sound ( e.g. , musical note , word ) varying in a small range of feature parameters form the scene 's \" background .", "label": "", "metadata": {}, "score": "54.891563"}
{"text": "Allen , E. J. , and Oxenham , A. J. ( 2013 ) .\" Interactions of pitch and timbre : how changes in one dimension affect discrimination of the other , \" in Abstracts of the 36th ARO Mid - Winter meeting : Association of Research Otolaryngologists , Vol . 36 ( Mt. Royal , NJ ) .", "label": "", "metadata": {}, "score": "54.904984"}
{"text": "Image Process .doi : 10.1109/TIP.2004.838707 .Bendixen , A. , SanMiguel , I. , and Schroger , E. ( 2012 ) .Early electrophysiological indicators for predictive processing in audition : a review .Psychophysiology 83 , 120 - 131 .", "label": "", "metadata": {}, "score": "54.956104"}
{"text": "j .max . k . [ . s . s . ] x .j .t . k . ) w ij are the asymmetric interaction weights between feature i and feature j that we want to find the optimal values of .", "label": "", "metadata": {}, "score": "54.968315"}
{"text": "However , the frequency domain parameters can be obtained by fitting the linearly stylised spectrum as outlined , which is conceptually simpler than the time domain curve fitting methods required to estimate the time domain parameters .Parameter contours .As previously mentioned , the study aims to capture short - term temporal information in the front - end prior to modelling longer - term information with the back - end .", "label": "", "metadata": {}, "score": "54.974644"}
{"text": "10.1016/j.cogbrainres.2004.08.004 View Article .Lourens T , van Berkel R , Barakova E : Communicating emotions and mental states to robots in a real time parallel framework using Laban movement analysis .Robotics Auton Syst 2010 , 58 : 1256 - 1265 .", "label": "", "metadata": {}, "score": "54.997997"}
{"text": "Pitch is extracted from a harmonicity analysis of spectrogram spectral slices , following a template matching approach ( Shamma and Klein , 2000 ; Walker et al . , 2011 ) .Only pitch estimates with a good match to the template are retained , and further smoothed using a median filter with a 5-sample window .", "label": "", "metadata": {}, "score": "55.039036"}
{"text": "We defined the feature set including the discrete face orientation feature as set ( a ) .On the other hand , the continuous face orientation feature is calculated by the previously explained image processing and compressed by PAA .The feature set including the continuous face orientation feature is defined as set ( b ) .", "label": "", "metadata": {}, "score": "55.08516"}
{"text": "Sobol - Shikler T : Automatic inference of complex affective states .Comput Speech Lang 2011 , 25 : 45 - 62 .10.1016/j.csl.2009.12.005 View Article .Schuller B , Batliner A , Steidl S , Seppi D : Recognising realistic emotions and affect in speech : state of the art and lessons learnt from the first challenge .", "label": "", "metadata": {}, "score": "55.157978"}
{"text": "As expected , an increase in MEr results in lowering the recognition rate .Figure 6 : The process of generating the eight facial points with uncertainty to simulate the errors in the point detectors .The upper left image represents a sample of 2D normal distribution .", "label": "", "metadata": {}, "score": "55.219418"}
{"text": "Acquisition of dialogues with recruited users for the evaluation of our proposal .Evaluation metrics .To compare the baseline and mental - state versions of the UAH system ( with both the simulated and recruited users ) we computed the mean value for the evaluation measures shown in Table 3 , which we extracted from different studies [ 56 - 58 ] .", "label": "", "metadata": {}, "score": "55.232445"}
{"text": "The charm of such an approach is that it is relatively easy to find a word , and where it begins and where it ends .In other modalities , it is way more difficult to delimit units .In this paper , we want to pursue different emotion units based on speech .", "label": "", "metadata": {}, "score": "55.277218"}
{"text": "As they themselves claim , the authors ' approach has no basis on the communication theory .Rather , the mental state stores and prioritizes features which are used for action selection .However , in spoken dialogue systems it is necessary to establish the relationship between mental states and the communicative acts .", "label": "", "metadata": {}, "score": "55.326378"}
{"text": "( a ) Estimated F 0 contour .( b )Linear approximation of F 0 contour .( c )Linear model parameters - \u03c4 , s and b ( or m ) .A small variation to the representation of the linear approximation to the contours involves the use of the value of the midpoint ( m ) of the contour instead of the initial values .", "label": "", "metadata": {}, "score": "55.332092"}
{"text": "Figure 6 .Comparisons of human and model results based on saliency ratings and detection performance .( A ) Correlation between averaged model saliency scores and human saliency ratings shown for all experiments .Averaging is performed between repeated experimental cases , and also between subjects for the human ratings .", "label": "", "metadata": {}, "score": "55.333076"}
{"text": "There are many different syntactic theories yielding different representations of deep structure .However , we resort to a shallow , surface structure , thus , neutralising many of these differences .We are dealing not with syntactically well - formed speech but with spontaneous , natural speech - this will be the rule and not the exception if we aim at applications in real - life scenarios .", "label": "", "metadata": {}, "score": "55.41153"}
{"text": "After that , the eight fiducial points were detected in the first frame ( neutral expression ) with the help of Valstar et al .approach [ 42 ] and then tracked using a dense optical flow tracking algorithm [ 43 ] in the rest sequence .", "label": "", "metadata": {}, "score": "55.413773"}
{"text": "Lee C , Narayanan S , Pieraccini R : Combining acoustic and language information for emotion recognition , in Seventh International Conference on Spoken Language Processing .Denver : September ; 2002:873 - 876 .Schuller B , Batliner A , Steidl S , Seppi D : Emotion recognition from speech : putting ASR in the loop , in Proceedings of IEEE International Conference on Acoustics , Speech and Signal Processing , ICASSP 2009 .", "label": "", "metadata": {}, "score": "55.443607"}
{"text": "The first row in each expression represents results using SVM classifier .The other row shows the results using KNN classifier .A number of points can be drawn from this matrix ; the happiness and surprise expressions are still recognized with high rate of 98.55 % and 98.75 % in the case of SVM and 92.75 % and 98.75 % in kNN case , respectively .", "label": "", "metadata": {}, "score": "55.522545"}
{"text": "Of course , we can not simply transfer this empirically obtained estimate onto Automatic Emotion Recognition ( AER ) .Yet we definitely will have to deal with a plainly lower classification performance .However , this constitutes the last step before AER , including full ASR and automatic segmentation , really can be used \" in the wild , \" that is , in real applications .", "label": "", "metadata": {}, "score": "55.543976"}
{"text": "10.1007/978 - 3 - 540 - 87391 - 4_78 View Article .Callejas Z , L\u00f3pez - C\u00f3zar R : Influence of contextual information in emotion annotation for spoken dialogue systems .Speech Commun 2008 , 50 ( 5):416 - 433 .", "label": "", "metadata": {}, "score": "55.545677"}
{"text": "Figure 7 .Summary of interaction weights that emerge from training the computational model .The model is trained using the same stimuli used in the experimental testing .Thicker lines denote higher weights .An arrow between features indicates that the origin feature of the line boosts the effect of the destination of the line .", "label": "", "metadata": {}, "score": "55.686935"}
{"text": "Pittsburgh , USA ; 2006:1822 - 1825 .Callejas Z , L\u00f3pez - C\u00f3zar R , \u00c1balos N , Griol D : Affective conversational agents : the role of personality and emotion in spoken interactions .In Conversational Agents and Natural Language Interaction : Techniques and Effective Practices .", "label": "", "metadata": {}, "score": "55.802963"}
{"text": "View at Google Scholar .2 , pp .2253 - 2256 , Antwerp , Belgium , August 2007 .View at Scopus .B. Schuller , S. Steidl , and A. Batliner , \" The Interspeech 2009 emotion challenge , \" in Proceedings of the 10th Annual Conference of the International Speech Communication Association ( Interspeech ' 09 ) , pp .", "label": "", "metadata": {}, "score": "55.82601"}
{"text": "10.1016/j.jad.2005.02.007 View Article .Osatuke K , Stiles WB : Relationship between mental states in depression : The assimilation model perspective .Psychiatry Res 2010 , in press .Batliner A , Burkhardt F , van Ballegooy M , N\u00f6th E : A taxonomy of applications that utilize emotional awareness .", "label": "", "metadata": {}, "score": "55.827675"}
{"text": "However , this state - of - the - art geometry - based approach exploits features derived from 68 facial points and requires prior knowledge of the person - specific neutral expression .The expression recognition rate using geometrical features is adversely affected by the errors in the facial point localization , especially for the expressions with subtle facial deformations .", "label": "", "metadata": {}, "score": "55.892006"}
{"text": "Unlike many other geometry - based approaches , the frame - based method does not rely on prior knowledge of a person - specific neutral expression ; this knowledge is gained through human intervention and not available in real scenarios .Additionally , we provide a method to investigate the performance of the geometry - based approaches under various facial point localization errors .", "label": "", "metadata": {}, "score": "55.97498"}
{"text": "Of course , automatic procedures have to adapt to this specific group - children 's speech is a challenge for an Automatic Speech Recognition ( ASR ) system [ 18 , 19 ] , as both acoustic and linguistic characteristics differ from those of adults [ 20 ] .", "label": "", "metadata": {}, "score": "55.985046"}
{"text": "Therefore , we conducted a 9-fold validation test , in which the amounts of training data and test data of each fold were unequal .Feature Set .We prepared two feature sets for the discrimination because we have investigated both \" discrete \" and \" continuous \" face orientation features .", "label": "", "metadata": {}, "score": "56.0131"}
{"text": "Table 10 .Summary of overall accuracies for systems evaluated on FAU Aibo corpus .It can be seen that the trends observed in the LDC corpus match the ones in these results .Similar to the results in Table 7 , systems that make use of temporal information ( HMM - based ) outperform the static system ( GMM ) when pitch and glottal parameters are considered , but not when vocal tract parameters are considered .", "label": "", "metadata": {}, "score": "56.052536"}
{"text": "However , the Kalman formulation remains a linearized approximation of the dynamic behavior of acoustic scenes .The use of predictive coding in the model takes a different direction from common modeling efforts of saliency in other modalities , particularly in vision .", "label": "", "metadata": {}, "score": "56.066223"}
{"text": "The dialogue manager was written in Java embedded in a standard VoiceXML application enhanced with ECMAScript .In our proposal , we employ statistical techniques for inferring user acts , which makes it easier porting it to different application domains .Also the proposed architecture is modular and thus makes it possible to employ different emotion and intention recognizers , as the intention recognizer is not linked to the dialogue manager as in the case of Pitterman et al .", "label": "", "metadata": {}, "score": "56.08495"}
{"text": "Initially , mental states were reduced to a representation of the information that an agent or system holds internally and it uses to solve tasks .Following this approach , Katoh et al .[15 ] proposed to use mental states as a basis to decide whether an agent should participate in an assignment according to its self - perceived proficiency in solving it .", "label": "", "metadata": {}, "score": "56.11234"}
{"text": "Thiessen and Saffran ( 2007 ) suggest that this developmental progression is due to statistical learning .Statistical learning plays two roles in this progression .The first , as demonstrated in Experiment 1 , is that infants are able to use conditional statistical information to extract a set of lexical forms from fluent speech .", "label": "", "metadata": {}, "score": "56.153904"}
{"text": "The effect of similarity means that when information is presented , the most similar stored exemplars are most activated and have the greatest influence on the response to the current information .These processes can account for a wide variety of distributional learning phenomena , including category learning , acquired distinctiveness , and the role of variability in facilitating learning of non - adjacent relations ( Thiessen and Pavlik , 2012 ) .", "label": "", "metadata": {}, "score": "56.161263"}
{"text": "The results show that the improved version of the system performs better in terms of duration of the dialogues , number of turns needed to succeed in the dialogue and number of confirmations and repetitions needed .Additionally , the users judged the system to be better when it could adapt its behaviour to their mental state .", "label": "", "metadata": {}, "score": "56.215717"}
{"text": "Speech Commun 1996 , 20 ( 2):151 - 170 .10.1016/S0167 - 6393(96)00050 - 7 View Article .Ververidis D , Kotropoulos C : Emotional speech recognition : resources , features and methods .Speech Commun 2006 , 48 : 1162 - 1181 .", "label": "", "metadata": {}, "score": "56.237137"}
{"text": "A simple yet popular method for reducing the vocabulary is exploited : a minimum training database word frequency , here two , determines the necessary minimum number of occurrences for a word in the database for being part of the vocabulary .", "label": "", "metadata": {}, "score": "56.244938"}
{"text": "In the \" Background \" section we describe the motivation of our proposal and related work .The section entitled \" New model for predicting the user mental state \" presents in detail the proposed model and how it can be included into the architecture of a spoken dialogue system .", "label": "", "metadata": {}, "score": "56.25878"}
{"text": "Moriyama T , Ozawa S : Emotion recognition and synthesis system on speech , in IEEE International Conference on Multimedia Computing and Systems , 1999 vol 1 . 1st edition .New York : IEEE ; 1999:840 - 844 .Sethu V , Ambikairajah E , Epps J : Pitch contour parameterisation based on linear stylisation for emotion recognition , in Proceedings of the 10th Annual Conference of the International Speech Communication Association ( INTERSPEECH-2009 ) .", "label": "", "metadata": {}, "score": "56.341064"}
{"text": "10.1109/TASSP.1986.1164910 View Article .Talkin D : A robust algorithm for pitch tracking ( RAPT ) , in Speech Coding and Synthesis .Elsevier , New York : by W Kleijn , K Paliwal ; 1995:495 - 518 .Nwe TL , Foo SW , De LC : Silva , Speech emotion recognition using hidden Markov models .", "label": "", "metadata": {}, "score": "56.370945"}
{"text": "Note that the user may make utterances other than the answer , such as filler words or interjections , which could provide clues as to the user 's internal state .Speech - Based Features .The Length until User 's Input .", "label": "", "metadata": {}, "score": "56.426674"}
{"text": "Feature Set ( a )The values correspond to the nine face orientations ( see Figure 11 ) expressed as a nine - dimensional vector for the feature of each frame .Feature Set ( b ) Feature set ( b ) includes the continuous face orientation value as described in Section 4.2 .", "label": "", "metadata": {}, "score": "56.426853"}
{"text": "doi : 10.1016/j.neuron.2005.08.039 .Rahne , T. , and Sussman , E. ( 2009 ) .Neural representations of auditory input accommodate to the context in a dynamically changing acoustic environment .Eur .J. Neurosci .doi : 10.1111/j.1460 - 9568.2008.06561.x .", "label": "", "metadata": {}, "score": "56.497192"}
{"text": "Our recognition method , based on the previous work described in [ 44 ] , firstly takes acoustic information into account to distinguish between the emotions which are acoustically more different , and secondly dialogue information to disambiguate between those that are more similar .", "label": "", "metadata": {}, "score": "56.504448"}
{"text": "Only contributions with an accepted paper will be eligible for the Challenge participation .The organisers preserve the right to re - evaluate the findings , but will not participate themselves in the Challenge .Participants are encouraged to compete in both Sub - Challenges .", "label": "", "metadata": {}, "score": "56.531075"}
{"text": "V. A. Petrushin , \" Emotion recognition in speech signal : experimental study , development , and application , \" in Proceeding of the 6th International Conference on Spoken Language Processing ( ICSLP ' 00 ) , pp .222 - 225 , 2000 .", "label": "", "metadata": {}, "score": "56.581337"}
{"text": "Cortex 22 , 1915 - 1922 .doi : 10.1093/cercor / bhr266 .Li , J. , Levine , M. D. , An , X. , Xu , X. , and He , H. ( 2012 ) .Visual saliency based on scale - space analysis in the frequency domain .", "label": "", "metadata": {}, "score": "56.598198"}
{"text": "( Year : 2009 ) .Statistical language learning in neonates revealed by event - related brain potentials .BMC Neurosci.10:2110.1186/1471 - 2202 - 10 - 2119284661 .Thiessen E. D.,Saffran J. R ..( Year : 2003 ) .When cues collide : use of stress and statistical cues to word boundaries by 7- to 9-month - old infants .", "label": "", "metadata": {}, "score": "56.60746"}
{"text": "793 - 796 , Pittsburgh , Pa , USA , 2006 .View at Scopus .J. B. Lovins , \" Development of a stemming algorithm , \" Mechanical Translation and Computational Linguistics , vol .11 , pp .22 - 31 , 1968 .", "label": "", "metadata": {}, "score": "56.649673"}
{"text": "Evanini K , Hunter P , Liscombe J , Suendermann D , Dayanidhi K , Pieraccini R : Caller experience : a method for evaluating dialog systems and its automatic prediction .In Proceedings of the 2008 Spoken Language Technology Workshop ( SLT 08 ) .", "label": "", "metadata": {}, "score": "56.65089"}
{"text": "In this work , we investigate the ability of perceiving human facial expression using geometrical features without any prior knowledge of person - specific neutral expression , since the neutral expression is usually manually annotated .Disregarding the offered annotated neutral expression ( offered by most databases ) is a step forward in the direction of fully automatic facial expression recognition .", "label": "", "metadata": {}, "score": "56.662003"}
{"text": "Once infants have discovered a set of words , they can identify language - specific acoustic cues by taking advantage of distributional information about those word forms ( Thiessen and Saffran , 2007 ; Lew - Williams and Saffran , 2012 ) .", "label": "", "metadata": {}, "score": "56.68946"}
{"text": "Results of the high - level dialogue features defined for the comparison of the mental - state and UAH baseline systems .As can be observed , on the one hand the success rate for the mental - state system is higher than the baseline .", "label": "", "metadata": {}, "score": "56.713207"}
{"text": "Interactions .An interaction between multiple factors indicates that the effect of one factor changes according to the levels of the others .Within - subjects ANOVA results , outlining the interactions from all experiments , are shown in Table 1 .", "label": "", "metadata": {}, "score": "56.71401"}
{"text": "Build - up of the tendency to segregate auditory streams : resetting effects evoked by a single deviant tone .J. Acoust .Soc .Am .doi : 10.1121/1.3488675 .Kim , K. , Lin , K.-H. , Walther , D. B. , Hasegawa - Johnson , M. A. , and Huang , T. S. ( 2014 ) .", "label": "", "metadata": {}, "score": "56.71791"}
{"text": "They found that emotional information can be useful to improve the dialogue strategies and predict system errors , but it was not employed in their system to adapt dialogue management .Boril et al .[36 ] measured speech production variations during the interactions of drivers with commercial automated dialogue systems .", "label": "", "metadata": {}, "score": "56.812393"}
{"text": "focused on the features of filled pauses such as little F0 and spectrum variation and formulated the filled pause likelihood .Here , the value of the F0 transition and the spectral envelope deformation at frame .is the product of the slope and the fitting error of the temporal transition of the linearized spectral envelope .", "label": "", "metadata": {}, "score": "56.836723"}
{"text": "Note that in this paper , we used the spoken word chain simulating 100 % correct word recognition , and a manual segmentation into SC and EC .For a fair comparison between SC and EC , this had to be done automatically .", "label": "", "metadata": {}, "score": "56.912155"}
{"text": "Thus , the emotion - related states we are dealing with have to be taken as \" perceived \" surface phenomena , at face value - at least as long as we do not employ physiological measurements , trying to find out a real ground truth .", "label": "", "metadata": {}, "score": "56.94045"}
{"text": "In this vein , we will employ both acoustic and linguistic features for the automatic classification of emotion - related user states .The Need for Segmentation .In this paper , we want to address different possibilities to segment emotional episodes .", "label": "", "metadata": {}, "score": "56.97487"}
{"text": "Williams JD , Young S : Partially observable Markov decision processes for spoken dialogue systems .Comput Speech Lang 2007 , 21 : 393 - 422 .10.1016/j.csl.2006.06.008 View Article .L\u00f3pez - C\u00f3zar R , Callejas Z , Kroul M , Nouza J , Silovsk\u00fd J : Two - level fusion to improve emotion classification in spoken dialogue systems .", "label": "", "metadata": {}, "score": "57.06057"}
{"text": "However , if infants learn that lexical stress is a cue to word - initial position , they may begin to use lexical stress as a cue to word segmentation ( e.g. , Johnson and Jusczyk , 2001 ) .If so , infants should segment different items from the trochaic segmentation stream than from the iambic segmentation stream , and show a different pattern of preference at test after exposure to these two languages .", "label": "", "metadata": {}, "score": "57.069572"}
{"text": "Working notes of the AAAI Spring Symposium on Reasoning about Mental States : Formal Theories and Applications 1993 , 143 - 149 .Nisimura R , Omae S , Kawahara H , Irino T : Analyzing dialogue data for real - world emotional speech classification .", "label": "", "metadata": {}, "score": "57.33652"}
{"text": "At this point constant parameterisation is preferred over individual optimisation ; thus , no alterations are undertaken with respect to number of iterations , quantisation , and so forth , among the different units and feature types to be classified .Table 7 : Evaluation in percent correct ; ( un-)weighted average recall ( UA / WA ) .", "label": "", "metadata": {}, "score": "57.381477"}
{"text": "251 - 253 , San Diego , Calif , USA , 2005 . F. de Rosis , A. Batliner , N. Novielli , and S. Steidl , \" ' You are sooo cool , Valentina .recognizing social attitude in speech - based dialogues with an ECA , \" in Affective Computing and Intelligent Interaction , A. Paiva , R. Prada , and R. W. Picard , Eds . , pp .", "label": "", "metadata": {}, "score": "57.419167"}
{"text": "This is a well - known numerical representation form of text in automatic document categorisation introduced in [ 38 ] .It has been successfully ported to recognise sentiments in [ 39 ] or emotion and interest in [ 40 ] .", "label": "", "metadata": {}, "score": "57.449265"}
{"text": "Listening tests were conducted to validate the use of linear approximations in this context .Automatic emotion classification experiments were carried out on the Linguistic Data Consortium emotional prosody speech and transcripts corpus and the FAU Aibo corpus to validate the proposed approach .", "label": "", "metadata": {}, "score": "57.483456"}
{"text": "The emotional categories assigned to each word in the chunk by all five listeners were combined heuristically to generate a single emotion label for the chunk .Validating linear approximations - listening test .Listening tests were conducted to determine whether linear approximations to pitch contour segments and glottal parameter contour segments retained sufficient information about the emotions being expressed .", "label": "", "metadata": {}, "score": "57.493717"}
{"text": "Another pioneer work which implemented the concept of mental state was the spoken dialogue system TRAINS-92 [ 9 ] .This system integrated a domain plan reasoner which recognized the user mental state and used it as a basis for utterance understanding and dialogue management .", "label": "", "metadata": {}, "score": "57.494087"}
{"text": "All the answers were assigned a numeric value between one and five ( in the same order as they appear in the questionnaire ) .Evaluation results .Table 4 shows the comparison of the different high - level measures for the mental - state and baseline systems .", "label": "", "metadata": {}, "score": "57.504066"}
{"text": "As speech - based features , we used four features : the length until the user 's input , the length of filler segments , the length of silent segments , and the length of filled pauses .As face orientation features , we used the sequence of three - dimensional face rotation angles and discrete face orientation frequencies .", "label": "", "metadata": {}, "score": "57.5187"}
{"text": "In order to reduce the computational cost required for solving the POMDP problem for dialogue systems in which many emotions and dialogue acts might be considered , the authors employed decision networks to complement POMDP .We propose an alternative to this statistical modelling which can also be used in realistic dialogue systems and evaluate it in a less emotional application domain in which emotions are produced more subtly .", "label": "", "metadata": {}, "score": "57.539665"}
{"text": "However , neither background nor foreground timbre factors have significant effects .Marginal means ( Figure 2A ) confirm that the three instruments are indeed relatively close to each other in timbre space ; as corroborated by published studies of timbre perception ( McAdams et al . , 1995 ) .", "label": "", "metadata": {}, "score": "57.53992"}
{"text": "J. Psychol .doi : 10.1348/000712601162103 .Garagnani , M. , and Pulvermuller , F. ( 2011 ) .From sounds to words : a neurocomputational model of adaptation , inhibition and memory processes in auditory change detection .Neuroimage 54 , 170 - 181 .", "label": "", "metadata": {}, "score": "57.547752"}
{"text": "Despite their complexity , these characteristics are to some extent rather static .Jokinen [ 1 ] identifies a more complex degree of adaptation in which the system adapts to the user 's intentions and state .Most spoken dialogue systems that employ user mental states address these states as intentions , plans or goals .", "label": "", "metadata": {}, "score": "57.60775"}
{"text": ", 2009 ; Kudo et al . , 2011 ) .The goal of Experiment 1A was to provide further behavioral evidence that infants are capable of segmenting fluent speech via conditional statistical information below 6 months .To do so , we exposed 5-month - old infants to an artificial language in which the only cue to segmentation is higher conditional relations between syllables within words relative to syllables spanning word boundaries ( part - words ) .", "label": "", "metadata": {}, "score": "57.66276"}
{"text": "Bui T , Poel M , Nijholt A , Zwiers J : A tractable hybrid DDN - POMDP approach to affective dialogue modeling for probabilistic frame - based dialogue systems .Nat Lang Eng 2009 , 15 ( 2):273 - 307 .", "label": "", "metadata": {}, "score": "57.81468"}
{"text": "[17 ] described that a system response that does not consider the difference between cases 1 and 2 will confuse the user .This problem is especially serious for systems with dialogs that finish in one or two user utterances .", "label": "", "metadata": {}, "score": "57.865334"}
{"text": "This suggests that infants may have more opportunity to learn from statistical information than previously thought .Second , they suggest that sensitivity to statistical information can play an important role in helping infants adapt to the acoustic structure of their native language .", "label": "", "metadata": {}, "score": "57.929573"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .Z. Wang , S. Wang , and Q. Ji , \" Capturing complex spatiotemporal relations among facial muscles for facial expression recognition , \" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ' 13 ) , pp .", "label": "", "metadata": {}, "score": "57.970955"}
{"text": "II : 10 , Exp .III : 10 ) .All experiments have the same set - up : Subjects listen to short sound clips through Sennheiser HD595 headphones in a sound proof booth and answer saliency - related questions on a computer .", "label": "", "metadata": {}, "score": "58.039207"}
{"text": "This excites one or more structural resonance frequencies within the feather , creating stable oscillations ( Bisplinghoff et al ., 1996 ; Mandre and Mahadevan , 2010 ; Manela and Howe , 2009b ) .This hypothesis does not invoke the existence of spatially discrete vortices that form around the feather and drive its motion .", "label": "", "metadata": {}, "score": "58.063095"}
{"text": "More recently , some authors have considered mental states as equivalent to emotional states [ 10 ] , given that affect is an evolutionary mechanism that plays a fundamental role in human interaction to adapt to the environment and carry out meaningful decision making [ 11 ] .", "label": "", "metadata": {}, "score": "58.089485"}
{"text": "To better understand this interaction , we performed planned t -tests comparing listening times to test items in the two conditions .These results indicate that infants show a different preference for test items after listening to the trochaic and iambic languages , as would be expected if they had learned to treat lexical stress as a cue to word segmentation .", "label": "", "metadata": {}, "score": "58.089943"}
{"text": "For this purpose , we employed LIBSVM [ 39 ] .K - Nearest - Neighbor ( kNN ) .kNN classification is one of the simplest classification methods , where a test sample is classified based on the closest previously known samples in the feature space [ 40 ] .", "label": "", "metadata": {}, "score": "58.092804"}
{"text": "Features capturing timbre have complex interactions between themselves depending on the experiment .It is important to note that the overall interactions observed reflect the redundancy in the computational features - e.g . , intensity is encoded , to some extent , in the spectrogram , and thus bandwidth , therefore these features tend to spike together , leading to likely interactions between them .", "label": "", "metadata": {}, "score": "58.11982"}
{"text": "As can be observed , both systems correctly understand the different user queries and obtain a similar evaluation regarding the perceived easiness in correcting errors made by the ASR module .However , the mental - state system has a higher evaluation rate regarding the user observed easiness in obtaining the data required to fulfil the complete set of objectives defined in the scenario , as well as the suitability of the interaction rate during the dialogue .", "label": "", "metadata": {}, "score": "58.171303"}
{"text": "The 5-month - olds in Experiments 1 and 2 are able to segment words from fluent speech via sensitivity to conditional statistical information , opening the possibility that sensitivity to conditional statistical cues plays a role in learning from a very young age ( c.f .", "label": "", "metadata": {}, "score": "58.23154"}
{"text": "Consequently , the proposed system models information on a segment - by - segment scale is larger than a frame - based scale but smaller than utterance level modelling .Specifically , linear approximations to temporal contours of formant frequencies , glottal parameters and pitch are used to model short - term temporal information over individual segments of voiced speech .", "label": "", "metadata": {}, "score": "58.256313"}
{"text": "[21 ] studied the appropriateness of the approaching behavior when addressing people in human - robot dialog .In the present work , we investigate how to estimate the user 's internal state in Phase 2 .We believe that user modeling in Phase 2 is important , even though it has not been investigated to date .", "label": "", "metadata": {}, "score": "58.27523"}
{"text": "2249 - 2252 , Antwerp , Belgium , August 2007 .View at Scopus .J. Su , H. Zhang , C. X. Ling , and S. Matwin , \" Discriminative parameter learning for Bayesian networks , \" in Proceedings of the 25th International Conference on Machine Learning ( ICML ' 08 ) , pp .", "label": "", "metadata": {}, "score": "58.299202"}
{"text": "To determine whether preference for type of test items ( words vs. part - words ) differed as a function of condition ( trochaic vs. iambic language exposure ) , a 2 \u00d7 2 ANOVA ( Test Item \u00d7 Condition ) was performed ( Figure ( 3 ) .", "label": "", "metadata": {}, "score": "58.335148"}
{"text": "Although emotion is gaining increasing attention from the dialogue systems community , most research described in the literature is devoted exclusively to emotion recognition .For example , a comprehensive and updated review can be found in [ 13 ] .In this paper we propose a mental - state prediction method which takes into account both the users ' intentions and their emotions , and describes how to incorporate such a state into the architecture of a spoken dialogue system to adapt dialogue management accordingly .", "label": "", "metadata": {}, "score": "58.3668"}
{"text": "In Proceedings of 10th Annual Conference of the International Speech Communication Association ( Interspeech 09 ) .Brighton , United Kingdom ; 2009:272 - 275 .Griol D , McTear MF , Callejas Z , L\u00f3pez - C\u00f3zar R , \u00c1balos N , Espejo G : A methodology for learning optimal dialog strategies .", "label": "", "metadata": {}, "score": "58.404457"}
{"text": "Griol D , Hurtado LF , Sanchis E , Segarra E : Acquiring and evaluating a dialog corpus through a dialog simulation technique .In Proceedings of the 8th Annual SIGdial Meeting on Discourse and Dialogue .Antwerp , Belgium ; 2007:29 - 42 .", "label": "", "metadata": {}, "score": "58.404823"}
{"text": "London , United Kingdom ; 2009:326 - 332 .Schatzmann J , Georgila K , Young S : Quantitative evaluation of user simulation techniques for spoken dialogue systems .In Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue .Lisbon , Portugal ; 2005:45 - 54 .", "label": "", "metadata": {}, "score": "58.443535"}
{"text": "At the same time , brute - forcing of features ( 1000 up to 50000 ) , for example , by analytical feature generation , partly also in combination with evolutionary generation , has become popular .It seems as if this ( slightly ) outperforms hand - crafted features while the individual worth of automatically generated features seems to be lower .", "label": "", "metadata": {}, "score": "58.475582"}
{"text": "The experimental data was collected on a Wizard of Oz basis , and the labels were decided by five evaluators .Finally , we conducted a discrimination experiment with the trained user model using combined features .As a three - class discrimination result , we obtained about 85 % accuracy in an open test .", "label": "", "metadata": {}, "score": "58.540947"}
{"text": "If infants were able to successfully segment the artificial language , they should respond differentially to word test trials than to part - word test trials ( e.g. , Saffran et al ., 1996 ) .While in principle , any group - level preference is indicative that infants are able to differentiate the items , the experiments most similar to this one have resulted in a novelty preference ( e.g. , Thiessen and Saffran , 2003 , 2007 ) .", "label": "", "metadata": {}, "score": "58.626762"}
{"text": "Width is initialized to 0 and is increased by 1 for each user turn generated to confirm , repeat data or ask the system for help .Once these parameters have been calculated , the emotion recognizer carries out a classification based on thresholds as schematized in Figure 3 .", "label": "", "metadata": {}, "score": "58.643272"}
{"text": "Piscataway ; 2009:4585 - 4588 .View Article .Fant G : Acoustic Theory of Speech Production .The Hague : Mouton ; 1960 .Schuller B , Rigoll G , Lang M : Hidden Markov model - based speech emotion recognition , in Proceedings of International Conference on Acoustics .", "label": "", "metadata": {}, "score": "58.715218"}
{"text": "Therefore , in a future work , we will employ a sequential discrimination method such as HMM or CRF to analyze and select the features .Moreover , the total number of data in our experiment was not enough , so we need to collect more data and confirm the validity of our proposed discrimination method and features .", "label": "", "metadata": {}, "score": "58.733826"}
{"text": "We know , however , that phonetic / psycholinguistic studies on the localisation of nonverbal signals within speech showed that listeners tend to structure the perception of these phenomena along the perception and comprehension of linguistic phenomena ( sentence processing ) [ 53 ] .", "label": "", "metadata": {}, "score": "58.76205"}
{"text": "To evaluate the proposed model for predicting the user mental state discussed in \" New model for predicting the user mental state \" section , we have developed an enhanced version of the UAH system in which we have included the module shown in Figure 1 .", "label": "", "metadata": {}, "score": "58.77787"}
{"text": "Overall , there is again almost no difference between chunk - based and word - based evaluation of UA .Although the figures are of course lower than for the within - unit evaluation displayed in Table 7 , it is reassuring that performance does not break down when we train with EC and test with SC or vice versa .", "label": "", "metadata": {}, "score": "58.86863"}
{"text": "The expression is classified using SVM , which provided better results than kNN in the last two experiments .Similarly , to the last experiment , we used LOOCV strategy , where each test sample is altered by ( 13 ) to generate 1000 new test samples with specific uncertainty .", "label": "", "metadata": {}, "score": "58.894882"}
{"text": "Rather than statistical cues and acoustic cues being in conflict ( as they are artificially placed in the iambic familiarization stream ) , conditional statistical information may actually allow infants to discover the dominant rhythmic patterns of their native language ( Thiessen and Saffran , 2007 ) .", "label": "", "metadata": {}, "score": "58.98558"}
{"text": "H\u00f6hle B.,Bijeljac - Babic R.,Herold B.,Weissenborn J.,Nazzi T ..( Year : 2009 ) .Language specific prosodic preferences during the first half year of life : evidence from German and French infants .Infant Behav .Dev.3 , 262 - 27410.1016/j.infbeh.2009.03.004 .", "label": "", "metadata": {}, "score": "59.07619"}
{"text": "Therefore it describes the performance under perfect speech recognition conditions .This follows the typical reporting of linguistic analysis results in emotion recognition , as it allows for better comparability of results [ 37 ] ; the corpus comes with the transcription , while speech recognition results would differ from site to site .", "label": "", "metadata": {}, "score": "59.121773"}
{"text": "669 - 676 , Singapore , 2005 . A. Batliner , S. Steidl , C. Hacker , and E. N\u00f6th , \" Private emotions versus social interaction : a data - driven approach towards analysing emotion in speech , \" User Modelling and User - Adapted Interaction , vol .", "label": "", "metadata": {}, "score": "59.176155"}
{"text": "This paper describes a method for estimating the internal state of a user of a spoken dialog system before his / her first input utterance .When actually using a dialog - based system , the user is often perplexed by the prompt .", "label": "", "metadata": {}, "score": "59.18811"}
{"text": "1455 - 1468 , 1999 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Holodynski and W. Friedlmeier , Development of Emotions and Emotion Regulation , Springer , New York , NY , USA , 2006 .", "label": "", "metadata": {}, "score": "59.220818"}
{"text": "The original data of this example is the same as the uppermost one in Figure 9 , and we found that the number of data is reduced while retaining the rough deviation of the original signal .Discrimination Experiment .Experimental Method .", "label": "", "metadata": {}, "score": "59.273293"}
{"text": "We employed compressed face orientation data .According to a preliminary experiment , we decided to compress the face orientation vectors into the 15 points ( .Experimental Results .Finally , we carried out an experiment using the features using the above - mentioned feature sets .", "label": "", "metadata": {}, "score": "59.29765"}
{"text": "Materials and methods .Participants .Half of these infants were exposed to a trochaic artificial language , and half to an iambic artificial language .To obtain data from 20 infants , it was necessary to run 29 infants .Stimuli .", "label": "", "metadata": {}, "score": "59.352127"}
{"text": "Visual statistical learning in infancy : evidence for domain general learning mechanism .Cognition83 , B35-B4210.1016/S0010 - 0277(02)00004 - 511869728 .Kudo N.,Nonaka Y.,Mizuno N.,Mizuno K.,Okanoya K ..( Year : 2011 ) .On - line statistical segmentation of a non - speech auditory stream in neonates as demonstrated by event - related brain potentials .", "label": "", "metadata": {}, "score": "59.38471"}
{"text": "However , tracking the orientation of the attentional spotlight in audition is challenging .Kayser et al .( 2005 ) have used a paradigm where they ask subjects to compare which of the two presented sound clips sounds more salient .", "label": "", "metadata": {}, "score": "59.407677"}
{"text": "One or more concepts represented the intention of the utterance , and a sequence of attribute - value pairs contained the information about the values provided by the user .An example of the semantic interpretation of an input sentence is shown in Figure 4 .", "label": "", "metadata": {}, "score": "59.435204"}
{"text": "Cognit.39 , 1085 - 109310.3758/s13421 - 011 - 0074 - 321312017 .Nazzi T.,Dilley L. C.,Jusczyk A. M.,Shattuck - Hufnagle S.,Jusczyk P. W ..( Year : 2005 ) .English - learning infants ' segmentation of verbs from fluent speech .", "label": "", "metadata": {}, "score": "59.459293"}
{"text": "The state - of - the - art geometry - based approaches entail prior - knowledge of person - specific neutral expression ; however , such information is not available in real - world scenarios .In contrast to that , we extract geometrical features from just eight facial points .", "label": "", "metadata": {}, "score": "59.481735"}
{"text": "A sample size of 10 infants was used based on a power analysis using an effect size calculated from Thiessen and Saffran 's ( 2003 )Experiment 3 , of which this experiment is a replication with a younger age group .", "label": "", "metadata": {}, "score": "59.488113"}
{"text": "The goal of our task is to discriminate the internal state of a user among these three States A , B , and C by observing the user 's behavior .This challenge is close to the research on turn - taking of the dialogue .", "label": "", "metadata": {}, "score": "59.61773"}
{"text": "In this paper , we do not preselect a subcorpus out of the whole database but model valence , that is , positive , idle ( neutral ) , and negative , for all the data ; the remaining cases are attributed to a rest class .", "label": "", "metadata": {}, "score": "59.694515"}
{"text": "Therefore , English - learners trochaic bias is likely acquired from experience with the language ( Thiessen and Saffran , 2007 ) .By contrast , sensitivity to conditional statistical information does not require language - specific knowledge ; it is a cue to word segmentation that is available cross - linguistically .", "label": "", "metadata": {}, "score": "59.702934"}
{"text": "Moreover , we use two machine learning algorithms ( SVM and kNN ) to classify the neutral - independent feature vector generated as shown in Section 2.1 .Table 3 shows the confusion matrix of our approach for the six basic expressions , plus neutral expression ( Ne ) for both machine learning algorithms .", "label": "", "metadata": {}, "score": "59.707302"}
{"text": "We propose a model for predicting the user mental state which can be integrated in the architecture of a spoken dialogue system as shown in Figure 1 .As can be observed , the model is placed between the natural language understanding ( NLU ) and the dialogue management phases .", "label": "", "metadata": {}, "score": "59.730667"}
{"text": "For example , Jokinen [ 1 ] describes different levels of adaptation .The simplest one is through personal profiles in which the users make static choices to customize the interaction ( e.g. whether they want a male or female system 's voice ) , which can be further improved by classifying users into preferences ' groups .", "label": "", "metadata": {}, "score": "59.75528"}
{"text": "Yacoub S , Simske S , Lin X , Burns J : Recognition of emotions in interactive voice response systems , in Proceedings of the 8th European Conference on Speech Communication and Technology , EUROSPEECH 2003 - INTERSPEECH 2003 .Geneva September 2003 , 1 - 4 : 729 - 732 .", "label": "", "metadata": {}, "score": "59.77208"}
{"text": "Evidence suggests that young infants and even neonates are sensitive to conditional statistical information ( Kirkham et al . , 2002 ; Teinonen et al ., 2009 ; Kudo et al . , 2011 ) .Further , one prior experiment indicates that 5- to 6-month - old infants are able to segment fluent speech via conditional statistical information ( Johnson and Tyler , 2010 ) .", "label": "", "metadata": {}, "score": "59.83821"}
{"text": "All feathers that we measured exhibited distinct behavior over three ranges of airspeeds ( Fig .3 ; supplementary material Fig .S2 ) .In range I ( low U air ) , the SLDV detected no feather motion above the noise floor of the device .", "label": "", "metadata": {}, "score": "59.85455"}
{"text": "4B tended to exhibit tip modes of vibration for which this artifact was especially strong .We suggest that the negative slopes in Fig .4B are not real , but are entirely attributable to this limitation to the SLDV measurements .", "label": "", "metadata": {}, "score": "59.85737"}
{"text": "Due to this , we used all the \" sessions \" of the dialog data for the later experiment even if the session belonged to Phase 3 .The estimation of the user 's internal state at Phase 3 is expected to be improved by using the contextual information ; however , this issue is not covered here .", "label": "", "metadata": {}, "score": "59.86162"}
{"text": "We formulate the facial expression recognition task as a multiclass learning process , where one class is assigned to each expression .SVM is a well - known classifier for its generalization capability .In the case of a binary classification task with training data .", "label": "", "metadata": {}, "score": "59.87738"}
{"text": "Psychol.39 , 706 - 71610.1037/0012 - 1649.39.4.70612859124 .Thiessen E. D.,Yee M. N ..( Year : 2010 ) .Dogs , bogs , labs , and lads : what phonemic generalizations indicate about the nature of children 's early word - form representations .", "label": "", "metadata": {}, "score": "59.959465"}
{"text": "Computational Model .The computational model produces a one - dimensional signal indicating the likelihood of salient events over time , corresponding to a \" saliency score .\" The model is run on the same stimuli used in the experiments , with interaction weights obtained by training on the ground truth about salient events .", "label": "", "metadata": {}, "score": "60.014008"}
{"text": "View at Google Scholar . H. Grice , \" Logic and conversation , \" in Syntax and Semantics , P. Cole and J. Morgan , Eds . , vol .3 of Speech Acts , pp .41 - 58 , Academic Press , New York , NY , USA , 1975 .", "label": "", "metadata": {}, "score": "60.033886"}
{"text": "Also in accordance with these guidelines , the metric used to quantify performance was the unweighted average recall ( UAR ) which should take into account relative imbalances in the number of occurrences of the different emotional states .A preliminary search was initially performed by comparing the accuracies of the systems constructed with a different number of states ( ranging from 2 to 5 ) and a varying number of Gaussian mixtures in each state ( ranging from 2 to 10 ) .", "label": "", "metadata": {}, "score": "60.045166"}
{"text": "Experiment 2 demonstrates that even 5-month - olds are capable of this kind of distributional learning .Exposed to a set of lexical items in isolation , 5-month - olds were able to integrate information across these exemplars to identify the only feature consistent across all of them : their lexical stress pattern .", "label": "", "metadata": {}, "score": "60.04994"}
{"text": "In a sense , the HMMs in the described system only model a second higher level of temporal variation instead of all the dynamic information .The number of states in each HMM determines how much of the variations in the contours between voiced segments in an utterance are modelled , which , in turn , depend on how many voiced segments are present in each utterance .", "label": "", "metadata": {}, "score": "60.054123"}
{"text": "If this account is correct , infants would necessarily be able to segment input via conditional statistical information before showing the ability to take advantage of language - specific cues .This developmental account involves two different aspects of sensitivity to statistical information .", "label": "", "metadata": {}, "score": "60.08541"}
{"text": "Additionally , we evaluated the behaviour of the mental - state version of the UAH system with six recruited users using the same set of type S1 and S2 scenarios designed for the user simulation .Four of them recorded 30 dialogues ( 15 scenarios with the baseline system and 15 with the mental - state system ) , and two of them recorded 15 dialogues ( 15 dialogues with the baseline or the mental - state system only ) .", "label": "", "metadata": {}, "score": "60.15631"}
{"text": "The lower row shows the distribution of the synthesized facial point samples which are generated by ( 13 ) .is the Euclidean distance between the detected point and the ground truth divided by width of the detected face .By far surprise expression is recognized with high rate even for noisy detection , which is plausible due to its distinctive facial deformation as could be noticed from Table 1 .", "label": "", "metadata": {}, "score": "60.16742"}
{"text": "The behavior of the detection error is not reported ; hence it is not clear if there are correlations among the point detection errors .Additionally , we have no information about the error distribution in .-coordinates , while the off - diagonal elements contain the covariances between the errors of both coordinates .", "label": "", "metadata": {}, "score": "60.19898"}
{"text": "We obtained the discrimination accuracy as high as 85.6 % , but we could not observe significant differences between the two face orientation features .A remaining problem is that , the features proposed in this study are not available until the user 's input utterance is observed , because we examined the segment until \" just \" before the user 's input .", "label": "", "metadata": {}, "score": "60.22445"}
{"text": "Each row of Table 7 shows the discrimination results for each feature set .From Table 7 , we find that the discrimination accuracy using feature set ( b ) was higher than feature set ( a ) , and that using feature set ( c ) was the highest .", "label": "", "metadata": {}, "score": "60.256"}
{"text": "The oral response generator generates an error when the selected answer involves the use of a data not provided by the user simulator .A set of 40 scenarios were manually defined to consider the different queries that may be performed by users .", "label": "", "metadata": {}, "score": "60.37429"}
{"text": "A low , medium , high or expert level is assigned using these measures ; .Most frequent objective of the user ; .Reference to the location of all the information regarding the previous interactions and the corresponding objective and subjective parameters for that user ; .", "label": "", "metadata": {}, "score": "60.385315"}
{"text": "Additional foreground songs with 0 semitone pitch difference are also used , with a change in another attribute ( intensity or timbre ) following the factorial experimental design .Tokens are amplitude normalized relative to their top 5%th value .Recordings of water and wind sounds ( one track for each ) are each normalized to have the same peak amplitude as the combined background , and further added to the background .", "label": "", "metadata": {}, "score": "60.463875"}
{"text": "Earlier work had indicated that linear approximations to pitch contours were acceptable for the purpose of emotion classification , and another listening test conducted as part of the work reported in this paper revealed that similar approximations to glottal parameter contours were acceptable as well .", "label": "", "metadata": {}, "score": "60.584267"}
{"text": "Acknowledgements .This research has been funded by the Spanish Ministry of Science and Innovation , under the project ASIES TIN2010 - 17344 : Adaptation in Smart Environments and Social Networks to Assist People with Special Needs .Authors ' original submitted files for images .", "label": "", "metadata": {}, "score": "60.587868"}
{"text": "Face Detection .The human face is detected using a well - trained Haar cascade classifier [ 36 , 37 ] .This classifier employs the Haar - like features , which are defined as the ratio of intensities taken from adjacent rectangles .", "label": "", "metadata": {}, "score": "60.605885"}
{"text": "The number of pairs where the listener assigned the same emotion to both versions , out of a maximum possible of 30 , is given ( as percentages ) in Figure 5 .These results suggest that the linear approximations to the glottal parameter contours are able to preserve emotion - specific information to a reasonable extent .", "label": "", "metadata": {}, "score": "60.615902"}
{"text": "Wang D , Narayanan S : Piecewise linear stylization of pitch via wavelet analysis , in Proceedings of INTERSPEECH 2005 - EUROSPEECH , 9th European Conference on Speech Communication and Technology .Lisbon September 2005 , 4 - 8 : 3277 - 3280 .", "label": "", "metadata": {}, "score": "60.633423"}
{"text": "Moreover , despite their success at segmenting on the basis of statistical cues , 5-month - old infants do not appear to have developed a trochaic bias .This is consistent with the claim that sensitivity to conditional statistical information develops earlier than sensitivity to language - specific prosodic patterns ( Thiessen and Saffran , 2003 ) .", "label": "", "metadata": {}, "score": "60.640076"}
{"text": "They are important for accessing the databases and constructing the system prompts .However , the only information necessary to determine the user intention and their objective in the dialogue is the presence or absence of concepts and attributes . 0 : The concept is not activated , or the value of the attribute has not yet been provided by the user .", "label": "", "metadata": {}, "score": "60.668846"}
{"text": "Prizes will be awarded to the Sub - Challenge winners ._ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "60.730072"}
{"text": "This consistent preference across the trochaic and iambic language indicates that infants segmented the same items from both familiarization streams .The only cue to segmentation that is identical across the streams is the conditional statistical information , indicating that infants segmented on the basis of statistical cues .", "label": "", "metadata": {}, "score": "60.79078"}
{"text": "Upon presentation of a scene , the subject reports whether they heard a salient event .Results of the behavioral experiments demonstrate the principles governing the influence of acoustic properties on stimulus - induced attention .In line with our stated hypothesis , we develop a computational model providing an implementation of predictive - coding to test for the first time whether the Bayesian brain framework can explain the perception of auditory saliency revealed by our behavioral experiments .", "label": "", "metadata": {}, "score": "60.849922"}
{"text": "As a further advantage of word - based processing of emotional speech , we see the better dovetailing of emotion and speech processing .These are practical considerations ; yet it might be plausible conceiving the word as the \" smallest meaningful emotional unit \" as well .", "label": "", "metadata": {}, "score": "60.892845"}
{"text": "English - learning infants prefer to listen to trochaic words over words with a weak - strong ( iambic ) pattern ( Jusczyk et al . , 1993 ) .When exposed to a stream of syllables , English - learning infants and English - speaking adults treat the stressed syllables as word onsets ( e.g. , Cutler and Norris , 1988 ; Echols et al . , 1997 ; Jusczyk et al . , 1999 ) .", "label": "", "metadata": {}, "score": "60.94687"}
{"text": "Hence , we guarantee that the facial points will fall within the variance of the training set .The first step in building PDM is to align the facial points of all training samples .We consider only frontal faces ; therefore , normalizing facial point positions to the detected face dimensions is supposed to satisfy the PDM requirements .", "label": "", "metadata": {}, "score": "60.96663"}
{"text": "Locating the eight facial points in the aforementioned experiments involves human intervention either by annotating keyframes within each image sequence in Section 3.1 , or by selecting frames with neutral expression to detect the facial points and track them afterwards in Section 3.2 .", "label": "", "metadata": {}, "score": "60.97841"}
{"text": "Thus , we could , for instance , not contrast SC with EC .Second , the result would be \" smeared \" because of the contra - factual assumption that all words belonging to a chunk necessarily belong to the same emotion class .", "label": "", "metadata": {}, "score": "61.01156"}
{"text": "\"A new auditory salience model predicts human judgments , \" in Program No . 462.20 .2012 Neuroscience Meeting Planner ( New Orleans , LA : Society for Neuroscience ) .Driver , J. ( 2001 ) .A selective review of selective attention research from the past century .", "label": "", "metadata": {}, "score": "61.055378"}
{"text": "Background .In traditional computational models of the human mind , it is assumed that mental processes respect the semantics of mental states , and the only computational explanation for such mental processes is a computing mechanism that manipulates symbols related to the semantic properties of mental states [ 14 ] .", "label": "", "metadata": {}, "score": "61.180756"}
{"text": "As a case in point , the one - to - one tutoring outperforms conventional group methods of instruction .Consequently , adapting one - to - one tutoring to student performance through a cognitive process ( nonverbal behavior recognition ) is crucial [ 8 ] .", "label": "", "metadata": {}, "score": "61.18814"}
{"text": "These models vary in their biological plausibility and anatomical fidelity to the circuitry of the visual system , and differ in their focus on sensory - based vs. cognitive - based processes for attentional bias of visual information .Very few models have explored the role of Bayesian inference in modeling visual saliency .", "label": "", "metadata": {}, "score": "61.19709"}
{"text": "This prediction , carried out for each user turn in the dialogue , makes it possible to adapt the system dynamically to the user needs .The mental state is built on the basis of the emotional state of the user and their intention , and is recognized by means of a module conceived as an intermediate phase between natural language understanding and the dialogue management in the architecture of the systems .", "label": "", "metadata": {}, "score": "61.27135"}
{"text": "[29 ] used the local binary patterns ( LBP ) for facial expression recognition .Several modified versions of LBP were also proposed for facial expression recognition , for example , local normal binary patterns ( LNBP ) [ 30 ] , local phase quantisers ( LPQ ) [ 31 ] , and local sign directional pattern ( LSDP ) [ 32 ] .", "label": "", "metadata": {}, "score": "61.31333"}
{"text": "In most trials , the likelihood of saliency is highest during the duration of the actual salient event : I : 61 % , II : 78 % , III : 92 % ( Figure 5A ) .When contrasting the model scores with human ratings , strong correlations are observed ( Figure 6A ) .", "label": "", "metadata": {}, "score": "61.316017"}
{"text": "Thus , it might be that linguistics and emotions are more intertwined - at least within interactions where emotional and non - emotional episodes alternate .If this is the case , the modelling of SC seems to be most adequate also from the point of view of cognition and comprehension .", "label": "", "metadata": {}, "score": "61.333534"}
{"text": "Once infants have extracted a small set of lexical items from the input , they can learn the phonological regularities that characterize words in the native language .Doing so entails taking advantage of distributional statistical information .Distributional statistical information relates to the frequency and variability of exemplars in the input ( e.g. , Zhao et al . , 2011 ) .", "label": "", "metadata": {}, "score": "61.386765"}
{"text": "Some stimuli are inherently conspicuous and pop out amidst certain backgrounds .The study of bottom - up attentional effects is ultimately an investigation of physical attributes of sensory space and integrative mechanisms that allow regions of this space to become salient .", "label": "", "metadata": {}, "score": "61.40981"}
{"text": "View Article .Fairclough SH : Fundamentals of physiological computing .Interact Comput 2009 , 21 : 133 - 145 .10.1016/j.intcom.2008.10.011 View Article .K Das , Rizzuto D , Nenadic Z : Mental state estimation for brain - computer interfaces .", "label": "", "metadata": {}, "score": "61.448807"}
{"text": "Thus , it is not a \" tidy \" stimulus - response sequence that can be followed by tracking the very same channel ; we are using only the recordings of the children 's speech .Therefore , we do not know what the Aibo is doing at the corresponding time or has been doing shortly before or after the child 's utterance .", "label": "", "metadata": {}, "score": "61.463356"}
{"text": "Arrow directions indicate direction of interaction : the origin feature has a relatively larger effect on the destination feature in all experiments .Double - sided arrows indicate that there is no clear weight either way .The weight and directionality of interactions observed are inferred from the coefficients of the fitted model , and are limited by the levels of sound features tested in this study .", "label": "", "metadata": {}, "score": "61.474087"}
{"text": "We denote the chunk triggering boundaries described in Tables 2 and 3 with the symbol \" .In Table 3 , rows and columns with chunk triggering labels are shaded in grey .Between angle brackets , first the syntactic , and after the colon the pause labels are given ; note that pause length has been corrected manually .", "label": "", "metadata": {}, "score": "61.501877"}
{"text": "The human responses , mapped to 0 and 1 , are averaged over factorial case repetitions , and also averaged between subjects .Furthermore , we observe that the model saliency scores increase as the level of saliency increases .The level or strength of saliency of a token is taken as the number of sound attributes in which the foreground is different than background .", "label": "", "metadata": {}, "score": "61.55381"}
{"text": "D'Alessandro C , Doval B : Voice quality modification for emotional speech synthesis , in Proceedings of the 8th European Conference on Speech Communication and Technology .Geneva : EUROSPEECH 2003 - INTERSPEECH 2003 ; 2003:1653 - 1656 .He L , Lech M , Allen N : On the importance of glottal flow spectral energy for the recognition of emotions in speech , in Proceedings of the 11th Annual Conference of the International Speech Communication Association , INTERSPEECH 2010 .", "label": "", "metadata": {}, "score": "61.564053"}
{"text": "Thus , a baseline algorithm which always chooses \" neutral \" would have a very high accuracy ( in our case 85 % ) , which is difficult to improve by classifying the rest of emotions , that are very subtlety produced .", "label": "", "metadata": {}, "score": "61.60096"}
{"text": "Likewise , the sounds and feather motions we recorded constituted a harmonic series ( supplementary material Fig .This complexity is likely to be greatest in feathers exhibiting tip modes of vibration ( Clark et al . , 2013 ) , as well as snipe feathers ( Reddig,1978 ) , as these tended to have dozens of harmonics ( e.g. supplementary material Fig .", "label": "", "metadata": {}, "score": "61.62156"}
{"text": "The temporal response of each spectrogram channel is analyzed using short - term Fourier transform with 200 ms windows with 1 ms overlap .The top 64 and bottom 64 channels of the spectrogram are treated as separate features in subsequent processing as high and low frequency spectrum features .", "label": "", "metadata": {}, "score": "61.622944"}
{"text": "In order to evaluate the benefits of including the mental - state prediction in the system , we have employed a user simulator to gather a corpus of new dialogues that allows obtaining a more detailed study with a higher range of emotional behaviours .", "label": "", "metadata": {}, "score": "61.654682"}
{"text": "Figure 4 shows the recognition rate of facial expression in neutral - independent compared to neutral - dependent case ; in the latter we normalize each feature to its corresponding value at the neutral expression .The average recognition rate is improved by approximately 6 % .", "label": "", "metadata": {}, "score": "61.66111"}
{"text": "A careful inspection of model feature interactions shows strong similarity with psychoacoustic findings ( Figures 4 , 7 ) , even though the model interaction weights are trained based on ground truth about deviant events , not on human results .In particular , pitch and intensity have a strong interaction in both human perception and the computational model .", "label": "", "metadata": {}, "score": "61.68332"}
{"text": "Speech Commun 2009 , 51 ( 10):875 - 882 .10.1016/j.specom.2009.05.005 View Article .Wolters M , Georgila K , Moore JD , Logie RH , MacPherson SE : Reducing working memory load in spoken dialogue systems .Interact Comput 2009 , 21 ( 4):276 - 287 .", "label": "", "metadata": {}, "score": "61.68857"}
{"text": "The features are selected by majority voting of a forward selection algorithm , a genetic search , and a ranking filter using the default values of their respective parameters provided by Weka [ 45 ] .Minimum value , maximum value , range , mean , median , standard deviation and value in the first and last voiced segments .", "label": "", "metadata": {}, "score": "61.69209"}
{"text": "For each of these clusters , a Kalman filter is initialized as shown below .A .^ .t . )[ .Z .n .i . )Z .n .i .Z .n .i . )", "label": "", "metadata": {}, "score": "61.877205"}
{"text": "Instead , sensitivity to phonological cues emerges from earlier sensitivity to conditional statistical information in a developmental progression .The cues to which infants are sensitive early in life , such as conditional statistical information or utterance boundaries ( e.g. , Christophe et al . , 2001 ; Seidl and Johnson , 2006 ) , require no prior experience with or knowledge about a specific language to use .", "label": "", "metadata": {}, "score": "61.88742"}
{"text": "In parallel , there is greater interest in physiologically probing change detection in vision , particularly its event - related brain potential ( ERP ) component of visual mismatch negativity ( vMMN ) .vMMN has been described in a number of recent studies over the last decade ( see Kimura , 2012 for a review ) , though it has only been probed using temporal sequences and changing stimuli .", "label": "", "metadata": {}, "score": "61.89276"}
{"text": "This phenomenon is called filled pause .Goto et al .proposed a method to detect filled pauses [ 32 ] and described that filled pauses serve to maintain the speaker 's turn and to express the mental state while thinking of the next utterance .", "label": "", "metadata": {}, "score": "61.912086"}
{"text": "In this experiment , we probe the effect of time in addition to the same three attributes tested earlier .Time refers to the placement of the foreground token in the scene , appearing in four possible time - quadrants .All tested factors are found to influence saliency ( Figure 2 ) .", "label": "", "metadata": {}, "score": "61.956913"}
{"text": "Materials and methods .Participants .Half of these infants were exposed to a trochaic artificial language , and half to an iambic artificial language .To obtain data from 20 infants , it was necessary to run 23 infants .The additional three infants were excluded ( two from the trochaic condition , one from the iambic condition ) for crying during the testing session .", "label": "", "metadata": {}, "score": "62.011375"}
{"text": "If so , they should be able to discover a prosodic commonality across the word forms to which they are exposed in a laboratory setting .To test this possibility , we exposed 5-month - old infants to lists of trochaic words in isolation and then presented them with either a stream of trochaic or iambic speech .", "label": "", "metadata": {}, "score": "62.13173"}
{"text": "Recognition of the internal states of the conversation partner ( i.e. , the user of the system ) is related to social interaction in human - human communication .Studies on the social interaction of human - computer interfaces have included conversations with robots [ 11 - 13 ] and virtual agents [ 14 - 17 ] .", "label": "", "metadata": {}, "score": "62.20194"}
{"text": "Riccardi G , Hakkani - T\u00fcr D : Grounding emotions in human - machine conversational systems .In Proceedings of the 1stInternational Conference on Intelligent Technologies for Interactive Entertainment .Madonna di Campiglio , Italy ; 2005:144 - 154 .View Article .", "label": "", "metadata": {}, "score": "62.215927"}
{"text": "Perception of speech reflects optimal use of probabilistic speech cues .Cognition108 , 804 - 80910.1016/j.cognition.2008.04.00418582855 .Gleitman L. R.,Gleitman H.,Landau B.,Wanner E ..( Year : 1988 ) .\" Where learning begins : initial representations for language learning , \" in Linguistics : The Cambridge Survey , Vol . 3 , Language : Psychological and Biological Aspects , ed .", "label": "", "metadata": {}, "score": "62.249283"}
{"text": "Similarly , 8- and 9-month - old infants weight language - specific cues , such as lexical stress , more strongly than conditional cues ( e.g. , Johnson and Jusczyk , 2001 ; Thiessen and Saffran , 2003 ) .", "label": "", "metadata": {}, "score": "62.277054"}
{"text": "We did not assess whether the 5-month - olds in this experiment would be able to learn an iambic pattern , which contradicts the predominant pattern of English words .The fact that 7-month - olds can learn such a pattern suggests that 5-month - olds may be able to do so as well , given that 5-month - olds have even less familiarity with the predominant pattern of English to overcome .", "label": "", "metadata": {}, "score": "62.281418"}
{"text": "Several different subunits have been investigated as for their impact on improving classification performance such as frame - based processing , or taking some other fixed interval ( percentage of whole utterance , .ms , or voiced / unvoiced decisions , just to mention the most important ones , cf .", "label": "", "metadata": {}, "score": "62.357018"}
{"text": "Soc .Am .10.1121/1.421103 View Article .Doval B : C d'Alessandro , N Henrich .The spectrum of glottal flow models .Acta Acustica united with Acustica 2006 , 92 : 1026 - 1046 .Alku P : Glottal wave analysis with pitch synchronous iterative adaptive inverse filtering , in Proceedings of the EUROSPEECH-1991 .", "label": "", "metadata": {}, "score": "62.35747"}
{"text": "In Proceedings of 3rd International Workshop on Affective Interaction in Natural Environments .Firenze , Italy ; 2010:75 - 80 .Pittermann J , Pittermann A , Minker W : Emotion recognition and adaptation in spoken dialogue systems .Int J Speech Technol 2010 , 13 : 49 - 60 .", "label": "", "metadata": {}, "score": "62.37229"}
{"text": "I-2 ) reveals that the lack of timbre effect is specific to the choice of instruments .Experiment II : Nature .The consistency of effects between Exp .I and II argues against possible ceiling confounds that could have resulted from the musical notes experiment .", "label": "", "metadata": {}, "score": "62.37998"}
{"text": "The output of this computational model is contrasted with the pyschoacoustical findings from the behavioral experiments , providing a springboard for exploring the role of inference , predictive representations , and non - linear sensory interactions in mediating attention in audition .", "label": "", "metadata": {}, "score": "62.471886"}
{"text": "The geometry - based approach strongly depends on the facial point detector .Interestingly , we provide a method to neutralize the analysis of geometry - based approach from specific feature detectors .We synthesized facial points with various uncertainties that supposed to match existing errors in the facial point detectors .", "label": "", "metadata": {}, "score": "62.511086"}
{"text": "Each experiment is preceded with a brief training session comprised of 7 - 12 trials that are similar to experimental trials but with feedback provided about which sound feature is changed in the foreground token .Subjects can adjust sound intensity to their individual comfort level in all experiments , at any time during the experiment .", "label": "", "metadata": {}, "score": "62.516205"}
{"text": "It should be noted that the representation of speech as a sum of harmonic sinusoids used in this re - synthesis method holds only for voiced speech and was only applied to segments where pitch estimates were available .The unvoiced segments of the original speech samples were retained during re - synthesis .", "label": "", "metadata": {}, "score": "62.534843"}
{"text": "When we considered the dialogues to be different only when a different sequence of user intentions was observed , the percentage was lower using the mental - state system , due to an increment in the variability of ways in which the users can provide the different data required .", "label": "", "metadata": {}, "score": "62.623665"}
{"text": "This derives from the fact that not the perfect word chain is needed as , for example , in transcription of speech .Some minor mistakes are caught by stemming and stopping , and not all words are necessarily needed .Insertions and substitutions are only critical if they change the \" tone \" of the affective content .", "label": "", "metadata": {}, "score": "62.737835"}
{"text": "We deal with the topic of segmenting emotion - related ( emotional / affective ) episodes into adequate units for analysis and automatic processing / classification - a topic that has not been addressed adequately so far .We concentrate on speech and illustrate promising approaches by using a database with children 's emotional speech .", "label": "", "metadata": {}, "score": "62.764374"}
{"text": "Large , between - type mode jumps ( which could be easily , unambiguously diagnosed ) have been omitted from Fig .5 ; examples of hypothesized within - type mode jumps ( which could not be unambiguously diagnosed ) are indicated with arrows ( Fig .", "label": "", "metadata": {}, "score": "62.802353"}
{"text": "( Different smaller units of analysis for the FAU Aibo Emotion Corpus were pursued in [ 23 ] . )Syntactic Chunks : SC .Finding the appropriate unit of analysis for emotion recognition has not posed a problem in studies involving acted speech with different emotions , using segmentally identical utterances ( cf .", "label": "", "metadata": {}, "score": "62.8052"}
{"text": "Concepts defined to require the user the attributes that are necessary for a specific query ( Subject - Name , Degree , Group - Name , Subject - Type , Lecturer - Name , Program - Name , Semester and Deadline ) .", "label": "", "metadata": {}, "score": "62.83352"}
{"text": "Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1687 - 6180 - 2011 - 6 ) contains supplementary material , which is available to authorized users .Introduction .In human conversation , speakers adapt their message and the way they convey it to their interlocutors and to the context in which the dialogue takes place .", "label": "", "metadata": {}, "score": "62.847076"}
{"text": "The use of more complex models may be investigated in the future .Speech re - synthesis .In the work reported herein , the purpose of the speech re - synthesis procedure is to determine temporal contours of the speech production model parameters and then to re - synthesise speech from these parameter contours or their linear approximations .", "label": "", "metadata": {}, "score": "62.899284"}
{"text": "This way , it is possible to obtain different degrees of emotional behaviour with which to evaluate the benefits of our proposal .Process for emotion generation for each turn of the user simulator .A user request for closing the dialogue is selected once the system has provided the information defined in the objective(s ) of the dialogue .", "label": "", "metadata": {}, "score": "62.919926"}
{"text": "The basic idea is to combine a so far rather neglected type of data ( children 's speech ) with \" natural \" emotional speech within a Wizard - of - Oz task .The children were not told to use specific instructions but to talk to the Aibo like they would talk to a friend .", "label": "", "metadata": {}, "score": "62.93038"}
{"text": "First , it provides a cue that allows infants to segment words from fluent speech , even without language - specific phonological knowledge .Second , once infants have identified a set of lexical forms , they can learn from the distribution of acoustic features across those word forms .", "label": "", "metadata": {}, "score": "62.934692"}
{"text": "This mean squared error was computed for all possible combinations of F g , A g and F c with the search space spanning all possible values of the three parameters with a resolution of 30 values for each parameter .The parameter set , .", "label": "", "metadata": {}, "score": "63.00908"}
{"text": "The results of Experiment 1B indicate that , regardless of whether they heard a language made up of trochaic words or iambic words , infants showed the same preference at test .This indicates that infants segmented the same items from both the trochaic and the iambic language .", "label": "", "metadata": {}, "score": "63.024532"}
{"text": "When a user 's utterance was not observed , that part ( from the beginning of the system 's prompt to the beginning of the next prompt ) was used as a session .Figure 3 shows an overview of one dialog session .", "label": "", "metadata": {}, "score": "63.03105"}
{"text": "Spectrograms from the working section before and after acoustic treatment are presented in supplementary material Fig .S1 .Background aerodynamic noise increased with U air ( supplementary material Fig .S1 ) .The microphone and nose cone had a flat frequency - response curve ( \u00b13", "label": "", "metadata": {}, "score": "63.056644"}
{"text": "If the recognized emotion is doubtful and the user has changed his behaviour several times during the dialogue , the dialogue manager changes to a system - directed initiative and adds at the end of each prompt a help message describing the available options .", "label": "", "metadata": {}, "score": "63.062927"}
{"text": "Perform .doi : 10.1037/0096 - 1523.16.2.398 .Micheyl , C. , Tian , B. , Carlyon , R. P. , and Rauschecker , J. P. ( 2005 ) .Perceptual organization of tone sequences in the auditory cortex of awake macaques .", "label": "", "metadata": {}, "score": "63.074066"}
{"text": "The disjoint school recordings will be used to obtain a natural partitioning into train ( Ohm ) and test ( Mont ) in the ongoing .Speech was transmitted with a wireless head set ( UT 14/20 TP SHURE UHF - series with microphone WH20TQG ) and recorded with a DAT - recorder .", "label": "", "metadata": {}, "score": "63.08393"}
{"text": "Stimuli .The artificial language used in this experiment had the same lexical items , word order , and statistical structure as the language used in Experiment 1A. Two versions of this language were used .In the trochaic language , lexical stress occurred in word - initial position , while in the iambic language lexical stress occurred in word - final position .", "label": "", "metadata": {}, "score": "63.092422"}
{"text": "However , the number of values of face orientation depends on the number of frames of the session and varies from around 100 to 700 .To simplify the calculation of discrimination function , we need to extract feature vectors of fixed dimension from these face orientation values .", "label": "", "metadata": {}, "score": "63.108223"}
{"text": "Deviance detection on feature streams .The Kalman filter is used because it is efficient , versatile , and simple to implement and interpret .At each feature channel , clustering on a short segment at the start of the feature decides the regularities to be predicted for that feature .", "label": "", "metadata": {}, "score": "63.19451"}
{"text": "n . i . . ]^ .t . )[ .v .w .b .w .v .b .w .v .b .v .w .b . 2 . ]Next , at every time instance , the model iteratively computes its Kalman gain K ( t ) , and updates its posterior estimate of the state \u00c2 ( t ) and .", "label": "", "metadata": {}, "score": "63.214592"}
{"text": "Preliminary experiments on the LDC corpus supported this choice .Each state utilised a 4-mixture Gaussian mixture model .For experiments performed on the FAU Aibo corpus , the number of states and mixtures were picked based on a preliminary search and the results are discussed in Section 7.2 .", "label": "", "metadata": {}, "score": "63.254993"}
{"text": "Segmentation for dealing with such para - linguistic phenomena is pivotal - no matter which definition we use to describe them .They are related to but are not by definition coextensive with linguistic units such as sentences , utterances , dialogue acts , and salience .", "label": "", "metadata": {}, "score": "63.29203"}
{"text": "Childers DG , Lee CK : Vocal quality factors : analysis , synthesis , and perception .J. Acoust .Soc .Am .10.1121/1.402044 View Article .Cabral J , Renals S , Richmond K , Yamagishi J : Towards an improved modeling of the glottal source in statistical parametric speech synthesis , in 6th ISCA Workshop on Speech Synthesis .", "label": "", "metadata": {}, "score": "63.31273"}
{"text": "End notes .Abbreviations .MLP : . multilayer perceptron .NLU : . natural language understanding .POMDP : . partially observable Markov decision .UAH : .Universidad al Habla ( University On the Line ) .UR : . user register .", "label": "", "metadata": {}, "score": "63.407257"}
{"text": "Again , machines should not try to be too clever ; the only possibility they have is to take the linguistic content of the user 's utterances at face value .Thus , both vocal and linguistic expression of emotions should be taken by machines along the lines of Grice 's cooperative principle , at face value , and not assuming any indirect use [ 3 ] ; this excludes , for example , irony , metaphor , and meiosis .", "label": "", "metadata": {}, "score": "63.419064"}
{"text": "The instruments in this experiment were manually selected such that the they are sufficiently distinguishable from each other , but not so much that listeners with normal hearing and musical training would detect each different note , as determined by short pilot investigations with few listeners .", "label": "", "metadata": {}, "score": "63.423763"}
{"text": "On the other hand , Figure 12 shows that there is a reduction in the system requests when the mental - state system was used .This explains a higher proportion of the inform system action in the mental - state system .", "label": "", "metadata": {}, "score": "63.444794"}
{"text": "To prevent fruitless debates , we use the rather vague term \" emotion - related episodes \" in the title to denote both emotions in a strict sense and related phenomena in a broader sense , which are found in the database our experiments are based on .", "label": "", "metadata": {}, "score": "63.44982"}
{"text": "The human mental state could be inferred using various modalities such as facial expressions , hand gestures , acoustic data , and biophysiological data [ 1 - 5 ] .The importance of knowing this mental state appears in different disciplines .", "label": "", "metadata": {}, "score": "63.454487"}
{"text": "In particular , the use of one - dimensional midpoint values ( MP ) is better than the use of two - dimensional S - I descriptions for glottal parameter contours ( F g and A g ) , and two - dimensional representations are better than one - dimensional ones for pitch contours .", "label": "", "metadata": {}, "score": "63.52646"}
{"text": "Instead , this would suggest that language - specific prosodic cues may be the earliest cue infants use to segment words from fluent speech .Additionally , it would suggest that knowledge about the prosodic form of words arises from some source other than statistical learning , perhaps such as learning solely from words in isolation .", "label": "", "metadata": {}, "score": "63.527"}
{"text": "The gathered sessions were labeled by five evaluators .Table 1 shows the number of matches of evaluations by five evaluators and Table 2 shows the results of a majority vote .As we expected , user 's internal state such as State A and B were often appeared in the task ( 2 ) .", "label": "", "metadata": {}, "score": "63.52839"}
{"text": "Finally , we take as a starting point the observation that the sounds we investigated here are tonal ; we therefore ignore aperiodic ( chaotic ) flutter ( Alben and Shelley , 2008 ; Manela and Howe , 2009b ) as this does not produce tonal sound .", "label": "", "metadata": {}, "score": "63.537117"}
{"text": "The two databases have significant differences , with the LDC corpus containing ' acted ' emotional speech collected from seven professional actors with the speech comprising preselected , semantically neutral phrases .The Aibo corpus on the other hand contains ' elicited ' emotional speech from 51 children with no restrictions on what is being said .", "label": "", "metadata": {}, "score": "63.600853"}
{"text": "Numeric variables are discretised using unsupervised ten - bin discretisation [ 42 ] .Multiclass decision is obtained by transformation into binary problems by taking the two largest classes , each .Classification .As mentioned above and carried out within [ 33 ] , we split the FAU Aibo Emotion Corpus into train and test partitions by schools of recording .", "label": "", "metadata": {}, "score": "63.65621"}
{"text": "The factor levels for subsequent experiments were also set with these criteria .Experiment I-2 An additional experiment is performed to validate the main effects of musical instruments on the perception of saliency .In this experiment , pitch ( 5 and 10 semitones higher and lower than the background mean ) , intensity ( 7 and 10 dB higher than the background tokens ) , and timbre are tested separately .", "label": "", "metadata": {}, "score": "63.673775"}
{"text": "These three ranges of vibratory behavior were exhibited by all feathers tested .S2C , E , G ) .As U air increased , low - amplitude motion at discrete frequencies in the feathers was detected by SLDV , which we termed range II .", "label": "", "metadata": {}, "score": "63.708523"}
{"text": "Any longer pause at words within all these units was defined as a nontriggering hesitation pause .Each end - of - turn was redefined as triggering a clause / phrase boundary as well .Note that our turn - triggering threshold of 1 s works well because in the whole database , only 17 end - of - turn ( .", "label": "", "metadata": {}, "score": "63.789642"}
{"text": "This process is illustrated in Figure 3 .The optimal weights w ij are computed using experimental stimuli .The ground truth about deviants in each channel i in these stimuli is : .y .i .t . ) for .", "label": "", "metadata": {}, "score": "63.82166"}
{"text": "The errors in the facial point location are drawn from identical independent normal distributions with zero mean and five different standard deviation values .To recognize the facial expressions , Ekman and Friesen [ 12 ] broke the facial expression down into smaller action units ( AUs ) , where each AU codes small visible changes in facial muscles .", "label": "", "metadata": {}, "score": "63.849823"}
{"text": "This procedure was iterative and supervised by an expert .The sequential order of the labelling process does not distort the linguistic and paralinguistic message .Needless to say , we do not claim that these classes represent children 's emotions ( emotion - related user states ) in general , only that they are adequate for the modelling of these children 's behaviour in this specific scenario .", "label": "", "metadata": {}, "score": "63.87082"}
{"text": "S1 scenarios contain 1.3 actions per user turn instead of the 1.5 actions in the baseline dialogues , whereas for the S2 scenarios the scores are 1.4 and 1.9 , respectively .This is again because the users have to explicitly provide and confirm more information using the baseline system .", "label": "", "metadata": {}, "score": "63.946426"}
{"text": "Consequently , if predictions have been matching the input for some time , the expectation is that predicted values will keep being encountered , leading to a decrease in the fit threshold .As the dynamical system evolves , a series of spikes are generated corresponding to times of salient events .", "label": "", "metadata": {}, "score": "63.94751"}
{"text": "In real systems , a heuristic solution such as incremental prompt [ 22 ] is employed , where the system offers a different prompt if the user does not respond within a certain time after the first prompt .The system should provide more detailed information to clarify the intention of the prompt in case 1 ; however , in case 2 , intervention from the system in the dialog is undesirable .", "label": "", "metadata": {}, "score": "63.987705"}
{"text": "Naylor PA , Anastasis K , Jon G , Mike B : Estimation of glottal closure instants in voiced speech using the DYPSA algorithm .IEEE Transactions on Audio , Speech , and Language Processing 2007 , 15 : 34 - 43 .", "label": "", "metadata": {}, "score": "64.01347"}
{"text": "4 ) .Only one microphone was used in our setup which , as a result of wind tunnel constraints , was downstream of the feather ( Fig .2B ) .The feathers can be imagined as dipole sound sources with maximum sound intensity radiated parallel to the axis of maximum vibration , which was perpendicular to flow ( Blake , 1986 ; Manela and Howe , 2009b ) .", "label": "", "metadata": {}, "score": "64.0394"}
{"text": "175 - 206 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Steidl , Automatic classification of emotion - related user states in spontaneous children 's speech , Ph.D. thesis , Logos , Berlin , Germany , 2009 . A. Batliner , V. Zeissler , C. Frank , J. Adelhardt , R. P. Shi , and E. N\u00f6th , \" We are not amused - but how do you know ?", "label": "", "metadata": {}, "score": "64.0464"}
{"text": "Words occurred in a pseudo - random order , with the constraint that no word could follow itself .Syllable - to - syllable transitional probabilities were 100 % within a word , and 33 % at word boundaries .Because there were no pauses or other acoustic cues to word boundaries in this artificial language , the conditional probabilities ( high within a word , low at boundaries ) provided the only cue to word segmentation .", "label": "", "metadata": {}, "score": "64.10132"}
{"text": "If a feature does not fit any of the Kalman predictions , it produces a spike at that instant , signaling a deviant ; and a Kalman filter for this novel value is initialized .Filters that are not updated for one second are reset .", "label": "", "metadata": {}, "score": "64.13299"}
{"text": "Before this age , researchers have asserted that infants lack the ability to extract words from fluent speech on the basis of statistical structure ( e.g. , H\u00f6hle and Weissenborn , 2003 ) .Others have proposed that the ability to segment words from fluent speech via transitional probabilities is intact earlier ( e.g. , Thiessen and Saffran , 2003 ; Johnson and Tyler , 2010 ) .", "label": "", "metadata": {}, "score": "64.15633"}
{"text": "Hence , the location of the eight points relative to the face position and size results in a useful 16-dimensional feature vector , generated from both .Geometrical Features .Geometrical features describe relative position of the facial points to each other .", "label": "", "metadata": {}, "score": "64.35389"}
{"text": "They showed that the accuracy of AU classifiers is improved by using a hybrid of SVM and HMM , where SVM is providing frame - level information employed as emission probabilities for HMMs .The remainder of this paper is structured as follows .", "label": "", "metadata": {}, "score": "64.36429"}
{"text": "C.-W. Hsu , C.-C. Chang , and C.-J. Lin , \" A practical guide to support vector classification , \" Tech .Rep. , Department of Computer Science , National Taiwan University , 2003 .View at Google Scholar SHARE ON .", "label": "", "metadata": {}, "score": "64.42354"}
{"text": "As can be observed in the figure , the emotion recognizer always chooses one of the three negative emotions under study , not taking neutral into account .This is due to the difficulty of distinguishing neutral from emotional speech in spontaneous utterances when the application domain is not highly affective .", "label": "", "metadata": {}, "score": "64.44936"}
{"text": "The results were consistent with prior experiments using these stimuli .After familiarization , 5-month - old infants distinguished between words and part - words , indicating that they had succeeded in parsing the speech signal .Discussion .The fact that infants were able to segment the artificial language used in this experiment is inconsistent with the common assertion that speech segmentation does not begin until around 7 months of age ( e.g. , Jusczyk and Aslin , 1995 ; H\u00f6hle and Weissenborn , 2003 ) .", "label": "", "metadata": {}, "score": "64.516075"}
{"text": "By that , we model two different approaches : in the one approach , emotion is sort of modelled as being part of linguistics , in the other one , emotion is an independent layer , in parallel to linguistics .The latter one might be more adequate for theoretical reasons - emotion is not ( fully or only ) part of linguistics .", "label": "", "metadata": {}, "score": "64.57291"}
{"text": "These works also focused on Phase 3 of a session .On the other hand , several works investigated the internal states in Phase 1 .Hudson et al .[19 ] and Begole et al .[20 ] discriminated a user 's interruptibility by analyzing subjects in an office and a user of instant messages , respectively .", "label": "", "metadata": {}, "score": "64.57642"}
{"text": "Each trial is presented only once .Trials consist of a dynamic background constructed by many sound tokens that overlap in time with varying density depending on the experiment ( Figure 1 ) .Background tokens are randomly selected from a pool of suitable tokens , leading to unique overall backgrounds in each trial .", "label": "", "metadata": {}, "score": "64.73326"}
{"text": "Because the agent 's facial expression changes after the user 's utterance , the influence of facial expression change of the agent to the user 's attitude was also excluded from the later analysis .As we split one dialog into more than one session , some of the sessions were turns in Phase 3 .", "label": "", "metadata": {}, "score": "64.753784"}
{"text": "Automatic - HMM ( three - state ) -based system using all model parameters .The classification accuracies obtained by the system making use of all the model parameter contours is higher than the accuracies obtained by any of the individual systems , as expected .", "label": "", "metadata": {}, "score": "64.75679"}
{"text": "Moreover , we propose that in the course of natural language acquisition , conditional statistical learning influences distributional learning .Infants are able to discover phonological patterns through the lexical forms that they learn via sensitivity to conditional statistical information .To fully understand the role of statistical learning in language acquisition , it will be necessary to develop models and theories that more thoroughly explore how sensitivity to conditional and distributional statistical learning interact to allow infants to adapt to the structure of their native language .", "label": "", "metadata": {}, "score": "64.826485"}
{"text": "In such online applications , there is normally an interaction between system and user .The system does not only monitor somehow the user 's emotional states but has to recognise and process linguistic content and semantics and illocutions ( dialogue acts ) in order to react appropriately .", "label": "", "metadata": {}, "score": "64.86605"}
{"text": "In the example below , we draw the ECs that belong to the same utterance described in the previous section .Chunks of ememes ( denoted as pairs of spoken word and emotion label , that is , \" word emotion_label \") are delimited by markers ( symbol \" .", "label": "", "metadata": {}, "score": "64.92218"}
{"text": "To each of these , the delta coefficients are additionally computed .We further add double - delta coefficients for a better modelling of context .Then the 12 functionals mean , standard deviation , kurtosis , skewness , minimum and maximum value , relative position , and range as well as two linear regression coefficients with their Mean Square Error ( MSE ) are applied on a chunk basis as depicted in Table 6 .", "label": "", "metadata": {}, "score": "65.0054"}
{"text": "Parameter contours are representative of these variations over an entire utterance and are characterised by a much longer duration when compared with descriptions provided by deltas and shifted deltas .The most common and probably best studied is the pitch ( F 0 ) contour , which is essentially pitch as a function of time , and its use in an automatic emotion recognition ( AER ) framework was the focus of a previous study [ 21 ] .", "label": "", "metadata": {}, "score": "65.08497"}
{"text": "Once infants have discovered the acoustic features that are consistent in their proto - lexicon , they can use these features as cues to subsequent word segmentation ( e.g. , Johnson and Jusczyk , 2001 ) .This transition is from language - general to language - specific cues is thought to take place between 7 and 9 months .", "label": "", "metadata": {}, "score": "65.112915"}
{"text": "To ensure the scale invariant features , the distances are normalized to the detected face width .The distances .represent the average of two mirrored values on the left and right sides of the face .Then , the two feature sets are concatenated to produce a vector of length 22 .", "label": "", "metadata": {}, "score": "65.21399"}
{"text": "^ .g .A .^ .g .F .^ .c . , that gave the lowest mean squared error was then selected as the best fit : .F .^ .g .A .^ .", "label": "", "metadata": {}, "score": "65.21624"}
{"text": "Although being a truism , we definitely need more realistic databases for deciding between such alternative approaches .Acknowledgments .This work originated in the CEICES initiative ( Combining Efforts for Improving Automatic Classification of Emotional User States ) taken in the European Network of Excellence HUMAINE [ 37 ] .", "label": "", "metadata": {}, "score": "65.216484"}
{"text": "We placed a digital video camera between the monitor and the user and recorded the user 's frontal face .The operator ( the first author of this paper ) operated the system behind the partition .In either task , the prompt was repeated at 15-second intervals if user did not make the input utterance .", "label": "", "metadata": {}, "score": "65.22891"}
{"text": "The syntactic and pause labels are explained in Table 2 .For this type of data , we could use a simplified version of the full set of syntactic - prosodic boundaries which is described in detail in [ 9 ] , for both German and English .", "label": "", "metadata": {}, "score": "65.26167"}
{"text": "Stylised glottal flow derivative magnitude spectrum , after the work of Doval et al .[ 32 ] .Estimation of glottal spectral parameters .However , numerous techniques have been proposed over the years that are based on the properties of the glottal flow signal [ 33 - 38 ] .", "label": "", "metadata": {}, "score": "65.297554"}
{"text": "Although no evidence has been found for a dedicated auditory saliency map in the brain , the well researched mechanisms of deviance detection in the auditory pathway could be potentially implicated in the perception of saliency in audition .The neural correlates of these mechanisms have long been investigated , leading to the birth of multiple theories ( Naatanen et al . , 1978 ; May and Tiitinen , 2010 ) .", "label": "", "metadata": {}, "score": "65.300095"}
{"text": "Feature level 0 on the x -axis corresponds to a foreground token with low level of saliency .As an example , for Experiment III , this corresponds to no difference in pitch or timbre , but a 10 db difference in intensity .", "label": "", "metadata": {}, "score": "65.32382"}
{"text": "However , neutral expression is recognized with high rate of 90.35 % and 79.18 % using SVM and kNN , respectively .We achieved an average recognition rate of 73.63 % and 67.12 % using SVM and kNN , respectively .These results indicate that SVM classifier outperforms kNN for facial expression recognition .", "label": "", "metadata": {}, "score": "65.40047"}
{"text": "The emotion recognizer .As the architecture shown in Figure 1 has been designed to be highly modular , different emotion recognizers could be employed within it .We propose to use an emotion recognizer based solely in acoustic and dialogue information because in most application domains the user utterances are not long enough for the linguistic parameters to be significant for the detection of emotions .", "label": "", "metadata": {}, "score": "65.40196"}
{"text": "Second , our experimental setup only allowed us to vary \u03b1 precisely with the airflow on ( Fig . 2 ) , while adjustment of \u03b2 was crude , impairing our ability to explore all pair - wise combinations of \u03b1 and \u03b2 .", "label": "", "metadata": {}, "score": "65.40855"}
{"text": "Most prior research indicates that infants do not discover this regularity until some time around 7 months ( e.g. , Jusczyk et al . , 1999 ; Thiessen and Saffran , 2003 ) .If we are right that discovering such phonological regularities requires infants to first identify a set of lexical items , the lack of a trochaic bias at 5 months likely indicates that infants have yet to become familiar with a sufficient number of words .", "label": "", "metadata": {}, "score": "65.40985"}
{"text": "One long turn , German original word sequence with syntactic and pause labels , and chunk boundaries : .If all 13642 turns are split into chunks , the chunk triggering procedure results in a total of 18216 chunks .Note that the chunking rules have been determined in a heuristic , iterative procedure ; we corroborated our initial hypotheses , for instance , that pauses between adjacent vocatives are longer on average than pauses after or before single vocatives , with the descriptive statistics given in Table 3 .", "label": "", "metadata": {}, "score": "65.414925"}
{"text": "Speech48 , 279 - 29810.1177/0023830905048003020116416938 .Nazzi T.,Iakimova I.,Bertoncini J.,Fr\u00e9donie S.,Alcantara C ..( Year : 2006 ) .Early segmentation of fluent by infants acquiring French : emerging evidence for crosslinguistic differences .J. Mem .Lang.54 , 283 - 29910.1016/j.jml.2005.10.004 .", "label": "", "metadata": {}, "score": "65.427536"}
{"text": "The degree of emotional homogeneity ( confidence ) given at the end of each chunk is simply the number of the labels the whole chunk is attributed to , divided by the number of all labels .These five chunks in this example belong all to N(egative ) .", "label": "", "metadata": {}, "score": "65.46888"}
{"text": "It is important to note that the real purpose of the data collection experiment , the elicitation of the emotions , was not known to the children , and none of them realised that Aibo was remote - controlled .The recorded speech data was then annotated independently at the word level by five listeners using 11 emotional categories .", "label": "", "metadata": {}, "score": "65.486435"}
{"text": "To improve the human - computer interaction ( HCI ) to be as good as human - human interaction , building an efficient approach for human emotion recognition is required .These emotions could be fused from several modalities such as facial expression , hand gesture , acoustic data , and biophysiological data .", "label": "", "metadata": {}, "score": "65.54615"}
{"text": "The information contained in UR i is a summary of the information provided by the user up to time i .That is , the semantic interpretation of the user utterances during the dialogue and the information that is contained in the user profile .", "label": "", "metadata": {}, "score": "65.55142"}
{"text": "The dialogue manager tailors the next system answer to the user state by changing the help providing mechanisms , the confirmation strategy and the interaction flexibility .The conciliation strategies adopted are , following the constraints defined in [ 51 ] , straightforward and well delimited not to make the user loose the focus on the task .", "label": "", "metadata": {}, "score": "65.56727"}
{"text": "In this study , we examined the aeroacoustic behavior of hummingbird tail feathers that vary in shape over a range of air velocities in a wind tunnel .All feathers tested fluttered and produced sound in the wind tunnel and many , though not all , of these feathers appear to have evolved acoustic functionality in courtship displays of these birds [ see online appendix of our previous publication ( Clark et al . , 2011a ) ] .", "label": "", "metadata": {}, "score": "65.666725"}
{"text": "An interaction between intensity and timbre , and between all four factors , is observed in only one experiment .Figure 4 .Summary of interaction weights based on behavioral tests with human listeners .Solid lines indicate two - way , dashed lines three - way and dotted lines four - way interactions .", "label": "", "metadata": {}, "score": "65.67911"}
{"text": "Recent research by H\u00f6hle et al .( 2009 ) , however , indicates that infants as young as 6 months are familiar with the predominant prosodic structure of words in their native language .H\u00f6hle et al . suggest that 6 months is below the age at which infants are able to segment words from fluent speech via conditional statistical cues .", "label": "", "metadata": {}, "score": "65.69281"}
{"text": "S2C ) .These motions were not visible , and appeared to be highly damped .Apparent declines in flutter amplitude with increases in U air ( e.g. red points in Fig .3 ) may be solely attributable to this limitation .", "label": "", "metadata": {}, "score": "65.77818"}
{"text": "One \" foreground \" note that varies in pitch ( Foreground pitch at 350 Hz ) and intensity ( 6 dB higher than background notes ) is introduced at a random location in the scene .In Experiments I and II , foreground tokens only appear in the second half of the scene , while in Experiment III , they can occur at any time .", "label": "", "metadata": {}, "score": "65.780334"}
{"text": "Speech Communication 2010 , 52 : 613 - 625 .10.1016/j.specom.2010.02.010 View Article .El Ayadi M , Kamel MS , Karray F : Survey on speech emotion recognition : features , classification schemes , and databases .Pattern Recognition 2011 , 44 : 572 - 587 .", "label": "", "metadata": {}, "score": "65.89177"}
{"text": "Each of the probed auditory attributes ( pitch , timbre and intensity ) is a complex physical property of sound that likely evokes several neural processing streams and engages multiple physiological nuclei along the auditory pathway .It remains to be seen whether the nature of interactions reported here reflects intrinsic neural mechanisms and topographies of feature maps in the sensory system ; or reveals perceptual feature integration processes at play in auditory scene analysis .", "label": "", "metadata": {}, "score": "65.9496"}
{"text": "Results .Experiments .Experiment I : Music .In this first experiment , we investigate the effect of pitch , intensity , and timbre on perception of saliency .Because timbre is a non - numeric attribute , we probe the effect of each musical instrument as a foreground ( T f ) and background ( T b ) timbre event .", "label": "", "metadata": {}, "score": "65.96436"}
{"text": "Figure 6 presents a schematic representation of the corpora used and the users that recorded the dialogues .Evaluation with a user simulator .User simulators make it possible to generate a large number of dialogues reducing the time and effort that would be needed for the detailed evaluation of the quality of the services provided by a dialogue system [ 52 ] .", "label": "", "metadata": {}, "score": "65.9828"}
{"text": "Experiment 1A demonstrated that 5-month - old infants are able to segment word forms from speech solely on the basis of conditional probability information .If infants of this age segment statistical words rather than trochaic disyllables , this would provide strong support for the idea that conditional information is one powerful language - universal cue that could be recruited to acquire language - specific knowledge such as the preferred position of stressed syllables within word forms .", "label": "", "metadata": {}, "score": "66.06186"}
{"text": "Frame - Based Facial Expression Recognition Using Geometrical Features .Institute for Information Technology and Communications ( IIKT ) , Otto - von - Guericke - University Magdeburg , P.O. Box 4210 , 39016 Magdeburg , Germany .Received 18 August 2013 ; Revised 9 January 2014 ; Accepted 10 February 2014 ; Published 16 April 2014 .", "label": "", "metadata": {}, "score": "66.13527"}
{"text": "For details , see the accompanying paper ( Clark et al . , 2013 ) .RESULTS .All feathers tested in the wind tunnel were capable of fluttering and producing audible tones .SLDV measurements revealed a nearly 1:1 correspondence between flutter and sound recorded by the microphone ( Clark et al . , 2011a ) .", "label": "", "metadata": {}, "score": "66.16146"}
{"text": "Although the help policy was adapted to emotion , the rest of the decisions of the dialogue manager were carried out without taking into account any emotional information .In our proposal , we merge the traditional view of the dialogue act theory in which communicative acts are defined as intentions or goals , with the recent trends that consider emotion as a vital part of mental states that makes it possible to carry out social communication .", "label": "", "metadata": {}, "score": "66.22693"}
{"text": "Further , unlike in Exp .I , one can not make out individual tokens even while actively attending to them , due to the high level of word overlap and noise in the source recording .Fifty - six words in the 175 - 233 Hz ( F3-A#3 ) range and of 0.5 - 1.2 s length are manually extracted at 8 kHz to be in the background .", "label": "", "metadata": {}, "score": "66.244896"}
{"text": "The importance of appropriate glottal models in the synthesis of natural sounding speech has been well established [ 22 , 23 ] , and the modification of glottal voice quality factors has been shown to be significant for the synthesis of emotional ( expressive ) speech [ 24 ] .", "label": "", "metadata": {}, "score": "66.415436"}
{"text": "First , we detected the parts of the face ( eyes and nose ) in each frame .This was done by template matching of the face region in a frame , which was determined through face detection [ 33 ] and tracking [ 34 ] .", "label": "", "metadata": {}, "score": "66.4185"}
{"text": "It should also be noted that the listeners were not aware that there were an equal number of samples from each of the five emotional categories .Automatic classification system .The front - end .In addition to subjective evaluations , the usefulness of linear approximations to speech parameter contours to automatic classification systems was evaluated .", "label": "", "metadata": {}, "score": "66.4475"}
{"text": "In the example shown in the figure , timbre was not varied .All tokens were clavichord notes .For both correct and false detection rates , values of 0 and 1 are adjusted to 0.01 and 0.99 , respectively .It is worth noting that similar results are obtained irrespective of the small adjustments to the correct and false detection rates .", "label": "", "metadata": {}, "score": "66.456474"}
{"text": "However , if the ability to parse speech on the basis of statistical cues is intact at an earlier age , infants should discriminate between words and part - words .Materials and methods .Participants .To obtain data from 10 infants , it was necessary to run 13 infants .", "label": "", "metadata": {}, "score": "66.46123"}
{"text": "It is different in offline applications ; the processing of movie databases in search for emotional episodes needs not be incremental and can be done in several passes .Thus , we can imagine one pass for emotion monitoring within the whole movie , and then a second pass for segmentation , and so on .", "label": "", "metadata": {}, "score": "66.46239"}
{"text": "Piccinini G : Functionalism , computationalism , and mental states .Stud Hist Philos Sci 2004 , 35 : 811 - 833 .10.1016/j.shpsa.2004.02.003 MathSciNet View Article .Katoh T , Hara H , Kinoshita T , Sugawara K , Shiratori N : Behavior of Agents Based on Mental States .", "label": "", "metadata": {}, "score": "66.46437"}
{"text": "We mentioned in the beginning that almost all studies have bypassed somehow the decision where to start or to end an emotion .Even in a - more or less realistic - verbal human - human or human - machine communication , a dialogue act / move or an \" utterance \" can be delimited easily by the act of turn taking , when the one partner finishes speaking and the other partner takes over .", "label": "", "metadata": {}, "score": "66.470245"}
{"text": "Piscataway ; 1993 .pp .542 - 545 .Vincent D , Rosec O , Chonavel T : Estimation of LF glottal source parameters based on an ARX model , in Proceedings of INTERSPEECH 2005 - EUROSPEECH , 9th European Conference on Speech Communication and Technology .", "label": "", "metadata": {}, "score": "66.58836"}
{"text": "His method is designed to infer combinations of affective states that can occur simultaneously and whose level of expression can change over time within a dialogue .By affective states , the author understands moods , emotions and mental states .Although he does not provide any definition of mental state , the categories employed in his experiments do not account for intentional information .", "label": "", "metadata": {}, "score": "66.60779"}
{"text": "Second , infants must be capable of learning from the distribution of lexical stress in word forms with which they are familiar before 6 months .Given that Experiment 1 demonstrated that infants are sensitive to conditional statistical regularities in linguistic input , a natural subsequent question to ask is whether infants at this age are also sensitive to distributional statistical regularities in linguistic input .", "label": "", "metadata": {}, "score": "66.67423"}
{"text": "Excerpt of a dialogue with its correspondent user profile and user register for one of the turns .Secondly , we assigned an emotion category to each user utterance .Our main interest was to study negative user emotional states , mainly to detect frustration because of system malfunctions .", "label": "", "metadata": {}, "score": "66.69136"}
{"text": "the Sleepiness Sub - Challenge , sleepiness of speakers has to be determined by a suited algorithm and acoustic features .Two classes have to be recognised accordingly : sleepiness or non - sleepiness .Again , the Challenge measure is unweighted average recall of the two classes .", "label": "", "metadata": {}, "score": "66.70067"}
{"text": "Similarly , Lee et al .[ 25 ] investigated mental - state decoding abilities in depressed women and found that they were significantly less accurate than non - depressed in identifying mental states from pictures of eyes .They accounted for mental states as beliefs , intentions and specially emotions , highlighting their relevance to understand behaviour .", "label": "", "metadata": {}, "score": "66.736885"}
{"text": "F .i . m .A .i . m .F .i . m .A . i .Consistently , the best UARs were obtained by systems using two - state HMMs .The optimal number of mixtures in each state is likely to be more dependent on feature dimensionality , but for the above - mentioned features , the highest UARs were obtained when five or six mixtures were used .", "label": "", "metadata": {}, "score": "66.75257"}
{"text": "733 - 736 , Geneva , Switzerland , September 2003 .B. Schuller , R. M\u00fcller , F. Eyben , et al . , \" Being bored ?Recognising natural interest by extensive audiovisual integration for real - life application , \" Image and Vision Computing , vol .", "label": "", "metadata": {}, "score": "66.80344"}
{"text": "We hypothesize that average flutter amplitude increased or remained constant with U air in range III of all feathers , and that data suggesting otherwise ( e.g. red squares in range III of Fig .3 ; supplementary material Fig .S2A ) are an artifact resulting from the measurement limits from the SLDV .", "label": "", "metadata": {}, "score": "66.806725"}
{"text": "for silences .Therefore , we chose the length of the filler and silence as efficient features for discriminating between States A and B. These facts indicate that subjects who were thinking about the answer ( e.g. , who were labeled as State B ) tended to be silent before answering , and so a long filler was considered to be a sign of \" thinking . \" Length of Filled Pause .", "label": "", "metadata": {}, "score": "66.885155"}
{"text": "The average of errors detected and corrected by the dialogue manager .We have considered only the errors that modify the values of the attributes and that could cause dialogue failure .Average number of uncorrected errors per dialogue ( n NCE ) .", "label": "", "metadata": {}, "score": "66.90397"}
{"text": "Example spectrogram of stimulus used in behavioral experiments .The spectrogram shows overlapping musical note tokens that compose a scene 's background , and one foreground note , outlined in the image .Their pitch and intensity values are sampled from a constrained distribution of values , emulating a busy scene with natural sounds ( Background pitch between 196 and 247 Hz ) .", "label": "", "metadata": {}, "score": "66.97561"}
{"text": "Range I : at low U air , no motion was detected ( no data present ) .Range II : at intermediate U air , small amplitude vibrations were recorded , but no sound was detected ( braces ) .Asterisk indicates the sound first detected .", "label": "", "metadata": {}, "score": "66.982445"}
{"text": "Soc .Am .10.1121/1.1912389 View Article .Klatt DH , Klatt LC : Analysis , synthesis , and perception of voice quality variations among female and male talkers .J. Acoust .Soc .Am .10.1121/1.398894View Article .Veldhuis R : A computationally efficient alternative for the Liljencrants - Fant model and its perceptual evaluation .", "label": "", "metadata": {}, "score": "67.02676"}
{"text": "The IAIF method can be used pitch synchronously ( with variable window lengths based on pitch ) or asynchronously ( with fixed windows ) .For pitch synchronous IAIF , the DYPSA algorithm [ 39 ] has been used to detect glottal closure instants and hence identify window boundaries .", "label": "", "metadata": {}, "score": "67.072845"}
{"text": "In the case of music , the number of musical notes is predetermined for each instrument , leading to a limited set of notes constrained in a small range of pitch .However , we examine the difference between number of errors in the first half vs. second half of each experiment , and find no significant difference ( 1-way within subjects ANOVA :", "label": "", "metadata": {}, "score": "67.07671"}
{"text": "This pattern was real , not merely experimental error .We suggest that these are the feathers for which sound loudness increased with increasing U air [ see also fig .1B in Clark et al .( Clark et al . , 2011a ) ] .", "label": "", "metadata": {}, "score": "67.09125"}
{"text": "Sci.14 , 1100 - 110610.1111/j.1467 -7687.2011.01056 .x21884325 .Lew - Williams C.,Saffran J. R ..( Year : 2012 ) .All words are not created equal : expectations about word length guide infant statistical learning .Cognition122 , 241 - 24610.1016/j.cognition.2011.10.00722088408 .", "label": "", "metadata": {}, "score": "67.11751"}
{"text": "Each token is amplitude normalized with its top value and applied a 0.05 s long onset and offset ramp .The background consists of a combination of four sequences of tokens with no delay .Foreground tokens are 10 and 13 dB higher from the cumulative background .", "label": "", "metadata": {}, "score": "67.30188"}
{"text": "Hence , recognizing human emotions by machines is considered an important step forward .Pantic et al .[ 6 ] argued that facial expressions are more important than body gestures and vocal expressions to the judgment of human behavior .For example , in our companion - based assistant system , facial expression is considered as a complementary aspect to hand gestures and other modalities [ 7 ] .", "label": "", "metadata": {}, "score": "67.33963"}
{"text": "If the detected objective matches the predicted intention , the system takes the information for granted and uses implicit confirmations .For example , if a student always asks for subjects of a certain degree , the system can directly disambiguate a subject if it is in several degrees .", "label": "", "metadata": {}, "score": "67.453384"}
{"text": "Figure 1 displays for each of the four classes and for all classes taken together , frequencies in percent for SC and EC with the length 1 to . words .The same information is given in the stacked histograms of Figure 2 ; in Figure 1 , relationships within classes and differences between type of chunk can be seen , whereas Figure 2 concentrates on frequencies across classes within one plot .", "label": "", "metadata": {}, "score": "67.516495"}
{"text": "S2 , which shows sound spectrograms and SLDV vibration spectra corresponding to the data plotted here ; plotted points reflect the peak velocity of the fundamental frequency of vibration .Fundamental frequency of vibration was positively correlated with U air in most of the feathers ( Fig .", "label": "", "metadata": {}, "score": "67.53136"}
{"text": "Information about emotional state is expressed via speech through numerous cues , ranging from low - level acoustic ones to high - level linguistic content ; several approaches to speech - based automatic emotion recognition , each taking advantage of a few of these cues , have been explored [ 1 - 9 ] .", "label": "", "metadata": {}, "score": "67.53496"}
{"text": "Among the classes shown in Table 4 , the \" system \" segment is for utterances by the system , and all of the other classes are for utterances by the user .We investigated the length of events of each class for all dialogs classified into States A and B and observed the difference in length between the two internal states in order to find features effective for discrimination .", "label": "", "metadata": {}, "score": "67.5385"}
{"text": "Thus so far , we found no indication that our children ( age 10 - 13 ) behave differently from adults in a principled way , as far as speech / linguistics in general or emotional states conveyed via speech are concerned .", "label": "", "metadata": {}, "score": "67.54944"}
{"text": "Each token is 1.2 s in duration and amplitude normalized relative to its maximum with 0.1 s onset and offset sinusoidal ramps .Four sequences of consecutive tokens , randomly chosen for each trial , are combined with 0.3 s phase delay to form a 5 s dynamic background .", "label": "", "metadata": {}, "score": "67.56273"}
{"text": "The difference between stressed and unstressed syllables is perceptually available to infants from a young age ( e.g. , Jusczyk and Thompson , 1978 ; Weber et al . , 2005 ) .To the extent that stressed and unstressed syllable systematically occur in particular word positions , this distinction can serve as a cue to word boundaries .", "label": "", "metadata": {}, "score": "67.61647"}
{"text": "Dyer JR , Shatz M , Wellman HM : Young children 's storybooks as a source of mental state information .Cogn Dev 2000 , 15 : 17 - 37 .10.1016/S0885 - 2014(00)00017 - 4 View Article .Lee L , Harkness KL , Sabbagh MA , Jacobson JA : Mental state decoding abilities in clinical depression .", "label": "", "metadata": {}, "score": "67.67404"}
{"text": "We see that on the one hand , the threshold for segmentation of 1 s is meaningful ; on the other hand , there are still many turns having more than 5 words per turn .This means that they tend to be longer than one intonation unit , one clause , or one elementary dialogue act unit , which are common in this restricted setting \" giving commands .", "label": "", "metadata": {}, "score": "67.688034"}
{"text": "In the cocktail party example , the salient shattering glasses would differ from the ambient sounds in acoustic attributes such as timbre , intensity , and location .We conduct human behavioral experiments to gain psychophysiological insight into the dimensions of auditory saliency and their interactions .", "label": "", "metadata": {}, "score": "67.69841"}
{"text": "In this paper , we defined three internal states ( State A , B , and C ) of a user of a dialog system before the first user utterance and investigated methods of modeling these states .It is important to estimate the user 's internal state before the user 's input in order to make the response of the system more appropriate , but this issue has not been studied to date .", "label": "", "metadata": {}, "score": "67.72015"}
{"text": "These factors may require that infants experience many more repetitions of a word in a natural language to segment it from fluent speech than is necessary in segmentation experiments .As this discussion indicates , much remains unknown about the exact age at which infants begin to segment words from fluent speech , and the number of lexical forms they are able to extract from fluent native language input ( for discussion , see Swingley , 2005 ) .", "label": "", "metadata": {}, "score": "67.75757"}
{"text": "( a ) Human face in neutral expression overlaid with 68 fiducial facial points [ 13 ] .( b , h )The detected face with the selected eight facial points .( c , i ) Human face in happy expression .", "label": "", "metadata": {}, "score": "67.835464"}
{"text": "Conclusions and future work .In this paper we have presented a method for predicting user mental states in spoken dialogue systems .These states are defined as the combination of the user emotional state and the predicted intention according to their objective in the dialogue .", "label": "", "metadata": {}, "score": "67.854515"}
{"text": "Callejas Z , L\u00f3pez - C\u00f3zar R : Implementing modular dialogue systems : a case study .In Proceedings of Applied Spoken Language Interaction in Distributed Environments ( ASIDE 05 ) .Aalborg , Denmark ; 2005 .Callejas Z , L\u00f3pez - C\u00f3zar R : Relations between de - facto criteria in the evaluation of a spoken dialogue system .", "label": "", "metadata": {}, "score": "67.892746"}
{"text": "The concordance ratio was calculated by the following equation : . )The results show that the decision of the evaluator E2 , E3 , and E4 are well accorded ; however , the decision of evaluator E5 does not match with that of others too much .", "label": "", "metadata": {}, "score": "67.92969"}
{"text": "( e , k ) Human face in surprise expression .( f )The six geometrical features ; . are the average of two mirrored values on the left and right sides of the face .( l ) Human face in sadness expression .", "label": "", "metadata": {}, "score": "67.953415"}
{"text": "Many other birds also produce tonal sounds during flight that are consistent with feather flutter ( C.J.C. and R.O.P. , manuscript in preparation ) , and production of non - vocal communication sounds appears to be evolutionarily labile ( Prum , 1998 ) .", "label": "", "metadata": {}, "score": "67.98798"}
{"text": "Additional foreground words with 0 semitone pitch difference are also used .The foreground onset is also manipulated by placing it in one of four 1.25 s long quadrants of the 5 s long trial , hence probing the effect of timing of foreground on perception of saliency .", "label": "", "metadata": {}, "score": "67.99066"}
{"text": "Therefore , we decided to collect natural dialogs .For the experiments , we prepared two tasks : ( 1 ) a simple information retrieval task and ( 2 ) a \" question and answer \" task .The information retrieval task simulated a restaurant guidance system .", "label": "", "metadata": {}, "score": "68.021614"}
{"text": "F . i .b .F .i .M .i .F .F .F .A .A .A .F . g .A . g .F .Note that the lengths of the contours are identical for all model parameters within each voiced segment .", "label": "", "metadata": {}, "score": "68.02556"}
{"text": "In this paper , we propose an approach to perceive human facial expression ( happiness , surprise , anger , disgust , fear , and sadness ) from captured face images .Additionally , we synthesize facial points with several uncertainties matching facial points detected with errors .", "label": "", "metadata": {}, "score": "68.02792"}
{"text": "Similarly , confusions of subtle expressions with neutral are present .In contrast with our evaluation on CK+ database , we achieved a lower recognition rate for neutral expression and a higher one for sadness .We achieved an average recognition rate of 68.04 % and 57.92 % using SVM and kNN classifiers , respectively .", "label": "", "metadata": {}, "score": "68.02879"}
{"text": "The practical motivation behind EC is that homogeneous groups of ememes might be easier to classify than single ememes ; for the recognition of each emotional class , we are exploiting the largest amount of contextual information , that is , the entire EC .", "label": "", "metadata": {}, "score": "68.04162"}
{"text": "This may obscure harmonics in a recording taken in a noisy environment such as a wind tunnel .As the sound recorded in a wind tunnel is time - invariant , a larger FFT window was warranted .van Casteren and colleagues ( van Casteren et al . , 2010 ) also found that snipe feathers flutter at half the vortex - shedding frequency they had predicted , but explain the twofold difference by suggesting that two vortices were shed per vibratory cycle .", "label": "", "metadata": {}, "score": "68.04291"}
{"text": "Nine annotators tagged the corpus twice and the final emotion assigned to each utterance was the one annotated by the majority of annotators .A detailed description of the annotation of the corpus and the intricacies of the calculation of inter - annotator reliability can be found in [ 50 ] .", "label": "", "metadata": {}, "score": "68.058624"}
{"text": "This paper focuses on the use of the source - filter model of speech production , and consequently the contours considered are those of the parameters of this model .Hence , an utterance can be compactly represented as a sequence of vectors , V : .", "label": "", "metadata": {}, "score": "68.199875"}
{"text": "It can be observed that there are also only slight differences between the values obtained for both corpora .There was a higher percentage of confirmations and questions in the corpus collected with real users due the higher average number of turns per dialogue in this corpus .", "label": "", "metadata": {}, "score": "68.20242"}
{"text": "t .F .T .Integration of saliency information among features .These spikes represent some probability of having a salient event at the time instance in which they occurred ; the higher the value , the more likely is saliency .", "label": "", "metadata": {}, "score": "68.21752"}
{"text": "240 - 245 , Ljubljana , Slovenia , 2006 .B. Pang , L. Lee , and S. Vaithyanathan , \" Thumbs up ?Sentiment classification using machine learning techniques , \" in Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ' 02 ) , pp .", "label": "", "metadata": {}, "score": "68.30291"}
{"text": "The back - end of this system is a 128-mixture GMM - based classifier that models the probability distributions of the feature values ( without taking into consideration any temporal dependence ) .Experimental results .LDC corpus .The automatic classification systems setup for a five - emotion ( neutral , anger , sadness , happiness and boredom ) classification problem on the LDC corpus was implemented in a speaker - independent configuration .", "label": "", "metadata": {}, "score": "68.344864"}
{"text": "Although separate timbre components ( T f , T b ) are not significant in every experiment , their interaction is significant ; demonstrating that the effect of timbre on saliency stems from the interplay of background and foreground .Further , while T f and T b do not separately interact with pitch in every experiment , the combined interaction PxT b xT f does .", "label": "", "metadata": {}, "score": "68.460495"}
{"text": "Speaker State Challenge broadens the scope by addressing two less researched speaker states while focusing on the crucial application domain of security and safety : the computational analysis of intoxication and sleepiness in speech .Apart from intelligent and socially competent future agents and robots , main applications are found in the medical domain and surveillance in high - risk environments such as steering or controlling .", "label": "", "metadata": {}, "score": "68.46994"}
{"text": "211486 ( SEMAINE ) , Grant no .IST-2002 - 50742 ( HUMAINE ) , and Grant no .IST-2001 - 37599 ( PF - STAR ) .The responsibility lies with the authors .References . A. Ortony , G. L. Clore , and A. Collins , The Cognitive Structure of Emotions , Cambridge University Press , Cambridge , NY , USA , 1988 .", "label": "", "metadata": {}, "score": "68.51093"}
{"text": "Fant G , Liljencrants J , Lin Q : A four - parameter model of glottal flow .STL - QPSR 1985 , 4 : 1 - 13 .Rosenberg AE : Effect of glottal pulse shape on the quality of natural vowels .", "label": "", "metadata": {}, "score": "68.61816"}
{"text": "Minimum value , maximum value , mean , median , standard deviation , value in the first voiced segment , value in the last voiced segment , correlation , slope , and error of the energy linear regression .Vocal effort , arousal of emotions .", "label": "", "metadata": {}, "score": "68.65514"}
{"text": "To better cope with this variety , all features are standardised per partition ( speaker group normalisation ) .Due to the high imbalance among classes ( cf .Table 1 ) , balancing of the training instances is further mandatory to achieve reasonable values of unweighted recall and thus avoid overfitting of strong classes ( here I(dle ) and N(egative ) [", "label": "", "metadata": {}, "score": "68.65794"}
{"text": "The duration of the stressed syllables was altered by lengthening only the vowels .The average duration of the stressed syllables was 310 ms , compared to 185 ms for unstressed syllables .These languages were identical to those used in Thiessen and Saffran 's ( 2003 ) Experiments 1 and 2 .", "label": "", "metadata": {}, "score": "68.660835"}
{"text": "View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C.C. Chang and C.-J. Lin , \" LIBSVM : a library for support vector machines , \" ACM Transactions on Intelligent Systems and Technology , vol .", "label": "", "metadata": {}, "score": "68.67714"}
{"text": "However , due to the reasons sketched passim and in the following , the single ememe , that is , the word , might not be the optimal unit , if it comes to processing in both higher linguistic and emotion modules .", "label": "", "metadata": {}, "score": "68.72925"}
{"text": "In conclusion , the sounds generated by snipe feathers do not appear to be the result of vortex shedding .( A ) Fundamental frequency of vibration ( from SLDV data ) and ( B ) reduced fundamental frequency as a function of U air , for 30 feathers of 26 types [ replotted from Clark et al .", "label": "", "metadata": {}, "score": "68.73946"}
{"text": "Emotion classification experiments were also performed using a GMM - based system with pitch , glottal parameter and formant parameter values as features in order to compare systems that model temporal information contained in linear contour approximations to those that modelled only the statistical distributions of the parameter values .", "label": "", "metadata": {}, "score": "68.753944"}
{"text": "[20 ] presented a study on mental - state estimation for Brain - Computer Interfaces , where the focus was on mental states obtained from the electrocorticograms of patients with medically intractable epilepsy .In this study , mental states were defined as a set of stages which the brain undergoes when a subject is engaged in certain tasks , and brain activity was the only way for the patients to communicate due to motor disabilities .", "label": "", "metadata": {}, "score": "68.8149"}
{"text": "Goal directed actions vs. grounding actions : Goal directed actions are requesting and providing information , while grounding actions are explicit and implicit confirmations , dialogue formalities ( greetings , instructions , etc . ) and unrecognized actions .In addition , we asked the recruited users to complete a questionnaire to assess their subjective opinion about system performance .", "label": "", "metadata": {}, "score": "68.909904"}
{"text": "Happiness and surprise are recognized with higher rate in both cases , and this is most likely due to the bigger facial deformations they cause , which could be easily measured by our method .Figure 4 : Recognition rate of human facial expression for CK+ database .", "label": "", "metadata": {}, "score": "68.92621"}
{"text": "The chosen straightforward strategy is random mixed up - sampling of sparse and downsampling of majority classes ' instances enforcing unit distribution while preserving the total number of instances .Note that the order of operations has an influence on ( un)weighted recall figures [ 33 ] ; we first standardise and then balance the training .", "label": "", "metadata": {}, "score": "68.96588"}
{"text": "Delaborde and Devillers [ 39 ] proposed a similar idea to analyze the immediate expression of emotion of a child playing with an affective robot .The robot reacted according to the prediction of the children emotional response .Although there was no explicit reference to \" mental state \" , their approach processed the child state and employed both emotion and the action that he would prefer according to an interaction profile .", "label": "", "metadata": {}, "score": "68.966415"}
{"text": "Thus , the actions that were considered in the representation of the children state are not directly comparable to the dialogue acts that we address in the paper .Very recently , other authors have developed affective dialogue models which take into account both emotions and dialogue acts .", "label": "", "metadata": {}, "score": "68.97154"}
{"text": "In [ 26 ] , the authors investigate the impairment derived from the inability to recognize others ' mental states as well as the impaired accessibility of certain self - states .This way , they involve into the concept of mental - state terms not only related to emotion ( happy , sad and fearful ) but also to personality , such as assertive , confident or shy .", "label": "", "metadata": {}, "score": "69.15939"}
{"text": "A modelling of part - of - speech ( POS ) sequences yields a classification performance , not much lower than one obtained with acoustic modelling [ 32 ] .POS modelling is rather robust because ASR confusions between words within one POS class have no effect .", "label": "", "metadata": {}, "score": "69.18937"}
{"text": "If the utterance is classified as doubtful_or_bored , it is passed through an additional step in which it is classified according to two dialogue parameters : depth and width .The precision values obtained with the MLP are discussed in detail in [ 44 ] where we evaluated the accuracy of the initial version of this emotion recognizer .", "label": "", "metadata": {}, "score": "69.23123"}
{"text": "Note that the chunk- and the word - based evaluations of WO coincide .Discussion .Classification Performance : The Reality Shock .The scientific community has been used to good or almost perfect classification performance in emotion recognition ; it is such figures that are remembered and implicitly defined as standard .", "label": "", "metadata": {}, "score": "69.34487"}
{"text": "By that , chunks that are mostly neutral but where some words clearly signal the subject 's positive state are considered to be P(ositive ) as well .The heuristic thresholds are adjusted by inspecting the resulting chunk labels .N(egative ) and R(est ) chunks are defined along the same lines .", "label": "", "metadata": {}, "score": "69.42989"}
{"text": "For the FAU Aibo Emotion Corpus , the vocabulary size is 1146 entries .But only a fraction of these words conveys relevant information about the underlying emotional states of a person .In order to reduce the information in a meaningful way , two methods can be applied : stopping and stemming .", "label": "", "metadata": {}, "score": "69.43963"}
{"text": "In such a case , we can hear the change in his tone of voice but most likely we will not be able to measure marked physiological changes .The son might notice such a \" fake \" emotion - if he is clever and has experienced it often enough - or not .", "label": "", "metadata": {}, "score": "69.77496"}
{"text": "As in the intention recognizer , the user simulation generates the user intention level , that is , the user simulator provides concepts and attributes that represent the intention of the user utterance .Additionally , we have added as a novel function the simulation of the output of the emotion recognizer .", "label": "", "metadata": {}, "score": "69.79149"}
{"text": "In each trial , subjects are presented two sound clips , one or none of which contains a salient token .The subject is asked \" Which clip contains a more salient event ? \" and is presented the options \" Clip 1\"/\"Clip 2\"/\"Equal .", "label": "", "metadata": {}, "score": "69.81381"}
{"text": "5A ) , maximizing speed will maximize sound loudness .Therefore , many birds have the option of modulating sound loudness by modulating speed .However , this is not true of all feathers , so for studies of sound function , this should be verified experimentally .", "label": "", "metadata": {}, "score": "69.83434"}
{"text": "Similarly to Lucey et al .'s approach , we achieved high recognition rates for the expressions that cause distinctive facial deformations ( happiness , surprise , and disgust ) .The recognition of other expressions ( anger , sadness , and fear ) experiences confusions due to their subtle facial deformations .", "label": "", "metadata": {}, "score": "70.04067"}
{"text": "For that reason , we neglect the sound produced by flow convecting away from the feather , and considered only the flow around the feather itself .The formation of vorticity around a solid results in time - varying aerodynamic pressure ( lift and drag ) , causing the solid to vibrate in forced response ( Blake , 1986 ) .", "label": "", "metadata": {}, "score": "70.05877"}
{"text": "The longer such a unit is , however , the higher is the probability that it does not only entail one emotional episode but two or more , and that it is \" smeared , \" that is , not unequivocal .", "label": "", "metadata": {}, "score": "70.12607"}
{"text": "These results indicate that the contour modelling approach is better than the static distribution modelling approach for pitch and glottal parameters .Also the pitch , glottal and vocal tract parameters describe independent ( based on the source - filter model ) and distinct components in the speech production mechanism and are hence independent of each other .", "label": "", "metadata": {}, "score": "70.17711"}
{"text": "In this section , we present the three units we will deal with in the following : the word ( ememe ) as basic unit , and as higher units syntactic chunks ( SCs ) consisting of 1 to . words .", "label": "", "metadata": {}, "score": "70.181984"}
{"text": "Tokyo , Japan ; 1998:199 - 204 .Beun RJ : Mental state recognition and communicative effects .J Pragmat 1994 , 21 : 191 - 214 .10.1016/0378 - 2166(94)90019 - 1 View Article .Dragoni AF : Mental states as multi - context systems .", "label": "", "metadata": {}, "score": "70.191086"}
{"text": "Other aspects which could be considered as mental states , such as intentions , had to be derived from these primitive ones .The transitions between mental states and the situations that trigger them have been studied from other perspectives differ from dialogue .", "label": "", "metadata": {}, "score": "70.202835"}
{"text": "For example , if the user does not understand the meaning of the system 's prompt , he or she could abandon the session without uttering a word ( case 1 ) .The other possibility is that the user is considering how to answer the prompt ( case 2 ) .", "label": "", "metadata": {}, "score": "70.24321"}
{"text": "On the other hand , it only detects frontal upright human faces with approximately up to 20 degree rotation around any axis .Samples of the face detector output are shown in Figures 2(b ) - 2(e ) and 2(h ) - 2(l ) .", "label": "", "metadata": {}, "score": "70.24755"}
{"text": "[34 ] , emotion can be understood more widely as a manipulation of the range of interaction affordances available to each counterpart in a conversation .Riccardi and Hakkani - T\u00fcr [ 35 ] studied the impact of emotion temporal patterns in user transcriptions , semantic and dialogue annotations of the How May I help you ? system .", "label": "", "metadata": {}, "score": "70.3987"}
{"text": "However , they only considered physical values such as hunger , pain or temperature .In psychophysiology , these transitions have been addressed by directly measuring the state of the brain .For example , Fairclough [ 19 ] surveyed the field of psychophysiological characterization of the user states , and defined mental states as a representation of the progress within a task - space or problem - space .", "label": "", "metadata": {}, "score": "70.46268"}
{"text": "Ratio users vs. system actions ( us / sysAct ) .Dialogue style / cooperativeness measures .System dialogue acts : confirmation of concepts and attributes , questions to require information and answers generated after a database query .Confirmation rate ( % confirm ) was computed as the ratio between the number of explicit confirmations turns ( n CT ) and the number of turns in the dialogue ( n CT/ n T ) .", "label": "", "metadata": {}, "score": "70.470634"}
{"text": "Any change in timbre or pitch is also counted as a high difference due to the experimental set - up , outlined in Methods .The separability of control trials from test trials demonstrated here is also reflected in the ROC plot .", "label": "", "metadata": {}, "score": "70.47957"}
{"text": "Control trial tokens vary in the same range of pitch and intensity as background tokens of test trials .Each third of the control trials uses one of the three bird sounds in this experiment .Experiment III : Speech .The background in the third experiment emulates a party scene where one can perceive that people are speaking , but can not make out what is being said .", "label": "", "metadata": {}, "score": "70.51921"}
{"text": "Semantically \" rich \" words , that is , content words such as nouns , adjectives , and verbs , tend to be marked emotionally to a higher extent than function words such as particles .For instance , in our data , more EC ( 22.5 % ) than SC ( 21.3 % ) consist only of .", "label": "", "metadata": {}, "score": "70.53221"}
{"text": "( Year : 2005 ) .Mommy and me : familiar names help launch babies into speech - stream segmentation .Psychol .Sci.16 , 298 - 30410.1111/j.0956 - 7976.2005.01531 .x15828977 .Clayards M. A.,Tanenhaus M. K.,Aslin R. N.,Jacobs R. A ..", "label": "", "metadata": {}, "score": "70.58647"}
{"text": "Each unit can therefore be mapped to a vector in this feature space .This frequency can be transformed in various ways [ 42 , page 311 ] , [ 38 ] .The logarithmic term frequency normalised to the inverse document ( here database ) frequency ( TFIDF ) in combination with normalisation to the unit length proved to be the best in our experiments .", "label": "", "metadata": {}, "score": "70.59531"}
{"text": "In this case , there is no consistent phonemic information across the three known words , but all three begin with a stressed syllable .Integrating information across a lexicon like this should lead infants to discover that lexical forms can vary in their phonemic identity , but show a consistent word - initial stress pattern .", "label": "", "metadata": {}, "score": "70.618355"}
{"text": "Moreover , normal distribution has the maximum entropy for a given mean and variance [ 47 ] .Consequently , the error in the position of the detected points is modeled by a bivariate Gaussian distribution , where each facial point location is altered as follows : .", "label": "", "metadata": {}, "score": "70.64453"}
{"text": "The confusion matrix depicting the results obtained by the proposed approach compared to Lucey et al .[ 13 ] published results is shown in Table 2 .We achieved an average recognition rate of 83.01 % compared to 83.15 % achieved by them , taken into consideration that removing contempt expression ( Co ) from their classification algorithm can lead to an improvement in their results .", "label": "", "metadata": {}, "score": "70.719986"}
{"text": "Figure 4 depicts all the parameter contours considered in this work .Parameter contour examples for the phrase ' two thousand one ' .Elements of the source - filter model - based speech characterisation including glottal parameters .Emotional speech corpora .", "label": "", "metadata": {}, "score": "70.72083"}
{"text": "This is different from typical F 0 contour stylisation [ 40 ] since contours in each voiced segment are represented by single line segments rather than piecewise linear approximations .This is done to ensure that a single three - dimensional vector can describe an entire contour for each voiced segment .", "label": "", "metadata": {}, "score": "70.72569"}
{"text": "Once again , SVM classifier outperforms kNN for facial expression recognition .Table 4 : The confusion matrix obtained by applying our approach on BU-4DFE database .The first row in each expression represents results using SVM classifier .The other row shows the results using KNN classifier .", "label": "", "metadata": {}, "score": "70.72767"}
{"text": "Boril H , Hansen JHL : Unsupervised equalization of Lombard effect for speech recognition in noisy adverse environments .IEEE Trans Audio Speech Lang Process 2010 , 28 ( 6):1379 - 1393 .View Article .Bosma W , Andre E : Exploiting emotions to disambiguate dialogue acts .", "label": "", "metadata": {}, "score": "70.77009"}
{"text": "Four sessions were excluded because they had the same number of votes for two different labels .As mentioned , we estimate the user 's internal state without referring to the user 's previous utterance ( i.e. , Phase 2 ) .", "label": "", "metadata": {}, "score": "70.79937"}
{"text": "The microphone was not in the aerodynamic wake of the feather .The top and bottom surfaces of the tunnel were lined with 2.54 cm acoustic foam .The lasers ( described in Clark et al . , 2013 ) and camera recorded the feather through the acrylic walls of the tunnel .", "label": "", "metadata": {}, "score": "70.837715"}
{"text": "This can be tolerated .However , for a turn lasting 10 seconds , the user had to wait 15 seconds - which simply is far too long .Taking any unit below chunk level of course results in even shorter processing time .", "label": "", "metadata": {}, "score": "70.84286"}
{"text": "The emotion recognizer detects the user emotional state by extracting an emotion category from the voice signal and the dialogue history .The intention recognizer takes the semantic representation of the user input and predicts the next user action .Then , in the mental - state composition phase , a mental - state data structure is built from the emotion and intention recognized and passed on to the dialogue manager .", "label": "", "metadata": {}, "score": "70.913284"}
{"text": "Specifically , the glottal model is assumed to be a two - pole low - pass system ( typically with both poles at unity ) whose effects are ' removed ' at the pre - emphasis stage of feature extraction .Section 2 explores the estimation of glottal parameters without making such an assumption .", "label": "", "metadata": {}, "score": "70.96126"}
{"text": "Airflow created vortex whistles around permanent fixtures within the working section , which we individually eliminated or ignored .To further reduce background noise and reverberation , 2.54 cm acoustic foam ( noise reduction coefficient : 0.72 ) was mounted on the floor and ceiling of the tunnel ( Fig .", "label": "", "metadata": {}, "score": "70.97565"}
{"text": "It is computed taking into account the information provided by the user throughout the history of the dialogue , and the last system turn .The formal description of the proposed model is as follows .Let A i be the output of the dialogue system ( the system answer ) at time i , expressed in terms of dialogue acts .", "label": "", "metadata": {}, "score": "71.02411"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. Jiang , Y. Fang , Z. Zhou , and Y. Tan , \" Active shape model with random forest for facial features detection , \" in Proceedings of the 21st", "label": "", "metadata": {}, "score": "71.044754"}
{"text": "The user 's internal states represent various aspects of the user , such as belief [ 1 ] , preference [ 2 ] , emotion [ 3 ] , and familiarity with the system [ 4 - 6 ] .These aspects can also be categorized according to their persistency : as the user 's knowledge and preference are persistent , they can be used for personalizing of the dialog system [ 7 ] .", "label": "", "metadata": {}, "score": "71.05968"}
{"text": "In [ 27 ] it has been shown that in a Wizard - of - Oz - scenario ( appointment scheduling dialogues ) , it is beneficial not to model whole turns but to divide them into smaller , syntactically and semantically meaningful chunks along the lines of [ 9 ] .", "label": "", "metadata": {}, "score": "71.09663"}
{"text": "The Bayesian brain uses generative models to predict sensory input , adjusting its internal probabilistic representations based on novel sensory information .In this setup , predictive coding corresponds to minimizing error between bottom - up sensations and top - down predictions , with the corresponding mismatch signaling the detection of a deviant .", "label": "", "metadata": {}, "score": "71.097046"}
{"text": "To respond appropriately , the spoken dialog system should be able to consider the user 's internal state before the user 's input .Conventional studies on user modeling have focused on the linguistic information of the utterance for estimating the user 's internal state , but this approach can not estimate the user 's state until the end of the user 's first utterance .", "label": "", "metadata": {}, "score": "71.114685"}
{"text": "The procedure of this experiment was identical to that used in Experiment 1A. Results .If infants segment fluent speech via sensitivity to conditional statistical information , they should show the same pattern of preference in the test phase , regardless of whether they heard the trochaic or iambic language , because the conditional statistical information is identical across these two languages .", "label": "", "metadata": {}, "score": "71.1203"}
{"text": "The Length of Speech Classification .Next , we investigated the audio signal of the segment before the user 's input in detail .As we can discriminate state C and the other states using the feature explained above , the remaining problem is how to discriminate dialog of States A and B. .", "label": "", "metadata": {}, "score": "71.227325"}
{"text": "Any change in the strength of an individual vortex results in a change in pressure ; therefore , as this vortex forms or dissipates , it produces sound ( Blake , 1986 ; Lighthill , 1952 ) .Vortex formation adjacent to a solid produces a dipole pattern of acoustic radiation ( Blake , 1986 ) .", "label": "", "metadata": {}, "score": "71.2368"}
{"text": "We suspect that the counter - examples ( Clark et al . , 2013 ) ( Fig .5 ) are the result of changes in feather aeroelastic deformation caused by changes in U air .Likewise , for many of the feathers the relative strength of harmonics varied with changes in U air , which may have been caused by small changes in aeroelastic deformation that slightly changed the mode shape of flutter .", "label": "", "metadata": {}, "score": "71.295746"}
{"text": "In the case of anger , if the dialogue history shows that there have been many errors during the interaction , the system apologizes and switches to DTMF ( Dual - Tone Multi - Frequency ) mode .If the user is assumed to be angry but the system is not aware of any error , the system 's prompt is rephrased with more agreeable phrases and the user is advised that they can ask for help at any time .", "label": "", "metadata": {}, "score": "71.33739"}
{"text": "Experiment II : Nature .The scene setup of this experiment is a busy natural forest environment with singing birds .Individual calls at approximately 4.9 kHz pitch and 1.3 - 1.5 s length are manually extracted at 44.1 kHz .Recordings of wind and water sounds are added to every trial to reduce signal - to - noise ratio , and make the task more challenging while retaining the \" natural \" scene set - up .", "label": "", "metadata": {}, "score": "71.54291"}
{"text": "4 , pp .941 - 944 , Honolulu , Hawaii , USA , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Scopus .J. Schwitalla , Gesprochenes Deutsch : Eine Einf\u00fchrung , Erich Schmidt , Berlin , Germany , 1997 .", "label": "", "metadata": {}, "score": "71.55954"}
{"text": "Finally , spike trains from multi - channel axes ( e.g. , different frequency channels in the high - frequency spectrogram ) are grouped together .If there are multiple spikes at the same time instant , the maximum one is recorded .", "label": "", "metadata": {}, "score": "71.580414"}
{"text": "As the number of possible sequences of states is very large , we establish a partition in this space ( i.e. in the history of the dialogue up to time i ) .Let UR i be what we call user register at time i .", "label": "", "metadata": {}, "score": "71.79316"}
{"text": "The first consists of 39 hours of speech , stemming from 154 speakers in gender balance , and will serve to evaluate features and algorithms for the estimation of speaker intoxication .The second features 10 hours of speech recordings of 99 subjects , annotated in 10 different levels of sleepiness .", "label": "", "metadata": {}, "score": "71.79697"}
{"text": ", 1996 ) that produces tonal sound .Self - sustaining periodic behavior is stable as a result of a feedback mechanism .A key mechanistic question is what roles the stiffness and resonance frequencies of the feather play in generating this feedback and ensuing sound .", "label": "", "metadata": {}, "score": "71.81096"}
{"text": "The current experiments are relevant to that question in two ways .First , they reinforce the claim that sensitivity to statistical information is apparent for linguistic input at a younger age than the commonly cited 7 - 8 months ( c.f .", "label": "", "metadata": {}, "score": "71.81494"}
{"text": "Face Orientation Feature .As mentioned above , the face orientation feature is thought to be effective for identifying a user 's internal state , but the classification into nine orientations is too coarse and was too difficult for the actual system to use because the labels were assigned manually .", "label": "", "metadata": {}, "score": "71.83508"}
{"text": "Table 9 displays the confusion matrix for the fusion of acoustics and linguistics for the word evaluation ( cf .the two last columns in Table 7 , row WO ) , giving an impression of the confusion between classes .The confusion between the classes is as expected ; no much confusion between P(ositive ) and N(egative ) , most confusion between I(dle ) ( and partly R(est ) ) and the other classes .", "label": "", "metadata": {}, "score": "71.91081"}
{"text": "It is argued that robust computer vision algorithms for face analysis and recognition are based on configural and shape features [ 35 ] .These features are defined as distances between facial components ( mouth , eye , eyebrow , nose , and jaw line ) .", "label": "", "metadata": {}, "score": "71.91772"}
{"text": "2 in the companion paper ( Clark et al . , 2013 ) ] .Essentially , all feathers produced loud higher harmonics , and some feathers had 30 or more harmonics ( supplementary material Fig .S2D ) .Moreover , individual feathers exhibited several types of non - linear responses to airflow not predicted by the vortex whistle hypothesis .", "label": "", "metadata": {}, "score": "71.949585"}
{"text": "Each image sequence starts with onset ( neutral expression ) and ends with a peak expression ( last frame ) .The offered peak expression is fully coded by Facial Action Coding System using FACS investigator guide .Keyframes within each image sequence were manually labeled with 68 points , and after that a gradient descent active appearance model ( AAM ) is used to fit these points in the remaining frames .", "label": "", "metadata": {}, "score": "71.979065"}
{"text": "A . t . )F .A . t .u . t . )Z .t . )H .A . t . )v .t . )The state vector and the system matrices reflect a random walk , and can be encoded as : .", "label": "", "metadata": {}, "score": "72.00838"}
{"text": "This can be due to the restriction defined for a maximum number of turns per dialogue in the user simulation .Also , there were more dialogues in which the recruited users asked for more information than strictly required to optimally fulfil their scenarios .", "label": "", "metadata": {}, "score": "72.01071"}
{"text": "The percentage of successfully completed tasks .In each scenario , the user has to obtain one or several pieces of information , and the dialogue success depends on whether the system provides the correct data ( according to the aims of the scenario ) or incorrect data to the user .", "label": "", "metadata": {}, "score": "72.02695"}
{"text": "Average feather velocity was also correlated with loudness in most but not all feathers ( Fig .4B ) .Our estimates of ' average velocity ' were biased as a result of limits on the SLDV ( Clark et al . , 2013 ) .", "label": "", "metadata": {}, "score": "72.033875"}
{"text": "Fig .3 shows three examples of three ranges of vibratory behavior ( I , II and III ) in airflow .An expanded version of Fig .3 showing the corresponding vibratory and acoustic spectra is presented in the supplementary material Fig .", "label": "", "metadata": {}, "score": "72.06073"}
{"text": "The choice of a brute force search was made purely due to the simplicity of the search algorithm even though it is not efficient .This approach was considered acceptable since the glottal parameters themselves are the focus of the work reported in this paper and not their estimation algorithms .", "label": "", "metadata": {}, "score": "72.117424"}
{"text": "Of course , this only holds for speech ; there is no equivalent - at least no one that can be defined and segmented easily - in the other modalities .Note that our \" ememe \" is the smallest emotional unit .", "label": "", "metadata": {}, "score": "72.22125"}
{"text": "Figure 13 shows a comparison between these categories .As can be observed , the dialogues provided by the mental - state system have a better quality , as the proportion of goal - directed actions is higher .Proportion of turns of goal directed actions , ground actions and rest of possible actions in the mental - state and baseline systems .", "label": "", "metadata": {}, "score": "72.23453"}
{"text": "Soc .Am .10.1121/1.1379076 View Article .Hui - Ling L : JO Smith III , Joint estimation of vocal tract filter and glottal source waveform via convex optimization , in 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics .", "label": "", "metadata": {}, "score": "72.30183"}
{"text": "Funchal , Portugal ; 2004:85 - 92 .Wilks Y , Catizone R , Worgan S , Turunen M : Some background on dialogue management and conversational speech for dialogue systems .Comput Speech Lang 2011 , 25 ( 2):128 - 139 .", "label": "", "metadata": {}, "score": "72.364944"}
{"text": "This can be done when the user logs in to the system before starting the dialogue .If the system does not have information about the identity of the user , we take the first user utterance as neutral assuming that he is not placing the telephone call already in a negative emotional state .", "label": "", "metadata": {}, "score": "72.37663"}
{"text": "The emotion recognizer obtains the user emotional state from the acoustics of their utterance as well as the dialogue history .The intention recognizer decides the next user action and their dialogue goal using a statistical approach that relies on the previous user input and system prompt .", "label": "", "metadata": {}, "score": "72.41014"}
{"text": "However , there is none , irrespective of the modalities .Arousal might be traced back in physiological signals by defining a threshold criterion based on , for example , Feeltrace annotations [ 8 ] , but this is way more difficult for valence .", "label": "", "metadata": {}, "score": "72.43866"}
{"text": "Control trials consist of just the background scene , while test trials have one \" foreground \" salient token in addition to the background .The foreground token differs from background tokens in one or more of the experiment factors ( i.e. , acoustic attributes of the foreground token ) .", "label": "", "metadata": {}, "score": "72.44734"}
{"text": "[ 27 ] .In some application domains , it is fundamental to recognize the affective state of the user to adapt the systems behaviour .For example , in emergency services [ 28 ] or intelligent tutors [ 29 ] , it is necessary to know the user emotional state to calm them down , or to encourage them in learning activities .", "label": "", "metadata": {}, "score": "72.451706"}
{"text": "Regarding the dialogue participant activity , Figure 10 shows the ratio of user versus system actions .The dialogues of the mental - state system have a higher proportion of system actions due to a reduction of the confirmation turns ( 0.015 significance ) .", "label": "", "metadata": {}, "score": "72.46192"}
{"text": "The ubiquity of deviance detection in sensory cortex raises the question of commonalities among different senses in attentional selection mechanisms ; or whether the parallels between audition and other senses are limited to change detection in dynamic sequences and time - dependent signals .", "label": "", "metadata": {}, "score": "72.47676"}
{"text": "I d , which he can use to log in to the system ; .Gender ; .Experience , which can be either 0 for novel users ( first time the user calls the system ) or the number of times the user has interacted with the system ; .", "label": "", "metadata": {}, "score": "72.57635"}
{"text": "Some of the knowledge obtained from these works might be useful for our problem ; however , it seems to be difficult to discriminate the user 's internal state at Phase 2 such as State A and State B using only cues for turn - taking .", "label": "", "metadata": {}, "score": "72.583954"}
{"text": "In the limit of negligible stiffness , flutter may be chaotic ( see also Alben and Shelley , 2008 ) unless periodicity is regulated by a fluid - generated process , such as upstream vortex formation ( Manela and Howe , 2009a ; Manela and Howe , 2009b ) .", "label": "", "metadata": {}, "score": "72.60759"}
{"text": "C.-W. Hsu , C.-C. Chang , and C.-J. Lin , \" A practical guide to support vector classification , \" Tech .Rep. , Department of Computer Science , National Taiwan University , 2003 .View at Google Scholar Segmenting into Adequate Units for Automatic Recognition of Emotion - Related Episodes : A Speech - Based Approach . 1 Pattern Recognition Laboratory , Friedrich - Alexander - Universit\u00e4t Erlangen - N\u00fcrnberg ( FAU ) , D-91058 Erlangen , Germany 2 ESAT , Katholieke Universiteit Leuven , B-3001 Leuven , Belgium 3 Institute for Human - Machine Communication , Technische Universit\u00e4t M\u00fcnchen ( TUM ) , D-80333 Munich , Germany .", "label": "", "metadata": {}, "score": "72.68921"}
{"text": "The baseline dialogues have a higher standard deviation ( 3.80 ) given that the proportion of number of turns per dialogue is more disperse .The dialogues gathered with the mental - state system have a smaller deviation ( 3.20 ) since the successful dialogues are usually those which require the minimum number of turns to achieve the objective(s ) predefined for both kinds of scenario .", "label": "", "metadata": {}, "score": "72.69161"}
{"text": "For the new user calls of the experiments ( described in the \" Evaluation methodology \" section ) , recruited users were provided with a numeric password .Once we have obtained the normalized features , we classify the corresponding utterance with a multilayer perceptron ( MLP ) into two categories : angry and doubtful_or_bored .", "label": "", "metadata": {}, "score": "72.816216"}
{"text": ", 1996 ) and produces ' flutter - induced sound ' .Our approach to testing these hypotheses was to test whether vortex formation in response to feather geometry alone is sufficient to explain the observed feather dynamics and sound production .", "label": "", "metadata": {}, "score": "72.83516"}
{"text": "ACKNOWLEDGEMENTS .We are indebted to three anonymous reviewers for helpful comments , Glenn Weston - Murphy in the Mechanical Engineering Department at Yale University for support and access to the wind tunnel , and to Anand Varma for use of two photos .", "label": "", "metadata": {}, "score": "72.862915"}
{"text": "It may also be privileged or otherwise protected by work product immunity or other legal rules .If you have received it by mistake please let us know by reply and then delete it from your system ; you should not copy it or disclose its contents to anyone .", "label": "", "metadata": {}, "score": "72.95508"}
{"text": "Stemming instead is a method for reducing different morphological forms of a word to its base form .The Iterated Lovins Stemmer [ 41 ] is used for the experiments in this paper .The main idea of the bag of words approach is the representation of words ( or lexemes if stemming is applied ) as numeric features .", "label": "", "metadata": {}, "score": "72.9601"}
{"text": "There are two different types of evaluation : first we evaluate for the whole units WO , SC , and EC ; note that the total number of chunks is different for each of these units .Then , we evaluate WO , SC , and EC by checking each word in these units whether it has been classified correctly , that is , attributed to the class the higher unit it belongs to has been annotated with .", "label": "", "metadata": {}, "score": "72.99919"}
{"text": "We therefore decided in favour of hybrid syntactic - prosodic criteria : higher syntactic boundaries always trigger chunking ; whereas lower syntactic boundaries do so only if the adjacent pause is .Note that in earlier studies , we found out that there is a rather strong correlation of more than 90 % between prosodic boundaries , syntactic boundaries , and dialogue act boundaries [ 9 ] .", "label": "", "metadata": {}, "score": "73.02319"}
{"text": "However , the son will be well advised to react as if the father had expressed \" real \" anger in his tone of voice as well .Moreover , it can not be argued that this is not an indication of negative valence - note that in this paper , we map our raw labels onto main classes representing positive , neutral , or negative valence .", "label": "", "metadata": {}, "score": "73.08614"}
{"text": "However , this difference was low because our mental state recognizer tends to classify utterances as emotional rather than neutral , as described in the section \" New model for predicting the user mental state \" .We have previously described the differences between both systems in terms of number of turns .", "label": "", "metadata": {}, "score": "73.12756"}
{"text": "Lexical stress was created by altering three parameters of the stimuli : pitch contour , amplitude , and duration .The pitch contour in the stressed syllables was based on the pitch contours of an adult native English speaker producing the lexical items .", "label": "", "metadata": {}, "score": "73.19985"}
{"text": "We quantitatively examined the onset and behavior of flutter as a function of feather size , shape and U air at a single , constant orientation , while we qualitatively explored the relationship between orientation and other variables .We selected a test orientation by placing the feather in the tunnel in an orientation simulating that feather in a flying bird with its tail widely spread ( Fig . 1 ) .", "label": "", "metadata": {}, "score": "73.20111"}
{"text": "Baker et al .[ 37 ] described a specific experience for the case of computer - based learning systems .They found that boredom significantly increases the chance that a student will game the system on the next observation .However , the authors do not describe any method to couple emotion and the space of afforded possible actions .", "label": "", "metadata": {}, "score": "73.227325"}
{"text": "A .i . m .F .i . m .A . i .Previously , a listening test was conducted on the LDC corpus and the results were reported in [ 21 ] .The overall accuracy obtained by 11 human listeners was 63.6 % .", "label": "", "metadata": {}, "score": "73.22858"}
{"text": "Deciding between Types of Units .At least three aspects are relevant for deciding between WO , SC , and EC - or any other type of sequencing emotional episodes : first , performance ; second , adequacy in real - life applications ; third , perceptual , cognitive adequacy .", "label": "", "metadata": {}, "score": "73.23273"}
{"text": "Thus , vortex formation and shedding are not sufficient to explain sound production by these feathers .In the companion paper ( Clark et al . , 2013 ) we show that the onset of flutter coincides with discrete structural resonance frequencies of feathers , providing further support for the aeroelastic flutter hypothesis .", "label": "", "metadata": {}, "score": "73.34212"}
{"text": "We conducted the unpaired .-test on the frequency of face direction in States A and B and found a significant difference in the frequency of the frontal frames .This result shows that the users who were considering the answer tended to turn their face away from the system compared to the perplexed users .", "label": "", "metadata": {}, "score": "73.377716"}
{"text": "The data collection was carried out on a Wizard - of - Oz basis .We prepared an agent with a simple cartoon - like face and synthesized voice and displayed it on an LCD monitor to encourage the user to pay attention to the front of the system .", "label": "", "metadata": {}, "score": "73.58316"}
{"text": "If the proportion of neutral raw labels is lower but at least 50 % and raw labels of only one other main class appear , the chunk is assigned I(dle ) as well .These are the cases where single words signal one nonneutral main class but where the proportion of these words is too low .", "label": "", "metadata": {}, "score": "73.843155"}
{"text": "Control trial tokens vary in the same range of pitch and intensity as background tokens of test trials .Sixty percent of control trials use one speaker , while forty percent use the other speaker .Computational Model .Computation of sound features .", "label": "", "metadata": {}, "score": "73.8598"}
{"text": "The data was collected from 51 children ( age 10 - 13 , 21 male , 30 female ) .The children were from two different schools , Mont and Ohm .The recordings took place in a classroom at each school .", "label": "", "metadata": {}, "score": "73.93576"}
{"text": "Depth represents the total number of dialogue turns up to a particular point of the dialogue , whereas width represents the total number of extra turns needed throughout a subdialogue to confirm or repeat information .The computation of depth and width is carried out according to the dialogue history , which is stored in log files .", "label": "", "metadata": {}, "score": "73.93805"}
{"text": "Experimental setup and coordinate system .( A ) Lab - based coordinate system ( side view ) .Rotation about Y was angle \u03b1 and rotations about the Z -axis ( projects out of the page ) was angle \u03b2 .", "label": "", "metadata": {}, "score": "73.9695"}
{"text": "This variable scaling of loudness with U air has an important implication for studies of the function of these sounds .Sound loudness may play a role in the function of a sound , such as in courtship displays .Flight speed is easily behaviorally modified ( e.g. by diving ) .", "label": "", "metadata": {}, "score": "74.02202"}
{"text": "According to this theory , dialogue is characterized as a set of actions to change the interlocutor 's mental state and reach the goals of the interaction .This way , the mental state is addressed as the user 's beliefs and intentions .", "label": "", "metadata": {}, "score": "74.05902"}
{"text": "( B ) Loudness plotted against average velocity of points measured across a feather .Lines are regressions for individual feathers .See the companion paper ( Clark et al . , 2013 ) for further details .Inset in B shows the same data replotted with different shapes indicating the mode of flutter ; tv , trailing vane .", "label": "", "metadata": {}, "score": "74.12489"}
{"text": "Here , the goal of our work is to develop a multimodal dialog system that behaves like a human receptionist , who should determine the user 's internal state by only observing the user 's behavior .Therefore , we decided to evaluate the sessions by evaluators ' observation .", "label": "", "metadata": {}, "score": "74.13771"}
{"text": "Each test condition is repeated eight times ( with non - identical backgrounds ) .25 % of trials are control trials .Control trial tokens vary in the same range of pitch and intensity as background tokens of test trials .", "label": "", "metadata": {}, "score": "74.20261"}
{"text": "F .^ .c . arg .min .Fg .Ag .Fc .G .^ .G .Fg .Ag .Fc .An overview of the glottal spectral parameter estimation method is given in Figure 2 .", "label": "", "metadata": {}, "score": "74.21797"}
{"text": "Competing interests .Authors ' Affiliations .Department of Languages and Computer Systems , CITIC - UGR , University of Granada .References .Jokinen K : Natural interaction in spoken dialogue systems .In Proceedings of the Workshop Ontologies and Multilinguality in User Interfaces .", "label": "", "metadata": {}, "score": "74.22374"}
{"text": "In these respects , the INTERSPEECH 2011Speaker State Challenge shall help bridging the gap between excellent research on paralinguistic information in spoken language and low compatibility of results .Two sub - challenges are addressed : .In the Intoxication Sub - Challenge , alcoholisation of speakers has to be determined as two - class classification task : alcoholised for a BAC exceeding 0.5 or non - alcoholised .", "label": "", "metadata": {}, "score": "74.28228"}
{"text": "Percentage of pairs for which both versions were assigned the same emotion by the listener .Each listener was given 30 pairs , one version using actual contours of and one using linear approximations to F g , A g and F c .", "label": "", "metadata": {}, "score": "74.31955"}
{"text": "Under this hypothesis , vortex shedding was predicted to occur at a constant St .Reduced frequency varied by an order of magnitude among the feathers tested , and in all feathers significantly declined with increasing airspeed ( Fig .5B ) .", "label": "", "metadata": {}, "score": "74.38801"}
{"text": "For spoken words , the deviance is detected during the first half of the token onset .In some cases , the model finds the offset deviance instead of onset deviance .( B ) Regardless of whether the maximum likelihood of saliency was inside the foreground token duration , the feature that the saliency was detected in is shown .", "label": "", "metadata": {}, "score": "74.40236"}
{"text": "Subjective evaluation .A listening test was conducted to determine whether linear approximations to glottal parameter contours are able to retain emotion - specific information .Fourteen untrained listeners were involved in this test .For each listener , 30 speech utterances were drawn at random from the LDC corpus such that there were 6 utterances from each of the five emotions ( neutral , anger , sadness , happiness and boredom ) .", "label": "", "metadata": {}, "score": "74.584496"}
{"text": "Unlike the white spaces between words on the written page , pauses do not consistently mark word boundaries in fluent speech .This is not troublesome for adults , who can identify word boundaries in large part due to their familiarity with the word forms in their native language ( e.g. , Nazzi et al . , 2005 ; Norris and McQueen , 2008 ) .", "label": "", "metadata": {}, "score": "74.67174"}
{"text": "Piscataway ; 1999:79 - 82 .Riegelsberger EL , Krishnamurthy AK : Glottal source estimation : methods of applying the LF - model to inverse filtering , in 1993 IEEE International Conference on Acoustics , Speech , and Signal Processing , ICASSP-93 , vol 2 . 2nd edition .", "label": "", "metadata": {}, "score": "74.75906"}
{"text": "The percentage of this kind of answers was lower in the corpus acquired with real users .This can be explained by the fact that it is less probable that simulated users provide useless information .In fact , there was a lower percentage of users ' turns classified as \" Other answers \" .", "label": "", "metadata": {}, "score": "74.77356"}
{"text": "The acoustic properties of individual feathers were tested in the working section of an Eiffel - style open wind tunnel in the Department of Mechanical Engineering at Yale University , near sea level .The usable area of the working section measured 61 cm wide by 33 cm tall , with a contraction area ratio of 8.75 .", "label": "", "metadata": {}, "score": "74.79914"}
{"text": "Using the codification previously described for the information in the UR , every dialogue begins with a dialogue register in which every value is equal to 0 in the greeting turn of the system .Each time the user provides information , it is used to update the previous UR and obtain the current one , as shown in Figure 5 .", "label": "", "metadata": {}, "score": "74.852844"}
{"text": "Scenarios of type S1 defined only one objective for the dialogue ( e.g. to obtain timetable information of a specific subject ) .Scenarios of type S2 defined two objectives for the dialogue ( e.g. to obtain timetables of a specific subject and registration deadlines for the corresponding degree ) .", "label": "", "metadata": {}, "score": "74.862"}
{"text": "In all , as sound loudness is clearly a salient feature of flutter - induced sounds that are incorporated into displays , multi - microphone experiments to quantify directionality of the sound field of a fluttering feather are warranted .Airflow and orientation - induced changes in feather aeroelastic bending seemed to greatly influence the dynamics of sound production of some of the feathers .", "label": "", "metadata": {}, "score": "74.86997"}
{"text": "i . m .A .i . m .F .i . m .A . i . . .Table 8 .Confusion matrix for the HMM - based system using all parameter contours , .i . i .", "label": "", "metadata": {}, "score": "74.93535"}
{"text": "The labelling of the system turns is similar to the labelling defined for the user turns .To do so , 30 task - dependent concepts were defined : .Task - independent concepts ( Affirmation , Negation , Not - Understood , New - Query , Opening and Closing ) .", "label": "", "metadata": {}, "score": "74.99664"}
{"text": "These perceptual features encapsulate much of the information that is extracted from the cochlea to mid - brain ( Yang et al . , 1992 ) .A limited number of studies have established the existence of two - way interactions in the perception of some of these features ( Melara and Marks , 1990 ; Allen and Oxenham , 2013 ) ; however , the extent of these interactions pertaining to attention is yet unknown .", "label": "", "metadata": {}, "score": "74.997696"}
{"text": "Emails are not secure and can not be guaranteed to be error free as they can be intercepted , amended , lost or destroyed , or contain viruses .Anyone who communicates with us by email is taken to accept these risks .", "label": "", "metadata": {}, "score": "75.260956"}
{"text": "However , the estimation of the corner frequency , F c , is affected to a much larger degree by the errors and the estimated values were not very reliable .Therefore , F c was ignored and only the glottal formant frequency and magnitudes , F g and A g , were used in the automatic classification results reported in this paper .", "label": "", "metadata": {}, "score": "75.35701"}
{"text": "Emotions affect the explicit message conveyed during the interaction .They change people 's voices , facial expressions , gestures and speech speed ; a phenomenon addressed as emotional colouring [ 30 , 31 ] .Emotions can also affect the actions that the user chooses to communicate with the system .", "label": "", "metadata": {}, "score": "75.43236"}
{"text": "It can be stand alone or modulated onto speech ( speech laughter ) .Laughter and speech laughter are mainly found at the end of syntactic units .This does not necessarily mean that laughter and speech / language are processed and generated in the same module , but it demonstrates a close relationship .", "label": "", "metadata": {}, "score": "75.46286"}
{"text": "Ratio of user versus system actions in the mental - state and baseline systems .Regarding dialogue style and cooperativeness , the histograms in Figures 11 and 12 , respectively , show the frequency of the most dominant user and system dialogue acts in the dialogues collected with the mental - state and baseline systems .", "label": "", "metadata": {}, "score": "75.50876"}
{"text": "Three sequences of bird calls with 0.5 s phase shift are added for a total duration of 6 s. The foreground token onset is randomly chosen between 58 % and 68 % of the trial length .Each individual background token is used at most two times within the same trial .", "label": "", "metadata": {}, "score": "75.525764"}
{"text": "Alternatively , sound is generated by an aeroelastic process , i.e. the stiffness and accompanying resonance frequencies of the airfoil are not negligible and play an important role in the dynamics of oscillation ( Bisplinghoff et al ., 1996 ) .", "label": "", "metadata": {}, "score": "75.62537"}
{"text": "Emotion recognition Paralinguistic information Pitch contours Formant contours Glottal spectrum Temporal information LDC emotional prosody speech corpus .Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1687 - 4722 - 2013 - 19 ) contains supplementary material , which is available to authorized users .", "label": "", "metadata": {}, "score": "75.64905"}
{"text": "Note the logarithmic y -axis .( B ) Reduced frequency of flutter as a function of U air .Length L is indicated ; this length was approximately parallel to flow for feathers exhibiting trailing vane modes ( blue ) , whereas it was at an oblique angle to flow for the remaining feathers .", "label": "", "metadata": {}, "score": "75.667984"}
{"text": "The feathers tested here differed primarily in shape ( Fig .5 ) and did not vary much in size ( dimensions in supplementary material Table S1 ; vane thickness is on the order of 10 \u03bcm ) .As all feathers are made of \u03b2 - keratin ( Bonser and Purslow , 1995 ; Brush , 1983 ) , the feathers tested here were of similar elastic modulus .", "label": "", "metadata": {}, "score": "75.74217"}
{"text": "The user can also be bored when the number of errors is low ( below 20 % ) but the dialogue has been long .If the dialogue has been short and with few errors , the user is considered to be doubtful because in the first stages of the dialogue is more likely that users are unsure about how to interact with the system .", "label": "", "metadata": {}, "score": "75.79283"}
{"text": "International Language Technologies Conference ( IS - LTC 06 ) .Ljubljana , Slovenia ; 2006:246 - 250 .Bickmore T , Giorgino T : Some novel aspects of health communication from a dialogue systems perspective .In Proceedings of AAAI Fall Symposium on Dialogue Systems for Health Communication .", "label": "", "metadata": {}, "score": "75.82788"}
{"text": "This process is illustrated in Figure 3 and modeled as : . x .i .t . ) x .i .t . ) w .i .i .j .[ .n . ] j .i . w .", "label": "", "metadata": {}, "score": "75.83307"}
{"text": "This is the case of recent psychological studies in which mental states do not cope with beliefs , intentions or actions , but rather are considered emotional states .For example , Dyer et al .[ 24 ] presented a study on the cognitive development of mental - state understanding of children in which they discovered the positive effect of storybook reading to make children more effective being aware of mental states .", "label": "", "metadata": {}, "score": "75.87982"}
{"text": "Our data suggest several predictions about the vorticity of a fluttering feather .Given the ( usually ) coherent nature of both the feather 's kinematics and the recorded sound , the vorticity probably forms coherent vortical structures .The feather 's kinematics were sometimes spatially complex , with different regions moving with differing frequencies and/or phase .", "label": "", "metadata": {}, "score": "75.899445"}
{"text": "Biol .Estimating a User 's Internal State before the First Input Utterance .Graduate School of Engineering , Tohoku University , 6 - 6 - 5 Aramaki aza Aoba , Aoba - ku , Sendai , Miyagi 980 - 8579 , Japan .", "label": "", "metadata": {}, "score": "76.03522"}
{"text": "Based on these linear approximations , the glottal contribution to the amplitudes of the pitch harmonics , G ( f , t ) , was computed as per the stylised glottal flow derivative spectrum ( Figure 1 ) used to estimate the glottal parameters .", "label": "", "metadata": {}, "score": "76.08828"}
{"text": "It is not only difficult to define \" emotion , \" it is difficult as well to find out where an emotional episode - whatever it is - begins and where it ends .It is difficult both for theoretical reasons - in order to know where it begins and ends , we have to know what it is - and for methodological / practical reasons as well , which we will detail below .", "label": "", "metadata": {}, "score": "76.10127"}
{"text": "In Proceedings of Interspeech'10 .Makuhari , Chiba , Japan ; 2010:502 - 505 .Baker RSJd , D'Mello SKD , Rodrigo MMT , Graesser AC : Better to be frustrated than bored : the incidence , persistence , and impact of learners ' cognitive - affective states during interactions with three different computer - based learning environments .", "label": "", "metadata": {}, "score": "76.15034"}
{"text": "Also of interest is that these results suggest that information about the first formant is significantly more important than information about the second and third formants .However , classification tests indicated that this does not hold for the LDC corpus where dropping information about the second and third formants caused the classification accuracy to reduce from 62.6 % to 57.5 % .", "label": "", "metadata": {}, "score": "76.164566"}
{"text": "In a previous study ( Clark et al . , 2011a ) we did not explicitly consider the underlying mechanism .Here , we present further data on how hummingbird feathers flutter as a function of airspeed .We have split the analyses into two parts .", "label": "", "metadata": {}, "score": "76.22542"}
{"text": "When emotions were also taken into account , i.e. when even with the same sequence of intentions two dialogues were considered different if the emotions observed were different , we obtained a higher percentage of different dialogues in the case of the simulated users .", "label": "", "metadata": {}, "score": "76.31293"}
{"text": "Finally , in some cases , at high test speeds the rachis or calamus of the feather suddenly broke .Such failure seemed to be caused or facilitated by flutter ( see Clark et al . , 2013 ) .Most of the feathers exhibited a positive correlation between loudness and U air ( Fig .", "label": "", "metadata": {}, "score": "76.32109"}
{"text": "This language was constructed such that two of the words - dapu and dobi - occurred twice as often ( 90 times ) as the other two words ( diti and bugo , each of which occurred 45 times ) .This ensures that test item foils can be constructed that differ solely on their conditional probabilities , rather than on the frequency with which infants hear them ( for discussion , see Aslin et al .", "label": "", "metadata": {}, "score": "76.340294"}
{"text": "Time synchronous or adjacent emotional messages conveyed by different modalities can be congruent or incongruent ; speech can even distort the emotional message conveyed via facial gesture or bio - signals because of lip and jaw movements .Here we find a high functional load on speech .", "label": "", "metadata": {}, "score": "76.34983"}
{"text": "Conclusion .Commonly automatic emotion recognition systems capture spectral information with frame - based information and temporal information by either computing a range of functionals over all the frame - level features in an utterance or by using a suitable back - end to model temporal variations of these frame - level features .", "label": "", "metadata": {}, "score": "76.40094"}
{"text": "Regarding the number of dialogue turns , the mental - state system produced shorter dialogues ( with a 0.000 significance value in the t test when compared to the number of turns of the baseline system ) .As shown in Table 4 , this general reduction in the number of turns is particularized also to the case of the longest , shortest and most seen dialogues for the mental - state system .", "label": "", "metadata": {}, "score": "76.43425"}
{"text": "Investigating bottom - up auditory attention .Department of Electrical and Computer Engineering , The Johns Hopkins University , Baltimore , MD , USA .Bottom - up attention is a sensory - driven selection mechanism that directs perception toward a subset of the stimulus that is considered salient , or attention - grabbing .", "label": "", "metadata": {}, "score": "76.43692"}
{"text": "This period contains silence , repairs , fillers , and breathy voice of the user .We manually determined this length for each dialog .Figure 4 shows the mean length of this segment in the \" neutral \" and \" other \" dialogs , where we can see large differences between the two types of dialog .", "label": "", "metadata": {}, "score": "76.50885"}
{"text": "The partition that we establish in this space is based on the assumption that two different sequences of states are equivalent if they lead to the same UR .After applying the above considerations and establishing the equivalence relations in the histories of dialogues , the selection of the best U i is given by : .", "label": "", "metadata": {}, "score": "76.57453"}
{"text": "Sound frequency is predicted to scale linearly with flow velocity ( Eqn 1 ) , and harmonics above 2 f are not expected .Under this whistle model , any motion of the feather is a forced response to the fluid flow , i.e. the motion is a vortex - induced vibration ( Williamson and Gorvardhan , 2004 ) .", "label": "", "metadata": {}, "score": "76.69002"}
{"text": "The dialogue manager considers that the dialogue is unsuccessful and decides to abort it when the following conditions hold : .The dialogue exceeds the maximum number of user turns , specified taking into account real dialogues for the task .The answer selected by the dialogue manager corresponds with a query not required by the user simulator .", "label": "", "metadata": {}, "score": "76.7949"}
{"text": "There is less agreement , however , on the optimal number of states for these HMMs , with some studies suggesting four states [ 48 , 49 ] and others suggesting thirty - two or more [ 13 , 14 ] .", "label": "", "metadata": {}, "score": "76.81525"}
{"text": "This hypothesis suggests that , once infants have discovered a set of word forms , they integrate information across them .Consider what would happen , for example , if an infant were familiar with the three words baby , diaper , and shoe , and integrated across these word forms .", "label": "", "metadata": {}, "score": "76.84725"}
{"text": "As ECs are more consistent - all words belonging to an EC belong to the same class - UA for EC is higher than for SC .On average , the unit \" word \" contains less information than the units SC and EC ; each unit consists of only one word whereas SC and EC mostly consist of more than one word .", "label": "", "metadata": {}, "score": "76.88728"}
{"text": "Psychology ISSN : 1664 - 1078 Publisher : Frontiers Media S.A. .Article Information Download PDF Copyright \u00a9 2013 Thiessen and Erickson .e - mail : thiessen@andrew.cmu.edu [ other ] This article was submitted to Frontiers in Language Sciences , a specialty of Frontiers in Psychology .", "label": "", "metadata": {}, "score": "76.940186"}
{"text": "In this experiment we only exposed infants to a trochaic bias .Previously , we have found that 7- and 9-month - old infants are able to learn an iambic bias that contradicts their native language .Therefore , it is likely that if 5-month - olds - who have less familiarity with the trochaic pattern of English than 7- or 9-month - olds - are able to learn a trochaic pattern , they would also be able to learn an iambic pattern .", "label": "", "metadata": {}, "score": "76.97305"}
{"text": "Output from the microphone was routed through an amplifier to a 24 bit recorder that sampled at 48 kHz ( Sound Devices 702 , Reedsburg , WI , USA ) .The microphone was calibrated with a B&K 4231 sound level calibrator .", "label": "", "metadata": {}, "score": "77.04812"}
{"text": "Stefan Steidl ( ICSI , USA ) .Anton Batliner ( FAU Erlangen - Nuremberg , Germany ) .Florian Schiel ( BASSS / LMU , Germany ) .Jarek Krajewski ( University of Wuppertal , Germany ) .The Challenge .", "label": "", "metadata": {}, "score": "77.256454"}
{"text": "To efficiently segment fluent speech , infants must discover the predominant phonological form of words in the native language .In English , for example , content words typically begin with a stressed syllable .To discover this regularity , infants need to identify a set of words .", "label": "", "metadata": {}, "score": "77.26029"}
{"text": "Given the representation of a dialogue as this sequence of pairs , the objective of the user intention recognizer at time i is to select an appropriate user answer U i .This selection is a local process for each time i , which takes into account the sequence of dialogue states that precede time i and the system answer at time i .", "label": "", "metadata": {}, "score": "77.37175"}
{"text": "The computation of such thresholds depends on the nature of the task for the dialogue system under study and how \" emotional \" the interactions can be .The intention recognizer .The methodology that we have developed for modelling the user intention extends our previous work in statistical models for dialogue management [ 46 ] .", "label": "", "metadata": {}, "score": "77.432205"}
{"text": "The checkerboard remained on screen , and the language continued to play , for 2 min .At the end of this time , the attention - getting movie reappeared on the screen .Once infants focused their gaze on the central monitor , the test phase began .", "label": "", "metadata": {}, "score": "77.53502"}
{"text": "These kinds of internal state should be estimated session - by - session .In this paper , we focus on the latter , transient states .These internal states are estimated based on the verbal and nonverbal information included in the interaction between the user and the system .", "label": "", "metadata": {}, "score": "77.57972"}
{"text": "For example , if infants extract the words BAby , DIAper , and SHOE , there is no consistent phonemic information .However , each of the words has a word - initial stress pattern .Integrating across these lexical forms would yield a representation that is not specific to any particular set of phonemes ( i.e. , is widely generalizable ) , but strongly indicates that lexical stress is associated with word - initial position .", "label": "", "metadata": {}, "score": "77.668365"}
{"text": "Each utterance is then represented as a sequence of N such vectors , .V .N . , ( where N is the number of voiced segments in the utterance ) by the front - end .The back - end .", "label": "", "metadata": {}, "score": "77.737"}
{"text": "Thus , \u03bd ( i ) is a 2 K + 1 dimensional vector , where K is the number of parameters chosen ( from P ) to represent each voiced segment , when slope and initial offset ( or midpoint ) are both used to represent a contour .", "label": "", "metadata": {}, "score": "77.75314"}
{"text": "After the tunnel was acoustically prepared for this experiment ( see below ) , velocity was calibrated using a Pitot tube attached to a pressure transducer .The wind tunnel was not designed for acoustic experiments , and had relatively high noise within the test section .", "label": "", "metadata": {}, "score": "77.79817"}
{"text": "Participation in the Intoxication Sub - Challenge .Participation in the Sleepiness Sub - Challenge .Novel features and algorithms for the analysis of speaker state .Cross - corpus and cross - task feature genericity analysis .Exploitation of speaker trait meta - information in speaker state analysis .", "label": "", "metadata": {}, "score": "77.90301"}
{"text": "The feather was backlit with an incandescent light .Wind tunnel experiments .Our goal was to study the onset of flutter as a function of feather size , shape and U air , at a constant orientation .We did not quantitatively explore the relationship between orientation ( \u03b1 and \u03b2 ) and other relevant parameters ( such as U air ) for several reasons .", "label": "", "metadata": {}, "score": "77.97125"}
{"text": "We hypothesize that these represent mode jumps within a mode type ( e.g. a jump from one type of tip mode to another ) , but this could not be unambiguously diagnosed from our data .Feather outlines are traced from photos .", "label": "", "metadata": {}, "score": "77.97652"}
{"text": "Background notes range between 587 and 740 Hz ( D5-F#5 ) .Each token is 1 s in duration and amplitude normalized relative to its top 10%th value with 0.5 s onset and 0.01 s offset sinusoidal ramps .Tokens overlap every 0.5 s , forming two sequences .", "label": "", "metadata": {}, "score": "78.0273"}
{"text": "Speech is the most basic medium of human - human communication and is expected to be one of the main modalities of more flexible man - machine interaction along with various intuitive interfaces rather than traditional text - based interfaces .One major topic of speech - based interfaces is the spoken dialog system .", "label": "", "metadata": {}, "score": "78.07932"}
{"text": "Thus , it is an empirical question to be addressed whether EC will be classified better than SC , if the process is fully automated .The best compromise between automation and performance seems to be WO .Here , we obtain better results than for SC - but still worse results than for EC .", "label": "", "metadata": {}, "score": "78.091736"}
{"text": "N .where N is the number of voiced segments in the utterance , and \u03bd ( i ) is a vector corresponding to the i th voiced segment , . i .s .F . i .b .F . i .", "label": "", "metadata": {}, "score": "78.13385"}
{"text": "It refers to a process or group of processes that act as selection mechanisms and allow the sensory and perceptual systems to form a processing bottleneck or focus cognitive resources on a subset of incoming stimuli deemed interesting .In the case of purely \" bottom - up \" attention , the selection process is driven by sensory cues that orient our attention to interesting events in the environment .", "label": "", "metadata": {}, "score": "78.143265"}
{"text": "In addition , the LDC corpus contains English speech while the Aibo corpus contains German speech .LDC emotional speech and transcripts corpus .The Emotional Prosody Speech and Transcripts corpus contain audio recordings , recorded at a sampling rate of 22,050 Hz , and the corresponding transcripts ( word level transcripts that lack time stamps ) .", "label": "", "metadata": {}, "score": "78.294334"}
{"text": "Then , some acoustic features may be the same for ' A ' neutral as for ' B ' angry , which would make the automatic classification fail for one of the users if the features are not normalized .The values for all features in the neutral style are stored in a user profile .", "label": "", "metadata": {}, "score": "78.29799"}
{"text": "Figure 1 shows the three phases in a session .Many conventional studies on user modeling have focused on the linguistic information of the user 's utterance and estimated the user 's internal states based on the dialog history ( i.e. , previously observed utterances made by the user and the system ) [ 8 - 10 ] .", "label": "", "metadata": {}, "score": "78.40428"}
{"text": "An example of the \" question - and - answer \" session is as follows : System : What is the date today ?User : Uhm ... , it 's May ...May 17th today .The questions were independent of each other and presented at random .", "label": "", "metadata": {}, "score": "78.49547"}
{"text": "( Of course , sometimes chunking together words belonging to different classes to the rest class , as we do , results in some \" smearing \" as well - but at least we do know where and up to what extent .", "label": "", "metadata": {}, "score": "78.571266"}
{"text": "View at Publisher \u00b7 View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. D. Lucas and T. Kanade , \" An iterative image registration technique with an application to stereo vision , \" in Proceedings of Imaging Understanding Workshop , pp .", "label": "", "metadata": {}, "score": "78.699036"}
{"text": "Q1: How well did the system understand you ? Q2: How well did you understand the system messages ? Q3: Was it easy to obtain the requested information ? Q4 : Was the interaction rate adequate ?Q5 : If the system made errors , was it easy for you to correct them ?", "label": "", "metadata": {}, "score": "78.71985"}
{"text": "The components of speech are vocal expression and linguistic content .Both components can be employed for signalling denotations and semantics , and for constituting illocutions ( such as dialogue acts ) , and for expressing connotations as paralinguistic messages ( such as emotions ) .", "label": "", "metadata": {}, "score": "78.76601"}
{"text": "Pitch , formant positions , and not yet fully developed co - articulation vary strongly , especially for younger children due to anatomical and physiological development [ 21 ] .Moreover , until the age of five / six , expression and emotion are strongly linked : children express their emotions even if no one else is present ; the expression of emotion can be rather intense .", "label": "", "metadata": {}, "score": "78.88547"}
{"text": "If we did not , we stopped the tunnel and bent the pin to a new value of \u03b2 so that rotating the sting would explore a new range of orientations .The feather was tested at up to five pin orientations at up to three relatively high values of U air .", "label": "", "metadata": {}, "score": "78.91097"}
{"text": "This gives an impression of the performance per class across all constellations .Basically , the picture is always the same : the mixed R(est ) class is recognised worst and almost evenly smeared across all classes .The highest recognition rates can be observed for the rather acoustically and linguistically marked N(egative ) instances - both for SC and EC ; P(ositive ) is in between .", "label": "", "metadata": {}, "score": "78.9948"}
{"text": "During the recording , the actors repeated each phrase as many times as necessary until they were satisfied that the emotion was expressed and then moved onto the next phrase .Only the last instance of each phrase was included in all the experiments reported herein .", "label": "", "metadata": {}, "score": "79.163414"}
{"text": "Sounds in everyday life seldom appear in isolation .We are constantly flooded with a cacophony of sounds that impinge on our ears at every instant .Our auditory system is tasked with sorting through this sensory flow , to attend to and identify sound objects of interest ; all while ignoring irrelevant distracters and ambient backgrounds - a phenomenon referred to as the \" cocktail party effect \" ( Cherry , 1953 ) .", "label": "", "metadata": {}, "score": "79.173935"}
{"text": "The reason is twofold : first , DMNB only requires lower memory and only a fraction of the computation time of SVM .( Sequential Minimal Optimisation training of SVM with linear Kernel demanded 200 times higher computation time than DMNB in parameterisation as below using [ 42 ] on an 8 GB RAM , 2.4 GHz , 64 Bit industry PC . )", "label": "", "metadata": {}, "score": "79.21798"}
{"text": "As in other objects that vibrate via structural resonance and have continuous energy input , such as a violin string , non - linearities are expected .For example , multiple resonance frequencies may be stimulated simultaneously , in which case harmonics may become mode - locked to the fundamental frequency , resulting in acoustic harmonics that are integer multiples of the fundamental ( Fletcher and Rossing , 1998 ) .", "label": "", "metadata": {}, "score": "79.45349"}
{"text": "Note that for purposes of comparison , Fig .5 depicts only the fundamental frequency of flutter , not the integer harmonics , despite the fact that the integer harmonics were usually prominent and sometimes contained more energy than the fundamental .", "label": "", "metadata": {}, "score": "79.470604"}
{"text": "Here , we define three internal states .In the first one ( State A ) , the user does not know how to answer the prompt .In the second one ( State B ) , the user is taking time to consider the answer .", "label": "", "metadata": {}, "score": "79.48984"}
{"text": "As can be observed , the system asks the user for different pieces of information before producing a response .A corpus of 100 dialogues was acquired with this system from student telephone calls .The callers were not recruited and the interaction with the system corresponded to the need of the users to obtain academic information .", "label": "", "metadata": {}, "score": "79.52229"}
{"text": "Sound is generated when a flowing fluid interacts with a solid structure such as a feather ( Blake , 1986 ) .The relative motion of the feather shears the flow , causing it to acquire angular momentum , termed vorticity .", "label": "", "metadata": {}, "score": "79.72435"}
{"text": "At a cocktail party , we can tune out surrounding sounds to listen to one specific conversation , but the shattering sound of a waiter dropping a tray of glasses will nonetheless cause us to pause to attend to the unexpected event .", "label": "", "metadata": {}, "score": "79.76156"}
{"text": "Bottom : segmentation based on trochaic bias .Estimating a User 's Internal State before the First Input Utterance .Graduate School of Engineering , Tohoku University , 6 - 6 - 5 Aramaki aza Aoba , Aoba - ku , Sendai , Miyagi 980 - 8579 , Japan .", "label": "", "metadata": {}, "score": "79.796005"}
{"text": "For as long as the infant maintained their gaze on the central monitor , the test trial continued , up to a maximum of 20 s. When the infant looked away for more than two consecutive seconds , the test trial ended and the attention - getting video reappeared on the central monitor .", "label": "", "metadata": {}, "score": "79.82926"}
{"text": "t .t . sin .t . kf . d\u03c4 .The pitch contours were estimated using the RAPT algorithm [ 47 ] , and the vocal tract and glottal spectra were estimated using the pitch synchronous IAIF algorithm [ 33 ] .", "label": "", "metadata": {}, "score": "79.84934"}
{"text": "In each case , the method chooses randomly ( 0.5 probabilities ) between an emotion ( doubtful , bored or angry ) and neutral .The probability of choosing the emotion rises to 0.7 when the same emotion was chosen in the previous turn , which allows simulating moderate changes of the emotional state .", "label": "", "metadata": {}, "score": "80.175385"}
{"text": "The data is downsampled to 16 kHz .Each recording session took some 30 minutes .The speech data were segmented automatically into speech files ( turns ) , triggering a turn boundary at pauses . 1 second .Note that here , the term \" turn \" does not imply any linguistic meaning ; however , it turned out that only in very few cases , this criterion wrongly decided in favour of a turn boundary instead of ( implicitly ) modelling a hesitation pause .", "label": "", "metadata": {}, "score": "80.49164"}
{"text": "Figure 9 : Example of sequential face orientation data when the user turns his face from frontal to lower right .Data Compression .The results of a preliminary examination showed that the simple descriptive statistics of face orientation ( e.g. , mean , variance , maximum , and minimum ) were not effective for discrimination .", "label": "", "metadata": {}, "score": "80.77511"}
{"text": "The Phenomena : Emotions or .The core phenomena consist of the big . being some figures between 4 and 8 or more ; this concept is mainly rooted in psychology , and has been challenged , elaborated , and extended amongst others in cognitive psychology , for instance in the OCC model [ 1 ] .", "label": "", "metadata": {}, "score": "80.79126"}
{"text": "Experiments .Healthy subjects with normal hearing participated in the experiments with informed consent , as approved by the institutional review board at the Johns Hopkins University , and were compensated for participation .Subjects were Johns Hopkins University students and scholars with an average age of 22.6 ( number of subjects were Exp .", "label": "", "metadata": {}, "score": "80.853165"}
{"text": "Collection and Analysis of Dialog Data .Dialog Tasks .We collected dialog data to analyze internal states of the users .There are two possibilities for collecting dialog data : collecting acted dialogs using actors and collecting natural dialogs using na\u00efve participants .", "label": "", "metadata": {}, "score": "80.909035"}
{"text": "10.1016/j.csl.2009.12.003 View Article .Copyright .\u00a9 Callejas et al ; licensee Springer .This article is published under license to BioMed Central Ltd.", "label": "", "metadata": {}, "score": "81.04237"}
{"text": "2D ) because of how we defined the coordinate system .Therefore , a feather 's geometry as a function of airflow was not well specified : at each U air \u00d7\u03b1\u00d7\u03b2 combination , every feather exhibited unique aeroelastic deformation that we lacked the means to quantify .", "label": "", "metadata": {}, "score": "81.16821"}
{"text": "Title : Frontiers in psychology Volume : 3 ISSN : 1664 - 1078 ISO Abbreviation : Front Psychol Publication Date : 2012 .Date Detail : .Created Date : 2013 - 01 - 21 Completed Date : 2013 - 01 - 22 Revised Date : 2013 - 08 - 13 .", "label": "", "metadata": {}, "score": "81.18155"}
{"text": "Many pauses of varying length are found , which can be hesitation pauses - the child produces slowly while observing the Aibo 's actions - or pauses segmenting into different dialogue acts - the child waits until he / she reacts to the Aibo 's actions .", "label": "", "metadata": {}, "score": "81.18651"}
{"text": "We investigated the underlying mechanics of flutter and sound production of a series of different feathers in a wind tunnel .All feathers tested were capable of fluttering at frequencies varying from 0.3 to 10 kHz .Loudness increased with airspeed in most but not all feathers .", "label": "", "metadata": {}, "score": "81.22446"}
{"text": "Pitch and timbre are extracted from the sound spectrogram , which is computed with 1 ms frames .The spectrogram computation mimics the processing known to occur from the cochlea to the mid - brain : Using a bank of 128 constant - Q bandpass log - scale filters , followed by high - pass , compression , and low - pass filtering then spectral sharpening following the model of Chi et al .", "label": "", "metadata": {}, "score": "81.3609"}
{"text": "However , changes in U air sometimes caused obvious mode jumps , in which the mode shape of flutter abruptly and dramatically shifted from one mode to another , such as from a tip mode to a trailing vane mode ( see Clark et al . , 2013 ) .", "label": "", "metadata": {}, "score": "81.43817"}
{"text": "R4 from white - bellied woodstar .The dependent variables investigated were : mode of flutter , flutter frequency , flutter velocity ( i.e. amplitude ) , sound frequency and loudness .We quantitatively investigated the effects of U air on all of these dependent variables , and qualitatively investigated the influence of \u03b1 , \u03b2 , shape , stiffness and aeroelastic deformation .", "label": "", "metadata": {}, "score": "81.5113"}
{"text": "A sememe consists of either a morpheme or a word indicating both semantic denotation and/or connotation either encoding a holistic meaning or being constituted by a feature bundle .We introduce the ememe as constituting \" pure \" connotation , indicated both by acoustic and linguistic means .", "label": "", "metadata": {}, "score": "81.53871"}
{"text": "The agent had always the neutral face at the beginning of the system prompt and changed its facial expression after the user 's utterance .The expression was decided by the operator ( the \" wizard \" ) according to appropriateness of the user 's response .", "label": "", "metadata": {}, "score": "81.55858"}
{"text": "One possible mechanism is a vortex whistle , in which the feedback mechanism is a consequence of intrinsically periodic fluid dynamic interactions with the geometry of the solid .Such aerodynamic mechanisms can be periodic when coherent vortices form and are shed by the object , generating a vortex street .", "label": "", "metadata": {}, "score": "81.82544"}
{"text": "Six of these trials were word trials , and six were part - word trials .Each test item occurred on three trials during the testing phase .Test trials were presented in random order .A test trial began with the attention - getting movie playing on the central monitor drawing the infants ' gaze forward .", "label": "", "metadata": {}, "score": "81.84472"}
{"text": "AUTHOR CONTRIBUTIONS .C.J.C. and R.O.P. initiated the study ; C.J.C. collected and analyzed wind tunnel data while D.O.E. provided SLDV ; R.O.P. provided equipment and support ; all authors contributed to writing the paper .Sonation in the male common snipe ( Capella gallinago gallinago L. ) is achieved by a flag - like fluttering of their tail feathers and consequent vortex shedding .", "label": "", "metadata": {}, "score": "81.94002"}
{"text": "Ememe Chunks : EC .The last unit of analysis investigated in this work consists of ememe chunks ( ECs ) .ECs are obtained from the ememe sequence by clustering together adjacent ememes belonging to the same main class .An EC is therefore an .", "label": "", "metadata": {}, "score": "82.05146"}
{"text": "In contrast with our conclusion , van Casteren and colleagues ( van Casteren et al . , 2010 ) attribute the sounds produced by common snipe ( Gallinago gallinago ) outer tail feathers to vortex shedding .However , inconsistencies and omissions undercut this conclusion .", "label": "", "metadata": {}, "score": "82.31088"}
{"text": "Words : WO .Based on the orthographic transcription ( transliteration ) of the speech data , a lexicon has been compiled consisting of 1146 words , of which 333 are word fragments .Beginning and end of each word were presegmented automatically using a forced alignment of the spoken word chain , and eventually manually corrected .", "label": "", "metadata": {}, "score": "82.387"}
{"text": "Authors ' original submitted files for images .Below are the links to the authors ' original submitted files for images .Competing interests .The authors declare that they have no competing interests .Authors ' Affiliations .The School of Electrical Engineering and Telecommunications , The University of New South Wales .", "label": "", "metadata": {}, "score": "82.60362"}
{"text": "As a result , the feather tended to express the same mode of flutter , and mode of flutter was relatively insensitive to orientation .We hypothesize that if aeroelastic bending and mode of flutter could be controlled , all of the feathers would exhibit a positive correlation between U air and frequency ( Fig .", "label": "", "metadata": {}, "score": "82.78716"}
{"text": "Multiple mechanisms appear to set the upper speed .Most commonly , at high U air the barbs on the feather started to become unzipped .This changes the shape of the feather , and either flutter ceased completely or loudness decreased .", "label": "", "metadata": {}, "score": "82.85237"}
{"text": ", 1996 ; Howe , 2008 ; Williamson and Gorvardhan , 2004 ) and it acts as a dipole radiator of sound .Because drag fluctuation has lower magnitude than lift ( Blake , 1986 ; Howe , 2008 ) , sound ensues primarily at f , with a weak 2nd harmonic at 2 f , and no higher harmonics .", "label": "", "metadata": {}, "score": "83.10951"}
{"text": "Two male hummingbirds with inner ( R1 ) to outer ( R5 ) rectrices labeled .Astrisk indicates feathers tested in this study .Photos courtesy of Anand Varma .Image of volcano hummingbird re - used ( from Clare et al .", "label": "", "metadata": {}, "score": "83.350555"}
{"text": "As a result of this , further increases in U air did not result in significant increases in amplitude .We suggest that this is the reason five feathers exhibited a negative correlation between U air and sound loudness ( Fig .", "label": "", "metadata": {}, "score": "83.423805"}
{"text": "Implicitly , it is normally taken for granted that these states to be modelled are produced and not only perceived .This difference can be illustrated by the following example : a father can get really angry with his son , and this can be heard in his tone of voice and seen in the outcome of physiological measurements .", "label": "", "metadata": {}, "score": "83.44151"}
{"text": "20 \u03bcPa ) at 1 m from the source were calculated assuming a uniform ( monopole ) pattern of sound radiation , and assuming that near - field effects were negligible .Both of these assumptions were violated , but this bias was likely constant across recordings for an individual feather ( see Discussion ) .", "label": "", "metadata": {}, "score": "83.44842"}
{"text": "Aeroelastic deformation , for our purposes , specifically refers to the significant static bending and twisting all feathers experienced in response to imposed aerodynamic forces , independent of any oscillatory behavior of flutter itself .This is distinct from a feather 's ' shape ' , which specifically refers to feather geometry in the absence of flow , with all barbs fully interlocked .", "label": "", "metadata": {}, "score": "83.755844"}
{"text": "The word test items were the infrequent words ( diti and bugo ) from the artificial language .Part - words were syllable conjunctions that occurred across the two more frequent words ( bida and pudo ) .During the infants ' exposure to the artificial language , both words and part - words occurred equally often .", "label": "", "metadata": {}, "score": "83.850555"}
{"text": "Thus , it is a foreground - background phenomenon : emotional has to be different from \" not emotional \" , that is , from neutral ( emotionally idle ) .Yet to start speaking and to finish speaking is a voluntary act , which normally can be detected and delimited by the listener .", "label": "", "metadata": {}, "score": "84.376366"}
{"text": "Finally , in our experiments , both the air and the feather were non - accelerating , whereas in a flying bird this will rarely be the case .Both acceleration reaction ( caused by accelerating fluid ) and a feather 's inertial loading ( caused by a feather 's acceleration ) could cause feathers to dynamically deform and adopt geometries not observed in static tests such as those done here .", "label": "", "metadata": {}, "score": "84.45166"}
{"text": "2A ) .The wind tunnel flow regime was non - accelerating freestream flow ( U air ) .Rotation about the Y -axis changed the angle of attack ( \u03b1 ) , while rotation about the Z -axis changed the sweep angle ( \u03b2 ) , both of which affected the aerodynamic forces upon the feather .", "label": "", "metadata": {}, "score": "84.5034"}
{"text": "Of these fifteen categories , five were chosen to conduct the five - class emotion classification experiments reported in this paper .These five emotional classes are neutral , anger , happiness , sadness and boredom .Four female and three male actors participated in the creation of this database and were provided with descriptions of each emotional context , including situational examples adapted from those used in the German study .", "label": "", "metadata": {}, "score": "84.58859"}
{"text": "Reduced frequency declined with airspeed for all feathers .The vortex whistle hypothesis does not make predictions that are supported by the available data , suggesting that vortex formation does not drive flutter .This conclusion does not imply a lack of vorticity or vortical structures in the air around a fluttering feather .", "label": "", "metadata": {}, "score": "84.76584"}
{"text": "IEEE :Piscataway ; 2007:611 - 614 .Copyright .\u00a9 Sethu et al . ; licensee Springer .This article is published under license to BioMed Central Ltd.PMID : 23335903 Owner : NLM Status : PubMed - not - MEDLINE .", "label": "", "metadata": {}, "score": "84.98036"}
{"text": "Distribution of Face Orientation .We analyzed the face orientation of the user in the segment before the user 's input as a visual feature .First , we investigated the tendency of the face orientation in the dialog data .We manually labeled the user 's face orientation as one of nine directions including frontal .", "label": "", "metadata": {}, "score": "85.106804"}
{"text": "Sci .doi : 10.1016/j.tics.2009.09.003 .Copyright \u00a9 2014 Kaya and Elhilali .This is an open - access article distributed under the terms of the Creative Commons Attribution License ( CC BY ) .The use , distribution or reproduction in other forums is permitted , provided the original author(s ) or licensor are credited and that the original publication in this journal is cited , in accordance with accepted academic practice .", "label": "", "metadata": {}, "score": "85.130104"}
{"text": "593 - 596 , 2012 .T. M. Cover and J. A. Thomas , Elements of Information Theory , Wiley - Interscience , Hoboken , NJ , USA , 2nd edition , 2006 .SUMMARY .Males in the ' bee ' hummingbird clade produce distinctive , species - specific sounds with fluttering tail feathers during courtship displays .", "label": "", "metadata": {}, "score": "85.145386"}
{"text": "Such vortex streets produce tones ( Blake , 1986 ; Fletcher , 1992 ; Howe , 2008 ; Nash et al . , 1999 ) .In particular , vortex formation and shedding at frequency f causes cyclical fluctuations in the fluid forces on the object , with lift fluctuating at frequency f and drag at frequency 2 f ( Blake , 1986 ; Howe , 2008 ) .", "label": "", "metadata": {}, "score": "85.232025"}
{"text": "We represent a dialogue as a sequence of pairs ( system - turn , user - turn ) .where A 1 is the greeting turn of the system ( the first dialogue turn ) , and U n is the last user turn .", "label": "", "metadata": {}, "score": "85.48271"}
{"text": "Geometric framework .A pennaceous feather has a central shaft , the rachis .Projecting out from either side of the rachis are barbs with differentiated distal and proximal barbules that interlock to form a flat , planar surface , the vane ( Fig . 2 ) .", "label": "", "metadata": {}, "score": "86.261314"}
{"text": "Academic Editor : Elisabeth Andre .Copyright \u00a9 2010 Anton Batliner et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "86.307625"}
{"text": "Procedure .An experiment outside the testing room observed the infant over closed - circuit video and recorded the duration of his or her gaze at the central monitor using the Habit X software ( Cohen et al ., 2004 ) .", "label": "", "metadata": {}, "score": "86.60649"}
{"text": "Color indicates the type of mode exhibited by the feather : blue , trailing vane ; red , tip ; yellow , torsional ; and green , white - bellied woodstar ( Chaetocercus mulsant ) R4 .Mode jumps that switch between types of modes ( e.g. from a tip mode to a trailing vane mode ) have been omitted ; see the companion paper ( Clark et al . , 2013 ) for a description of this phenomenon .", "label": "", "metadata": {}, "score": "86.85413"}
{"text": "Concretely , we have considered three negative emotions : anger , boredom and doubtfulness , where the latter refers to a situation in which the user is uncertain about what to do next ) .Following the proposed approach , our emotion recognizer employs acoustic information to distinguish anger from doubtfulness or boredom and dialogue information to discriminate between doubtfulness and boredom , which are more difficult to discriminate only by using phonetic cues .", "label": "", "metadata": {}, "score": "86.85565"}
{"text": "For example , copter is very likely to occur after heli ; but many words could potentially occur after helicopter .Conditional statistics - such as transitional probability ( e.g. , Saffran et al ., 1996 ) - reflect the likelihood of co - occurrence among elements of the input .", "label": "", "metadata": {}, "score": "86.994354"}
{"text": "The total number of user turns was 422 and the recorded material has duration of 150 min .In order to endow the system with the capability to adapt to the user mental state , we carried out two different annotations of the corpus : intention and emotional annotation .", "label": "", "metadata": {}, "score": "87.14636"}
{"text": "And you might be able to control your physiological signals , but only up to a certain extent - you can not stop your heart beats .( Interestingly , perception in these different modalities is different as well : you can not stop hearing even while sleeping - you only can use some ear protection decreasing the noise - but you can stop looking by simply closing your eyes , while awake or while sleeping . )", "label": "", "metadata": {}, "score": "87.253784"}
{"text": "( 1 ) where U air is air flow velocity and L is an appropriate length scale , such as the thickness of the boundary layer [ e.g. table 11.3 in Blake ( Blake , 1986 ) ] .Though the classic examples of vortex whistles are non - streamlined bodies such as a ' singing ' telephone wire , streamlined airfoils can also whistle by this mechanism ( Blake , 1986 ; Nash et al . , 1999 ) .", "label": "", "metadata": {}, "score": "87.389496"}
{"text": "All 60 utterances were then presented to the listener in random order and for each he / she was asked to pick one out of the five emotional categories that they could associate with the utterance .Their decisions were then analysed to determine for how many of the 30 possible pairs the listener selected the same emotion for both utterances .", "label": "", "metadata": {}, "score": "87.7379"}
{"text": "39 - 58 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Scopus .25 , no .4 , pp .193 - 222 , 1998 .View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "88.47662"}
{"text": "Two speakers situated next to the central LCD monitor were used to present the audio stimuli .At the beginning of the experiment , the infants ' attention was attracted to the central LCD monitor by the presentation of a colorful Winnie the Pooh video , accompanied by an attention - getting phrase .", "label": "", "metadata": {}, "score": "88.70786"}
{"text": "Introducing the turn - taking mechanism to a spoken dialog system is believed to make the dialog system to interact to the user in more natural and effective way .Notably , Edlund and Nordstrand [ 29 ] have researched on the multimodal dialog system and examined the turn - taking between the user and an agent ( an animated talking head ) that made a gesture such as head motion and gaze control .", "label": "", "metadata": {}, "score": "89.22202"}
{"text": "A .^ .t .K .t . )Z .t . )H .F .A .^ .t .^ .t . )I .K .t . )H . )F .", "label": "", "metadata": {}, "score": "89.53554"}
{"text": "5A ) .Harmonic content of the sound varies with how the feather is aerodynamically activated , much as the harmonics of a violin depend on how it is bowed ( Fletcher and Rossing , 1998 ) .Loudness as a function of feather velocity ( V feather , averaged across all sampled points on the feather ) , airspeed ( U air ) and frequency , for 31 hummingbird tail feathers .", "label": "", "metadata": {}, "score": "89.538925"}
{"text": "We centered the origin at the calamus ( Fig .2A ) .We did not quantify \u03b1 or \u03b2 , because variable aeroelastic deformation ( bending and twisting ) meant these variables were not defined down the length of the feather , making quantitative comparisons as a function of \u03b1 or \u03b2 difficult to interpret .", "label": "", "metadata": {}, "score": "89.935684"}
{"text": "To fully understand the behavior of our proposed approach under facial point localization errors , we depicted most confusions that occurred between the facial expressions in Figure 8 .Sadness expression is mostly confused with anger , and this confusion is dramatically increasing for higher uncertainties of facial point location .", "label": "", "metadata": {}, "score": "89.9901"}
{"text": "Number of dialogs for one subject was different from subject to subject .The total number of session was 199 ( 22.1 sessions per user in average , .Subjective Evaluation of the Internal States .Next , we evaluated the users ' internal states to make \" ground truths \" of the internal states .", "label": "", "metadata": {}, "score": "90.22504"}
{"text": "s .F . i .b .F .i . m .F . g .i . m .A . g .i . m .F .i . m .A .i . m .", "label": "", "metadata": {}, "score": "90.2793"}
{"text": "These predictions do not prescribe some sort of ' 1 motion-1 vortex ' model .We suggest that aerodynamic excitation of a feather causes it to flutter when aerodynamic energy received exceeds structural damping , and that periodicity is set in part by the structural resonance of the feather .", "label": "", "metadata": {}, "score": "90.37682"}
{"text": "We also consider two alternative hypotheses of the origin of flutter , and present data rejecting one of these mechanisms , a vortex whistle mechanism .As many , perhaps all , flight feathers can flutter when exposed to the right aerodynamic forces , we do not address which of the feathers tested here actually produces sound in wild birds .", "label": "", "metadata": {}, "score": "90.44026"}
{"text": "This , along with the presence of strong harmonics , multiple modes of flutter and several other non - linear effects indicates that flutter is not simply a vortex - induced vibration , and that the accompanying sounds are not vortex whistles .", "label": "", "metadata": {}, "score": "91.02727"}
{"text": "With the airflow turned on , the feather could be rotated around the Y -axis ( angle \u03b1 ) from the outside of the tunnel ( Fig .2A ) .In order to change \u03b2 , the pin connecting the feather to the pin vise was bent , with the airflow off ( Fig .", "label": "", "metadata": {}, "score": "91.33738"}
{"text": "The stimuli used in this experiment were identical to those used in Thiessen and Saffran 's ( 2003 ) Experiment 3 .Infants were exposed to an artificial language containing four bisyllabic nonsense words : diti , bugo , dapu , and dobi .", "label": "", "metadata": {}, "score": "91.70595"}
{"text": "FAU Aibo corpus .Similar to the experiments performed on the LDC corpus , those performed on the FAU Aibo corpus were constructed as a five - class emotion classification problem .The emotions involved , however , were different ( Anger , Emphatic , Neutral , Positive and Rest ) as outlined in Section 4.2 .", "label": "", "metadata": {}, "score": "91.89213"}
{"text": "Senior Researcher and Lecturer .Technische Universit\u00e4t M\u00fcnchen .Institute for Human - Machine Communication .D-80333 M\u00fcnchen .Germany .schuller@xxxxxx ._ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "92.03965"}
{"text": "The pitch contour varied as a function of whether the syllable began with a voiced or a voiceless consonant .For voiced consonants , the pitch contour traced an inverted parabola , peaking near the midpoint of the vowel .For voiceless consonants , the pitch contour began near the peak , and traced a falling plateau .", "label": "", "metadata": {}, "score": "92.635925"}
{"text": "The first experiment uses a background of non - melodic natural instrument sounds .Non - sustained single notes from the RWC Musical Instrument Sound Database ( Goto et al . , 2003 ) are extracted for Pianoforte ( Normal , Mezzo ) , Acoustic Guitar ( Al Aire , Mezzo ) , Clavichord ( Normal , Forte ) at 44.1 kHz .", "label": "", "metadata": {}, "score": "93.39131"}
{"text": "Human speech is an acoustic waveform generated by the vocal apparatus , whose parameters are modulated by the speaker to convey information .The physical characteristics and the mental state of the speaker also determine how these parameters are affected and , consequently , how speech conveys the intended , and on occasion unintended , information .", "label": "", "metadata": {}, "score": "93.48186"}
{"text": "The glottal source parameters .Glottal flow models .In the well - established and commonly used speech production model [ 12 ] , the glottal flow that serves as the input to the vocal tract is modelled as the response of a filter .", "label": "", "metadata": {}, "score": "93.71005"}
{"text": "F . i .b .F .i . m .F . g .i . m .A . g .i . m .F .i . m .A .i . m .F .", "label": "", "metadata": {}, "score": "93.85498"}
{"text": "Copyright \u00a9 2014 Anwar Saeed et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "94.24346"}
{"text": "For example , System : Are you looking for a restaurant of any region ?User : At region A. System : How much is your budget ?In the experiment , the operator required the subjects to achieve ambiguous task to search an \" affordable \" restaurant of Japanese food .", "label": "", "metadata": {}, "score": "94.43487"}
{"text": "At the feather 's base , the rachis inserts into the feather follicle via a straight , barbless region called the calamus .We mounted the feathers by the calamus so that they projected down into the working section of a wind tunnel ( below ) .", "label": "", "metadata": {}, "score": "94.47273"}
{"text": "F . i .b .F .i . m .F . g .i . m .A . g .i . m .F .i . m .A . i .i . i .", "label": "", "metadata": {}, "score": "94.54719"}
{"text": "Motion ( flutter ) is the result of fluid forcing alone , and the accompanying sound is referred to as a whistle ( Chanaud , 1970 ) .Alternatively , the feedback may lie in the exchange of fluid dynamic forces exerted on the solid , and the dynamic mechanical ( elastic / inertial ) forces of the solid exerted back on the fluid .", "label": "", "metadata": {}, "score": "94.84108"}
{"text": "F .i . m .F .i . m .A . g .i . m .F .i . m .A . i .i . i .s .F .i . m .", "label": "", "metadata": {}, "score": "95.272095"}
{"text": "A . g .i . m .F .i . m .F .i . m .F .i . m .A .i . m .A .i . m .A . i . i .", "label": "", "metadata": {}, "score": "95.50766"}
{"text": "Feather is R5 from Anna 's hummingbird .( B ) Experimental setup used to record sounds , vibrations and video of feathers in the working section of a wind tunnel .A sting projected into the working section from the top of the wind tunnel , and could be rotated about its longitudinal axis from outside the tunnel ( rounded arrow ) .", "label": "", "metadata": {}, "score": "96.274475"}
{"text": "The wizard causes the Aibo to perform a fixed , predetermined sequence of actions , which takes no account of what the child says .The children believed that the Aibo was reacting to their orders - albeit often not immediately .", "label": "", "metadata": {}, "score": "96.586975"}
{"text": "FAU Aibo emotion corpus .The German FAU Aibo Emotion Corpus [ 43 ] consists of spontaneous emotionally coloured children 's speech with recordings of 51 German children ( 30 females and 21 males ) aged between 10 and 13 from two different schools .", "label": "", "metadata": {}, "score": "96.69449"}
{"text": "K .t . )F .^ .t .F .T .H .T .H .F .^ .t .F .T .H .T .A .^ .t . )", "label": "", "metadata": {}, "score": "96.891685"}
{"text": "i . m .A . g .i . m .F .i . m .A . i .i . i .s .F . i .b .F .i . m .F . g .", "label": "", "metadata": {}, "score": "96.90024"}
{"text": "Five did not : rufous hummingbird ( Selasphorus rufus ) R2 , scintillant hummingbird ( Selasphorus scintilla ) R2 , broad - tailed ( Selasphorus platycercus ) R2 , purple - throated woodstar ( Philodice mitchellii ) R4 and magenta - throated woodstar ( Philodice bryantae ) R5 ( Fig .", "label": "", "metadata": {}, "score": "97.57814"}
{"text": "i . i .s .F . i .b .F .i . m .F . g .i . m .A . g .i . m .F .i . m .A .", "label": "", "metadata": {}, "score": "97.630905"}
{"text": "i . m .F .i . m .A . g .i .s .F . i .b .F . i . s .A . i .b .A . i .i . i .", "label": "", "metadata": {}, "score": "98.41556"}
{"text": "The values of the output layer can be viewed as the a posteriori probability of selecting the different user intention given the current situation of the dialogue .The UAH dialogue system .Universidad Al Habla ( UAH - University on the Line ) is a spoken dialogue system that provides spoken access to academic information about the Department of Languages and Computer Systems at the University of Granada , Spain [ 48 , 49 ] .", "label": "", "metadata": {}, "score": "98.55908"}
{"text": "The sting could be rotated from outside the wind tunnel , changing \u03b1 , whereas \u03b2 was changed by bending the pin to which the feather was glued .( D )Under this coordinate system , angles \u03b1 and \u03b2 did not shift when feathers bent or twisted in airflow ; these are changes in aeroelastic deformation .", "label": "", "metadata": {}, "score": "98.61758"}
{"text": "Percentage of different dialogues ( % diff ) .Number of repetitions of the most seen dialogue ( # repMS ) .Number of turns of the most seen dialogue ( # turnsMS ) .Number of turns of the shortest dialogue ( # turnsSh ) .", "label": "", "metadata": {}, "score": "99.16457"}
{"text": "The head of the pin was removed , and the portion of the pin projecting from the feather was inserted into a pin vise at the end of a sting that projected down from the top of the tunnel ( Fig .", "label": "", "metadata": {}, "score": "99.97997"}
{"text": "G .F . g .A . g .F .c .A . g .g . g .A . g .g . g .c .A . g . g .c .c . m .", "label": "", "metadata": {}, "score": "102.688675"}
{"text": "Copyright \u00a9 2012 Yuya Chiba and Akinori Ito .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "103.83898"}
{"text": "Copyright \u00a9 2012 Yuya Chiba and Akinori Ito .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "103.83898"}
{"text": "Thank you very much and all the best , .Stefan Steidl , Anton Batliner , Florian Schiel , Jarek Krajewski , and Bjoern Schuller ._ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "104.65614"}
{"text": "The children were given different tasks where they had to direct Sony 's dog - like robot Aibo to certain objects and along given ' parcours ' .They were told that they should talk to Aibo like a real dog and in particular reprimand it or praise it as appropriate .", "label": "", "metadata": {}, "score": "105.78612"}
{"text": "Nlm Unique ID : 101550902 Medline TA : Front Psychol Country : Switzerland .Other Details : .Languages : eng Pagination : 590 Citation Subset : - .Affiliation : .Department of Psychology , Carnegie Mellon University Pittsburgh , PA , USA .", "label": "", "metadata": {}, "score": "107.28119"}
{"text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "112.09358"}
{"text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "112.09358"}
