{"text": "We study the maximum weight matching problem in the semi - streaming model , and improve on the currently best one - pass algorithm due to Zelke ( STACS'2008 ) by devising a deterministic approach whose performance guarantee is 4.91 .In addition , we study preemptive online algorithms , a sub - class of one - pass algorithms where we are only allowed to maintain a feasible matching in memory at any point in time .", "label": "", "metadata": {}, "score": "34.291096"}
{"text": "Our techniques allow us to prove essentially the same bounds as if we knew the optimal learning rate in advance .Moreover , such techniques apply to a wide class of on - line algorithms , including p - norm algorithms for generalized linear regression and Weighted Majority for linear regression with absolute loss .", "label": "", "metadata": {}, "score": "38.099125"}
{"text": "Whether this is a good idea or not depends on the robustness with respect to deviations from the postulated model .We study this question experimentally in a res ... \" .Some learning techniques for classification tasks work indirectly , by first trying to fit a full probabilistic model to the observed data .", "label": "", "metadata": {}, "score": "42.49533"}
{"text": "We consider a single machine scheduling problem that seeks to minimize a generalized cost function : given a subset of jobs we must order them so as to minimize the sum of job - dependent cost functions .In a recent paper , Cheung and Shmoys provided a primal - dual algorithm for the problem and claimed that is a 2-approximation .", "label": "", "metadata": {}, "score": "42.725735"}
{"text": "We present a simple algorithm for playing a repeated game .We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy .Our bounds are nonasymptotic and hold for any opponent .", "label": "", "metadata": {}, "score": "43.36808"}
{"text": "We present a simple algorithm for playing a repeated game .We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy .Our bounds are nonasymptotic and hold for any opponent .", "label": "", "metadata": {}, "score": "43.36808"}
{"text": "Our method is based on rounding the linear programming relaxation of the corresponding covering problem .Besides the simplicity of the analysis , which mainly relies on decomposing the constraint matrix of the LP into totally balanced matrices .Unlike previous work , our algorithm generalizes to the weighted and partial versions of the basic problem .", "label": "", "metadata": {}, "score": "43.836308"}
{"text": "Most of the performance bounds for on - line algorithms in this framework assume a constant learning rate .To achieve these bounds the learning rate must be optimized based on a posteriori information .This information depends on the wh ... \" .", "label": "", "metadata": {}, "score": "44.247505"}
{"text": "To compare the paradigms we consider a model which postulates a single binary - valued hidden variable on which all other attributes depend .In this model , nding the most likely value of any one variable ( given known values for the others ) reduces to testing a linear function of the observed values .", "label": "", "metadata": {}, "score": "44.439354"}
{"text": "Just a few examples : An information theoretic lemma from Raz 's paper was used in many other contexts ( in cryptography , in equivalence of sampling and searching , etc ) .The \" correlated sampling \" technique that Holenstein used was applied in many other works ( in communication complexity , in PCP , etc ) .", "label": "", "metadata": {}, "score": "44.590424"}
{"text": "The proof of correctness is via essentially the same invariant as for Set Cover .Weighted Vertex Cover is a special case .Maximum Fractional Bipartite Matching .Such problems do not require non - uniform increments , so a simple algorithm analogous to the unweighted Set Cover algorithm ( but for packing ) will do : .", "label": "", "metadata": {}, "score": "45.506325"}
{"text": "At the heart of every Local Ratio algorithm is the update step in which a certain \" model \" function is subtracted from the current weight function in order to make some items tight .Subsequently , these tight items are chosen in the solution , a clean - up step is performed , and the procedure is repeated until feasibility is attained .", "label": "", "metadata": {}, "score": "45.53527"}
{"text": "How can you use this information to plan the cheapest route ?In this paper we design algorithms for this problem , as well as other problems in this framework .We study the partial capacitated vertex cover problem .The input consists of a graph and a covering requirement .", "label": "", "metadata": {}, "score": "45.58772"}
{"text": "We propose a framework that aims to identify true predictions among a large set of candidate predictions by selecting for each read a unique mapping that collectively imply conflict - free predictions .We formulate this problem as the maximum facility location problem , for which we propose LP - rounding heuristics .", "label": "", "metadata": {}, "score": "45.95999"}
{"text": "Large superscalar execution cores of future processors may take up so much area that a load from memory requires multiple cycles to propagate across the core , access the cache , and propagate the result back .Multiple L0 caches .In this section , we present a hardware unintensive variant of the Weighted Majority algorithm , and experimentally demonstrate the performance of our algorithm .", "label": "", "metadata": {}, "score": "46.038605"}
{"text": "For example : .How does Plotkin - Shmoys solve the linear programming relaxation of unweighted vertex cover ?Weighted vertex cover ?Set cover ?Bipartite matching ?What is the simplest example in which the Arora - Kale algorithm is doing something interesting ?", "label": "", "metadata": {}, "score": "46.15738"}
{"text": "Its goal is to predict almost as well as the best sequence of such experts chosen off - line by partit ... \" .In this paper , we examine on - line learning problems in which the target concept is allowed to change over time .", "label": "", "metadata": {}, "score": "46.357224"}
{"text": "Most of the performance bounds for on - line algorithms in this framework assume a constant learning rate .To achieve these bounds the learning rate must be optimized based on a posteriori information .This information depends on the whole sequence of examples and thus it is not available to any strictly on - line algorithm .", "label": "", "metadata": {}, "score": "46.409256"}
{"text": "This paper give efficient algorithms to find a popular matching , or in case none exists , to establish so .We study the partial vertex cover problem .We provide a primal - dual 2-approximation algorithm which runs in O(n log n + m ) time by avoiding the exhaustive guessing step of a previous algorithm .", "label": "", "metadata": {}, "score": "46.79159"}
{"text": "I ... .It might not be known as a \" pattern \" but basically it 's a way of ( low - frequency ) order management where a ' target portfolio ' is built using notional amounts which is then compared against an ' actual ... .", "label": "", "metadata": {}, "score": "47.49581"}
{"text": "If I constraint exposure to one of the factors to be a constant and set the exposure to other factors as zero .It is ok to use a diagonal factor ... .I am comparing the efficient frontier of a set of portfolios that are in and out of sample .", "label": "", "metadata": {}, "score": "47.60209"}
{"text": "It might not be known as a \" pattern \" but basically it 's a way of ( low - frequency ) order management where a ' target portfolio ' is built using notional amounts which is then compared against an ' actual ... .", "label": "", "metadata": {}, "score": "47.772964"}
{"text": "It might not be known as a \" pattern \" but basically it 's a way of ( low - frequency ) order management where a ' target portfolio ' is built using notional amounts which is then compared against an ' actual ... .", "label": "", "metadata": {}, "score": "47.772964"}
{"text": "Moreover , we develop an optimal O(n log n ) time algorithm to find this permutation .The enrichment of high - degree nodes in essential proteins , known as the centrality - lethality rule , suggests that the topological prominence of a protein in a protein interaction network may be a good predictor of its biological importance .", "label": "", "metadata": {}, "score": "48.062313"}
{"text": "State - of - the - art matching algorithms fail at coping with large real - world instances , which may involve millions of users and items .We propose the first distributed algorithm for computing near - optimal solutions to large - scale generalized matching problems like the one above .", "label": "", "metadata": {}, "score": "48.12262"}
{"text": "The goal is to compute the weights each stock should have in a portfolio .If I do something ... .I 'm working through the implementation of a risk budgeting approach as described in the recent Roncalli paper .The idea is that the portfolio manager sets a contribution of total portfolio volatility ... .", "label": "", "metadata": {}, "score": "48.240932"}
{"text": "We compare these , in a controlled fashion , against an algorithm ( a version of Winnow ) that attempts to nd a good l .. by Adam Grove , Dan Roth - In Neural Information Processing Systems , 1998 . \" ...", "label": "", "metadata": {}, "score": "48.63781"}
{"text": "Two cases in point : .The simplex algorithm .For both methods , it was \" well known \" that they worked well in practice , and for the first , it was known that it took worst - case exponential time .", "label": "", "metadata": {}, "score": "48.98389"}
{"text": "We aim for a universal solution that performs well without adaptation for any possible machine behavior .For the objective of minimizing the total weighted completion time , we design a polynomial time deterministic algorithm that finds a universal scheduling sequence with a solution value within 4 times the value of an optimal clairvoyant algorithm that knows the disruptions in advance .", "label": "", "metadata": {}, "score": "49.011806"}
{"text": "Consider the latter type of game ( with the goal of minimizing payoff ) , where the experts correspond to the elements $ e$ of your set system .Given the correct stopping condition ( discussed below ) , this process gives you exactly the Set - Cover algorithm discussed at the start .", "label": "", "metadata": {}, "score": "49.236572"}
{"text": "We show how to transform algorithms that assume that all experts are always awake to algorithms that do not require this assumption .We also show how to derive corresponding loss bounds .Our method is very general , and can be applied to a large family of online learning algorithms .", "label": "", "metadata": {}, "score": "49.3053"}
{"text": "However , I think this question can actually be a good one , if people give examples from their own experience where the proof of a widely - believed conjecture in their subarea led to new insights .It was painfully obvious to me and everyone else I talked to that the best you could do would be to apply Grover 's algorithm recursively , both to the OR and to the ANDs .", "label": "", "metadata": {}, "score": "49.630196"}
{"text": ".. she does not even assume the existence of such a mechanism . by Olivier Bousquet , Manfred K. Warmuth - JOURNAL OF MACHINE LEARNING RESEARCH , 2002 . \" ...In this paper , we examine on - line learning problems in which the target concept is allowed to change over time .", "label": "", "metadata": {}, "score": "49.65454"}
{"text": "A sequence is called \\alpha - robust when , for any possible constraint , the maximal or minimal prefix of the sequence that satisfies the constraint is at most a factor \\alpha from an optimal packing or covering .In this work we address the fact that many problem instances may allow for a much better robustness guarantee than the pathological worst case instances .", "label": "", "metadata": {}, "score": "49.673725"}
{"text": "Its goal is to predict almost as well as the best sequence of such experts chosen off - line by partitioning the training sequence into k + 1 sections and then choosing the best expert for each section .We build on methods developed by Herbster and Warmuth and consider an open problem posed by Freund where the experts in the best partition are from a small pool of size m. We propose algorithms that solve this open problem by mixing the past posteriors maintained by the master algorithm .", "label": "", "metadata": {}, "score": "49.915302"}
{"text": "In this paper we present a 2-approximation algorithm for it .Consider the problem of assigning applicants to jobs .Each applicant has a weight and provides a preference list ranking a subset of the jobs .An applicant x may prefer one matching over the other ( or be indifferent between them , in case of a tie ) based on the jobs x gets in the two matchings and x 's personal preference .", "label": "", "metadata": {}, "score": "50.035595"}
{"text": "One may say the algorithm makes educated guesses .The same trick can be used to get a 2-approximation for capacitated vertex cover , a more general version of the basic problem .Challenges in Selecting Paths for Navigational Queries M. Vidal , L. Raschid , and J. Mestre WebDB 2004 .", "label": "", "metadata": {}, "score": "50.04447"}
{"text": "Our contribution is a polynomial delay , polynomial space algorithm for enumerating all exact solutions plus further approxi- mate solutions , whose components are guaranteed to be within an absolute error of one of the optimum .Our experiments indicate that these approximate solutions are reasonably close to the optimal ones , in terms of the cumulative error .", "label": "", "metadata": {}, "score": "50.161034"}
{"text": "The algorithms you have in mind typically have variants that run in a nearly linear number of iterations , but each iteration typically requires at least linear time as well .I discuss some of these algorithms below .Some useful functions .", "label": "", "metadata": {}, "score": "50.28208"}
{"text": "Far from just confirming that the quantum query complexity of one annoying problem was what everyone expected it to be , these techniques turned out to represent one of the biggest advances in quantum computing theory since Shor 's and Grover 's algorithms .", "label": "", "metadata": {}, "score": "50.54992"}
{"text": "I am optimizing a stock portfolio with a few factors .If I constraint exposure to one of the factors to be a constant and set the exposure to other factors as zero .It is ok to use a diagonal factor ... .", "label": "", "metadata": {}, "score": "50.94609"}
{"text": "We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms , sometimes called \" experts . \" These simple algorithms belong to the multiplicative weights family of algorithms .The performance of these algorithms degrades only logarithmically with the number of experts , making them particularly useful in applications where the number of experts is very large .", "label": "", "metadata": {}, "score": "51.176285"}
{"text": "( A similar algorithm appears in the paper Chandra mentioned .Vertex Cover is of course a special case . )( Remark : Note that the iteration bound does not depend on the number of sets , just the number of elements .", "label": "", "metadata": {}, "score": "51.31246"}
{"text": "We show that the Weighted Majority algorithm applied to an ensemble of branch predictors yields a prediction scheme that results in a 5 - 11 % reduction in mispredictions .We also demonstrate that a variant of the Weighted Majority algorithm that is simplified for efficient hardware implementation still achieves misprediction rates that are within 1.2 % of the ideal case .", "label": "", "metadata": {}, "score": "51.38428"}
{"text": "Our analysis is almost tight .The objective of this paper is to characterize classes of problems for which a greedy algorithm finds solutions close to optimum .To that end , we introduce the notion of k - extendible systems , a natural generalization of matroids , and show that a greedy algorithm is a 1/k - factor approximation for these systems .", "label": "", "metadata": {}, "score": "51.59173"}
{"text": "This analysis yields a new , simple proof of the min - max theorem , as well as a provable method of approximately solving a game .A variant of our game - playing algorithm is proved to be optimal in a very strong sense . by Yoav Freund , Robert E. Schapire , Yoram Singer , Manfred K. Warmuth - In 29th", "label": "", "metadata": {}, "score": "51.61737"}
{"text": "We provide a lower bound of 4.967 on the competitive ratio of any such deterministic algorithm , and hence show that future improvements will have to store in memory a set of edges which is not necessarily a feasible matching .We study the interval constrained coloring problem , a combinatorial problem arising in the interpretation of data on protein structure emanating from experiments based on hydrogen / deuterium exchange and mass spectrometry .", "label": "", "metadata": {}, "score": "51.72003"}
{"text": "To my enormous frustration , though , I was unable to prove any lower bound better than the trivial \u03a9(n 1/4 ) . \"Going physicist \" and \" handwaving the answer \" never looked more appealing !:-D .To prove this result , Andris imagined feeding a quantum algorithm a superposition of different inputs ; he then studied how the entanglement between the inputs and the algorithm increased with each query the algorithm made .", "label": "", "metadata": {}, "score": "52.267605"}
{"text": "We generalize the recent relative loss bounds for on - line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded .The generalization allows the sequence to be partitioned into segments , and the goal is to bound the additional loss of the algorithm over the sum of the losses of the best experts for each segment .", "label": "", "metadata": {}, "score": "52.30132"}
{"text": "The Lov\u00e1sz Local Lemma is a nice example [ 1].I would therefore like to know .are there specific examples in theoretical computer science where a rigorous proof of a believed - to - be - true statement has led to new insight into the nature of the underlying problem ?", "label": "", "metadata": {}, "score": "52.31411"}
{"text": "The feature common to the a ..In particular , it has been shown that the Lempel - Ziv algorithm universally achieves quite a small regret when compared to the best finite state predictor [ 15].A more general analysis of universal p .. \" ...", "label": "", "metadata": {}, "score": "52.4483"}
{"text": "This problem has several natural applications , e.g. , in urban transportation and network security , and in a sense combines the multicut problem and the minimum membership set cover problem .We give approximation algorithms and hardness of approximation for different variants of the problem .", "label": "", "metadata": {}, "score": "52.46987"}
{"text": "Our results are proven without requiring complicated concentration - of - measure arguments and they hold for arbitrary on - lin ... \" .In this paper we show that on - line algorithms for classification and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk .", "label": "", "metadata": {}, "score": "52.47043"}
{"text": "The problem of predicting the outcome of a conditional branch instruction is a prerequisite for high performance in modern processors .It has been shown that combining different branch predictors can yield more accurate prediction schemes , but the existing research only examines selection - based approaches where one predictor is chosen without considering the actual predictions of the available predictors .", "label": "", "metadata": {}, "score": "52.55261"}
{"text": "In this paper , we study RSD from a parametrized complexity perspective .More specifically , we present efficient algorithms to compute the RSD probabilities under the condition that the number of agent types , alternatives , or objects is bounded .", "label": "", "metadata": {}, "score": "52.607224"}
{"text": "Back to the nature of your question .The key value of ' rigor ' here was in providing the hypothesis class one learns over ( weighted majorities over the original hypothesis class ) and efficient algorithms to find them .It is \" obvious \" that the best way of computing the PARITY of $ n$ bits with an unbounded fan - in circuit of depth $ d$ is the following recursive strategy .", "label": "", "metadata": {}, "score": "53.055862"}
{"text": "A key feature of our approach is that we can simultaneously compute heavy subgraphs for a range of cardinality constraints , thus making it particularly suited for browsing operations during visual analytics .We show how our techniques can be applied in a variety of application settings such as discovering functional modules ( most deviant subnetworks ) in protein - protein interaction graphs , identifying active cores of topical subgraphs of the Wikipedia graph , detection of communities in social networks , etc . .", "label": "", "metadata": {}, "score": "53.276398"}
{"text": "But let 's say I assumed the returns of prices follow the ... .We just learned about cash - matching through dedicated portfolios ( using risk free bonds ) in my class that concerned mathematical programming .However , in an aside one of the notes said : It should be ... .", "label": "", "metadata": {}, "score": "53.39793"}
{"text": "Recently , Konemann et al ( ESA 2006 ) showed how to use an \u03b1 - LMP as a \" black - box \" to get an ( \u03b1 4/3)-approximation for any covering problem .First we show that the result of Konemman et al is best possible for covering problems in general .", "label": "", "metadata": {}, "score": "53.5542"}
{"text": "We present an algorithm that constructs for each instance a solution with a robustness factor arbitrarily close to optimal .Distributed social networking services show promise to solve data ownership and privacy problems associated with centralized approaches .Smartphones could be used for hosting and sharing users data in a distributed manner , if the associated high communication costs and battery usage issues could be mitigated .", "label": "", "metadata": {}, "score": "53.560036"}
{"text": "Abstract .We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms , sometimes called \" experts . \" These simple algorithms belong to the multiplicative weights family of algorithms .The performance of these algorithms degrades only loga ... \" .", "label": "", "metadata": {}, "score": "53.59861"}
{"text": "The problem of predicting the outcome of a conditional branch instruction is a prerequisite for high performance in modern processors .It has been shown that combining different branch predictors can yield more accurate prediction schemes , but the existing research only examines selection- ... \" .", "label": "", "metadata": {}, "score": "53.601837"}
{"text": "Journal of Computer and System Sciences , 55(1):119 - 139 , 1997 .The weak / strong learning question started out as a primarily theoretical concern , but this sequence of ' reproofs ' resulted in a beautiful algorithm , one of the most influential results in machine learning .", "label": "", "metadata": {}, "score": "53.65405"}
{"text": "Their historical returns and some variables like RSI , ATR , EMAs for all 3 of them .The goal is to compute the weights each stock should have in a portfolio .If I do something ... .How does one calculate the return on a portfolio if the assets in that portfolio were held for varying periods of time ?", "label": "", "metadata": {}, "score": "53.797424"}
{"text": "To present the algorithm , first rewrite the problem as a general covering problem .Here is the algorithm : .The algorithm returns a $ ( 1+O(\\varepsilon))$-approximate solution in $ O(n\\log(n)/\\varepsilon^2)$ iterations , where $ n$ is the number of covering constraints .", "label": "", "metadata": {}, "score": "54.06968"}
{"text": "Can anyone please help on how ... .Does anyone know of a python library / source that is able to calculate the traditional mean - variance portfolio ?To press my luck , any resources where the library / source also contains functions such as ... .", "label": "", "metadata": {}, "score": "54.404305"}
{"text": "Can anyone please help on how ... .Does anyone know of a python library / source that is able to calculate the traditional mean - variance portfolio ?To press my luck , any resources where the library / source also contains functions such as ... .", "label": "", "metadata": {}, "score": "54.404305"}
{"text": "Suppose I have 3 stocks .Their historical returns and some variables like RSI , ATR , EMAs for all 3 of them .The goal is to compute the weights each stock should have in a portfolio .If I do something ... .", "label": "", "metadata": {}, "score": "54.584473"}
{"text": "Suppose I have 3 stocks .Their historical returns and some variables like RSI , ATR , EMAs for all 3 of them .The goal is to compute the weights each stock should have in a portfolio .If I do something ... .", "label": "", "metadata": {}, "score": "54.584473"}
{"text": "The additional loss becomes O(k log n + k log(L / k ) ) , where L is the loss of the best partition with k +1segments .Our algorithms for tracking the predictions of the best expert are simple adaptations of Vovk 's original algorithm for the single best expert case .", "label": "", "metadata": {}, "score": "54.7079"}
{"text": "Given a vertex coloring of the graph , the weight of a color class is the maximum weight any node in the class .The problem is to compute a legal coloring of the graph such that sum of the weights of the color classes is minimized .", "label": "", "metadata": {}, "score": "54.77604"}
{"text": "I am trying to determine a step - by - step algorithm for calculating a portfolio 's VaR using monte carlo simulations .It seems to me that the literature for this is extraordinarily opaque for something ... .You are considering an investment in the stock .", "label": "", "metadata": {}, "score": "54.930588"}
{"text": "I am trying to determine a step - by - step algorithm for calculating a portfolio 's VaR using monte carlo simulations .It seems to me that the literature for this is extraordinarily opaque for something ... .You are considering an investment in the stock .", "label": "", "metadata": {}, "score": "54.930588"}
{"text": "In the single segment case , the additional loss is proportional to log n , where n is the number of experts and the constant of proportionality depends on the loss function .Our algorithms do not produce the best partition ; however the loss bound shows that our predictions are close to those of the best partition .", "label": "", "metadata": {}, "score": "55.028526"}
{"text": "This kind of oracle is the same as the separation oracle required to apply the ellipsoid algorithm to the dual problem .For packing problems such as set packing , you need an oracle that , given weights on the elements , returns a set minimizing the total weight .", "label": "", "metadata": {}, "score": "55.15502"}
{"text": "\" ...The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .We first give a formal framework for the problem .", "label": "", "metadata": {}, "score": "55.202484"}
{"text": "We generalize the recent relative loss bounds for on - line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded .The generalization allows the sequence to be partitioned into segments , and the goal is to bound th ... \" .", "label": "", "metadata": {}, "score": "55.396538"}
{"text": "Anyway , things became very interesting after Schapire 's paper .His solution produced a majority - of - majority over hypotheses in the original class .Then came : .Yoav Freund .Boosting a weak learning algorithm by majority .", "label": "", "metadata": {}, "score": "55.4388"}
{"text": "Each constraint is made up of a closed interval ( protein segment ) and requirements on the number of elements that belong to each color class ( exchange rates observed in the experiments ) .We prove that testing feasibility is NP - hard and provide approximation algorithms that produce coloring that just slightly violate the constraints .", "label": "", "metadata": {}, "score": "55.56588"}
{"text": "However , some of the algorithmic techniques necessary for efficiency ( such as non - uniform increments and dropping satisfied covering constraints ) do n't seem to carry over into the learning theory setting naturally .Likewise , algorithms for mixed packing and covering LPs ( e.g. these ) do n't seem to have natural analogues in the learning - theory setting .", "label": "", "metadata": {}, "score": "56.275723"}
{"text": "I use the geometric brownian motion to model the prices .But let 's say I assumed the returns of prices follow the ... .We just learned about cash - matching through dedicated portfolios ( using risk free bonds ) in my class that concerned mathematical programming .", "label": "", "metadata": {}, "score": "56.329445"}
{"text": "I do n't comment here on SDP algorithms .Note that the particular algorithms that you mention do not run in nearly linear time .( There is a nearly linear - time algorithm for explicitly given packing or covering problems .", "label": "", "metadata": {}, "score": "57.324753"}
{"text": "Here consider the problem of max - coloring paths and its generalization , max - coloring a broad class of trees .Tight upper and lower bounds on the time complexity of this problem .Consider the problem of matching applicants to jobs under one - sided preferences .", "label": "", "metadata": {}, "score": "57.40935"}
{"text": "Now suppose we want a smaller error probability .The history .The legend says it was given to a student to prove , before it was realized that the \" obvious \" statement is simply false .The proof used information theoretic ideas , and was said to be inspired by an idea of Razborov in communication complexity .", "label": "", "metadata": {}, "score": "57.874763"}
{"text": "Our main result is an algorithm that finds a matching - probing strategy having a small constant approximation ratio .An interesting aspect of our approach is that we compare the cost our solution to the optimal edge - probing strategy .", "label": "", "metadata": {}, "score": "58.088215"}
{"text": "I need to construct an equally weighted portfolio that goes long in the 3 highest returns and short in the 3 lowest returns .The portfolio needs to be re - balanced ... .If we want to calculate VAR of a portfolio using variance covariance matrix ( delta normal method ) , containing equities , forwards and options , how do we treat each asset class for making the variance ... .", "label": "", "metadata": {}, "score": "58.14173"}
{"text": "I need to construct an equally weighted portfolio that goes long in the 3 highest returns and short in the 3 lowest returns .The portfolio needs to be re - balanced ... .I have a Monte Carlo model which measures the Value at Risk ( VaR ) for given portfolio .", "label": "", "metadata": {}, "score": "58.185066"}
{"text": "The objective is to find a minimum cost assignment of edges to vertices such that the total demand of assigned edges fulfills the prescribed requirement .We present a unified framework for approximating different variants of partial capacitated vertex cover .Our approach is based on the Local Ratio technique and sophisticated charging schemes .", "label": "", "metadata": {}, "score": "58.197758"}
{"text": "Furthermore , when applied to concrete on - line algorithms , our results yield tail bounds that in many cases are comparable or better than the best known bounds . ... sophisticated statistical tools required by risk analyses based on uniform convergence .", "label": "", "metadata": {}, "score": "58.207977"}
{"text": "Each applicant has a weight and provides a preference list ranking a subset of the jobs .An applicant x may prefer one matching over the other ( or be indifferent between them , in case of a tie ) based on the jobs x gets in the two matchings and x 's personal preference .", "label": "", "metadata": {}, "score": "58.21257"}
{"text": "In the context of TCS , these results breath a lot of life in the context of ( 1 ) multiplicative weight algorithms and ( 2 ) hard - core set results .About ( 1 ) , I 'd just like to clarify that AdaBoost can be seen as an instance of the multiplicative weights / winnow work of Warmuth / Littlestone ( Freund was a Warmuth student ) , but there is a lot of new insight in the boosting results .", "label": "", "metadata": {}, "score": "58.57228"}
{"text": "The first step in the analysis of data produced by ultra - high - throughput next - generation sequencing technology is to map short sequence ' reads ' to a reference genome , if available .Sequencing errors , repeat regions , and polymorphisms may lead a read to align to multiple locations in the genome reasonably well .", "label": "", "metadata": {}, "score": "58.648186"}
{"text": ".. roblem domain .It simply keeps one weight per expert , representing the belief in the expert 's prediction , and then decreases the weight as a function of the loss of the expert . by Nicolo Cesa - Bianchi , Alex Conconi , Claudio Gentile - IEEE Transactions on Information Theory , 2001 . \" ...", "label": "", "metadata": {}, "score": "58.77039"}
{"text": "Share .OpenURL .Abstract .We consider the problem of dynamically apportioning resources among a set of options in a worst - case on - line framework .The model we study can be interpreted as a broad , abstract extension of the well - studied on - line prediction model to a general decision - theoretic setting .", "label": "", "metadata": {}, "score": "58.800034"}
{"text": "Instead of paying log n for choosing the best expert in each section we first pay log bits in the bounds for identifying the pool of m experts and then log m bits per new section .In the bounds we also pay twice for encoding the boundaries of the sections .", "label": "", "metadata": {}, "score": "58.878525"}
{"text": "A ( navigational ) query may be answered by traversing multiple alternate paths between a start source and a target source .These paths typically have different benefits and evaluation costs .In prior research , we developed ESearch , an algorithm based on a Deterministic Finite Automaton , which exhaustively enumerates all paths to answer a navigational query .", "label": "", "metadata": {}, "score": "58.888702"}
{"text": "To understand PST - style algorithms for packing problems it is good to look at algorithms for approximately solving the multicommodity flow problem which is where PST evolved from .Neal Young 's paper describes set cover in detail .I thought Arora - Kale - Hazan 's survey also makes the connection between the experts framework and packing / covering solvers explicit .", "label": "", "metadata": {}, "score": "59.105255"}
{"text": "To do this , he proved the Switching Lemma which has been a very valuable tool in proving lower bounds for circuits , parallel algorithms , and proof systems .This is one of the few instances where we know that a particular natural algorithm is optimal , and it led to a very detailed understanding of the power of $ AC^0$. There are other barriers such as Relativization and Algebrization .", "label": "", "metadata": {}, "score": "59.107815"}
{"text": "For such games , welfare maximizing partitions constitute desirable ways to cluster the vertices of the graph .We present both intractability results and approximation algorithms for computing welfare maximizing partitions .We study the properties of the intersection of independence systems .", "label": "", "metadata": {}, "score": "59.197548"}
{"text": "I am comparing the efficient frontier of a set of portfolios that are in and out of sample .The first period is from 1991 - 01 - 03 until 1992 - 10 - 03 and the second one from 1992 - 10 - 03 until 1994 - 03 - 03 .", "label": "", "metadata": {}, "score": "59.214348"}
{"text": "I am comparing the efficient frontier of a set of portfolios that are in and out of sample .The first period is from 1991 - 01 - 03 until 1992 - 10 - 03 and the second one from 1992 - 10 - 03 until 1994 - 03 - 03 .", "label": "", "metadata": {}, "score": "59.214348"}
{"text": "I would like to calculate the Conditional Value at Risk for a portfolio .To be honest , I 'm trying for a few days to find an example to calculate for an entire portfolio , not just for one security and ...", "label": "", "metadata": {}, "score": "59.679466"}
{"text": "I would like to calculate the Conditional Value at Risk for a portfolio .To be honest , I 'm trying for a few days to find an example to calculate for an entire portfolio , not just for one security and ...", "label": "", "metadata": {}, "score": "59.679466"}
{"text": "Any way to tell ?I have ... .I am trying to work out how to determine weights for the assets in order to form a portfolio .The ratio I am using is EV / EBIT , hence the smaller the better .", "label": "", "metadata": {}, "score": "59.807793"}
{"text": "Any way to tell ?I have ... .I am trying to work out how to determine weights for the assets in order to form a portfolio .The ratio I am using is EV / EBIT , hence the smaller the better .", "label": "", "metadata": {}, "score": "59.807793"}
{"text": "We present a quasi - polynomial time $ ( 1+\\epsilon)$-approximation algorithm .We also show how this implies a $ ( e + \\epsilon)$-approximation for the problem of scheduling jobs on a single machine where each job has its own cost function and the objective to minimize the sum of the completion costs .", "label": "", "metadata": {}, "score": "59.828995"}
{"text": "It is modeled by a transfer graph -- vertices represent the storage devices , and edges represent data transfers .Every transfer takes one unit of time .A vertex can handle one transfer at a time , and it is said to complete when all the edges ( transfers ) incident on it complete .", "label": "", "metadata": {}, "score": "59.995438"}
{"text": "Let $ X_s$ be the number of times the adversary chose set $ s$ during the play .The adversary chose $ s^t$ to minimize this payoff .$ $ That is , normalizing $ X$ gives a fractional set cover of size at most $ ( 1+O(\\varepsilon))$ times optimum .", "label": "", "metadata": {}, "score": "60.068806"}
{"text": "We design and optimize the critical circuits in a superscalar execution core .At comparable clock speeds , an instruction window implemented with our circuits can simultaneously wakeup and schedule 128 instruc - tions , compared to only twenty instructions in the Alpha 21264 .", "label": "", "metadata": {}, "score": "60.188698"}
{"text": "Can anyone please help on how ... .I have a Monte Carlo model which measures the Value at Risk ( VaR ) for given portfolio .I use the geometric brownian motion to model the prices .But let 's say I assumed the returns of prices follow the ... .", "label": "", "metadata": {}, "score": "60.433456"}
{"text": "Can anyone please help on how ... .I have a Monte Carlo model which measures the Value at Risk ( VaR ) for given portfolio .I use the geometric brownian motion to model the prices .But let 's say I assumed the returns of prices follow the ... .", "label": "", "metadata": {}, "score": "60.433456"}
{"text": "MobiTribe groups devices into tribes among intended content consumers , which replicate content and serve it using low - cost network connections by exploiting time elasticity of user generated content dissemination .We develop a grouping algorithm based on a combination of a bipartite b - matching and a greedy heuristic algorithm which groups mobile devices into tribes in polynomial time .", "label": "", "metadata": {}, "score": "60.448563"}
{"text": "We just learned about cash - matching through dedicated portfolios ( using risk free bonds ) in my class that concerned mathematical programming .However , in an aside one of the notes said : It should be ... .recently I am doing cross sectional regressions , and getting confused about missing returns .", "label": "", "metadata": {}, "score": "60.860424"}
{"text": "We just learned about cash - matching through dedicated portfolios ( using risk free bonds ) in my class that concerned mathematical programming .However , in an aside one of the notes said : It should be ... .recently I am doing cross sectional regressions , and getting confused about missing returns .", "label": "", "metadata": {}, "score": "60.860424"}
{"text": "The first period is from 1991 - 01 - 03 until 1992 - 10 - 03 and the second one from 1992 - 10 - 03 until 1994 - 03 - 03 .I ... .It might not be known as a \" pattern \" but basically it 's a way of ( low - frequency ) order management where a ' target portfolio ' is built using notional amounts which is then compared against an ' actual ... .", "label": "", "metadata": {}, "score": "61.391914"}
{"text": "Insights for the problem and more consequences .This is important because most hardness of approximation results are based on projection games , and unique games are a special case of projection games .There are also quantitative improvements in games on expanders ( by Ricky Rosen and Ran Raz ) , and more .", "label": "", "metadata": {}, "score": "61.52611"}
{"text": "The second experiment is a collaborative - filtering task for making movie recommendations .Here , we present results comparing RankBoost to nearest - neighbor and regression algorithms . by Mark Herbster , Manfred , K. Warmuth , Gerhard Widmer , Miroslav Kubat - In Proceedings of the 12th International Conference on Machine Learning , 1995 . \" ...", "label": "", "metadata": {}, "score": "61.53965"}
{"text": "As an application of our results , we show an improved approximation algorithm for stochastic probing with deadlines over k - systems .Voting and assignment are two of the most fundamental settings in social choice theory .For both settings , random serial dictatorship ( RSD ) is a well - known rule that satisfies anonymity , ex post efficiency , and strategyproofness .", "label": "", "metadata": {}, "score": "61.58992"}
{"text": "I am looking for more details to perform simple and log returns for an entire portfolio .However , I 've only been able to find the following semi - reliable source ( see Page 9 and Page 19 ) : Here are my ... .", "label": "", "metadata": {}, "score": "62.19136"}
{"text": "I am looking for more details to perform simple and log returns for an entire portfolio .However , I 've only been able to find the following semi - reliable source ( see Page 9 and Page 19 ) : Here are my ... .", "label": "", "metadata": {}, "score": "62.19136"}
{"text": "I am looking for more details to perform simple and log returns for an entire portfolio .However , I 've only been able to find the following semi - reliable source ( see Page 9 and Page 19 ) : Here are my ... .", "label": "", "metadata": {}, "score": "62.19136"}
{"text": "I am looking for more details to perform simple and log returns for an entire portfolio .However , I 've only been able to find the following semi - reliable source ( see Page 9 and Page 19 ) : Here are my ... .", "label": "", "metadata": {}, "score": "62.19136"}
{"text": "Minimizing total payoff .You can derive a similar strategy for the setting where the goal is to minimize , rather than maximize , the total payoff .Your regret , which you still want to minimize , is $ \\sum_t p^t\\cdot a^t - \\min_i a^t_i$. Connection to Lagrangian - relaxation algorithms .", "label": "", "metadata": {}, "score": "62.36161"}
{"text": "The main drawback of the popular matching concept is that it is not guaranteed to exist .In this work we address this issue by considering mixed matchings , that is , probability distribution over matchings .We show that popular mixed matchings always exist and design polynomial time algorithms for finding one .", "label": "", "metadata": {}, "score": "62.46943"}
{"text": "Arguably , one of the most important techniques in the design of combinatorial algorithms is the primal - dual schema in which the cost of the primal solution is compared to the cost of a dual solution .In this dissertation we study the primal - dual schema in the design of approximation algorithms for a number of covering and scheduling problems .", "label": "", "metadata": {}, "score": "62.595848"}
{"text": "The verifier performs some check on $ a_1 $ and $ a_2 $ ( depending on $ q_1,q_2 $ ) and decides whether to accept or reject .If $ x\\in L$ , there exists a provers strategy that the verifier always accepts .", "label": "", "metadata": {}, "score": "63.227608"}
{"text": "Here 's a sketch of the proof of the performance guarantee .( To see why , recall that the gradient of Lmin$(Ax)$ with respect to $ x$ is $ ( g(Ax))^T A$ .Then , the step size $ \\varepsilon$ is chosen just small enough so that no coordinate of $ A x$ increases by more than $ \\varepsilon$. FWIW , the inequalities involved in proving the invariant are essentially the same as those involved in proving the Chernoff bound .", "label": "", "metadata": {}, "score": "63.558502"}
{"text": "Most of the discussion there was about cases showing the importance of proof , which people on CSTheory probably do not need to be convinced about .In my experience proofs need to be more rigorous in theoretical computer science than in many parts of continuous mathematics , because our intuition so often turns out to be wrong for discrete structures , and because the drive to create implementations encourages more detailed arguments .", "label": "", "metadata": {}, "score": "63.643875"}
{"text": "I 'd like to compare the returns of a portfolio segregated by groups to the returns of those groups in total .So say for example I have a portfolio with 40 % Industrials and 60 % Technology , then over ...", "label": "", "metadata": {}, "score": "63.704655"}
{"text": "We study this question experimentally in a restricted , yet non - trivial and interesting case : we consider a conditionally independent attribute ( CIA ) model which postulates a single binary - valued hidden variable z on which all other attributes ( i.e. , the target and the observables ) depend .", "label": "", "metadata": {}, "score": "63.851143"}
{"text": "If we want to calculate VAR of a portfolio using variance covariance matrix ( delta normal method ) , containing equities , forwards and options , how do we treat each asset class for making the variance ... .Suppose I have 3 stocks .", "label": "", "metadata": {}, "score": "64.30061"}
{"text": "Edit : The kind of answer I had in mind is like those by Scott and matus .Moreover , I 'm trying to avoid arguments about whether something qualifies or not ; unfortunately the nature of the question may be intrinsically problematic .", "label": "", "metadata": {}, "score": "64.4768"}
{"text": "For Example : $ t_0 $ Buy AAPL at 100 $ t_5 $ Buy MSFT at 20 $ t_1$$_0 $ Sell MSFT at ... .I am optimizing a stock portfolio with a few factors .If I constraint exposure to one of the factors to be a constant and set the exposure to other factors as zero .", "label": "", "metadata": {}, "score": "64.61687"}
{"text": "For Example : $ t_0 $ Buy AAPL at 100 $ t_5 $ Buy MSFT at 20 $ t_1$$_0 $ Sell MSFT at ... .I am optimizing a stock portfolio with a few factors .If I constraint exposure to one of the factors to be a constant and set the exposure to other factors as zero .", "label": "", "metadata": {}, "score": "64.61687"}
{"text": "I think the following example spawned a lot of research which had results of the kind you are looking for , at least if I follow the spirit of your LLL example .Robert E. Schapire .The strength of weak learnability .", "label": "", "metadata": {}, "score": "64.645096"}
{"text": "The exponential penalties in the pessimistic estimator come from the using the Chernoff bound in the analysis of the rounding scheme .This basic idea is explained further in the paper Chandra mentioned . )Fractional Weighted Set Cover ( and general fractional Covering ) .", "label": "", "metadata": {}, "score": "64.88415"}
{"text": "If I have 3 different currency trades ( ex short EURSEK , short NZDUSD , long USDJPY ) , how do I size each trade if I wish to allocate risk equally in order to target a 12 % portfolio volatility ( allowing ... .", "label": "", "metadata": {}, "score": "65.22154"}
{"text": "If I have 3 different currency trades ( ex short EURSEK , short NZDUSD , long USDJPY ) , how do I size each trade if I wish to allocate risk equally in order to target a 12 % portfolio volatility ( allowing ... .", "label": "", "metadata": {}, "score": "65.22154"}
{"text": "We learn CIA with two techniques : the standard EM algorithm , and a new algorithm we develop based on covariances .We compare these , in a controlled fashion , against an algorithm ( a version of Winnow ) that attempts to find a good linear classifier directly .", "label": "", "metadata": {}, "score": "65.714096"}
{"text": "We introduce a novel approach for predicting physical contacts in protein purifications and compare its performance to four established methods for scoring co - complexed protein pairs based on several high - confidence experimental reference sets .In this paper , we address the problem of finding such cardinality constrained subgraphs from large node - weighted graphs that maximize the sum of weights of selected nodes .", "label": "", "metadata": {}, "score": "65.95986"}
{"text": "This dis - sertation addresses the problems of implementing wide issue , out - of - order execution , superscalar processors capable of handling hundreds of in - flight instructions .The specific issues covered by this dissertation are the critical circuits that comprise the superscalar core , the increasing level - one data cache latency , the need for more accurate branch prediction to keep such a large processor busy , and the difficulty in quickly evaluating such complex processor designs .", "label": "", "metadata": {}, "score": "66.021736"}
{"text": "The merits of each approach are intuitively clear : inducing a model is more expensive c ... \" .The merits of each approach are intuitively clear : inducing a model is more expensive computationally , but may support a wider range of predictions .", "label": "", "metadata": {}, "score": "66.62664"}
{"text": "Of course , this is \" just another day in the wonderful world of math and TCS .\" Even if everyone \" already knows \" X is true , proving X very often requires inventing new techniques that then get applied far beyond X , and in particular to problems for which the right answer was much less obvious a priori .", "label": "", "metadata": {}, "score": "66.8497"}
{"text": "Combinatorial optimization problems such as routing , scheduling , covering and packing problems abound in everyday life .Unfortunately , most problems of interest are NP - hard .One way to cope with NP - hardness is to relax the optimality requirement and instead look for solutions that are provably close to the optimum .", "label": "", "metadata": {}, "score": "67.25278"}
{"text": "We show how the resulting learning algorithm can be applied to a variety of problems , including gambling , multiple - outcome prediction , repeated games and prediction of points in R n .We also show how the weight - update rule can be used to derive a new boosting algorithm which does not require prior knowledge about the performance of the weak learning algorithm . 1 Introduction A gambler , frustrated by persistent horse - racing losses and envious of his friends ' winnings , decid ...", "label": "", "metadata": {}, "score": "67.4372"}
{"text": "We consider various classical graph - theoretic optimization problem when restricted to D - DI graphs .Our algorithms are based on classical results in algorithmic graph theory and new structural properties of D - DI graphs that may be of independent interest .", "label": "", "metadata": {}, "score": "67.6102"}
{"text": "We also consider the variant where jobs have release dates .We study the problem of assigning papers to referees .We propose to optimize a number of criteria that aim at achieving fairness among referees / papers .Some of these variants can be solved optimally in polynomial time , while others are NP - hard , in which case we design approximation algorithms .", "label": "", "metadata": {}, "score": "68.078415"}
{"text": "Any way to tell ?I have ... .I 'm currently comparing empirically the differences between Markowitz and Kelly portfolios .I calculated the Kelly weights for monthly return observations over 10 years for a sample of 50 stocks from ... .", "label": "", "metadata": {}, "score": "68.29506"}
{"text": "Any way to tell ?I have ... .I 'm currently comparing empirically the differences between Markowitz and Kelly portfolios .I calculated the Kelly weights for monthly return observations over 10 years for a sample of 50 stocks from ... .", "label": "", "metadata": {}, "score": "68.29506"}
{"text": "The idea is that the portfolio manager sets a contribution of total portfolio volatility ... .How does one calculate the return on a portfolio if the assets in that portfolio were held for varying periods of time ?For Example : $ t_0 $ Buy AAPL at 100 $ t_5 $ Buy MSFT at 20 $ t_1$$_0 $ Sell MSFT at ... .", "label": "", "metadata": {}, "score": "68.325226"}
{"text": "As previous work showed , this implies tight bounds for the Strong Price of Anarchy of the bin packing game .Finally , we study the pure Price of Anarchy of the parametric Bin Packing game for which we show nearly tight upper and lower bounds .", "label": "", "metadata": {}, "score": "68.61356"}
{"text": "As far as I know , Natural proof is not the only barrier preventing us from proving P vs NP . -Mohammad Al - Turkistany Oct 12 ' 10 at 20:16 .@Kaveh , \" It is really hard to prove that $ P\\ne NP$ \" is not equivalent to natural proofs most likely wo n't prove $ P\\ne NP$. There are other barriers . -", "label": "", "metadata": {}, "score": "68.813"}
{"text": "We give approximation algorithms and hardness for the problem .Recently published large - scale data sets of tandem - affinity protein purifications have allowed unprecedented insights into the organization of cellular protein complexes .Several computational methods have been developed to detect co - complexing proteins in these data , aiding in the identification of biologically relevant protein complexes .", "label": "", "metadata": {}, "score": "68.94893"}
{"text": "Suppose you have a two - prover proof system for a language $ L$ : Given input $ x$ , known to everyone , a verifier sends question $ q_1 $ to prover 1 , and question $ q_2 $ to prover 2 .", "label": "", "metadata": {}, "score": "69.30684"}
{"text": "The problem of combining preferences arises in several applications , such as combining the results of different search engines .This work describes an efficient algorithm for combining multiple preferences .We first give a formal framework for the problem .We then describe and analyze a new boosting algorithm for combining preferences called RankBoost .", "label": "", "metadata": {}, "score": "70.44882"}
{"text": "There are uncomputable recursively enumerable ( RE ) sets ( such as the Halting problem ) .Matiyasevich proved that any recursively enumerable set is Diophantine .This immediately implies the impossibility of Hilbert 's 10th problem . -Mohammad Al - Turkistany Oct 12 ' 10 at 20:04 .", "label": "", "metadata": {}, "score": "70.90735"}
{"text": "Choose $ j$ so that the partial derivative of Lmax$(Px)$ with respect to $ x_j$ is at most the partial derivative of Lmin$(Cx)$ with respect to $ x_j$. Increase $ x_j$ by $ \\delta$ , where $ \\delta$ is chosen maximally such that no constraint $ P_i x$ or remaining constraint $ C_i x$ increases by more than $ \\varepsilon$. Assuming the given problem is feasible , the algorithm returns an $ x$ such that $ Px\\le 1 $ and $ Cx\\ge 1-O(\\varepsilon)$.", "label": "", "metadata": {}, "score": "72.06495"}
{"text": "( If you are interested in the algorithms , but not the proof details , you can skip ahead . )Fractional Set Cover .Fix a Set - Cover instance .Let $ A$ denote the element / set incidence matrix .", "label": "", "metadata": {}, "score": "72.406845"}
{"text": "What exactly is meant by \" underlying problem ? \" Do you mean to suggest only problems where there is a deeper problem than a particular statement ? -Philip White Oct 12 ' 10 at 18:16 . -Kaveh Oct 12 ' 10 at 19:00 .", "label": "", "metadata": {}, "score": "72.43027"}
{"text": "The covariances and returns of ... .I 'm currently comparing empirically the differences between Markowitz and Kelly portfolios .I calculated the Kelly weights for monthly return observations over 10 years for a sample of 50 stocks from ... .", "label": "", "metadata": {}, "score": "72.8396"}
{"text": "The covariances and returns of ... .I 'm currently comparing empirically the differences between Markowitz and Kelly portfolios .I calculated the Kelly weights for monthly return observations over 10 years for a sample of 50 stocks from ... .", "label": "", "metadata": {}, "score": "72.8396"}
{"text": "This paper had a ' reproof ' of Schapire 's result , but now the constructed hypothesis used only a single majority .Along these lines , the two then produced another reproof , called AdaBoost : .Yoav Freund and Robert E. Schapire .", "label": "", "metadata": {}, "score": "73.208435"}
{"text": "We discuss two experiments we carried out to assess the performance of RankBoost .In the first experiment , we used the algorithm to combine different WWW search strategies , each of which is a query expansion for a given domain .", "label": "", "metadata": {}, "score": "73.23686"}
{"text": "You receive payoff $ p^t\\cdot a^t$ for the round .The game stops after some number of rounds .Your goal is to minimize your regret in comparison to any single expert ( i.e. , pure strategy ) $ i$. That is , your goal is to minimize $ ( \\max_i a^t_i ) - \\sum_t p^t\\cdot a^t$. Recall that $ G(y)$ is the gradient of Lmax$(y)$. Remark : I think , as Freund and Schapire point out , a \" boosting \" algorithm ( in learning theory ) is also implicit in this analysis .", "label": "", "metadata": {}, "score": "73.38135"}
{"text": "I have daily returns of 10 stocks .I need to construct an equally weighted portfolio that goes long in the 3 highest returns and short in the 3 lowest returns .The portfolio needs to be re - balanced ... .", "label": "", "metadata": {}, "score": "73.999306"}
{"text": "I have daily returns of 10 stocks .I need to construct an equally weighted portfolio that goes long in the 3 highest returns and short in the 3 lowest returns .The portfolio needs to be re - balanced ... .", "label": "", "metadata": {}, "score": "73.999306"}
{"text": "In the second half we give better - than - greedy linear time approximation algorithms for b - matching with approximation ratios of 1/2 and 2/3 - \u03b5 .The cost of transmitting at radius r from vertex u is given by an arbitrary non - decreasing cost function c u ( r ) .", "label": "", "metadata": {}, "score": "74.020226"}
{"text": "We then cast their algorithm as a local ratio algorithm and show that in fact it has an approximation ratio of 4 .Additionally , we consider a more general problem where jobs has release dates and can be preempted .For this version we give a 4\u03ba - approximation algorithm where \u03ba is the number of distinct release dates . . .", "label": "", "metadata": {}, "score": "74.120804"}
{"text": "The conference and journal versions of the same paper are aggregated into a single entry .The local links in each entry point to the most up - to - date version of a paper .You can hide the abstracts if you like .", "label": "", "metadata": {}, "score": "74.88687"}
{"text": "The vast computational resources in billion - transistor VLSI microchips can continue to be used to build aggressively clocked uniprocessors for extracting large amounts of instruction level parallelism .This dis - sertation addresses the problems of implementing wide issue , out - of - order execution , supe ... \" .", "label": "", "metadata": {}, "score": "75.57814"}
{"text": "Just for fun , here is a curious alternative algorithm for Perfect Bipartite Matching .If you are interested in the details of the proof , please ask ... .Mixed Packing and Covering .You may have asked about bipartite matching hoping for an example of a mixed packing and covering problem , that is , one of the form $ $ \\exists x ?", "label": "", "metadata": {}, "score": "76.65663"}
{"text": "Mohammad Al - Turkistany Oct 12 ' 10 at 20:54 .( Ha ha only serious . )The idea that some algorithmic problems require an exponential number of steps , or exaustive search over all possibilities , was raised since the 50s and perhaps before .", "label": "", "metadata": {}, "score": "77.30937"}
{"text": "This paper resolved the question : are strong and weak PAC learning equivalent ?I can not tell you for certain whether the people in that circle ( Schapire , Valiant , Kearns , Avrim Blum , . felt strongly one way or another ( i.e. if this is already an instance of what you seek ) , though I have some suspicions , and you can form your own by looking at the papers around then .", "label": "", "metadata": {}, "score": "78.262"}
{"text": "We present a rigorous analysis of five genomewide protein interaction networks for Saccharomyces cerevisiae supporting our hypothesis and reject two previously proposed explanations for the centrality - lethality rule .The interval constrained coloring problem is the abstraction of a problem that arises in biochemistry .", "label": "", "metadata": {}, "score": "78.283264"}
{"text": "He used this new formalism to prove that the Halting problem is undecidable ( i.e. Halting problem ca n't be solved by any algorithm ) .This led to a long research program which proved the impossibility of the Hilbert 10th problem .", "label": "", "metadata": {}, "score": "80.0039"}
{"text": "The major breakthrough of Cook and Levin was to put this idea on rigorous grounds .This , of course , changed everything .Update : I just realized that my answer like the nice answer of Turkistany addresses the title of the question \" rigour leading to insight \" but perhaps not the specific wording which was about \" rigorous proof to a theorem \" .", "label": "", "metadata": {}, "score": "81.1104"}
{"text": "What about starting with specific algorithms and then using them as data points to generalize ?Such as , people design a few greedy algorithms , and eventually the field develops a notion of a problem with optimal substructure . -Aaron Sterling Oct 13 ' 10 at 3:01 . 8 Answers 8 .", "label": "", "metadata": {}, "score": "81.74405"}
{"text": "Indeed , by turning the problem of choosing a good \" model \" into an optimization problem of its own , we obtain improved approximations for Data Migration to minimize the average vertex completion time .Suppose you are planning a road trip across the Unites States : You want to go from New York City to San Francisco .", "label": "", "metadata": {}, "score": "86.21011"}
{"text": "Whereas the doubling trick restarts the on - line algorithm several ti ... .This paper reviews some results in this area ; the new material in it includes the proofs for the performance of the Aggregating Algorithm in the problem of linear regression with square loss .", "label": "", "metadata": {}, "score": "87.56332"}
{"text": "@ChandraChekuri :It 's rather delayed , but I 'm wondering if you should make this an answer ? 1 Answer 1 .Luca , since a year has passed , you probably have researched your own answer .I 'm answering some of your questions here just for the record .", "label": "", "metadata": {}, "score": "88.74754"}
{"text": "$ $ The invariant implies $ $ \\max Px \\le 2\\ln(m ) + ( 1+O(\\varepsilon ) ) \\min Cx . $ $ At termination the left - hand side is $ \\Omega(\\log(m)/\\varepsilon)$ , proving the performance guarantee .In Step 2.1 , the desired $ j$ must exist as long as the original problem is feasible .", "label": "", "metadata": {}, "score": "92.71179"}
