{"text": "In addition , we show that this generalized CRE can be used as an alternative to differential entropy to derive information - based optimization criteria for system identification purpose .Introduction .The concept of entropy is important for studies in many areas of engineering such as thermodynamics , mechanics , or digital communications .", "label": "", "metadata": {}, "score": "26.570856"}
{"text": "26 - 37 , Jan. 1980 .I followed their introduction to . A. Wehrl , \" General properties of entropy , \" Reviews of Modern Physics , vol .50 , no . 2 , pp .221 - 260 , Apr. 1978 . who never uses the term .", "label": "", "metadata": {}, "score": "31.115257"}
{"text": "\\ ) Since a graph and its adjacency matrix essentially determine the same information , it is customary to pass freely back and forth between a graph and its adjacency matrix when specifying a vertex shift or edge shift .Topological entropy for continuous dynamical systems , in particular for shift spaces , was introduced by Adler , Konheim and McAndrew [ 1965].", "label": "", "metadata": {}, "score": "31.361639"}
{"text": "And who invented this term ?I looked in : .J. E. Shore and R. W. Johnson , \" Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross - entropy , \" Information Theory , IEEE Transactions on , vol .", "label": "", "metadata": {}, "score": "35.09431"}
{"text": "and .I. Good , \" Maximum Entropy for Hypothesis Formulation , Especially for Multidimensional Contingency Tables , \" The Annals of Mathematical Statistics , vol .34 , no . 3 , pp .911 - 934 , 1963 .but both papers define cross - entropy to be synonymous with KL - divergence .", "label": "", "metadata": {}, "score": "35.190334"}
{"text": "In this case I expect the Haar measure to maximize the entropy .Is there an analog of information theory developed in this framework ?I am particularly interested in the case where .Any references or insights are greatly welcome !", "label": "", "metadata": {}, "score": "35.259155"}
{"text": "There is a another nice proof that is based on the fact that the entropy of x+y is at least the minimum of the individual entropies .Then one shows that by adding more and more terms in the summation , the entropy can only increase , and by the CLT , one get that this is a Gaussian .", "label": "", "metadata": {}, "score": "37.676697"}
{"text": "It should remain the same if we reorder the probabilities assigned to different values of \\(X\\ ; \\ ) .The uncertainty about two independent random variables should be the sum of the uncertainties about each of them .He then showed that the only measure of uncertainty that satisfies all these conditions is the entropy , defined as .", "label": "", "metadata": {}, "score": "39.09065"}
{"text": "In this case all reasonable definitions of the informational dimension coincide , and this is just the Hausdorff dimension of the measure $ \\mu$ ( i.e. , the infimum of the Hausdorff dimensions of sets of full measure ) .Both the Hausdorff dimension of a measure and its entropy have the same property : the bigger they are , the more \" equidistributed \" is the measure .", "label": "", "metadata": {}, "score": "39.752224"}
{"text": "My preliminary thought was using the Central Limit Theorem .As adding 2 independent variables \" grows \" the Entropy .2 steps are needed for that : 1 .Make sure any variable with a certain Variance can be described by the sum of 2 other independent variables .", "label": "", "metadata": {}, "score": "40.22458"}
{"text": "In many cases , one can explicitly compute entropy .\\ )This result relies on Perron - Frobenius Theory ( Seneta [ 1980 ] ) .\\ )This gives a means of computing the entropy of a sofic shift .", "label": "", "metadata": {}, "score": "40.420692"}
{"text": "What I do n't see is why if I set g and f otherway around it does n't prove just the opposite ?Where do we use the properties of f or g to prove the one with Gaussian Distribution has a bigger Entropy ( Of course I was aware of the Log property ) .", "label": "", "metadata": {}, "score": "40.767933"}
{"text": "\\(e\\ ) .nats .bans .Interpretation .To understand what \\(I(X;Y)\\ ) actually means , we first need to define entropy and conditional entropy .Qualitatively , entropy is a measure of uncertainty - the higher the entropy , the more uncertain one is about a random variable .", "label": "", "metadata": {}, "score": "40.915302"}
{"text": "perhaps someone else here knows of such generalizations . )Another approach is to use the lebesgue decomposition theorem and use differential entropy for the part that has a density wrt lebesgue measure , and something else for the other part . -", "label": "", "metadata": {}, "score": "41.304127"}
{"text": "Finally , from what I have been reading lately , it seems that this is hopeless : just the fact of having an infinite measure space seems to break down all nice properties of the discrete entropy .However , I want to keep this question open to further comments from people with more acquaintance with information theory .", "label": "", "metadata": {}, "score": "42.29543"}
{"text": "One can not overstate the importance of entropy as an invariant .Yet it is somewhat crude ; it is perhaps not surprising that a single numerical invariant would not suffice to completely capture the many intricacies of shift spaces , even of sofic shifts or SFT 's .", "label": "", "metadata": {}, "score": "42.63022"}
{"text": "So to answer the question : if you want to deal with non - continuous distributions , you have to tweak your definition of differential entropy .( how much of that derivation you can re - use depends on the measure you choose . )", "label": "", "metadata": {}, "score": "42.74365"}
{"text": "Mutual information is intimately related to the Kullback - Leibler divergence ( Cover and Thomas , 1991 ) , a very natural measure of the \" distance \" between two distributions .It is defined as follows : for any two distributions \\(P(z)\\ ) and \\(Q(z)\\ , \\ ) .", "label": "", "metadata": {}, "score": "43.41388"}
{"text": "\\ )A vast array of notions of isomorphism in between conjugacy and measure - theoretic isomorphism continue to be studied ( e.g. , Parry and Tuncel [ 1982 ] , Marcus and Tuncel [ 2001 ] ) .Both measure - theoretic and topological entropy are descendants of information - theoretic entropy introduced by Shannon [ 1948].", "label": "", "metadata": {}, "score": "43.9411"}
{"text": "But , as shown in the main text , that probability can be made arbitrarily small .A detailed proof of the channel coding theorem is beyond the scope of this article and we refer the readers to the relevant literature ( Shannon and Weaver 1949 , Cover and Thomas 1990 ) .", "label": "", "metadata": {}, "score": "44.002865"}
{"text": "This was the beginning of a rigorous theory of constrained coding ( Marcus , Roth and Siegel [ 1998 ] ) .The major concepts of symbolic dynamics naturally generalize to higher dimensions , i.e. to spaces of ( multi - dimensional ) arrays rather than spaces of ( 1-dimensional ) sequences .", "label": "", "metadata": {}, "score": "44.372986"}
{"text": "The support of a stationary process is a shift space , and one can view the language of a shift space as the set of possible outcomes of such a process .In ergodic theory , there is a notion of measure - theoretic entropy , and the topological entropy of a shift space is the supremum of the measure - theoretic entropies of stationary processes with this support .", "label": "", "metadata": {}, "score": "44.913837"}
{"text": "388 - 400 , Springer , Ambleside , UK , July 2003 .M. Rao , Y. Chen , B. C. Vemuri , and F. Wang , \" Cumulative residual entropy : a new measure of information , \" IEEE Transactions on Information Theory , vol .", "label": "", "metadata": {}, "score": "45.451893"}
{"text": "18 - 19 of Shannon and Weaver 's The Mathematical Theory of Communication ( 1949 ) and p. 76 of Wiener 's Cybernetics ( 1948 ) .Thanks !I checked those references , but I am still having trouble finding the term \" cross entropy \" or a matching equation .", "label": "", "metadata": {}, "score": "45.66505"}
{"text": "The concept of CRE has found nice interpretations and applications in the fields of reliability ( see [ 5 ] where the concept of dynamic CRE is introduced ) and images alignment [ 3 ] .Shannon entropy can be seen as a particular case of exponential entropy , when entropy order tends to 1 .", "label": "", "metadata": {}, "score": "45.876152"}
{"text": "6 , pp .1220 - 1228 , 2004 .View at Publisher \u00b7 View at Google Scholar .M. Asadi and Y. Zohrevand , \" On the dynamic cumulative residual entropy , \" Journal of Statistical Planning and Inference , vol .", "label": "", "metadata": {}, "score": "45.89427"}
{"text": "But when they come to ... .Let $ n(t)$ denote the number of electrons in this ensemble .Shannon 's entropy measures the information content by means of probability .Is it the information content or the information that increases or decreases with entropy ?", "label": "", "metadata": {}, "score": "46.275276"}
{"text": "What can we infer if the entropy of this error term decreases ?Can we conclude that the error is reducing and the system is behaving close to the desired signal 's behavior ?You seem to be asking a lot of questions on Information Theory that are quite vague .", "label": "", "metadata": {}, "score": "46.634087"}
{"text": "Abstract .We consider the cumulative residual entropy ( CRE ) a recently introduced measure of entropy .While in previous works distributions with positive support are considered , we generalize the definition of CRE to the case of distributions with general support .", "label": "", "metadata": {}, "score": "46.79441"}
{"text": "S. Kullback and R. Leibler , \" On information and sufficiency , \" The Annals of Mathematical Statistics , vol .22 , no . 1 , pp .79 - 86 , 1951 .I looked in .T. M. Cover and J. A. Thomas , Elements of Information Theory ( Wiley Series in Telecommunications and Signal Processing ) .", "label": "", "metadata": {}, "score": "46.88709"}
{"text": "Section 2 introduces the GCRE definition .Some properties of GCRE are discussed in Section 3 .In Section 4 , we introduce cumulative entropy rate and mutual information rate .Section 5 deals with maximum GCRE distributions .With a view to illustrate the potentiality of GCRE , in Section 6 , we show on a simple example a possible benefit of GCRE for systems identification .", "label": "", "metadata": {}, "score": "46.9152"}
{"text": "In Shannon 's approach , discrete values and absolutely continuous distributions are treated in a somewhat different way through entropy and differential entropy , respectively .Considering the complementary cumulative distribution function ( CCDF ) instead of the probability density function in the definition of differential entropy leads to a new entropy measure named cumulative residual entropy ( CRE ) [ 3 , 4 ] .", "label": "", "metadata": {}, "score": "48.009163"}
{"text": "Absolutely !I will rephrase it .Thank you - Crist\u00f3bal Guzm\u00e1n Apr 18 ' 14 at 21:04 .entropy is defined by a dynamical system , not just a space .Which dynamical system are you considering here ? -Asaf Apr 19 ' 14 at 17:47 .", "label": "", "metadata": {}, "score": "48.14545"}
{"text": "I disagree with this statement .Arguably , the most basic notion of entropy is Shannon entropy , which assigns to every finite probability space a number measuring its effective size : en.wikipedia.org/wiki/Shannon_entropy - Tobias Fritz Apr 21 ' 14 at 0:46 .", "label": "", "metadata": {}, "score": "48.17829"}
{"text": "For example , \" he \" and \" write \" are probably both quite frequent unigrams , but the bigram \" he write \" is highly unlikely because it violates number agreement between the subject and the object .Like any relative entropy , that value is indeed guaranteed to be non - negative ; e.g. see Cover , T. M. and Thomas , J. A. ( 1991 ) , Elements of Information Theory , Wiley , New York .", "label": "", "metadata": {}, "score": "48.65675"}
{"text": "We also complete the power moment constrained maximum CRE distributions problem that was adressed in [ 7 ] , for classes of distributions that have lower - unbounded supports .Finally , we illustrate the potential superiority of the proposed generalized CRE ( GCRE ) against differential entropy in mutual information - based estimation problems .", "label": "", "metadata": {}, "score": "48.859486"}
{"text": "Intrinsic Markov chains .Trans .Amer .Math .Soc . , 112 ( 1964 ) , 55 - -66 .W. Parry and S. Tuncel .Classification Problems in Ergodic Theory .LMS Lecture Note Series 67 ( 1982 ) , Cambridge Univ .", "label": "", "metadata": {}, "score": "49.05113"}
{"text": "\\ )When designing a channel , then , one has control over only two quantities\\[M\\ ] and \\(P_X(x)\\ .The three steps of the communication channel are illustrated as .\\ ] .However , the channel coding theorem ( Shannon and Weaver , 1949 ) states that one can come close , so long as \\(M\\ ) is not too large .", "label": "", "metadata": {}, "score": "49.363358"}
{"text": "An active area of research is designing codes - known as error correcting codes - that come as close as possible to the limit given by mutual information .References .Cover , T.M. and Thomas , J.A. ( 1991 ) .", "label": "", "metadata": {}, "score": "49.422073"}
{"text": "John Wiley & Sons , New York , NY .Gray , R.M. ( 1990 ) .Entropy and Information Theory .Springer - Verlag , New York , NY .Where have you seen a definition suggesting ( pointwise ) MI must be non - negative ?", "label": "", "metadata": {}, "score": "49.47416"}
{"text": "Trans .Amer .Math .Soc.,114( 1965 ) 309 - -319 .R. Adler and B. Marcus .Topological Entropy and Equivalence of Dynamical Systems .Mem .Amer .Math .Soc .R. Adler and B. Weiss .", "label": "", "metadata": {}, "score": "49.51978"}
{"text": "K. A. S. Immink .Codes for Mass Data Storage .Shannon Foundation Press , 2nd edition , 2004 .P. W. Kasteleyn .The statistics of dimers on a lattice .Physica A , 27 ( 1961 ) 1209 - -1225 .", "label": "", "metadata": {}, "score": "49.8014"}
{"text": "The deterministic mapping from the set of messages to the set of codewords is called the encoding function .\\ ) .\\ ) .There are two elements of a communication channel that should be emphasized .First , the number of messages is typically much less than the number of possible messages .", "label": "", "metadata": {}, "score": "49.855503"}
{"text": "6 , pp .1931 - 1941 , 2007 .View at Publisher \u00b7 View at Google Scholar .J. N. Kapur , Maximum Entropy Methods in Science and Engineering , John Wiley & Sons , New York , NY , USA , 1989 .", "label": "", "metadata": {}, "score": "50.082558"}
{"text": "As a consequence , if . is a Markov chain , we have the data processing inequality for GCRE : .Entropy and Mutual Information Rates .Entropy Rate .The GCRE of a stochastic process . is defined by .l .", "label": "", "metadata": {}, "score": "50.198784"}
{"text": "What I 'm trying to say is that the question is ill posed because differential entropy is defined wrt continuous distributions -- ie distributions with a density $ f$ wrt regular integration ( Lebesgue measure ) .continuing in another comment .", "label": "", "metadata": {}, "score": "50.341423"}
{"text": "C. E. Shannon , \" A Mathematical Theory of Communication , \" Bell system technical journal , vol .Does n't mention cross entropy ( and has a strange definition of \" relative entropy \" : \" The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols \" ) .", "label": "", "metadata": {}, "score": "50.746937"}
{"text": "So , for sofic shifts ( in particular , SFT 's ) all of the periodic point information is determined by a finite collection of complex numbers , namely the zeros and poles of the zeta function .While this is not a complete invariant ( Kim and Roush [ 1999 ] , Wagoner [ 2004 ] ) , it encapsulates many very strong invariants that go well beyond entropy and the zeta function .", "label": "", "metadata": {}, "score": "50.888397"}
{"text": "If we now let .with \\(\\epsilon\\ ) positive , we have automatically satisfied Eq .( 5 ) , in the large \\(n\\ ) limit .As \\(n\\ ) increases , one comes closer to this bound .The connection between mutual information and the number of messages that can be sent is a deep one , but it turns out to be fairly easy to understand , as can be seen with the following example .", "label": "", "metadata": {}, "score": "50.996407"}
{"text": "Springer , 1998 .D. Lind and B. Marcus .An Introduction to Symbolic Dynamics and Coding .Cambridge University Press , 1995 .The former assumes basic first - year graduate mathematics , while the latter assumes only very modest prerequisites .", "label": "", "metadata": {}, "score": "51.2871"}
{"text": "Thank you for clearing a lot of doubts .Last thing , by information do we mean the amount of information since I remember reading that Shannon 's theory is related to how much information in bits can be communicated over a noiseless channel .", "label": "", "metadata": {}, "score": "51.30775"}
{"text": "What I would like to know is if entropy increases , does this mean that information increases ?If there are 2 signals , one is the desired and the other is the measurement signal .Let error be the difference between the two .", "label": "", "metadata": {}, "score": "51.31102"}
{"text": "Their variance is equal hence the Entropy of the Gaussian is bigger .Is that make any sense ? -Drazick Jan 13 ' 10 at 9:32 .Drazick : the proof you suggest can be made to work , but is vastly more difficult than the Jensen 's inequality argument in Deane 's post below .", "label": "", "metadata": {}, "score": "51.32166"}
{"text": "when the limit exists .Theorem 4 .For stationary processes , the limit exists .Proof .Consider .The first line follows from the fact that conditioning reduces entropy and the second follows from the stationarity ( see [ 2 ] for the equivalent proof in the case of Shannon entropy ) .", "label": "", "metadata": {}, "score": "51.761467"}
{"text": "As far as I can see , the OP 's question is about \" entropy \" in the sense of Shannon entropy , not Kolmogorov - Sinai entropy . - Tobias Fritz Apr 21 ' 14 at 2:42 .2 Answers 2 .", "label": "", "metadata": {}, "score": "52.276924"}
{"text": "See ( Gray , 1990 ) for details .The units of information depend on the base of the logarithm .If base 2 is used ( the most common , and the one used here ) , information is measured in bits .", "label": "", "metadata": {}, "score": "52.509026"}
{"text": "However , it is a measure ideally suited for analyzing communication channels .Abstractly , a communication channel can be visualized as a transmission medium which receives an input \\(x\\ ) and produces an output \\(y\\ .\\ )If the channel is noiseless , the output will be equal to the input .", "label": "", "metadata": {}, "score": "52.690742"}
{"text": "Why do models of submolecular phenomena involving randomness work ?Do these phenomena appear random to other submolecular particles ?For example , people can use Einstein - Smoluchowski to characterize ... .According to kinetic theory , average kinetic energy is proportional to temperature .", "label": "", "metadata": {}, "score": "52.764133"}
{"text": "Why does a cloth or a mask reduce the intensity of a stale smell ?Are the \" smell \" ... .How to get the equation for entropy $ S$ ... .I 'm making lecture notes , and I felt that it would be important to explain some of the math behind collision theory .", "label": "", "metadata": {}, "score": "52.886276"}
{"text": "Drazick Jan 16 ' 10 at 9:35 .Ok first , the entropy you 're talking about is the differential entropy $ -\\int f(x ) \\ln f(x ) d\\mu(x)$. The problem is that $ \\mu$ is Lebesgue measure .The set of continuous probability distributions is the set of distributions that have a density ( i.e. radon - nikodym derivative ) wrt Lebesgue measure .", "label": "", "metadata": {}, "score": "52.991745"}
{"text": "References .C. E. Shannon , \" A mathematical theory of communication , \" Bell System Technical Journal , vol .27 , pp .379 - 423 , 1948 .View at Google Scholar .J. M. Cover and J. H. Thomas , Elements of Information Theory , Wiley Interscience , New York , NY , USA , 2006 .", "label": "", "metadata": {}, "score": "53.05401"}
{"text": "See , for example , these lecture notes .An information - theoretic approach has been developed in the context of scattering theory , mainly for the unitary group , but I imagine the results are readily transposed to the orthogonal group .", "label": "", "metadata": {}, "score": "53.097755"}
{"text": "\\ ) Specifically , the channel coding theorem states the following : .First , define the channel capacity , \\(C\\ , \\ ) as the maximum mutual information with respect to the input distribution , \\(P_X\\ , \\ ) .\\ ] .", "label": "", "metadata": {}, "score": "53.218773"}
{"text": "High mutual information indicates a large reduction in uncertainty ; low mutual information indicates a small reduction ; and zero mutual information between two random variables means the variables are independent .Contents . \\ ] .\\ )The focus here is on discrete variables , but most results derived for discrete variables extend very naturally to continuous ones - one simply replaces sums by integrals .", "label": "", "metadata": {}, "score": "53.61654"}
{"text": "For some channels , in order to achieve reliable transmission , it is necessary to encode arbitrary messages to sequences that satisfy certain constraints , such as a bound on the number of consecutive zeros between two ones .Symbolic dynamics has played an important role in providing a framework for constructing such encoders .", "label": "", "metadata": {}, "score": "53.775063"}
{"text": "The conditional entropy is the average uncertainty about \\(X\\ ) after observing a second random variable \\(Y\\ , \\ ) and is given by .\\ ) .( 1 ) can be written .\\ ] .Mutual information is therefore the reduction in uncertainty about variable \\(X\\ , \\ ) or the expected reduction in the number of yes / no questions needed to guess \\(X\\ ) after observing \\(Y\\ .", "label": "", "metadata": {}, "score": "53.852592"}
{"text": "Using Stirling 's formula and taking the large \\(n\\ ) limit , this expression can be written .\\ ] .where the last equality follows from Eq .This analysis , which required only the use of Stirling 's formula ( and can be easily extended to the general case ) , provides a link between information theory and communications channels .", "label": "", "metadata": {}, "score": "53.933525"}
{"text": "I 'm not familiar with some of the concepts you presented .Could you make it a bit clearer ?Thanks . -Drazick Jan 12 ' 10 at 8:54 .Sure .First let me make sure I understood your question : you are comfortable with the proof that , out of the set of all continuous distributions with variance 1 , the one with maximum entropy is the gaussian distribution .", "label": "", "metadata": {}, "score": "53.98294"}
{"text": "I have n't got the time to check references where these facts could be proven , so I will wait a couple of days for more answers . - Crist\u00f3bal Guzm\u00e1n Apr 20 ' 14 at 23:24 .Otherwise ( i.e. , for singular measures ) differential entropy does not make much sense .", "label": "", "metadata": {}, "score": "54.168026"}
{"text": "Simulation Results .With a view to emphasize the potential practical interest of GCRE , we consider a simple system identification problem .Here , we consider an . such that RVs . and .show the highest dependence .Shannon MI between . and . is given by . , where . is Shannon differential entropy .", "label": "", "metadata": {}, "score": "54.248177"}
{"text": "W. Krieger .On the subsystems of topological Markov chains .Ergod .Th .& Dynam .Sys . , 2 ( 1982 ) , 195 - -202 . D. Lind .Multi - dimensional symbolic dynamics .Symbolic Dynamics and its Applications , ed .", "label": "", "metadata": {}, "score": "54.402145"}
{"text": "For simulations , we have chosen .Gaussian and .with a Laplace distribution : .e . x .p .We consider an experiment with . and noise variance equal to 0.2 .Estimation is carried out from observation of .", "label": "", "metadata": {}, "score": "54.72695"}
{"text": "Conclusion .We have shown that the concept of cumulative residual entropy ( CRE ) introduced in [ 3 , 4 ] can be extended to distributions with general supports .Generalized CRE ( GCRE ) shares many nice features of CRE .", "label": "", "metadata": {}, "score": "55.134552"}
{"text": "More quantitatively , consider two random variables , \\(X\\ ) and \\(Y\\ , \\ ) whose mutual information is \\(I(X , Y)\\ .\\ )Now consider a third random variable , \\(Z\\ , \\ ) that is a ( probabilistic ) function of \\(Y\\ ) only .", "label": "", "metadata": {}, "score": "55.653595"}
{"text": "Here for the first time symbolic systems are treated in the abstract , as objects in their own right .This abstract study was motivated both by the intrinsic mathematical interest of symbolic systems and the need to better understand them in order to apply symbolic techniques to continuous systems .", "label": "", "metadata": {}, "score": "55.9477"}
{"text": "An Introduction to Ergodic Theory .Springer Graduate Texts in Math , 79 ( 1982 ) .P. Walters ( editor ) .Symbolic Dynamics and Its Applications .Contemporary Mathematics 135 ( 1992 ) .B. Weiss .Subshifts of finite type and sofic systems .", "label": "", "metadata": {}, "score": "56.31501"}
{"text": "Second Ed.,Springer , 1980 .C. Shannon .A mathematical theory of communication .Bell Sys .Tech .J. , 27 ( 1948 ) , 379 - -423 , 623 - -656 . S. Smale .Differentiable dynamical systems .", "label": "", "metadata": {}, "score": "56.781487"}
{"text": "Matus Telgarsky Jan 12 ' 10 at 10:29 .anyway there 's no way to plug a non - continuous distribution into the standard definition of differential entropy .( as in , the standard definition is implicitly ' with respect to lebesgue measure ' .", "label": "", "metadata": {}, "score": "57.027714"}
{"text": "With this understanding , all binary sequences are obtained as symbolic orbits .In general , given \\(T\\ , \\ ) we seek a partition for which the set of symbolic orbits is constrained by some explicit rule .For example , Markov partitions for hyperbolic toral automorphisms give rise to shifts of finite type , defined below ( Adler and Weiss [ 1970 ] ) ) .", "label": "", "metadata": {}, "score": "57.055824"}
{"text": "Shift spaces are classified , up to various kinds of invertible encodings , by combinatorial , algebraic , topological and measure - theoretic invariants .The subject is intimately related to dynamical systems , ergodic theory , automata theory and information theory .", "label": "", "metadata": {}, "score": "57.283894"}
{"text": "Constrained systems and coding for recording .Chapter in Handbook on Coding Theory ( ed . by R. Brualdi , C. Huffman and V. Pless ) , Elsevier , 1998 ( updated version $ \\sim$marcus / Handbook/ here ) .M. Morse and G. A. Hedlund .", "label": "", "metadata": {}, "score": "57.405277"}
{"text": "This may be a more appropriate question for the Mathematics stack exchange , if so forgive its extraneousness .We know the expected value of the distribution , analogous to the weighted arithmetic mean ... .Could anybody refer me to some literature ( textbooks or research , although preferably textbooks ! ) dedicated to explaining quantitatively the phenomenon of evaporation of fluids ?", "label": "", "metadata": {}, "score": "57.4331"}
{"text": "Concepts in Statistical Mechanics \" - Arthur Hobson 2 . \"The Mathematical Theory of Communication \" - Claude E. Shannon & Warren Weaver 3 . \"The Uncertain Reasoner 's Companion \" - J. B. Paris 4 . \" Elements of Information Theory \" - Thomas M. Cover & Joy A. Thomas 5 . \"", "label": "", "metadata": {}, "score": "57.999496"}
{"text": "Algorithms for sliding block codes -- an application , of symbolic dynamics to information theory .IEEE Trans .Inform .Theory , 29 ( 1983 ) , 5 - -22 .R. Adler , A. Konheim , and M. McAndrew .", "label": "", "metadata": {}, "score": "58.05356"}
{"text": "So you can use it to talk about situations where you are n't sending numbers , or where the numbers are just arbitrarily assigned to particular messages or symbols that you need to send .What you want for your second question is the conditional entropy of the measurement random variable $ X$ given the random variable $ Y$ that represents what was sent .", "label": "", "metadata": {}, "score": "58.08963"}
{"text": "p .for .[ . [ .In [ 7 ] , this result is derived from the log - sum inequality , but of course it can also be derived from the Euler - Lagrange equation along the same lines as in the proof of Theorem 6 .", "label": "", "metadata": {}, "score": "58.159256"}
{"text": "There has also been much progress on the factor and embedding problems for SFT 's and sofic shifts .Some of these are described in Boyle , Marcus and Trow [ 1987].Other directions .Symbolic dynamics is intimately tied to ergodic theory , which is the study of the dynamics of measure - preserving transformations , and in particular stationary stochastic processes ( where the measure space is a space of sequences and the transformation is the shift map ) .", "label": "", "metadata": {}, "score": "58.798553"}
{"text": "e . x .p . for .Thus the solution , if it exists , is an exponential distribution .In fact , the first and second power moment constraints must be such that .[ . 2 . ][ . .", "label": "", "metadata": {}, "score": "58.943253"}
{"text": "However , both Rao et al . 's CRE and its exponential entropy generalization by Zografos and Nadarajah lead to entropy - type definitions that assume either positive valued RVs or apply to . otherwise .Although the positive case is of great interest for many applications , CRE and exponential entropies entail difficulties when working with RVs with supports that are not restricted to positive values . , without further hypothesis than in [ 4 ] .", "label": "", "metadata": {}, "score": "59.307396"}
{"text": "This difference is important since the use of an iterative local optimization technique would have failed in general to find Shannon 's estimated MI global optimum , because of its many local maxima .Of course , this drawback can be partly solved by kernel smoothing of the empirical distribution of . , for instance by using the method proposed in [ 10 ] .", "label": "", "metadata": {}, "score": "60.0179"}
{"text": "\\ ) Second , we have not discussed how to optimize mutual information with respect to \\(P_X(x)\\ .\\ )In general this must be done numerically .For the above example , though , there it is simple to find an analytic solution .", "label": "", "metadata": {}, "score": "60.17666"}
{"text": "and .[ . 2 . ]Then the maximum GCRE symmetric solution for the CCDF of X is given by .e . x .p .for . , which is the CCDF of a logistic distribution .The moment constraints lead to .", "label": "", "metadata": {}, "score": "60.539364"}
{"text": "Robert Gallager .Information Theory and Reliable Communication .New York : John Wiley and Sons , 1968 .ISBN 0 - 471 - 29048 - 3 .Imre Csiszar , Janos Korner .Information Theory : Coding Theorems for Discrete Memoryless Systems Akademiai Kiado : 2nd edition , 1997 .", "label": "", "metadata": {}, "score": "60.686333"}
{"text": "\\ ] This follows directly from the definition , Eq .Mutual information is additive for independent variables .\\ ] .This follows easily from the definition of the mutual information .It 's also a property that information should have .", "label": "", "metadata": {}, "score": "60.776154"}
{"text": "Here , I 'm trying to show that the collision of ... .Some years ago from now I 've seem some basic details about what was then called \" kinetic theory of gases \" where the study of property of gases was made by statistical considerations about the momentum ... .", "label": "", "metadata": {}, "score": "60.888992"}
{"text": "And there are several collections of articles on special areas of the subject , such as Blanchard , Maass and Nogueira [ 2000 ] , Walters [ 1992 ] , and S. Williams [ 2004].Finally , Boyle [ 2007 ] has written a survey of Open Problems .", "label": "", "metadata": {}, "score": "60.921146"}
{"text": "G. A. Hedlund .Endomorphisms and automorphisms of the shift dynamical system .Math .Systems Theory , 3 ( 1969 ) , 320 - -375 .M. Hochman and T. Meyerovitch .A characterization of the entropies of multidimensional shifts of finite type .", "label": "", "metadata": {}, "score": "61.008327"}
{"text": "As usual , one constructs a codebook containing only a subset of possible strings of 0s and 1s ; that is , for strings of length \\(n\\ , \\ ) the codebook contain far fewer than \\(2^n\\ ) messages .How would one figure out which message was sent ?", "label": "", "metadata": {}, "score": "61.063324"}
{"text": "\\ [ I(X;Z ) \\le I(X;Y ) \\ , . \\ ] .This inequality , which again is a property information should have , is easy to prove , . \\ ] .The first equality follows from Eq .\\ ) .", "label": "", "metadata": {}, "score": "61.17965"}
{"text": "This assumption results in no loss of generality for the purposes of symbolic dynamics .\\ ) .For example , the golden mean shift is the set of all sequences obtained by reading off the vertex sequence in paths that traverse the graph shown in Figure 3 .", "label": "", "metadata": {}, "score": "61.33383"}
{"text": "249 - -316 .M. Boyle , B. H. Marcus , and P. Trow .Resolving Maps and the Dimension Group for Shifts of Finite Type .Mem .Amer .Math .Soc ., 377 ( 1987 ) .T. Cover and J. Thomas .", "label": "", "metadata": {}, "score": "61.442413"}
{"text": "Early work on these difficult issues can be found in Berger [ 1966 ] , Kastelyn [ 1967 ] , and R.M. Robinson [ 1971].Lind [ 2004 ] gives an expository treatment of multi - dimensional symbolic dynamics and E. A. Robinson [ 2004 ] does the same for tiling systems .", "label": "", "metadata": {}, "score": "61.742905"}
{"text": "The aim is to cover technical topics at a level comparable to a tutorial or a basic book .The exact level of specificity may be hard to describe , but thinking of a good introductory graduate - level course you took may be a good start .", "label": "", "metadata": {}, "score": "61.94187"}
{"text": "Suppose that we want to transmit \\ ( M \\ ) messages using codewords of length \\ ( n \\ .\\ )This is defined as .\\ ] .\\ ] .This provides a bound on the number of messages , \\ ( M \\ , \\ ) that can be sent almost error free using codewords of length \\ ( n \\ ) .", "label": "", "metadata": {}, "score": "62.18847"}
{"text": "In the same way , it is clear that ., we do not have such a nice property .However , let us consider the important particular case where the distribution of .has a symmetry of the form .l .", "label": "", "metadata": {}, "score": "62.203068"}
{"text": "Update ------------------------------ .Due to a request , I will further explain .Before I start , for notation and definitions I refer to Cover & Thomas book ( Elements of Information Theory ) .However , in the case your r.v . 's are defined over a compact group $ G$ ( we may restrict here to the ones that are absolutely continuous w.r.t .", "label": "", "metadata": {}, "score": "62.645576"}
{"text": "Mem .Amer .Math .Soc . , 98 ( 1970 ) . A. V. Aho , J. E. Hopcroft , and J. D. Ullman .The Design and Analysis of Computer Algorithms .Addison - Wesley , 1974 .M.-P. B\u00e9al .", "label": "", "metadata": {}, "score": "63.1777"}
{"text": "Amer .Math .Soc.,73 ( 1967 ) 747 - -817 .J. Wagoner .Strong shift equivalence theory .Symbolic Dynamics and its Applications , ed .S. Williams , Proc .Symp .Appl .Math . , 2004 , 121 -- 154 .", "label": "", "metadata": {}, "score": "63.346264"}
{"text": "\\ ) Symbolic dynamics can also be used in classifying dynamical systems ; here , the problem of determining when one dynamical system is \" equivalent \" to another becomes , via symbolic dynamics , a coding problem .Hadamard [ 1898 ] is generally credited with the first use of symbolic dynamics techniques in his analysis of geodesic flows on surfaces of negative curvature .", "label": "", "metadata": {}, "score": "63.603603"}
{"text": "Amer .J. Math . , 60 ( 1938 ) , 815 - -866 .M. Morse and G. A. Hedlund .Symbolic dynamics II : Sturmian trajectories .Amer .J. Math . , 62 ( 1940 ) , 1 - -42 .", "label": "", "metadata": {}, "score": "63.779564"}
{"text": "EDIT I misunderstood the question ; I thought it was asking about entropy for distributions without a density wrt Lebesgue ; all it is asking for is a proof without any conditions on the density .Deane Yang provides such a proof in his answer to the question .", "label": "", "metadata": {}, "score": "63.925262"}
{"text": "Symbolic Dynamics and its Applications , ed .S. Williams , Proc .Symp .Appl .Math . , 2004 , 81 -- 120 .D. Rudolph .Fundamentals of Measurable Dynamics .Oxford Univ .Press , 1990 . E. Seneta .", "label": "", "metadata": {}, "score": "64.01933"}
{"text": "Thomas M. Cover , Joy A. Thomas .Elements of information theory , 1st Edition .New York : Wiley - Interscience , 1991 .ISBN 0 - 471 - 06259 - 6 . : 2nd Edition .New York : Wiley - Interscience , 2006 .", "label": "", "metadata": {}, "score": "64.50382"}
{"text": "If , on the other hand , \\(R \\ge C\\ , \\ ) then errors are likely .More formally , .In the above theorem , \\(R\\ ) is called the rate .The idea is illustrated in Figure 1 .", "label": "", "metadata": {}, "score": "64.588776"}
{"text": "Symbolic dynamics evolved as a tool for analyzing dynamical systems by discretizing space .Imagine a point following a trajectory in a space .Partition the space into finitely many pieces , each labeled by a different symbol .A symbolic orbit is obtained by writing down the sequence of symbols corresponding to the successive partition elements visited by the point in its orbit .", "label": "", "metadata": {}, "score": "64.72221"}
{"text": "\\ ) .\\ )It can be shown that the language of a shift space determines the shift space uniquely , and so one can equally well describe a shift space by specifying the \" occurring \" or \" allowed \" blocks , rather than the forbidden blocks .", "label": "", "metadata": {}, "score": "64.895164"}
{"text": "It is straightforward to show that each \\(p_n(X)\\ ) is an invariant .\\ ] The inequality can be strict .In fact , there are shift spaces with positive entropy but no periodic points at all .\\ ] .The periodic point information can be conveniently combined into a single invariant , known as the zeta function .", "label": "", "metadata": {}, "score": "64.90593"}
{"text": "\\ ] .where .\\ [ h(q )\\equiv -q \\log q - ( 1-q ) \\log ( 1-q ) \\ ] .and the second equality in Eq .( 7 ) follows from a small amount of algebra .", "label": "", "metadata": {}, "score": "64.90714"}
{"text": "Symp .Appl .Math . , 2004 , 81 -- 120 . D. Lind and K. Schmidt .Symbolic and Algebraic Dynamical Systems .Handbook of Dynamical Systems , ed .B. Hasselblatt , A. Katok , Elsevier , 2002 , 765 -- 812 . A.", "label": "", "metadata": {}, "score": "65.10605"}
{"text": "However , the problems discussed in this article are even more difficult in this setting .Even the seemingly innocuous problem of determining if a two - dimensional SFT , defined by a finite list of forbidden patterns of arrays , is nonempty turns out to be undecidable .", "label": "", "metadata": {}, "score": "65.7545"}
{"text": "Axiom A diffeomorphisms have rational zeta functions .Bull .London Math .Soc . , 3 ( 1971 ) 215 - -220 .B. Marcus and S. Tuncel .Resolving Markov Chains onto Bernoulli shifts via positive polynomials .AMS Memoirs , v. 150 , n. 710 ( 2001 ) .", "label": "", "metadata": {}, "score": "65.8987"}
{"text": "Cover and Thomas 's book is indeed the right place to learn about this .The statement basically follows by convexity , in the form of Jensen 's inequality .Here is the way it is usually presented : .Let $ f$ be the probability density of a real random variable .", "label": "", "metadata": {}, "score": "65.93898"}
{"text": "Symp .Pure Math .A.M.S. , 14 ( 1970 ) 43 - -50 .M. Boyle .Open Problems in Symbolic Dynamics . to appear , 2007 .M. Boyle and D. Handelman .The spectra of nonnegative matrices via symbolic dynamics .", "label": "", "metadata": {}, "score": "66.050865"}
{"text": "Such an SFT is called a vertex shift . %Vertex shifts are really the same as the 1-step SFT 's , and up to conjugacy , all SFT 's can be realized as vertex shifts .It is sometimes useful to use the edges , rather than vertices , as symbols ; the sequences are sequences of edges obtained by following valid paths .", "label": "", "metadata": {}, "score": "66.082275"}
{"text": "Masson , 1993 .R. Berger .The Undecidability of the Domino Problem .Mem .Amer .Math .Soc . , 1966 .J. Berstel and D. Perrin .Theory of Codes .Academic Press , 1985 .F. Blanchard , A. Maass , and A. Nogueira .", "label": "", "metadata": {}, "score": "66.1064"}
{"text": "Symbolic dynamics applies to non - invertible dynamical systems as well , although in this case one considers only forward iterations , and the symbolic trajectories are one - sided infinite sequences .In this way , one uses symbolic dynamics to study dynamical systems .", "label": "", "metadata": {}, "score": "66.243744"}
{"text": "e . x .p .e . x .p .Positive Random Variables .It has been shown in [ 7 ] that the maximum CRE ( i.e. , the maximum GCRE under additional nonnegative constraint ) distribution has CCDF in the form .", "label": "", "metadata": {}, "score": "66.27875"}
{"text": "For the first appearance of \\(h\\ ) in Eq . \\ )Thus , this channel is optimal when 0 and 1 appear on the input end with equal probability , and the channel capacity is \\(1-h(q)\\ .\\ ) .", "label": "", "metadata": {}, "score": "66.57515"}
{"text": "For instance , consider the dynamical system in Figure 1 .Here , \\(X\\ ) is the unit square , and \\(T\\ ) is a transformation of \\(X\\ . \\ )A portion of the orbit of a point \\(x \\in X\\ ) is drawn .", "label": "", "metadata": {}, "score": "66.87083"}
{"text": "Full shifts and shift spaces .\\ ) .The advantage of working with bi - infinite sequences is that the shift map is invertible .\\ )For simplicity , concepts in this article are described in the two - sided context .", "label": "", "metadata": {}, "score": "66.98249"}
{"text": "Does anyone know what the equation above is called , and who invented it or has a nice presentation of it ? 2 Answers 2 .It seems to be closely related to the concept of Kullback - Leibler divergence ( see Kullback and Leibler , 1951 ) .", "label": "", "metadata": {}, "score": "67.03803"}
{"text": "If , on the other hand , one used only a subset of the messages , as in Figure 1 b , then decoding could be done perfectly .Figure 1 : How to transmit information error free .a. A scheme that does not transmit information error free .", "label": "", "metadata": {}, "score": "67.12839"}
{"text": "While the even shift is not an SFT , it is a sofic shift , as presented in Figure 4 .Note that here distinct edges ( or vertices ) are allowed to have the same label .Sofic shifts can be regarded as \" finite - state systems \" : in a labelled graph , vertices can be viewed as state information which connects sequences in the past with sequences in the future .", "label": "", "metadata": {}, "score": "67.18489"}
{"text": "SFT 's and sofic shifts are useful in modeling dynamical systems and applications to data recording .However , these classes barely scratch the surface of the range of behaviours exhibited by general shift spaces .Of particular interest are minimal shift spaces , those in which every point has a dense forward orbit .", "label": "", "metadata": {}, "score": "67.34332"}
{"text": "However , the conjugacy problem for SFT 's and sofic shifts remains very much open .There has been much progress on variants of the conjugacy problem .Irreducible SFT 's are completely classified by shift equivalence up to eventual conjugacy ( Kim and Roush [ 1979 , 1988 ] ) .", "label": "", "metadata": {}, "score": "67.35996"}
{"text": "\\ ) Such an SFT is called an \\(M\\)-step SFT , in analogy with the \" finite memory \" property of \\(M\\)-step Markov chains .One - step SFT 's are also called topological Markov chains .The golden mean shift \\(X\\ ) is a 1-step SFT , since it is only the last symbol of an allowed block that determines whether a given symbol can be concatenated at the end .", "label": "", "metadata": {}, "score": "67.38681"}
{"text": "Then , one simply looks for the codeword that has approximately 90 % 1s at positions in the string where the received symbol was 1 , and approximately 90 % 0s at positions in the string where the received symbol was 0 .", "label": "", "metadata": {}, "score": "67.43082"}
{"text": "The use of edges rather than vertices often permits the use of smaller graphs , at the expense of allowing multiple edges all with the same starting vertex and same ending vertex .\\ )Irreducible SFT 's are easier to understand than general SFT 's , and one can study a general SFT by studying its maximal irreducible subshifts ( called irreducible components ) .", "label": "", "metadata": {}, "score": "67.542206"}
{"text": "Other important directions include one - sided shift spaces , countable state symbolic systems , orbit equivalence , flow equivalence , the automorphism group of a shift space , cellular automata , substitution systems and connections with knot theory .References .", "label": "", "metadata": {}, "score": "67.57434"}
{"text": "The noisy channel maps each of them to any of the messages within the circles in the right panel .If one were to try to decode these messages , errors would be made on a regular basis .That 's because multiple messages lie within each circle , so for each message that is received , more than one message could have been sent .", "label": "", "metadata": {}, "score": "67.594574"}
{"text": "I already noticed that early papers were using \" cross entropy \" to mean \" KL divergence \" .( Note that the Kullback paper is in my question . ) - Neil G Jul 10 ' 12 at 16:59 .Sorry , I missed the Kullback paper in the question - Itamar Jul 10 ' 12 at 17:19 Mutual information is one of many quantities that measures how much one random variables tells us about another .", "label": "", "metadata": {}, "score": "67.85562"}
{"text": "Let . and .be two RVs .We define the cumulative mutual information between . and . as follows : . l .o .g .l .o .g .l .o .g .l .", "label": "", "metadata": {}, "score": "68.00429"}
{"text": "1 Answer 1 .They all increase or decrease together .The entropy of a random variable $ X$ is just another number summarizing some quality of that random variable .$ In this case the function of $ X$ you care about is the log ( base 2 ) of the probability mass function .", "label": "", "metadata": {}, "score": "68.43254"}
{"text": "Distance is in quotes because the Kullback - Leibler divergence is not a true distance : it is not symmetric , and it does not obey the triangle inequality ( Cover and Thomas , 1991 ) .\\ ) .From Eq . \\ ] .", "label": "", "metadata": {}, "score": "68.46213"}
{"text": "l .o .g .l .o . g .Then , .l .o .g .e . x .p .for .[ .[ . . .5.1 .Example .We set the constraints .", "label": "", "metadata": {}, "score": "68.55963"}
{"text": "LMS Lecture Notes 279 , Cambridge University Press , 2000 .R. Bowen .On Axiom A Diffeomorphisms .NSF - CBMS Lectures , v. 71,1978 .R. Bowen and O. E. Lanford .Zeta functions of restrictions of the shift transformation .", "label": "", "metadata": {}, "score": "68.599495"}
{"text": "Wiley , 1991 .J. Hadamard .Les surfaces \u00e0 courbures oppos\u00e9es et leurs lignes g\u00e9od\u00e9siques .Journal de Mathematiques Pures et Appliqu\u00e9es , 4 ( 1898 ) 27 - -73 .G. A. Hedlund .Sturmian minimal sets .Amer .", "label": "", "metadata": {}, "score": "68.60217"}
{"text": "Except for trivial cases , SFT 's and sofic shifts are not minimal .Invariants of conjugacy and variants of the conjugacy problem .The conjugacy problem is the problem of determining when two given shift spaces are conjugate .An invariant of conjugacy is any property or object associated to a shift space that is preserved under conjugacy .", "label": "", "metadata": {}, "score": "68.72236"}
{"text": "Because of noise , a particular input message could map to any one of the messages inside the circles in the right hand panels .If one used all possible messages , as in Figure 1 a , and tried to decode - to map \\(Y\\ ) back to \\(X\\ ) - one would make errors .", "label": "", "metadata": {}, "score": "68.83981"}
{"text": "The conjugacy problem for SFT 's reduces to the conjugacy problem for vertex shifts ( or edge shifts ) since every SFT is conjugate to such a shift .Invariants for SFT 's and sofic shifts are naturally described via graphs .", "label": "", "metadata": {}, "score": "68.854485"}
{"text": "\\ ) .In analogy with the characterization of shift spaces as closed shift - invariant sets , sliding block codes can be characterized in a topological manner : namely , as the maps between shift spaces that are continuous and commute with the shift .", "label": "", "metadata": {}, "score": "69.11801"}
{"text": "It is a straightforward exercise in large deviation theory ( see Chapter 12 of Cover & Thomas , 1991 ) to show that \\(P_n\\ ) decreases exponentially with \\ ( n \\ .\\ )In fact , as motivated in the example below , in the large \\(n\\ ) limit , \\(P_n\\ ) is given by .", "label": "", "metadata": {}, "score": "69.12612"}
{"text": "D. T. Pham , \" Fast algorithm for estimating mutual information , entropies and score functions , \" in Proceedings of the 4th International Symposium on Independent Component Analysis and Blind Signal Separation ( ICA ' 03 ) , pp .17 - 22 , Nara , Japan , April 2003 .", "label": "", "metadata": {}, "score": "69.593506"}
{"text": "\\ )The specific choice of metric is not important , so long as it reflects this notion of \" closeness \" \" .A shift space ( or shift or subshift ) is a closed , shift - invariant subset of a full shift .", "label": "", "metadata": {}, "score": "69.60747"}
{"text": "\\ ( .\\ ) .prime shift .\\(X\\ ) is the set of all binary sequences such that between any two successive 1 's , .the number of 0 's is prime .One can take for \\(\\mathcal F\\ ) the collection .", "label": "", "metadata": {}, "score": "70.08026"}
{"text": "In what follows $ f(r , p , t)$ is the usual one - particle distribution function of a monatomic gas , normalised to the ... .I am a martial arts instructor always looking for the next best way to enhance the performance of my students .", "label": "", "metadata": {}, "score": "70.31978"}
{"text": "l .o .g . where . is the dimension of the random vector .Clearly , this formula is valid both for a discrete or an absolutely continuous random variable ( RV ) , or with both a discrete and an absolutely continuous part , because it resorts to the CCDF of .", "label": "", "metadata": {}, "score": "70.48709"}
{"text": "Estimation performance is calculated from 200 successive experiments .Estimation of .from Shannon MI leads to bias and standard deviation that are equal to 0.032 and 0.18 , respectively , while they are equal to 0.004 and 0.06 , respectively , for GCRE MI .", "label": "", "metadata": {}, "score": "70.73105"}
{"text": "The spectra of nonnegative integer matrices via formal power series .J. Amer .Math .Soc .K. H. Kim and F. W. Roush .Some results on decidability of shift equivalence .J. Combinatorics , Info .Sys .Sci . , 4 ( 1979 ) , 123 - -146 .", "label": "", "metadata": {}, "score": "71.466385"}
{"text": "Gillespie proposed a stochastic framework for simulating chemical reactions which is predicated on non - reactive elastic collisions serving to ' uniformize ' particle position so that the assumption of ... .Currently I 'm trying to understand Lattice Boltzmann Method for solving CFD problems .", "label": "", "metadata": {}, "score": "71.96572"}
{"text": "\\ ) \" , then the average number of yes / no questions it takes to guess \\(x\\ ) lies between \\(H(X)\\ ) and \\(H(X)+1\\ ) ( Cover and Thomas , 1991 ) .This gives quantitative meaning to \" uncertainty \" : it is the number of yes / no questions it takes to guess a random variables , given knowledge of the underlying distribution and taking the optimal question - asking strategy .", "label": "", "metadata": {}, "score": "71.98263"}
{"text": "Let $ G$ be a locally compact group .It is known that every locally compact group has a unique right - Haar measure ( up to a multiplicative constant ) .Similarly one can define the left - Haar measure as being invariant for left shifts .", "label": "", "metadata": {}, "score": "73.262314"}
{"text": "] First let us remark that from the proof of the existence of CRE in [ 4 ] , it is sufficient to prove the result when . is a scalar RV , that is . , and for .Then , letting .", "label": "", "metadata": {}, "score": "73.45659"}
{"text": "If it is one - to - one and onto , then it is called a ( topological ) conjugacy ( this is equivalent to the condition that it has an inverse which is a sliding block code ) .Example trivial is an embedding but not a factor code , while Examples 2 to 1 and golden to even are factor codes , but not embeddings .", "label": "", "metadata": {}, "score": "73.596405"}
{"text": "Decidability of shift equivalence .Proceedings of Maryland Special Year in Dynamics 1986 - -87,Springer - Verlag Lecture Notes in Math . , 1342 ( 1988 ) , 374 - -424 .K. H. Kim and F. W. Roush .Williams Conjecture is false for irreducible subshifts .", "label": "", "metadata": {}, "score": "73.63829"}
{"text": "g .l .o .g .l .o .g .l .o .g .l .o .g .l .o .g .s are independent and have lower bounded supports with respective lower bounds . . ]", "label": "", "metadata": {}, "score": "73.65183"}
{"text": "I . ] 0 . ] I . ] 1 . ] I . ]Finally , putting all pieces together one finally proves convergence of right - hand side of ( 2 ) .A Few Properties of GCRE .Let us now exhibit a few more interesting properties of the GCRE .", "label": "", "metadata": {}, "score": "73.94319"}
{"text": "o .g . because .Conditional GCRE definition is a direct extension of the definition of conditional CRE : the conditional GCRE of . knowing that . is equal to . is defined by .l .o .g .", "label": "", "metadata": {}, "score": "75.77492"}
{"text": "We will denote by .the complementary cumulative distribution function ( survival function ) of a multivariate RV .[ . . ] of dimension .We denote by .the GCRE of .that we define by .l .o .", "label": "", "metadata": {}, "score": "75.80588"}
{"text": "Ria George Jul 28 ' 14 at 14:15 Main Page .From Webresearch .These are the modest beginnings of an ambitious project .We want to change how researchers distribute information , exchange ideas , and collaborate .To get started , here is a Wiki .", "label": "", "metadata": {}, "score": "75.81274"}
{"text": "If a sliding block code \\(\\phi\\colon X\\to Y\\ ) is onto , then it is called a factor code or factor map , and \\(Y\\ ) is a factor of \\(X\\ .\\ )If \\(\\phi\\ ) is one - to - one , then it is called an embedding of \\(X\\ ) into \\(Y\\ .", "label": "", "metadata": {}, "score": "76.12752"}
{"text": "According to the Kinetic Theory of Matter , temperature is nothing but a measure of the kinetic energy of matter .My textbook says that the change in internal energy of a system is the heat gained plus ... .I am looking into some new physics and had the following question come up : You have a neutral gas of let 's say , CO atoms at 1 nanoTorr .", "label": "", "metadata": {}, "score": "76.18247"}
{"text": "You can see some conventions we would like to follow .You can also see some convenient formating features in dummy page , and get up to speed on basic Wiki usage on the Wiki quickstart guide .If you have any suggestions , please add them in the suggestion page .", "label": "", "metadata": {}, "score": "76.93526"}
{"text": "\\ )These problems , in particular for SFT 's and sofic shifts ( introduced in the next section ) , have been central to symbolic dynamics .Shifts of finite type and sofic shifts .A shift of finite type ( SFT ) is a shift space that can be described by a finite list of forbidden blocks .", "label": "", "metadata": {}, "score": "77.145706"}
{"text": "Math.,77 ( 1973 ) , 462 - -474 .R. F. Williams .Classification of subshifts of finite type .Annals of Math .98 ( 1973 ) , 120 - -153 ; erratum , Annals of Math .S. Williams .", "label": "", "metadata": {}, "score": "77.47237"}
{"text": "o .g .I . ][ .I . ] 0 . ]otherwise .The existence of .[ . .]I . [ .[ .I . ] 0 . ] I . ] 0 . ]", "label": "", "metadata": {}, "score": "78.04842"}
{"text": "[ .I . ][ .I . ] 1 . ] I . ] I . ] 1 . ] I . ] I . ] 1 . ] I . ] I . ] 1 . ] I . ]", "label": "", "metadata": {}, "score": "78.923546"}
{"text": "It is often easier to work with presentations which are right resolving , meaning that at any given state , all outgoing edges have distinct labels ( as inFigure 4 ) .In fact , every sofic shift can be presented in such a way , and every irreducible sofic shift has a unique minimal right resolving presentation .", "label": "", "metadata": {}, "score": "79.2798"}
{"text": "K. Petersen .Ergodic Theory .Cambridge Univ .Press , 1989 .R. M. Robinson .Undecidability and non - periodicity for tilings of the plane .Inventiones Math . , 12 ( 1971 ) , 177 - -209 .E.A. Robinson .", "label": "", "metadata": {}, "score": "79.82331"}
{"text": "Thank you .Is there a hay to measure mutual information in this framework ? -Crist\u00f3bal Guzm\u00e1n Apr 25 ' 14 at 21:53 .Yes - one can define the \" mutual dimension \" as $ dim(X)+dim(Y)-dim(X\\times Y)$. Considerations like this have been used in the theory of fractals .", "label": "", "metadata": {}, "score": "79.88619"}
{"text": "I 'm teaching undergrad thermo this semester and to my surprise several students are having trouble conceptualizing heat capacity and thermal conductivity as different properties ; they can apply them ... .The question Why does the air pressure at the surface of the earth exactly equal the weight of the entire air column above it asks why the air pressure at any elevation is equal to the weight of the ... .", "label": "", "metadata": {}, "score": "79.93901"}
{"text": "$ -\\int f\\log f dx$ .You want to prove among all real random variables with finite Shannon entropy and variance equal to $ 1 $ , the Shannon entropy is maximized only for Gaussians .Given two probability densities $ f$ and $ g$ , since $ \\log$ is a concave function , Jensen 's inequality tells us that .", "label": "", "metadata": {}, "score": "79.94292"}
{"text": "Another prominent example is : .Morse shift . complement .The shift space whose allowed blocks are those blocks contained . in some \\(A_n\\ ) is called the Morse shift .non - overlapping blocks of length \\(N\\ : \\ ) .", "label": "", "metadata": {}, "score": "79.98891"}
{"text": "Clearly , like the CRE , the GCRE is a positive and concave function of .In addition , existence of GCRE can be established without further assumption upon distribution than those assumed for the CRE in [ 4 ] .Theorem 1 . if for some .", "label": "", "metadata": {}, "score": "80.52176"}
{"text": "A version of the pumping lemma from automata theory shows that the prime shift is not sofic ( Aho , Hopcroft , and Ullman [ 1974 ] ) .And it is not hard to see that the Morse shift is not sofic .", "label": "", "metadata": {}, "score": "81.30824"}
{"text": "How does your proof work ?I guess I 'm missing something simple , sorry for not seeing it .Could you retry to explain it ?Thanks . -Drazick Jan 14 ' 10 at 19:27 .The derivative of $ f$ simply never enters the picture ; $ f$ does n't even need to be continuous .", "label": "", "metadata": {}, "score": "81.715256"}
{"text": "Neither the prime shift nor the Morse shift is an SFT .Any \\(M\\)-step SFT \\(X\\ ) is conjugate to a 1-step SFT .\\ )It is not hard to see that the induced sliding block code is a one - to - one factor map , and that its image is a 1-step SFT .", "label": "", "metadata": {}, "score": "83.41376"}
{"text": "Philip .----------------------------------------------------------------Philip Resnik , Assistant Professor Department of Linguistics and Institute for Advanced Computer Studies .Symbolic Dynamics is the study of shift spaces , which consist of infinite or bi - infinite sequences defined by a shift - invariant constraint on the finite - length sub - words .", "label": "", "metadata": {}, "score": "84.83876"}
{"text": "golden mean shift .\\(X\\ ) is the set of all binary sequences with no two consecutive 1 's .\\ ) .even shift .\\(X\\ ) is the set of all binary sequences so that between any two 1 's there are an even number of 0 's .", "label": "", "metadata": {}, "score": "85.28459"}
{"text": "For a sofic shift , the zeta function turns out to be a rational function , i.e. , quotient of two polynomials .This can be shown by analyzing properties of a right resolving presentation of the sofic shift .\\ )", "label": "", "metadata": {}, "score": "85.59484"}
{"text": "x_0x_1\\dots \\in X\\ , \\ ) a shift space over \\(\\mathcal A \\ .y_0y_1\\dots\\ ) over another alphabet \\(\\mathcal C\\ ) as follows .Fix integers \\(m\\ ) and \\(n\\ ) with \\(-m\\le n\\ .\\ ] .\\ ) .", "label": "", "metadata": {}, "score": "85.761894"}
{"text": "g .l .o .g .l .o .g .l .o .g .l .o .g .Let us define . by .l .o .g .l .o .", "label": "", "metadata": {}, "score": "86.317184"}
{"text": "-Neil G Jul 10 ' 12 at 8:06 .You can also search backwards in Google scholar for articles with different aliases published up to a certain year ( e.g. , cross - entropy up to 1980 ) .-Itamar Jul 10 ' 12 at 8:37 .", "label": "", "metadata": {}, "score": "86.491776"}
{"text": "Symbolic Dynamics and its Applications , ed .S. Williams , Proc .Symp .Appl .Math .S. Williams ( editor ) .Symbolic Dynamics and its Applications .Proc .Symp .Appl .Math .B. Kitchens .", "label": "", "metadata": {}, "score": "86.49959"}
{"text": "If cost provides 1 bit of information about taste and height provides 2 bits of information about weight , then it makes sense that cost+weight should provide 3 bits of information about taste+height .Assuming , of course , that squirrels ( and their weights ) do not influence wines ( and their prices ) .", "label": "", "metadata": {}, "score": "87.57761"}
{"text": "The shift map and its inverse are the simplest examples of sliding block codes .Below are some other simple examples .2 to 1 . \\ )The induced sliding block code \\(\\phi\\ ) is a two - to - one map from \\(X\\ ) to itself . golden to even . trivial .", "label": "", "metadata": {}, "score": "90.11869"}
{"text": "b. A scheme that does transmit information error free .A sufficiently small subset of the messages are sent so that there is only one message inside each circles .Decoding can now be done perfectly - the red dot on the right can now be decoded only as the red dot on the left .", "label": "", "metadata": {}, "score": "90.89714"}
{"text": "Did n't get how is should work for not smooth functions . -Drazick Jan 12 ' 10 at 8:56 .Nothing about this argument requires $ f$ or $ g$ to be smooth .- Mark Meckes Jan 13 ' 10 at 16:10 .", "label": "", "metadata": {}, "score": "93.39322"}
{"text": "Here is a review with pointers to the literature .Thank you for the references .I will wait for answers about Information Theory and the $ O(n)$ case .- Crist\u00f3bal Guzm\u00e1n Apr 18 ' 14 at 21:24 .I think Berg 's result is very far from what the OP was asking .", "label": "", "metadata": {}, "score": "94.96715"}
{"text": "Lab - STICC ( CNRS FRE 3167 ) , Institut Telecom , Telecom Bretagne , Technopole Brest Iroise , CS 83818 , 29238 Brest C\u00e9dex , France .Received 6 April 2008 ; Accepted 19 June 2008 .Copyright \u00a9 2008 Noomane Drissi et al .", "label": "", "metadata": {}, "score": "99.18617"}
{"text": "The thomas - cover book is not enough here .", "label": "", "metadata": {}, "score": "107.852936"}
{"text": "Apr 19 ' 14 at 2:52 .@AnthonyQuas --- I 've added the information - theoretic perspective , hopefully approaching more closely what the OP is looking for .-Carlo Beenakker Apr 19 ' 14 at 5:04 .Thank you , Carlo .", "label": "", "metadata": {}, "score": "109.016556"}
{"text": "Incorrect matches are color - coded in green .For the likely codeword , only about 10 % of the symbols are green , whereas for the unlikely codeword , about 50 % are green .There are two points to this example .", "label": "", "metadata": {}, "score": "110.84838"}
