{"text": "Earlier studies have investigated a number of the issues discussed here in the context of constructing better classifiers .A discussion of some of the issues involved can be found in [ 43 ] .Here , we examined these issues in the context of word sense disambiguation .", "label": "", "metadata": {}, "score": "32.58801"}
{"text": "Table 2 indicates that the result of TSA with WordNet+ODP achieves the best performance to disambiguate words .The performances obtained for nouns are sensibly higher than the one obtained for verbs , confirming the claim that topical describing information is crucial to determine the unique sense of ambiguous term .", "label": "", "metadata": {}, "score": "32.82431"}
{"text": "decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .This experiment involved the following steps : . # deriving a lexicon from the WordNet data files which contains all possible semantic tags for each noun , adjective , adverb and verb .", "label": "", "metadata": {}, "score": "34.981556"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "35.642048"}
{"text": "Different researchers have made use of different sets of features , for example local collocates such as first noun to the left and right , second word to the left / right and so on .However , a more common feature set is to take all the words in a window of words around the ambiguous words , treating the context as an unordered bag of words .", "label": "", "metadata": {}, "score": "35.642048"}
{"text": "Select only the instances where inst.attachment is N : .Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns .For example , given the pair of nouns \" team \" and \" researchers , \" the classifier should predict the preposition \" of \" .", "label": "", "metadata": {}, "score": "36.620827"}
{"text": "In [ 1 ] , Stevenson et al .use supervised learners with linguistic features extracted from the context of the word in combination with MeSH terms for disambiguation .The UMLS has been used , by Humphrey et al . , as a knowledge source for assigning the correct sense for a given word [ 13 ] .", "label": "", "metadata": {}, "score": "37.6709"}
{"text": "These approaches can be neither properly classified as knowledge or corpus based but use part of both approaches .A good example of this is Luk 's system [ 12 ] which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "37.908974"}
{"text": "The sentence frames state that different senses of verbs may occur with an infinitive , with a transitive , and with an intransitive syntactic frame .The syntactic structure information provided by WordNet is rather scarce and not enough to implement the task of verb disambiguation .", "label": "", "metadata": {}, "score": "38.14584"}
{"text": "Table 1 : The performance of the topic identification based on extracting topic discriminative terms .The Performance of Word Sense Disambiguation .We compare our WSD approach based on topical and semantic association ( TSA ) using WordNet + ODP with other state - of - the - art WSD approaches , namely , the ExtLesk algorithm and the SSI algorithm .", "label": "", "metadata": {}, "score": "38.594353"}
{"text": "( 1996 ) which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .A similarity matrix is thus formed which is subject to cluster analysis to determine groups of semantically related instances of terms .", "label": "", "metadata": {}, "score": "38.614075"}
{"text": "For a given document , supposing that its topic category is accurately discriminated , the correct sense of the ambiguous term is identified through the corresponding topic and semantic contexts .We firstly extract topic discriminative terms from document and construct topical graph based on topic span intervals to implement topic identification .", "label": "", "metadata": {}, "score": "38.935387"}
{"text": "View Article .Pedersen T , Bruce R : Distinguishing word senses in untagged text .Second Conference on Empirical Methods in Natural Language Processing .Hsu G , Lin C : A comparison of methods for multi - class support vector machines .", "label": "", "metadata": {}, "score": "38.97191"}
{"text": "In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has .Now that we 've defined our feature extractor , we can use it to train a new \" decision tree \" classifier ( to be discussed in 4 ): .", "label": "", "metadata": {}, "score": "39.040024"}
{"text": "Related Work .Word sense disambiguation is the ability to identify the words ' sense in a computational manner [ 1 ] .We can broadly overview two main approaches to WSD , namely , machine learning and external knowledge sources .", "label": "", "metadata": {}, "score": "39.305504"}
{"text": "Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment .Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) .", "label": "", "metadata": {}, "score": "39.51918"}
{"text": "However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result .( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall .", "label": "", "metadata": {}, "score": "39.691093"}
{"text": "This figure shows the plots of ' ' error rate ' ' versus ' ' sample size ' ' with different sense distributions of PCA data set ( case where the 2 ambiguous senses are very similar ) using 5-fold cross validation .", "label": "", "metadata": {}, "score": "41.076023"}
{"text": "Abend et al .[16 ] introduce a novel supervised learning model for mapping verb instances to VN classes , using rich syntactic features and class membership constraints .The above two methods are based on supervised learning methods with rich features based on part - of - speech tags , word stems , surrounding and cooccurring words , and dependency relationships .", "label": "", "metadata": {}, "score": "41.11561"}
{"text": "We obtain a large improvement when adopting the WSD algorithm based on topical - semantic association graph .Acknowledgments .References .Z.-Y. Niu , D.-H. Ji , and C. L. Tan , \" Learning model order from labeled and unlabeled data for partially supervised classification , with application to word sense disambiguation , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "41.35646"}
{"text": "The Grammar of Sense : using part - of - speech tags as a first step in semantic disambiguation .To appear in Journal of Natural Language Engineering , 4(3 ) .Word - sense disambiguation using statistical models of Roget 's categories trained on large corpora .", "label": "", "metadata": {}, "score": "41.538322"}
{"text": "There are many probability distributions that we could choose for the ten senses , such as : .Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other .", "label": "", "metadata": {}, "score": "41.617012"}
{"text": "We construct a feature vector .for the instance .the same way as in the learning step .The induced learning model ( classifier ) from the learning step will be employed to classify it ( assign .to one of the two senses .", "label": "", "metadata": {}, "score": "41.633514"}
{"text": "What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .An example of this is the dynamic matching technique [ 10 ] which examines all instances of a given term in a corpus and compares the contexts in which they occur for common words and syntactic patterns .", "label": "", "metadata": {}, "score": "41.874535"}
{"text": "Several papers [ 29 , 30 ] realized this issue and reported results for the baseline .More specifically , they excluded samples with a majority sense larger than a threshold because they realized the contribution of the classifier would not be much for those cases .", "label": "", "metadata": {}, "score": "41.899048"}
{"text": "Many researchers proposed use of population - based approaches to generate diverse set of classifiers .Although some promising results have been achieved by current AI - based ensembles to IDSs , there are still challenges that lie ahead for researchers in this area .", "label": "", "metadata": {}, "score": "42.171852"}
{"text": "Therefore , this type of method may not be applicable and ML approaches may be useful .Recently , Humphrey [ 26 ] proposed another type of statistical - based method to resolve the ambiguity problem within the UMLS Metathesaurus .They used a Journal Descriptor Indexing ( JDI ) method , which is ultimately based on statistical associations between words in a training set of MEDLNE citations and a small set of journal descriptors assumed to be inherited by the citations .", "label": "", "metadata": {}, "score": "42.359707"}
{"text": "Firstly , compared with the syntactic structure of the target verb , the partial of senses may be filtered through sentence frames .Secondly , we calculate the similarity between the domain terms in the form of .and topic chains of the noun 's object to choose the verb sense of maximum similarity .", "label": "", "metadata": {}, "score": "42.4014"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "42.501965"}
{"text": "These clusters were then mapped onto the closest sense from the appropriate lexicon .Unfortunately the results are not very encouraging , Pedersen reports 65 - 66 % correct disambiguation depending on the learning algorithm used .This result should be compared against that fact that , in the corpus he used , 73 % of the instances could be correctly classified by simply choosing the most frequent sense .", "label": "", "metadata": {}, "score": "42.501965"}
{"text": "Each context word .or with .or combination and in any distribution .We want to determine that , if we see a context word . suggests that this example belongs to .or to .Thus , we use as features those context words .", "label": "", "metadata": {}, "score": "42.519966"}
{"text": "This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of BPD data set ( where there are 3 ambiguous senses that are different ) using 5-fold cross validation and \" one - vs - rest \" algorithm .", "label": "", "metadata": {}, "score": "42.71707"}
{"text": "Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % .It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : .", "label": "", "metadata": {}, "score": "42.99446"}
{"text": "In the following example , assume that the target word .has 10 instances already labeled with one of two senses as shown in Table 1 .Class . are the instances of .with the first sense , while .are the instances of . instances in the second sense .", "label": "", "metadata": {}, "score": "42.997215"}
{"text": "How do you think that your results might be different if you used a different feature extractor ?What features are relevant in this distinction ?Build a classifier that predicts when each word should be used .However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others .", "label": "", "metadata": {}, "score": "43.092857"}
{"text": "Secondly , the other vertices are iteratively integrated into the different topical clusters according to the adjacency relationship and the previous calculation result of similarity for topic chains .The isolated individuals and too small topical clusters will be ignored .Finally , owing to the fact that the conventional document tends to contain a relatively small number of topics , we focus on those higher density components and choose top - level cooccurrence topic concepts as document 's topic category describing information .", "label": "", "metadata": {}, "score": "43.23329"}
{"text": "An additional issue is that this study limited the disambiguation of gene symbols to gene senses and one other category called \" non - gene sense \" , but the actual sense in this category was not resolved .This could be critical for NLP systems accessing phenotypic or disease - related information .", "label": "", "metadata": {}, "score": "43.247753"}
{"text": "The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used .Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another .", "label": "", "metadata": {}, "score": "43.259766"}
{"text": "In general , simple classifiers always treat each input as independent from all other inputs .In many contexts , this makes perfect sense .For example , decisions about whether names tend to be male or female can be made on a case - by - case basis .", "label": "", "metadata": {}, "score": "43.337646"}
{"text": "and the class label based on the values . through . as defined above .We utilized the training corpus of the labeled instances of the word to be disambiguated to compile the list of all context words ( . as explained above ; all instances of one sense are under one class label .", "label": "", "metadata": {}, "score": "43.3844"}
{"text": "This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .This training can be carried out on either a disambiguated or raw corpus , where a disambiguated corpus is one where the semantics of each polysemous lexical item is marked and a raw corpus one without such marking .", "label": "", "metadata": {}, "score": "43.5561"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "43.59017"}
{"text": "This set of techniques requires a training corpus which has already been disambiguated .In general a machine learning algorithm of some kind is applied to certain features extracted from the corpus and used to form a representation of each of the senses .", "label": "", "metadata": {}, "score": "43.59017"}
{"text": "We subsequently based our assessment of performance on error rates and associated standard errors .Our method also differs from related work because the sample size for each sense is always fixed , whereas in related work the sample size for the entire corpus is generally fixed but not the sample sizes of the senses .", "label": "", "metadata": {}, "score": "43.641335"}
{"text": "Determining the Unique Sense through Choosing the Maximal Similarity .The reason for this is that the task of disambiguating the nouns and noun phrases form are easy to implement through calculating the similarity of topic and semantic ; nevertheless , the verb form is not suitable for directly calculating similarity . is mainly a process of topic and semantic context comparison between a target term and other adjoining ones .", "label": "", "metadata": {}, "score": "43.669266"}
{"text": "Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet Agirre , E. and Rigau , G. ( 1996 ) .", "label": "", "metadata": {}, "score": "43.71871"}
{"text": "Finally , given a noun object , we can attain other synonyms verbs and retrieve the list of objects .If the result exists in the match content with object , then we choose the sense for the target verb .Experimental Result .", "label": "", "metadata": {}, "score": "43.800476"}
{"text": "Our system requires that each term should have instances in two or more species with at least 3 occurrences in each species .The results of Wang et al . are shown in Table 7 , whereas the results of our proposed system are shown in Table 8 in terms of precision , recall , and F1 .", "label": "", "metadata": {}, "score": "43.908333"}
{"text": "Meanwhile , a certain sense of the ambiguous target is associated with a particular topic , so that multiple senses can be distinguished through topical information .Consequently , we propose a topical - semantic association model that exploits the local feature and global feature in the context of ambiguous term to determine its unique sense .", "label": "", "metadata": {}, "score": "44.015087"}
{"text": "They reported an F - measure of over 0.7 for genes with sufficient number of known document references .Liu [ 29 ] investigated the effect of window size and claimed that biomedical ambiguous words needed a larger window size than general English ambiguous words .", "label": "", "metadata": {}, "score": "44.05507"}
{"text": "Table 2 : The performance of disambiguating through TSA versus other state - of - the art algorithms .Conclusions .In this paper , we propose a novel approach for word sense disambiguation based on topical and semantic association .Our experiments show that the topic categories of Open Directory Project merged into WordNet are of high quality and , more importantly , it enables external knowledge - based WSD applications to perform better than the existing methods of only using WordNet .", "label": "", "metadata": {}, "score": "44.192413"}
{"text": "The main classes of approaches of word sense disambiguation include supervised methods and unsupervised methods .The supervised methods rely on training and learning phases that require a dataset or corpus containing manually disambiguated instances to be used to train the system [ 5 , 6 ] .", "label": "", "metadata": {}, "score": "44.256516"}
{"text": "[ 1 ] The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning of a word in a particular usage can only be determined by examining its context .This is , in general , a trivial task for the human language processing system , for example consider the following two sentences , each with a different sense of the word bank : .", "label": "", "metadata": {}, "score": "44.506927"}
{"text": "Our approach in this paper is a supervised approach .In this paper , we present and evaluate a supervised method for biomedical word sense disambiguation .The method is based on machine learning and uses some feature selection techniques in constructing feature vectors for the words to be disambiguated .", "label": "", "metadata": {}, "score": "44.53653"}
{"text": "We can treat RTE as a classification task , in which we try to predict the True / False label for each pair .Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level .", "label": "", "metadata": {}, "score": "44.871956"}
{"text": "These topic span intervals will be used to determine the appropriate size of context for the ambiguous term .To achieve the whole process of leveraging topic discriminative term for topic identification , we will design the Algorithm 1 .Determining the Unique Sense of Ambiguous Term Using Topical - Semantic Association Graph .", "label": "", "metadata": {}, "score": "44.87904"}
{"text": "In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns .For example , we might introduce the abstract concept of \" polar verb \" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar .", "label": "", "metadata": {}, "score": "44.883163"}
{"text": "When the words are monosemous , semantic feature is the best results ( 91.0 % ) ; in contrast , positional + statistical feature and topic span distribution feature are better than semantic feature ( 80.8 % and 83.1 % ) .", "label": "", "metadata": {}, "score": "44.88385"}
{"text": "The topical features are based on the topical describing information from the above - mentioned preprocessing steps of topic identification .The representation of context problem can be formally stated as follows .( i ) .For the syntactic feature , each sentence is analyzed for the parse tree .", "label": "", "metadata": {}, "score": "44.953873"}
{"text": "Although it can be possible to gain insight by studying them , it typically takes a lot more work .But all explicit models can make predictions about new \" unseen \" language data that was not included in the corpus used to build the model .", "label": "", "metadata": {}, "score": "44.97749"}
{"text": "Continuing on , the classifier checks if the word ends in \" s \" .1.5 Exploiting Context .By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix .", "label": "", "metadata": {}, "score": "45.04323"}
{"text": "The goal of this chapter is to answer the following questions : .How can we identify particular features of language data that are salient for classifying it ?How can we construct models of language that can be used to perform language processing tasks automatically ?", "label": "", "metadata": {}, "score": "45.100166"}
{"text": "Consequently , in order to achieve disambiguation task , there are several challenges , as follows : . combining topic chain and disambiguation context into topic semantic profile for identifying topic discriminative term and constructing topical graph based on the topic span intervals of topic discriminative term to implement the document 's topic identification , . determining the unique sense of ambiguous term using topical - semantic association graph , paying more attention to exploiting syntactic features , semantic features , and topical features to implement verb and noun disambiguation .", "label": "", "metadata": {}, "score": "45.131653"}
{"text": "Consider the problem of classifying documents by their content , for example into spam and non - spam e - mails .Imagine that documents are drawn from a number of classes of documents which can be modelled as sets of words where the ( independent ) probability that the i - th word of a given document occurs in a document from class C can be written as .", "label": "", "metadata": {}, "score": "45.244507"}
{"text": "Our study is different because it quantified the effect of similarity of senses , and studied the relation between \" similarity of senses \" and other issues such as \" sample size \" and \" sense distribution \" .Podowski 's [ 28 ] work covered task types 2 and 3 , while Hatzivassiloglou 's [ 10 ] work addressed task type 4 .", "label": "", "metadata": {}, "score": "45.266357"}
{"text": "Words having no semantic tags ( determiners , prepositions , auxiliary verbs , etc . ) are ignored . constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .", "label": "", "metadata": {}, "score": "45.284157"}
{"text": "Of course , we do n't usually build naive Bayes classifiers that contain two identical features .However , we do build classifiers that contain features which are dependent on one another .For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature .", "label": "", "metadata": {}, "score": "45.40873"}
{"text": "The hierarchical structure is the common characteristics in knowledge representation , such as the hypernym / hyponym relations in WordNet or the topic coverage in ODP .We utilize the hierarchical structure features for measuring the semantic similarity , that is , node depth and node distance .", "label": "", "metadata": {}, "score": "45.43405"}
{"text": "This agrees with work by Rifkin and Klatau [ 34 ] .A description of the different multi - class algorithms is provided in the Methods section .Table 5 .Results for BPD data set .Annotation of the table : Dist : Distribution of senses ; S. Size : sample size ; Err .", "label": "", "metadata": {}, "score": "45.46564"}
{"text": "To appear in Journal of Natural Language Engineering , 4(3 ) .use large lexicons ( generally machine readable dictionaries ) and the information associated with the senses ( such as part - of - speech tags , topical guides and selectional preferences ) to indicate the correct sense .", "label": "", "metadata": {}, "score": "45.512703"}
{"text": "In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships .Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another .", "label": "", "metadata": {}, "score": "45.51574"}
{"text": "In their study , rare senses ( senses appearing in less than 40 documents ) were excluded from the testing set .This makes the disambiguation task easier because it reduces the problem of sparse senses .In addition , the training set was created based on long - form and short - form pairs , where ambiguous words not having long - forms were not tested .", "label": "", "metadata": {}, "score": "45.6651"}
{"text": "The results presented here agree with general results presented in the literature on the performance of classifiers [ 43 - 45 ] .Future work .To further analyze the effects of \" sample size \" , \" sense distribution \" and \" degree of difficulty \" on the error rate , an error decomposition model will be explored .", "label": "", "metadata": {}, "score": "45.744892"}
{"text": "Therefore it is important that we understand the different elements affecting their performance .Methods .After manually reviewing a set of WSD papers in the biomedical domain , different issues associated with performance were enumerated .For an initial study , we conducted experiments to evaluate the effect of three confounding issues : \" sample size \" , \" sense distribution \" and \" degree of difficulty \" , and we used an automatically generated data set .", "label": "", "metadata": {}, "score": "45.76358"}
{"text": "Subcognitive questions are designed to probe the network of cultural and perceptual associations that we naturally develop in the course of our daily lives .The paper presents a corpus - based approach to answering subcognitive questions .An algorithm for classifying the semantic relations in noun - modifiers , based on distributional information .", "label": "", "metadata": {}, "score": "45.86595"}
{"text": "The node pair with the shorter distance between has the greater similarity than that of the pair with the longer distance between them .Assume .The Topic Identification Algorithm .A topic similarity set of topic discriminative terms which occur in the text fragment will share the identical topic and similar semantic context .", "label": "", "metadata": {}, "score": "45.87267"}
{"text": "Another example is the work of Pedersen [ 11 ] who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "45.92706"}
{"text": "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions .An example will help to illustrate this principle .Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) .", "label": "", "metadata": {}, "score": "45.93843"}
{"text": "Approaches .Knowledge based .Under this approach disambiguation is carried out using information from an explicit lexicon or knowledge base .The lexicon may be a machine readable dictionary , thesaurus or it may be hand - crafted .This is one of most popular approaches to word sense disambiguation and amongst others , work has been done using existing lexical knowledge sources such as WordNet [ 5 ] and LDOCE [ 6 ] .", "label": "", "metadata": {}, "score": "45.99347"}
{"text": "These heterogeneous features can not be used all together to train a single classifier ( and even if they could - by converting all features into a vector of scalar values - such a training is unlikely to be successful ) .", "label": "", "metadata": {}, "score": "46.03112"}
{"text": "But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word .Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) .", "label": "", "metadata": {}, "score": "46.051666"}
{"text": "The names classifier that we have built generates about 100 errors on the dev - test corpus : .Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders .", "label": "", "metadata": {}, "score": "46.161407"}
{"text": "Most of the above papers reporting on the use of ML for WSD follow a similar pattern .A set of ambiguous words is selected , a corpus for each word is collected , and the different senses within the corpus are annotated ( automatically or manually ) .", "label": "", "metadata": {}, "score": "46.31585"}
{"text": "It 's important to understand what we can learn about language from an automatically constructed model .One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models .Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns .", "label": "", "metadata": {}, "score": "46.38712"}
{"text": "Meanwhile , we also recorded the percentage of ambiguous words that were removed from the ambiguous word - list for different thresholds .We removed words with frequencies higher than 10 % , 1 % , 0.1 % and 0.05 % from the two lists of the mouse organism .", "label": "", "metadata": {}, "score": "46.419853"}
{"text": "1.6 Sequence Classification .In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs .In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence .", "label": "", "metadata": {}, "score": "46.443573"}
{"text": "They proposed different concepts to describe the improved performance , reduced generalization error , and successful applications of ensembles to different fields over individual classifier .For example , Allwein et al .[59 ] interpreted the improved performance in the framework of large margin classifiers , Kleinberg in the reference of Stochastic Discrimination theory [ 60 ] , and Breiman in the terms of the bias variance analysis [ 61 ] .", "label": "", "metadata": {}, "score": "46.48873"}
{"text": "Discussions .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "46.56359"}
{"text": "A Method for WSD .A word sense disambiguation method is an algorithm that assigns the most accurate sense to a given word in a given context .Our method is a supervised method requiring a training corpus that contains manually disambiguated instances of the ambiguous words .", "label": "", "metadata": {}, "score": "46.576744"}
{"text": "Generally speaking , these methods have the potential bottleneck and limitation .However , almost all the methods , without exception , depend on the context in which the ambiguous word occurs .Moreover , the context size of the target word is too small to convey enough meaning for being disambiguated at a fine - grained level .", "label": "", "metadata": {}, "score": "46.73935"}
{"text": "Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values .As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions .", "label": "", "metadata": {}, "score": "46.803696"}
{"text": "Given all that , the major difference between our disambiguation strategy and these existing approaches is that we focus on term - concept association and concept - topic association , moreover , in the way of determining the appropriate size of disambiguation context .", "label": "", "metadata": {}, "score": "46.89215"}
{"text": "In addition , the approach of domain - oriented disambiguation [ 12 ] is similar to our idea .The hypothesis of this approach is that the knowledge of a topic or domain can help disambiguate words in a particular domain text [ 1 ] .", "label": "", "metadata": {}, "score": "46.90419"}
{"text": "If the topic concept occurs , then the branch of the corresponding topic chain is determined for the unique sense .Otherwise , the sense is fixed through choosing the semantic branch of the maximum similarity .The formula ( 7 ) can be defined as follows : . , respectively , denote the topic chain and disambiguation context similarity .", "label": "", "metadata": {}, "score": "46.93361"}
{"text": "Consider .In addition , if there do not exist other TDTs in the conflict interval , the split intervals have a bias for the greater weight of TDT .On the basis of pruned topical graph , the document 's topical describing information is formed through detecting the high - density components and choosing top - level cooccurrence topic concepts of topic chains .", "label": "", "metadata": {}, "score": "47.0897"}
{"text": "If each ambiguous gene symbol in an article were accompanied by its corresponding long form , the disambiguation task would be much easier .Schijvenaars [ 13 ] showed that 33 % of the human genes in their thesaurus were affected by homonymy .", "label": "", "metadata": {}, "score": "47.172253"}
{"text": "\" Sample size ' , \" sense distribution \" and \" degree of difficulty \" were three of multiple confounding issues that affect the performance of a WSD classifier .Results from our experiments demonstrated that these three factors were intrinsically connected .", "label": "", "metadata": {}, "score": "47.19935"}
{"text": "A good example of this is Luk 's system A. Luk .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 . which uses the textual definitions of senses from a machine readable dictionary ( LDOCE ) to identify relations between senses .", "label": "", "metadata": {}, "score": "47.201813"}
{"text": "Some researchers proposed to use a distributed environment in which each node is assigned a part of dataset .An ensemble method was used to fuse or select predictions .Thirdly , an important feature of IDS is the capability to adapt to dynamic behavior of intrusive and normal traffic .", "label": "", "metadata": {}, "score": "47.25866"}
{"text": "Mapping a WordNet Sense to an ODP 's Category Label .We aim to construct a mapping relation from a WordNet sense to an ODP 's category label .Our proposed approach effectively fuses the semantic knowledge with hierarchical topic category to generate topic semantic knowledge profile for expediently handling a series of research hot issues , such as information extraction , topic identification , and word sense disambiguation .", "label": "", "metadata": {}, "score": "47.606026"}
{"text": "We could then use those interactions to adjust the contributions that individual features make .To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : .", "label": "", "metadata": {}, "score": "47.64652"}
{"text": "The paper argues that word sense disambiguation for machine translation should be based on the co - occurrence frequency of the context words near a given target word .Latest revision as of 04:12 , 25 June 2012 .This book contains one of the earliest definitions of the term statistical semantics , as \" statistical study of meanings of words and their frequency and order of recurrence \" .", "label": "", "metadata": {}, "score": "47.666332"}
{"text": "Hence , in the presence of a training set , only the conditional probabilities are computed since the structure is unique .Once , the network is quantified , it is possible to classify any new object giving its attribute values using Baye 's rule .", "label": "", "metadata": {}, "score": "47.66823"}
{"text": "Here , value . 0.5 signifies its non - participation .The set of ensembles provides the classification trade - offs for the user for different objective functions .Phase 3 of the proposed approach integrates the predictions of base classifiers to get a prediction of the final ensemble .", "label": "", "metadata": {}, "score": "47.840813"}
{"text": "In the previous work , [ 17 ] , we introduced a method for term disambiguation and evaluated it with biomedical terms to disambiguate gene and protein names in medical texts .The method relies on representing the instances of the word to be disambiguated , . , as a feature vector , and the components of this vector are neighborhood context words in the training instances .", "label": "", "metadata": {}, "score": "47.86573"}
{"text": "M. Weeber , J. Mork , and A. Aronson , \" Developing a test collection for biomedical word sense disambiguation , \" in Proceedings of the Symposium American Medical Informatics Association ( AMIA ' 01 ) , 2001 .M. F. Porter , \" An algorithm for suffix stripping , \" Program , vol .", "label": "", "metadata": {}, "score": "47.97978"}
{"text": "Selected subsets of feature are used to train accurate and diverse base classifiers .Final ensemble is constructed by using ensemble selection method .They reported improvement of ID in terms of detection rate and false - positive rate over other related approaches .", "label": "", "metadata": {}, "score": "48.054214"}
{"text": "Yarowsky reports that the system correctly classifies senses 96 % of the time .Word Sense Disambiguation has several debates within the field as to whether the senses offered in existing dictionaries are adequate to distinguish the subtle meanings used in text contexts and how to evaluate the overall performance of a WSD system .", "label": "", "metadata": {}, "score": "48.206036"}
{"text": "Many researchers applied and evaluated AI - based techniques using different evaluation datasets for ID .They reported many challenges related to AI - based techniques and dataset for ID .Each technique may have its own region in the feature space where it performs the best [ 26 ] .", "label": "", "metadata": {}, "score": "48.33052"}
{"text": "View Article PubMed .Maglott D , Ostell J , Pruitt KD , Tatusova T : Entrez Gene : Gene - centered information at NCBI .Nucleic Acids Res 2005 , 3 : D54-D58 .Yngve VH : Syntax and the problem of multiple meaning .", "label": "", "metadata": {}, "score": "48.35412"}
{"text": "We evaluate our topical identification algorithm using the precision ( the number of correct documents over the number of all documents ) on SemCor .Table 1 summarizes the performance of extracting the topic discriminative terms based on the selection of different features .", "label": "", "metadata": {}, "score": "48.374306"}
{"text": "In Proceedings of COLING'96 and LDOCE J. Guthrie , L. Guthrie , Y. Wilks and H. Aidinejad , Subject - Dependent Co - Occurrence and Word Sense Disambiguation , ACL-91 , pp .146 - 152 .The information in these resources has been used in several ways , for example Wilks and Stevenson Y. Wilks and M. Stevenson .", "label": "", "metadata": {}, "score": "48.449043"}
{"text": "In order to solve these problems , many researchers utilized AI - based ensembles for ID successfully .They proved that AI - based ensembles can improve detection performance over a single technique / classifier [ 27 - 29 ] .The concept of ensemble is to employ multiple base classifiers and their individual predictions are combined in some way to obtain reliable and more accurate predictions [ 17 , 25 ] .", "label": "", "metadata": {}, "score": "48.46457"}
{"text": "Effects of \" sense distribution \" have been addressed in other papers [ 30 , 37 ] because it is believed that the performance of a WSD classifier may change if the distribution of the different senses is unbalanced .For example , when there is a majority sense for an ambiguous word , the improvement of a WSD classifier is believed to be very small .", "label": "", "metadata": {}, "score": "48.56352"}
{"text": "The same study , which was also performed for the Fly organism , showed similar results , but with slightly higher ambiguity rates .This study shows that the ambiguity among gene symbols , English words and other biomedical terms is extensive and the distribution of ambiguity is very sparse .", "label": "", "metadata": {}, "score": "48.755775"}
{"text": "Diversity among the base classifiers can be measured by implicit or explicit methods [ 47 , 49 ] .In order to evaluate the performance , different performance metrics can be computed based upon benchmarked datasets .Discussion .Over the past decade , ID based upon ensemble approaches has been a widely studied topic , being able to satisfy the growing demand of reliable and intelligent IDS .", "label": "", "metadata": {}, "score": "48.779663"}
{"text": "The applications of SVM are abound ; in particular , in NLP domain like text categorization , relation extraction , named entity recognition , SVM proved to be the best performer .The Disambiguation Step In the testing step , we want to disambiguate an instance .", "label": "", "metadata": {}, "score": "48.797752"}
{"text": "The objective of this method is to offer an alternative classification when base classifiers disagree ( arbiter tree ) or to combine the predictions of base classifiers by learning their relationships with the correct class labels ( combiner trees ) [ 80 , 81 ] .", "label": "", "metadata": {}, "score": "48.806187"}
{"text": "View Article .Engelson SP , Dagan I : Minimizing manual annotation cost in supervised training from corpora .34th Annual Meeting of Association for Computational Linguistics 319 - 326 .Pustejovsky J , Castano J , Cochran B , Kotecki M , Morrell M : Automatic extraction of acronym - meaning pairs from MEDLINE databases .", "label": "", "metadata": {}, "score": "48.848484"}
{"text": "More experiments may be conducted by using different values of these parameters .The proposed approach is validated using small subsets of benchmark datasets only , whereas its applicability can be tested by conducting more experiments with real network traffic in the field of ID .", "label": "", "metadata": {}, "score": "48.87009"}
{"text": "Other reasons for combining different classifiers include [ 26 ] the following .( 1 ) a designer may have access to a number of different classifiers , each developed in a different context and for an entirely different representation / description of the same problem .", "label": "", "metadata": {}, "score": "48.913826"}
{"text": "The classifier will be then used to disambiguate unseen and unlabeled examples in the application phase .One of the main strength of this method is that the features are selected for learning and classification .Feature Selection The features selected from the training examples have great impact on the effectiveness of the machine learning technique .", "label": "", "metadata": {}, "score": "48.93192"}
{"text": "But , availability of irrelevant and redundant feature affects detection performance of classifiers .Homogeneous ensembles focus on different features of training dataset and/or different training subsets and/or other ways to generate diverse base classifiers .Applications of the AI - based ensembles revealed that they have pros and cons .", "label": "", "metadata": {}, "score": "48.99567"}
{"text": "( or mostly in . , then the MI indicates this as shown in ( 2 ) .Thus , MI can be used as a means to estimate the amount of information interaction between a context work and a class label .", "label": "", "metadata": {}, "score": "49.019867"}
{"text": "The Literature Review .Ensemble techniques / classifiers have been recently applied to overcome the limitations of a single classifier system in different fields [ 12 - 14 ] .Such attention is encouraged by the theoretical [ 12 ] and experimental [ 15 ] studies , which illustrate that ensembles can improve the results of traditional single classifiers .", "label": "", "metadata": {}, "score": "49.041718"}
{"text": "Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens .Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' :", "label": "", "metadata": {}, "score": "49.145393"}
{"text": "In this paper , we also demonstrated that ambiguity of biomedical entities is a significant problem , which has a substantial impact on text mining and retrieval tasks in the biomedical domain .ML methods are still needed for WSD , which is critical for increasing the accuracy of biomedical natural language , text mining , and information retrieval systems .", "label": "", "metadata": {}, "score": "49.152878"}
{"text": "These approaches combine complementary multiple classifiers .They use combined knowledge to meet the challenges of ID - like high false alarm rate , low detection accuracy , and better performance in lack of sufficient amount of quality training dataset .The results of ensemble approach are proved to be improved than single best classifier .", "label": "", "metadata": {}, "score": "49.31092"}
{"text": "Other confounding issues of WSD .Other issues in addition to sample size , distribution of senses , and difficulty of the task also affect the performance and subsequent assessment of WSD classifiers , as noted below : .As often discussed in various papers , different features were evaluated to see their contribution to classifier performance [ 10 , 20 , 29 ] .", "label": "", "metadata": {}, "score": "49.33934"}
{"text": "4 , pp .678 - 692 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Navigli and P. Velardi , \" Structural semantic interconnections : a knowledge - based approach to word sense disambiguation , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "49.360214"}
{"text": "Distinguishing word senses in untagged text .In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .who compared three different unsupervised learning algorithms on 13 different words .Each algorithm was trained on text with was tagged with either the WordNet or LDOCE sense for the word but the algorithm had no access to the truce senses .", "label": "", "metadata": {}, "score": "49.4213"}
{"text": "Nor an improvement on the ensemble 's average performance can be guaranteed except for certain special cases ( Fumera 2005 ) .Hence combining classifiers may not necessarily beat the performance of the best classifier in the ensemble , but it certainly reduces the overall risk of making a particularly poor selection .", "label": "", "metadata": {}, "score": "49.469765"}
{"text": "108 - 117 , 2002 .M. S. Kamel and N. M. Wanas , \" Data dependence in combining classifiers , \" in Proceedings of 4th International Workshop on Multiple Classifier Systems ( MCS ' 03 ) , T. Windeattand and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "49.47667"}
{"text": "For simplicity , and without loss of generality , we assume that we have two senses ( two class labels ) .Moreover , following the same intuitive reasoning of mutual information , MI , we define another method , M2 , for selecting the words as features to be included in the feature vectors as follows : .", "label": "", "metadata": {}, "score": "49.568558"}
{"text": "The main variations are usually in the selection of features and choice of machine - learning algorithms .Experiments are usually performed on a fixed amount of documents ( i.e. 1,000 abstracts ) per an ambiguous word , where the entire set consists of all the senses , and the sense distribution is generally uneven .", "label": "", "metadata": {}, "score": "49.588753"}
{"text": "Also words with parentheses or square brackets are not ignored and part of speech is not used .After the text preprocessing is completed , for each word we convert the instances into numeric feature vectors .Then , we use SVM for training and testing with 5-fold cross validation 5FCV such that 80 % of the instances are used for training and the remaining 20 % are used for testing , and this is repeated five times by changing the training - testing portions of the data .", "label": "", "metadata": {}, "score": "49.611107"}
{"text": "The system was tested on data gathered at Google , Inc. and two universities in USA and Europe , showing promising results .However , they used anomaly detection technique ( Bayesian technique ) to model attribute inputs without taking into account typical semantic differences between classes of characters ( alphabetic , numeric , and non - alphanumeric ) , which usually determine their meaning .", "label": "", "metadata": {}, "score": "49.62875"}
{"text": "The data instances including normal as well as attack instances are randomly selected to create a subset of the benchmark dataset for our experiments .The selected dataset is further preprocessed by converting discrete feature values to numeric ones as described in Kumar et al .", "label": "", "metadata": {}, "score": "49.71326"}
{"text": "This article focuses on classification related applications of ensemble learning , however , all principle ideas described below can be easily generalized to function approximation or prediction type problems as well .An ensemble - based system is obtained by combining diverse models ( henceforth classifiers ) .", "label": "", "metadata": {}, "score": "49.730217"}
{"text": "Mooney RJ : Comparative experiments on disambiguating word senses : An illustration of the role of bias in machine learning .Proc 1996 Conf on Empirical Methods in Natural Language Processing 82 - 91 .Ng HT , Lee HB : Integrating multiple knowledge sources to disambiguate word sense : An examplar - based approach .", "label": "", "metadata": {}, "score": "49.864353"}
{"text": "The labeled training instances will be used to extract the word features for the feature vectors .Suppose the word .labeled with sense . or .( i.e. , in the set .or in the set . can be viewed as . and .", "label": "", "metadata": {}, "score": "49.87713"}
{"text": "Many definitions exist to evaluate similarity between an alarm and a meta - alarm .In fact , the distance between an alarm and a meta - alarm is defined in terms of correlation between them , which is in turn defined as an application of a distance function to features characterizing each of the raised alarms .", "label": "", "metadata": {}, "score": "49.91688"}
{"text": "4 , pp .330 - 345 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Navigli and M. Lapata , \" An experimental study of graph connectivity for unsupervised word sense disambiguation , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "49.940056"}
{"text": "For the target verb , we firstly distinguish a sentence which is the sentence frame and identify corresponding object and subject , respectively .For the target noun , we focus on the modifier structure , the parallel structure , and subordinate clause for subject or object .", "label": "", "metadata": {}, "score": "50.01529"}
{"text": "The resulting synergy has been shown to be an effective way for building IDSs with improved performance in terms of detection accuracy and false - positive rate .It may be concluded that by considering appropriate base classifiers , training sample size & combination method , the performance of hybrid classifier / ensemble can be improved .", "label": "", "metadata": {}, "score": "50.024044"}
{"text": "For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities .", "label": "", "metadata": {}, "score": "50.047844"}
{"text": "For document topic identification , we can define a feature for each word , indicating whether the document contains that word .To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus .", "label": "", "metadata": {}, "score": "50.19871"}
{"text": "During the process of our algorithm , we aim to determinate the mapping between WordNet and ODP .Formally , given the sense of a term in WordNet , we acquire a mapping to an ODP 's topic chain as follows : .", "label": "", "metadata": {}, "score": "50.25102"}
{"text": "To understand the effects of increased sample size on the error rate , we stratified by the sense distribution and then tested the null hypothesis of no difference between the error rates obtained under the different sample sizes using the sign test .", "label": "", "metadata": {}, "score": "50.28268"}
{"text": "This is the event model typically used for document classification , with events representing the occurrence of a word in a single document ( see bag of words assumption ) .The likelihood of observing a histogram x is given by .", "label": "", "metadata": {}, "score": "50.297226"}
{"text": "In addition , papers should also characterize the difficulty of the WSD task , the WSD situations addressed and not addressed , as well as the ML methods and features used .This should lead to an improved understanding of the generalizablility and the limitations of the methodology .", "label": "", "metadata": {}, "score": "50.34285"}
{"text": "In this study , we used 4 abbreviations from Liu 's abbreviation list .However , we used a different method to collect the datasets because we wanted to control the sample sizes of the senses for our experiments .Leroy [ 30 ] tried to reduce the training sample size by supplying external knowledge from the UMLS for supervised machine learning algorithms , but the results were not promising .", "label": "", "metadata": {}, "score": "50.38346"}
{"text": "The returned dictionary , known as a feature set , maps from feature names to their values .Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' .", "label": "", "metadata": {}, "score": "50.38794"}
{"text": "Using all of the knowledge sources , the SVM method achieved the highest accuracy rate of 65.4 % .Another type of WSD approach uses established knowledge from curated terminology systems [ 23 , 24 ] .In the biomedical domain , Schijvenaars [ 13 ] developed a simple thesaurus - based algorithm to disambiguate human gene symbols using training data from PubMed abstracts and annotations from the Online Mendelian Inheritance in Man(OMIM )", "label": "", "metadata": {}, "score": "50.575626"}
{"text": "The diversity among base classifiers is maintained implicitly .The detection rate for each class is treated as a separate objective .Here , the multiobjective GA is real coded and uses crossover and mutation operators and an elitist replacement strategy .", "label": "", "metadata": {}, "score": "50.588524"}
{"text": "Ginter [ 27 ] introduced a new family of classifiers , which were based on an ordering and weighing of the feature vectors obtained from word counts and word co - occurrence in the text .This method was used to determine whether a term was a gene versus a protein and achieved 86 % accuracy .", "label": "", "metadata": {}, "score": "50.603508"}
{"text": "In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters .In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label .", "label": "", "metadata": {}, "score": "50.625656"}
{"text": "For example , consider the part - of - speech tagging task .At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : .", "label": "", "metadata": {}, "score": "50.700844"}
{"text": "It should be noted that unsupervised disambiguation can not actually label specific terms as a referring to a specific concept : that would require more information than is available .What unsupervised disambiguation can achieve is word sense discrimination , it clusters the instances of a word into distinct categories without giving those categories labels from a lexicon ( such as WordNet synsets ) .", "label": "", "metadata": {}, "score": "50.72863"}
{"text": "The proposed approach can discover an optimized set of features that can be further used to train NB classifiers and ensemble of NBs thereof with a good support and a detection rate from benchmark datasets ( in comparison with well - known ensemble methods like bagging and boosting ) .", "label": "", "metadata": {}, "score": "50.770767"}
{"text": "Therefore , individual classifiers in an ensemble system need to make different errors on different instances .The intuition , then , is that if each classifier makes different errors , then a strategic combination of these classifiers can reduce the total error , a concept not too dissimilar to low pass filtering of the noise .", "label": "", "metadata": {}, "score": "50.830967"}
{"text": "Let 's begin by finding out what the most common suffixes are : .Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features .Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties .", "label": "", "metadata": {}, "score": "50.98257"}
{"text": "The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features .For this example , we 'll start by just looking at the final letter of a given name .", "label": "", "metadata": {}, "score": "51.043755"}
{"text": "The classifiers are generated by using training on KDD99 dataset .They observed in the experiments that different models provided complementary information about the patterns to be classified .The final prediction of ensemble is computed based upon highest score of base classifiers .", "label": "", "metadata": {}, "score": "51.05847"}
{"text": "Use of multiple classifiers is supported by the statement that if one classifier fails to detect an attack , then another should detect it [ 20 ] .However , to create an efficient ensemble , we are still facing numerous difficulties : how can we generate diverse base classifiers ?", "label": "", "metadata": {}, "score": "51.219105"}
{"text": "113 - 141 , 2001 .View at Google Scholar \u00b7 View at Scopus . A. Sharkey , \" Types of multi - ney systems , \" in Multiple Classifier Systems , Third International Workshop ( MCS ' 02 ) , F. Roli and J. Kittler , Eds . , vol .", "label": "", "metadata": {}, "score": "51.283752"}
{"text": "we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word .However , this regular expression tagger had to be hand - crafted .", "label": "", "metadata": {}, "score": "51.333115"}
{"text": "[28 ] proposed a hybrid approach to detect intrusions .They utilized Bayesian networks ( BNs ) and classification and regression trees ( CARTs ) and their ensemble to generate hybrid system .They empirically proved that CART performed best for Normal , Probe , and U2R and the ensemble approach worked best for R2L and DoS. The heterogeneous ensemble was generated by training the individual classifiers from reduced KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "51.403194"}
{"text": "No assumptions are made about the original distribution ( e.g. normal vs. other ) of the documents .Analysis of variance models are versatile statistical tools for studying the relation between error rates and sense distribution , sample size , and degree of difficulty of a task .", "label": "", "metadata": {}, "score": "51.43569"}
{"text": "Many researchers proposed fuzzy set theory to combine base classifiers using fuzzy aggregation connectives to determine ensemble prediction [ 76 , 77 ] .Fuzzy combination methods are effective as they measure the strength of every subset of classifiers .Thus to determine the class of any unclassified instance is the decision of ensemble which is based upon competence of every subset of based classifiers [ 50 ] .", "label": "", "metadata": {}, "score": "51.439995"}
{"text": "Note .Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings .But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute .", "label": "", "metadata": {}, "score": "51.526802"}
{"text": "The proposed approach can discover individual solutions and ensemble solutions thereof with a good support and a detection rate from benchmark datasets ( in comparison with well - known ensemble methods like bagging and boosting ) .In addition , the proposed approach is a generalized classification approach that is applicable to the problem of any field having multiple conflicting objectives , and a dataset can be represented in the form of labelled instances in terms of its features .", "label": "", "metadata": {}, "score": "51.584793"}
{"text": "If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing .", "label": "", "metadata": {}, "score": "51.690475"}
{"text": "Such a set of classifiers is said to be diverse .Classifier diversity can be achieved in several ways .Preferably , the classifier outputs should be class - conditionally independent , or better yet negatively correlated .The most popular method is to use different training datasets to train individual classifiers .", "label": "", "metadata": {}, "score": "51.694412"}
{"text": "The full - form in the title or abstract of the article was then replaced with the ambiguous abbreviation , and the appropriate sense was noted separately .Table 1 shows the number of articles that were obtained for the different abbreviations and senses .", "label": "", "metadata": {}, "score": "51.696465"}
{"text": "The user may adopt static or a dynamic strategy to choose an appropriate ensemble from a pool of ensembles ( evolved in Phase 2 ) .Here in this work , we selected the ensemble classifier using a static strategy based on its performance on the training data in terms of predefined performance metrics .", "label": "", "metadata": {}, "score": "51.702576"}
{"text": "If both classifiers agree then the output is decided accordingly .If there is a conflict then the decision given by the classifier with the highest weight is taken into account .By using hybrid approach , the authors reported that Normal , Probe , and DOS could be detected with 100 % accuracy and U2R and R2L with 84 % and 99.47 % accuracies , respectively .", "label": "", "metadata": {}, "score": "51.8262"}
{"text": "These 10 words will be used to compose the feature vectors for training or testing examples of the terms to be disambiguated .For example , a simple feature vector of size 5 can be as follows : .This feature vector represents an instance that has the first , third , and fourth context words available in its context , and 1.23 is the MI value of the context word with the highest MI .", "label": "", "metadata": {}, "score": "51.83162"}
{"text": "References .M. Stevenson , Y. Guo , R. Gaizauskas , and D. Martinez , \" Knowledge sources for word sense disambiguation of biomedical text , \" in Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing ( BioNLP ' 08 ) , pp .", "label": "", "metadata": {}, "score": "51.898117"}
{"text": "These features include syntactic features , semantic features , and topical features .The syntactic features are based on the preprocessing steps of the input text , such as tokenization , part - of - speech tagging , chunking , and parsing .", "label": "", "metadata": {}, "score": "51.920506"}
{"text": "\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions .Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation .", "label": "", "metadata": {}, "score": "51.922638"}
{"text": "Of course , this assumption is unrealistic ; features are often highly dependent on one another .We 'll return to some of the consequences of this assumption at the end of this section .This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another .", "label": "", "metadata": {}, "score": "52.033894"}
{"text": "As can be seen , all measures of comprehensive features perform better than the each feature .Especially , topic span distribution feature ( 86.2 % ) plays a more important role for improving the accuracy rate of documents ' topic identification .", "label": "", "metadata": {}, "score": "52.04311"}
{"text": "When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made .A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i .", "label": "", "metadata": {}, "score": "52.07137"}
{"text": "These posts have all been labeled with one of 15 dialogue act types , such as \" Statement , \" \" Emotion , \" \" ynQuestion \" , and \" Continuer .\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts .", "label": "", "metadata": {}, "score": "52.097153"}
{"text": "Most of popular ensemble methods proposed and implemented in the literature utilize data level .These methods are used to generate different training sets and a learning algorithm , which can be applied to the obtained subsets of data in order to produce multiple hypotheses .", "label": "", "metadata": {}, "score": "52.118015"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , [ 9 ] decided nonetheless to perform an experiment to see how well words can be semantically disambiguated using techniques that have proven to be effective in part - of - speech tagging .", "label": "", "metadata": {}, "score": "52.15155"}
{"text": "The proposed approach is a generalized classification approach that is applicable to the problem of any field having multiple conflicting objectives , and a dataset can be represented in the form of labelled instances in terms of its features .Conclusion and Scope for Future Work .", "label": "", "metadata": {}, "score": "52.170925"}
{"text": "The first use of the term statistical semantics .The paper argues that word sense disambiguation for machine translation should be based on the co - occurrence frequency of the context words near a given target word .A Novel Approach to Word Sense Disambiguation Based on Topical and Semantic Association .", "label": "", "metadata": {}, "score": "52.18516"}
{"text": "Many feature selection techniques for ensemble classifiers are proposed in the literature which can be further investigated in [ 15 , 16 , 103 ] .Data Level .This level focuses on ensemble generation phase of ensemble learning process .Here , different data subsets are used to train the pool of base classifiers .", "label": "", "metadata": {}, "score": "52.189663"}
{"text": "One solution is to make use of a lexicon , which describes how different words relate to one another .Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data .", "label": "", "metadata": {}, "score": "52.19631"}
{"text": "Combination of classifiers involves development of ensemble at generation and selection phases of learning , whereas ensemble integration phase involve combination of different predictions of multiple classifiers .In the following paragraphs , we presented important AI - based ensembles studies proposed in the last decade and compared them by various evaluation metrics .", "label": "", "metadata": {}, "score": "52.239838"}
{"text": "Merkel M , Andersson M : Combination of contextual features for word sense disambiguation .SENSEVAL-2 Workshop 123 - 127 .Bruce R , Wiebe J : Word sense disambiguation using decomposable models .Proceedings of the Thirty - second Annual Meeting of the Association of Computational Linguistics 139 - 146 .", "label": "", "metadata": {}, "score": "52.252068"}
{"text": "An equivalent way of thinking about these methods consists in encoding each class as a bit string ( named codeword ) and in training a different two - class base classifier in order to separately learn each codeword bit .When the classifiers are applied to classify new points , a suitable measure of dissimilarity between the codeword computed by the ensemble and the codeword classes is used to predict the class ( e.g. , Hamming distance ) [ 100 ] .", "label": "", "metadata": {}, "score": "52.25304"}
{"text": "Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \" , \" .Next , the classifier checks if the word ends in \" the \" , in which case it 's almost certainly a determiner .", "label": "", "metadata": {}, "score": "52.263985"}
{"text": "Constructing Topical - Semantic Association Graph .We fully exploit the interrelationships between topical graph and context space to construct topical - semantic association graph .Figure 1 shows the example of the topical - semantic association graph .We take the proximal terms in the syntactic structure as adjoining feature , disambiguation context as semantic feature , and the topic chain of proximal terms and TDTs in topic span interval as topic feature .", "label": "", "metadata": {}, "score": "52.275154"}
{"text": "Hence , the formulation of reoccurrence topic span interval ( TSI ) for a certain TDT is defined as .Then , we exploit position feature , statistical feature , semantic feature , and topic span distribution feature to identify and extract topic discriminative term .", "label": "", "metadata": {}, "score": "52.282654"}
{"text": "When the annotators did not assign any sense for an instance , then that instance is tagged with \" none \" .Only one term \" association \" with all of its 100 instances were annotated none and so dropped from the testing .", "label": "", "metadata": {}, "score": "52.28787"}
{"text": "The underlying complex decision boundary can then be approximated by an appropriate combination of different classifiers .In many applications that call for automated decision making , it is not unusual to receive data obtained from different sources that may provide complementary information .", "label": "", "metadata": {}, "score": "52.35328"}
{"text": "The former approach involves the use of different classifiers to take a unique decision about the data pattern typically related to a single network packet whereas the later approach is mainly aimed at providing a high - level description of the detected pattern / attack by using the outputs of different classifiers / IDS .", "label": "", "metadata": {}, "score": "52.376343"}
{"text": "To be able to identify whether there are significant differences in the error rates due to different sample sizes and sense distributions while controlling for the abbreviation used , we used Friedman 's procedure .Notice that if we stratify by the abbreviation , the mean error rates form a two - way table where the columns correspond to different sample sizes and the rows correspond to different sense distributions .", "label": "", "metadata": {}, "score": "52.377007"}
{"text": "We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : .Next , we 'll define a simple feature extractor that checks what words the post contains : .Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment .", "label": "", "metadata": {}, "score": "52.412346"}
{"text": "275 - 298 , Springer , 2006 .View at Google Scholar .G. K. Savova , A. R. Coden , I. L. Sominsky et al . , \" Word sense disambiguation across two domains : biomedical literature and clinical notes , \" Journal of Biomedical Informatics , vol .", "label": "", "metadata": {}, "score": "52.487038"}
{"text": "Agirre et al .proposed a graph - based WSD technique which is considered unsupervised but relies on UMLS [ 2 ] .The concepts of UMLS are represented as a graph , and WSD is done using personalized page rank algorithm [ 2 ] .", "label": "", "metadata": {}, "score": "52.491867"}
{"text": "In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing , Providence , RI , August 1997 .Statistical sense disambiguation with relatively small corpora using dictionary definitions .In Proceedings of the 33rd Meetings of the Association for Computational Linguistics ( ACL-95 ) , pages 181 - 188 , Cambridge , M.A. , 1995 .", "label": "", "metadata": {}, "score": "52.541294"}
{"text": "Gender Identification .In 4 we saw that male and female names have some distinctive characteristics .Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male .", "label": "", "metadata": {}, "score": "52.55405"}
{"text": "Data set for experiments .Four abbreviations were used in the experiments .Table 1 lists the detailed information about the abbreviations and their senses .These abbreviations were originally specified in the ABBR data set [ 8 ] .We chose them by considering the different levels of semantic similarity among their senses .", "label": "", "metadata": {}, "score": "52.708267"}
{"text": "In the process of generating topical graph , the subgraph is firstly constructed through immediate overlap of topic span intervals .Then , the multiple subgraphs are connected to the whole topical graph through the immediate adjoining relationship of topic span intervals of TDT , and these intervals are not overlapped .", "label": "", "metadata": {}, "score": "52.835003"}
{"text": "Bioinformatics 2004 , 20 : 2597 - 2604 .View Article PubMed .Schijvenaars BJ , Mons B , Weeber M , Schuemie MJ , van Mulligen EM , Wain HM , et al .: Thesaurus - based disambiguation of gene symbols .", "label": "", "metadata": {}, "score": "52.898834"}
{"text": "Section 3 lists the reasons and benefits for combining multiple base classifiers .Various taxonomies proposed in the literature are presented in Section 4 .The section also describes various methods used at different levels to generate ensembles .Section 5 highlights various AI - based ensembles proposed for ID during the last decade .", "label": "", "metadata": {}, "score": "52.910164"}
{"text": "No doubt , the proposed method reported improved performance but it suffers from the limitation of incremental learning .It requires continuous retraining for a changing environment .Zainal et al .[ 30 ] proposed a heterogeneous ensemble of different classifiers and used weighted voting method for combining their predictions .", "label": "", "metadata": {}, "score": "52.98059"}
{"text": "Many efforts are being done to improve DR and FPR of the IDSs [ 1 ] .In the beginning , the research focus was to rule based IDSs and statistical IDSs .But , with large data sets , the results of these IDSs become unsatisfactory .", "label": "", "metadata": {}, "score": "52.99583"}
{"text": "Hu et al .[112 ] suggested an AdaBoost algorithm - based ensemble which in turn uses decision stump as base classifiers .They utilized continuous and categorical features separately without any forced conversion .The proposed system was evaluated using KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "53.067337"}
{"text": "Supervised ML methods have also been applied to WSD in the biomedical domain .Hatzivassiloglou [ 10 ] developed a disambiguation system to determine the class of a known biomedical named entity by choosing one of three pre - defined senses : gene , RNA , protein .", "label": "", "metadata": {}, "score": "53.09568"}
{"text": "The problem is that words often have more than one meaning , sometimes fairly similar and sometimes completely different .The meaning . of a .word in a particular usage can only be determined by examining its context . . .", "label": "", "metadata": {}, "score": "53.145355"}
{"text": "( 1 ) Ensemble comprise of multiple weak classifiers instead of single classifier .The multiple classifiers complement weaknesses of each other and hence improve the performance .( 2 ) Ensembles use the combined knowledge to model the hypothesis of the problem upon different subset of dataset or feature subspace .", "label": "", "metadata": {}, "score": "53.202072"}
{"text": "Certain problems are just too difficult for a given classifier to solve .In fact , the decision boundary that separates data from different classes may be too complex , or lie outside the space of functions that can be implemented by the chosen classifier model .", "label": "", "metadata": {}, "score": "53.234825"}
{"text": "235 - 240 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . D. Dligach and M. Palmer , \" Novel semantic features for verb sense disambiguation , \" in Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , ACL-08 : HLT , pp .", "label": "", "metadata": {}, "score": "53.24636"}
{"text": "The more senses of term or phrase denote the weaker capacity of topic discrimination .The greater topic span distribution denotes the stronger topic representation .Consequently , we define the formula ( 1 ) for calculating the weight of TDT as follows : . are user - specified , the values of which are dynamically adjusted according to experimental effect .", "label": "", "metadata": {}, "score": "53.3307"}
{"text": "The basic principle is to evaluate several individual pattern classifiers , and integrate them in order to reach a classification that is better than the one obtained by each of them separately .Our overview focused on supervised AI - based ensemble for intrusion detection proposed in last decade , since historically these were the first to be studied and applied to several application domains .", "label": "", "metadata": {}, "score": "53.33254"}
{"text": "An experiment in semantic tagging using hidden markov model tagging .In Vossen , P. , Adriaens , G. , Calzolari , N. , Sanfilippo , A. , and Wilks , Y. , editors , Proceedings of the ACL / EACL'97 Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources .", "label": "", "metadata": {}, "score": "53.341507"}
{"text": "They reported that the proposed method provides significant improvement of prediction accuracy in ID .Muda et al .[120 ] proposed a combined approach of clustering and classification .The clustering is performed by using K - means algorithm to form groups of similar data in earlier stage .", "label": "", "metadata": {}, "score": "53.356113"}
{"text": "The process can then be repeated until all of the inputs have been labeled .This strategy is demonstrated in 1.7 .First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far .", "label": "", "metadata": {}, "score": "53.369705"}
{"text": "44 - 51 , 2010 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus : : The first use of the term ' ' statistical semantics ' ' .The paper argues that word sense disambiguation for machine translation should be based on the co - occurrence frequency of the context words near a given target word .", "label": "", "metadata": {}, "score": "53.387917"}
{"text": "Classifier level may consider different models and may design base learners for specific ensemble methods .At third level , different subsets of features can be used for the classifiers .Finally , different data subsets , so that each base classifier in the ensemble is trained on its own data , can be used to build up the committee of learning machines .", "label": "", "metadata": {}, "score": "53.424927"}
{"text": "The method has been evaluated with the benchmark dataset NLM - WSD with various settings and in biomedical entity species disambiguation .The evaluation results showed that the approach is very competitive and outperforms recently reported results of other published techniques .", "label": "", "metadata": {}, "score": "53.449623"}
{"text": "One way to capture this intuition that distribution ( i ) is more \" fair \" than the other two is to invoke the concept of entropy .In the discussion of decision trees , we described entropy as a measure of how \" disorganized \" a set of labels was .", "label": "", "metadata": {}, "score": "53.457394"}
{"text": "Make use of this fact to build a consecutive classifier for labeling dialog acts .Be sure to consider what features might be useful .See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas .", "label": "", "metadata": {}, "score": "53.476616"}
{"text": "Do you find any of them surprising ?Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier .", "label": "", "metadata": {}, "score": "53.550503"}
{"text": "G. Rogova , \" Combining the results of several neural network classifiers , \" Neural Networks , vol .7 , no .5 , pp .777 - 781 , 1994 .L. Lam and C. Y. Suen , \" Optimal combinations of pattern classifiers , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "53.581787"}
{"text": "For each abbreviation , we measured error rates of the SVM classifier under different combinations of sample size , sense distribution , cross validation scheme ( 5-fold vs. 10-fold ) , and multi - class SVM algorithms ( for BPD only , which has 3 different senses ) .", "label": "", "metadata": {}, "score": "53.598545"}
{"text": "Then the probability that a given document D contains all of the words , given a class C , is .( This technique of \" log - likelihood ratios \" is a common technique in statistics .In the case of two mutually exclusive alternatives ( such as this example ) , the conversion of a log - likelihood ratio to a probability takes the form of a sigmoid curve : see logit for details . )", "label": "", "metadata": {}, "score": "53.623123"}
{"text": "Conclusion .Several different independent aspects affect performance when using ML techniques for WSD .We found that combining them into one single result obscures understanding of the underlying methods .Although we studied only four abbreviations , we utilized a well - established statistical method that guarantees the results are likely to be generalizable for abbreviations with similar characteristics .", "label": "", "metadata": {}, "score": "53.69149"}
{"text": "This can be done by using ( 1 ) different initialization parameters of base classifiers ; or ( 2 ) different subsets of feature space ( feature level ) ; or ( 3 ) different data subsets ( data level ) to train the base classifiers .", "label": "", "metadata": {}, "score": "53.709213"}
{"text": "These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True .The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these .", "label": "", "metadata": {}, "score": "53.76731"}
{"text": "That is , .[ 4 ] .With a multinomial event model , samples ( feature vectors ) represent the frequencies with which certain events have been generated by a multinomial where is the probability that event i occurs ( or K such multinomials in the multiclass case ) .", "label": "", "metadata": {}, "score": "53.775352"}
{"text": "If this set of classifiers is fixed , the problem focuses on the ensemble integration phase .It is also possible to use a fixed combiner and optimize the set of input classifiers ; the problem focuses on the generation and selection phases .", "label": "", "metadata": {}, "score": "53.836037"}
{"text": "return features . words ( ' pos / cv957_8737 .The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) .", "label": "", "metadata": {}, "score": "53.867058"}
{"text": "Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns .Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models .", "label": "", "metadata": {}, "score": "53.90102"}
{"text": "Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels .Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set .", "label": "", "metadata": {}, "score": "53.91975"}
{"text": "A linear classifier , one that is capable of learning linear boundaries , can not learn this complex non - linear boundary .However , appropriate combination of an ensemble of such linear classifiers can learn any non - linear boundary .", "label": "", "metadata": {}, "score": "53.927"}
{"text": "Some IDSs have been developed based on single - classification technique while other IDSs ( called hybrid / ensemble IDS ) implement more - than - one - classfication technique .Ensemble - based IDSs have many advantages over the IDS implementing single technique ( refer to Section 2 ) .", "label": "", "metadata": {}, "score": "53.988773"}
{"text": "n .c .e . s . . .( .We also use the baseline method which is the most frequent sense ( mfs ) for each word .This lead , to a total of 31 words tested in this evaluation , and 18 words were dropped because they do not have at least two instances annotated for each one of two senses .", "label": "", "metadata": {}, "score": "54.031677"}
{"text": "This is perhaps the primary reason why ensemble based systems are used in practice : what is the most appropriate classifier for a given classification problem ?The most commonly used procedure - choosing the classifiers with the smallest error on training data - is unfortunately a flawed one .", "label": "", "metadata": {}, "score": "54.045483"}
{"text": "For investigation of NB as a base classifier , ensemble generation is done by using different subsets of the feature space ( feature level ) .Diverse set of NB classifiers is generated by optimizing those using different subsets of the feature space of the training data set .", "label": "", "metadata": {}, "score": "54.101257"}
{"text": "def rte_features ( rtepair ) : . return features .Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \" Recognizing Text Entailment \" Feature Extractor .The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference .", "label": "", "metadata": {}, "score": "54.149765"}
{"text": "Our method also outperforms all 10 other methods in 12 out of 31 words followed by NB which outperforms the rest in 7 words .Stevenson et al . in their paper [ 1 ] report extensive accuracy results of their method ( we call it Stevenson-2008 ) along with four other methods including Joshi-2005 and McInnes-2007 , with various combinations of words from NLM - WSD corpus used for testing .", "label": "", "metadata": {}, "score": "54.16407"}
{"text": "6 , pp .40 - 49 , 2004 .View at Google Scholar .T. Dietterich , \" Ensemble methods in machine learning , \" in Proceedings of Workshop on Multiple Classifier Systems , pp . 1 - 15 , 2000 .", "label": "", "metadata": {}, "score": "54.19496"}
{"text": "67 - 86 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . K. Tsutsumida , J. Okamoto , S. Ishizaki , M. Nakatsuji , A. Tanaka , and T. Uchiyama , Study of Word Sense Disambiguation System That Uses Contextual Features Approach of Combining Associative Concept Dictionary and Corpus[C ] , LREC , 2010 . A. Montoyo , M. Palomar , G. Rigau , and A. Suarez , \" Combining Knowledge- and Corpus - based Word - Sense - Disambiguation Methods[C ] , \" CoRR abs/1109 .", "label": "", "metadata": {}, "score": "54.243546"}
{"text": "Our evaluation is done on 31 words ( as explained in Section 3 ) .We obtained the results of the other methods on these 31 words from the references shown in Table 6 to allow for direct comparison .The best result reported in their paper is 87.8 % using all words with VSM model and for McInnes 85.3 % also with the whole set [ 1 ] .", "label": "", "metadata": {}, "score": "54.262688"}
{"text": "Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data .These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering .", "label": "", "metadata": {}, "score": "54.28322"}
{"text": "Similar approach of multiclassifier systems is also advocated by Sabhnani and Serpen [ 23 ] by combining three different machine learning techniques , namely , an ANN , k - means clustering , and a Gaussian classifier .However , they do not provide further implementation details about training the classifiers , nor about determining output of the ensemble .", "label": "", "metadata": {}, "score": "54.29461"}
{"text": "But some reported that certain classification algorithms were better than others .It is still an unclear issue , probably due to the interaction of different combinations of issues .The comparison between different classifiers should be a carefully controlled experiment .", "label": "", "metadata": {}, "score": "54.36666"}
{"text": "( b )During prediction , the same feature extractor is used to convert unseen inputs to feature sets .These feature sets are then fed into the model , which generates predicted labels .In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks .", "label": "", "metadata": {}, "score": "54.39162"}
{"text": "Two ambiguous gene symbol lists were formed as a result of the comparisons : a gene - English list ( containing gene symbols ambiguous with general English words ) and a gene - UMLS list ( containing gene symbols ambiguous with biomedical terms ) .", "label": "", "metadata": {}, "score": "54.408783"}
{"text": "The proposed approach is a three - phased technique as described in subsequent paragraphs .Phases 1 and 2 are multiobjective in nature and use multiobjective GA to generate a set of base classifiers and ensembles thereof , respectively .The set of base classifiers and their ensembles exhibit classification trade - offs for the user .", "label": "", "metadata": {}, "score": "54.470886"}
{"text": "For example , a sample testing set with size 20 and sense distribution ( 0.5 , 0.5 ) means 10 samples in the set are with one sense and the other 10 samples are with the other sense .The estimated sense distribution is listed in the last column of Table 1 , which is calculated based on the number of retrieved articles for each sense and the number of retrieved articles for all the senses .", "label": "", "metadata": {}, "score": "54.529613"}
{"text": "7 , no . 3 , pp .788 - 792 , 1996 .G. Giacinto and F. Roli , \" Approach to the automatic design of multiple classifier systems , \" Pattern Recognition Letters , vol .22 , no . 1 , pp .", "label": "", "metadata": {}, "score": "54.554596"}
{"text": "The smaller subset of classifiers may be selected according to clustering [ 91 ] or by selecting the classifiers whose performance exceeds specific threshold values .However , in the general literature on classifier combination , it is observed that there is no evidence supporting the use of base classifiers of the same type or different types [ 33 , 46 ] .", "label": "", "metadata": {}, "score": "54.64579"}
{"text": "In this case , the classifier combination involves merging the individual ( usually weaker and/or diverse ) classifiers to obtain a single ( stronger ) expert of superior performance .Examples of this approach include bagging predictors ( Breiman 1996 ) , boosting ( Schapire 1990 ) , AdaBoost ( Freund 2001 ) and their many variations .", "label": "", "metadata": {}, "score": "54.680466"}
{"text": "Such a classifier can not learn the boundary shown in Figure 2 .Now consider a collection of circular decision boundaries generated by an ensemble of such classifiers as shown in Figure 3 , where each classifier labels the data as class O or class X , based on whether the instances fall within or outside of its boundary .", "label": "", "metadata": {}, "score": "54.691856"}
{"text": "Many researchers also used these methods to reduce the false alarms by correlating similar alarms [ 18 , 88 ] .These methods are employed to analyze root cause of false alarms [ 17 ] .( v ) Statistical Selection Method .", "label": "", "metadata": {}, "score": "54.74942"}
{"text": "Markatou M , Tian H , Biswas S , Hripcsak G : Analysis of variance of cross - validation estimators of the generalization error .Journal of Machine Learning Research 2005 , 6 : 1127 - 1168 .Resnik P , Yarowsky D : Distinguishing systems and distinguishing senses : New evaluation tools for words sense disambiguation .", "label": "", "metadata": {}, "score": "54.799812"}
{"text": "They reported the better performance of proposed hybrid approach over single Na\u00efve Bayes classifier over KDD 1999 dataset .But the proposed method suffers from limitation that it is unable to detect similar attacks like U2R and R2L. Here , architecture of system can be parallel , cascading , or hierarchical [ 35 ] , the classifiers can be combined by ensemble- or hybrid - combining approach .", "label": "", "metadata": {}, "score": "54.80446"}
{"text": "4544 - 4566 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. P. W. Duin , \" The combining classifier : to train or not to train ? \" in Proceedings of 16th International Conference on Pattern Recognition ( ICPR ' 02 ) , pp .", "label": "", "metadata": {}, "score": "54.859104"}
{"text": "[ 2 ] However , the situation is not as bad as Bar - Hillel feared , there have been several advances in word sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .", "label": "", "metadata": {}, "score": "54.86612"}
{"text": "Individual features make their contribution to the overall decision by \" voting against \" labels that do n't occur with that feature very often .In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature .", "label": "", "metadata": {}, "score": "54.898647"}
{"text": "View at Scopus .O. Abend , R. Reichart , and A. Rappoport , \" A supervised algorithm for verb disambiguation into VerbNet classes , \" in Proceedings of the 22nd International Conference on Computational Linguistics ( Coling ' 08 ) , pp .", "label": "", "metadata": {}, "score": "54.906094"}
{"text": "34 , no .4 , pp .369 - 374 , 1998 .View at Google Scholar .G. Giacinto and F. Roli , \" Dynamic classifier fusion , \" in Proceedings of the Multiple Classifier Systems .First International Workshop ( MCS ' 00 ) , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "54.943985"}
{"text": "119 - 139 , 1997 .J. Kittler , M. Hatef , R. P. W. Duin , and J. Mates , \" On combining classifiers , \" IEEE Trans . on Pattern Analysis and Machine Intelligence , vol .20 , no . 3 , pp .", "label": "", "metadata": {}, "score": "54.9481"}
{"text": "To check how reliable the resulting classifier is , we compute its accuracy on the test set .And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging .", "label": "", "metadata": {}, "score": "54.97759"}
{"text": "Liu H , Johnson SB , Friedman C : Automatic resolution of ambiguous terms based on machine learning and conceptual relations in the UMLS .J Am Med Inform Assoc 2002 , 9 : 621 - 636 .View Article PubMed .", "label": "", "metadata": {}, "score": "54.98426"}
{"text": "Many techniques generate a pool of homogeneous base classifiers , for examples , genetic algorithms .Although in ensembles , the combined knowledge is important , it is very computationally expensive to combine a large number of classifiers from the whole population [ 33 ] .", "label": "", "metadata": {}, "score": "55.02337"}
{"text": "In particular , the identity of the previous word is included as a feature .It is clear that exploiting contextual features improves the performance of our part - of - speech tagger .For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \" large \" or the word \" gubernatorial \" .", "label": "", "metadata": {}, "score": "55.061234"}
{"text": "945 - 954 , 1995 .K. Woods , W. P. J. Kegelmeyer , and K. Bowyer , \" Combination of multiple classifiers using local accuracy estimates , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .19 , no .", "label": "", "metadata": {}, "score": "55.06421"}
{"text": "Further details can be studied in [ 4 , 5 ] .Since the first introduction , IDSs have been evaluated using a number of different ways based upon evaluation datasets [ 6 ] .Various features of IDS can be evaluated , which may range from performance and correctness to usability .", "label": "", "metadata": {}, "score": "55.0726"}
{"text": "See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK .3 Evaluation .In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model .The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it .", "label": "", "metadata": {}, "score": "55.095306"}
{"text": "But , still some research issues exist .The major issues include diversity among the base classifiers , ensemble size , computational overhead , input feature space , and combining strategy .Ensemble Classifiers .The ensembles involve the employment of multiple base classifiers and combine their predictions to obtain reliable and more accurate predictions .", "label": "", "metadata": {}, "score": "55.096375"}
{"text": "Detecting patterns is a central part of Natural Language Processing .Words ending in -ed tend to be past tense verbs ( 5 . )Frequent use of will is indicative of news text ( 3 ) .These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic .", "label": "", "metadata": {}, "score": "55.109093"}
{"text": "The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) .return sents . 2.2 Identifying Dialogue Act Types .When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker .", "label": "", "metadata": {}, "score": "55.167614"}
{"text": "In particular , for each consecutive word index i , a score is computed for each possible current and previous tag .This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences .", "label": "", "metadata": {}, "score": "55.169197"}
{"text": "However , the practice of AI - based classifiers reveals that each of them has advantages and disadvantages for intrusion detection .Ensemble has the power to combine the strengths of these classifiers in such a way that their disadvantages will be compensated , thus offering better solutions .", "label": "", "metadata": {}, "score": "55.191975"}
{"text": "In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2Choosing The Right Features .Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model .", "label": "", "metadata": {}, "score": "55.23977"}
{"text": "At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification .In 1 , we saw several examples of corpora where documents have been labeled with categories .", "label": "", "metadata": {}, "score": "55.30509"}
{"text": "6 , no.3 , pp .21 - 45 , 2006 . is being used in a textual context .A more technical discussion of WSD follows below .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "55.338394"}
{"text": "In this paper , a three - phase , approach is proposed to generate solutions to a simple chromosome design in the first phase .In the first phase , a Pareto front of noninferior individual solutions is approximated .In the second phase of the proposed approach , the entire solution set is further refined to determine effective ensemble solutions considering solution interaction .", "label": "", "metadata": {}, "score": "55.34932"}
{"text": "This is the approach taken by Hidden Markov Models .Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags .However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags .", "label": "", "metadata": {}, "score": "55.399956"}
{"text": "So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ?One problem that arises is that the classifier can end up \" double - counting \" the effect of highly correlated features , pushing the classifier closer to a given label than is justified .", "label": "", "metadata": {}, "score": "55.4412"}
{"text": "These activities correspond to ensemble generation , ensemble selection , and ensemble integration phases of the ensemble learning process [ 18 ] .Most of the existing ensemble classifiers aim at maximizing the overall detection accuracy by employing multiple classifiers .The generalizations made concerning ensemble classifiers are predominantly suitable in the field of ID .", "label": "", "metadata": {}, "score": "55.46069"}
{"text": "To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label .Every feature is assumed to be entirely independent of every other feature , given the label .", "label": "", "metadata": {}, "score": "55.461994"}
{"text": "Zainal et al .[35 ] proposed heterogeneous ensemble of linear genetic programming ( LGP ) , adaptive neural fuzzy inference system ( ANFIS ) , and random forest ( RF ) for ID .Base classifiers are generated by using class - specific features of KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "55.491642"}
{"text": "A somewhat better approach is to ensure that the training set and test set are taken from different documents : .If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : .", "label": "", "metadata": {}, "score": "55.534615"}
{"text": "In press . Y. Miyao , K. Sagae , R. S\u00e6tre , T. Matsuzaki , and J. Tsujii , \" Evaluating contributions of natural language parsers to protein - protein interaction extraction , \" Bioinformatics , vol .25 , no . 3 , pp .", "label": "", "metadata": {}, "score": "55.567852"}
{"text": "[ 38 , 39 ] to generate an ensemble of base classifiers .The generation of the ensemble was completed in two stages using modified NSGA - II [ 40 ] .In the first stage , a set of base classifiers was generated .", "label": "", "metadata": {}, "score": "55.625984"}
{"text": "The decisions made by each classifier can then be combined by any of the combination rules described below .Confidence Estimation .The very structure of an ensemble based system naturally allows assigning a confidence to the decision made by such a system .", "label": "", "metadata": {}, "score": "55.63945"}
{"text": "The findings of research work in each study are systematically summarized and compared , which allows us to clearly identify existing research challenges for intrusion detection and underline research directions .It is expected that this paper can serve as a practical channel through the maze of the literature .", "label": "", "metadata": {}, "score": "55.648476"}
{"text": "In the hierarchical architecture , individual classifiers are combined into a structure , which is similar to that of a decision tree classifier .The tree nodes , however , may now be associated with complex classifiers demanding a large number of features .", "label": "", "metadata": {}, "score": "55.683147"}
{"text": "This level focuses on ensemble selection phase of ensemble learning process .It determines which base classifiers are used to constitute the ensemble prediction .Many researchers investigated the combination of base classifiers at this level very advantageous particularly for ID [ 23 , 28 - 30 , 32 - 44 ] .", "label": "", "metadata": {}, "score": "55.729286"}
{"text": "Some of these combination rules operate on class labels only , whereas others need continuous outputs that can be interpreted as support given by the classifier to each of the classes .Xu et al ( Xu 1992 ) .defines three types of base model outputs to be used for classifier combination .", "label": "", "metadata": {}, "score": "55.871433"}
{"text": "How likely is a given input value with a given label ?What is the most likely label for an input that might have one of two values ( but we do n't know which ) ?The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier .", "label": "", "metadata": {}, "score": "55.901463"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .Contents .One of the first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .", "label": "", "metadata": {}, "score": "56.021507"}
{"text": "This could also be due to the existence of other confounding factors in the datasets that were used .In our study , we controlled for this factor by using \" bag - of - word \" features in all experiments , but it would be interesting to see if the performance improves when different feature vectors are used .", "label": "", "metadata": {}, "score": "56.08405"}
{"text": "The system achieved an accuracy rate of 92.7 % on an automatically generated testing set .Schijvenaars 's study described an effective method for gene disambiguation , but the evaluation results were limited to certain conditions .The automatically generated testing set contained human genes symbols that appeared as long - form and short - form pairs ( e.g. prostate specific antigen ( PSA ) ) in articles , where at least 6 articles were determined to be associated with each gene sense .", "label": "", "metadata": {}, "score": "56.106003"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , F. Roli , J. Kittler , and T. Windeatt , Eds . , vol .3077 , pp . 1 - 15 , 2004 .L. I. Kuncheva , Combining Pattern Classifiers , Methods and Algorithms .", "label": "", "metadata": {}, "score": "56.111465"}
{"text": "Otherwise , your evaluation results may be unrealistically optimistic .Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features .Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label .", "label": "", "metadata": {}, "score": "56.143787"}
{"text": "However , there is no comprehensive review of ensembles in general and AI - based ensembles for ID to examine and understand their current research status to solve the ID problem .Here , an updated review of ensembles and their taxonomies has been presented in general .", "label": "", "metadata": {}, "score": "56.14613"}
{"text": "history.append(tag ) .history.append(tag ) .return zip(sentence , history ) .1.7 Other Methods for Sequence Classification .One shortcoming of this approach is that we commit to every decision that we make .For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake .", "label": "", "metadata": {}, "score": "56.21364"}
{"text": "Along the way we will study some important machine learning techniques , including decision trees , naive Bayes ' classifiers , and maximum entropy classifiers .We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) .", "label": "", "metadata": {}, "score": "56.225315"}
{"text": "Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : .", "label": "", "metadata": {}, "score": "56.25361"}
{"text": "But , computations of multiple predictions in ensembles increase computational overhead .Many researchers and practitioners advocate ensemble classifiers by keeping following points in mind .( 1 )The availability of enormous computational power ( to cope up computational overhead of ensemble classifiers ) ; ( 2 ) lack of quality training data for realistic evaluation ; ( 3 ) improved performance ( over single classifier ) of ensembles .", "label": "", "metadata": {}, "score": "56.25528"}
{"text": "These assumptions lead to two distinct models , which are often confused .[ 9 ] [ 10 ] .When dealing with continuous data , a typical assumption is that the continuous values associated with each class are distributed according to a Gaussian distribution .", "label": "", "metadata": {}, "score": "56.333355"}
{"text": "We computed average DR , average FPR , CID , and DR of each target class from confusion matrices .The representative techniques used in this investigation are naive Bayes and its ensembles using bagging and boosting .We utilized WEKA software package to compute the results of NB and its ensembles using bagging and boosting methods .", "label": "", "metadata": {}, "score": "56.39454"}
{"text": "devised a rule based system to disambiguate biomedical entity names , like gene products , based on species .In that approach [ 9 ] , some parsing techniques are used and syntactic parse tree with paths between words to determine if there exists a path between species word and the entity name .", "label": "", "metadata": {}, "score": "56.398144"}
{"text": "In order for this process to be effective , the individual experts must exhibit some level of diversity among themselves , as described later in this article in more detail .Within the classification context , then , the diversity in the classifiers - typically achieved by using different training parameters for each classifier - allows individual classifiers to generate different decision boundaries .", "label": "", "metadata": {}, "score": "56.48434"}
{"text": "To ensure that individual boundaries are adequately different , despite using substantially similar training data , weaker or more unstable classifiers are used as base models , since they can generate suffi - ciently different decision boundaries even for small perturbations in their training parameters .", "label": "", "metadata": {}, "score": "56.50567"}
{"text": "Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs .The Brill tagger , described in ( 1 ) , is a good example of this strategy .", "label": "", "metadata": {}, "score": "56.55917"}
{"text": "View Article PubMed .Humphrey SM , Rogers WJ , Kilicoglu H , Demner - Fushman D , Rindflesch TC : Word sense disambiguation by selecting the best semantic type based on Journal Descriptor Indexing : Preliminary experiment .Journal of the American Society for Information Science and Technology 2006 , 57 : 96 - 113 .", "label": "", "metadata": {}, "score": "56.567425"}
{"text": "The evaluation results of our method compare very well with those reported in [ 9 ] as shown in Table 7 .From their results ( Table 7 ) , we notice that the best overall performance was obtained with the ML method ( machine learning ) with precision , recall , and F1 values being equal at 82.69 .", "label": "", "metadata": {}, "score": "56.571667"}
{"text": "View at Google Scholar .View at Google Scholar .F. Hristea , M. Popescu , and M. Dumitrescu , \" Performing word sense disambiguation at the border between unsupervised and knowledge - based techniques , \" Artificial Intelligence Review , vol .", "label": "", "metadata": {}, "score": "56.68689"}
{"text": "If we decide to select a subgroup , how do we go about it ?Then , once the subgroup has been selected , how can we combine the outputs of these classifiers ?Previous studies in the field of intrusion detection have attempted various techniques to generate effective ensembles such as bagging , boosting , and random subspace .", "label": "", "metadata": {}, "score": "56.691032"}
{"text": "( 4 ) Some unstable classifiers like neural networks show different results with different initializations due to the randomness inherent in the training procedure .Instead of selecting the best network and discarding the others , one can combine various networks .", "label": "", "metadata": {}, "score": "56.71028"}
{"text": "When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance .", "label": "", "metadata": {}, "score": "56.73548"}
{"text": "Multiple comparisons , adjusted for multiple testing , indicated that when the overall significance level is 0.1 , the sense distributions ( 0.5 , 0.5 ) and ( 0.6 , 0.4 ) impact the error rate .These results show that almost balanced sense distributions and rather large training sample sizes reduce the error rate to approximately half of our best guess , which is using the majority sense .", "label": "", "metadata": {}, "score": "56.74154"}
{"text": "We therefore adjust our feature extractor to include features for two - letter suffixes : .Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : .", "label": "", "metadata": {}, "score": "56.745438"}
{"text": "In Proceedings of the 14th International Conference on Computational Linguistics ( COLING-92 ) , pages 454 - 460 , Nantes , France , 1992 .This approach attempts to disambiguate words using information which is gained by training on some corpus , rather that taking it directly from an explicit knowledge source .", "label": "", "metadata": {}, "score": "56.816765"}
{"text": "When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation .Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set .", "label": "", "metadata": {}, "score": "56.819183"}
{"text": "However , it has been shown that a properly trained ensemble decision is usually correct if its confidence is high , and usually incorrect if its confidence is low .Using such an approach then , the ensemble decisions can be used to estimate the posterior probabilities of the classification decisions ( Muhlbaier 2005 ) .", "label": "", "metadata": {}, "score": "56.819466"}
{"text": "The target word . is shown in bold face .In this example , . is the total number of training examples .The values of a , b , c , d for .is more highly related with the class . than . , and so it has more discriminating power than . , and this is quantified by their MI values .", "label": "", "metadata": {}, "score": "56.822132"}
{"text": "4 , pp .609 - 619 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A.-C. Le , A. Shimazu , V.-N. Huynh , and L.-M. Nguyen , \" Semi - supervised learning integrated with classifier combination for word sense disambiguation , \" Computer Speech and Language , vol .", "label": "", "metadata": {}, "score": "56.83689"}
{"text": "[ 1 ] .The results of the three methods ( single , subset , full ) in Table 6 are taken directly from Agirre et al .[ 2 ] .In another work , Jimeno - Yepes and Aronson evaluate four unsupervised methods on the whole NLM - WSD set [ 4 ] as well as NB and combination of the four methods .", "label": "", "metadata": {}, "score": "56.888687"}
{"text": "6225 - 6232 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. Khreich , E. Granger , A. Miri , and R. Sabourin , \" Iterative Boolean combination of classifiers in the ROC space : an application to anomaly detection with HMMs , \" Pattern Recognition , vol .", "label": "", "metadata": {}, "score": "56.901005"}
{"text": "4 , pp .217 - 225 , 2009 .View at Google Scholar .C. Xiang , P. C. Yong , and L. S. Meng , \" Design of multiple - level hybrid classifier for intrusion detection system using Bayesian clustering and decision trees , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "56.90793"}
{"text": "These methods try to choose the best classifiers among the set of the available base classifiers .The final prediction of ensemble is the prediction of selected base classifier or fused prediction of subset of base classifiers as described in aforementioned text .", "label": "", "metadata": {}, "score": "56.909054"}
{"text": "First , we construct a list of documents , labeled with the appropriate categories .For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative .words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews .", "label": "", "metadata": {}, "score": "56.921852"}
{"text": "We obtained the data from the project of Wang et al .[ 9 ] .From their data , we tested the biomedical entity names that occur in at least two species with at least 3 occurrences in each species .", "label": "", "metadata": {}, "score": "56.92518"}
{"text": "Once again , there are many distributions that are consistent with this new piece of information , such as : .But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) .", "label": "", "metadata": {}, "score": "56.96913"}
{"text": "Kruegel et al .[109 ] proposed a multimodel approach that uses a number of different anomaly detection techniques ( Bayesian technique ) to detect attacks against web servers and web - based applications .The multimodels help to reduce the vulnerability of the detection process with respect to mimicry attacks .", "label": "", "metadata": {}, "score": "56.987698"}
{"text": "In this manner , the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model .[ 3 ] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below .", "label": "", "metadata": {}, "score": "56.993614"}
{"text": "Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word .This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features .", "label": "", "metadata": {}, "score": "57.030945"}
{"text": "But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have .If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label .", "label": "", "metadata": {}, "score": "57.09574"}
{"text": "They found that these multistrategy techniques , particularly the belief function , performed better than all three neural nets individually .The overall performance was also comparable to or better than a single neural net trained on the entire feature set ; however , the single neural net did a better job identifying previously unseen attacks .", "label": "", "metadata": {}, "score": "57.119205"}
{"text": "( 3 ) Since ensemble use the multiple classifiers , so it helps to find the global solution that leads to reduce the false alarm rate and increase the detection accuracy .( 4 ) Unstable base classifiers help to generate the diverse set of base classifiers for efficient ensemble .", "label": "", "metadata": {}, "score": "57.13325"}
{"text": "The more the TDTs in the certain text fragment are , the more chance there is that they are related to a similar topic content . topical subgraphs .The vertices are represented for corresponding topic span intervals ( TSI ) of TDT ; meanwhile , these TDTs associate with the corresponding topic semantic profiles which include disambiguation contexts and topic chains .", "label": "", "metadata": {}, "score": "57.186417"}
{"text": "References . A. Patcha and J. M. Park , \" An overview of anomaly detection techniques : existing solutions and latest technological trends , \" Computer Networks , vol .51 , pp .3448 - 3470 , 2007 .View at Publisher \u00b7 View at Google Scholar .", "label": "", "metadata": {}, "score": "57.24295"}
{"text": "The different neural networks were trained using different features of KDD cup 1999 dataset .They concluded that a multistrategy combination technique like belief function outperforms other representative techniques .A multiclassifier system of NNs was also advocated by Sabhnani and Serpen [ 22 ] .", "label": "", "metadata": {}, "score": "57.25139"}
{"text": "The selection approach , specially its dynamic form , selects one ( or more ) classifier from the ensemble according to the prediction performance of these classifiers on similar data from the validation set .The ensemble integration phase involves many strategies to combine multiple predictions because these strategies have performance variability when tackling different problems .", "label": "", "metadata": {}, "score": "57.274815"}
{"text": "Researchers also proposed that there are trainable and nontrainable ensembles .Trainable ensembles need additional training to create the ensemble ( either during the base classifier training or after all base classifiers is trained ) [ 33 ] .On the other hand , nontrainable ensembles do not need training after the base classifiers have been induced [ 55 , 65 ] .", "label": "", "metadata": {}, "score": "57.28833"}
{"text": "For a detailed overview of these and other combination rules , see ( Kuncheva 2005 ) .Other applications of ensemble systems .Ensemble based systems can be used in problem domains other than improving the generalization performance of a classifier .", "label": "", "metadata": {}, "score": "57.291714"}
{"text": "T.K. Ho , \" Complexity of classification problems and comparative advantages of combined classifiers , \" Int .Workshop on Multiple Classifier Systems , lecture Notes on Computer Science , Vol .1857 , pp .97 - 106 , 2000 , Springer- Verlag . F. Roli , G. Giacinto , \" Design of Multiple Classifier Systems , \" in H. Bunke and A. Kandel ( Eds . )", "label": "", "metadata": {}, "score": "57.294983"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .This takes a small number of seed definitions of the senses of some word ( the seeds could be WordNet synsets or definitions from some lexicon ) and uses these to classify the \" obvious \" cases in a corpus .", "label": "", "metadata": {}, "score": "57.381035"}
{"text": "Z. Zheng and R. Srihari , \" Optimally combining positive and negative feature for text categorization , \" in Proceedings of the Workshop on Learning from Imbalanced Data Sets II ( ICML ' 03 ) , 2003 .C. D. Manning and H. Schutze , Foundations of Statistical Natural Language Processing , The MIT Press , 1999 .", "label": "", "metadata": {}, "score": "57.474495"}
{"text": "return features .Example 1.2 ( code_gender_features_overfitting .py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features .The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus .", "label": "", "metadata": {}, "score": "57.47837"}
{"text": "The general problem with these methods is their reliance on disambiguated corpora which are expensive and difficult to obtain .This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .", "label": "", "metadata": {}, "score": "57.521713"}
{"text": "Thus , the input will never be assigned this label , regardless of how well the other features fit the label .In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur .", "label": "", "metadata": {}, "score": "57.57058"}
{"text": "[ 38 , 39 ] , the objectives are to minimize the size of the ensemble and maximize the accuracy .Consequently , the drawback of their approach is to create a single best solution based on general performance metrics .The same concept was further extended by Egen [ 4 ] by conducting similar experiments with different evaluation functions for creating an ensemble of ANNs as base classifiers in the presence of imbalanced datasets using NSGA - II .", "label": "", "metadata": {}, "score": "57.620735"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , Vol .1857 , pp . 1 - 15 , 2000 , Springer - Verlag .L. K. Hansen and P. Salamon , \" Neural network ensembles , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "57.625854"}
{"text": "6 , pp .1088 - 1100 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Stevenson , E. Agirre , and A. Soroa , \" Exploiting domain information for word sense disambiguation of medical documents , \" Journal of the American Medical Informatics Association , vol .", "label": "", "metadata": {}, "score": "57.646286"}
{"text": "When comparing the performance of WSD classifiers , those metrics are critical because they indicate whether or not an improvement is statistically significant ; if there is a large deviation , there may not actually be an improvement even though one error rate is smaller than the other .", "label": "", "metadata": {}, "score": "57.700546"}
{"text": "The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline .Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline .", "label": "", "metadata": {}, "score": "57.799095"}
{"text": "In this section , we exploit the SemCor corpus to evaluate our approach .The SemCor corpus was the largest freely available textual corpus of semantically annotated words and has been extensively used in evaluating WSD systems .Topic Identification Based on Extracting TDT .", "label": "", "metadata": {}, "score": "57.801666"}
{"text": "This ( also called stacked generalization ) is a way of combining multiple classifiers using the concept of a metalearner [ 79 ] .Unlike bagging and boosting , stacking may be utilized to combine classifiers of different types .Note that steps ( 1 ) to ( 3 ) are the same as cross - validation , but instead of using a winner - takes - all approach , the base learners are combined , possibly non linearly .", "label": "", "metadata": {}, "score": "57.844097"}
{"text": "Processing of high - dimensional data for ID is highly computationally expensive .This cause may lose real - time capability of IDS .The computation overhead may be reduced by applying feature reduction techniques , which can be further explored in [ 15 , 16 ] .", "label": "", "metadata": {}, "score": "57.921364"}
{"text": "The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label .The label whose likelihood estimate is the highest is then assigned to the input value .5.1 illustrates this process .", "label": "", "metadata": {}, "score": "57.93806"}
{"text": "This decision is reached by calculating a number of anomaly scores : one for the query itself and one for each attribute .A query is reported as anomalous if at least one of these anomaly scores is above the corresponding detection threshold .", "label": "", "metadata": {}, "score": "57.98063"}
{"text": "Unfortunately , the number of possible tag sequences is quite large .Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence .In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) .", "label": "", "metadata": {}, "score": "58.013626"}
{"text": "401 - 407 , 2001 .L. I. Kuncheva , \" Switching between selection and fusion in combining classifiers : An ex - periment , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B : Cybernetics , vol .", "label": "", "metadata": {}, "score": "58.03663"}
{"text": "More recently , supervised machine learning ( ML ) technologies have received considerable attention and have shown promising results [ 16 - 18 ] .Bruce [ 19 ] applied a Bayesian algorithm and chose features based on their \" informative \" nature .", "label": "", "metadata": {}, "score": "58.065636"}
{"text": "Thus , if this term blood pressure is found in a medical text , the reader has to manually judge and determines which one of these three senses is intended in that text .Word sense disambiguation contributes in many important applications including the text mining , information extraction , and information retrieval systems [ 1 , 2 , 4 ] .", "label": "", "metadata": {}, "score": "58.073242"}
{"text": "The proposed approach is capable of producing a pool of noninferior individual solutions and ensemble solutions thereof which exhibit classification trade - offs for the user .By using certain heuristics or prior domain knowledge , a user can select an ideal solution as per application specific requirements .", "label": "", "metadata": {}, "score": "58.17666"}
{"text": "[ 11 ] .Despite the fact that the far - reaching independence assumptions are often inaccurate , the naive Bayes classifier has several properties that make it surprisingly useful in practice .In particular , the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one - dimensional distribution .", "label": "", "metadata": {}, "score": "58.17875"}
{"text": "In addition , we used an SVM classifier for all the experiments .Since the goals of our study did not include the comparison of different algorithms , we do not present related results here .Other studies showed that different ML algorithms had similar performance for WSD tasks [ 29 , 30 ] .", "label": "", "metadata": {}, "score": "58.207973"}
{"text": "Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis .First , we select a development set , containing the corpus data for creating the model .This development set is then subdivided into the training set and the dev - test set .", "label": "", "metadata": {}, "score": "58.236282"}
{"text": "Word sense disambiguation ( WSD ) is a fundamental problem in nature language processing , the objective of which is to identify the most proper sense for an ambiguous word in a given context .Although WSD has been researched over the years , the performance of existing algorithms in terms of accuracy and recall is still unsatisfactory .", "label": "", "metadata": {}, "score": "58.257294"}
{"text": "If there was a conflict then the decision given by the classifier with the highest weight is taken into account .They showed that the performance of the proposed method is superior to that of single usage of base classification methods .", "label": "", "metadata": {}, "score": "58.286816"}
{"text": "This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of RSV data set ( case where the 2 ambiguous senses both refer to viruses but the viruses are different types of viruses ) using 5-fold cross validation .", "label": "", "metadata": {}, "score": "58.290577"}
{"text": "In his 2000 review article , Dietterich lists three primary reasons for using an ensemble based system : i ) statistical ; ii ) computational ; and iii ) representational ( Dietterich 2000 ) .Note that these reasons are similar to those listed above .", "label": "", "metadata": {}, "score": "58.297287"}
{"text": "For example , a series of multilayer perceptron ( MLP ) neural networks can be trained by using different weight initializations , number of layers / nodes , error goals , etc .Adjusting such parameters allows one to control the instability of the individual classifiers , and hence contribute to their diversity .", "label": "", "metadata": {}, "score": "58.32804"}
{"text": "Using these three basic architectures , we can build even more complicated classifier combination systems .He listed eighteen different methods for classifier combination and divided them into different categories according to their trainability , adaptivity , and requirement on the output of individual classifiers .", "label": "", "metadata": {}, "score": "58.361588"}
{"text": "In other words , f 2 is an exact copy of f 1 , and contains no new information .When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose .", "label": "", "metadata": {}, "score": "58.401474"}
{"text": "In 1975 .Kelly and Stone .Computer Recognition of English Word Senses , Amsterdam : North - Holland . published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "58.40294"}
{"text": "Everything else being equal , one may be tempted to choose at random , but with that decision comes the risk of choosing a particularly poor model .Using an ensemble of such models - instead of choosing just one - and combining their outputs by - for example , simply averaging them - can reduce the risk of an unfortunate selection of a particularly poorly performing classifier .", "label": "", "metadata": {}, "score": "58.434883"}
{"text": "[ 2 ] .In the multivariate Bernoulli event model , features are independent booleans ( binary variables ) describing inputs .Like the multinomial model , this model is popular for document classification tasks , [ 9 ] where binary term occurrence features are used rather than term frequencies .", "label": "", "metadata": {}, "score": "58.48015"}
{"text": "Finally , they assumed that the training set is without attacks , by filtering it with a signature - based IDS , in order to throw out at least known attacks .Similar approach was also proposed by Corona et al .", "label": "", "metadata": {}, "score": "58.508636"}
{"text": "Note .Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative .Retrain the classifier with these new features , and test its accuracy .", "label": "", "metadata": {}, "score": "58.539158"}
{"text": "Error rates for each combination of sense distribution and sample size were averaged using 30 runs .Statistical methodology .To quantify the effects of sample size , sense distribution and difficulty of the task on the error rate , appropriate statistical methods were used .", "label": "", "metadata": {}, "score": "58.558716"}
{"text": "The original data set for PCA contained 6 different senses , but we only used the two that were very similar for our experiments .We used a simple \" full - form substitution \" method to automatically generate a data set for the experiments described in this paper , and this dataset was partitioned into training and testing sets .", "label": "", "metadata": {}, "score": "58.560143"}
{"text": "They utilized multiple classifiers and tried to exploit their strengths .They used C4.5 decision tree [ 114 ] , Na\u00efve Bayes [ 115 ] , k - NN clustering [ 116 ] , VFI - voting feature intervals [ 34 ] , and OneR [ 117 ] classifiers as base classifiers over five malware datasets .", "label": "", "metadata": {}, "score": "58.65185"}
{"text": "While naive Bayes often fails to produce a good estimate for the correct class probabilities , [ 12 ] this may not be a requirement for many applications .For example , the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is more probable than any other class .", "label": "", "metadata": {}, "score": "58.66323"}
{"text": "1075 - 1086 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Mihalcea , \" Knowledge - based methods for WSD[C ] , \" in Word Sense Disambiguation Algorithms and Applications , pp .", "label": "", "metadata": {}, "score": "58.671577"}
{"text": "Given a collection of labeled samples L and unlabeled samples U , start by training a naive Bayes classifier on L .Until convergence , do : .Predict class probabilities for all examples x in .Re- train the model based on the probabilities ( not the labels ) predicted in the previous step .", "label": "", "metadata": {}, "score": "58.771923"}
{"text": "Due to a variety of senses or the vague sense for a given term , the determination of its topic category label is the most difficult problem .Restricting semantic disambiguation context is a standard technique to mitigate the problem of term 's cross topic .", "label": "", "metadata": {}, "score": "58.781807"}
{"text": "6 , pp .1088 - 1100 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Chen and H. Al - Mubaid , \" Context - based term disambiguation in biomedical literature , \" in Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference ( FLAIRS ' 06 ) , pp .", "label": "", "metadata": {}, "score": "58.78691"}
{"text": "Rep. 460 , Department of Statistics .University of California at Berkeley , 1996 .View at Google Scholar .G. Tsoumakas , L. Angelis , and I. Vlahavas , \" Selective fusion of heterogeneous classifiers , \" Intelligent Data Analysis , vol .", "label": "", "metadata": {}, "score": "58.869175"}
{"text": "Figure 1 graphically illustrates this concept , where each classifier - trained on a different subset of the available training data - makes different errors ( shown as instances with dark borders ) , but the combination of the ( three ) classifiers provides the best decision boundary .", "label": "", "metadata": {}, "score": "58.871666"}
{"text": "The main limitation with supervised methods is the need for a corpus of manually disambiguated instances of the ambiguous words .However , the advances in automatic text annotation and tagging techniques with the help of the plethora of knowledge sources like ontologies and text literature in the biomedical domain will help lessen this limitation .", "label": "", "metadata": {}, "score": "58.881813"}
{"text": "Various classification techniques ( classifiers ) from different disciplines have been applied to detect the intrusions efficiently .Examples of these techniques include statistical techniques , artificial - intelligence- ( AI- ) based techniques , and its subfield techniques [ 5 , 19 , 20 ] .", "label": "", "metadata": {}, "score": "58.90823"}
{"text": "26 ] .The basic reason for adopting this taxonomy is its simplicity , popularity , and it covers basic aspects for building diverse pool of base classifiers .The author highlighted that diverse pool of classifiers can be generated by using different methods at four levels .", "label": "", "metadata": {}, "score": "58.940422"}
{"text": "N. C. Oza and K. Tumer , \" Input Decimation Ensembles : Decorrelation through Dimensio - nality Reduction , \" 2nd Int .Workshop on Multiple Classifier Systems , in Lecture Notes in Computer Science , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "58.956055"}
{"text": "WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .first problems that is encountered by any natural language processing system is that of lexical ambiguity , be it syntactic or semantic .", "label": "", "metadata": {}, "score": "59.012856"}
{"text": "It is due to the fact that there are not higher degree centrality vertices in topic graph .This often degrades performance , as too many low - degree centrality vertices may lead to more difficulty in identify the document 's topic .", "label": "", "metadata": {}, "score": "59.017426"}
{"text": "We first segment the data by the class , and then compute the mean and variance of in each class .Let be the mean of the values in associated with class c , and let be the variance of the values in associated with class c .", "label": "", "metadata": {}, "score": "59.01883"}
{"text": "Chebrolu et al .[ 23 ] and Abraham and Thomas [ 24 ] used weighted voting to compute the output of an ensemble of CART and BN and reported improved results for intrusion detection .Perdisci et al .[ 25 ] proposed a clustering based fusion method that reduces the volume of alarms produced by the IDS .", "label": "", "metadata": {}, "score": "59.04048"}
{"text": "Different models are generated by using a wide range of different features of client queries .The system derives automatically the parameter profiles associated with web applications ( e.g. , length and structure of parameters ) and relationships between queries ( e.g. , access times and sequences ) from the analyzed data .", "label": "", "metadata": {}, "score": "59.067947"}
{"text": "values of 100 , 200 , and 300 .With . , for example , each training example will be represented by a vector of 100 entries such that the first entry represent the context word .with the highest MI value , and the second entry represents the context word with the second highest MI value and so on .", "label": "", "metadata": {}, "score": "59.069263"}
{"text": "The approach is based on the motivation that human experts use different feature sets to detect different kinds of attacks .They generated different neural network - based classifiers by training them using different feature subsets of KDD cup 99 dataset , namely , intrinsic , content , and traffic features .", "label": "", "metadata": {}, "score": "59.0953"}
{"text": "However , this purpose of dataset is also under criticism [ 121 ] .Therefore , using these datasets only is not sufficient to reveal the efficiency of a learning algorithm .So , new and good quality benchmark datasets need to be developed for realistic evaluation of IDS .", "label": "", "metadata": {}, "score": "59.098774"}
{"text": "The training data subset for the second classifier \\(C_2\\ ) is chosen as the most informative subset , given \\(C_1\\ .\\ ) Specifically , \\(C_2\\ ) is trained on a training data only half of which is correctly classified by \\(C_1\\ , \\ ) and the other half is misclassified .", "label": "", "metadata": {}, "score": "59.10438"}
{"text": "All the results showed that the technique is fairly successful and effective in the disambiguation task .Thus , more research work should be exerted to carry out further improvements on the performance of this technique .In future work of this research , we plan to investigate the possibility of disambiguating entity names when all instances of that entity are occurring in one species .", "label": "", "metadata": {}, "score": "59.1317"}
{"text": "In the latter case , classifier outputs are often normalized to the [ 0 , 1 ] interval , and these values are interpreted as the support given by the classifier to each class , or as class - conditional posterior probabilities .", "label": "", "metadata": {}, "score": "59.157196"}
{"text": "( iv )Clustering - Based Selection Method .This method employs clustering technique to search subset of base classifiers which perform similar predictions about the unclassified instance .Then the method selects a model from each cluster to select subset of available base classifiers .", "label": "", "metadata": {}, "score": "59.177624"}
{"text": "The average accuracy results of NB and two combinations ( NB , CombSW , and CombV ) on our 31 word - subset are 86 % , 73.1 % , and 72.1 % respectively which are lower than our results , see Table 6 .", "label": "", "metadata": {}, "score": "59.22377"}
{"text": "Once we have a decision tree , it is straightforward to use it to assign labels to new input values .What 's less straightforward is how we can build a decision tree that models a given training set .But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \" decision stump \" for a corpus .", "label": "", "metadata": {}, "score": "59.260117"}
{"text": "It contains data for four words : hard , interest , line , and serve .Choose one of these four words , and load the corresponding data : .Using this dataset , build a classifier that predicts the correct sense tag for a given instance .", "label": "", "metadata": {}, "score": "59.312607"}
{"text": "We will learn more about the naive Bayes classifier later in the chapter .For now , let 's just test it out on some names that did not appear in its training data : .Observe that these character names from The Matrix are correctly classified .", "label": "", "metadata": {}, "score": "59.317276"}
{"text": "When the senses are well separated , any increase in the sample size results in a statistically significant decrease of the error rate .This holds for all sense distributions and it is in agreement with the finding that for BSA there was no significant effect of the sense distributions on the error rates for the different sample sizes used .", "label": "", "metadata": {}, "score": "59.34401"}
{"text": "We call these values w[label ] and w[f , label ] the parameters or weights for the model .Using the naive Bayes algorithm , we set each of these parameters independently : .However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values .", "label": "", "metadata": {}, "score": "59.344154"}
{"text": "L. I. Kuncheva , J. C. Bezdek , and R. Duin , \" Decision templates for multiple classifier fu - sion : an experimental comparison , \" Pattern Recognition , vol .34 , no . 2 , pp .299 - 314 , 2001 .", "label": "", "metadata": {}, "score": "59.37403"}
{"text": "The related studies of AI - based ensembles are compared by set of evaluation metrics driven from ( 1 ) architecture & approach followed ; ( 2 ) different methods utilized in different phases of ensemble learning ; ( 3 ) other measures used to evaluate classification performance of the ensembles .", "label": "", "metadata": {}, "score": "59.41443"}
{"text": "Kuncheva [ 46 ] proposed a basic taxonomy for generating a diverse pool of classifiers .She proposed that diverse classifiers can be generated by using various methods at four different levels , namely , ( 1 ) combination level ; ( 2 ) classifier level ; ( 3 ) feature level ; ( 4 ) data level .", "label": "", "metadata": {}, "score": "59.467102"}
{"text": "The fuzzy inference module implements nonlinear mappings from the outputs of the neurofuzzy classifiers of the pervious layer to the final output space which specifies if the input data are normal or intrusive .The genetic algorithm is used to optimize the structure of neurofuzzy engine .", "label": "", "metadata": {}, "score": "59.472298"}
{"text": "They proposed to partition the original dataset into two subsets .The first subset is reserved to form the metadataset and the second subset is used to build the base - level classifiers .This classifier ( Metaclassifier ) combines the different predictions into a final one .", "label": "", "metadata": {}, "score": "59.49023"}
{"text": "View at Scopus .C. Xiang , P. C. Yong , and L. S. Meng , \" Design of multiple - level hybrid classifier for intrusion detection system using Bayesian clustering and decision trees , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "59.492294"}
{"text": "This process is recursively repeated [ 85 ] .( iii ) Dynamic Classifier Selection Method .This method measure the competence of each base classifier to determine the prediction of ensemble classifier .The competence of base classifier can be determined dynamically either by using prior information about base classifiers or by posterior information produced by them in terms of their predictions [ 86 , 87 ] .", "label": "", "metadata": {}, "score": "59.50586"}
{"text": "These days [ [ WordNet ] ] is the usual dictionary in question .WSD has been investigated in computational linguistics as a specific task for well over 40 years , though the acronym is newer .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "59.518703"}
{"text": "( ii )A numerical component consists in a quantification of different links in the DAG by a conditional probability distribution of each node in the context of its parents .Naive Bayes are simple Bayes networks which are composed of DAGs with only one root node ( called parent ) representing the unobserved node and several children , corresponding to observed nodes , with the strong assumption of independence among child nodes in the context of their parent .", "label": "", "metadata": {}, "score": "59.545944"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. B. Langdon and B. F. Buxton , \" Genetic programming for improved receiver operating characteristics , \" in Proceedings of the 2nd International Conference on Multiple Classifier System , J. Kittler and F. Roli , Eds . , pp .", "label": "", "metadata": {}, "score": "59.576412"}
{"text": "129 - 145 , 2006 .View at Google Scholar .F. V. Jensen , An Introduction to Bayesian Networks , vol .210 , UCL Press , London , UK , 1996 .N. B. Amor , S. Benferhat , and Z. Elouedi , \" Naive Bayes vs decision trees in intrusion detection systems , \" in Proceedings of the ACM Symposium on Applied Computing , pp .", "label": "", "metadata": {}, "score": "59.57831"}
{"text": "Benchmark Datasets .The performance of the proposed approach is measured based on benchmark datasets .In the literature , various benchmark data sets are proposed for validation of the IDSs .As per statistics of a survey of 276 papers published between 2000 and 2008 conducted by Tavallaee [ 53 ] , most of the researchers used publicly available benchmark datasets for evaluating their network based approaches .", "label": "", "metadata": {}, "score": "59.616188"}
{"text": "Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features .We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value .", "label": "", "metadata": {}, "score": "59.63153"}
{"text": "2.1 Sentence Segmentation .Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence .", "label": "", "metadata": {}, "score": "59.689224"}
{"text": "Here , each member is trained by different dataset .Ensemble output is determined by one classifier .In nutshell , fusion based combination methods combine all the outputs of the base classifiers , whereas selection based combination methods try to choose the best classifiers among the set of the available base classifiers .", "label": "", "metadata": {}, "score": "59.72877"}
{"text": "One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation .In particular , we subdivide the original corpus into N subsets called folds .", "label": "", "metadata": {}, "score": "59.78146"}
{"text": "The proposed approach is capable of producing a pool of solutions that address the limitations of the existing techniques , striving to obtain a single solution in which there is no control on classification trade - offs ( for application specific requirements ) .", "label": "", "metadata": {}, "score": "59.788235"}
{"text": "The Learning Phase From the labeled training examples of the word , we build the feature vectors using the top context words selected by MI or M2 as features .After that , we use the support vector machine ( SVM ) [ 23 ] as the learner to train the classifier using the training vectors .", "label": "", "metadata": {}, "score": "59.81778"}
{"text": "However there is a concern that a very limited set of words may have accounted for the vast majority of ambiguity .Therefore , for each ambiguous word , we calculated its frequency , which is defined as the ratio between the number of abstracts containing the word and the total number of abstracts in the pool .", "label": "", "metadata": {}, "score": "59.833046"}
{"text": "The performance of solutions for training and test data of ITFS - KDD ( 10 features ) dataset is described in Figure 7 .The overview of classification results of KDD subsets obtained with NB and its ensembles ( bagging and boosting methods ) and the proposed approach ( AMGA2-NB ) with respect to different evaluation criteria is depicted in Table 6 .", "label": "", "metadata": {}, "score": "59.834778"}
{"text": "This level focuses on ensemble generation phase of ensemble learning process .Here , pool of classifiers is generated by using different feature subsets of dataset for the training of the base classifiers .Basic reason behind this level is to improve the computational efficiency of the ensemble and to increase the accuracy [ 46 ] .", "label": "", "metadata": {}, "score": "59.860115"}
{"text": "But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \" fly , \" knowing that the previous word is \" a \" will allow us to determine that it is functioning as a noun , not a verb .", "label": "", "metadata": {}, "score": "59.868347"}
{"text": "However the observed decrease in error rate was more dramatic in the cases where the different senses were well separated .For example , in BSA , the error rate dropped to approximately 5 % when the sample size was 80 and the sense distributions were almost balanced , and it was approximately 8 % for other distributions with the same size .", "label": "", "metadata": {}, "score": "59.942493"}
{"text": "And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large .A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label .", "label": "", "metadata": {}, "score": "59.942657"}
{"text": "The authors reported 84.1 % , 99.5 % , 14.1 % , and 31.5 % detection of Probe , DoS , U2R , and R2L attack classes .Yan and Hao [ 111 ] presented ensemble of neural network for ID based upon improved MOGA ( improvement of NSGA - II ) .", "label": "", "metadata": {}, "score": "59.984745"}
{"text": "There are different types of WSD and some are more difficult than others .For example , if two senses are syntactically different , a reliable part of speech tagging method could be effective in resolving the ambiguity .For senses that correspond to the same syntactic category , the similarity of their semantic categories will affect the difficulty of the task ( i.e. the bovine serum albumin sense of BSA is substantially different from the body surface area sense ) .", "label": "", "metadata": {}, "score": "59.99593"}
{"text": "Liu H , Teller V , Friedman C : A multi - aspect comparison study of supervised word sense disambiguation .J Am Med Inform Assoc 2004 , 11 : 320 - 331 .View Article PubMed .Leroy G , Rindflesch TC : Effects of information and machine learning algorithms on word sense disambiguation with small datasets .", "label": "", "metadata": {}, "score": "59.99644"}
{"text": "Moreover , the advances in ontology development and integration in the biomedical domain will facilitate even more the process of automatic text annotation .In this paper , we reported a machine learning approach for biomedical WSD .The approach was evaluated with a benchmark dataset , NLM - WSD , to facilitate the comparison with the results of previous work .", "label": "", "metadata": {}, "score": "59.998848"}
{"text": "This has meant that many of these algorithms have been tested on very small numbers of different words , often as few as 10 .It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .", "label": "", "metadata": {}, "score": "59.999184"}
{"text": "For that , we use feature selection techniques such as mutual information ( MI ) [ 19 , 20 ] as follows .For each context word .that do not contain .that do not contain .Therefore , the mutual information ( MI ) can be defined as .", "label": "", "metadata": {}, "score": "60.01387"}
{"text": "To begin with , they 're simple to understand , and easy to interpret .This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features .", "label": "", "metadata": {}, "score": "60.025116"}
{"text": "AI - based techniques and their ensembles can help to address this important issue , but still only a small number of researchers have focused it so far .Most of the methods discussed in this paper have their roots in the ensemble learning process .", "label": "", "metadata": {}, "score": "60.04068"}
{"text": "Different ensemble members were generated by training from reduced dataset .The final outputs were decided as follows : each classifier 's output is given a weight ( 0 - 1 scale ) depending on the generalization performance during the training process .", "label": "", "metadata": {}, "score": "60.055756"}
{"text": "We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) .", "label": "", "metadata": {}, "score": "60.139923"}
{"text": "For each sense , we recorded all the retrieved PMIDs , randomly selected 250 , and then obtained the corresponding abstracts to form a data pool , from which all the experiments were drawn .Feature vector and machine - learning algorithm .", "label": "", "metadata": {}, "score": "60.152428"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Moreira and E. Mayoraz , \" Improved pairwise coupling classification with correcting classifiers , \" in Proceedings of the 10th European Conference on Machine Learning , C. Nedellec and C. Rouveirol , Eds . , vol .", "label": "", "metadata": {}, "score": "60.240265"}
{"text": "Revision as of 04:20 , 25 June 2012 .Word Sense Disambiguation ( WSD ) is the process of identifying the sense of a polysemic word .In modern WSD systems , the senses of a word are typically taken from some specified dictionary .", "label": "", "metadata": {}, "score": "60.250763"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky [ 13 ] .", "label": "", "metadata": {}, "score": "60.277565"}
{"text": "language processing by part - of - speech taggers which predict .the . syntactic category of words . in text .with high levels of accuracy . E. Brill .Transformation - based error - driven learning . and .natural language processing : A case study in part of speech tagging .", "label": "", "metadata": {}, "score": "60.333324"}
{"text": "553 - 568 , 1997 .View at Google Scholar \u00b7 View at Scopus .L. Xu , A. Krzyzak , and C. Y. Suen , \" Methods of combining multiple classifiers and their applications to handwriting recognition , \" IEEE Transactions on Systems , Man and Cybernetics , vol .", "label": "", "metadata": {}, "score": "60.34018"}
{"text": "Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low .In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2", "label": "", "metadata": {}, "score": "60.411774"}
{"text": "129 - 156 , 1999 .View at Google Scholar . H. Ishibuchi and Y. Nojima , \" Evolutionary multiobjective optimization for the design of fuzzy rule - based ensemble classifiers , \" International Journal of Hybrid Intelligent Systems , vol .", "label": "", "metadata": {}, "score": "60.442596"}
{"text": "Proc EMNLP 2002 , 41 - 48 .Mohammad S , Pedersen T : Combining lexical and syntactic features for supervised word sense disambiguation .Proc of the CoNLL .Wilks Y , Fass D , Guo C , MacDonald J , Plate T , Slator B : Providing Machine Tractable Dictionary Tools Cambridge , MA : MIT Press 1990 .", "label": "", "metadata": {}, "score": "60.46826"}
{"text": "SIGIR'04 Workshop on Search and Discovery in BioInformatics .Hatzivassiloglou V , Duboue PA , Rzhetsky A : Disambiguating proteins , genes , and RNA in text : a machine learning approach .Bioinformatics 2001 , 17 ( Suppl 1 ) : S97 - 106 .", "label": "", "metadata": {}, "score": "60.54115"}
{"text": "( ii )Cascading Classifiers Method .In this method , different base classifiers are employed sequentially to unclassified instance and confidence level of first classifier is recorded .If its level is high enough then its prediction is the ensemble final prediction .", "label": "", "metadata": {}, "score": "60.576138"}
{"text": "In bioinformatics and computational biology , there are quite a few tasks similar to WSD like biomedical term disambiguation , gene protein name disambiguation , and disambiguating species for biomedical named entities [ 9 - 11 ] .The task of biomedical named entity disambiguation or classification is an augmentation of the well - known task of biomedical named entity recognition ( NER ) .", "label": "", "metadata": {}, "score": "60.580257"}
{"text": "The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) .Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets .", "label": "", "metadata": {}, "score": "60.610405"}
{"text": "Both the experts themselves and the gating network requires the input instances for training .Several mixture - of - experts models can also be further combined to obtain a hierarchical mixture of experts ( Jordan 1994 ) .Mixture of experts are particularly useful when different experts are trained on different parts of the feature space , or when heterogeneous sets of features are available to be used for a data fusion problem .", "label": "", "metadata": {}, "score": "60.681786"}
{"text": "Realizing of course that semantic tagging is a much more difficult problem than part - of - speech tagging , Segond , F. , Schiller , A. , Grefenstette , G. , and Chanod , J. ( 1997 ) .An experiment in semantic tagging using hidden markov model tagging .", "label": "", "metadata": {}, "score": "60.744175"}
{"text": "California : Morgan Kaufmann .A Naive Bayes supervised learning algorithm is used to extract important words and phrases from documents .The features that characterize keyphrases include early occurrence and relatively high frequency in the given document .Latent Semantic Analysis ( LSA ) is evaluated as a measure of word similarity , using multiple - choice synonym questions from the Test of English as a Foreign Language .", "label": "", "metadata": {}, "score": "60.758278"}
{"text": "where is the probability of class generating the term .This event model is especially popular for classifying short texts .It has the benefit of explicitly modelling the absence of terms .Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one .", "label": "", "metadata": {}, "score": "60.75925"}
{"text": "( 1 ) Generating a random population of individuals that represents a solution to the underlying problem .( 2 ) Evaluating the population by computing the fitness function of each individual .( 3 ) Elevating high quality individuals by selecting them from the entire population .", "label": "", "metadata": {}, "score": "60.76292"}
{"text": "The pool of solutions provides classification trade - offs to the user .Out of pool of solutions , the user can select an ideal solution as per application - specific requirements .The rest of the paper is organized as follows .", "label": "", "metadata": {}, "score": "60.882042"}
{"text": "The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree .A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples .", "label": "", "metadata": {}, "score": "60.943096"}
{"text": "Then , MI ( or M2 ) value is computed for all context words .Then , the context words . are ordered based on their MI values , and the top .words .with highest MI values are selected as features .", "label": "", "metadata": {}, "score": "60.945084"}
{"text": "It is useful for testing whether one random variable in a pair tends to have larger ( smaller or simply different ) values than the other random variable in the pair .In our case , the random variables in the pair are the error rates obtained under the different sample sizes used .", "label": "", "metadata": {}, "score": "60.968384"}
{"text": "The classifiers obtained highest accuracies on different categories of intrusions are used to detect corresponding category of intrusions .They reported that classifier combination results the improvement of classification performance .They reported that probability of detection of 88.7 % , 97.3 % , 29.8 % , and 9.6 % with 0.4 % false alarm rate for Probe , DoS , U2R , and 0.1 % for R2L attack classes , respectively .", "label": "", "metadata": {}, "score": "60.981224"}
{"text": "Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) .Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) .The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \u00d7 Precision \u00d7 Recall ) / ( Precision + Recall ) .", "label": "", "metadata": {}, "score": "61.05567"}
{"text": "We can systematically evaluate the classifier on a much larger quantity of unseen data : .Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : .This listing shows that the names in the training set that end in \" a \" are female 33 times more often than they are male , but names that end in \" k \" are male 32 times more often than they are female .", "label": "", "metadata": {}, "score": "61.05934"}
{"text": "Many researchers suggested the use of feature selection / reduction techniques [ 16 , 122 ] .These techniques help remove irrelevant and redundant features and identify appropriate features for intrusion detection .The reduction in features reduces the amount of audit data for effective IDS .", "label": "", "metadata": {}, "score": "61.071198"}
{"text": "Kuncheva and Whitaker [ 50 ] proposed different metrics to measure diversity .The diversity in ensembles can be obtained by using different ( 1 ) starting point in hypothesis space ; ( 2 ) set of accessible hypotheses ; ( 3 ) traversal of hypothesis space [ 47 ] .", "label": "", "metadata": {}, "score": "61.09317"}
{"text": "In the gated parallel variant , the outputs of individual classifiers are selected or weighted by a gating device before they are combined .In the cascading architecture , individual classifiers are invoked in a linear sequence .The number of possible classes for a given pattern is gradually reduced as more classifiers in the sequence have been invoked .", "label": "", "metadata": {}, "score": "61.09697"}
{"text": "Frank , E. , Paynter , G.W. , Witten , I.H. , Gutwin , C. , and Nevill - Manning , C.G. ( 1999 ) .Domain - specific keyphrase extraction .In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence ( IJCAI-99 ) , pp .", "label": "", "metadata": {}, "score": "61.10341"}
{"text": "But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set .An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded .", "label": "", "metadata": {}, "score": "61.106834"}
{"text": "Scaling Up to Large Datasets .Python provides an excellent environment for performing basic text processing and feature extraction .If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages .", "label": "", "metadata": {}, "score": "61.146446"}
{"text": "It was presented to show the degree of difficulty among different abbreviations .Results from 5-fold cross - validation showed no statistical difference with results from 10-fold cross - validation , which indicated 5-fold cross - validation might be used in evaluation in order to save computational power ( for a discussion of the relative merits of 5-fold cross - validation vs. 10-fold cross - validation , see [ 35 ] ) .", "label": "", "metadata": {}, "score": "61.152462"}
{"text": "Finally , diversity can also be achieved by using different features , or different subsets of existing features .In fact , generating different classifiers using random feature subsets is known as the random subspace method ( Ho 1998 ) , as described later in this article .", "label": "", "metadata": {}, "score": "61.16343"}
{"text": "For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances .But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times .", "label": "", "metadata": {}, "score": "61.209286"}
{"text": "Using an external archive that stores a large number of solutions provides useful information about the search space as well as tends to generate a large number of Pareto points at the end of the simulation .At every iteration , a small number of solutions are created using the genetic variation operators .", "label": "", "metadata": {}, "score": "61.23986"}
{"text": "[ 27 ] used the different features of dataset to generate ensemble solutions based on evolutionary algorithms .Toosi and Kahani [ 28 ] proposed a neurofuzzy classifier to classify instances of KDD cup 1999 dataset into five classes .But , a great time consuming is a big problem .", "label": "", "metadata": {}, "score": "61.25393"}
{"text": "The outputs of these classifiers on their pseudo - training blocks , along with the actual correct labels for those blocks constitute the training dataset for the Tier 2 classifier ( see Figure 7 ) .Jordan and Jacobs ' mixture of experts ( Jacobs 1991 ) generates several experts ( classifiers ) whose outputs are combined through a ( generalized ) linear rule .", "label": "", "metadata": {}, "score": "61.28171"}
{"text": "This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another .Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set .", "label": "", "metadata": {}, "score": "61.3554"}
{"text": "The task of a model is to assign a probability value to either a query as a whole or one of the query 's attributes .This probability value reflects the probability of the occurrence of the given feature value with regards to an established profile .", "label": "", "metadata": {}, "score": "61.383213"}
{"text": "3.2 Accuracy .The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled .The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 .", "label": "", "metadata": {}, "score": "61.421577"}
{"text": "I . and . is the total number of training examples .MI is a well - known concept in information theory and statistical learning .MI is a measure of interaction and common information between two variables [ 22 ] .", "label": "", "metadata": {}, "score": "61.503437"}
{"text": "Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set .But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data .", "label": "", "metadata": {}, "score": "61.507187"}
{"text": "The methods are in general characterized by a two step approach : first , at learning of the classes as a set of independent classification problems ; second , at combination of the predictions by exploiting the associations between classes that describe the hierarchy .", "label": "", "metadata": {}, "score": "61.533302"}
{"text": "Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable .A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets .", "label": "", "metadata": {}, "score": "61.537903"}
{"text": "The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent .", "label": "", "metadata": {}, "score": "61.57081"}
{"text": "Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model .In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] .", "label": "", "metadata": {}, "score": "61.576378"}
{"text": "The proposed method uses the correlation between alarms and meta alarms to reduce the volume of alarms of the IDSs .A hierarchical hybrid system was also proposed by Xiang et al .[26 ] .But , the proposed system leads to high false positive rate .", "label": "", "metadata": {}, "score": "61.580822"}
{"text": "Statistical tests [ 39 , 40 ] can be used to compare different classifiers .It is very important that the baseline of a classification task is reported because it shows how much of an improvement there is using a classifier as compared to the baseline .", "label": "", "metadata": {}, "score": "61.622883"}
{"text": "The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \" in parallel . \"5 Naive Bayes Classifiers .In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value .", "label": "", "metadata": {}, "score": "61.64445"}
{"text": "[ 13 ] .This prior probability distribution might be based on our knowledge of frequencies in the larger population , or on frequency in the training set .However , given the sample the evidence is a constant and thus scales both posteriors equally .", "label": "", "metadata": {}, "score": "61.7193"}
{"text": "First objective is to present an updated review of ensembles and their taxonomies in general for supervised classification .Third objective is to highlight research gaps and directions in developing efficient ensemble for ID .Paper Overview The rest of paper is organized as follows .", "label": "", "metadata": {}, "score": "61.75019"}
{"text": "They employed three learners VSM ( vector space model ) , Na\u00efve Bayes ( NB ) , and SVM .The results included in Table 6 are their best results with VSM and ( linguistic + MeSH ) features [ 1 ] .", "label": "", "metadata": {}, "score": "61.76302"}
{"text": "On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score .4 Decision Trees .In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers .", "label": "", "metadata": {}, "score": "61.766052"}
{"text": "In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : .Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male .", "label": "", "metadata": {}, "score": "61.76713"}
{"text": "The same study , which was also performaned for the Fly organism , showed similar results , but with slightly higher ambiguity rates .For a more complete description of this study and the results , please [ see additional file 1 ] .", "label": "", "metadata": {}, "score": "61.771393"}
{"text": "For any given instance , the class chosen by most number of classifiers is the ensemble decision .Since the training datasets may overlap substantially , additional measures can be used to increase diversity , such as using a subset of the training data for training each classifier , or using relatively weak classifiers ( such as decision stumps ) .", "label": "", "metadata": {}, "score": "61.804306"}
{"text": "For all other sense distributions , an increase in the sample size did not produce a significant reduction in the error rate - that is , there are no statistically significant differences between the error rates .We would like to stress here a limitation of the current study .", "label": "", "metadata": {}, "score": "61.83216"}
{"text": "( 1 ) Decision optimization : it refers to methods to choose and optimize the combiner for a fixed ensemble of base classifiers .This method corresponds to level A ( combination level as described above ) , ( 2 ) Coverage optimization : it refers to methods for creating diverse base classifiers assuming a fixed combiner .", "label": "", "metadata": {}, "score": "61.876003"}
{"text": "Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn .This is especially true when the size of the training set , the number of features , and the number of labels are all large .", "label": "", "metadata": {}, "score": "61.886307"}
{"text": "Identifying Topic Discriminative Term .Definition 4 ( topic discriminative term ) .Topic discriminative term ( TDT ) can be applied to characterize and highlight a term or phrase 's related subject matters in the document .The term or phrase of high discriminative should be strongly associated with the semantic context , owns the number of sense as little as possible , and explicitly specifies the topical category .", "label": "", "metadata": {}, "score": "61.89344"}
{"text": "Important methods proposed in the literature are described in the following section .( i )The Test and Select Method .This method describes a greedy method which adds a new classifier to ensemble if it reduces the squared error [ 71 ] .", "label": "", "metadata": {}, "score": "61.930515"}
{"text": "418 - 435 , 1992 . E. Allwein , R. E. Schapire , and Y. Singer , \" Reducing Multiclass to Binary : A Unifying Approach for Margin Classifiers , \" Journal of Machine Learning Research , vol .1 , pp .", "label": "", "metadata": {}, "score": "61.970127"}
{"text": "Tables 2 , 3 and 4 display the results for BSA , PCA and RSV , each of which has two senses .The distribution shown with bold font in column 1 is the estimated distribution of the senses , which is calculated based on the number of retrieved articles for each sense and the number of retrieved articles for all the senses .", "label": "", "metadata": {}, "score": "62.016945"}
{"text": "These papers are important in that they report on useful methods and provide insights and overall results .However , a deeper and more systematic analysis is needed in order to obtain a better understanding of the different factors affecting the performance of ML methods for WSD .", "label": "", "metadata": {}, "score": "62.039654"}
{"text": "G. Fumera and F. Roli , \" A Theoretical and Experimental Analysis of Linear Combiners for Multiple Classifier Systems , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol .27 , no .6 pp .942 - 956 , 2005 .", "label": "", "metadata": {}, "score": "62.05519"}
{"text": "Khreich et al .[ 2 ] proposed an iterative Boolean combination ( IBC ) technique for efficient fusion of the responses from any crisp or soft detector trained on fixed - size datasets in the ROC space .The proposed technique applies all Boolean functions to combine the ROC curves corresponding to multiple classifiers .", "label": "", "metadata": {}, "score": "62.078823"}
{"text": "To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams .Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset .The label True indicates that the entailment holds , and False , that it fails to hold .", "label": "", "metadata": {}, "score": "62.117325"}
{"text": "In supervised learning - based classification , ensembles have been successfully employed to different application domains .In the literature , many researchers have proposed different ensembles by considering different combination methods , training datasets , base classifiers , and many other factors .", "label": "", "metadata": {}, "score": "62.120136"}
{"text": "The error of this hypothesis with respect to the current distribution is calculated as the sum of distribution weights of the instances misclassified by \\(h_t\\ ) ( Equation 4 ) .AdaBoost . M1 requires that this error be less than \\(1/2\\ .", "label": "", "metadata": {}, "score": "62.13042"}
{"text": "The population size is equal to the number of desired solutions input by the user .Phase 2 generates another improved approximation of optimal Pareto front consisting of a set of nondominated ensembles based on a pool of nondominated solutions as base classifiers ( output of phase 1 ) which also exhibit classification trade - offs ( depicted in Figure 2 ) .", "label": "", "metadata": {}, "score": "62.136765"}
{"text": "Figure 8 : Training and test performance of noninferior NB based ensembles for ISCX 2012 data subset .Discussion .The results obtained in this study clearly highlight the benefits of trained NB and its ensembles by using the proposed technique .", "label": "", "metadata": {}, "score": "62.141273"}
{"text": "To test the null hypothesis of no differences in the error rates among the different sample sizes ( and overall probability distribution ) for the BSA and PCA abbreviations , we used Friedman 's test .Then we performed sub - analysis using the sign - test ( see Methods section for details ) .", "label": "", "metadata": {}, "score": "62.30802"}
{"text": "Selection of base classifiers may be done from pool of classifiers , which are trained using different induction algorithms ( called heterogeneous ensembles ) or the same induction algorithm ( called homogeneous ensembles ) .Many researcher generated ensemble by selecting heterogeneous base classifiers [ 23 , 24 , 27 - 29 , 36 ] .", "label": "", "metadata": {}, "score": "62.40142"}
{"text": "Error Rate versus Sample Size with different sense distributions of BSA data set .This figure shows the plots of \" error rate \" versus \" sample size \" with different sense distributions of BSA data set ( case where the 2 ambiguous senses are very different ) using 5-fold cross - validation .", "label": "", "metadata": {}, "score": "62.408386"}
{"text": "This is exactly what the Maximum Entropy classifier does as well .In particular , for each joint - feature , the Maximum Entropy model calculates the \" empirical frequency \" of that feature - i.e. , the frequency with which it occurs in the training set .", "label": "", "metadata": {}, "score": "62.461304"}
{"text": "Feature selection .As mentioned earlier in this article , one way to improve diversity in the ensemble is to train individual classifiers different subsets of the available features .Selecting the feature subsets at random is known as the random subspace method , a term coined by ( Ho 1998 ) , who used it on constructing decision tree ensembles .", "label": "", "metadata": {}, "score": "62.49604"}
{"text": "In this method , the ensembles are populated one classifier at the time .Each classifier is trained on selective subset of data from the original dataset .For the first base classifier , the data is selected uniformly .For successive classifiers , the sampling distribution is continuously updated so that instances that are more difficult to classify are selected more often than those that are easy to classify .", "label": "", "metadata": {}, "score": "62.550972"}
{"text": "115 - 127 , 2012 .View at Publisher \u00b7 View at Google Scholar . E. Zitzler , K. Deb , and L. Thiele , \" Comparison of multiobjective evolutionary algorithms : empirical results , \" Evolutionary Computation , vol . 8 , no . 2 , pp .", "label": "", "metadata": {}, "score": "62.564316"}
{"text": "36 , no . 1 , pp .105 - 139 , 1999 .View at Google Scholar \u00b7 View at Scopus .R. E. Banfield , L. O. Hall , K. W. Bowyer , and W. P. Kegelmeyer , \" A comparison of decision tree ensemble creation techniques , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "62.60965"}
{"text": "For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified .", "label": "", "metadata": {}, "score": "62.64398"}
{"text": "3.5 Cross - Validation .In order to evaluate our models , we must reserve a portion of the annotated data for the test set .As we already mentioned , if the test set is too small , then our evaluation may not be accurate .", "label": "", "metadata": {}, "score": "62.711617"}
{"text": "The range of sample size per sense ranges from 10 - 40 , with increments of 10 per sense .Average error rates ( Err .Rate ) and average standard errors ( SE ) were reported for each combination of distribution and sample size ( see Methods section ) .", "label": "", "metadata": {}, "score": "62.766457"}
{"text": "If the entity has 5 or more occurrences in one species , we repeat five times using 5FCV as in Section 4.1 .We extracted and tested our system on a total 465 instances of entity names with an average of 8 instances per species for each entity name .", "label": "", "metadata": {}, "score": "62.80407"}
{"text": "In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : .Furthermore , the remaining probabilities appear to be \" evenly distributed .", "label": "", "metadata": {}, "score": "62.80899"}
{"text": "It is often difficult to obtain appropriate lexical resources ( especially for texts in a specialized sublanguage ) .This lack of resources has led several researchers to explore the use of unannotated , raw , corpora to perform unsupervised disambiguation .", "label": "", "metadata": {}, "score": "62.810116"}
{"text": "59 ] .The statistics of selected ISCX 2012 data subset are depicted in Table 2 .Performance Metrics .Measuring this ability of the IDS is important to both industry as well as the research community .It helps us to tune the IDS in a better way as well as compare different IDSs .", "label": "", "metadata": {}, "score": "62.820457"}
{"text": "We used differential evolution ( DE ) operator as crossover operator for mating the population .DE has the advantage of not requiring a distribution index , and it is self - adaptive in that the step size is automatically adjusted depending upon the distribution of the solutions in the search space .", "label": "", "metadata": {}, "score": "62.840813"}
{"text": "Applying GA is valuable for its robustness in performing a global search in search space compared with other representative techniques .Several researchers employed single and multiple objective genetic algorithms for finding a set of noninferior solutions for the problem of ID .", "label": "", "metadata": {}, "score": "62.858475"}
{"text": "This allowed Luk to produce a system which used the information in lexical resources as a way of reducing the amount of text needed in the training corpus .Another example of this approach is the unsupervised algorithm of Yarowsky D. Yarowsky .", "label": "", "metadata": {}, "score": "62.859943"}
{"text": "The global feature perspective is characterized by topical association knowledge , namely , the topical describing information .The Representation of Context for the Ambiguous Term .The representation of ambiguous term in the context space is an important decisive factor for choosing the appropriate sense .", "label": "", "metadata": {}, "score": "62.88349"}
{"text": "In particular , we notice that when the sense distribution ( P1 , P2 ) was very unbalanced ( i.e. 0.9 , 0.1 ) , then the error rate was almost equal to the minority sense proportion .All these findings indicate that the effectiveness of an increase in the sample size is very dependent on the degree of difficulty .", "label": "", "metadata": {}, "score": "62.944862"}
{"text": "Ginter F , Boberg J , Salakoski T , Salakoski T : New Techniques for Disambiguation in Natural Language and Their Application to Biological Text .Journal of Machine Learning Research 2004 , 5 : 605 - 621 .Podowski RM , Cleary JG , Goncharoff NT , Amoutzias G , Hayes WS : AZuRE , a scalable system for automated term disambiguation of gene and protein names .", "label": "", "metadata": {}, "score": "63.00698"}
{"text": "Boosting methods have been crowned as the most accurate available off - the - shelf classifiers on a wide variety of datasets [ 107 ] .But , it is observed that boosting methods are sensitive to noise and outliers , especially for small datasets [ 46 , 56 , 107 ] .", "label": "", "metadata": {}, "score": "63.060226"}
{"text": "To estimate the parameters for a feature 's distribution , one must assume a distribution or generate nonparametric models for the features from the training set .[ 8 ] .The assumptions on distributions of features are called the event model of the Naive Bayes classifier .", "label": "", "metadata": {}, "score": "63.0679"}
{"text": "The i th binary SVM classifier is trained by considering all instances associated with the i th class as positive examples and the others as negative instances .It applies the N classifiers and chooses the one with the highest confidence .", "label": "", "metadata": {}, "score": "63.07076"}
{"text": "The phase evolves ensembles by combining the Pareto front of nondominated solutions instead of the entire population like other studies [ 50 ] .The detection rate for each class is treated as a separate objective .Here , we are interested in those solutions which are noninferior and exhibit classification trade - offs .", "label": "", "metadata": {}, "score": "63.13208"}
{"text": "The modified polynomial mutation operator is used to mutate the offspring 's solutions .The Proposed Evolutionary Approach .This section describes the proposed approach based on multiobjective GA to create a set of base classifiers and ensembles thereof .The proposed approach follows overproduce and choose approach .", "label": "", "metadata": {}, "score": "63.135963"}
{"text": "Out of the 100 instances of depression , 85 instances are tagged with the first sense , and remaining 15 instances are tagged with \" None \" ( i.e. , no instances tagged with a second sense ) , and so it was excluded in this evaluation .", "label": "", "metadata": {}, "score": "63.13921"}
{"text": "The proposed approach involves three phases to create the ensemble as described in Section 3.1 .In phase 1 ( ensemble generation phase ) , AMGA2 optimizes an archive of a diverse set of feature subsets of datasets for predicting the target class .", "label": "", "metadata": {}, "score": "63.19569"}
{"text": "The paper clearly shows that some researchers have applied their knowledge to address different issues at these phases for intrusion detection .But still there is a need to focus more on issues of each phase of ensemble learning .It is expected that new discoveries and a deepened understanding of different techniques suitable for different phases in ensemble learning of ID problem will be the subject of future work .", "label": "", "metadata": {}, "score": "63.224686"}
{"text": "In the biomedical named entity disambiguation , the extracted entity names ( e.g. , gene product names ) will be applied onto a process such that each occurrence should be disambiguated as either gene name or protein name as the same name can refer to a gene or protein .", "label": "", "metadata": {}, "score": "63.243805"}
{"text": "4 , pp .497 - 508 , 2001 .R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .", "label": "", "metadata": {}, "score": "63.26573"}
{"text": "DC represents the horizontal synonyms relation and the vertical hypernyms relation from lower - level concept to upper - level concept .Simultaneously , the glosses can also be available to calculate the semantic similarity .Definition 3 ( topic semantic profile ) .", "label": "", "metadata": {}, "score": "63.282173"}
{"text": "( i )Converting all words to lowercase .( ii ) Removing stopwords : removing all common function words like \" is \" \" the \" \" in \" , ... and so forth .( iii )Performing word stemming using Porter stemming algorithm [ 25 ] .", "label": "", "metadata": {}, "score": "63.367126"}
{"text": "The NLM - WSD corpus contains 50 ambiguous terms with 100 instances for each term for a total of 5000 examples .Each example is basically a Medline abstract containing one or more occurrences of the ambiguous word .The instances of these ambiguous terms were disambiguated by 11 annotators who assigned a sense for each instance [ 24 ] .", "label": "", "metadata": {}, "score": "63.38893"}
{"text": "There are several scenarios where using an ensemble based system makes statistical sense , which are discussed below in detail .For example , we typically ask the opinions of several doctors before agreeing to a medical procedure , we read user reviews before purchasing an item ( particularly big ticket items ) , we evaluate future employees by checking their references , etc .", "label": "", "metadata": {}, "score": "63.416374"}
{"text": "Xiang et al .[36 ] proposed a hierarchical hybrid system involving multiple - level hybrid classifier , which combines the supervised decision tree classifiers and unsupervised Bayesian clustering to detect intrusions .It was able to achieve a higher true positive rate than previously reported in the literature on the original training and test sets of the KDD Cup 99 dataset .", "label": "", "metadata": {}, "score": "63.416904"}
{"text": "The NB classifiers trained using an optimized subset of features formulate the base classifiers for the ensembles .In phase 2 ( ensemble selection phase ) , AMGA2 is again used to create an archive of the ensembles that also exhibit classification trade - offs .", "label": "", "metadata": {}, "score": "63.445473"}
{"text": "They claimed 94.71 % detection accuracy with 3.8 % of false alarm rate of old as well as new attacks .Chen et al .[ 41 ] suggested a hybrid flexible neural - tree - based IDS based on flexible neural tree , evolutionary algorithm , and particle swarm optimization ( PSO ) .", "label": "", "metadata": {}, "score": "63.462585"}
{"text": "5.3 Non - Binary Features .We have assumed here that each feature is binary , i.e. that each input either has a feature or does not .Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \" color - is - red \" .", "label": "", "metadata": {}, "score": "63.510834"}
{"text": "Combination Level .This level focuses on ensemble integration phase of ensemble learning process .Here , predictions of base classifiers are combined in some way to improve the performance of ensemble .The researchers proposed that there are three main ways in combining classifiers , namely , fusion , selection , and Mixture of expert systems [ 33 ] .", "label": "", "metadata": {}, "score": "63.56224"}
{"text": "When the amount of training data is too large to make a single classifier training difficult , the data can be strategically partitioned into smaller subsets .Each partition can then be used to train a separate classifier which can then be combined using an appropriate combination rule ( see below for different combination rules ) .", "label": "", "metadata": {}, "score": "63.57402"}
{"text": "For example , decision trees can be very effective at capturing phylogeny trees .However , decision trees also have a few disadvantages .One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small .", "label": "", "metadata": {}, "score": "63.578743"}
{"text": "The test set serves in our final evaluation of the system .For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set .The division of the corpus data into different subsets is shown in 1.3 .", "label": "", "metadata": {}, "score": "63.58718"}
{"text": "It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value .In order to build a decision stump , we must first decide which feature should be used .", "label": "", "metadata": {}, "score": "63.66538"}
{"text": "The diversity in ensembles refers to different errors made by different base classifiers on data records .In order to produce diverse classifiers , researchers used two types of methods : ( 1 ) Implicit ; ( 2 ) Explicit [ 47 , 49 ] .", "label": "", "metadata": {}, "score": "63.681908"}
{"text": "The values in chromosome represent the features of the dataset to be given as input to NB classifiers .The size of chromosomes is equal to the number of features of a dataset under consideration .Each chromosome represents a subset of features of a dataset .", "label": "", "metadata": {}, "score": "63.713516"}
{"text": "We used . , and the window size is 5 .The accuracy results of this first evaluation ( EV1 ) are shown in Table 4 .The detailed results of this evaluation are included in Table 5 .In the second evaluation ( EV2 ) and third evaluation ( EV3 ) , we changed the parameter and the word / features selection formula .", "label": "", "metadata": {}, "score": "63.728745"}
{"text": "The different methods cited in the above section can be summarized in Table 1 .Fusion is the simples and popular method to combine different base classifiers .This method works on the assumption that all base classifiers are of same importance but practically it may not be true , whereas in selection method , generally , only one classifier is chosen to label an unclassified instance .", "label": "", "metadata": {}, "score": "63.741665"}
{"text": "On the other hand , the purpose of modular systems is to break down a complex problem into several subproblems so that each learning algorithm either solves a different task or trained by different training set .The taxonomy proposed by Sharkey [ 63 ] was further extended by Rokach [ 64 ] .", "label": "", "metadata": {}, "score": "63.750233"}
{"text": "Troika combines base classifiers in three stages : specialists level , metaclassifiers , and super classifier .In order to conclude which ensemble performs best over multiple datasets , they followed the procedure proposed in [ 118 ] .Wang et al .", "label": "", "metadata": {}, "score": "63.794395"}
{"text": "Biomedical WSD ( NLM - WSD ) .Dataset We used the benchmark dataset NLM - WSD for biomedical word sense disambiguation [ 24 ] .This dataset was created as a unified and benchmark set of ambiguous medical terms that have been reviewed and disambiguated by reviewers from the field .", "label": "", "metadata": {}, "score": "63.890755"}
{"text": "Results for BSA data set .Annotation of the table : Dist : Distribution of senses ; S. Size : sample size ; Err .Rate : Error Rate ; SE : Standard Error of error rates ; CV : cross - validation ; .", "label": "", "metadata": {}, "score": "63.90474"}
{"text": "These approaches based on the external resource usually have lower performance than the machine learning ways , but they have the advantage of a higher precision rate and a wider coverage .These approaches are overly dependent on the knowledge completeness and richness .", "label": "", "metadata": {}, "score": "63.950836"}
{"text": "In one instance , c - myc might refer to a human gene , while in another instance it refers to a mouse gene .For example , in Table 3 , the biomedical entity name BCL-2 ( a protein name ) in the first text ( no . 1 ) is human while in the second one is a mouse protein .", "label": "", "metadata": {}, "score": "64.001396"}
{"text": "J. McCumber , \" Information system security : a comprehensive model , \" in Proceedings of the 14th National Computer Security Conference , Baltimore , Md , USA , 1991 .W. Khreich , E. Granger , A. Miri , and R. Sabourin , \" Iterative Boolean combination of classifiers in the ROC space : an application to anomaly detection with HMMs , \" Pattern Recognition , vol .", "label": "", "metadata": {}, "score": "64.04493"}
{"text": "Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) .", "label": "", "metadata": {}, "score": "64.047226"}
{"text": "The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres .What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training .", "label": "", "metadata": {}, "score": "64.065475"}
{"text": "View at Google Scholar \u00b7 View at Scopus . D. Corne , N. Jerram , J. Knowles , et al . , \" Pesa - ii : regionbased selection in evolutionary multiobjective optimization , \" in Proceedings of the Genetic and Evolutionary Computation Conference ( GECCO ' 01 ) , Citeseer , 2001 .", "label": "", "metadata": {}, "score": "64.0665"}
{"text": "Another benefit of bagging methods is that they are parallel in nature in both the training and classification phases , whereas the boosting method is sequential in nature [ 33 ] .AI Based Ensembles for ID .Many researchers employed AI - based ensembles and hybrid approaches to improve performance of IDS .", "label": "", "metadata": {}, "score": "64.09849"}
{"text": "[ 22 ] .Sabhnani and Serpen [ 23 ] proposed a multi - classifier approach to detect intrusions .They utilized different classifiers , namely , an ANN , k - means clustering , and a Gaussian classifier to classify different classes of intrusions by using KDD 1999 dataset .", "label": "", "metadata": {}, "score": "64.10795"}
{"text": "On the other hand , if the input values have a wide variety of labels , then there are many labels with a \" medium \" frequency , where neither P(l ) nor log 2P(l ) is small , so the entropy is high .", "label": "", "metadata": {}, "score": "64.11441"}
{"text": "497 - 501 , 1995 .L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" Fuzzy Sets and Systems , L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" vol .", "label": "", "metadata": {}, "score": "64.160286"}
{"text": "A chromosome encoding scheme is proposed to represent the individual classifiers .Furthermore , the proposed approach is used to find an improved Pareto front consisting of ensemble solutions .The multiobjective GA used in this paper is archive based microgenetic algorithm 2 ( AMGA2 ) [ 48 ] , which is an effective algorithm for finding optimal trade - offs for multiple criteria .", "label": "", "metadata": {}, "score": "64.19905"}
{"text": "The generated ensembles can be used to detect the intrusions accurately .For intrusion detection problem , the proposed approach could consider conflicting objectives simultaneously like detection rate of each attack class , error rate , accuracy , diversity , and so forth .", "label": "", "metadata": {}, "score": "64.23567"}
{"text": "7411 - 7419 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . D. Parrott , X. Li , and V. Ciesielski , \" Multi - objective techniques in genetic programming for evolving classifiers , \" in Proceedings of the IEEE Congress on Evolutionary Computation ( CEC ' 05 ) , pp .", "label": "", "metadata": {}, "score": "64.26886"}
{"text": "View Article PubMed .Gaudan S , Krisch H , Rebholz - Schuhmann D : Resolving abbreviations to their senses in Medline .Bioinformatics 2005 , 21 : 3658 - 3664 .View Article PubMed .Schuemie MJ , Kors JA , Mons B : Word sense disambiguation in the biomedical domain : an overview .", "label": "", "metadata": {}, "score": "64.386795"}
{"text": "The IDSs are evaluated by comparing the true - positive rate ( i.e. , the percentage of attacks that were correctly recognized ) and the false - positive rate ( i.e. , the percentage of legitimate traffic flagged as an attack ) .", "label": "", "metadata": {}, "score": "64.396576"}
{"text": "The predictions are combined by a majority vote .The performance of RF is comparable to AdaBoost , but is more robust to noise [ 57 , 95 ] .( iv ) Boosting .Boosting [ 96 ] is popular meta - algorithm for generating ensemble [ 33 ] .", "label": "", "metadata": {}, "score": "64.45332"}
{"text": "The methodology is easy to apply , it provides a principled way of studying the effects of the different factors on the error rate , and since it is based on a strong theoretical foundation , it guarantees that the results to apply to all abbreviations with similar characteristics .", "label": "", "metadata": {}, "score": "64.4717"}
{"text": "Now , assume that each classifier has a probability p of making a correct decision .Weighted majority voting .Other combination rules .There are several other combination rules , which are arguably more sophisticated than the ones listed above .", "label": "", "metadata": {}, "score": "64.52707"}
{"text": "In our study , we proposed a simple \" full - term substitution \" method , which is described in more detail in the Methods section , to automatically generate training data , but this is only applicable for abbreviations .In this study , we used a \" full - form substitution \" method to automatically generate the data set for the experiments , which is an artificial training set .", "label": "", "metadata": {}, "score": "64.539536"}
{"text": "These metrics depend on the ordering of the cases , not the actual predicted values .As long as ordering is preserved , it makes no difference .These metrics measure how well the attack instances are ordered before normal instances and can be viewed as a summary of model performance across all possible thresholds .", "label": "", "metadata": {}, "score": "64.64678"}
{"text": "The evaluation results proved the competitiveness of the proposed approach as it outperforms some recently published techniques including supervised techniques .Related Work .In the biomedical domain , the applications of text mining and machine learning techniques were quite successful and encouraging [ 6 ] .", "label": "", "metadata": {}, "score": "64.64903"}
{"text": "In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high .In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest .", "label": "", "metadata": {}, "score": "64.671936"}
{"text": "Here , the authors addressed the problem related to the presence of noise ( i.e. , attacks ) in the training set .The proposed model composed of a set of ( independent ) application - specific modules .Each module , composed by multiple HMM ensembles , is trained using queries on a specific web application and , during the operational phase , outputs a probability value for each query on this web application .", "label": "", "metadata": {}, "score": "64.69783"}
{"text": "Here , GA has produced a set of solutions in Pareto front in a single run without incorporating any domain knowledge or any other heuristics about the problem .A comprehensive review of various multiobjective GAs can be further referred to in [ 43 , 47 , 49 ] .", "label": "", "metadata": {}, "score": "64.826126"}
{"text": "[40 ] suggested a hybrid of SVM and clustering to cut down the training time .A hierarchical clustering algorithm is engaged to establish boundary points in the data that best separate the two classes .These boundary points are used to train the SVM .", "label": "", "metadata": {}, "score": "64.86304"}
{"text": "View at Publisher \u00b7 View at Google Scholar .V. Engen , Machine learning for network based intrusion detection [ Ph.D. thesis ] , Bournemouth University , June 2010 .G. D. Guvenir , \" Classification by voting feature intervals , \" in Proceedings of the European Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "64.883965"}
{"text": "In the first layer , there are five ANFIS modules which are trained to explore the intrusive activity from the input data .Each ANFIS module belongs to one of the classes in the dataset each providing an output which specifies the degree of relativity of the data to the specific class .", "label": "", "metadata": {}, "score": "64.90778"}
{"text": "44 - 51 , 2010 .View at Google Scholar .G. Kumar and K. Kumar , \" An information theoretic approach for feature selection , \" Security and Communication Networks , vol .5 , pp .178 - 185 , 2012 .", "label": "", "metadata": {}, "score": "64.91521"}
{"text": "Here , each member is trained by the same dataset with all features .To determine final prediction of ensemble , the combiner applies some method to combine the predictions of ensemble members in certain way to get final ensemble prediction , for example , average or majority vote ( most popular ) method .", "label": "", "metadata": {}, "score": "64.923256"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. K. Savova , A. R. Coden , I. L. Sominsky et al . , \" Word sense disambiguation across two domains : biomedical literature and clinical notes , \" Journal of Biomedical Informatics , vol .", "label": "", "metadata": {}, "score": "64.92474"}
{"text": "In order to test the proposed approach , test dataset is directly fed to different base classifiers .Their predictions are combined in this phase to give the final output of the ensemble .In this work , we computed the final prediction of ensemble by using the majority voting method because of its popularity as depicted in Figure 3 . integration of predictions of the base classifiers to get a final prediction of the ensemble .", "label": "", "metadata": {}, "score": "64.964325"}
{"text": "Discussion and Conclusion .The main weakness of the supervised and machine - learning - based methods for WSD is their dependency on the annotated training text which includes manually disambiguated instances of the ambiguous word [ 2 , 17 ] .", "label": "", "metadata": {}, "score": "65.016464"}
{"text": "918 - 924 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. N. Toosi and M. Kahani , \" A new approach to intrusion detection based on an evolutionary soft computing model using neuro - fuzzy classifiers , \" Computer Communications , vol .", "label": "", "metadata": {}, "score": "65.03213"}
{"text": "Use the dev - test set to check your progress .Once you are satisfied with your classifier , check its final performance on the test set .How does the performance on the test set compare to the performance on the dev - test set ?", "label": "", "metadata": {}, "score": "65.08665"}
{"text": "Translation .In Machine Translation of Languages : Fourteen Essays , ed . by Locke , W.N. and Booth , A.D. Cambridge , MA : MIT Press .Computer Recognition of English Word Senses , Amsterdam : North - Holland .Word sense disambiguation using conceptual density .", "label": "", "metadata": {}, "score": "65.14198"}
{"text": "347 - 352 , 1993 .L. Xu , A. Krzyzak , and C.Y. Suen , Methods for combining multiple classifiers and their applications to handwriting recognition , IEEE Trans . on Systems , Man , and Cyb . , Vol .", "label": "", "metadata": {}, "score": "65.160416"}
{"text": "349 - 358 , Cambridge , UK , 2001 .M. Sewell , \" Ensemble Learning , \" Research Note RN/11/02 , UCL department of computer science , 2011 . E. Mayoraz and M. Moreira , \" On the decomposition of polychotomies into dichotomies , \" in Proceedings of the XIV International Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "65.19055"}
{"text": "These solutions formulate the base classifiers as candidate solutions for the ensemble generation in phase 2 .In phase 1 , AMGA2 is real coded using its crossover and mutation operators .The values in chromosome and its size depend upon the type of base classifier and corresponding encoding scheme .", "label": "", "metadata": {}, "score": "65.20525"}
{"text": "Second challenge to tackle in AI - based ensembles is the enormous amount of audit data that make it difficult to build effective IDS .The processing of enormous amount of data increases computational overhead and causes delay in detection of intrusions .", "label": "", "metadata": {}, "score": "65.21647"}
{"text": "It 's common to start with a \" kitchen sink \" approach , including all the features that you can think of , and then checking to see which features actually are helpful .We take this approach for name gender features in 1.2 .", "label": "", "metadata": {}, "score": "65.226974"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Chen and H. Al - Mubaid , \" Context - based term disambiguation in biomedical literature , \" in Proceedings of the 19th International Florida Artificial Intelligence Research Society Conference ( FLAIRS ' 06 ) , pp .", "label": "", "metadata": {}, "score": "65.28418"}
{"text": "Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data .Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input .", "label": "", "metadata": {}, "score": "65.33046"}
{"text": "173 - 180 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. L. Allwein , R. E. Schapire , and Y. Singer , \" Reducing multiclass to binary : a unifying approach for margin classifiers , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "65.336975"}
{"text": "So , for a particular instance to be classified if all of them have different opinions , then their scores are considered .The classifier having the highest score is declared as winner and used to predict the final output of the ensemble .", "label": "", "metadata": {}, "score": "65.36763"}
{"text": "Several other metrics are utilized by researchers to measure the performance of IDS .These metrics can be divided into three classes : threshold , ranking , and probability metrics [ 7 , 8 ] .Threshold metrics include classification rate ( CR ) , F - measure ( FM ) , and cost per example ( CPE ) .", "label": "", "metadata": {}, "score": "65.38851"}
{"text": "Word sense disambiguation is the task of determining the correct sense of a given word in a given context .In the general language domain , and within natural language processing ( NLP ) , the word sense disambiguation ( WSD ) problem has been studied and investigated extensively over the past few decades [ 1 , 2 ] .", "label": "", "metadata": {}, "score": "65.49571"}
{"text": "The proposed system suffers from limitation of incremental learning .It requires continuous retraining for changing environment .Cretu et al .[113 ] proposed a micromodel - based ensemble of anomaly sensors to sanitize the training data .Here , different models are generated to produce provisional labels for each training input , and models are combined in a voting scheme to determine which parts of the training data may represent attacks .", "label": "", "metadata": {}, "score": "65.4973"}
{"text": "They used fuzzy clustering technique to generate different homogeneous training subsets from heterogeneous training set , which are further used to ANN models as base models .Finally , a metalearner , fuzzy aggregation module , was employed to aggregate these results .", "label": "", "metadata": {}, "score": "65.5659"}
{"text": "Each classifier 's training set is generated by randomly drawing , with replacement , N examples - where N is the size of the original training dataset ; many of the original examples may be repeated in the resulting training set while others may be left out .", "label": "", "metadata": {}, "score": "65.58843"}
{"text": "For BPD , which has three different senses , three different multi - class SVM methods [ 47 ] : \" mc - svm \" , \" one - vs - rest \" , \" one - vs - one \" , were used . \" Mc - svm \" implements the algorithm with a decision function which considers all classes at once , while \" one - vs - rest \" and \" one - ve - one \" are constructed by combining several binary SVM classifiers .", "label": "", "metadata": {}, "score": "65.62201"}
{"text": "The proposed approach consists of encoding of chromosomes that provides an optimized subset of features of a dataset .The optimized feature subset can be furthered to train a diverse set of NB classifiers that formulate base classifiers for ensembles .AMGA2 algorithm is employed to build multiobjective optimization model that generates an optimized subset of features with simultaneous consideration of detection rate of each attack class in the dataset .", "label": "", "metadata": {}, "score": "65.68706"}
{"text": "T. Dietterich and G. Bakiri , \" Error - correcting output codes : a general method for improving multiclass inductive learning programs , \" in Proceedings of the of Santa fe Institute Studies in the Sciences of Complexity , pp .395 - 395 , Citeseer , 1994 .", "label": "", "metadata": {}, "score": "65.71081"}
{"text": "word . sense disambiguation and it is now at a stage where lexical ambiguity in text can be resolved with a reasonable degree of accuracy .first introduced by Warren Weaver in 1949 W. Weaver .In Machine Translation . of .", "label": "", "metadata": {}, "score": "65.732254"}
{"text": "However , we can see that our method 's performance is reasonably well standing in terms of precision , recall , and F1 .The main strength of this method is in using MI values as weights encoded in the feature vectors .", "label": "", "metadata": {}, "score": "65.733826"}
{"text": "Figure 4 shows plots of the error rate versus sample size for each distribution of the BPD data set based on the 5-fold cross validation using the \" one - vs - rest \" algorithm .The plots for the four different sense distributions are very similar and actually agree with results obtained indicating that the effect of the different distributions on the error rate is insignificant .", "label": "", "metadata": {}, "score": "65.77605"}
{"text": "Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs .This test set typically has the same format as the training set .", "label": "", "metadata": {}, "score": "65.8675"}
{"text": "The framework used by supervised classification is shown in 1.1 .Figure 1.1 : Supervised Classification .( a )During training , a feature extractor is used to convert each input value to a feature set .These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section .", "label": "", "metadata": {}, "score": "65.903275"}
{"text": "6.3 Generative vs Conditional Classifiers .An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer .The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair .", "label": "", "metadata": {}, "score": "65.92238"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Xu , M. Markatou , R. Dimova , H. Liu , and C. Friedman , \" Machine learning and word sense disambiguation in the biomedical domain : design and evaluation issues , \" BMC Bioinformatics , vol .", "label": "", "metadata": {}, "score": "65.96577"}
{"text": "In 2000 , the UMLS Metathesaurus [ 6 ] , a comprehensive resource that specifies and categorizes biomedical concepts , contained 9,416 ambiguous terms , and in 2004 , the number increased to 21,295 , an increase of 126 % within 4 years [ 7 ] .", "label": "", "metadata": {}, "score": "65.96954"}
{"text": "For example , consider a classifier that determines the correct word sense for each occurrence of the word bank .If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 .", "label": "", "metadata": {}, "score": "65.98812"}
{"text": "M. P. Perrone and L. N. Cooper , \" When networks disagree : ensemble methods for hybrid neural networks , \" in Artificial Neural Networks for Speech and Vision , R. J. Mammone , Ed . , pp .126 - 142 , Chapman & Hall , London , UK , 1993 .", "label": "", "metadata": {}, "score": "66.02802"}
{"text": "A hybrid strategy can loop back and forth multiple times between methods or can embed one method within another method .In this study , we adopted and presented the taxonomy proposed by Kuncheva [ 46 ] and additional aspects borrowed from Jain et al .", "label": "", "metadata": {}, "score": "66.04117"}
{"text": "These techniques produce better results for the majority attack classes .But , these techniques detect minority classes like U2R and R2L poorly .This proves that NB trained using conventional methods for bagging and boosting is more biased towards the majority attack classes .", "label": "", "metadata": {}, "score": "66.053955"}
{"text": "Researchers have proposed different taxonomies to categorize ensembles .Since research of ensemble is continuously evolving , there is no existing taxonomy that covers every aspect of ensembles .The literature review of important taxonomies is as follows .Jain et al .", "label": "", "metadata": {}, "score": "66.11459"}
{"text": "238 - 247 , 2001 .G. Brown , J. Wyatt , R. Harris , X. Yao , \" Diversity Creation Methods : A Survey and Categorisation , \" Journal of Information Fusion ( Special issue on Diversity in Multiple Classifier Systems ) .", "label": "", "metadata": {}, "score": "66.141014"}
{"text": "10 , pp .2201 - 2212 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Zainal , M. Maarof , S. Shamsuddin , et al . , \" Ensemble classifiers for network intrusion detection system , \" Journal of Information Assurance and Security , vol .", "label": "", "metadata": {}, "score": "66.14902"}
{"text": "AI - based techniques and their ensembles are presently attracting considerable attention from the research community for intrusion detection .Their features , such as flexibility , adaptability , new pattern recognition , fault tolerance , learning capabilities , high computational speed , and error resilience for noisy data , fit the prerequisite of building effective IDS .", "label": "", "metadata": {}, "score": "66.15958"}
{"text": "A novel evolutionary approach for effective intrusion detection is proposed in Section 3 .This section also gives details of experiments including a brief description of GA , NB , benchmark data sets , performance metrics followed by experimental setup , and the results of the proposed approach using NB as a base classifier .", "label": "", "metadata": {}, "score": "66.17722"}
{"text": "[ book review ] , \" IEEE Transactions on Neural Networks , vol .18 , no . 3 , pp .964 - 964 , 2007 .View at Publisher \u00b7 View at Google Scholar . E. M. Dos Santos , Static and dynamic overproduction and selection of classifier ensembles with genetic algorithms [ Ph.D. thesis ] , Montreal , 2008 .", "label": "", "metadata": {}, "score": "66.1971"}
{"text": "View at Google Scholar \u00b7 View at Scopus . Y. Guan , C. L. Myers , D. C. Hess , Z. Barutcuoglu , A. A. Caudy , and O. G. Troyanskaya , \" Predicting gene function in a hierarchical context with an ensemble of classifiers , \" Genome Biology , vol .", "label": "", "metadata": {}, "score": "66.22464"}
{"text": "405 - 410 , 1997 .S. B. Cho and J. H. Kim , \" Combining multiple neural networks by fuzzy integral for robust classification , \" IEEE Transactions on Systems , Man and Cybernetics , vol .25 , no . 2 , pp .", "label": "", "metadata": {}, "score": "66.229126"}
{"text": "The negative sign converts the distance metric into a support value , whose largest value can be zero in case of a perfect match .Note that this output does not match any of the code words exactly , and this is where the error correcting ability of the ECOC lies .", "label": "", "metadata": {}, "score": "66.23667"}
{"text": "Finally , we conduct experiments on the standard data set SemCor to evaluate the performance of the proposed method , and the results indicate that our approach achieves relatively better performance than existing approaches .Introduction .Up to present , diverse WSD methods have been proposed .", "label": "", "metadata": {}, "score": "66.35073"}
{"text": "Value of RMSE lies in range from 0 to 1 .The metric is minimized when the predicted value for each attack class coincides with the true conditional probability of that class being normal class .Generally , these metrics are computed from confusion matrix .", "label": "", "metadata": {}, "score": "66.35783"}
{"text": "241 - 259 , 1992 .T. K. Ho , J. J. Hull , and S. N. Srihari , \" Decision combination in multiple classifier systems , \" IEEE Trans . on Pattern Analy .Machine Intel . , vol .16 , no . 1 , pp .", "label": "", "metadata": {}, "score": "66.457184"}
{"text": "In the statistics and computer science literature , Naive Bayes models are known under a variety of names , including simple Bayes and independence Bayes .[ 1 ] : 482 .Naive Bayes is a simple technique for constructing classifiers : models that assign class labels to problem instances , represented as vectors of feature values , where the class labels are drawn from some finite set .", "label": "", "metadata": {}, "score": "66.45929"}
{"text": "10 , pp .993 - 1001 , 1990 .R. E. Schapire , \" The Strength of Weak Learnability , \" Machine Learning , vol .5 , no . 2 , pp .197 - 227 , 1990 .R. A. Jacobs , M. I. Jordan , S. J. Nowlan , and G. E. Hinton , \" Adaptive mixtures of local ex - perts , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "66.474976"}
{"text": "Details of CID can be further studied in Gu et al .[ 60 ] .Keeping these points in view , we computed TPR , FPR , and CID to evaluate the performance of the proposed technique and compare it with other representative techniques in the field .", "label": "", "metadata": {}, "score": "66.512985"}
{"text": "2212 , pp .85 - 103 , 2001 .View at Google Scholar .P. Garc\u00eda - Teodoro , J. D\u00edaz - Verdejo , G. Maci\u00e1 - Fern\u00e1ndez , and E. V\u00e1zquez , \" Anomaly - based network intrusion detection : techniques , systems and challenges , \" Computers & Security , vol .", "label": "", "metadata": {}, "score": "66.55066"}
{"text": "Abstractly , naive Bayes is a conditional probability model : given a problem instance to be classified , represented by a vector representing some n features ( independent variables ) , it assigns to this instance probabilities .The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values , then basing such a model on probability tables is infeasible .", "label": "", "metadata": {}, "score": "66.61303"}
{"text": "Decision Template Method .The main concept of decision template is to compare a prototypical answer of ensemble for prediction of class of a given instance .The method may use different similarity measure to evaluate the matching between matrix of classifiers output and matrix of templates .", "label": "", "metadata": {}, "score": "66.70187"}
{"text": "It can be observed from the reporting results that AMGA2-NB ( NB trained with the proposed technique ) reported superior performance than NB and its ensembles based on bagging and boosting .AMGA2-NB reported the detection of normal and attack classes up to 95.2 % and 92.7 % , respectively .", "label": "", "metadata": {}, "score": "66.766815"}
{"text": "Algebraic combiners .Algebraic combiners are non - trainable combiners , where continuous valued outputs of classifiers are combined through an algebraic expression , such as minimum , maximum , sum , mean , product , median , etc .Specifically .", "label": "", "metadata": {}, "score": "66.84363"}
{"text": "Affiliated with .Abstract .Background .Automated techniques have been developed that address the WSD problem for a number of text processing situations , but the problem is still a challenging one .Thus , there is a need to explicitly address the factors and to systematically quantify their effects on performance .", "label": "", "metadata": {}, "score": "66.85167"}
{"text": "In the first stage it combines all base classifiers using specialist classifiers which have a dichotomous model .Second stage contains k metaclassifiers which are used to learn the prediction characteristics of the specialist classifiers .Each metaclassifier is in charge of one class only and will combine all the specialist classifiers which are able to classify their own particular class .", "label": "", "metadata": {}, "score": "66.85285"}
{"text": "The estimate of the standard error was then obtained by averaging the above values over the 30 runs .Gene ambiguity for mining MEDLINE .To determine the extent of the gene ambiguity problem in MEDLINE , we searched MEDLINE abstracts to determine how many abstracts contained gene symbols that were ambiguous with general English words or biomedical terms .", "label": "", "metadata": {}, "score": "66.853966"}
{"text": "Given new training data , a new pool of HMMs is generated from newly acquired data using different HMM states and initializations .The responses from these newly trained HMMs are then combined with those of the previously trained HMMs in ROC space using the incremental Boolean combination ( incrBC ) technique .", "label": "", "metadata": {}, "score": "66.856964"}
{"text": "Each combination of labels and features that receives its own parameter is called a joint - feature .Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values .Note .", "label": "", "metadata": {}, "score": "66.87338"}
{"text": "Two recent tutorials written by the current curator of this article also provide a comprehensive overview of ensemble systems ( Polikar 2006 , Polikar 2007 ) .Diversity .The success of an ensemble system - that is , its ability to correct the errors of some of its members - rests squarely on the diversity of the classifiers that make up the ensemble .", "label": "", "metadata": {}, "score": "66.90025"}
{"text": "We begin by selecting the overall best decision stump for the classification task .We then check the accuracy of each of the leaves on the training set .Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf .", "label": "", "metadata": {}, "score": "66.900345"}
{"text": "( 5 )Repeating the above steps till termination criteria are satisfied .A large number of methods have been developed to implement steps for GAs .However , major issues consist of representation of individuals , fitness evaluation mechanism , variation operators of crossover and mutation , and deciding the termination criteria .", "label": "", "metadata": {}, "score": "66.96792"}
{"text": "During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk .", "label": "", "metadata": {}, "score": "66.98504"}
{"text": "Dietterich and Bakiri introduced ECOC to be used within the ensemble setting ( Dietterich 1995 ) .The idea is to use a different class encoding for each member of the ensemble .The encodings constitute a binary C by T code matrix , where C and T are the number of classes and ensemble size , respectively , combined by the minimum Hamming distance rule .", "label": "", "metadata": {}, "score": "67.00929"}
{"text": "They used the weighted voting method to compute the final prediction of the system .However , the models developed based on these techniques attempted to obtain a single solution .They have a lack in providing classification trade - offs for application specific requirements .", "label": "", "metadata": {}, "score": "67.014336"}
{"text": "M. Y. Su , K. C. Chang , H. F. Wei , and C. Y. Lin , \" Feature weighting and selection for a real - time network intrusion detection system based on GA with KNN , \" Intelligence and Security Informatics , vol .", "label": "", "metadata": {}, "score": "67.01463"}
{"text": "Final ensemble prediction is the weighted voting of base classifiers .They empirically proved that by assigning proper weights to classifiers in ensemble approach improves the detection accuracy of all classes of network traffic than individual classifier .Menahem et al .", "label": "", "metadata": {}, "score": "67.08539"}
{"text": "As the Friedman 's test indicated , the effect of the sense distribution on the error rate is significant .When the sense distribution is ( 0.5 , 0.5 ) there are statistically significant differences between the pairs of error rates produced under sample size ( 20 and 120 ) , the sample sizes ( 40 and 120 ) and the sample sizes ( 80 and 120 ) .", "label": "", "metadata": {}, "score": "67.09494"}
{"text": "To solve this problem , Gu et al .[ 60 ] proposed a new objective metric called intrusion detection capability ( CID ) considering base rate , TPR , and FPR collectively .CID possesses many important features .For example , . it naturally takes into account all the important aspects of detection capability , that is , FPR , FNR , positive predictive value ( PPV ) [ 19 ] , negative predictive value ( NPV ) , and base rate ( the probability of intrusions ) ; .", "label": "", "metadata": {}, "score": "67.12198"}
{"text": "In this way the base classifier focuses on the hardest instances .Then the boosting algorithm combines the base rules taking a weighted majority vote of the base classifiers which are based on the accuracy of the classifiers [ 46 ] .", "label": "", "metadata": {}, "score": "67.18761"}
{"text": "In many practical applications , parameter estimation for naive Bayes models uses the method of maximum likelihood ; in other words , one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods .Despite their naive design and apparently oversimplified assumptions , naive Bayes classifiers have worked quite well in many complex real - world situations .", "label": "", "metadata": {}, "score": "67.198"}
{"text": "R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .4 , pp .59 - 72 , 2007 .", "label": "", "metadata": {}, "score": "67.228134"}
{"text": "170 - 174 , ACM , July 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .N. Chawla , \" C4 .5 and imbalanced data sets : investigating the effect of sampling method , probabilistic estimate , and decision tree structure , \" in Proceedings of the ICML Workshop on Learning from Imbalanced Datasets II , 2003 .", "label": "", "metadata": {}, "score": "67.25798"}
{"text": "As was mentioned before , there are several methods for identifying the most informative feature for a decision stump .One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature .", "label": "", "metadata": {}, "score": "67.3016"}
{"text": "For each combination of error rates we have a sample of 30 observations .To exemplify , assume the pair consisted of the error rates obtained under sample size 20 and 40 .Then the set of observations was comprised of those error rates obtained from the 30 simulation runs .", "label": "", "metadata": {}, "score": "67.355415"}
{"text": "[ 27 ] studied the SVM , ANNs ( artificial neural networks ) , LGPs ( linear genetic programs ) , and MARSs ( Multivariate Adaptive Regression Splines ) for classification of KDD dataset into five classes .As these classifiers obtained better performance over the others to detect different classes of intrusion in terms of detection accuracy , attack severity , training & testing time ( scalability ) .", "label": "", "metadata": {}, "score": "67.372955"}
{"text": "View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. G. Dietterich , \" Ensemble methods in machine learning , \" in Proceedings of the Multiple Classifier Systems .First International Workshop ( MCS ' 00 ) , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "67.38457"}
{"text": "While in bagging each instance is drawn with equal probability from the available training dataset , in wagging each instance is extracted according to a weight stochastically assigned .( iii ) Random Forest ( RF ) .This method is a version of bagging which comprised of decision trees ( DTs ) [ 95 ] .", "label": "", "metadata": {}, "score": "67.44774"}
{"text": "5 , pp . 1651 - 1686 , 1998 .Y. S. Huang and C. Y. Suen , \" Behavior - knowledge space method for combination of mul - tiple classifiers , \" Proc . of IEEE Computer Vision and Pattern Recog .", "label": "", "metadata": {}, "score": "67.461205"}
{"text": "B. L. Humphreys , D. A. B. Lindberg , H. M. Schoolman , and G. O. Barnett , \" The unified medical language system : an informatics research collaboration , \" Journal of the American Medical Informatics Association , vol .5 , no . 1 , pp . 1 - 11 , 1998 .", "label": "", "metadata": {}, "score": "67.52367"}
{"text": "4 , pp .507 - 521 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Google Scholar .Z. S. Pan , S. C. Chen , G. B. Hu , and D. Q. Zhang , \" Hybrid neural network and C4.5 for misuse detection , \" in Proceedings of the International Conference on Machine Learning and Cybernetics , pp .", "label": "", "metadata": {}, "score": "67.6252"}
{"text": "Both of these methods differ in their function evaluation .The former study proposed to optimize the classifiers by minimizing the aggregated error of each class and maximizing diversity among them .Since the error on each class is not treated as separate objective ; this is similar to a general error measure such as mean square error MSE , which has the same issues as the implementation of Parrot function , being biased towards the major class(es ) .", "label": "", "metadata": {}, "score": "67.69936"}
{"text": "79 - 87 , 1991 .M. J. Jordan and R. A. Jacobs , \" Hierarchical mixtures of experts and the EM algorithm , \" Neural Computation , vol .6 , no . 2 , pp .181 - 214 , 1994 . D. H. Wolpert , \" Stacked generalization , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "67.71097"}
{"text": "Similar to bagging , boosting also creates an ensemble of classifiers by resampling the data , which are then combined by majority voting .However , in boosting , resampling is strategically geared to provide the most informative training data for each consecutive classifier .", "label": "", "metadata": {}, "score": "67.77638"}
{"text": "However , conditional models can not be used to answer the remaining questions 3 - 6 .However , this additional power comes at a price .Because the model is more powerful , it has more \" free parameters \" which need to be learned .", "label": "", "metadata": {}, "score": "67.832245"}
{"text": "If a vast majority of the classifiers agree with their decisions , such an outcome can be interpreted as the ensemble having high confidence in its decision .If , however , half the classifiers make one decision and the other half make a different decision , this can be interpreted as the ensemble having low confidence in its decision .", "label": "", "metadata": {}, "score": "67.87195"}
{"text": "The reason may be either class imbalance in training dataset or 11 attack types in these two classes only appear in the test dataset , not the training set , and they constitute more than 50 % of the data .However , ensembles utilized the combined knowledge of multiple classifiers to improve the performance in these minority attacks classes .", "label": "", "metadata": {}, "score": "67.88599"}
{"text": "418 - 435 , 1992 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Domingos and M. Pazzani , \" On the optimality of the simple Bayesian classifier under zero - one loss , \" Machine Learning , vol .", "label": "", "metadata": {}, "score": "67.96753"}
{"text": "The AI based techniques have reported certain improvements in the results to detect the intrusions .Many researchers analyzed various AI based techniques empirically and compared their performance for detection of intrusions .Findings of representative empirical comparative analysis are as follows .", "label": "", "metadata": {}, "score": "68.03039"}
{"text": "24 , no . 2 , pp .123 - 140 , 1996 .Y. Freund and R. E. Schapire , \" Decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "68.08106"}
{"text": "[42 ] proposed a 3-tier hybrid approach to detect intrusions .First tier of system is a signature - based approach to filter the known attacks using black list concept .Second tier of system is anomaly detector that uses the white list concept to distinguish the normal and attack traffic that has by passed first tier .", "label": "", "metadata": {}, "score": "68.08113"}
{"text": "In order to validate the evaluation results of IDS on a simulated dataset , one has to develop a methodology to quantify the similarity of simulated and real network traces .KDD cup 1999 dataset and its original form possess some special features , such as huge volume , high dimension , and highly skewed data distribution .", "label": "", "metadata": {}, "score": "68.11105"}
{"text": "View at Publisher \u00b7 View at Google Scholar .M. Sabhnani and G. Serpen , \" Application of machine learning algorithms to KDD intrusion detection dataset within misuse detection context , \" in Proceedings of the International Conference on Machine Learning ; Models , Technologies and Applications ( MLMTA ' 03 ) , pp .", "label": "", "metadata": {}, "score": "68.213806"}
{"text": "The information gain is then equal to the original entropy minus this new , reduced entropy .The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain .", "label": "", "metadata": {}, "score": "68.24651"}
{"text": "In biomedical text , named entities , like gene name , are used the same way irrespective of the species of the entity .As a result , it will be difficult to extract relevant medical information automatically from texts using information extraction system .", "label": "", "metadata": {}, "score": "68.27525"}
{"text": "Selection is guaranteed by design to give at least the same training accuracy as the best individual classifier .However , the model might overtrain , giving a deceptively low training error .To guard against overtraining we may use confidence intervals and nominate a classifier only when it is significantly better than the others [ 46 ] .", "label": "", "metadata": {}, "score": "68.36398"}
{"text": "The poor results may be due to an imbalance of instances of a specific class(es ) or the inability of techniques to represent a correct hypothesis of the problem based on available training data .Moreover , several theoretical and empirical reasons including statistical , representational , and computational reasons exist that also advocate the use of ensemble based techniques over the single techniques [ 12 ] .", "label": "", "metadata": {}, "score": "68.39783"}
{"text": "( 2 ) Some times more than a single training set is available , each collected at a different time or in a different environment .These training sets may even use different features .( 3 ) Different classifiers trained on the same data may not only differ in their global performances , but they also may show strong local differences .", "label": "", "metadata": {}, "score": "68.426956"}
{"text": "511 - 525 , 2005 .View at Google Scholar .G. Kumar and K. Kumar , \" The use of artificial - intelligence - based ensembles for intrusion detection : a review , \" Applied Computational Intelligence and Soft Computing , vol .", "label": "", "metadata": {}, "score": "68.45166"}
{"text": "The modules are network to monitor , Data collection & storage unit , Data analysis & processing unit , and signal [ 4 , 5 ] as depicted in Figure 1 .Based upon these modules , IDSs can be categorized into different classes like host - based IDS ( HIDS ) versus network - based IDS ( NIDS ) , misuse- or signature - based IDS versus anomaly - based IDS , Passive IDS versus Active IDS , and so forth [ 5 ] .", "label": "", "metadata": {}, "score": "68.488174"}
{"text": "To evaluate the proposed approach , it is implemented in VC++ .NB is used as a base classifier as per finding of state - of - the - art literature in the field of ID .The performance of the proposed approach is evaluated based on benchmark datasets for ID , namely , KDD cup 1999 and ISCX 2012 dataset .", "label": "", "metadata": {}, "score": "68.49271"}
{"text": "Definition 5 ( the reoccurrence topic span of TDT ) .When a sentence is a basic processing unit , the reoccurrence topic span ( TS ) of TDT is defined as text spans for expressing the particular topical meaning , which starts form the first occurrence of TDT and ends to the last occurrence TDT .", "label": "", "metadata": {}, "score": "68.55424"}
{"text": "4 , pp .361 - 377 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. C. Tan , D. Gilbert , and Y. Deville , \" Multi - class protein fold classification using a New Ensemble Machine Learning Approach , \" Genome Informatics , vol .", "label": "", "metadata": {}, "score": "68.56678"}
{"text": "Decision lists [ 14 ] are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .The learning proceeds in this way until all corpus instances are classified .", "label": "", "metadata": {}, "score": "68.61809"}
{"text": "The algorithms described above have their built in combination rules , such as simple majority voting for bagging , weighted majority voting for AdaBoost , a separate classifier for stacking , etc .However , an ensemble of classifiers can be trained simply on different subsets of the training data , different parameters of the classifiers , or even with different subsets of features as in random subspace models .", "label": "", "metadata": {}, "score": "68.630936"}
{"text": "The link between the two can be seen by observing that the decision function for naive Bayes ( in the binary case ) can be rewritten as \" predict class if the odds of exceed those of \" .Expressing this in log - space gives : .", "label": "", "metadata": {}, "score": "68.68245"}
{"text": "Most widely used metrics by the intrusion detection research community are true positive rate ( TPR ) and false positive rate ( FPR ) .Based upon values of these two metrics only , it is very difficult to determine best IDS among different IDSs .", "label": "", "metadata": {}, "score": "68.69217"}
{"text": "The performance variation of various classifiers for different intrusions may be described by two aspects .The first aspect is the different design principles of classifiers which work to optimize different parameters .For example , SVM is designed to minimize structural risk based upon statistical theory whereas ANN to minimize empirical risk in which classification function is derived by minimizing the mean square error over the training dataset .", "label": "", "metadata": {}, "score": "68.76549"}
{"text": "View at Scopus .W. Leigh , R. Purvis , and J. M. Ragusa , \" Forecasting the NYSE composite index with technical analysis , pattern recognizer , neural network , and genetic algorithm : a case study in romantic decision support , \" Decision Support Systems , vol .", "label": "", "metadata": {}, "score": "68.828064"}
{"text": "Bayes networks are one of the most widely used graphical models to represent and handle uncertain information [ 51 , 52 ] .Generally , Bayes networks are described by two components : graphical component and numerical component .( i )", "label": "", "metadata": {}, "score": "68.83653"}
{"text": "The classifiers are generated by using different feature subset of training dataset .They proved empirically that result of the proposed method is improved .They performed experiments by using 41 features and 12 features of KDD dataset .They reported 98.39 % , 98.75 % , 99.70 % , and 99.09 % detection of Probe , DoS , U2R , and R2L attack classes using 41 features of KDD dataset .", "label": "", "metadata": {}, "score": "68.85414"}
{"text": "Thresholds are fixed independently for each application - specific module .Perdisci et al .[ 88 ] proposed a clustering - based fusion module to combine multiple alarms that help to reduce the volume of alarms produced by IDSs .The produced meta - alarms provide the system administrator with a concise high - level description of the attack .", "label": "", "metadata": {}, "score": "68.91492"}
{"text": "Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987 are then used to make generalisations based on the corpus instances classified so far and these lists are then re - applied to the corpus to classify more instances .", "label": "", "metadata": {}, "score": "68.98595"}
{"text": "I. H. Witten and E. Frank , Data Mining : Practical Machine Learning Tools and Techniques , The Morgan Kaufmann Series in Data Management Systems , Morgan Kaufmann , San Francisco , Calif , USA , 2nd edition , 2005 .C. M. Bishop , Pattern Recognition and Machine Learning , Information Science and Statistics , Springer , New York , NY , USA , 2006 .", "label": "", "metadata": {}, "score": "69.0324"}
{"text": "In case of a tie , the winner is randomly chosen .The multiobjective GA method discussed in phase 1 is again applied in phase 2 .Here , multiobjective GA is real coded having values from 0 to 1 .Value . 0.5 signifies nonparticipation concerned base classifiers in creating the ensembles .", "label": "", "metadata": {}, "score": "69.039764"}
{"text": "101 - 115 , 2008 .View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. N. Toosi and M. Kahani , \" A new approach to intrusion detection based on an evolutionary soft computing model using neuro - fuzzy classifiers , \" Computer Communications , vol .", "label": "", "metadata": {}, "score": "69.08282"}
{"text": "This figure shows the plots of \" error rate \" versus \" sample size \" for BSA , RSV and PCA data sets with fixed distribution of \" ( 0.5 , 0.5 ) \" using 5-fold cross validation .Discussion .", "label": "", "metadata": {}, "score": "69.086624"}
{"text": "Most of attacks are likely to generate multiple related alarms .This flood of mostly false alarms makes it very difficult to identify the hidden true positives ( i.e. , those alarms that correctly flag attacks ) [17 ] .Current intrusion - detection systems do not make it easy for network administrator to logically group related alerts .", "label": "", "metadata": {}, "score": "69.12053"}
{"text": "It applies these N(N-1)/2 SVM classifiers and the class assignment is determined by a voting strategy ( e.g. the class chose by the maximum number of SVM classifiers wins ) .The performance was measured using both a 5-fold and a 10-fold cross - validation method .", "label": "", "metadata": {}, "score": "69.196976"}
{"text": "Output code method works by manipulating the coding of classes in multiclass classification problems .Here ensembles are designed to partially correct errors performed by the base classifiers by exploiting the redundancy in the bit - string representation of the classes [ 25 , 105 ] .", "label": "", "metadata": {}, "score": "69.211716"}
{"text": "The value of threshold metrics lies in range from 0 to 1 .Ranking metrics include false - positive rate ( FPR ) , detection rate ( DR ) , precision ( PR ) , and area under ROC curve ( ROC ) .", "label": "", "metadata": {}, "score": "69.25176"}
{"text": "The combination of the classifiers is then based on the given instance : the classifier trained with data closest to the vicinity of the instance , according to some distance metric , is given the highest credit .One or more local experts can be nominated to make the decision ( Jacobs 1991 , Woods 1997 , Alpaydin 1996 , Giacinto 2001 ) .", "label": "", "metadata": {}, "score": "69.27367"}
{"text": "10 , pp .2201 - 2212 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .L. Khan , M. Awad , and B. Thuraisingham , \" A new intrusion detection system using support vector machines and hierarchical clustering , \" The International Journal on Very Large Data Bases , vol .", "label": "", "metadata": {}, "score": "69.305855"}
{"text": "918 - 924 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .N. B. Anuar , H. Sallehudin , A. Gani , and O. Zakari , \" Identifying false alarm for network intrusion detection system using hybrid data mining and decision tree , \" Malaysian Journal of Computer Science , vol .", "label": "", "metadata": {}, "score": "69.350174"}
{"text": "1381 - 1394 , 2012 .View at Publisher \u00b7 View at Google Scholar .K. Y. Fung , C. K. Kwong , K. W. M. Siu , and K. M. Yu , \" A multi - objective genetic algorithm approach to rule mining for affective product design , \" Expert Systems with Applications , vol .", "label": "", "metadata": {}, "score": "69.370735"}
{"text": "Bishop [ 67 ] proposed five methods for combining the individual classifiers .The methods are Bayesian model averaging , committees , boosting , tree - based models , and conditional mixture models .Boosting is further divided into two types : ( 1 ) minimizing exponential error ; ( 2 ) error functions for boosting .", "label": "", "metadata": {}, "score": "69.37276"}
{"text": "All the solutions in this set are noninferior to any other solutions when all objectives are considered .In fact , evolutionary algorithms especially multiobjective GA attempt to optimize each individual objective to a maximum extent .Thus , considering the multiple criteria of the intrusion detection problem , GAs can be used in two ways .", "label": "", "metadata": {}, "score": "69.390945"}
{"text": "59 - 72 , 2007 .R. Polikar , \" Ensemble based systems in decision making , \" IEEE Circuits and Systems Magazine , vol .6 , no.3 , pp .21 - 45 , 2006 .Muhlbaier M. , Topalis A. , Polikar R. , \" Ensemble confidence estimates posterior probability , \" 6th Int .", "label": "", "metadata": {}, "score": "69.55586"}
{"text": "Figure 8 shows a particular code matrix for a 5-class problem that uses 15 encodings .This encoding , suggested in ( Dietterich 1995 ) , is a ( pseudo ) exhaustive coding because it includes all possible non - trivial and non - repeating codes .", "label": "", "metadata": {}, "score": "69.55829"}
{"text": "Iteratively , support vectors are calculated and the SVM is tested against a stopping criterion to determine if a desirable threshold of accuracy has been achieved .Otherwise the iterative process continues .The authors reported 91 % , 97 % , 23 % , and 43 % detection of Probe , DoS , U2R , and R2L attack classes .", "label": "", "metadata": {}, "score": "69.56004"}
{"text": "The IBC technique was further improved as incremental Boolean combination ( incrBC ) by the authors [ 14 ] .The incrBC is a ROC - based system to efficiently adapt ensemble of HMM ( EoHMMs ) over time , from new training data , according to a learn - and - combine approach without multiple iterations .", "label": "", "metadata": {}, "score": "69.61683"}
{"text": "Ensemble integration involves the combination of predictions of set of base classifiers selected in ensemble selection phase .It can use two different methods : ( 1 ) combination ( also called fusion ) ; or ( 2 ) selection [ 46 ] .", "label": "", "metadata": {}, "score": "69.6225"}
{"text": "262 - 294 , 2000 .View at Google Scholar .G. Kumar , K. Kumar , and M. Sachdeva , \" An empirical comparative analysis of feature reduction methods for intrusion detection , \" International Journal of Information and Telecommunication , vol .", "label": "", "metadata": {}, "score": "69.69295"}
{"text": "One solution to this problem is to stop dividing nodes once the amount of training data becomes too small .Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test .", "label": "", "metadata": {}, "score": "69.69557"}
{"text": "Conclusion .In this paper , we aimed to further an understanding of the different factors affecting the performance of ML techniques for WSD by systematically simulating a variety of situations where different sample size , sense distribution , degree of difficulty , and cross validation methods were used .", "label": "", "metadata": {}, "score": "69.70875"}
{"text": "View Article PubMed .Friedman M : The use of ranks to avoid the assumption of normality implicit in the analysis of variance .Journal of the American Statistical Association 1937 , 32 : 675 - 701 .View Article .Rifkin R , Klatau A : In defense of one - vs - all classification .", "label": "", "metadata": {}, "score": "69.720184"}
{"text": "The strategy used to update the archive relies on the domination level and the diversity of the solutions and the current size of the archive and is based on the nondominated sorting concept borrowed from NSGA - II [ 40 ] .", "label": "", "metadata": {}, "score": "69.738205"}
{"text": "Aronson AR : Effective mapping of biomedical text to the UMLS Metathesaurus : the MetaMap program .Proc AMIA Symp 2001 , 17 - 21 : 17 - 21 .Weeber M , Klein H , Aronson AR , Mork JG , de Jong - van den Berg LT , Vos R : Text - based discovery in biomedicine : the architecture of the DAD - system .", "label": "", "metadata": {}, "score": "69.77353"}
{"text": "The three classifiers are combined through a three - way majority vote .The pseudocode and implementation detail of boosting is shown in Figure 5 . \\ )Also , the ensemble error is a training error bound .Hence , a stronger classifier is generated from three weaker classifiers .", "label": "", "metadata": {}, "score": "69.79814"}
{"text": "Finally , the ensemble integration phase involves fusion strategy ( majority voting method ) to combine the predictions of the selected classifiers .In our experiments , we selected the solution for comparison with other classifiers having a better value of the CID .", "label": "", "metadata": {}, "score": "69.81303"}
{"text": "However , IBC does not allow to efficiently adapt a fusion function over time when new data becomes available , since it requires a fixed number of classifiers .The IBC technique was further improved as incremental Boolean combination ( incrBC ) by the authors [ 119 ] .", "label": "", "metadata": {}, "score": "69.86174"}
{"text": "As shown in Figure 5 , PCA , which has two very close senses , had much higher error rates than BSA , which has two unrelated senses .Therefore , when comparing the performance of different WSD systems , data sets with the same degree of difficulty should be used .", "label": "", "metadata": {}, "score": "69.90408"}
{"text": "The percentage improvement of the results of the proposed technique over the other techniques is also depicted in Table 8 .The results indicate that the proposed technique helps to enhance the average detection rate , reduce average false positive rate , and increase CID values over that of other techniques .", "label": "", "metadata": {}, "score": "70.02934"}
{"text": "32 , no . 2 , pp .137 - 143 , 2000 .View at Google Scholar \u00b7 View at Scopus .G. Kumar and K. Kumar , \" The use of multi - objective genetic algorithm based approach to create ensemble of ann for intrusion detection , \" International Journal of Intelligence Science , vol .", "label": "", "metadata": {}, "score": "70.04502"}
{"text": "View Article .Weston J , Watkins C : Multiclass support vector machines .Proceedings of ESANN99 .Copyright .\u00a9 Xu et al .2006 .This article is published under license to BioMed Central Ltd.A Learning - Based Approach for Biomedical Word Sense Disambiguation .", "label": "", "metadata": {}, "score": "70.04668"}
{"text": "Diversity of classifiers in bagging is obtained by using bootstrapped replicas of the training data .That is , different training data subsets are randomly drawn - with replacement - from the entire training dataset .Each training data subset is used to train a different classifier of the same type .", "label": "", "metadata": {}, "score": "70.1199"}
{"text": "26 ] grouped different classifier combination schemes into three main categories according to their architecture : ( 1 ) parallel ; ( 2 ) cascading ( or serial combination ) ; ( 3 ) hierarchical ( tree like ) .In the parallel architecture , all the individual classifiers are invoked independently , and their results are then combined by a combiner .", "label": "", "metadata": {}, "score": "70.13414"}
{"text": "Using Bayes ' theorem , the conditional probability can be decomposed as .In practice , there is interest only in the numerator of that fraction , because the denominator does not depend on and the values of the features are given , so that the denominator is effectively constant .", "label": "", "metadata": {}, "score": "70.13837"}
{"text": "To cover various aspects of ensembles , many researchers proposed different taxonomies for ensembles .Keeping the advantages of AI - based techniques over other techniques and ensembles , many researchers proposed AI - based ensembles for ID .However , there exists no comprehensive review of taxonomies of ensembles ( in general ) and AI - based ensembles for intrusion detection ( ID ) ( in specific ) .", "label": "", "metadata": {}, "score": "70.13934"}
{"text": "This proves that there is neither a perfect combination strategy , nor one generally outperforming each other .This statement can be used as theoretical guideline for a malicious user to disrupt or evade the system , once combination strategy implemented is known to them .", "label": "", "metadata": {}, "score": "70.15841"}
{"text": "[5 ] Still , a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches , such as boosted trees or random forests .[ 6 ] .An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification .", "label": "", "metadata": {}, "score": "70.24329"}
{"text": "There are indications in the literature that bagging and boosting learn better from imbalanced data .However , the experiments here have demonstrated that these algorithms remain biased towards the majority class(es ) .However , an improvement of results is noticed up to 36 % in DR and 55 % in FPR approximately over bagging based ensemble of NB for KDD cup 1999 dataset .", "label": "", "metadata": {}, "score": "70.29182"}
{"text": "He proved that multiobjective GA based approach is an effective way to train the ANN which works well for minority attack classes in imbalanced datasets .He proposed two - phase process for intrusion detection .In the first phase , he generated a set of base classifiers of ANNs by optimizing their weights assuming a fixed number of hidden layers and the number of neurons per hidden layer in ANN .", "label": "", "metadata": {}, "score": "70.2947"}
{"text": "All the words in the title and abstract of the articles were used as features for machine learning and an SVM algorithm was used to generate a classifier .We used a package called \" Spider \" [ 46 ] to perform all the SVM training and testing .", "label": "", "metadata": {}, "score": "70.29695"}
{"text": "262 - 294 , 2000 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Shiravi , H. Shiravi , M. Tavallaee , and A. A. Ghorbani , \" Toward developing a systematic approach to generate benchmark datasets for intrusion detection , \" Computers & Security , vol .", "label": "", "metadata": {}, "score": "70.298195"}
{"text": "# constructing a training corpus and a test corpus from the semantically tagged Brown corpus ( manually tagged by the WordNet team ) by extracting tokens for the HMM bigrams .# computing a HMM model based on the training corpus , runnig the tagger on the test corpus and comparing the results with the original tags in the test corpus .", "label": "", "metadata": {}, "score": "70.323975"}
{"text": "Incremental learning refers to the ability of an algorithm to learn from new data that may become available after a classifier ( or a model ) has already been generated from a previously available dataset .Hence , an incremental learning algorithm must learn the new information , and retain previously acquired knowledge , without having access to previously seen data .", "label": "", "metadata": {}, "score": "70.3925"}
{"text": "L. I. Kuncheva , \" A theoretical study on six classifier fusion strategies , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .24 , no . 2 , pp .281 - 286 , 2002 .S. B. Cho and J. H. Kim , \" Multiple network fusion using fuzzy logic , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "70.47037"}
{"text": "160 - 171 , Berlin , Germany , 1998 .T. G. Dietterich and G. Bakiri , \" Error - correcting output codes : a general method for improving multiclass inductive learning programs , \" in Proceedings of the 9th AAAI National Conference on Artificial Intelligence , pp .", "label": "", "metadata": {}, "score": "70.48202"}
{"text": "The labelled connection records consist of 41 features and 01 attack type .The labelled connection records consist of 22 different attack types categorized into 04 classes whereas unlabeled dataset consists of 20 known and 17 unknown attack types .The 41 features can be divided into three categories , namely , basic features of individual TCP connections , content features within a connection suggested by domain knowledge , and Traffic features computed using a two - second time window .", "label": "", "metadata": {}, "score": "70.48954"}
{"text": "We now determine the probability distribution for the sex of the sample . where and are the parameters of normal distribution which have been previously determined from the training set .Note that a value greater than 1 is OK here - it is a probability density rather than a probability , because height is a continuous variable .", "label": "", "metadata": {}, "score": "70.51384"}
{"text": "Generally , the intrusion detection problem encounters a trade - off between multiple conflicting criteria such as the detection rate of attack classes , accuracy , and diversity .An exact solution to such multiobjective problem at which decision variables satisfy the related conditions and all objectives have attained corresponding optimal values may not even exist [ 35 ] .", "label": "", "metadata": {}, "score": "70.52522"}
{"text": "195 - 204 , 2008 .View at Publisher \u00b7 View at Google Scholar .J. Xiao and H. Song , \" A novel intrusion detection method based on adaptive resonance theory and principal component analysis , \" in Proceedings of the International Conference on Communications and Mobile Computing ( CMC ' 09 ) , pp .", "label": "", "metadata": {}, "score": "70.5328"}
{"text": "The second way to solve multiobjective optimization problems is by using GA producing a set of noninferior solutions .This set of noninferior solutions represents trade - offs between multiple criteria which are identified as a Pareto optimum front [ 4 , 44 ] .", "label": "", "metadata": {}, "score": "70.54729"}
{"text": "This update rule ensures that the weights of all correctly classified instances and the weights of all misclassified instances always add up to \\ ( 1/2\\ .\\ )More specifically , the requirement for the training error of the base classifier to be less than \\ ( 1/2\\ ) forces teh algorithm to correct at least one mistake made by the previous base model .", "label": "", "metadata": {}, "score": "70.753136"}
{"text": "J. Friedman and P. Hall , \" On bagging and nonlinear estimation , \" Tech .Rep. , Statistics Department , University of Stanford , Palo Alto , Calif , USA , 2000 .View at Google Scholar .L. I. Kuncheva , F. Roli , G. L. Marcialis , and C. A. Shipp , \" Complexity of data subsets generated by the random subspace method : an experimental investigation , \" in Multiple Classi_er Systems .", "label": "", "metadata": {}, "score": "70.75756"}
{"text": "217 - 225 , 2009 .View at Google Scholar .G. Wang , J. Hao , J. Mab , and L. Huang , \" A new approach to intrusion detection using artificial neural networks and fuzzy clustering , \" Expert Systems with Applications , vol .", "label": "", "metadata": {}, "score": "70.768"}
{"text": "Finally , the representational reason is to address to cases when the chosen model can not properly represent the sought decision boundary , which is discussed under divide and conquer section above .Perhaps one of the earliest work on ensemble systems is Dasarathy and Sheela 's 1979 paper ( Dasarathy 1979 ) , which first proposed using an ensemble system in a divide - and - conquer fashion , partitioning the feature space using two or more classifiers .", "label": "", "metadata": {}, "score": "70.82495"}
{"text": "Dietterich TG : Approximate statistical tests for comparing supervised classification learning algorithms .Neural Computation 1998 , 10 : 1895 - 1924 .View Article PubMed .Salzberg SL : On comparing classifiers : Pitfalls to avoid and a recommended approach .", "label": "", "metadata": {}, "score": "70.84721"}
{"text": "[ 31 ] proposed an approach based on NN and fuzzy clustering .Fuzzy clustering helps to generate homogeneous training subsets from heterogeneous training datasets which are further used to train NN models .They reported improved performance in terms of detection precision and stability .", "label": "", "metadata": {}, "score": "70.94332"}
{"text": "Four ambiguous abbreviations : BPD , BSA , PCA , and RSV , were used in this study .They were chosen because they were associated with varying degrees of differences between their respective senses .For example , two of the senses of PCA studied are very similar whereas two senses of BSA are very different .", "label": "", "metadata": {}, "score": "70.958015"}
{"text": "If a classifier can not meet this requirement , the algorithm aborts .The normalized error \\(\\beta_t\\ , \\ ) is then computed ( Equation 5 ) so that the actual error that is in the [ 0 0.5 ] interval is mapped to [ 0 1 ] interval .", "label": "", "metadata": {}, "score": "71.03303"}
{"text": "In Table 6 , the results of the three methods ( Joshi-2005 , McInnes-2007 , and Stevenson-2008 ) are taken from Stevenson et al .[ 1 ] .These three methods are supervised methods and used various machine learning algorithm and wide sets of features .", "label": "", "metadata": {}, "score": "71.11118"}
{"text": "The corpus data is divided into two sets : the development set , and the test set .The development set is often further subdivided into a training set and a dev - test set .Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set .", "label": "", "metadata": {}, "score": "71.15007"}
{"text": "^ Narasimha Murty , M. ; Susheela Devi , V. ( 2011 ) .Pattern Recognition : An Algorithmic Approach .ISBN 0857294946 .^ John , George H. ; Langley , Pat ( 1995 ) .Estimating Continuous Distributions in Bayesian Classifiers .", "label": "", "metadata": {}, "score": "71.17102"}
{"text": "Heterogeneous ensembles exploit the characteristics of different classifiers to improve the results over single classifier .It is supported by the fact that different base classifiers perform differently upon different categories of intrusions ( e.g. , DoS , Probe , U2R , R2L , etc . )", "label": "", "metadata": {}, "score": "71.189186"}
{"text": "The entire solutions are further refined to obtain ensemble solutions in the second phase of the approach .The predictions of individual solutions are fused together to compute final prediction of the ensemble using the majority voting method in phase 3 of the proposed approach .", "label": "", "metadata": {}, "score": "71.239426"}
{"text": "Boosting was the predecessor of the AdaBoost family of algorithms - which arguably became one of the most popular machine learning algorithms in recent times .Since these seminal works , research in ensemble systems have expanded rapidly , appearing often in the literature under many creative names and ideas .", "label": "", "metadata": {}, "score": "71.242004"}
{"text": "A decision tree is a simple flowchart that selects labels for input values .This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels .To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node .", "label": "", "metadata": {}, "score": "71.27753"}
{"text": "The most appealing way to reduce false alarms is to develop better IDSs that generate fewer false alarms .The process of reducing the false alarms is very challenging because the false alarms are the result of many problems .These causes require IDS to be faster , flexible ( instead of strict thresholds ) , and adaptive ( instead of fixed rules ) , and dynamic learning of new patterns and aggregate logically corelated false alarms to identify root cause of alarms .", "label": "", "metadata": {}, "score": "71.37269"}
{"text": "Research , vol .2 , pp .263 - 286 , 1995 .T. K. Ho , \" Random subspace method for constructing decision forests , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .20 , no . 8 , pp .", "label": "", "metadata": {}, "score": "71.55628"}
{"text": "Ensemble learning is the process by which multiple models , such as classifiers or experts , are strategically generated and combined to solve a particular computational intelligence problem .Ensemble learning is primarily used to improve the ( classification , prediction , function approximation , etc . ) performance of a model , or reduce the likelihood of an unfortunate selection of a poor one .", "label": "", "metadata": {}, "score": "71.60849"}
{"text": "The amount of WSD research in the biomedical domain is not proportional to the extent of the problem .As an example , in the biomedical texts , the term \" blood pressure \" has three possible senses according to the Unified Medical Language System ( UMLS )", "label": "", "metadata": {}, "score": "71.629776"}
{"text": "HIDSs and NIDSs have been designed to perform misuse detection and anomaly detection .Anomaly based ID allows detecting unknown attacks for which the signatures have not yet been extracted [ 2 ] .In practice , anomaly detectors generate false alarms due , in large part , to the limited data used for training and to the complexity of underlying data distributions that may change dynamically over time .", "label": "", "metadata": {}, "score": "71.74321"}
{"text": "G. Wang , H. Jinxing , M. Jian , and H. Lihua , \" A new approach to intrusion detection using Artificial Neural Networks and fuzzy clustering , \" Expert Systems with Applications , vol .37 , no . 9 , pp .", "label": "", "metadata": {}, "score": "71.74907"}
{"text": "To demonstrate the extent of the ambiguity problem in MEDLINE we searched MEDLINE abstracts to determine how many abstracts contained gene symbols that were ambiguous with general English words or biomedical terms .We repeated the same procedure for the fly and yeast organisms as well .", "label": "", "metadata": {}, "score": "71.80022"}
{"text": "Khreich et al .[ 2 ] proposed an iterative Boolean combination ( IBC ) method to efficiently fuse the predictions from multiple classifiers .The IBC efficiently exploits all Boolean functions applied to the ROC curves and requires no prior assumptions about conditional independence of classifiers or convexity of ROC curves .", "label": "", "metadata": {}, "score": "71.8177"}
{"text": "Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set .Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution .", "label": "", "metadata": {}, "score": "71.82849"}
{"text": "With the same sample size change , the error rate for PCA dropped from 43.00 % to only 28.53 % .Results for BPD are shown in Table 5 , which contains the results from three different multi - class SVM algorithms .", "label": "", "metadata": {}, "score": "71.87959"}
{"text": "The problem of WSD was first introduced by Warren Weaver in 1949 [ 3 ] .In 1975 Kelly and Stone [ 4 ] published a book explicitly listing their rules for disambiguation of word senses .As large - scale lexical resources became available in the 1980s , the automatic extraction of lexical knowledge became possible , disambiguation was still knowledge- or dictionary - based though .", "label": "", "metadata": {}, "score": "71.917725"}
{"text": "Stacked Generalization .In Wolpert 's stacked generalization ( or stacking ) , an ensemble of classifiers is first trained using bootstrapped samples of the training data , creating Tier 1 classifiers , whose outputs are then used to train a Tier 2 classifier ( meta - classifier ) ( Wolpert 1992 ) .", "label": "", "metadata": {}, "score": "71.94356"}
{"text": "Majority of research works described here were trained & tested on KDD cup 1999 dataset .Since these works are evaluated in different environments using different training and test datasets extracted from KDD cup 1999 dataset , these studies can not be critically analyzed based upon these reported results .", "label": "", "metadata": {}, "score": "71.960304"}
{"text": "( vi ) Metalearning Method .The method employs a second level of combiner to fuse the predictions of base classifiers for determining final ensemble prediction , for example , stacking .In stacking , the predictions of base classifiers are fed to an intermediate combiner to perform trained combinations of predictions of base classifiers [ 79 ] .", "label": "", "metadata": {}, "score": "72.10064"}
{"text": "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier .Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label .", "label": "", "metadata": {}, "score": "72.156296"}
{"text": "103 - 130 , 1997 .View at Google Scholar \u00b7 View at Scopus .R. O. Duda , P. E. Hart , and D. G. Stork , Pattern Classification , John Wiley & Sons , New York , NY , USA , 2nd edition , 2001 .", "label": "", "metadata": {}, "score": "72.18106"}
{"text": "Shatkay H , Feldman R : Mining the biomedical literature in the genomic era : an overview .J Comput Biol 2003 , 10 : 821 - 855 .View Article PubMed .Fukuda K , Tamura A , Tsunoda T , Takagi T : Toward information extraction : identifying protein names from biological papers .", "label": "", "metadata": {}, "score": "72.211655"}
{"text": "( i ) Bagging .Bagging ( bootstrap aggregating ) is originally proposed by Breiman [ 61 ] .The method is dependent on the instability of the base classifiers .The instability of base classifiers refers to sensitivity to configuration of base classifier and/or training data .", "label": "", "metadata": {}, "score": "72.30575"}
{"text": "[5 ] .Recent advances in the field of AI led many researchers to apply AI - based techniques for ID successfully .The major difference between AI - based and traditional IDSs is that only AIs can learn new rules automatically , whereas in traditional systems the security administrator must add new rules for each new attack type or each new allowed program .", "label": "", "metadata": {}, "score": "72.30731"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" Experiments with a new boosting algorithm , \" in Proceedings of the 30th International Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "72.33052"}
{"text": "C. Kruegel , F. Valeur , and G. Vigna , Intrusion Detection and Correlation , Challenges and Solution , Advances in Information Security , Springer , 2005 .View at Scopus .View at Publisher \u00b7 View at Google Scholar .J. W. Haines , R. P. Lippmann , D. J. Fried , E. Tran , S. Boswell , and M. A. Zissman , \" DARPA intrusion detection system evaluation : design and procedures , \" Tech .", "label": "", "metadata": {}, "score": "72.38406"}
{"text": "1917 of Lecture Notes in Computer Science , pp .849 - 858 , 2000 .View at Google Scholar .S. Tiwari , G. Fadel , and K. Deb , \" AMGA2 : improving the performance of the archive - based micro - genetic algorithm for multi - objective optimization , \" Engineering Optimization , vol .", "label": "", "metadata": {}, "score": "72.38411"}
{"text": "2732 - 2752 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .N. Giannopoulos , V. Moulianitis , and A. Nearchou , \" Multi - objective optimization with fuzzy measures and its application to flow - shop scheduling , \" Engineering Applications of Artificial Intelligence , vol .", "label": "", "metadata": {}, "score": "72.4043"}
{"text": "5.4 The Naivete of Independence .The reason that naive Bayes classifiers are called \" naive \" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) .In particular , almost all real - world problems contain features with varying degrees of dependence on one another .", "label": "", "metadata": {}, "score": "72.45871"}
{"text": "As a supervised technique , this method consists of two stages learning ( or training ) stage and a testing ( or application ) stage .The trained models ( classifiers ) produced from the learning phase will then be used to disambiguate unseen and unlabeled examples in the testing phase .", "label": "", "metadata": {}, "score": "72.5816"}
{"text": "The distribution \\(D\\ ) starts out as uniform ( Equation 3 in Figure 6 ) , so that all instances have equal probability to be drawn into the first data subset \\(S_1\\ .\\ )At each iteration t , a new training set is drawn , and a weak classifier is trained to produce a hypothesis \\(h_t\\ .", "label": "", "metadata": {}, "score": "72.6317"}
{"text": "In contrast , evolutionary algorithm seems to be well suited for the solution of multiobjective optimization ( MOO ) problems mainly due to their inherent characteristics concerning the population set based exploration of the search space of a given problem [ 35 ] .", "label": "", "metadata": {}, "score": "72.73534"}
{"text": "The main objective of IDS is to classify intrusive and nonintrusive network activities in an efficient manner .The process of intrusion detection involves the tasks : ( 1 ) data acquisition/ collection ; ( 2 ) data Preprocessing & feature selection ; ( 3 ) model selection for data analysis ; ( 4 ) classification and result analysis [ 3 ] .", "label": "", "metadata": {}, "score": "72.83646"}
{"text": "The discussion so far has derived the independent feature model , that is , the naive Bayes probability model .The naive Bayes classifier combines this model with a decision rule .One common rule is to pick the hypothesis that is most probable ; this is known as the maximum a posteriori or MAP decision rule .", "label": "", "metadata": {}, "score": "72.88141"}
{"text": "Learn + + primarily for incremental learning problems that do not introduce new classes ( Polikar2001 ) , and Learn + + .NC for those that introduce new classes with additional datasets ( Muhlbaier 2008 ) are two examples of ensemble based incremental learning algorithms .", "label": "", "metadata": {}, "score": "72.96844"}
{"text": "An early sceptic was Bar - Hillel who famously proclaimed that \" sense ambiguity could not be resolved . by .electronic computer either current or imaginable \" .Y. Bar - Hillel .Language and Information .Addison - Wesley , 1964 .", "label": "", "metadata": {}, "score": "73.003555"}
{"text": "Table 8 : Percentage improvement of the results of the proposed technique using NB as a base classifier .In a nutshell , the empirical investigation and comparison of the results indicate the following .( i )The proposed approach outperforms the individual representative techniques in terms of identified performance metrics .", "label": "", "metadata": {}, "score": "73.13129"}
{"text": "Genetic Algorithm ( GA ) .GA is population based search technique that has been identified to perform better than the classical heuristics or gradient approaches [ 35 ] .GAs provides better solutions particularly for multimodels , nondifferentiable or discontinuous functions .", "label": "", "metadata": {}, "score": "73.214455"}
{"text": "Ensemble learning process has three phases : ( 1 ) ensemble generation ; ( 2 ) ensemble selection ; ( 3 ) ensemble integration .Ensemble generation is homogenous if the same induction algorithm is used to generate all the classifiers of the ensemble , otherwise it is said to be heterogeneous .", "label": "", "metadata": {}, "score": "73.36899"}
{"text": "Deciding whether an email is spam or not .Deciding what the topic of a news article is , from a fixed list of topic areas such as \" sports , \" \" technology , \" and \" politics . \"Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution .", "label": "", "metadata": {}, "score": "73.40059"}
{"text": "[ 32 ] for intrusion detection .The system was unable to detect the intrusions of U2R and R2L attack classes .Khreich et al .[ 33 ] proposed an iterative Boolean combination ( IBC ) technique for efficient fusion of the responses from any crisp or soft detector trained on fixed - size datasets in the ROC space .", "label": "", "metadata": {}, "score": "73.46222"}
{"text": "10 Exercises .Find out what type and quantity of annotated data is required for developing such systems .Why do you think a large amount of data is required ?Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set .", "label": "", "metadata": {}, "score": "73.642166"}
{"text": "M. Re and G. Valentini , \" Integration of heterogeneous data sources for gene function prediction using decision templates and ensembles of learning machines , \" Neurocomputing , vol .73 , no . 7 - 9 , pp .1533 - 1537 , 2010 .", "label": "", "metadata": {}, "score": "73.68882"}
{"text": "In this investigation , we used AMGA2 as a multiobjective genetic algorithm because of its benefits over other representative algorithms [ 48 ] .The implementation of AMGA2 algorithm takes the following input parameters : ( i ) number of function evaluations ; ( ii ) number of desired solutions ; ( iii ) random seed ; ( iv ) output file .", "label": "", "metadata": {}, "score": "73.695496"}
{"text": "We computed the standard deviation of the error rate as follows .Recall that for each abbreviation , each sense distribution and each sample size we run the experiment 30 times .The error rate was computed using both a 5-fold and a 10-fold cross - validation scheme .", "label": "", "metadata": {}, "score": "73.7144"}
{"text": "These probabilities , accompanied by the predictions of each of the classifiers , are passed on to the selector , which then determines the final output .These probabilities can be used to stochastically select the expert , or to choose the expert according to a winner - takes - all paradigm , or as weights to combine the outputs of the multiple base classifiers [ 33 , 46 ] .", "label": "", "metadata": {}, "score": "73.90065"}
{"text": "B. V. Dasarathy and B. V. Sheela ( 1979 ) , \" Composite classifier system design : concepts and me - thodology , \" Proceedings of the IEEE , vol .67 , no .5 , pp .708 - 713 .", "label": "", "metadata": {}, "score": "73.97257"}
{"text": "The ensemble solutions in improved Pareto front reported improved detection results based on benchmark datasets for intrusion detection .In the third phase , a combination method like majority voting method is used to fuse the predictions of individual solutions for determining prediction of ensemble solution .", "label": "", "metadata": {}, "score": "73.97389"}
{"text": "357 - 374 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Kumar , K. Kumar , and M. Sachdeva , \" An empirical comparative analysis of feature reduction methods for intrusion detection , \" International Journal of Information and Telecommunication Technology , vol .", "label": "", "metadata": {}, "score": "74.05185"}
{"text": "The paper will help the better understanding of different directions in which research of ensembles has been done in general and specifically : field of intrusion detection systems ( IDSs ) .Introduction .The threat of Internet attacks is quite real and frequent so this has increased a need for securing information on any network on the Internet .", "label": "", "metadata": {}, "score": "74.08357"}
{"text": "A synopsis of linguistic theory 1930 - 1955 .In Studies in Linguistic Analysis , pp . 1 - 32 .Oxford : Philological Society .Reprinted in F.R. Palmer ( ed . ) , Selected Papers of J.R. Firth 1952 - 1959 , London : Longman ( 1968 ) .", "label": "", "metadata": {}, "score": "74.0861"}
{"text": "1471 - 2105 - 7 - 334-S1.doc Additional File 1 : Supplementary material for gene ambiguity for mining MEDLINE ( DOC 925 KB ) .Authors ' contributions .HX carried out data collection , programming , experiments using SVM and drafted the manuscript .", "label": "", "metadata": {}, "score": "74.11403"}
{"text": "Chen L , Liu H , Friedman C : Gene name ambiguity of eukaryotic nomenclatures .Bioinformatics 2005 , 21 : 248 - 256 .View Article PubMed .Schuemie MJ , Weeber M , Schijvenaars BJA , van Mulligen EM , van der Eijk CC , Jelier R , et al .", "label": "", "metadata": {}, "score": "74.398895"}
{"text": "G. Kumar and K. Kumar , \" A novel evaluation function for feature selection based upon information theory , \" in Proceedings of the IEEE International Conference on Electrical and Computer Engineering ( CCECE ' 11 ) , pp .000395- 000399 , Niagara Falls , Canada , May 2011 .", "label": "", "metadata": {}, "score": "74.47922"}
{"text": "9 Further Reading .Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python .For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) .", "label": "", "metadata": {}, "score": "74.489456"}
{"text": "This process is illustrated in 5.2 and 5.3 .Figure 5.2 : Calculating label likelihoods with naive Bayes .Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data .", "label": "", "metadata": {}, "score": "74.49701"}
{"text": "Finally , Section 6 concludes the paper and presents future research directions .Intrusion Detection .An intrusion detection system ( IDS ) defined as \" an effective security technology , which can detect , prevent and possibly react to the computer attacks \" is one of the standard components in security infrastructures .", "label": "", "metadata": {}, "score": "74.50919"}
{"text": "Figures 1 , 2 and 3 show the error rate versus the sample size for each distribution of the BSA , PCA and RSV data sets with 5-fold cross - validation .As the figures indicate , the reduction of the error rate as a function of the sample size is more dramatic for BSA than for PCA .", "label": "", "metadata": {}, "score": "74.51808"}
{"text": "This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .Abstract .In the biomedical domain , word sense ambiguity is a widely spread problem with bioinformatics research effort devoted to it being not commensurate and allowing for more development .", "label": "", "metadata": {}, "score": "74.60858"}
{"text": "[ 37 ] by suggesting an evaluation function which was later known as Parrot function .They proposed to use accuracy of each target class as a separate objective in their evaluation function for multiobjective GA .Here , accuracy of each class refers to correctly classified instances of that class .", "label": "", "metadata": {}, "score": "74.73133"}
{"text": "Since naive Bayes is also a linear model for the two \" discrete \" event models , it can be reparametrised as a linear function .Obtaining the probabilities is then a matter of applying the logistic function to , or in the multiclass case , the softmax function .", "label": "", "metadata": {}, "score": "74.81857"}
{"text": "73 , no . 7 - 9 , pp .1533 - 1537 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. K. Chan and S. J. Stolfo , \" On the accuracy of meta - learning for scalable data mining , \" Journal of Intelligent Information Systems , vol . 8 , no . 1 , pp .", "label": "", "metadata": {}, "score": "74.95242"}
{"text": "In each case , a final decision is made by combining the individual decisions of several experts .In doing so , the primary goal is to minimize the unfortunate selection of an unnecessary medical procedure , a poor product , an unqualified employee or even a poorly written and misguiding article .", "label": "", "metadata": {}, "score": "75.00627"}
{"text": "Figure 7 : Training and test performance of noninferior NB based ensembles for ITFS - KDD ( 10 features ) data subset .Results of ISCX 2012 Dataset .The performance of solutions for training and test data of ISCX 2012 dataset is described in Figure 8 .", "label": "", "metadata": {}, "score": "75.01007"}
{"text": "It is spam if ( i.e. , ) , otherwise it is not spam .^ Caruana , R. ; Niculescu - Mizil , A. ( 2006 ) .An empirical comparison of supervised learning algorithms .Proc . 23rdInternational Conference on Machine Learning .", "label": "", "metadata": {}, "score": "75.159164"}
{"text": "3541 , pp .326 - 335 , Seaside .Monterey , CA , June 2005 .M. Muhlbaier , A. Topalis , R. Polikar , \" Learn++ .NC : Combining Ensemble of Classifiers with Dynamically Weighted Consult - and - Vote for Efficient Incremental Learning of New Classes , \" IEEE Transactions on Neural Networks , In press , 2008 .", "label": "", "metadata": {}, "score": "75.208015"}
{"text": "For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are .", "label": "", "metadata": {}, "score": "75.25059"}
{"text": "The method selects those base classifiers which perform better performance than others .Then the method combines the selected base classifiers through majority voting method ( described in previous section ) [ 89 ] .Mixture of Expert Systems .This method is a general method similar to ensemble selection [ 90 ] .", "label": "", "metadata": {}, "score": "75.49115"}
{"text": "18 - 28 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. C. Ponce , \" Intrusion detection system with artificial intelligence , \" in Proceedings of the FIST Conference , Universidad Pontificia Comillas de Madrid , 2004 , edition : 1/28 .", "label": "", "metadata": {}, "score": "75.57698"}
{"text": "Definition 1 ( topic chain ) .A topic chain ( TC ) is a branch of topic hierarchy and represents a sequence of ordered topic category label terms in ODP .It represents a notation of .Definition 2 ( disambiguation context ) .", "label": "", "metadata": {}, "score": "75.65395"}
{"text": "The parameters used as input by the user to AMGA2 are depicted in Table 3 .Other simulation parameters are tuned automatically by AMGA2 for KDD cup 1999 dataset and the ISCX 2012 dataset are presented in Tables 4 and 5 , respectively .", "label": "", "metadata": {}, "score": "75.65823"}
{"text": "Results of KDD Cup 1999 Dataset .The proposed approach is applied to various data subsets of KDD cup 1999 dataset that produces a set of noninferior solutions using NB as base classifiers .The performance of solutions for training and test data of KDD 1 dataset is as depicted in Figure 4 .", "label": "", "metadata": {}, "score": "75.904434"}
{"text": "Some iterative optimization techniques are much faster than others .When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods .", "label": "", "metadata": {}, "score": "75.91974"}
{"text": "R. Anand , K. Mehrotra , C. K. Mohan , and S. Ranka , \" Efficient classification for multiclass problems using modular neural networks , \" IEEE Transactions on Neural Networks , vol .6 , no . 1 , pp .", "label": "", "metadata": {}, "score": "76.16196"}
{"text": "( iv )The ensembles evolved with the proposed technique provide better solutions and also achieve a higher detection accuracy .( v )Higher values of CID for the proposed approach proved the superiority over the existing individual techniques and their ensembles using bagging and boosting .", "label": "", "metadata": {}, "score": "76.26891"}
{"text": "I. Corona , D. Ariu , and G. Giacinto , \" HMM - web : a framework for the detection of attacks against web applications , \" in Proceedings of the IEEE International Conference on Communications ( ICC ' 09 ) , June 2009 .", "label": "", "metadata": {}, "score": "76.42486"}
{"text": "146 - 156 , 2002 .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the Margin : A New Explanation for the Effectiveness of Voting Methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "76.62381"}
{"text": "1 Supervised Classification .Classification is the task of choosing the correct class label for a given input .In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance .", "label": "", "metadata": {}, "score": "76.62442"}
{"text": "The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers to the edge of a river and in the second to a building .However , the task has proved to be difficult for computer and some have believed that it would never be solved .", "label": "", "metadata": {}, "score": "76.63643"}
{"text": "In EV3 , we kept .Table 5 contains the results of EV2 and EV3 .To judge on performance of our method and compare our results with similar techniques , we included several reported results from three recent publications from 2008 to 2010 [ 1 , 2 , 4 ] with our results in Table 6 under the same experimental settings .", "label": "", "metadata": {}, "score": "76.82498"}
{"text": "The statistics of selected subsets of NSL - KDD datasets used in our experiments are as depicted in Table 1 .Table 1 : Statistics of subsets of KDD cup 1999 dataset as training and test dataset .In order to overcome the limitations of KDD cup 1999 dataset , Shiravi et al .", "label": "", "metadata": {}, "score": "77.08859"}
{"text": "Hand DJ : Construction and assessment of classification rules Chichester , England : John Wiley & Sons 1997 .Hand DJ : Assessing Classification Rules .Journal of Applied Statistics 1994 , 21 : 3 - 16 .View Article .Fukunaga K , Hayes RR : Effect of sample size in classifier design .", "label": "", "metadata": {}, "score": "77.21687"}
{"text": "Supervisor classifier selects the most suitable member of the ensemble on the basis of the available input data .Two additional components are incorporated in mixture of expert 's model : ( 1 ) a gating network ; ( 2 ) selector .", "label": "", "metadata": {}, "score": "77.28349"}
{"text": "Minimum rule .\\(\\alpha\\rightarrow \\infty \\Rightarrow\\ ) maximum rule .Voting based methods .The ensemble then chooses class J that receives the largest total vote : .Majority ( plurality ) voting .Under the condition that the classifier outputs are independent , it can be shown the majority voting combination will always lead to a performance improvement .", "label": "", "metadata": {}, "score": "77.32784"}
{"text": "MM and CF conceived of the study , and participated in its design and coordination and helped to draft the manuscript .MM also performed statistical analysis and interpreted the results .HL advised in the design of study .All authors read and approved the final manuscript .", "label": "", "metadata": {}, "score": "77.35844"}
{"text": "Final prediction of ensemble is generated by fusing different predictions of individual base classifiers .Generally fusion of predictions is performed by using majority voting method .However , this is not always possible due to the size of the dataset .", "label": "", "metadata": {}, "score": "77.448944"}
{"text": "The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier .But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier .", "label": "", "metadata": {}, "score": "77.539116"}
{"text": "338 - 345 , 1995 .M. V. Mahoney and P. K. Chan , \" An analysis of the 1999 DARPA / Lincoln laboratory evaluation data for network anomaly detection , \" Tech .Rep. CS-200302 , Computer Science Department , Florida Institute of Technology , 2003 .", "label": "", "metadata": {}, "score": "77.614944"}
{"text": "This training algorithm is an instance of the more general expectation - maximization algorithm ( EM ) : the prediction step inside the loop is the E -step of EM , while the re - training of naive Bayes is the M -step .", "label": "", "metadata": {}, "score": "77.694786"}
{"text": "In AdaBoost . M1 , bootstrap training data samples are drawn from a distribution \\(D\\ ) that is iteratively updated such that subsequent classifiers focus on increasingly difficult instances .This is done by adjusting \\(D\\ ) such that previously misclassified instances are more likely to appear in the next bootstrap sample .", "label": "", "metadata": {}, "score": "77.755035"}
{"text": "L. Lam and C. Y. Suen , \" Application of majority voting to pattern recognition : an analysis of its behavior and performance , \" IEEE Transactions on Systems , Man , and Cybernetics A , vol .27 , no .", "label": "", "metadata": {}, "score": "77.800415"}
{"text": "True negatives are irrelevant items that we correctly identified as irrelevant .False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant .False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant .", "label": "", "metadata": {}, "score": "78.028534"}
{"text": "M. Sabhnani and G. Serpen , \" Application of machine learning algorithms to KDD intrusion detection dataset within misuse detection context , \" in Proceedings of the International Conference on Machine Learning ; Models , Technologies and Applications ( MLMTA ' 03 ) , pp .", "label": "", "metadata": {}, "score": "78.05248"}
{"text": "5.5 The Cause of Double - Counting .The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined .", "label": "", "metadata": {}, "score": "78.0702"}
{"text": "Threshold Plurality Vote Method .This method is further generalization of majority vote method proposed by Xu et al .[ 73 ] .( iii ) Na\u00efve Bayes Decision Method : This method assumes the conditional independence between classifiers .The method selects the class with the highest posterior probability computed through the estimated class conditional probabilities and Bayes ' theorem [ 74 , 75 ] .", "label": "", "metadata": {}, "score": "78.1066"}
{"text": "The user may select an ideal solution as per application specific requirements .The major issue in the proposed approach is that it takes long time to compute fitness functions in various generations .It may be overcome by computing the function values in parallel .", "label": "", "metadata": {}, "score": "78.11334"}
{"text": "S. Peddabachigari , A. Abraham , C. Grosan , and J. Thomas , \" Modeling intrusion detection system using hybrid intelligent systems , \" Journal of Network and Computer Applications , vol .30 , no . 1 , pp .114 - 132 , 2007 .", "label": "", "metadata": {}, "score": "78.15607"}
{"text": "For example , when the total sample size is 20 and 5-fold cross validation is used the size of the training set is 16 , while if the sample size is 80 the size of the training set is 64 .", "label": "", "metadata": {}, "score": "78.26311"}
{"text": "Nonavailability of signatures of novel attacks in databases leads to high false alarm rate and low detection accuracy .In fact , practitioners as well as researchers have observed that IDSs can easily trigger thousands of alarms per day , upto 99 % of which are false positives ( i.e. , alarms that were mistakenly triggered by benign events ) [", "label": "", "metadata": {}, "score": "78.30351"}
{"text": "Fusion - Based Combination Methods .These methods combine the predictions of the base classifiers to determine ensemble prediction .The major methods proposed in literature are described below .( i ) Majority Voting Method .In majority voting ensemble , each base classifier votes for specific class and the class that collects majority of vote is predicted as ensemble final prediction [ 70 - 72 ] .", "label": "", "metadata": {}, "score": "78.37145"}
{"text": "Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand .", "label": "", "metadata": {}, "score": "78.41443"}
{"text": "A number of natural language processing systems in the biomedical domain reported decreased precision due to the ambiguity problem [ 3 , 4 ] .Weeber [ 5 ] found that in order to replicate Swanson 's literature - based discovery of the involvement of magnesium deficiency in migraine , it was important to resolve the ambiguity of an abbreviation mg , which can denote either magnesium or milligram .", "label": "", "metadata": {}, "score": "78.51195"}
{"text": "G. Kumar , K. Kumar , and M. Sachdeva , \" The use of artificial intelligence based techniques for intrusion detection - a review , \" Artificial Intelligence Review , vol .34 , no .4 , pp .369 - 387 , 2010 .", "label": "", "metadata": {}, "score": "78.63858"}
{"text": "The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 7 - 334 ) contains supplementary material , which is available to authorized users .Background .During the last few years , there has been a surge of interest in information extraction and text mining of the biomedical literature [ 1 , 2 ] .", "label": "", "metadata": {}, "score": "78.70587"}
{"text": "View at Scopus . H. Al - Mubaid , \" Context - based technique for biomedical term classification , \" in Proceedings of the IEEE Congress on Evolutionary Computation ( CEC ' 06 ) , pp .5726 - 5733 , Vancouver , Canada , July 2006 .", "label": "", "metadata": {}, "score": "78.94888"}
{"text": "For PCA , the estimated distribution was ( 0.67 , 0.33 ) .Four different sample sizes were used ( 20 , 40 , 80 and 120 ) , and for each , a proportional sample for each sense was obtained based on the particular distribution .", "label": "", "metadata": {}, "score": "79.09546"}
{"text": "The weighted majority voting then chooses the class \\(\\omega\\ ) receiving the highest total vote from all classifiers .This result may appear to go against the conventional wisdom ( see Occam 's razor ) indicating that adding too many classifiers - beyond a certain limit - would eventually lead to overfitting of the data .", "label": "", "metadata": {}, "score": "79.10147"}
{"text": "Table 6 contains the results for 11 methods : baseline method ( mfs ) , our method ( last column ) , and 9 other methods from recent work published in 2008 to 2010 ( from [ 1 , 2 , 4 ] ) .", "label": "", "metadata": {}, "score": "79.13553"}
{"text": "4 , pp .377 - 401 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. A. Coello Coello , \" A comprehensive survey of evolutionary - based multiobjective optimization techniques , \" Knowledge and Information Systems , vol .", "label": "", "metadata": {}, "score": "79.43344"}
{"text": "The goal of this stage is to produce Troika 's final prediction .The inputs of the super classifier are the outputs produced by the metaclassifiers from the previous stage .In the training phase , the super classifier learns the conditions which enable one or more of the metaclassifiers to predict correctly or incorrectly .", "label": "", "metadata": {}, "score": "79.512344"}
{"text": "Some attacks are prevented by the first line of defense whereas some bypass them .Such attacks must be detected as soon as possible so that damage may be minimized and appropriate corrective measures may be taken .Several techniques from different disciplines are being employed for the accurate intrusion detection systems ( IDSs ) .", "label": "", "metadata": {}, "score": "79.84143"}
{"text": "View at Scopus .M. Tavallaee , An adaptive hybrid intrusion detection system [ Ph.D. thesis ] , University of New Brunswick , 2011 .J. McHugh , \" Testing intrusion detection systems : a critique of the 1998 and 1999 darpa intrusion detection system evaluations as performed by lincoln laboratory , \" ACM Transactions on Information and System Security , vol .", "label": "", "metadata": {}, "score": "80.739784"}
{"text": "The set of activities that violates security objectives is called intrusion .Out of these phases , the perfect detection of an intrusion is the most important .As only after correct detection of intrusion , correct reaction and recovery phase of information security can be implemented .", "label": "", "metadata": {}, "score": "80.781555"}
{"text": "However , the performance of NSGA - II degrades for the real world problems having more than three objectives and large population [ 41 ] .Evolutionary Approach for Intrusion Detection .A novel evolutionary approach based on multiobjective GA for intrusion detection is proposed .", "label": "", "metadata": {}, "score": "80.90446"}
{"text": "The KDD 99 derived from DARPA 1998 & 1999 datasets are main benchmarks used to evaluate the performance of network intrusion detection systems .However , they are suffering from a fatal drawback : failing to realistically simulate a real - world network [ 102 , 121 ] .", "label": "", "metadata": {}, "score": "81.41179"}
{"text": "( viii ) Boolean Combination ( BC ) Methods .Boolean functions especially the conjunction AND and disjunction OR operations have recently been investigated to combine predictions of different classifiers within the ROC space [ 2 ] .These methods were shown to improve performance .", "label": "", "metadata": {}, "score": "82.06233"}
{"text": "He refined the KDD cup 1999 dataset and named it as NSL - KDD dataset .As the number of connection records in training and test data set is very large , so it is practically very difficult to use the whole data set .", "label": "", "metadata": {}, "score": "82.26515"}
{"text": "They generated HMMs models as base classifiers by training them using different number of HMM states and random initializations .They applied multiple HMM to dataset and final prediction is computed by exploiting all Boolean functions applied to the ROC curves .", "label": "", "metadata": {}, "score": "82.278656"}
{"text": "import math def entropy ( labels ) : .Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump .", "label": "", "metadata": {}, "score": "82.30232"}
{"text": "The details can be further explored in [ 7 ] .KDD cup 1999 dataset is most popular publically available evaluation benchmarked dataset .But , the dataset is critically discussed in the literature for being nowadays outdated due to the type of attacks and background traffic used and for the methodology implemented for building it [ 14 ] .", "label": "", "metadata": {}, "score": "82.34082"}
{"text": "This is problematic because it will wipe out all information in the other probabilities when they are multiplied .Therefore , it is often desirable to incorporate a small - sample correction , called pseudocount , in all probability estimates such that no probability is ever set to be exactly zero .", "label": "", "metadata": {}, "score": "82.47295"}
{"text": "In Proceedings of the 33rd Annual Meeting of the Association for Computational Lainguistics ( ACL ' 95 ) , pages 189 - 196 , Cambridge , MA , 1995 .Learning decision lists .Machine Learning , 2(3):229 - 246 , 1987", "label": "", "metadata": {}, "score": "82.596275"}
{"text": "The authors reported superior performance of Troika over other stacking methods .Discussion .Which is better bagging or boosting ?Many researchers compared the two methods including some large - scale experiments [ 56 , 57 , 107 , 108 ] .", "label": "", "metadata": {}, "score": "82.72523"}
{"text": "57 ] presented a new dataset for validation of an IDS at Information Security Center of excellence ( ISCX ) .The dataset is available in the packet capture form .filename1.csv , where filename is the name of the 7z ( packet capture ) file .", "label": "", "metadata": {}, "score": "82.99652"}
{"text": "18 , no .6 , pp .1369 - 1378 , 2007 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. R. Quinlan , C4.5 Programs for Machine Learning , Morgan Kaufmann , San Mateo , Calif , USA , 1997 .", "label": "", "metadata": {}, "score": "83.26932"}
{"text": "AAAI Fall Symp 93 98 - 107 .Hamosh A , Scott AF , Amberger J , Bocchini C , Valle D , McKusick VA : Online Mendelian Inheritance in Man ( OMIM ) , a knowledgebase of human genes and genetic disorders .", "label": "", "metadata": {}, "score": "83.48519"}
{"text": "177 - 189 , Springer , Cagliari , Italy , 2000 .X. Yao and M. Md.Islam , \" Evolving artificial neural network ensembles , \" IEEE Computational Intelligence Magazine , vol .3 , pp .31 - 42 , 2008 .", "label": "", "metadata": {}, "score": "83.57512"}
{"text": "R. Polikar , L. Udpa , S. S. Udpa , and V. Honavar , \" Learn++ : An incremental learning algo - rithm for supervised neural networks , \" IEEE Transactions on Systems , Man and Cybernetics Part C : Applications and Reviews , vol .", "label": "", "metadata": {}, "score": "83.87097"}
{"text": "2732 - 2752 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Axelsson , \" Research in intrusion detection system - a survey , \" Tech .Rep. CMU / SEI , 1999 .", "label": "", "metadata": {}, "score": "84.042336"}
{"text": "The majority voting method is used to integrate the predictions of base classifiers to get a prediction of the final ensemble .The results of experiments are computed on a Windows PC with Core i3 - 2330 M 2.20 GHz CPU and 2 GB RAM .", "label": "", "metadata": {}, "score": "84.1492"}
{"text": "In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \" automotive \" label .But it then considers the effect of each feature .In this example , the input document contains the word \" dark , \" which is a weak indicator for murder mysteries , but it also contains the word \" football , \" which is a strong indicator for sports documents .", "label": "", "metadata": {}, "score": "84.33833"}
{"text": "# The boy leapt from the bank into the cold water .# The van pulled up outside the bank and three masked men got out .We immediately recognise that in the first sentence bank refers . to .the edge . of a . river and in the second to a building .", "label": "", "metadata": {}, "score": "84.3553"}
{"text": "Cross validation type selection is typically used for training the Tier 1 classifiers : the entire training dataset is divided into T blocks , and each Tier-1 classifier is first trained on ( a different set of ) T -1 blocks of the training data .", "label": "", "metadata": {}, "score": "84.35608"}
{"text": "For example , a fruit may be considered to be an apple if it is red , round , and about 10 cm in diameter .A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple , regardless of any possible correlations between the color , roundness and diameter features .", "label": "", "metadata": {}, "score": "85.113266"}
{"text": "206 - 217 , 2003 .View at Google Scholar .R. Moskovitch , Y. Elovici , and L. Rokach , \" Detection of unknown computer worms based on behavioral classification of the host , \" Computational Statistics and Data Analysis , vol .", "label": "", "metadata": {}, "score": "85.18014"}
{"text": "Error - correcting Output Coding ( ECOC ) [ 100 ] , and data - driven ECOC [ 101 ] .( vii ) Troika .Troika is an improvement to stacking proposed by Menahem et al .[ 106 ] .", "label": "", "metadata": {}, "score": "85.18109"}
{"text": "The single objective is further optimized by GA to produce a single solution .Generally , prior knowledge about the problem or some heuristics guides the GA to produce a single solution .By changing the parameters of the algorithm and executing the algorithm repeatedly , more solutions can be produced .", "label": "", "metadata": {}, "score": "85.33972"}
{"text": "Bagging works well if classifier predictions of base classifiers were independent and classifiers had the same individual accuracy , and then the majority vote is guaranteed to improve on the individual performance [ 46 ] .( ii ) Wagging .Wagging method is a variant of bagging .", "label": "", "metadata": {}, "score": "85.35959"}
{"text": "For the details of the ambiguity study , please refer to the sub - section \" Gene Ambiguity for mining MEDLINE \" in the Methods section .Research in automated WSD can be traced back to the 1950s [ 15 ] .", "label": "", "metadata": {}, "score": "85.54538"}
{"text": "4 , pp .371 - 395 , 2002 .View at Google Scholar \u00b7 View at Scopus .View at Scopus .S. Tiwari , Development and integration of geometric and optimization algorithms for packing and layout design [ Ph.D. thesis ] , Clemson University , 2009 .", "label": "", "metadata": {}, "score": "85.60542"}
{"text": "130 - 137 , 1980 .View at Google Scholar Naive Bayes has been studied extensively since the 1950s .With appropriate preprocessing , it is competitive in this domain with more advanced methods including support vector machines .[ 2 ] It also finds application in automatic medical diagnosis .", "label": "", "metadata": {}, "score": "86.01488"}
{"text": "Support Vector Machine ( SVM ) classifiers were applied to an automatically generated data set containing four ambiguous biomedical abbreviations : BPD , BSA , PCA , and RSV , which were chosen because of varying degrees of differences in their respective senses .", "label": "", "metadata": {}, "score": "87.20899"}
{"text": "A particular limitation of boosting is that it applies only to binary classification problems .This limitation is removed with the AdaBoost algorithm .AdaBoost .Arguably the best known of all ensemble - based algorithms , AdaBoost ( Adaptive Boosting ) extends boosting to multi - class and regression problems ( Freund 2001 ) .", "label": "", "metadata": {}, "score": "88.240135"}
{"text": "The industry faces the challenges of fast changing trends of attacking the internet resources , inability of conventional techniques to protect the internet resources from a variety of attacks , and biases of individual techniques towards specific attack class(es ) .Developing effecting techniques is necessary for securing valuable internet resources from attacks .", "label": "", "metadata": {}, "score": "88.286415"}
{"text": "The raw training dataset contains about 4 GB of TCP connection data in the form of 5 million connection records .Similarly , test data set contains about 2 million records .KDD cup 1999 dataset utilizes TCP / IP level information and embedded with domain - specific heuristics to detect intrusions at the network level .", "label": "", "metadata": {}, "score": "88.31964"}
{"text": "Smaller increases in the sample size had an insignificant effect .We performed further sub - analysis using non - parametric multiple comparisons to identify the pairs of sample sizes that differ when the abbreviations BSA and RSV were analyzed .This analysis revealed that in the case of BSA the improvements in terms of error rate were statistically significant across distributions as the sample size increased from 20 to 40 .", "label": "", "metadata": {}, "score": "88.53993"}
{"text": "View at Scopus .G. Forman , \" An Extensive Empirical study of feature selection metrics for text classification , \" Journal of Machine Learning Research , vol .3 , pp .1289 - 1305 , 2003 .View at Google Scholar .", "label": "", "metadata": {}, "score": "88.78894"}
{"text": "View at Google Scholar .J. McHugh , \" Testing intrusion detection systems : a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln laboratory , \" ACM Transactions on Information and System Security , vol .", "label": "", "metadata": {}, "score": "88.916595"}
{"text": "View at Scopus The Use of Artificial - Intelligence - Based Ensembles for Intrusion Detection : A Review . 1 Department of Computer Application , Shaheed Bhagat Singh State Technical Campus , Ferozepur , Punjab 152004 , India 2 Department of Computer Science & Engineering , Punjab Institute of Technology , Kapurthala , Punjab 144601 , India .", "label": "", "metadata": {}, "score": "89.38815"}
{"text": "View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. Deb , A. Anand , and D. Joshi , \" A computationally efficient evolutionary algorithm for real - parameter optimization , \" Evolutionary Computation , vol .", "label": "", "metadata": {}, "score": "89.59515"}
{"text": "Furthermore , in species disambiguation , the term c - myc is a gene , but it can be either in a human gene ( homo sapiens ) or mouse gene ( mus musculus ) depending on the context [ 9 - 11 , 14 - 16 ] .", "label": "", "metadata": {}, "score": "89.76611"}
{"text": "This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .Abstract .A novel evolutionary approach is proposed for effective intrusion detection based on benchmark datasets .", "label": "", "metadata": {}, "score": "92.12373"}
{"text": "9 , supplement 11 , article S7 , 2008 .View at Google Scholar .57 , no . 1 , pp .96 - 113 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "92.67801"}
{"text": "Acknowledgements .This work was supported by part by Grants R01 LM7659 , R01 LM8635 from the National Library of Medicine , and Grants NSF - DMS-0504957 , NSF- IIS-0430743 from the National Science Foundation .We would like to thank Lyudmila Shagina for providing technical support .", "label": "", "metadata": {}, "score": "93.18156"}
{"text": "Challenge 3 , Pair 81 ( False ) .T :According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart .H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory .", "label": "", "metadata": {}, "score": "93.41659"}
{"text": "Department of Biomedical Informatics , Columbia University .Department of Biostatistics , Bioinformatics and Biomathematics , Georgetown University Medical Center .References .Krallinger M , Valencia A : Text - mining and information - retrieval services for molecular biology .Genome Biol 2005 , 6 : 224 .", "label": "", "metadata": {}, "score": "96.73013"}
{"text": "For BSA , RSV and BPD , we found that the effect of the sense distribution on the error rate was insignificant .For PCA this effect was significant .The effect of different sample sizes on the error rate was significant for BSA , RSV , and BPD .", "label": "", "metadata": {}, "score": "97.486336"}
{"text": "e . s .t .o .t . a .l .n .o . . .o .f .t .e . s .t .e . d .i .n .s .", "label": "", "metadata": {}, "score": "111.36496"}
{"text": "i .t .h .c .o .r .r .e . c . t . a .s .s .i . g .n .e . d .s .e . n .", "label": "", "metadata": {}, "score": "113.88661"}
{"text": "A .c .c . u .r . a .c .y .n .o . . .o .f .i .n .s .t . a .n .c .e . s .", "label": "", "metadata": {}, "score": "115.60087"}
{"text": "T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism .", "label": "", "metadata": {}, "score": "116.70627"}
{"text": "Academic Editor : Farid Melgani .Copyright \u00a9 2012 Gulshan Kumar and Krishan Kumar .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "121.10703"}
{"text": "Eleventh Conf . on Uncertainty in Artificial Intelligence .Morgan Kaufmann .pp .338 - 345 .Design of an Evolutionary Approach for Intrusion Detection .Shaheed Bhagat Singh State Technical Campus , Ferozepur , Punjab 152004 , India .Received 21 August 2013 ; Accepted 16 September 2013 .", "label": "", "metadata": {}, "score": "121.79788"}
{"text": "Academic Editors : C.-C. Chang and F. Yu .Copyright \u00a9 2013 Xin Wang et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "121.85568"}
