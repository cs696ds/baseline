{"text": "All models are used during search , i.e. they are incorporated directly into the log - linear model combination of the decoder .Phrase table smoothing with triplet lexicon models and with discriminative word lexicons are novel contributions .We also propose a new regularization technique for IBM model 1 by means of the Kullback - Leibler divergence with the empirical unigram distribution as regularization term .", "label": "", "metadata": {}, "score": "55.84771"}
{"text": "By default , the scripts use : interpolated , modified Kneser - Ney smoothing , bigram cutoff 3 , trigram cutoff 5 , with an unknown word .I used the top 5 K , 20 K , and 64 K words occurring in the training text as vocabularies .", "label": "", "metadata": {}, "score": "57.610535"}
{"text": "The system then uses quinphone models ( VTLN / SAT trained ) and MLLR with an additional FV transform to process the data ( P4 ) .This stage is repeated twice more while increasing the number of MLLR transforms ( P5/P6 ) .", "label": "", "metadata": {}, "score": "62.360863"}
{"text": "This allows the use of a much wider range of parallel corpora for training , and can be combined with a standard phrase - table using conventional smoothing methods .Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase - based system .", "label": "", "metadata": {}, "score": "63.38452"}
{"text": "Here , as others have done previously ( e.g. [ 15 ] ) , we experimented with building separate language models for each of the 3 data sources and then interpolating the language models .For efficiency and ease of use in decoding , a model merging process was employed using tools supplied by Entropic Ltd. , that gives a similar effect to explicit model interplotation but saves run - time computation and storage .", "label": "", "metadata": {}, "score": "63.679234"}
{"text": "Furthermore the 1998 4-gram language model gave a constant 15 % improvement ( over all test sets ) in perplexity over the equivalent model used in the 1997 evaluation .The overall decoding process proceeds as for the 1997 system , but with a couple of additional stages .", "label": "", "metadata": {}, "score": "63.94357"}
{"text": "In real - world language modelling applications , prediction are ' smoothed ' with statistics from shorter contexts ( Chen and Goodman , 1998 ) .I ran a ' Witten - Bell ' smoothed trigram model on the Alice text : it outperformed both the other Markov models across the . \" ...", "label": "", "metadata": {}, "score": "64.62675"}
{"text": "The final hypothesis combination uses word - level confidence scores based on an N - best homogeneity measure .These are used with the NIST ROVER program [ 1 ] to produce the final output .We first compared the effect of using the additional training data in the BNtrain98 set .", "label": "", "metadata": {}, "score": "65.78629"}
{"text": "While the system produces good results it is computationally expensive : a companion paper [ 12 ] discusses a version of the system that runs in less than ten times real - time on commodity hardware .This work is in part supported by an EPSRC grant on ' ' Multimedia Document Retrieval ' ' reference GR / L49611 and by a grant from DARPA .", "label": "", "metadata": {}, "score": "66.11395"}
{"text": "Overall these changes to the system reduced the error rate by 13 % on the 1997 evaluation data and the final system had an overall word error rate of 13.8 % for the 1998 evaluation data sets .Significant progress in the accurate transcription of broadcast news data has been made over the last few years so that we are now at a point where such systems can be used for a variety of tasks such as audio indexing and retrieval .", "label": "", "metadata": {}, "score": "66.44819"}
{"text": "In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .We explore sourceto - target models with phrase - level as well as sentence - level scoring and target - to - source models with scoring on phrase level only .", "label": "", "metadata": {}, "score": "67.27759"}
{"text": "( Note : there is an erratum on p. 592 , Fig . 7 .You can skip section 4 , as we 'll just discuss Eisner 's method for calculating expectations . )Lari and Young , 1990 .Applications of stochastic context - free grammars using the Inside - Outside algorithm .", "label": "", "metadata": {}, "score": "67.64288"}
{"text": "I 'm not sure where you could find a source stating that trigrams outperform other models , but if you cite Chen and Goodman you can at least say that higher order n - grams are only viable with large data sets .", "label": "", "metadata": {}, "score": "67.77396"}
{"text": "This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "68.41568"}
{"text": "Version for less experienced programmers : Write a program that reads in a text file and produces a file containing unigram , bigram , and trigram MLE probability estimates .Write a second program that reads the data file , inputs text strings one per line , and produces as output the unigram , bigram , or trigram model probability for the string .", "label": "", "metadata": {}, "score": "68.89787"}
{"text": "The idea is that , instead of using the minimal training samples to define proper posteriors for computation of Bayes factors and the ... . \" ... Abstract .Bayesian model comparison requires the specification of a prior distribution on the parameter space of each candidate model .", "label": "", "metadata": {}, "score": "69.24144"}
{"text": "X. Luo of JHU supplied code to help with the soft - clustering experiments .Fiscus , J.G. ( 1997 )A Post - Processing System to Yield Reduced Word Error Rates : Recogniser Output Voting Error Reduction ( ROVER ) .", "label": "", "metadata": {}, "score": "69.25456"}
{"text": "We conclude with an overview of evaluation and notes on future directions . by Michel Simard , Nicola Ueffing , Pierre Isabelle - In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation ( WMT-07 , 2007 . \" ...This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "69.45959"}
{"text": "The paper is arranged as follows .We first give details of the broadcast news data used in the experiments , then give an outline of the overall system used in the 1997 evaluation .The subsequent sections give the details of a number of experiments that we performed in system development .", "label": "", "metadata": {}, "score": "69.57154"}
{"text": "The basic adaptation approach in our system remains MLLR for both means and variances [ 2 ] .In addition , for the quinphone stage of iterative unsupervised adaptation , the effect of a single full variance ( FV ) transform [ 3 ] was investigated .", "label": "", "metadata": {}, "score": "69.8984"}
{"text": "P1 to P3 use triphones and P4-P6 quinphones .The results ( over the complete 1998 evaluation set ) for each of these stages , together with additional contrasts , is shown in Table 6 .There is a 12 % reduction in error by using gender dependent models and VTLN ( P1 to P2 ) and a further 7 % from using MLLR .", "label": "", "metadata": {}, "score": "70.003265"}
{"text": "System details can be found in [ 16 ] .The data segmentation [ 4 ] aims to generate acoustically homogeneous speech segments and discard non - speech portions such as pure music .It uses a set of Gaussian mixture models to classify the data as to type ( wideband speech , narrow - band speech , pure music , speech and music ) , and then any pure music is discarded .", "label": "", "metadata": {}, "score": "70.27588"}
{"text": "We therefore did not include FD modelling in the 1998 evaluation system .The soft - clustering technique developed at JHU [ 9 ] had shown worthwhile reductions in word error rate on the Switchboard corpus and we performed a preliminary evaluation on Broadcast News data .", "label": "", "metadata": {}, "score": "70.41913"}
{"text": "This survey presents a tutorial overview of state - of - the - art SMT at the beginning of 2007 .We begin with the context of the current research , and then move to a formal problem description and an overview of the four main subproblems : translational equivalence modeling , mathematical modeling , parameter estimation , and decoding .", "label": "", "metadata": {}, "score": "71.285385"}
{"text": "This data was annotated to ensure that each segment was acoustically homogeneous ( same speaker , background noise condition and channel ) .The LDC released a further tranche of data of similar size in 1998 ( about 71 hours of data ) .", "label": "", "metadata": {}, "score": "71.516525"}
{"text": "The system takes the 1997 system and includes the additional acoustic training data in BNtrain98 ; cluster - based normalisation and VTLN ; the revised language modelling data and build procedure and full variance adaptation with SAT training .The word N - grams were trained by interpolating ( and merging ) component LMs trained on the acoustic transcriptions , the broadcast news texts and the newspaper texts .", "label": "", "metadata": {}, "score": "71.56549"}
{"text": "This is followed by generating a lattice for each segment using the adapted triphone models with a bigram LM , expanding these lattices using a word 4-gram interpolated with a category trigram LM , and performing iterative lattice rescoring and MLLR adaptation with a set of quinphone HMMs .", "label": "", "metadata": {}, "score": "71.73859"}
{"text": "( For some reason this , and not the original Baker paper , is the standard reference .The derivation of the update equations is not as simple as it could be , I think . )Chiang , 2003 , Mildly context sensitive grammars for estimating maximum entropy models .", "label": "", "metadata": {}, "score": "72.118675"}
{"text": "Using a clustering procedure and a set of smoothing rules the final segments to be processed by the decoder are generated .For recognition , each frame of input speech is represented by a 39 dimensional feature vector that consists of 13 ( including ) MF - PLP cepstral parameters and their first and second differentials .", "label": "", "metadata": {}, "score": "72.18094"}
{"text": "Central to several objective approaches to Bayesian model selection is the use of training samples ( subsets of the data ) , so as to allow utilization of improper objective priors .The most common prescription for choosing training samples is to choose them to be as small as possible , subject to yielding proper posteriors ; these are called minimal training samples . . .. d Lin ( 2002 ) , and Paulo ( 2002 ) .", "label": "", "metadata": {}, "score": "72.831924"}
{"text": "The word error rate on BNeval97 of 14.3 % ( including FV and SAT ) represents a 13 % reduction relative to the same stage of the 1997 evaluation system [ 16 ] .We have recently experimented with discriminative training of large vocabulary systems and using the frame discrimination ( FD ) technique [ 13 ] .", "label": "", "metadata": {}, "score": "73.416916"}
{"text": "The combined set of 1997 and 1998 data is denoted BNtrain98 .System development mainly used the 1997 Hub4 evaluation data , BNeval97 .BNeval97 was taken from a number of sources broadcast in October / November 1996 and was presented to the system as a single 3 hour file .", "label": "", "metadata": {}, "score": "73.51805"}
{"text": "Because of this , using trigrams is a good compromise which often yields good results .In a study by Chen and Goodman ( 1998 ) , the effect of varying n - gram orders on the performance of a language model was compared .", "label": "", "metadata": {}, "score": "73.65321"}
{"text": "The context of a whole corpus of automatic translations rather than a single sentence is taken into account in order to achieve high alignment quality .The confusion network is rescored with a special language model , and the consensus translation is extracted as the best path .", "label": "", "metadata": {}, "score": "73.68196"}
{"text": "Abstract .Bayesian model comparison requires the specification of a prior distribution on the parameter space of each candidate model .In this connection two concerns arise : on the one hand the elicitation task rapidly becomes prohibitive as the number of models increases ; on the other hand numerous prior specifications can only exacerbate the well - known sensitivity to prior assignments , thus producing less dependable conclusions .", "label": "", "metadata": {}, "score": "74.31028"}
{"text": "This poorer performance was also reflected in the number of frames assigned to multiple speaker segments : 1.6 % for BNeval97 but 4.3 % for BNeval98 .This paper has described the development and performance of the 1998 HTK broadcast news transcription system .", "label": "", "metadata": {}, "score": "74.5372"}
{"text": "I. . ... andard word alignment and translation models described in Section IV - A1 .The system also used two large language models , both trained on the same data : a 4-gram language model using modified Kneser - Ney smoothing , and a suffix array language model with arbitrary history l .. \" ...", "label": "", "metadata": {}, "score": "74.606"}
{"text": "The category - trigram used 1000 automatically derived word classes and was trained using LMtrain98 .Category bigrams and trigrams were added only if the leave - one training set likelihood improved and the final category model contained 0.85 million bigrams and 9.4 million trigrams .", "label": "", "metadata": {}, "score": "74.99549"}
{"text": "4.2 Collocation Correction with Phrase - based SMT We implement our approach in the fram ... . \" ...We report on efforts to build large - scale translation systems for eight European language pairs .We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation .", "label": "", "metadata": {}, "score": "75.0453"}
{"text": "We developed a fast implementation technique to make FD training on large HMM sets practical and on the WSJ / NAB task FD gives similar reductions in word error rate ( about 5 % relative ) to lattice - based MMIE with a much smaller computational cost .", "label": "", "metadata": {}, "score": "75.0739"}
{"text": "SMT has made tremendous strides in less than two decades , and many popular tec ... \" .Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .", "label": "", "metadata": {}, "score": "75.09305"}
{"text": "We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation . \" ...In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .", "label": "", "metadata": {}, "score": "75.99808"}
{"text": "Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase .", "label": "", "metadata": {}, "score": "76.37017"}
{"text": "IEEE Workshop on Automatic Speech Recognition and Understanding , pp .347 - 354 , Santa Barbara .Tools . \" ...Central to several objective approaches to Bayesian model selection is the use of training samples ( subsets of the data ) , so as to allow utilization of improper objective priors .", "label": "", "metadata": {}, "score": "76.38748"}
{"text": "The full recognition results from the various stages of operation are included .This section describes the various data sets that have been used in the experiments reported in the paper .The baseline acoustic corpus available in 1997 used recorded audio from various US broadcast news shows ( television and radio ) .", "label": "", "metadata": {}, "score": "76.39569"}
{"text": "One effective way to do that is to post - edit the translations produced by a vanilla RBMT system using a specially - trained statistical machine translation ( SMT ) system .Our experiments indicate that this method is just as effective as manual customization of system dictionaries in reducing the need for manual post - editing . ... entence - length feature .", "label": "", "metadata": {}, "score": "76.88548"}
{"text": "In particular , we 've found that a kind of model called a deep convolutional neural network can achieve reasonable performance on hard visual recognition tasks - matching or exceeding human performance in some domains .Researchers have demonstrated steady progress in computer vision by validating their work against ImageNet - an academic benchmark for computer vision .", "label": "", "metadata": {}, "score": "76.93038"}
{"text": "In particular , the example illustrated in Fig .1 has useful tutorial value , and I regular refer people to it .The models used in the example are deliberately crude , to construct a clear example .Nevertheless , I think it is worth explicitly reviewing why the more powerful model suffers from the catch - up phenomenon , and how it might be avoided through hierarchical modelling .", "label": "", "metadata": {}, "score": "76.99407"}
{"text": "Here 's an opinionated guide on how to develop a containerized application on your local machine with Vagrant , and then deploy it to GCE .We use an nginx container with a customized index page as an example .Schedule of Topics .", "label": "", "metadata": {}, "score": "78.3782"}
{"text": "Finally , the relative merits of each procedure are evaluated through simulated and real data sets .Key words and phrases : Bayes factor , compatible prior , conjugate prior , g - prior , hypothesis testing , Kullback - Leibler projection , nested model , variable selection . \" ...", "label": "", "metadata": {}, "score": "78.38254"}
{"text": "The HTK Broadcast News Transcription System used in the 1997 DARPA / NIST Hub4 evaluation had an overall word error rate of 15.8 % .This paper describes a number of experiments with , and developments of , that system .Some of these were included in 1998 HTK Hub4 evaluation system .", "label": "", "metadata": {}, "score": "78.42245"}
{"text": "It can be seen that the new training corpus reduces the WER by a 0.7 % absolute and a further 0.5 % absolute reduction was obtained by using a merged interpolated language model .The merged interpolated models gave most improvement on the spontaneous speech portions of the data .", "label": "", "metadata": {}, "score": "78.45478"}
{"text": "Is your idf computed just of the corpus of documents you 're interested in ?If so you could try using a much larger general corpus , or a combination of the two . - hippietrail Oct 26 ' 12 at 5:54 .", "label": "", "metadata": {}, "score": "78.62473"}
{"text": "Given an encompassing , or full , model together with a prior on its parameter space , we review and summarize a few procedures for deriving priors under a submodel , namely marginalization , conditioning , and Kullback - Leibler projection .", "label": "", "metadata": {}, "score": "78.89987"}
{"text": "This paper [ Chen&Goodman ] is a pretty good summary of many of them .In particular , you sound like you might be interested in the Kneser - Ney smoothing algorithm that works in the way you suggest ( backing off to lower length n - grams ) .", "label": "", "metadata": {}, "score": "79.02419"}
{"text": "5 ] and applied in their respective translation systems for phrase table smoothing .Chiang et al .[ 15 ] suggested morphology - based and provenance - based improvements to the Koehn - Och - Marcu method recently ... ABSTRACT .", "label": "", "metadata": {}, "score": "79.11711"}
{"text": "[ Before ROVER combination an alignment pass was run to get exact word timings .Due to the effects of automatic segmentation this process reduces the WER by about 0.1 % absolute .] Table 6 : Word error rates for each stage of the 1998 HTK broadcast news evaluation system ( also P4 FV contrast ) .", "label": "", "metadata": {}, "score": "79.11921"}
{"text": "Chen , Stanley F. and Goodman , Joshua .An empirical study of smoothing techniques for language modeling .Harvard University , Center for Research in Computing Technology , TR-10 - 98:1 - 63 , 1998 .This is a recipe to train word n - gram language models using the newswire text provided in the English Gigaword corpus ( 1200 M words of NYT , APW , AFE , XIE ) .", "label": "", "metadata": {}, "score": "79.2077"}
{"text": "Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .The consensus translation is computed by weighted majority voting on a confusion network , similarly to the well - established ROVER approach of Fiscus for combining speech recognition hypotheses .", "label": "", "metadata": {}, "score": "79.262085"}
{"text": "The triphone HMMs were estimated using BNtrain97 and contained 6684 decision - tree clustered states [ 17 ] , each with 12 Gaussians per state while the quinphone models used 8180 states and 16 Gaussians per state .The HMMs were initially trained on all the wide - band analysed training data .", "label": "", "metadata": {}, "score": "79.387245"}
{"text": "The results show that the proposed estimator outperforms modified Kneser - Ney smoothing in terms of perplexity on unseen data .Keywords : machine learning ; categorical variables ; classification hierarchies ; language modelling ; statistical estimation We propose a novel machine learning technique that can be used to estimate probability distributions for categorical random variables that are equipped with a natural set of classification hierarchies , such as words equipped with word class hierarchies , wordnet hierarchies , and suffix and affix hierarchies .", "label": "", "metadata": {}, "score": "79.64074"}
{"text": "Feature function weights in the log - linear model are set using Och 's minimum ... . \" ...Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .", "label": "", "metadata": {}, "score": "79.992096"}
{"text": "In this paper , we study the use of power priors in Bayesian reinforcement learning .We start by describing the basics of power prior distributions .We then develop power priors for unknown Markov decision processes incorporating historical data .Finally , we apply the power priors approach to learning an intervention timing task . \" ...", "label": "", "metadata": {}, "score": "80.26521"}
{"text": "Experimental results on the test data of the previous campaign are presented . ... to Canadian universities for research and education purposes . \" ...Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .", "label": "", "metadata": {}, "score": "80.50221"}
{"text": "The reduced bandwidth models are used for data classified as narrow band .The system uses the LIMSI 1993 WSJ pronunciation dictionary augmented by pronunciations from a TTS system and hand generated corrections for a 65k word vocabulary .The 1997 system used N - gram language models trained on 132 million words of broadcast news texts , the LDC - distributed 1995 newswire texts , and the transcriptions from BNtrain97 ( LMtrain97 ) .", "label": "", "metadata": {}, "score": "80.8061"}
{"text": "but the equations look pretty complicated .- adi92 Jun 11 ' 10 at 17:19 .But I guess , I could somehow adapt some of them to do something meaningful with IDFs as well .I really do n't want to do something too elaborate or complicated .", "label": "", "metadata": {}, "score": "80.87015"}
{"text": "In particular , the example illustrated in Fig .1 has useful tutorial value , and I regular refer people to it .The models used in the example are deliberately crude , to construct a clear example .Nevertheless , I think it is worth explicitl ... \" .", "label": "", "metadata": {}, "score": "81.88443"}
{"text": "This transcription is used for both gender selection as well as VTLN warp selection for each segment cluster .Gender dependent VTLN models are then used ( P2 ) to provide a revised transcription which is used to estimate global mean and variance MLLR transforms for each cluster .", "label": "", "metadata": {}, "score": "82.00952"}
{"text": "For instance , it has been stated ( verbally ) to me on several occasions that trigram models outperform PCFG s. .What would be a good source to cite for trigram models being something of a standard ?What would be a good source to cite for the claim that trigram models outperform more complex models such as PCFGs ?", "label": "", "metadata": {}, "score": "82.12865"}
{"text": "Mixture of Experts [ 17 ] , 5 .Incorporating Prior Information by Creating Virtual Examples [ 13 ] , 6 .INTEGRATIVE ANALYSIS OF DATA , LITERATURE , AND EXPERT KNOWLEDGE BY BAYESIAN NETWORKS -p. 28/41The two - step , Bayesian methodology for the fusion of knowledge and data Black -boxsmodel Samples Expert ...", "label": "", "metadata": {}, "score": "82.311966"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "82.37979"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "82.37979"}
{"text": "The results show that the proposed estimator outperforms modified Kneser - Ney smoothing in terms of perplexity on unseen data .Keywords : machine learning ; categorical variables ; classification hierarchies ; language modelling ; statistical estimation .dc.contributor.corporation .Copenhagen Business School .", "label": "", "metadata": {}, "score": "83.03748"}
{"text": "( Only tangentially related .Implicitly uses the idea of semirings to generalize Inside - Outside and conditional random fields to a large class of grammar formalisms beyond CFG . )Paul Smith , \" Statistics , Machine Learning and Data Mining \" , Monday , October 25 , 2004 , 4:00 pm , MATH 3206 , Abstract is at the Mathematics Graduate Minicourse Series page , along with links to relevant papers .", "label": "", "metadata": {}, "score": "83.15154"}
{"text": "If I were dealing with probabilities , I would try interpolation or backoff models .I am not sure what assumptions / intuitions those models leverage to perform well , and so how well they would do for IDF scores .Anybody has any better ideas ?", "label": "", "metadata": {}, "score": "83.48823"}
{"text": "Finally , we decided to use a different ( though similarly sized ) portion of newspaper texts covering 1995 to February 1998 ( about 70MW in total ) .All these sources excluded data from the designated test epochs .This corpus was denoted LMtrain98 .", "label": "", "metadata": {}, "score": "83.87169"}
{"text": "Here are some OOV% and perplexity results measured on three different held - out evaluation test sets : Hierarchy - based Partition Models : Using Classification Hierarchies to .We propose a novel machine learning technique that can be used to estimate probability distributions for categorical random variables that are equipped with a natural set of classification hierarchies , such as words equipped with word class hierarchies , wordnet hierarchies , and suffix and affix hierarchies .", "label": "", "metadata": {}, "score": "83.890076"}
{"text": "This FV transform was used with , for the wideband data , HMMs estimated with a single iteration of speaker adaptive training ( SAT ) [ 14 ] to update the mean parameters .The effect of these changes is shown in Table 5 .", "label": "", "metadata": {}, "score": "84.034874"}
{"text": "tokenized / news.20 ? ?maybe ... ... allows you to run a command and see what it does to your files without actually doing it !After reviewing the operations listed , you can then decide whether you really want these things to happen or not .", "label": "", "metadata": {}, "score": "84.12805"}
{"text": "Option 1 .Define language model uses linear interpolation to combine evidence from models at the word level and the character level .( The previous assignments provide you with implementations of these component models .Clever , huh ? )Train the coefficients of the interpolated model using EM .", "label": "", "metadata": {}, "score": "84.16321"}
{"text": "Table 7 : OOV rate and perplexities of the 1998 evaluation LMs .Perplexities shown for trigram ( tg ) , 4-gram ( fg ) and word 4-gram interpolated with category trigram ( fgintcat ) .The out - of - vocabulary ( OOV ) rate and perplexity of these language models on BNeval97 and the two halves of the BNeval98 set is shown in Table 7 .", "label": "", "metadata": {}, "score": "84.88286"}
{"text": "Exercise for more experienced programmers : same exercise but using character N - grams up to arbitary N. ( You can hard - code N. ) .[ September 9 ] Topic Area 2 .Extend the previous assignment so your language model estimates use smoothed probabilities .", "label": "", "metadata": {}, "score": "85.13336"}
{"text": "We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "85.18587"}
{"text": "I do n't really know how how you might integrate them with IDF scores , or even if that 's really what you want to do .Most of these smoothing models look like they work with moving probability mass around from term to term in elaborate ways , to make the model be able to recognize a language better .", "label": "", "metadata": {}, "score": "85.6762"}
{"text": "When two nested models are compared , using a Bayes factor , from an objective standpoint , two seemingly conflicting issues emerge at the time of choosing parameter priors under the two models .On the one hand , for moderate sample sizes , the evidence in favor of the smaller model can be infl ... \" .", "label": "", "metadata": {}, "score": "85.99778"}
{"text": "Such evidence , often available as historical data , can be quite useful when learning a new task from reinforcement .In this paper , we study the ... \" .Power priors allow us to introduce into a Bayesian algorithm a relative precision parameter that controls the influence of external evidence on a new task .", "label": "", "metadata": {}, "score": "86.264305"}
{"text": "Topics : .Mehryar Mohri and Michael Riley ( 2002 ) .Tutorial on Weighted Finite - State Transducers in Speech Recognition , ( Part 1 ) , International Conference on Spoken Language Processing 2002 ( ICSLP ' 02 ) , Denver , Colorado , September 2002 .", "label": "", "metadata": {}, "score": "86.41081"}
{"text": "We use a maximum likelihood technique to select the best data warp factor via a parabolic search .It is important when comparing the warped data likelihoods to properly take into account the effect of the transformation .We have done this implicitly by performing variance normalisation on the data .", "label": "", "metadata": {}, "score": "87.011444"}
{"text": "Furthermore , in line with the triphone figures , the overall gain for 1998 trained MLLR adapted quinphone models was 0.4 % absolute due to VTLN .For the 1998 system , the additional transcriptions from the 1998 acoustic training were available .", "label": "", "metadata": {}, "score": "87.18642"}
{"text": "This result still holds for text actually generated from a 2nd order Markov model , when the trigram statistics are matched to English characters .The subjective Bayesian demands an explanation : we should use the model we believe , regardless of how much data we have .", "label": "", "metadata": {}, "score": "87.30203"}
{"text": "I am trying to use IDF scores to find interesting phrases in my pretty huge corpus of documents .For example , \" you 've never tried \" has a very high idf , while each of the component unigrams have very low idf .", "label": "", "metadata": {}, "score": "87.852936"}
{"text": "There is a 6 % gain from employing the category trigram and 4-gram over the trigram alone , and a 7 % gain moving from adapted triphones to adapted quinphones : most of which ( 5 % ) was due to the full variance adaptation .", "label": "", "metadata": {}, "score": "88.114075"}
{"text": "His abstract is at the Logic and AI Seminar Series , along with links to relevant papers .See Noah 's slides plus a very nice handout containing mathematical details .( Note : you might need texpoint , the tool that lets you put latex equations into Powerpoint slides , to view the formulas properly . )", "label": "", "metadata": {}, "score": "89.08897"}
{"text": "I take it that \" you 've never tried \" is a phrase that you do n't want to extract , but which has high IDF .The problem will be that there are going to be a vast number of n - grams that only occur in one document and so have the largest possible IDF score .", "label": "", "metadata": {}, "score": "89.367386"}
{"text": "Our key assumption is that collocation errors are often caused by semantic similarity in the first language ( L1language ) of the writer .An analysis ... \" .We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora .", "label": "", "metadata": {}, "score": "89.57337"}
{"text": "Table 4 : % WER on BNeval97 for different trigram LMs with VTLN unadapted triphone HMMs with either pooled data or ( merged ) interpolated LMs .The effect of using three different LMs on BNeval97 with VTLN data and 1998 unadapted triphone HMMs is shown in Table 4 .", "label": "", "metadata": {}, "score": "89.62181"}
{"text": "Many thanks to Bill Byrne , David Chiang , Bonnie Dorr , Jason Eisner , Christof Monz , David Smith , Noah Smith , and undoubtedly others I 'm forgetting , for discussions about the syllabus .Responsibility for the outcome is , of course , completely indeterminate .", "label": "", "metadata": {}, "score": "90.22215"}
{"text": "Experiments with no adaptation ( or cluster - based normalisation ) showed that the word error rate ( WER ) was reduced by up to 0.9 % absolute .However when MLLR adaptation and VTLN were applied ( see below ) the WER gain was reduced to 0.4 % absolute .", "label": "", "metadata": {}, "score": "90.72412"}
{"text": "When two nested models are compared , using a Bayes factor , from an objective standpoint , two seemingly conflicting issues emerge at the time of choosing parameter priors under the two models .On the one hand , for moderate sample sizes , the evidence in favor of the smaller model can be inflated by diffuseness of the prior under the larger model .", "label": "", "metadata": {}, "score": "90.96217"}
{"text": "An analysis of a large corpus of annotated learner English confirms this assumption .We evaluate our approach on real - world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance , homophones , and WordNet synonyms . ... where f denotes a foreign phrase in the L1 language .", "label": "", "metadata": {}, "score": "91.10744"}
{"text": "Initially we used bandwidth independent , gender independent triphones to evaluate the technique and under these conditions it gave a 1 % absolute reduction in WER .However , when bandwidth dependent , gender dependent models with variance normalisation and MLLR adaptation were used , there was no WER advantage and hence soft clustering was not used in the 1998 evaluation system .", "label": "", "metadata": {}, "score": "91.12413"}
{"text": "Researchers both internal and external to Google have published papers describing all these models but the results are still hard to reproduce .We 're now taking the next step by releasing code for running image recognition on our latest model , Inception - v3 .", "label": "", "metadata": {}, "score": "91.391174"}
{"text": "ACL Workshop on SMT , 2007 . \" ...We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "91.41231"}
{"text": "Up to six state - of - the - art statistical phrase - based translation systems from different project partners were combined in the experiments .Significant improvements in translation quality from Spanish to English and from English to Spanish in comparison with the best of the individual MT systems were achieved under official evaluation conditions .", "label": "", "metadata": {}, "score": "91.52131"}
{"text": "With reference to finitely discrete data models , we show that these two issues can be dealt with jointly , by combining intrinsic priors and non - local priors in a new unified class of priors .We illustrate our ideas in a running Bernoulli example , then we apply them to test the equality of two proportions , and finally we deal with the more general case of logistic regression models . .", "label": "", "metadata": {}, "score": "91.67274"}
{"text": "Finally , the effect of the automatic segmentation procedure on the BNeval98 set was investigated .On BNeval97 we had found that automatic segmentation had produced very similar overall accuracy to manually defined segments .However on BNeval98 the automatic segmenter faired more poorly .", "label": "", "metadata": {}, "score": "92.4505"}
{"text": "The evaluation results are presented for each of the NIST ' ' focus ' ' conditions which are shown in Table 1 .The proportion of data of each type in BNeval97 and BNeval98 is given in Table 2 .It can be seen that there is a rather different distribution data type between the two sets : particularly for F0 , F2 , F4 and FX .", "label": "", "metadata": {}, "score": "92.48509"}
{"text": "Similarly , using quinphone models with MLLR a gain of 0.5 % in WER was achieved with increased training data .We also did some experiments that used automatic segmentation of the extended training data to try and ensure that the segments used in training were acoustically homogeneous but this provided no additional improvements .", "label": "", "metadata": {}, "score": "93.07325"}
{"text": "dc.contributor.department .Institut for Internationale Sprogstudier og Vidensteknologi ( . en_US .dc.contributor.departmentshort .ISV ( . en_US .dc.contributor.departmentuk .Department of International Language Studies and Computational Linguistics ( Tools . \" ...Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .", "label": "", "metadata": {}, "score": "94.02037"}
{"text": "A few sample results we obtained at Google on this data are detailed at : papers / naaclhlt2013 .pdf .Besides the scripts needed to rebuild the training / held - out data , it also makes available log - probability values for each word in each of ten feld - out data sets , for each of the following baseline models : . unpruned Katz ( 1.1B n - grams ) , . unpruned Interpolated Kneser - Ney ( 1.1B n - grams ) , .", "label": "", "metadata": {}, "score": "98.06828"}
{"text": "If they were Wikipedia pages on cities , I would expect names of their famous landmarks and attributes to show up instead .My intuition was that I could get such phrases by calculating the IDF of each ( 1,2,3,4)-gram and picking the phrases with lowest IDF to represent a document .", "label": "", "metadata": {}, "score": "98.821625"}
{"text": "We found an overall improvement in WER with cluster - based variance normalisation of 0.3 % absolute and a further 0.6 % absolute by applying VTLN in both training and testing without adaptation .However with mean and variance MLLR adaptation the separate beneficial effect of variance normalisation and VTLN is much reduced .", "label": "", "metadata": {}, "score": "98.98975"}
{"text": "I 'll read again more closely .- Julie Oct 10 ' 12 at 21:52 .I 've never read the book , so you 've probably gotten it right . - jlovegren Oct 11 ' 12 at 0:17 . 1 Answer 1 .", "label": "", "metadata": {}, "score": "100.23428"}
{"text": "Foundations , as used here , are the things that the cutting - edge research papers assume you already understand .To the extent possible , the readings here are available on the Web . \"Additional readings \" are optional links pointing either to material you should already know ( but might want to review ) or to related material you might be interested in .", "label": "", "metadata": {}, "score": "100.34828"}
{"text": "Option 2 .Come up with ( do not look up ! ) an EM algorithm for estimating the parameters of this model .Implement your algorithm , train on a large sample of English , and compare the probabilities of \" colorless green ideas sleep furiously \" versus \" furiously sleep ideas green colorless \" .", "label": "", "metadata": {}, "score": "100.72654"}
{"text": "As you 've shown , a rare phrase is n't necessarily interesting .A phrase with rare words in it is likely to be rare itself , but that does n't mean it will be interesting either . -Tommy Herbert Jun 11 ' 10 at 16:44 .", "label": "", "metadata": {}, "score": "117.50508"}
