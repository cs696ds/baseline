{"text": "We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .", "label": "", "metadata": {}, "score": "34.2829"}
{"text": "It is local , yet can handle also compositional structures . ... full parse of free - text sentences ( e.g. , Bod ( 1992 ) , Magerman ( 1995 ) , Collins ( 1997 ) , Ratnaparkhi ( 1997 ) , and Sekine ( 1998 ) ) .", "label": "", "metadata": {}, "score": "36.770065"}
{"text": "We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .", "label": "", "metadata": {}, "score": "37.13803"}
{"text": "Some of these were included in 1998 HTK Hub4 evaluation system .Other experiments which did n't lead to overall word error rate reductions include discriminative training using the frame discrimination method and use of the soft - clustering technique .The paper is arranged as follows .", "label": "", "metadata": {}, "score": "37.825737"}
{"text": "Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .", "label": "", "metadata": {}, "score": "38.045372"}
{"text": "A methodology that improves the performance of the learning algorithm by means of an automatic reordering of the output sentences is presented .This technique yields a greater deg ... \" .The use of Subsequential Transducers ( a kind of FiniteState Models ) in Automatic Translation applications is considered .", "label": "", "metadata": {}, "score": "38.235786"}
{"text": "We describe two computationally - tractable ways of estimating the parameters of such grammars from a training corpus of synta ... \" .Log - linear models provide a statistically sound framework for Stochastic \" Unification - Based \" Grammars ( SUBGs ) and stochastic versions of other kinds of grammars .", "label": "", "metadata": {}, "score": "38.64891"}
{"text": "At this point , it should be noted that the method illustrated in .FIG .7 is not limited to a unified language model , or even an N - gram language model , but rather , can be helpful in forming language models of any type used in a language processing system where the model is based on a task - independent corpus .", "label": "", "metadata": {}, "score": "38.69454"}
{"text": "Two instantiations of this approach are studied and experimental results for Noun - Phrase ... \" .A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally .The approach learns to identify syntactic patterns by combining simple predictors to produce a coherent inference .", "label": "", "metadata": {}, "score": "38.697372"}
{"text": "In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .", "label": "", "metadata": {}, "score": "38.95094"}
{"text": "Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .", "label": "", "metadata": {}, "score": "39.379646"}
{"text": "More specifically , we present the following results : ( 1 )We exhibit a mutual reduction between query containment and the recognition problem for view - based query answering for C2RPQc 's , i.e. , checking whether a given tuple is in the certain answer to a query .", "label": "", "metadata": {}, "score": "39.471455"}
{"text": "On one hand we provide an empirical comparison of the different models proposed in previous work , as well as experimental data on the different choices left open in those designs .On the other hand we explore the scalability of these models by using larger modular programs as benchmarks .", "label": "", "metadata": {}, "score": "40.084934"}
{"text": "This technique narrows the task - independent corpus , but can identify yet more examples of task specific sentences , phrases , etc . .FIG .7 illustrates a method 200 for creating a language model for a selected application from a task - independent corpus in the manner discussed above .", "label": "", "metadata": {}, "score": "40.16486"}
{"text": "A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .Creating a language model for a language processing system US 7286978 B2 .", "label": "", "metadata": {}, "score": "40.234528"}
{"text": "We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .", "label": "", "metadata": {}, "score": "40.316948"}
{"text": "First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .", "label": "", "metadata": {}, "score": "40.6821"}
{"text": "The language models which we present differ from previous models based on stochastic context - free grammars in that they are highly lexical .In particular , they include the familiar n - gram models as a natural subclass .The motivation for considering this class is to estimate the contribution which grammar can make to reducing the relative entropy of natural language . \"", "label": "", "metadata": {}, "score": "40.910038"}
{"text": "The unified language model has the potential of overcoming the weaknesses of both the word N - gram & CFG language models .However , there is no clear way to leverage domain - independent training corpus or domain - independent language models , including the unified language models , for domain specific applications .", "label": "", "metadata": {}, "score": "41.319656"}
{"text": "( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .", "label": "", "metadata": {}, "score": "41.60519"}
{"text": "We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .", "label": "", "metadata": {}, "score": "41.726482"}
{"text": "In this paper , we start looking into the basic properties of XML data exchange , that is , restructuring of XML documents that conform to a source DTD under a target DTD , and answering queries written over the target schema .", "label": "", "metadata": {}, "score": "41.865345"}
{"text": "The experimental results show that the model combining general and specific features alleviates the sparse data problem .In addition , the weighted probabilistic model based on information gain ratio outperforms the non - weighted model . ... distinctions that have to be taken intosaccount .", "label": "", "metadata": {}, "score": "42.1923"}
{"text": "The paper presents experimental results for recognizing noun phrase , subject - verb and verb - object patterns in l ! ]n - glish .Since the learning approach enables easy port - ing to new domains , we plan to apply it to syntac - tic patterns in other languages and to sub - language patterns for information extraction . ... full parsing and instead to rely only on local information .", "label": "", "metadata": {}, "score": "42.38045"}
{"text": "Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .", "label": "", "metadata": {}, "score": "42.592957"}
{"text": "Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .", "label": "", "metadata": {}, "score": "42.737976"}
{"text": "We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .", "label": "", "metadata": {}, "score": "42.781914"}
{"text": "Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .", "label": "", "metadata": {}, "score": "42.82297"}
{"text": "Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .", "label": "", "metadata": {}, "score": "43.182236"}
{"text": "We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .", "label": "", "metadata": {}, "score": "43.245327"}
{"text": "The subsequent sections give the details of a number of experiments that we performed in system development .This is followed by a description of , and the results from , the 1998 Hub4 evaluation system .The full recognition results from the various stages of operation are included .", "label": "", "metadata": {}, "score": "43.59336"}
{"text": "Working within a concrete task allows us to compare ... . by Marcia Mu\u00f1oz , Vasin Punyakanok , Dan Roth , Day Zimak - IN PROCEEDINGS OF EMNLP - WVLC&apos;99 .ASSOCIATION FOR COMPUTATIONAL LINGUISTICS , 1999 . \" ...A SNoW based learning approach to shallow parsing tasks is presented and studied experimentally .", "label": "", "metadata": {}, "score": "43.721558"}
{"text": "Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .", "label": "", "metadata": {}, "score": "43.785408"}
{"text": "Here , as others have done previously ( e.g. [ 15 ] ) , we experimented with building separate language models for each of the 3 data sources and then interpolating the language models .For efficiency and ease of use in decoding , a model merging process was employed using tools supplied by Entropic Ltd. , that gives a similar effect to explicit model interplotation but saves run - time computation and storage .", "label": "", "metadata": {}, "score": "43.91714"}
{"text": "In a first aspect , a task dependent unified language model for a selected application is created from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .", "label": "", "metadata": {}, "score": "43.940598"}
{"text": "We developed a fast implementation technique to make FD training on large HMM sets practical and on the WSJ / NAB task FD gives similar reductions in word error rate ( about 5 % relative ) to lattice - based MMIE with a much smaller computational cost .", "label": "", "metadata": {}, "score": "44.02012"}
{"text": "Nevertheless , while the CFG provides us with a deeper structure , it is still inappropriate for robust spoken language processing since the grammar is almost always incomplete .A CFG - based system is only good when you know what sentences to speak , which diminishes the value and usability of the system .", "label": "", "metadata": {}, "score": "44.087303"}
{"text": "The evaluation of grammars is not obvious , typically we can assess if the discrepancies between the observables and the prediction of a model consists in under or over - generation .Therefore , it could be necessary to provide a measure of simplicity to select the most ' ' simple ' ' hypothesis given some facts .", "label": "", "metadata": {}, "score": "44.106007"}
{"text": "Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .", "label": "", "metadata": {}, "score": "44.139576"}
{"text": "Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .", "label": "", "metadata": {}, "score": "44.16383"}
{"text": "We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . ... algorithms that use general classifiers to yield the inference .Working within a concrete task allows us to compare ... . by Young - Sook Hwang , So - Young Park , Hoo - Jung Chung , Yong - Jae Kwak , Hae - Chang Rim , 2001 . \" ...", "label": "", "metadata": {}, "score": "44.191223"}
{"text": "Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .", "label": "", "metadata": {}, "score": "44.33621"}
{"text": "Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .", "label": "", "metadata": {}, "score": "44.367706"}
{"text": "The structure of this book follows the standard introductory courses to linguistics , but the emphasis will sometimes be on some advanced notions .Even for ground knowledge , it might be useful to add some readings to fully understand the subject .", "label": "", "metadata": {}, "score": "44.939438"}
{"text": "First and second N - gram language models are built from the word phrases and the task - independent corpus , respectively .The first N - gram language model and the second N - gram language model are combined to form a third N - gram language model .", "label": "", "metadata": {}, "score": "44.95159"}
{"text": "On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .", "label": "", "metadata": {}, "score": "45.2724"}
{"text": "Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .", "label": "", "metadata": {}, "score": "45.507294"}
{"text": "We therefore did not include FD modelling in the 1998 evaluation system .The soft - clustering technique developed at JHU [ 9 ] had shown worthwhile reductions in word error rate on the Switchboard corpus and we performed a preliminary evaluation on Broadcast News data .", "label": "", "metadata": {}, "score": "45.682755"}
{"text": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem- identifying phrase structure .The first is a Markovian approach t ... \" .", "label": "", "metadata": {}, "score": "45.899723"}
{"text": "In doing that , we compare two ways of modeling the problem of learning to recognize patterns and suggest that shallow parsing patterns are bet- ter learned using open / close predictors than using inside / outside predictors . ... to full - sentence parsers .", "label": "", "metadata": {}, "score": "46.058075"}
{"text": "As different applications are developed for language processing , task - dependent ( domain dependent ) language models may be more appropriate , due to their increased specificity , which can also make the language models more accurate than a larger , general purpose language model .", "label": "", "metadata": {}, "score": "46.087307"}
{"text": "To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .", "label": "", "metadata": {}, "score": "46.308628"}
{"text": "On the basis of regular tree grammars , we present a formal framework for XML schema languages .This framework helps to describe , compare , and implement such schema languages in a rigorous manner . ument validation .Our contributions are as follows : 1 .", "label": "", "metadata": {}, "score": "46.483643"}
{"text": "Preferably , assigning probabilities to terminals of the context - free grammars includes normalizing the probabilities of the terminals from the N - gram language model in each of the context - free grammars as a function of the terminals in the corresponding context - free grammar .", "label": "", "metadata": {}, "score": "46.591938"}
{"text": "At least , an appropriate theory must characterize the set of expressions and the set of meanings .For the first goal , an answer can be found in the previous chapters with a generation approach based on phonology , morphology and syntax , taken together .", "label": "", "metadata": {}, "score": "46.65445"}
{"text": "For application developers , a CFG is also often highly labor - intensive to create .A second form of a language model is an N - gram model .Because the N - gram can be trained with a large amount of data , the n - word dependency can often accommodate both syntactic and semantic shallow structure seamlessly .", "label": "", "metadata": {}, "score": "46.657986"}
{"text": "We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .", "label": "", "metadata": {}, "score": "46.82708"}
{"text": "A estimation of the frequencies of use of error paramete ... . \" ...The interest in using Finite - State Models in a large variety of applications is recently growing as more powerful techniques for learning them from examples have been developed .", "label": "", "metadata": {}, "score": "47.037838"}
{"text": "If desired , the second language model can be weighted based on whether the identified text is believed to be accurate .The weighting can be based on the amount of text identified in the task - independent corpus , the number of queries used , etc . .", "label": "", "metadata": {}, "score": "47.114853"}
{"text": "This model is tightly integrated with standard acoustic - phonetic models of the input language and the resulting global model directly supplies , through Viterbi search , an optimal output - language sentence for each input -language utterance .Several extensions to this framework , recently developed to cope with the increasing difficulty of translation tasks , are reviewed .", "label": "", "metadata": {}, "score": "47.17396"}
{"text": "The proposed descriptions of morphology , syntax and semantics are really enriching .Moreover , the concise work of bibliography completes the book and gives to the reader a broad overview of these domains .The large part dedicated to the weighted model of syntax illustrates the purpose of this book .", "label": "", "metadata": {}, "score": "47.250603"}
{"text": "This class derives from link grammar , a context - free formalism for the description of natural language .We describe an algorithm for determining maximum - likelihood estimates of the parameters of these models .The language models which we present differ from previous models based on stochastic context - free grammars in that they are highly lexical .", "label": "", "metadata": {}, "score": "47.25488"}
{"text": "X. Luo of JHU supplied code to help with the soft - clustering experiments .Fiscus , J.G. ( 1997 )A Post - Processing System to Yield Reduced Word Error Rates : Recogniser Output Voting Error Reduction ( ROVER ) .", "label": "", "metadata": {}, "score": "47.421844"}
{"text": "The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .", "label": "", "metadata": {}, "score": "47.52597"}
{"text": "The former re ... . by Dennis Shasha , Jason T. L. Wang , Rosalba Giugno - In Symposium on Principles of Database Systems , 2002 . \" ...Modern search engines answer keyword - based queries extremely efficiently .The impressive speed is due to clever inverted index structures , caching , a domain - independent knowledge of strings , and thousands of machines .", "label": "", "metadata": {}, "score": "47.58918"}
{"text": "The first part is about combinatorical approaches that put the emphasis on the way words combine .Reducing the vocabulary complexity can be carried out by defining classes of words by distributional equivalence .And given a reduced set of word classes , categorial grammars are introduced as a mean of characterizing the set of well - formed expressions .", "label": "", "metadata": {}, "score": "47.623314"}
{"text": "Although we show that inclusion and intersection are already intractable for very weak expressions , we also identify some tractable cases .For equivalence , we only prove an initial tractability result leaving the complexity of more general cases open .The main motivation for this research comes from database theory , or more specifically XML and semi - structured data .", "label": "", "metadata": {}, "score": "47.654762"}
{"text": "We show a learning - based method for low - level vision problems .We set - up a Markov network of patches of the image and the underlying scene .A factorization approximation allows us to easily learn the parameters of the Markov network from synthetic examples of image / scene pairs , and to e ciently prop ... \" .", "label": "", "metadata": {}, "score": "47.678062"}
{"text": "We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem - identifying phrase structure .", "label": "", "metadata": {}, "score": "47.810406"}
{"text": "We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .", "label": "", "metadata": {}, "score": "47.88399"}
{"text": "The method is oriented for learning to parse any selected subset of target syntactic structures .It is local , yet can handle also compositional structures .In this paper , a memory - based parsing method is extended for handling compositional structures .", "label": "", "metadata": {}, "score": "47.890545"}
{"text": "The attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability .We show how a variety of methods and models relate to this framework including models for self - similar and 1/f processes .", "label": "", "metadata": {}, "score": "47.93151"}
{"text": "( 3 ) We present a technique to obtain view - based query answering algorithms that compute the whole set of tuples in the certain answer , instead of requiring to check each tuple separately .The technique is parametric wrt the query language , and can be applied both to C2RPQc 's and to tree - queries . ... een investigated and largely solved for relational databases .", "label": "", "metadata": {}, "score": "48.143547"}
{"text": "We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .", "label": "", "metadata": {}, "score": "48.21061"}
{"text": "In this paper we show how to define CHR programs that extend arbitrary solvers and fully interact with them .In the process , we examine how to compile such programs to perform as little recomputation as possible , and describe how to build index structures for CHR constraints that are modified automatically when variables in the underlying solver change .", "label": "", "metadata": {}, "score": "48.2574"}
{"text": "These models flourished in the eighties and early nineties in parallel to object oriented models and their influence gradually faded with the emergence of other database models , particularly the geographical , spatial , semistructured and XML .Recently , the need to manage information with inherent graph - like nature has brought back the relevance of the area .", "label": "", "metadata": {}, "score": "48.318954"}
{"text": "Tools . by Makoto Murata , Dongwon Lee , Murali Mani - EXTREME MARKUP LANGUAGES , 2001 . \" ...On the basis of regular tree grammars , we present a formal framework for XML schema languages .This framework helps to describe , compare , and implement such schema languages in a rigorous manner .", "label": "", "metadata": {}, "score": "48.32257"}
{"text": "This iterative process can be repeated as necessary until the errors are corrected and a suitable N - gram model has been obtained .As discussed above , the task - independent corpus is a general corpus and in fact it is likely that most of the corpus is unrelated to the task or application that the developer is interested in .", "label": "", "metadata": {}, "score": "48.747063"}
{"text": "Then , each of the learned grammars was used to stochastically synthesize new melodies ( Composition ) or to classify test melodies ( Style Recognition ) .Our previous studies in this field showed the need of a proper music coding scheme .", "label": "", "metadata": {}, "score": "49.06731"}
{"text": "A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .", "label": "", "metadata": {}, "score": "49.113495"}
{"text": "Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .", "label": "", "metadata": {}, "score": "49.125156"}
{"text": "The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .", "label": "", "metadata": {}, "score": "49.164276"}
{"text": "7 wherein the same reference numerals have been used to identify similar steps .However , method 220 can be used to create an N - gram language model having the non - terminal tokens of the context - free grammars .", "label": "", "metadata": {}, "score": "49.171005"}
{"text": "6 illustrates a method 180 for creating a unified language model for a selected application from a task - independent corpus that includes a large number of phrases that may be of different context .Simple parsing of the task - independent corpus with context - free grammars for the task - dependent application may cause errors , which will then propagate to the N - gram model upon application of an N - gram algorithm .", "label": "", "metadata": {}, "score": "49.42956"}
{"text": "..The previous work was specific for the relational model .However , there is as yet no consensus on how to address the problem of well - designed data in the XML setting [ 10 , 3].It is problematic to evaluate XML normal forms based on update anomalies ; while ... . \" ... Abstract .", "label": "", "metadata": {}, "score": "49.83466"}
{"text": "We apply this to the \\super - resolution & quot ; problem ( estimating high frequency details from a low - resolution image ) , showing good results .For the motion estimation problem , we show resolution of the aperture problem and lling - in arising from application of the same probabilistic machinery . .", "label": "", "metadata": {}, "score": "49.844284"}
{"text": "A language model provides a method or means of specifying which sequences of words in the vocabulary are possible , or in general provides information about the likelihood of various word sequences .Speech recognition is often considered to be a form of top - down language processing .", "label": "", "metadata": {}, "score": "50.057907"}
{"text": "In particular , we develop two general approaches for an important subproblem- identifying phrase structure .The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state - observation dependencies .", "label": "", "metadata": {}, "score": "50.103764"}
{"text": "Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .", "label": "", "metadata": {}, "score": "50.153168"}
{"text": "We describe the relationship between four different methods that can be used to generate valid approximations : the \" Bethe method , \" the \" junction graph method , \" the \" cluster variation method , \" and the \" region graph method . \"", "label": "", "metadata": {}, "score": "50.214417"}
{"text": "It illustrates the generative nature of the memory model framework in that it is able to generate all valid read - write linkings . \" ...Refactoring is a technique to restructure code in a disciplined way originating from the OO - community .", "label": "", "metadata": {}, "score": "50.228607"}
{"text": "Chapter 3 discusses the issue of modularity further .2.7 Chapter Conclusions We have studied several ways in which optimizations based on static analysis can be guaranteed correct for progra ...Graphical Models for Machine Learning and Digital Communication ( 1998 ) .", "label": "", "metadata": {}, "score": "50.4303"}
{"text": "Several models for context - sensitive analysis of modular programs have been proposed , each with different characteristics and representing different trade - offs .The advantage of these context - sensitive analyses is that they provide information which is potentially more accurate than that provided by context - free analyses .", "label": "", "metadata": {}, "score": "50.465515"}
{"text": "As mentioned in the Background section , the application developer should be provided with an efficient method in which an appropriate language model 16 can be created for the selected application .In some applications , a standard N - gram language model will work and any improvements in developing such a model will be valuable .", "label": "", "metadata": {}, "score": "50.47325"}
{"text": "Afterward , weighted regular languages are discussed with the main devices to generate them , weighted finite state automaton / transducers and hidden markov models .The chapter ends with a discussion of external evidence .Chapter 6 : ' ' Semantics ' '", "label": "", "metadata": {}, "score": "50.487045"}
{"text": ".. led out for many NL processing applications , including MT , even in limited domains .Recently , many NL and Computational Linguistic researchers are ( re-)consideri ... .by Francisco Casacuberta , Enrique Vidal - COMPUTATIONAL LINGUISTICS , 2004 . \" ...", "label": "", "metadata": {}, "score": "50.5869"}
{"text": "Often , the rules are applied ... . \" ...Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse .", "label": "", "metadata": {}, "score": "50.594894"}
{"text": "Several models for context - sensitive analysis of modular programs have been proposed , each with different characteristics and representing different trade - offs .The advantage of these context - sensitive analyses is that they provide information which is potentially more accurate than that p ... \" .", "label": "", "metadata": {}, "score": "50.770252"}
{"text": "Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .", "label": "", "metadata": {}, "score": "50.78157"}
{"text": "Then the author tackles the development of formal theory based on Montague grammar .The main concern is to replace Tarski - style induction on sub - formulas by a strict left - to - right method of building the semantic analysis .", "label": "", "metadata": {}, "score": "50.826862"}
{"text": "Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .", "label": "", "metadata": {}, "score": "50.891075"}
{"text": "Furthermore , these models are automatically learned from training data consisting of pairs of natural - language / formal - language sentences .The need for training data is dramatically reduced by performing a two - step learning process based on lexical / phrase categoria tion .", "label": "", "metadata": {}, "score": "50.9264"}
{"text": "And to alleviate the sparse data problem , it integrates general features with specific features .In the training stage , we select useful features after measuring information gain ratio of each features and assign higher weight to more informative feature by adopting the information gain ratio .", "label": "", "metadata": {}, "score": "50.965794"}
{"text": "This technique is based on formal relations between finite - state transducers and rational grammars .Given a training corpus of source - target pairs of sentences , the proposed approach uses statistical alignment methods to produce a set of conventional strings from which a stochastic rational grammar ( e.g. , an n - gram ) is inferred .", "label": "", "metadata": {}, "score": "51.040493"}
{"text": "A second goal is to describe how this topic fits into the even larger field of MR methods and concepts - in particular making ties to topics such as wavelets and multigrid methods .A third is to provide several alternate viewpoints for this body of work , as the methods and concepts we describe intersect with a number of other fields .", "label": "", "metadata": {}, "score": "51.074738"}
{"text": "The word error rate on BNeval97 of 14.3 % ( including FV and SAT ) represents a 13 % reduction relative to the same stage of the 1997 evaluation system [ 16 ] .We have recently experimented with discriminative training of large vocabulary systems and using the frame discrimination ( FD ) technique [ 13 ] .", "label": "", "metadata": {}, "score": "51.113155"}
{"text": "Context - sensitive analysis provides information which is potentially more accurate than that provided by context - free analysis .Such information can then be applied in order to validate / debug the program and/or to specialize the program obtaining important improvements .", "label": "", "metadata": {}, "score": "51.162704"}
{"text": "The identified text of the task - independent corpus is more relevant to the selected task or application ; therefore , a language model derived from the identified text may be more specific than a language model based on the complete task - independent corpus .", "label": "", "metadata": {}, "score": "51.234848"}
{"text": "Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .", "label": "", "metadata": {}, "score": "51.243305"}
{"text": "Furthermore the 1998 4-gram language model gave a constant 15 % improvement ( over all test sets ) in perplexity over the equivalent model used in the 1997 evaluation .The overall decoding process proceeds as for the 1997 system , but with a couple of additional stages .", "label": "", "metadata": {}, "score": "51.243584"}
{"text": "For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .", "label": "", "metadata": {}, "score": "51.451385"}
{"text": "The proposed model exploits context features around the focus word .And to alleviate the sparse data problem , it integrates general features with specific feat ... \" .In this paper , we define the chunking problem as a classification of words and present a weighted probabilistic model for a text chunking .", "label": "", "metadata": {}, "score": "51.533245"}
{"text": "The phrases associated with these grammars would not normally be spoken in the selected application .Thus , the extent or size of the plurality of context - free grammars is less during speech recognition , corresponding to less required storage space in the computer 50 than was used for parsing the task - independent corpus .", "label": "", "metadata": {}, "score": "51.563057"}
{"text": "A method for creating a language model from a task - independent corpus is provided .In one embodiment , a task dependent unified language model is created .The unified language model includes a plurality of context - free grammars having non - terminals and a hybrid N - gram model having at least some of the same non - terminals embedded therein .", "label": "", "metadata": {}, "score": "51.58528"}
{"text": "The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project .Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .Traditional approaches to parsing are highly complex and problem specific .", "label": "", "metadata": {}, "score": "51.730515"}
{"text": "The interest in using Finite - State Models in a large variety of applications is recently growing as more powerful techniques for learning them from examples have been developed .Language Understanding can be approached this way as a problem of language translation in which the target language is a formal language rather than a natural one .", "label": "", "metadata": {}, "score": "51.84893"}
{"text": "Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .", "label": "", "metadata": {}, "score": "51.86277"}
{"text": "Our experimental results shed light on the practical implications of the different design choices and of the models themselves . ...y preliminary experimental data was also reported for an implementation of some of these models in the context of the Ciao system .", "label": "", "metadata": {}, "score": "51.89984"}
{"text": "P1 to P3 use triphones and P4-P6 quinphones .The results ( over the complete 1998 evaluation set ) for each of these stages , together with additional contrasts , is shown in Table 6 .There is a 12 % reduction in error by using gender dependent models and VTLN ( P1 to P2 ) and a further 7 % from using MLLR .", "label": "", "metadata": {}, "score": "51.968285"}
{"text": "We study the complexity of the inclusion , equivalence , and intersection problem for simple regular expressions arising in practical XML schemas .We obtain lower ... \" .We study the complexity of the inclusion , equivalence , and intersection problem for simple regular expressions arising in practical XML schemas .", "label": "", "metadata": {}, "score": "51.996414"}
{"text": "The basic adaptation approach in our system remains MLLR for both means and variances [ 2 ] .In addition , for the quinphone stage of iterative unsupervised adaptation , the effect of a single full variance ( FV ) transform [ 3 ] was investigated .", "label": "", "metadata": {}, "score": "52.00714"}
{"text": "l . .]P .s .u . t .i ._ . )Here represents the special end - of - sentence word .Three different methods are used to calculate the likelihood of a word given history inside a context - free grammar non - terminal .", "label": "", "metadata": {}, "score": "52.10357"}
{"text": "At step 164 , the task - independent corpus is parsed with the plurality of context - free grammars obtained in step 162 in order to identify word occurrences in the task - independent corpus of each of the semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "52.14455"}
{"text": "The motivation for considering this class is to estimate the contribution which grammar can make to reducing the relative entropy of natural language . \" \" ...Abstract .Functional logic programming languages combine the most important declarative programming paradigms , and attempts to combine these paradigms have a long history .", "label": "", "metadata": {}, "score": "52.318436"}
{"text": "The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .", "label": "", "metadata": {}, "score": "52.370667"}
{"text": "In particular , [ 22 ] shows that containment of conjunctive queries is NP - comp ... ABSTRACT .This paper presents the development of the HTK broadcast news transcription system for the November 1998 Hub4 evaluation .Overall these changes to the system reduced the error rate by 13 % on the 1997 evaluation data and the final system had an overall word error rate of 13.8 % for the 1998 evaluation data sets .", "label": "", "metadata": {}, "score": "52.396927"}
{"text": "To create a general purpose language model , such as an N - gram language model , a task - independent corpus of training data can be used and applied as discussed above to an N - gram algorithm .Task - independent corpora are readily available and can comprise compilations of magazines , newspapers , etc . , to name just a few .", "label": "", "metadata": {}, "score": "52.434193"}
{"text": "We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .", "label": "", "metadata": {}, "score": "52.4626"}
{"text": "This was similarly transcribed at the speaker turn level but did n't distinguish between background conditions which meant that marked training segments were no longer necessarily homogeneous .The combined set of 1997 and 1998 data is denoted BNtrain98 .System development mainly used the 1997 Hub4 evaluation data , BNeval97 .", "label": "", "metadata": {}, "score": "52.477303"}
{"text": "As described above , commonly the context - free grammars are written by a developer having at least some knowledge of what phrases may be used in the selected application for each of the semantic or syntactic concepts , but the extent of knowledge about such phrases is not complete .", "label": "", "metadata": {}, "score": "52.535316"}
{"text": "Word phrases are generated from the plurality of context - free grammars .The context - free grammars are used for formulating an information retrieval query from at least one of the word phrases .The task - independent corpus is queried based on the query formulated and text in the task - independent corpus is identified based on the query .", "label": "", "metadata": {}, "score": "52.580315"}
{"text": "The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .", "label": "", "metadata": {}, "score": "52.59246"}
{"text": "We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .", "label": "", "metadata": {}, "score": "52.625076"}
{"text": "One of these areas is machine translation , in which the approaches that are based on building models automatically from training examples are becoming more and more attractive .Finite - state transducers are very adequate for use in constrained tasks in which training samples of pairs of sentences are available .", "label": "", "metadata": {}, "score": "52.65181"}
{"text": "Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .", "label": "", "metadata": {}, "score": "52.75459"}
{"text": "The mapping from the input to the output language is modeled in terms of a finite state translation model which is learned from examples of input - output sentences of the task considered .Thi ... \" .A fully integrated approach to Speech - Input Language Translation in limited - domain applications is presented .", "label": "", "metadata": {}, "score": "52.8173"}
{"text": "Thus , to first understand the main concepts in this book , the reader should be familiar with the content of Manning and Sch\u00fctze ( 1999 ) .Chapter 2 : ' ' Elements ' 'A primary concern of mathematical linguistics is to define useful and relevant elements and their associated sets .", "label": "", "metadata": {}, "score": "52.84751"}
{"text": "However there is still much interest in reducing the error rate of such systems further which will increase the potential for further applications as well as establishing techniques for the accurate transcription of general audio material .The HTK Broadcast News Transcription System used in the 1997 DARPA / NIST Hub4 evaluation had an overall word error rate of 15.8 % .", "label": "", "metadata": {}, "score": "53.059578"}
{"text": "Tools . \" ...We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints .In particular , we develop two general approaches for an important subproblem - identifying phrase structure .", "label": "", "metadata": {}, "score": "53.109756"}
{"text": "Some very preliminary experimental results have also been reported for some of these models which provided initial evidence on their potential .However , further experimentation , which is needed in order to understand the many issues left open and to show that the proposed modes scale and are usable in the context of large , real - life modular programs , was left as future work .", "label": "", "metadata": {}, "score": "53.28885"}
{"text": "The second is an extension of constraint satisfaction formalisms .We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing . 1 Introduction In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints .", "label": "", "metadata": {}, "score": "53.332287"}
{"text": "The training data are stored as - is , in efficient suttix - tree data structures .Generalization is performed on - line at recognition time by compar - ing subsequences of the new text to positive and negative evidence in the corIms .", "label": "", "metadata": {}, "score": "53.398766"}
{"text": "Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .", "label": "", "metadata": {}, "score": "53.40252"}
{"text": "IEEE Workshop on Automatic Speech Recognition and Understanding , pp .347 - 354 , Santa Barbara .OCLC:26847322 .Abstract : \" In this paper we present a new class of language models .This class derives from link grammar , a context - free formalism for the description of natural language .", "label": "", "metadata": {}, "score": "53.419186"}
{"text": "Each of the identified word occurrences is replaced with corresponding non - terminal tokens .A N - gram model is then built having the non - terminal tokens .A third aspect is a method for creating a language model for a selected application from a task - independent corpus .", "label": "", "metadata": {}, "score": "53.69227"}
{"text": "Each of the context - free grammars include words or terminals present in the task - independent corpus to form the semantic or syntactic concepts .The task - independent corpus with the plurality of context - free grammars is parsed to identify word occurrences of each of the semantic or syntactic concepts and phrases .", "label": "", "metadata": {}, "score": "53.735397"}
{"text": "We consider conjunctive two - way regular path queries ( C2RPQc 's ) , which extend regular path queries with two fea ... \" .Abstract .The basic querying mechanism over semistructured data , namely regular path queries , asks for all pairs of objects that are connected by a path conforming to a regular expression .", "label": "", "metadata": {}, "score": "53.778454"}
{"text": "As for all the subjects addressed by the author , this is not a cookbook .More basic and pedagogical readings are necessary if the reader wants to understand or to build some NLP systems .Here , the purpose is more about the mathematical foundation of such systems and tries to describe their limits .", "label": "", "metadata": {}, "score": "53.877884"}
{"text": "The uniform model does not capture the empirical word distribution underneath a context - free grammar non - terminal .A better alternative is to inherit existing domain - independent word tri - gram probabilities .These probabilities need to be appropriately normalized in the same probability space .", "label": "", "metadata": {}, "score": "53.908882"}
{"text": "Generally , another aspect of the present invention includes using the context - free grammars for the task - dependent application to form phrases , sentences or sentence fragments that can then be used as queries in an information retrieval system .", "label": "", "metadata": {}, "score": "54.161583"}
{"text": "Whereas this development is quite understandable , motivations and goals are not obvious .At this point , some readers may wonder : what is the contribution in terms of knowledge or in terms of a deeper understanding of phonology ?Thus , this chapter is not easy to read , but what is more important , is a bit disappointing .", "label": "", "metadata": {}, "score": "54.27545"}
{"text": "The interest in modeling musical style resides in the use of these models in applications , such as Automatic Composition and Automatic Musical Style Recognition .We have studied three GI Algorithms , which have been previously applied successfully in other fields .", "label": "", "metadata": {}, "score": "54.55505"}
{"text": "Also , the search engine 114 accesses the language model 16 .The language model 16 is a unified language model or a word N - gram or a context - free grammar that is used in identifying the most likely word represented by the input speech .", "label": "", "metadata": {}, "score": "54.592632"}
{"text": "An application of Grammatical Inference ( GI ) in the field of Music Processing is presented , were Regular Grammars are used for modeling musical style .The interest in modeling musical style resides in the use of these models in applications , such as Automatic Composition and Automatic Musical St ... \" .", "label": "", "metadata": {}, "score": "54.700447"}
{"text": "These probability values collectively form the N - gram language model .Some aspects of the invention described below can be applied to building a standard statistical N - gram model .As is also well known in the art , a language model can also comprise a context - free grammar .", "label": "", "metadata": {}, "score": "54.72165"}
{"text": "It can be seen that there is a rather different distribution data type between the two sets : particularly for F0 , F2 , F4 and FX .The HTK Broadcast News system runs in a number of stages .This is followed by generating a lattice for each segment using the adapted triphone models with a bigram LM , expanding these lattices using a word 4-gram interpolated with a category trigram LM , and performing iterative lattice rescoring and MLLR adaptation with a set of quinphone HMMs .", "label": "", "metadata": {}, "score": "54.818977"}
{"text": "Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .", "label": "", "metadata": {}, "score": "54.849464"}
{"text": "We give semantic characterizations of the expressive power of navigational XPath ( a.k.a .Core XPath ) in terms of first order logic . XPath can be used to specify sets of nodes and sets of paths in an XML document tree .", "label": "", "metadata": {}, "score": "55.000328"}
{"text": "Advances and Trends in Speech Recognition and Coding ( volume 147 of NATO - ASI Series F : Computer and Casacuberta and Vidal Translation with Finite - State Transducers Systems Sciences ) .Springer - Verlag , Berlin and Heidelberg .Tools . \" ...", "label": "", "metadata": {}, "score": "55.047424"}
{"text": "It should simply be noted that the language model created from the identified text according to the present technique works better with information retrieval techniques that identify more relevant text of the task - independent corpus .The text identified in the task - independent corpus based on the query is indicated at step 210 .", "label": "", "metadata": {}, "score": "55.065857"}
{"text": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .", "label": "", "metadata": {}, "score": "55.09816"}
{"text": "With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .", "label": "", "metadata": {}, "score": "55.359413"}
{"text": "Similarity between the query and segments of the task - independent corpus can be computed using cosine similarity measure .These are generally well - known techniques in the field of information retrieval .Alternatively , the query can include Boolean logic ( \" and \" , \" or \" , etc . ) as may be desired to combine word phrases .", "label": "", "metadata": {}, "score": "55.3664"}
{"text": "However , each of the context - free grammars of the second plurality is more appropriate for the selected application .Referring back to the proper name example provided above , the second plurality of context - free grammars could include a CFG : .", "label": "", "metadata": {}, "score": "55.444603"}
{"text": "Therefore , probabilities of the terminals from the N - gram language model need to be appropriately normalized in the same probability space as the terminals present in the corresponding context - free grammar .i in W. The likelihood of W under the segmentation T is therefore .", "label": "", "metadata": {}, "score": "55.50573"}
{"text": "It can be seen that the new training corpus reduces the WER by a 0.7 % absolute and a further 0.5 % absolute reduction was obtained by using a merged interpolated language model .The merged interpolated models gave most improvement on the spontaneous speech portions of the data .", "label": "", "metadata": {}, "score": "55.540745"}
{"text": "Inside each context - free grammar , the standard probabilistic context - free grammar can be used .However , without real data pertaining to the specific task or application , an estimate for each of the terminal probabilities can not be easily determined .", "label": "", "metadata": {}, "score": "55.55458"}
{"text": "We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .", "label": "", "metadata": {}, "score": "55.563915"}
{"text": "For instance , by way of example , one set of context - free grammars of a larger plurality of context - free grammars for a software application or task concerning scheduling meetings or sending electronic mail may comprise : . etc . .", "label": "", "metadata": {}, "score": "55.6356"}
{"text": "Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .", "label": "", "metadata": {}, "score": "55.70448"}
{"text": "A N - gram model is built having the non - terminal tokens .A second plurality of context - free grammars is obtained for at least some of the same non - terminals representing the same semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "55.773975"}
{"text": "We use a maximum likelihood technique to select the best data warp factor via a parabolic search .It is important when comparing the warped data likelihoods to properly take into account the effect of the transformation .We have done this implicitly by performing variance normalisation on the data .", "label": "", "metadata": {}, "score": "55.94204"}
{"text": "System details can be found in [ 16 ] .The data segmentation [ 4 ] aims to generate acoustically homogeneous speech segments and discard non - speech portions such as pure music .It uses a set of Gaussian mixture models to classify the data as to type ( wideband speech , narrow - band speech , pure music , speech and music ) , and then any pure music is discarded .", "label": "", "metadata": {}, "score": "56.229637"}
{"text": "The uHMT requires no training of any kind .While extremely simple , we show using a series of image estimation /denoising experiments that these two new models retain nearly all of the key structure modeled by the full HMT .Finally , we propose a fast shift - invariant HMT estimation algorithm that outperforms other wavelet - based estimators in the current literature , both in mean - square error and visual metrics . ...", "label": "", "metadata": {}, "score": "56.295803"}
{"text": "When this set of outputs is linguistically structured , we talk about linguistic pattern recognition .Despite of this distinction , chapter 8 addresses the main issues of pattern recognition such as quantization , document classification and Markov processes .Chapter 9 is dedicated to automatic speech recognition ( ASR ) and handwriting recognition ( OCR ) .", "label": "", "metadata": {}, "score": "56.39698"}
{"text": "claim 7 and having instructions further comprising : . building a second N - gram language model from the word phases from the plurality of context - free grammars ; and .combining the first - mentioned N - gram language model and the second N - gram language model to form a third N - gram language model .", "label": "", "metadata": {}, "score": "56.56327"}
{"text": "Finding the posterior probability distribution for a grid - structured Markov network with loops is computationally expensive and avariety ofapproximations have been proposed [ 13 , 12,20].Strong empir ... . by Jonathan S. Yedidia , William T. Freeman , Yair Weiss - IEEE Transactions on Information Theory , 2005 . \" ...", "label": "", "metadata": {}, "score": "56.665123"}
{"text": "We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph .We explain how to obtain regionbased free energy approximations that improve the Bethe approximation , and corresponding generalized belief propagation ( GBP ) algorithms .", "label": "", "metadata": {}, "score": "56.8086"}
{"text": "This technique yields a greater degree of synchrony between the input and output samples .The proposedapproachleads to a reduction in the number of samples necessary to learn the transducer and a reduction in the size of the model so obtained . .", "label": "", "metadata": {}, "score": "56.867786"}
{"text": "We set - up a Markov network of patches of the image and the underlying scene .A factorization approximation allows us to easily learn the parameters of the Markov network from synthetic examples of image / scene pairs , and to e ciently propagate image information .", "label": "", "metadata": {}, "score": "56.919044"}
{"text": "Theoretical foundations of data exchange have recently been investigated for relational data .In this paper , we star ... \" .Data exchange is the problem of finding an instance of a target schema , given an instance of a source schema and a specification of the relationship between the source and the target .", "label": "", "metadata": {}, "score": "56.99694"}
{"text": "In many ways , network monitoring and inference problems bear a strong resemblance to other \" inverse problems \" in which key aspects of a system are not directly observable .Familiar signal processing problems such as tomographic image reconstruction , system identification , and array processing all have interesting interpretations in the networking context .", "label": "", "metadata": {}, "score": "57.05504"}
{"text": "These MR methods have found application and permeated the literature of a widely scattered set of disciplines , and one of our principal objectives is to present a single , coheren ... \" .This paper reviews a significant component of the rich field of statistical multiresolution ( MR ) modeling and processing .", "label": "", "metadata": {}, "score": "57.07554"}
{"text": "For example , a bi - gram ( or 2-gram ) language model considers the previous word as having an influence on the next word .where w is a word of interest : .Also , the probability of a word sequence is determined based on the multiplication of the probability of each word given its history .", "label": "", "metadata": {}, "score": "57.082672"}
{"text": "This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .", "label": "", "metadata": {}, "score": "57.180088"}
{"text": "Furthermore , the syntax is tied to semantics in a desired manner .Then the phrase structure and the Context Free Grammar are introduced and discussed .The second part is about the grammatical approaches which emphasizes the notion of grammatical primitives : dependency , linking , and valency and in the third part the semantics - driven theories of syntax are discussed , in particular the issues of frame semantics and knowledge representation .", "label": "", "metadata": {}, "score": "57.195457"}
{"text": "In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .", "label": "", "metadata": {}, "score": "57.20794"}
{"text": "EVALUATION In my opinion , this is a very interesting book which provides an introduction to linguistics through a mathematical point of view .This book is maybe more intended for mathematicians than linguists .One of its goals is to bridge the gap between a mathematical formalization and the philosophical or pragmatic expectations of linguistic theories .", "label": "", "metadata": {}, "score": "57.40593"}
{"text": "Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .", "label": "", "metadata": {}, "score": "57.42374"}
{"text": "The belief propagation ( BP ) algorithm is an efficient way to solve these problems t ... \" .Important inference problems in statistical physics , computer vision , error - correcting coding theory , and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs .", "label": "", "metadata": {}, "score": "57.579582"}
{"text": "In this chapter , the basic Zipf 's law is introduced .Chapter 5 : ' ' Syntax ' 'The theory of syntax addresses three strongly interconnected ranges of facts : the combinatorical possibilities of words , the internal structure of sentences , the fit between what is being said and what is seen in the world .", "label": "", "metadata": {}, "score": "57.59951"}
{"text": "Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .", "label": "", "metadata": {}, "score": "57.71427"}
{"text": "The reader is therefore quickly lost among the several and often briefly introduced mathematical notations .A lot of time could be spent to understand some important distinction between notations among chapters .This lack does not help the rich purpose of this book .", "label": "", "metadata": {}, "score": "57.716805"}
{"text": "Such information can then be applied in order to validate / debug the program and/or to specialize the program obtaining important improvements . ... am analysis was presented in [ 4 ] , which also presented some preliminary data from its implementation in the context of the Ciao system .", "label": "", "metadata": {}, "score": "58.090965"}
{"text": "wm ) is represented as follows : .The .P .w . wm . )i . m .P .w .i .H .i . )N - gram model is obtained by applying an N - gram algorithm to a corpus ( a collection of phrases , sentences , sentence fragments , paragraphs , etc ) of textual training data .", "label": "", "metadata": {}, "score": "58.13411"}
{"text": "An N - gram model is then built at step 168 using an N - gram algorithm , the N - gram model having the non - terminal tokens embedded therein .At step 170 , a second plurality of context - free grammars is obtained suitable for the selected application .", "label": "", "metadata": {}, "score": "58.138287"}
{"text": "This poorer performance was also reflected in the number of frames assigned to multiple speaker segments : 1.6 % for BNeval97 but 4.3 % for BNeval98 .This paper has described the development and performance of the 1998 HTK broadcast news transcription system .", "label": "", "metadata": {}, "score": "58.216972"}
{"text": "[ Before ROVER combination an alignment pass was run to get exact word timings .Due to the effects of automatic segmentation this process reduces the WER by about 0.1 % absolute .] Table 6 : Word error rates for each stage of the 1998 HTK broadcast news evaluation system ( also P4 FV contrast ) .", "label": "", "metadata": {}, "score": "58.225327"}
{"text": "In this manner , the size of the task - independent corpus is reduced prior to parsing so that method 180 may execute more quickly .Appropriate context - free grammars can then be determined and included in the plurality of context - free grammars at step 182 .", "label": "", "metadata": {}, "score": "58.243095"}
{"text": "The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .", "label": "", "metadata": {}, "score": "58.24566"}
{"text": "Its basic syntax is as follows : ( . \" ...In this paper we report on JmmSolve , a solver implementation of CCM machines , a framework for Java memory models .It illustrates the generative nature of the memory model framework in that it is able to generate all valid read - write linkings .", "label": "", "metadata": {}, "score": "58.451035"}
{"text": "The system takes the 1997 system and includes the additional acoustic training data in BNtrain98 ; cluster - based normalisation and VTLN ; the revised language modelling data and build procedure and full variance adaptation with SAT training .The word N - grams were trained by interpolating ( and merging ) component LMs trained on the acoustic transcriptions , the broadcast news texts and the newspaper texts .", "label": "", "metadata": {}, "score": "58.604614"}
{"text": "Initially we used bandwidth independent , gender independent triphones to evaluate the technique and under these conditions it gave a 1 % absolute reduction in WER .However , when bandwidth dependent , gender dependent models with variance normalisation and MLLR adaptation were used , there was no WER advantage and hence soft clustering was not used in the 1998 evaluation system .", "label": "", "metadata": {}, "score": "58.62211"}
{"text": "Image Processing , 1999 . \" ...Wavelet - domain hidden Markov models have proven to be useful tools for statistical signal and image processing .The hidden Markov tree ( HMT ) model captures the key features of the joint probability density of the wavelet coefficients of real - world data .", "label": "", "metadata": {}, "score": "58.881943"}
{"text": "Graph database models can be characterized as those where data structures for the schema and instances are modeled as graphs or generalizations of them , and data manipulation is expressed by graph - oriented operations and type constructors .These models flourished in the eighties and early nineties i ... \" .", "label": "", "metadata": {}, "score": "58.915054"}
{"text": "Similarly , using quinphone models with MLLR a gain of 0.5 % in WER was achieved with increased training data .We also did some experiments that used automatic segmentation of the extended training data to try and ensure that the segments used in training were acoustically homogeneous but this provided no additional improvements .", "label": "", "metadata": {}, "score": "58.91957"}
{"text": "FIG .5 illustrates a first method 160 for creating or building a language model .The method 160 includes a step 162 for obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "58.99533"}
{"text": "If desired , the identified text can be extracted , copied or otherwise stored separate from the task - independent corpus as an aid in isolating relevant text and providing easier processing .FIG .9 is a block diagram illustrating another aspect of the present invention .", "label": "", "metadata": {}, "score": "59.02724"}
{"text": "Modern search engines answer keyword - based queries extremely efficiently .The impressive speed is due to clever inverted index structures , caching , a domain - independent knowledge of strings , and thousands of machines .Several research efforts have attempted to generalize keyword search to keytree and keygraph searching , because trees and graphs have many applications in next - generation database systems .", "label": "", "metadata": {}, "score": "59.07649"}
{"text": "Task - dependent corpora , on the other hand , are typically not available .These corpora must be laboriously compiled , and even then , may not be very complete .A broad aspect of the invention includes a method for creating a task or domain dependent unified language model for a selected application from a task - independent corpus .", "label": "", "metadata": {}, "score": "59.14541"}
{"text": "Combining DTDs and dependencies makes some XML data exchange settings inconsistent .We investigate the consistency problem and determine its exact complexity .We then move to query answering , and prove a dichotomy theorem that classifies data exchange settings into those over which query answering is tractable , and those over which it is coNP - complete , depending on classes of regular expressions used in DTDs .", "label": "", "metadata": {}, "score": "59.187687"}
{"text": "As previously mentioned , this book requires a sufficient general mathematical maturity .However , I doubt reading this book needs no prior knowledge of linguistics or languages is needed .This book is not a ' ' stand - alone ' ' one .", "label": "", "metadata": {}, "score": "59.25318"}
{"text": "One of these areas is machine translation , in which the approaches that are based on building models automatically from training examples are becoming more and more attrac ... \" .Finite - state transducers are models that are being used in different areas of pattern recognition and computational linguistics .", "label": "", "metadata": {}, "score": "59.29547"}
{"text": "Each block is partitioned into two disjoint subsets , T 1 and T 2 .( b ) T k 2 is used to estimate the parameters of E k .The Viterbi re - estimation technique described in section 3 is employed to this end .", "label": "", "metadata": {}, "score": "59.374138"}
{"text": "Then a large part of the chapter is dedicated to computational phonology with Finite State Transducers and Biautomatons .Chapter 4 : ' ' Morphology ' ' Morphology is maybe the most vexing part of linguistics from a mathematical point of view because of radical typological differences , flexible boundaries and near truth .", "label": "", "metadata": {}, "score": "59.452374"}
{"text": "Table 4 : % WER on BNeval97 for different trigram LMs with VTLN unadapted triphone HMMs with either pooled data or ( merged ) interpolated LMs .The effect of using three different LMs on BNeval97 with VTLN data and 1998 unadapted triphone HMMs is shown in Table 4 .", "label": "", "metadata": {}, "score": "59.66791"}
{"text": "Stochastic models are important in pattern recognition if good estimations for their parameters are provided .The problem of parameter estimation has been well studied for stochastic grammars , but this is not the case of EC parameters .This work is aimed at providing solutions to adequately solve it .", "label": "", "metadata": {}, "score": "59.985386"}
{"text": "A second aspect is a method for creating a task dependent unified language model for a selected application from a task - independent corpus .The task dependent unified language model includes embedded context - free grammar non - terminal tokens in a N - gram model .", "label": "", "metadata": {}, "score": "60.04039"}
{"text": "When the third N - gram language model 252 is built having non - terminals , the word phrases or synthetic data at block 242 typically will also include the non - terminals as well .When the context - free grammars are used to generate synthetic data , probabilities for the word phrases formed with the non - terminals and the terminals of the non - terminals can be chosen as desired ; for instance , each can be assigned equal probability .", "label": "", "metadata": {}, "score": "60.06465"}
{"text": "For sets of nodes , XPath is equally expressive as first ord ... \" .We give semantic characterizations of the expressive power of navigational XPath ( a.k.a .Core XPath ) in terms of first order logic . XPath can be used to specify sets of nodes and sets of paths in an XML document tree .", "label": "", "metadata": {}, "score": "60.10396"}
{"text": "etc .This type of grammar does not require an in - depth knowledge of formal sentence structure or linguistics , but rather , a knowledge of what words , phrases , sentences or sentence fragments are used in a particular application or task .", "label": "", "metadata": {}, "score": "60.14952"}
{"text": "The A / D converter 104 converts the analog speech signal into a sequence of digital signals , which is provided to the feature extraction module 106 .In one embodiment , the feature extraction module 106 is a conventional array processor that performs spectral analysis on the digital signals and computes a magnitude value for each frequency band of a frequency spectrum .", "label": "", "metadata": {}, "score": "60.16322"}
{"text": "State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .", "label": "", "metadata": {}, "score": "60.21084"}
{"text": "The word phrases can include some or all of the various combinations and permutations defined by the associated context - free grammars where the non - terminal tokens include multiple words .At step 206 , at least one query is formulated for an information retrieval system using at least one of the generated word phrases .", "label": "", "metadata": {}, "score": "60.23219"}
{"text": "The chapter ends with the more general problem of inductive learning from the perspective of complexity , introducing notions such as the minimum description length .Chapter 8 and 9 : ' ' Linguistic pattern recognition ' ' and ' ' Speech and handwriting ' ' Chapter 8 summarizes the area of linguistic pattern recognition , and two well - known applications are described in chapter 9 : speech and handwriting .", "label": "", "metadata": {}, "score": "60.37839"}
{"text": "For intersection , we show that the complexity only carries over for DTDs . ... ition ) and XML Schema Definitions ( XSDs ) [ 8 , 9 ] are the most widely spread .The former aresTable 1 .Factor Abbr . a ?", "label": "", "metadata": {}, "score": "60.46304"}
{"text": "This FV transform was used with , for the wideband data , HMMs estimated with a single iteration of speaker adaptive training ( SAT ) [ 14 ] to update the mean parameters .The effect of these changes is shown in Table 5 .", "label": "", "metadata": {}, "score": "60.546318"}
{"text": "FIG .2 and accessible by the processing unit 51 or another suitable processor .In addition , the lexicon storage module 110 , the acoustic model 112 , and the language model 16 are also preferably stored in any of the memory devices shown in .", "label": "", "metadata": {}, "score": "60.752182"}
{"text": "Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .", "label": "", "metadata": {}, "score": "60.854073"}
{"text": "Results from previous studies have been improved . ... ch the objets or processes of interest can be adequately represented as strings of symbols .But there are many other areas in which GI can lead to interesting applications .One of these areas is Music Processing .", "label": "", "metadata": {}, "score": "60.906006"}
{"text": "The category - trigram used 1000 automatically derived word classes and was trained using LMtrain98 .Category bigrams and trigrams were added only if the leave - one training set likelihood improved and the final category model contained 0.85 million bigrams and 9.4 million trigrams .", "label": "", "metadata": {}, "score": "60.925735"}
{"text": "Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .", "label": "", "metadata": {}, "score": "60.977978"}
{"text": "The system of . claim 2 having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the set of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .", "label": "", "metadata": {}, "score": "61.087715"}
{"text": "Using a clustering procedure and a set of smoothing rules the final segments to be processed by the decoder are generated .For recognition , each frame of input speech is represented by a 39 dimensional feature vector that consists of 13 ( including ) MF - PLP cepstral parameters and their first and second differentials .", "label": "", "metadata": {}, "score": "61.1035"}
{"text": "The reduced bandwidth models are used for data classified as narrow band .The system uses the LIMSI 1993 WSJ pronunciation dictionary augmented by pronunciations from a TTS system and hand generated corrections for a 65k word vocabulary .The 1997 system used N - gram language models trained on 132 million words of broadcast news texts , the LDC - distributed 1995 newswire texts , and the transcriptions from BNtrain97 ( LMtrain97 ) .", "label": "", "metadata": {}, "score": "61.435368"}
{"text": "Essentially , the N - gram language model 142 ( also known as a hybrid N - gram model ) of the unified language model 140 includes an augmented vocabulary having words and at least some of the non - terminals .", "label": "", "metadata": {}, "score": "61.458355"}
{"text": "The author can therefore express another definition of natural classes as sets of segments that can be decomposed by fewer features than their individual members .Then supra - segmental theory and auto - segments are described using a multitiered representation .", "label": "", "metadata": {}, "score": "61.544018"}
{"text": "Table 7 : OOV rate and perplexities of the 1998 evaluation LMs .Perplexities shown for trigram ( tg ) , 4-gram ( fg ) and word 4-gram interpolated with category trigram ( fgintcat ) .The out - of - vocabulary ( OOV ) rate and perplexity of these language models on BNeval97 and the two halves of the BNeval98 set is shown in Table 7 .", "label": "", "metadata": {}, "score": "61.588615"}
{"text": "For sets of nodes , XPath is equally expressive as first order logic in two variables .For paths , XPath can be defined using four simple connectives , which together yield the class of first order definable relations which are safe for bisimulation .", "label": "", "metadata": {}, "score": "61.62753"}
{"text": "We study two families of error - correcting codes defined in terms of very sparse matrices .\" MN \" ( MacKay -- Neal ) codes are recently invented , and \" Gallager codes \" were first investigated in 1962 , but appear to have been largely forgotten , in spite of their excellent properties .", "label": "", "metadata": {}, "score": "61.727005"}
{"text": "We study two families of error - correcting codes defined in terms of very sparse matrices .\" MN \" ( MacKay -- Neal ) codes are recently invented , and \" Gallager codes \" were first investigated in 1962 , but appear to have been largely forgotten , in spite of their excellent properties .", "label": "", "metadata": {}, "score": "61.727005"}
{"text": "In this paper we investigate how to extend a generic constraint solver that provides not only tell constraints ( by adding the constraint to the store ) but also ask tests ( by checking whether the constraint is entailed by the store ) , with general ask constraints .", "label": "", "metadata": {}, "score": "61.775856"}
{"text": "FIG .1 is a block diagram of a language processing system .FIG .2 is a block diagram of an exemplary computing environment .FIG .3 is a block diagram of an exemplary speech recognition system .FIG .", "label": "", "metadata": {}, "score": "61.777092"}
{"text": "We found an overall improvement in WER with cluster - based variance normalisation of 0.3 % absolute and a further 0.6 % absolute by applying VTLN in both training and testing without adaptation .However with mean and variance MLLR adaptation the separate beneficial effect of variance normalisation and VTLN is much reduced .", "label": "", "metadata": {}, "score": "61.897774"}
{"text": "Signi ca nt amount of work has been devoted recently to develop learning techniques that can be used to generate partial ( shallow ) analysis of natural language sentences rather than a full parse .We conclude that directly learning to perform these tasks as shallow parsers do is advantageous over full parsers both in terms of performance and robustness to new and lower quality texts . \" ...", "label": "", "metadata": {}, "score": "61.960846"}
{"text": "The system then uses quinphone models ( VTLN / SAT trained ) and MLLR with an additional FV transform to process the data ( P4 ) .This stage is repeated twice more while increasing the number of MLLR transforms ( P5/P6 ) .", "label": "", "metadata": {}, "score": "62.03975"}
{"text": "d we are after are useful in understanding and further designing the language .They are also useful because they allow us to transfer already known results and techniques to the world of XPath .All characterizations we give with respect to other languages are constructive and given in terms of translations .", "label": "", "metadata": {}, "score": "62.066536"}
{"text": "Furthermore , in line with the triphone figures , the overall gain for 1998 trained MLLR adapted quinphone models was 0.4 % absolute due to VTLN .For the 1998 system , the additional transcriptions from the 1998 acoustic training were available .", "label": "", "metadata": {}, "score": "62.074223"}
{"text": "Step 162 represents obtaining context - free grammars having non - terminal tokens to represent the semantic or syntactic concepts in the task - independent corpus , the non - terminal tokens having terminals present in the task - independent corpus .", "label": "", "metadata": {}, "score": "62.11605"}
{"text": "We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .", "label": "", "metadata": {}, "score": "62.240944"}
{"text": "At step 208 , the task - independent corpus is queried based on the query formulated .The particular information retrieval technique used to generate and execute the query against the task - independent corpus is not critical to this feature of the present invention .", "label": "", "metadata": {}, "score": "62.460785"}
{"text": "Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .", "label": "", "metadata": {}, "score": "62.510517"}
{"text": "This transcription is used for both gender selection as well as VTLN warp selection for each segment cluster .Gender dependent VTLN models are then used ( P2 ) to provide a revised transcription which is used to estimate global mean and variance MLLR transforms for each cluster .", "label": "", "metadata": {}, "score": "62.52886"}
{"text": "In particular , at step 182 , a plurality of context - free grammars is obtained .For example , a task - dependent application may require modeling the day of the week as a semantic concept in the N - gram model .", "label": "", "metadata": {}, "score": "62.546482"}
{"text": ".. graph traversals .Lore uses four kinds of indices to accelerate ( regular ) path expression searching .For each edge label l in the graph , a value index ( Vindex ) is used to index all the nodes that have i .. \" ...", "label": "", "metadata": {}, "score": "62.82793"}
{"text": "Step 190 is similar to Step 170 and includes obtaining a second set of context - free grammars suited for the selected application .Used during language processing such as speech recognition , the N - gram model having the non - terminal tokens and the plurality of context - free grammars associated with the task - dependent application is stored on a computer readable medium accessible by the speech recognition module 100 .", "label": "", "metadata": {}, "score": "62.857765"}
{"text": "We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .", "label": "", "metadata": {}, "score": "63.03408"}
{"text": "storing the N - gram model and a second plurality of context - free grammars comprising at least some of the same non - terminals representing the same semantic or syntactic concepts in the memo , each of the context - free grammars of the second plurality being more appropriate for use in the selected application .", "label": "", "metadata": {}, "score": "63.14753"}
{"text": "The generation approach is a definitional method which aims to enumerate all the elements of a given set .Because techniques used in generation are not always useful , an equally important concern is to solve the membership problem .Therefore a variety of grammars are introduced with emphasis on generating grammar , string - rewriting rules and context sensitive - grammars .", "label": "", "metadata": {}, "score": "63.342133"}
{"text": "As discussed above , the task - independent corpus is a compilation of sentences , phrases , etc . that is not directed at any one particular application , but rather , generally shows , through a wide variety of examples , how words are ordered in a language .", "label": "", "metadata": {}, "score": "63.481983"}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a language model , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . generating word phases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phases ; . querying a task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and .", "label": "", "metadata": {}, "score": "63.502357"}
{"text": "To go beyond segments , the example of final devoicing in Russian leads to the definition of the natural classes : sets of segments that frequently appear together in rules .As the phonemic alphabet is the result of an algorithm , the set of natural classes is also forced by the phonological patterning of languages .", "label": "", "metadata": {}, "score": "63.607765"}
{"text": ".. an appropriate instance of a solver type class , and thus the solver is known to provide at least the constraints included in the type class interface .In order for a constraint solver to be extended by CHRs , the solver needs to provide ask versions of the constraints that it supports .", "label": "", "metadata": {}, "score": "63.663345"}
{"text": "There is a 6 % gain from employing the category trigram and 4-gram over the trigram alone , and a 7 % gain moving from adapted triphones to adapted quinphones : most of which ( 5 % ) was due to the full variance adaptation .", "label": "", "metadata": {}, "score": "63.72567"}
{"text": "Preferably , the N - gram model having the non - terminal tokens and the second plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts are stored on a computer readable medium accessible by the speech recognizer 100 .", "label": "", "metadata": {}, "score": "63.76752"}
{"text": ".. by Brendan J. Frey , David J. C. MacKay - In Neural Information Processing Systems , 1997 . \" ...Until recently , artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles .", "label": "", "metadata": {}, "score": "63.9899"}
{"text": "The N - gram language model 142 will be used to first predict words and non - terminals .Then , if a non - terminal has been predicted , the plurality of context - free grammars 144 is used to predict terminals as a function of the non - terminals .", "label": "", "metadata": {}, "score": "64.01224"}
{"text": "Prior to a detailed discussion of the present invention , an overview of an operating environment may be helpful .FIG .2 and the related discussion provide a brief , general description of a suitable computing environment in which the invention can be implemented .", "label": "", "metadata": {}, "score": "64.11052"}
{"text": "While the semantic basis of CHRs allows them to extend arbitrary underlying constraint solvers , in practice , all current implementations only extend Herbrand equation so ... \" .Constraint Handling Rules ( CHRs ) are a high - level committed choice programming language commonly used to write constraint solvers .", "label": "", "metadata": {}, "score": "64.122444"}
{"text": "Then , the lexeme is defined as a maximal set of paradigmatically related word .The paradigmatic forms can be generated by inflection or derivation .The language of words can be infinite , some variants of the generative apparatus described in the chapter 2 are needed to bear the task of enumerating all words given a finite set of morphemes .", "label": "", "metadata": {}, "score": "64.12454"}
{"text": "In the embodiment illustrated , the language model 16 can be an N - gram language model or a unified language model comprising a context - free grammar specifying semantic or syntactic concepts with non - terminals and a hybrid N - gram model having non - terminals embedded therein .", "label": "", "metadata": {}, "score": "64.1886"}
{"text": "In the embodiment illustrated in .FIG .9 , block 240 represents the context - free grammars obtained ( for example , authored by the developer ) for the selected task or application .The context - free grammars are used to generate synthetic data or word phrases 242 in a manner similar to step 204 of methods 200 and 220 .", "label": "", "metadata": {}, "score": "64.236916"}
{"text": "We discuss the construction of MR models on trees and show how questions that arise in this context make contact with wavelets , state space modeling of time series , system and parameter identification , and hidden . ... th Markov random fields ( MRFs ) and with the large class 1398 PROCEEDINGS OF THE IEEE , VOL .", "label": "", "metadata": {}, "score": "64.24717"}
{"text": "FIGS .5 - 8 are flow charts for different aspects of the present invention .FIG .9 is a block diagram of another aspect of the present invention .DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS .FIG .1 generally illustrates a language processing system 10 that receives a language input 12 and processes the language input 12 to provide a language output 14 .", "label": "", "metadata": {}, "score": "64.25517"}
{"text": "However , it has recently been discovered that the tw ... \" .Until recently , artificial intelligence researchers have frowned upon the application of probability propagation in Bayesian belief networks that have cycles .The probability propagation algorithm is only exact in networks that are cycle - free .", "label": "", "metadata": {}, "score": "64.34567"}
{"text": "We prove that these codes are \" very good , \" in that sequences of codes exist which , when optimally decoded , achieve information rates up to the Shannon limit .This result holds not only for the binary - symmetric channel but also for any channel with symmetric stationary ergodic noise .", "label": "", "metadata": {}, "score": "64.692"}
{"text": "While the generation process of the set of phonemes is clearly described and highlights the necessity of a mathematical formalization , the rest of the chapter is more obscure to me .A previous reading of the work of Goldsmith on the autosegmental theory is really helpful to understand the motivations and goals of such theory .", "label": "", "metadata": {}, "score": "65.107185"}
{"text": "To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .", "label": "", "metadata": {}, "score": "65.14075"}
{"text": "In one embodiment , the acoustic model 112 includes a senone tree associated with each Markov state in a Hidden Markov Model .The Hidden Markov models represent , in one illustrative embodiment , phonemes .The tree search engine 114 also accesses the lexicon stored in module 110 .", "label": "", "metadata": {}, "score": "65.15679"}
{"text": "2 .Furthermore , the tree search engine 114 is implemented in processing unit 51 ( which can include one or more processors ) or can be performed by a dedicated speech recognition processor employed by the personal computer 50 .In the embodiment illustrated , during speech recognition , speech is provided as an input into the system 100 in the form of an audible voice signal by the user to the microphone 92 .", "label": "", "metadata": {}, "score": "65.17941"}
{"text": "Finite - state transducers are veryadequate for use in constrained tasks in which training samples of pairs of sentences are available .A technique for inferring finite - state transducers is proposed in this article .This technique is based on formalrelations between finite - state transducers and rational grammars .", "label": "", "metadata": {}, "score": "65.184074"}
{"text": "C .. by Mark Coates , Alfred Hero , Robert Nowak , Bin Yu - IEEE Signal Processing Magazine , 2002 . \" ...Today 's Internet is a massive , distributed network which continues to explode in size as ecommerce and related activities grow .", "label": "", "metadata": {}, "score": "65.258865"}
{"text": "Similar results hold in the nondeterministic case .Finally , ( 4 ) the output languages of deterministic n - pebble tree transducers form a hierarchy with respect to the number n of pebbles .Now , consider the ( sequential ) composition of tree translations .", "label": "", "metadata": {}, "score": "65.32234"}
{"text": "The radical typological differences can be simply illustrated : no amount of expert knowledge of Modern English is sufficient to describe the case system of Modern Russian .In this part , the author presents a prosodic hierarchy describing syllables , moras , feet , cola and a typology for words and stress .", "label": "", "metadata": {}, "score": "65.33963"}
{"text": "Referring to .FIG .4 , a unified language model 140 includes a combination of an N - gram language model 142 and a plurality of context - free grammars 144 .Specifically , the N - gram language model 142 includes at least some of the same non - terminals of the plurality of context - free grammars 144 embedded therein such that in addition to predicting words , the N - gram language model 142 also can predict non - terminals . where ( h 1 , h 2 , . . .", "label": "", "metadata": {}, "score": "65.36352"}
{"text": "The system of . claim 8 and having instructions further comprising : . storing the N - gram model having the non - terminal tokens and the plurality of context - free grammars having non - terminal tokens representing task dependent semantic or syntactic concepts on the memory .", "label": "", "metadata": {}, "score": "65.42371"}
{"text": "Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .", "label": "", "metadata": {}, "score": "65.733116"}
{"text": "This aims to describe how to encode a speech signal for a relevant representation of information .Then , the use of phonemes as hidden units is described and discussed with a central question : how the discrete and ' ' meaningless ' ' units used in phonology can be recognized from continuous acoustic signals and what are the related issues ?", "label": "", "metadata": {}, "score": "65.74222"}
{"text": "The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .", "label": "", "metadata": {}, "score": "65.7818"}
{"text": "The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .A word language model is built from the corpus .Probabilities of terminals of at least some of the context - free grammars are normalized and assigned as a function of corresponding probabilities obtained for the same terminals from the word language model .", "label": "", "metadata": {}, "score": "65.87581"}
{"text": "In addition , those skilled in the art will appreciate that the invention can be practiced with other computer system configurations , including hand - held devices , multiprocessor systems , microprocessor - based or programmable consumer electronics , network PCs , minicomputers , mainframe computers , and the like .", "label": "", "metadata": {}, "score": "65.9376"}
{"text": "Abstract .Functional logic programming languages combine the most important declarative programming paradigms , and attempts to combine these paradigms have a long history .The declarative multi - paradigm language Curry is influenced by recent advances in the foundations and implementation of functional logic languages .", "label": "", "metadata": {}, "score": "65.963455"}
{"text": "Although described herein where the speech recognition system 100 uses HMM modeling and senone trees , it should be understood that this is but one illustrative embodiment .As appreciated by those skilled in the art , the speech recognition system 100 can take many forms and all that is required is that it uses the language model 16 and provides as an output the text spoken by the user .", "label": "", "metadata": {}, "score": "65.99653"}
{"text": "A system , comprising : . memory ; . a processor adapted to access the memory ; . instructions stored on the memory and executable by the processor which , when implemented , execute a method to build a unified language model for a selected application , the method comprising : . accessing a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application ; . building a word language model from a corpus ; and .", "label": "", "metadata": {}, "score": "66.26271"}
{"text": "Although many encouraging results have been obtained in recent years , the development o .. \" ...The evolution of programming languages is the stepwise introduction of abstractions hiding the underlying computer hardware and the details of program execution .Assembly languages introduce mnemonic instructions and symbolic ... \" .", "label": "", "metadata": {}, "score": "66.469315"}
{"text": "In a network environment , program modules depicted relative to the personal computer 50 , or portions thereof , can be stored in the remote memory storage devices .As appreciated by those skilled in the art , the network connections shown are exemplary and other means of establishing a communications link between the computers can be used .", "label": "", "metadata": {}, "score": "66.500824"}
{"text": "Q .h . )P . word .w .u . t .i .l .u . t .i .l .The normalization is performed the same as in Equation ( 7 ) .Multiple segmentations may be available for W due to the ambiguity of natural language .", "label": "", "metadata": {}, "score": "66.71454"}
{"text": "_ Foundations of Statistical Natural Language Processing_.Cambridge , MA : MIT Press .Jelinek , F. 1997 ._ Statistical Methods for Speech Recognition_.Cambridge , MA : MIT Press .ABOUT THE REVIEWER A. Allauzen is associate professor in the informatic department of the University Paris 11 ( Orsay ) , and at the LIMSI - CNRS in the Spoken Language Processing group .", "label": "", "metadata": {}, "score": "66.74916"}
{"text": "In model - theoretic semantics , the meaning representations are formulas , and they can be seen as just technical devices used for disambiguating expressions with multiple meanings .The standard formal theory meets the essential criteria .This theory starts with the description of Montague grammar .", "label": "", "metadata": {}, "score": "66.902435"}
{"text": "The problem for N - gram models is that a lot of data is needed and the model may not be specific enough for the desired application .Since a word - based N - gram model is limited to n - word dependency , it can not include longer - distance constraints in the language whereas CFG can .", "label": "", "metadata": {}, "score": "67.10134"}
{"text": "Finally , we decided to use a different ( though similarly sized ) portion of newspaper texts covering 1995 to February 1998 ( about 70MW in total ) .All these sources excluded data from the designated test epochs .This corpus was denoted LMtrain98 .", "label": "", "metadata": {}, "score": "67.29372"}
{"text": "The 0-pebble tree transducer can be simulated by the macro tree transducer , which , by the first result , implies that ( 2 ) \u03c4 can be realized by an ( n+1)-fold composition of macro tree transducers .Conversely , every macro tree transducer can be simulated by a composition of 0-pebble tree transducers .", "label": "", "metadata": {}, "score": "67.37715"}
{"text": "The four main results on deterministic transducers are : First , ( 1 ) the translation \u03c4 of an n - pebble tree transducer can be realized by a composition of n + 1 0-pebble tree transducers .Next , the pebble t ... \" .", "label": "", "metadata": {}, "score": "67.39976"}
{"text": "The language processing system 10 processes the spoken language and provides as an output , recognized words typically in the form of a textual output .During processing , the speech recognition system or module 10 can access a language model 16 in order to determine which words have been spoken .", "label": "", "metadata": {}, "score": "67.423676"}
{"text": "Factor Abbr .( a1 + \u00b7 \u00b7 \u00b7 + an ... . by Diego Calvanese , Maurizio Lenzerini , Moshe Y. Vardi - Revised Papers of the 8th International Workshop on Database Programming Languages ( DBPL 2001 ) , volume 2397 of LNCS , 2001 . \" ... Abstract .", "label": "", "metadata": {}, "score": "67.746445"}
{"text": "Finite - state transducers are used to model the translation process .Furthermore , these models are automatically learned from ... \" .Language Understanding in limited domains is here approached as a problem of language translation in which the target language is a formal language rather than a natural one .", "label": "", "metadata": {}, "score": "67.75614"}
{"text": "This grammar is finally converted into a finite - state transducer .The proposed methods are assessed through a series of machine translation experiments within the framework of the EuTrans project . \" ...The problem of Error - Correcting Parsing ( ECP ) using an insertion - deletion -substitution error model and a Finite State Machine is examined .", "label": "", "metadata": {}, "score": "67.79254"}
{"text": "As technology advances and speech and handwriting recognition is provided in more applications , the application developer must be provided with an efficient method in which an appropriate language model can be created for the selected application .SUMMARY OF THE INVENTION .", "label": "", "metadata": {}, "score": "67.909515"}
{"text": "First , they add the inverse operator , which allows for expressing navigations in the database that traverse the edges both backward and forward .Second , they allow for using conjunctions of atoms , where each atom specifies that a regular path query with inverse holds between two terms , where each term is either a variable or a constant .", "label": "", "metadata": {}, "score": "67.99387"}
{"text": "While the ask - test must be implemented by the solver writer , the compiler can extend this to provide ask behaviour for complex combinations of constraints , including constraints from multiple solvers .We illustrate the use of this approach within the HAL system . ... aints which include existential variables and involve more than one solver , to the primitive ask - tests supported by the solvers .", "label": "", "metadata": {}, "score": "68.053825"}
{"text": "The n - pebble tree transducer was recently proposed as a model for XML query languages .The four main results on deterministic transducers are : First , ( 1 ) the translation \u03c4 of an n - pebble tree transducer can be realized by a composition of n + 1 0-pebble tree transducers .", "label": "", "metadata": {}, "score": "68.10394"}
{"text": "These probability distributions are later used in executing a Viterbi or similar type of processing technique .Upon receiving the code words from the feature extraction module 106 , the tree search engine 114 accesses information stored in the acoustic model 112 .", "label": "", "metadata": {}, "score": "68.224525"}
{"text": "The problem of Error - Correcting Parsing ( ECP ) using an insertion - deletion -substitution error model and a Finite State Machine is examined .The Viterbi algorithm can be straightforwardly extended to perform ECP , though the resulting computational complexity can become prohibitive for many applications . \" ...", "label": "", "metadata": {}, "score": "68.35809"}
{"text": "The baseline acoustic corpus available in 1997 used recorded audio from various US broadcast news shows ( television and radio ) .This amounted to a total of 72 hours of usable data ( BNtrain97 ) .This data was annotated to ensure that each segment was acoustically homogeneous ( same speaker , background noise condition and channel ) .", "label": "", "metadata": {}, "score": "68.361435"}
{"text": "Both forms of language processing can benefit from a language model .One common technique of classifying is to use a formal grammar .The formal grammar defines the sequence of words that the application will allow .One particular type of grammar is known as a \" context - free grammar \" ( CFG ) , which allows a language to be specified based on language structure or semantically .", "label": "", "metadata": {}, "score": "68.8048"}
{"text": "Wavelet - domain hidden Markov models have proven to be useful tools for statistical signal and image processing .The hidden Markov tree ( HMT ) model captures the key features of the joint probability density of the wavelet coefficients of real - world data .", "label": "", "metadata": {}, "score": "68.81807"}
{"text": "Information and Knowledge Processing PUBLISHER : Springer YEAR :2007 .Alexandre Allauzen , Department of Informatic , University Paris 11 ( Orsay ) , and LIMSI - CNRS .SUMMARY As mentioned in the forewords , this book requires a sufficient general mathematical maturity , but no prior knowledge of linguistic or languages .", "label": "", "metadata": {}, "score": "68.81957"}
{"text": "Experiments with no adaptation ( or cluster - based normalisation ) showed that the word error rate ( WER ) was reduced by up to 0.9 % absolute .However when MLLR adaptation and VTLN were applied ( see below ) the WER gain was reduced to 0.4 % absolute .", "label": "", "metadata": {}, "score": "69.22112"}
{"text": "We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .", "label": "", "metadata": {}, "score": "69.225136"}
{"text": "Abstract .In this paper we investigate how to extend a generic constraint solver that provides not only tell constraints ( by adding the constraint to the store ) but also ask tests ( by checking whether the constraint is entailed by the store ) , with general ask constraints .", "label": "", "metadata": {}, "score": "69.38838"}
{"text": "The present invention relates to language modeling .More particularly , the present invention relates to creating a language model for a language processing system .Accurate speech recognition requires more than just an acoustic model to select the correct word spoken by the user .", "label": "", "metadata": {}, "score": "69.468506"}
{"text": "As appreciated by those skilled in the art , the language model 16 can be used in other language processing systems besides the speech recognition system discussed above .For instance , language models of the type described above can be used in handwriting recognition , Optical Character Recognition ( OCR ) , spell - checkers , language translation , input of Chinese or Japanese characters using standard PC keyboard , or input of English words using a telephone keypad .", "label": "", "metadata": {}, "score": "69.57701"}
{"text": "The 1998 evaluation data , BNeval98 , consisted of two 1.5 hour data sets : the first drawn from a similar epoch as the 1997 data and the second drawn from June 1998 .The evaluation results are presented for each of the NIST ' ' focus ' ' conditions which are shown in Table 1 .", "label": "", "metadata": {}, "score": "69.72044"}
{"text": "Once the clitics is defined and can be stripped away , six criteria are used to define the ' ' wordhood ' ' .Given a set of morphemes , these criteria delineate a formal language of words .The categorization of morphemes gives rise to a similar categorization of the processes that operate on them .", "label": "", "metadata": {}, "score": "69.79529"}
{"text": "A fourth aspect is a method for creating a language model for a selected application from a task - independent corpus .The method includes obtaining a plurality of context - free grammars comprising non - terminal tokens representing semantic or syntactic concepts of the selected application .", "label": "", "metadata": {}, "score": "70.288765"}
{"text": "In this paper we apply the ideas of refactoring to Prolog programs .We start by presenting a catalogue of refactorings .Then we discuss ViPReSS , our refactoring browser , and our experience with applying ViPReSS to a big Prolog legacy system . by An\u00e1lisis Y Verificaci\u00f3n De , Programas Modulares , Jes\u00fas Correas Fern\u00e1ndez , Licenciado En Inform\u00e1tica , Facultad De Inform\u00e1tica , Candidato Jes\u00fas Correas , Licenciado En Inform\u00e1tica , Germ\u00e1n Puebla , Licenciado En Inform\u00e1tica , El Secretario , A Beatriz Agradecimientos . \" ... presentada en la Facultad de Inform\u00e1tica de la Universidad Polit\u00e9cnica de Madrid para la obtenci\u00f3n del t\u00edtulo de Doctor en Inform\u00e1tica ... \" . ...", "label": "", "metadata": {}, "score": "70.37251"}
{"text": "Commonly , a plurality of context - free grammars comprising non - terminal tokens representing various semantic or syntactic concepts are used .For instance , other semantic or syntactic concepts include geographical places , regions , titles , dates , times , currency amounts , and percentage amounts to name a few .", "label": "", "metadata": {}, "score": "70.66519"}
{"text": "Unlike the existing results on program transformation refactoring can require user input to take certain decisions .In ... \" .Refactoring is a technique to restructure code in a disciplined way originating from the OO - community .It aims to improve software readability , maintainability and extensibility .", "label": "", "metadata": {}, "score": "70.71275"}
{"text": "While the system produces good results it is computationally expensive : a companion paper [ 12 ] discusses a version of the system that runs in less than ten times real - time on commodity hardware .This work is in part supported by an EPSRC grant on ' ' Multimedia Document Retrieval ' ' reference GR / L49611 and by a grant from DARPA .", "label": "", "metadata": {}, "score": "70.96791"}
{"text": "claim 6 and having instructions further comprising : . storing the identified text of the task independent corpus separate from the task independent corpus .The system of .claim 9 wherein building the second N - gram language model includes using only the identified text .", "label": "", "metadata": {}, "score": "71.0253"}
{"text": "P .w . )T .S .W . )P .W .T . )Although the present invention has been described with reference to preferred embodiments , workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention .", "label": "", "metadata": {}, "score": "71.03184"}
{"text": "i 2 . . .u t .A CFG state constrains the possible words that can follow the history .u t .i 1 u t .i 2 . . .u t .The likelihood of observing u t .", "label": "", "metadata": {}, "score": "71.58078"}
{"text": "The final hypothesis combination uses word - level confidence scores based on an N - best homogeneity measure .These are used with the NIST ROVER program [ 1 ] to produce the final output .We first compared the effect of using the additional training data in the BNtrain98 set .", "label": "", "metadata": {}, "score": "71.6595"}
{"text": "Finally , the effect of the automatic segmentation procedure on the BNeval98 set was investigated .On BNeval97 we had found that automatic segmentation had produced very similar overall accuracy to manually defined segments .However on BNeval98 the automatic segmenter faired more poorly .", "label": "", "metadata": {}, "score": "72.49554"}
{"text": "Step 224 then includes replacing each of the identified word occurrences with corresponding non - terminal tokens for selected non - terminals ( i.e. excluding the non - terminals which may have been introduced to prevent mistakes during parsing ) .Step 212 would then include building an N - gram model having non - terminal tokens .", "label": "", "metadata": {}, "score": "72.758896"}
{"text": "The need for training data is dramatically reduced by performing a two - level learning process based on lexical / phrase categorization .Successful experiments are presented on a task consisting in the \" understanding \" of Spanish natural - language sentences describing dates and times , where the target formal language is the one used in the popular Unix command \" at \" . \" ...", "label": "", "metadata": {}, "score": "72.90665"}
{"text": "claim 15 wherein the word language model comprises an N - gram language model and wherein the corpus comprises a task independent corpus .The system of .claim 16 and having instructions further comprising : . generating word phrases from the plurality of context - free grammars ; . formulating an information retrieval query from at least one of the word phrases ; . querying the task independent corpus based on the query formulated ; . identifying associated text in the task independent corpus based on the query ; and . wherein building a N - gram language model includes using the identified text .", "label": "", "metadata": {}, "score": "72.93624"}
{"text": "A good strategy for readers would be to manually create their own glossary of notations when reading this chapter if they do not want to return to it periodically to find the meaning of notation found in the further chapters .Chapter 3 : ' ' Phonology ' ' First this chapter formalizes the well - known definition of phonemes as minimum concatenative units of sound .", "label": "", "metadata": {}, "score": "73.17284"}
{"text": "The system of .claim 6 wherein the language model is an N - gram language model .The system of .claim 7 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and . wherein building the N - gram language model comprises building a N - gram model having the non - terminal tokens .", "label": "", "metadata": {}, "score": "73.3757"}
{"text": "A factor graph is a bipartite graph that expresses how a \" global \" function of many variables factors into a product of \" local \" functions .Factor graphs subsume many other graphical models including Bayesian networks , Markov random fields , and Tanner graphs .", "label": "", "metadata": {}, "score": "73.57469"}
{"text": "A factor graph is a bipartite graph that expresses how a \" global \" function of many variables factors into a product of \" local \" functions .Factor graphs subsume many other graphical models including Bayesian networks , Markov random fields , and Tanner graphs .", "label": "", "metadata": {}, "score": "73.57469"}
{"text": "The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .", "label": "", "metadata": {}, "score": "73.71076"}
{"text": "Block 248 illustrates application of an N - gram algorithm to obtain the second N - gram language model 250 .A third N - gram language model 252 is formed by combining the first N - gram language model 246 and the second N - gram language model 250 .", "label": "", "metadata": {}, "score": "73.85669"}
{"text": "Thus , the feature extraction module 106 provides , at its output the feature vectors ( or code words ) for each spoken utterance .The feature extraction module 106 provides the feature vectors ( or code words ) at a rate of one feature vector or ( code word ) approximately every 10 milliseconds .", "label": "", "metadata": {}, "score": "73.89199"}
{"text": "Error - Correcting ( EC ) techniques allow for coping with divergences in pattern strings with regard to their \" standard \" form as represented by the language L accepted by a regular or context - free grammar .There are two main types of EC parsers : minimum - distance and stochastic .", "label": "", "metadata": {}, "score": "74.22417"}
{"text": "Error - Correcting ( EC ) techniques allow for coping with divergences in pattern strings with regard to their \" standard \" form as represented by the language L accepted by a regular or context - free grammar .There are two main types of EC parsers : minimum - distance and stochastic .", "label": "", "metadata": {}, "score": "74.22417"}
{"text": "Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .", "label": "", "metadata": {}, "score": "74.25573"}
{"text": "At step 186 , each of the identified word occurrences for non - terminals representing concepts which are of interest to the target application is replaced with the corresponding non - terminal token as defined by the corresponding context - free grammar .", "label": "", "metadata": {}, "score": "74.38377"}
{"text": "In the case of an N - gram language model or a hybrid N - gram language model , step 212 will commonly require use of an N - gram algorithm .FIG .8 illustrates a method 220 similar to the method 200 of .", "label": "", "metadata": {}, "score": "74.4502"}
{"text": "Top - down language processing begins with the largest unit of language to be recognized , such as a sentence , and processes it by classifying it into smaller units , such as phrases , which in turn , are classified into yet smaller units , such as words .", "label": "", "metadata": {}, "score": "74.633125"}
{"text": "The triphone HMMs were estimated using BNtrain97 and contained 6684 decision - tree clustered states [ 17 ] , each with 12 Gaussians per state while the quinphone models used 8180 states and 16 Gaussians per state .The HMMs were initially trained on all the wide - band analysed training data .", "label": "", "metadata": {}, "score": "74.76996"}
{"text": "FIG .3 .It should be noted that the entire system 100 , or part of speech recognition system 100 , can be implemented in the environment illustrated in .FIG .2 .For example , microphone 92 can preferably be provided as an input device to the computer 50 , through an appropriate interface , and through the A / D converter 104 .", "label": "", "metadata": {}, "score": "75.54443"}
{"text": "In this paper , we greatly simplify the HMT model by exploiting the inherent self - similarity of real - world images .This simplified model specifies the HMT parameters with just nine metaparameters ( independent of the size of the image and the number of wavelet scales ) .", "label": "", "metadata": {}, "score": "75.886"}
{"text": "Thus , other mathematical tools have to be proposed to deal with for example size normalization ( rescaling ) , stroke width or font weights .These aspects are addressed by mathematical morphology and more precisely by the operations of reflection , translation , dilatation , erosion , opening , closing and boundary .", "label": "", "metadata": {}, "score": "76.02842"}
{"text": "CROSS - REFERENCE TO RELATED APPLICATION .The present application is a continuation of and claims priority of U.S. patent application Ser .No .09/585,298 , filed Jun. 1 , 2000 , the content of which is hereby incorporated by reference in its entirety .", "label": "", "metadata": {}, "score": "76.26514"}
{"text": "This option is illustrated in dashed lines for block 264 and arrows 266 and 268 .Of course , if this option is chosen the identified text 210 would not be provided directly to the N - gram algorithm 248 , but rather to block 264 .", "label": "", "metadata": {}, "score": "76.462456"}
{"text": "A weighted language is defined as a mapping from strings to real values .The overall question is how a weighted language successfully approximates another , and this introduces the mathematical theory of density .A special case of interest is when the density is zero .", "label": "", "metadata": {}, "score": "77.560326"}
{"text": "Other input devices ( not shown ) can include a joystick , game pad , satellite dish , scanner , or the like .A monitor 77 or other type of display device is also connected to the system bus 53 via an interface , such as a video adapter 78 .", "label": "", "metadata": {}, "score": "77.68697"}
{"text": "Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .", "label": "", "metadata": {}, "score": "78.782455"}
{"text": "claim 1 wherein the plurality of context - free grammars include at least one context - free grammar having a non - terminal token for a phrase that can be mistaken for one of the desired task dependent semantic or syntactic concepts .", "label": "", "metadata": {}, "score": "79.468315"}
{"text": "Tile common practice for approaching this task is by tedious manual definition of possible pat - tern structures , often in the h)rm of re ... \" .Tile common practice for approaching this task is by tedious manual definition of possible pat - tern structures , often in the h)rm of regular expres - sions or finite automata .", "label": "", "metadata": {}, "score": "82.13386"}
{"text": "Finite - state transducers are models that are being used in different areas of pattern recognition and computational linguistics .One of these areas is machine translation , in which the approaches that are based on building models automatically from training examples are becoming more and more attrac ... \" .", "label": "", "metadata": {}, "score": "82.41846"}
{"text": "The feature extraction module 106 divides the digital signal received from the A / D converter 104 into frames that include a plurality of digital samples .Each frame is approximately 10 milliseconds in duration .The frames are then encoded by the feature extraction module 106 into a feature vector reflecting the spectral characteristics for a plurality of frequency bands .", "label": "", "metadata": {}, "score": "83.6756"}
{"text": "claim 9 and having instructions further comprising : . parsing the identified text of the task independent corpus with the plurality of context - free grammars to identify word occurrences for each of the semantic or syntactic concepts ; . replacing each of the identified word occurrences with corresponding non - terminal tokens ; and .", "label": "", "metadata": {}, "score": "83.67752"}
{"text": "When a pair of nuclear - powered Russian submarines was reported patrolling off the eastern seaboard of the U.S. last summer , Pentagon officials expressed wariness over the Kremlin 's motivations .\" While the official did not divulge the methods used by the Navy to track submarines , the Times added that such . by Mark Johnson , Stuart Geman , Stephen Canon , Zhiyi Chi , Stefan Riezler , 1999 . \" ...", "label": "", "metadata": {}, "score": "83.79516"}
{"text": "Generally , program modules include routine programs , objects , components , data structures , etc . that perform particular tasks or implement particular abstract data types .Tasks performed by the programs and modules are described below and with the aid of block diagrams and flow charts .", "label": "", "metadata": {}, "score": "84.75534"}
{"text": "Today 's Internet is a massive , distributed network which continues to explode in size as ecommerce and related activities grow .The heterogeneous and largely unregulated structure of the Internet renders tasks such as dynamic routing , optimized service provision , service level verification , and detection of anomalous / malicious behavior increasingly challenging tasks .", "label": "", "metadata": {}, "score": "84.924225"}
{"text": "This paper surveys the foundations of functional logic programming that are relevant for Curry , the main features of Curry , and extensions and applications of Curry and functional logic programming . ... th partially instantiated structures , so that some programming techniques for functional logic programming [ 21,71,72 ] can not be applied in Mercury .", "label": "", "metadata": {}, "score": "85.39403"}
{"text": "W .T . )i . m .P .t .i .t .i .t .i .i . m .P .u . t .i ._ .t .i . )", "label": "", "metadata": {}, "score": "85.49678"}
{"text": "In a distributed computing environment , program modules can be located in both local and remote memory storage devices .With reference to .The system bus 53 can be any of several types of bus structures including a memory bus or memory controller , a peripheral bus , and a local bus using any of a variety of bus architectures .", "label": "", "metadata": {}, "score": "85.66314"}
{"text": "u t .i 1 u t .i 2 . . .u t .i k ] from the context - free grammar non - terminal t i .In the case when t i itself is a word ( \u016b t .", "label": "", "metadata": {}, "score": "86.0452"}
{"text": "A number of program modules can be stored on the hard disk , magnetic disk 59 , optical disk 61 , ROM 54 or RAM 55 , including an operating system 65 , one or more application programs 66 , other program modules 67 , and program data 68 .", "label": "", "metadata": {}, "score": "86.20828"}
{"text": "A basic input / output system 56 ( BIOS ) , containing the basic routine that helps to transfer information between elements within the personal computer 50 , such as during start - up , is stored in ROM 54 .The hard disk drive 57 , magnetic disk drive 58 , and optical disk drive 60 are connected to the system bus 53 by a hard disk drive interface 62 , magnetic disk drive interface 63 , and an optical drive interface 64 , respectively .", "label": "", "metadata": {}, "score": "88.71489"}
{"text": "P .u . t .i .l .h . )P . word .u . t .i .l .u . t .i .l .u . t .i .l .w .", "label": "", "metadata": {}, "score": "89.75816"}
{"text": "P .u . t .i ._ .t .i . )[ .l .u ._ .t .i .P .u . t .l .u . t .i . u . t .", "label": "", "metadata": {}, "score": "90.450874"}
{"text": "..Message - passing algorithms are a practical and powerful way to solve such problems .The centrality of such problems and the utility of messagepassing algorithms for solving them is an explanati ... . \" ...When a pair of nuclear - powered Russian submarines was reported patrolling off the eastern seaboard of the U.S. last summer , Pentagon officials expressed wariness over the Kremlin 's motivations .", "label": "", "metadata": {}, "score": "92.18193"}
{"text": "The personal computer 50 can operate in a networked environment using logic connections to one or more remote computers , such as a remote computer 79 .FIG .2 .The logic connections depicted in .FIG .2 include a local area network ( LAN ) 81 and a wide area network ( WAN ) 82 .", "label": "", "metadata": {}, "score": "92.55983"}
{"text": "Assembly languages introduce mnemonic instructions and symbolic . ... uctures that are only partially known , so that some programming techniques for functional logic programming [ 6 , 26 ] can not be applied with Mercury .However , both languages are based on a strict operational semantics that does not support optimal evaluation as in functional programming .", "label": "", "metadata": {}, "score": "92.830666"}
{"text": "For instance , the task - independent corpus includes various instances of how proper names are used .For example , the task - independent corpus could have sentences like : \" Bill Clinton was present at the meeting \" and \" John Smith went to lunch at the conference \" .", "label": "", "metadata": {}, "score": "95.296684"}
{"text": "However , the task - independent corpus might contain references to a person called \" Joe Friday \" .In this manner , during parsing of the task - independent corpus , instances of days of the week will be identified separate from instances where \" Friday \" is the last name of an individual .", "label": "", "metadata": {}, "score": "96.9579"}
{"text": "This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .", "label": "", "metadata": {}, "score": "102.20181"}
{"text": "When used in a LAN networking environment , the personal computer 50 is connected to the local area network 81 through a network interface or adapter 83 .When used in a WAN networking environment , the personal computer 50 typically includes a modem 84 or other means for establishing communications over the wide area network 82 , such as the Internet .", "label": "", "metadata": {}, "score": "103.765915"}
