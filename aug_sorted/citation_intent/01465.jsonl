{"text": "Our results provide the first known empir - ical evidence that lexical semantics are in - deed useful for SMT , despite claims to the contrary . \" ...In this paper , we propose a novel string - todependency algorithm for statistical machine translation .", "label": "", "metadata": {}, "score": "34.969063"}
{"text": "We demonstrate the potential of the method by applying it to a small number of examples and showing that the paraphrases are more similar than the non - paraphrases . \" ...We present a method for lexical simplification .Simplification rules are learned from a comparable corpus , and the rules are applied in a context - aware fashion to input sentences .", "label": "", "metadata": {}, "score": "35.627743"}
{"text": "An automatic speech recognizer may function normally with respect to the inventive meaning token dictionary .In particular , the invention readily may be incorporated into existing automatic speech recognition systems in which the dictionaries may be manipulated or replaced .The output from the speech recognizer , however , is in the form of meaning tokens rather than simple transcriptions of ordinary spoken words .", "label": "", "metadata": {}, "score": "35.873405"}
{"text": "Therefore , a piece of compressed data DOUT composed of a piece of positional information and an identification code can be directly output from the code table .Accordingly , a concept of a dictionary search disappears by predicting an occurrence of a character or a character string .", "label": "", "metadata": {}, "score": "36.659"}
{"text": "In addition , the output from the speech recognition system may be substantially free of ambiguities , allowing the downstream application program to use deterministic interpretation algorithms ( which are simpler and more efficient than non - deterministic algorithms ) .Because the meaning tokens may have longer pronunciations than ordinary words , more data may be used to distinguish words in the speech recognition dictionary .", "label": "", "metadata": {}, "score": "37.129868"}
{"text": "Our future work might focus on the application of the proposed dictionary learning method to other classification problems .The proposed algorithm could also be extended to multiple object tracking or the tracking of specific class ( e.g. , humans or their parts ) given certain application environments .", "label": "", "metadata": {}, "score": "37.149353"}
{"text": "In order to achieve higher efficiency in datasets of very low entropy , a method that encodes long sequences of measurement differences as a single symbol would be needed .Although the proposed method could be adapted for that purpose , doing so might jeopardize its versatility .", "label": "", "metadata": {}, "score": "37.53936"}
{"text": "Next , a reproducing ( or decoding ) processing for the compressed data according to the third embodiment of the present invention is described with reference to FIG .14(C ) .In case where the fixed code exists in the input buffer 22A ( YES ) , the procedure proceeds to a step P3 , and the fixed dictionary data of the fixed dictionary buffer 22D are searched .", "label": "", "metadata": {}, "score": "37.703094"}
{"text": "In the LZSS coding method , functions in data search are improved .FIRST PREVIOUSLY PROPOSED ART : .FIGS .2(A ) , 2(B ) and 2(C ) explanatorily shows a coding processing in which the same piece of character string is undesirably duplicated in a dictionary buffer .", "label": "", "metadata": {}, "score": "37.71091"}
{"text": "Therefore , the description of the decoding operation is omitted .By using the cyclic search of the dictionary data , a longest agreement of the dictionary data with a successive data portion of the input data strings to be compressed can be found out , and a piece of agreement information of the longest agreement can be coded .", "label": "", "metadata": {}, "score": "37.85147"}
{"text": "Automatic paraphrasing is an important component in many natural language processing tasks .In this paper we present a new parallel corpus with paraphrase annotations .We adopt a definition of paraphrase based on word - alignments and show that it yields high inter - annotator agreement .", "label": "", "metadata": {}, "score": "38.01384"}
{"text": "Automatic paraphrasing is an important component in many natural language processing tasks .In this paper we present a new parallel corpus with paraphrase annotations .We adopt a definition of paraphrase based on word - alignments and show that it yields high inter - annotator agreement .", "label": "", "metadata": {}, "score": "38.01384"}
{"text": "Our expe ... \" .In this paper , we propose a novel string - todependency algorithm for statistical machine translation .With this new framework , we employ a target dependency language model during decoding to exploit long distance word relations , which are unavailable with a traditional n - gram language model .", "label": "", "metadata": {}, "score": "38.67568"}
{"text": "Specifically , it encodes portions of the input using the result of a dynamic programming algorithm , where the subproblems are finding the approximately optimal encoding ( the one with minimal post - range - encoding size ) of the substring of length L starting at the byte being compressed .", "label": "", "metadata": {}, "score": "38.68535"}
{"text": "We investigate unsupervised techniques for acquiring monolingual sentence - level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web - based news sources .Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy ... \" .", "label": "", "metadata": {}, "score": "39.27913"}
{"text": "The coding operation was described hereinbefore with reference to FIG .9 .In case where the compressed data is reproduced ( or decoded ) , the compressed data is compressed by using the ring - shaped dictionary buffer 22E according to the same decoding flow chart as the decoding flow chart shown in FIG .", "label": "", "metadata": {}, "score": "39.40649"}
{"text": "The binary tree approach follows the hash chain approach , except that it logically uses a binary tree instead of a linked list for chaining .Since the string to search for and the string to insert are the same , it is possible to perform both dictionary search and insertion ( which requires to rotate the tree ) in a single tree traversal .", "label": "", "metadata": {}, "score": "39.474533"}
{"text": "Multiple meaning tokens may be associated with each of one or more polysemous vocabulary words contained in the speech recognition dictionary .The automatic speech recognition system may include a language analyzer that is configured to extract meaning from the sequence of meaning tokens provided by the speech recognizer based upon a set of task - specific semantic rules .", "label": "", "metadata": {}, "score": "39.72177"}
{"text": "Here , it is assumed that the term \" plugin \" is registered in the dictionary 42 as a keyword for which related information is to be displayed .It is possible to use a method such as common morphological analysis , or a method of repeatedly applying the longest match method from the first word of the text .", "label": "", "metadata": {}, "score": "40.125336"}
{"text": "That is , the dictionary data or the dictionary data strings are expanded to the input memory region , and a piece of input data or an input data string agreeing with a piece of dictionary data or a dictionary data string can be found out .", "label": "", "metadata": {}, "score": "40.152184"}
{"text": "1 may be implemented .FIG .3 is a block diagram of the speech recognizer of .FIG .1 .FIG .4 is a block diagram of a meaning token dictionary associating a pronunciation with a meaning token having dictionary spelling signifying a single meaning .", "label": "", "metadata": {}, "score": "40.18238"}
{"text": "Summary of the Proposed Algorithm .The proposed algorithm is summarized in Algorithm 2 based on the descriptions above .In Algorithm 2 , the proposed dictionary learning algorithm is the most computational , while the online training and classification process does not take much running time , since the efficient linear SVM is applied .", "label": "", "metadata": {}, "score": "40.25013"}
{"text": "Thus , the parsing process implemented by the user application often must test numerous hypotheses using complicated , non - deterministic methods to determine the correct meaning from a natural language input .SUMMARY .The invention features systems and methods of automatic speech recognition in which the vocabulary words of a speech recognition dictionary correspond to meaning tokens each of which signifies a single meaning .", "label": "", "metadata": {}, "score": "40.512012"}
{"text": "Next , a third data processing method representing a third data compressing method and a third data reproducing ( or decoding ) method according to the third embodiment of the present invention is described with reference to FIG .14(B ) .", "label": "", "metadata": {}, "score": "40.656807"}
{"text": "We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser . \" ...Previous work has used monolingual parallel corpora to extract and generate paraphrases .", "label": "", "metadata": {}, "score": "41.16945"}
{"text": "Our Proposal .Inspired by the discussions above , this paper considers the dictionary learning problem for online visual tracking , as well as a visual tracking algorithm , incremental discriminative structured dictionary learning ( IDSDL)-VT , including incremental discriminative structured dictionary learning and multiple linear classifiers .", "label": "", "metadata": {}, "score": "41.23941"}
{"text": "With this method , it is not necessary for each client terminal to store a large volume dictionary .In addition , by a webpage provider or other business actively registering beforehand keywords for which information is to be provided to a user , it is possible to use the keywords as an additional function such as a sales promotion in an electronic shopping mall .", "label": "", "metadata": {}, "score": "41.26191"}
{"text": "There is a dictionary learning process in the algorithm proposed by Liu et al .[ 12 ] , but it is a generative approach .Moreover , the algorithm proposed by Zhong et al .[ 20 ] is a hybrid one , and its discriminative ability is not based on the binary classifier , but the reconstruction error , which is used to generatively create weights for confidence modeling .", "label": "", "metadata": {}, "score": "41.333145"}
{"text": "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments . ... ol .", "label": "", "metadata": {}, "score": "41.448025"}
{"text": "Accordingly , it is not required to include the code table in the compressed data , and the data compressing efficiency can be enhanced .Also , the input data can be dynamically coded for each of bytes or characters of the input data .", "label": "", "metadata": {}, "score": "41.54407"}
{"text": "However , when a size of the dictionary buffer 2B is increased , a searching time is considerably increased in general .Also , when a size of the dictionary buffer 2B is increased , it is required to lengthen a data length of a piece of positional information for a piece of data to be coded .", "label": "", "metadata": {}, "score": "41.59116"}
{"text": "The procedure is similar to ( Chiang , 2007 ) except that we maintain tree structures on the target side , instead of strings .We use a statistical CFG parser to parse the English side of the training data , and extract dependency trees with Magerman 's rules ( 1995 ) .", "label": "", "metadata": {}, "score": "41.701584"}
{"text": "In addition , multiple meaning tokens may be associated with each of one or more polysemous vocabulary words that are contained in the meaning token dictionary .In this way , language analyzer 14 and any downstream application program may easily distinguish each vocabulary word output by speech recognizer 12 , resulting in higher overall recognition accuracy .", "label": "", "metadata": {}, "score": "41.87329"}
{"text": "when measurements are compressed using dictionaries generated by other sets .Each column corresponds to the values obtained when the set on the top row is used to generate the dictionary .Despite the longer symbol lengths obtained when Set 3 is used , the proposed method performed better that LEC in all the cases presented in Table 4 .", "label": "", "metadata": {}, "score": "42.37812"}
{"text": "Using alignment techniques from phrasebased statistical machine translation , we show how paraphrases ... \" .Previous work has used monolingual parallel corpora to extract and generate paraphrases .We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .", "label": "", "metadata": {}, "score": "42.423946"}
{"text": "Accordingly , an agreement data string obtained by comparing the dictionary data string formed by using the input data to be compressed with pieces of input data to be compressed input one after another can be coded .A coding method is concretely described .", "label": "", "metadata": {}, "score": "42.583454"}
{"text": "This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse - trees of source and target language sentences .", "label": "", "metadata": {}, "score": "42.63449"}
{"text": "Proposed Scheme .Our objective is to devise a simple compression method which approaches the performance of optimal entropy coding while relying on a fixed dictionary .This can be observed in Figure 1 , which shows the probability distribution of the differences between consecutive measurements for each of the datasets in Table 1 .", "label": "", "metadata": {}, "score": "42.810543"}
{"text": "The output from the speech recognizer 12 , however , is in the form of meaning tokens rather than simple transcriptions of ordinary spoken words .The meaning token dictionary may simplify the work that must be performed by language analyzer 14 and any downstream application program that uses an automatic speech recognition system for spoken language input .", "label": "", "metadata": {}, "score": "43.17472"}
{"text": "Overall , it can be concluded that the proposed tracker achieves better performance than the other state - of - the - art algorithms .Dictionary Learning Results and Time .In this paper , a dictionary learning algorithm called incremental discriminative structured dictionary learning ( IDSDL ) is proposed to learn from positive and negative samples , joined to construct a structured dictionary with a newly established randomly permuted unit matrix for sparse representation .", "label": "", "metadata": {}, "score": "43.208126"}
{"text": "Finally , the proposed scheme , although extremely simple , outperforms LEC [ 2 ] for all the considered datasets and ALFC [ 4 ] in the vast majority of cases .The most promising direction we envision for the future is to improve our ability of understanding the reference measurement datasets so that we can compensate for deficiencies in the dataset during dictionary generation .", "label": "", "metadata": {}, "score": "43.30273"}
{"text": "The speech recognizer is configured to convert spoken input into a sequence of meaning tokens contained in the speech recognition dictionary and corresponding to a sequence of vocabulary words most likely to have been spoken by a user .a speech recognition dictionary comprising a plurality of meaning tokens , wherein a single meaning token has a same meaning associated with plural different spoken words that have different pronunciations but similar spoken meanings ; and .", "label": "", "metadata": {}, "score": "43.315823"}
{"text": "In these works , constraints have been considered on the class labels , learning process and sparse representation coefficients , and discriminability has been enforced in sparse codes during the dictionary learning process to improve classification accuracy .However , they fail to consider the dictionary design from a discriminative perspective .", "label": "", "metadata": {}, "score": "43.33648"}
{"text": "The speech recognition dictionary comprises a plurality of meaning tokens each associated with one or more pronunciations of one or more vocabulary words and signifying a single meaning .The invention also features a computer program for automatically recognizing speech .The speech recognition dictionary resides on the computer - readable medium and comprises a plurality of meaning tokens each associated with one or more pronunciations of one or more vocabulary words and signifying a single meaning .", "label": "", "metadata": {}, "score": "43.47434"}
{"text": "Generation of proposed dictionary , which is composed of positive and negative template patches learned by IDSDL and trivial templates .The corresponding patches are cropped separately from the positive and negative samples around the target based on different sampling radii .", "label": "", "metadata": {}, "score": "43.595055"}
{"text": "Generation of proposed dictionary , which is composed of positive and negative template patches learned by IDSDL and trivial templates .The corresponding patches are cropped separately from the positive and negative samples around the target based on different sampling radii .", "label": "", "metadata": {}, "score": "43.595055"}
{"text": "This paper considers the problem of automatic assessment of local coherence .We present a novel entity - based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text .We view coherence assessment as a ranking learning problem and show that the ... \" .", "label": "", "metadata": {}, "score": "44.165752"}
{"text": "The method of .claim 12 , wherein multiple meaning tokens are associated with each of one or more polysemous vocabulary words contained in the speech recognition dictionary .The method of .claim 12 , further comprising extracting meaning from the sequence of meaning tokens based upon a set of task - specific semantic rules .", "label": "", "metadata": {}, "score": "44.200394"}
{"text": "Validation of the Method Using Alternative Dictionaries .With the purpose of demonstrating the generality of the proposed approach , we evaluated the performance of our method when datasets other than Set 1 were used to generate the dictionary .Table 3 shows the results of this evaluation .", "label": "", "metadata": {}, "score": "44.38259"}
{"text": "However , in many cases , the user may not be able to formulate queries without observing the raw sensor data beforehand .As a consequence , a number of lossless compression methods for WSNs have been proposed .S - LZW [ 7 ] is an adaptation of the celebrated Lempel - Ziv - Welch ( LZW ) algorithm [ 14 ] for resource - constrained wireless sensor nodes .", "label": "", "metadata": {}, "score": "44.475433"}
{"text": "For this reason , we use a sophisticated SVM - based initial tagger .A second initial tagger based on the hidden Markov model ( HMM ) is used for comparison .The second component is the space of transfo ... . by", "label": "", "metadata": {}, "score": "44.641544"}
{"text": "This can be verified in Table 3 , where we can see that the average symbol lengths obtained using the dictionaries generated by sets that caused the highest percentages of symbols not present ( especially Set 8) are not significantly higher than those obtained using other sets .", "label": "", "metadata": {}, "score": "44.717697"}
{"text": "Our method is completely language independent and unsupervised , which provides a promising avenue for constructing accurate multi - lingual or cross - lingual information retrieval systems that are exible and adaptive .We nd that although the segmentation accuracy of self - supervised segmentation is not as high as some other segmentation methods , it is enough to give comparable ( in some cases even better ) retrieval performance .", "label": "", "metadata": {}, "score": "44.875885"}
{"text": "Furthermore , it does not require any alignment or correspondence among the complex and simpl ... \" .We present a method for lexical simplification .Simplification rules are learned from a comparable corpus , and the rules are applied in a context - aware fashion to input sentences .", "label": "", "metadata": {}, "score": "44.94866"}
{"text": "Also , an allowable storage capacity of a magnetic disk apparatus or the like can be substantially increased , and the apparatus considerably contributes to shorten the transmission time required for the data transmission .Next , a data processing apparatus and a data processing method according to sixth to eighth embodiments of the present invention are described with reference to FIGS .", "label": "", "metadata": {}, "score": "44.949036"}
{"text": "5 , the keyword markup module 41 accesses and searches a dictionary of the web server 13a or 13b .The keyword markup module 41 searches the dictionary based on keywords that are included in extracted text , and when there is a matching keyword , acquires an event handler .", "label": "", "metadata": {}, "score": "45.093952"}
{"text": "Compared with the dictionary learning papers referred to above , we do not solely rely on the optimization , but focus on the dictionary design with separate learning to improve the discriminative ability of the sparse coefficients for classification .Moreover , the dictionary is built on a patch level , and thus , a spatial multi - dictionary learning structure is established .", "label": "", "metadata": {}, "score": "45.251698"}
{"text": "Also , in the data processing method according to the present invention , the compressed data composed of the positional information and the identification code can be directly output from the code table .Therefore , the data compressing apparatus for coding the input data at a high speed and the data reproducing apparatus for decoding the compressed data at a high speed can be manufactured while predicting the occurrence of the character or the character string .", "label": "", "metadata": {}, "score": "45.261223"}
{"text": "Therefore , the description of the contrivances are omitted .As is described above , as shown in FIG .16(B ) , the input data string to be compressed written in the input memory region successively connected to the dictionary memory region is regarded as the dictionary data string .", "label": "", "metadata": {}, "score": "45.27188"}
{"text": "In order to account for the limited computational capabilities of wireless sensor nodes , the method employs a quantization mechanism .Adaptive prediction avoids the requirement of defining the filtering coefficients a priori while still allowing the system to adjust to dynamic changes in the source .", "label": "", "metadata": {}, "score": "45.307842"}
{"text": "We present a novel entity - based representation of discourse which is inspired by Centering Theory and can be computed automatically from raw text .We view coherence assessment as a ranking learning problem and show that the proposed discourse representation supports the effective learning of a ranking function .", "label": "", "metadata": {}, "score": "45.393906"}
{"text": "[ 0060 ] In the first embodiment , the client terminal 11 has the dictionary .As was described above , it is possible to register keywords , for which related information is to be displayed , in the web server 13a of a webpage provider .", "label": "", "metadata": {}, "score": "45.478058"}
{"text": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .This method requires a source - language dependency parser , target language word segmentation and an unsupervised word alignment component .", "label": "", "metadata": {}, "score": "45.487465"}
{"text": "This paper describes a general scheme for segmenting text by inferring the position of word boundaries , thus supplying a necessary preprocessing step for applications like those mentioned above .Unlike other approaches , which involve a dictionary of legal words and are therefore language - specific , it works by using a corpus of already segmented text for training and thus can easily be retargeted for any language for which a suitable corpus of segmented material is available .", "label": "", "metadata": {}, "score": "45.636185"}
{"text": "2(A ) , 2(B ) and 2(C ) explanatorily shows a coding processing in which the same piece of character string is undesirably duplicated in a dictionary buffer ; .FIG .3(A ) is a constitutional view of a first conventional data compressing apparatus according to a second previously proposed art ; .", "label": "", "metadata": {}, "score": "45.798885"}
{"text": "Then , the positive and negative samples are sampled around the current estimated location of the target .Based on the learned results , sparse coefficients are obtained to train the SVM classifiers so that updated models are generated .Details of the IDSDL algorithm could refer to the last section , while the classifier training is described as follows .", "label": "", "metadata": {}, "score": "45.941864"}
{"text": "If there is no foreground detected , the process sends the dictionary learned during the tracking process in this camera .All the other trackers corresponding to different cameras would replace the old dictionary with a newly received one , so that the visual information on the dictionary level can be shared across the network .", "label": "", "metadata": {}, "score": "46.111423"}
{"text": "This method has an advantage in that it is possible for the web server 13a to markup keywords beforehand in an HTML document that will be transmitted to the client terminal 11 .However , in this method , a user can display related information only when accessing the web server retaining the keywords , but can not display the related information when accessing a web server not retaining the keywords .", "label": "", "metadata": {}, "score": "46.156574"}
{"text": "This is because target data is easily collected from internet environments and it is easy for the prediction task to quantitatively evaluate the accuracy .The method deals with both text sequential data and numerical sequential data related to evaluation objects .", "label": "", "metadata": {}, "score": "46.23551"}
{"text": "Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy that pairs initial ( presumably summary ) sentences from different news stories in the same cluster .We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation .", "label": "", "metadata": {}, "score": "46.33169"}
{"text": "2 In what follows we compare two strategies for unsupervised construction of such a corpus , one employing string similarity and the other associating sentences that may overlap very little at the s .. \" ...We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .", "label": "", "metadata": {}, "score": "46.33619"}
{"text": "Therefore , the higher the occurrence frequency of the input data , the higher the probability that the input data is converted into a shorter positional information .Therefore , the compressing efficiency for a series of data can be heightened as compared with that in the Huffman coding method in which an occurrence probability of each of characters is calculated .", "label": "", "metadata": {}, "score": "46.473312"}
{"text": "Similar to the decompression format situation , no complete natural language specification of the encoding techniques in 7-zip or xz seems to exist , other than the one attempted in the following text .The range encoder can not make any interesting choices , and can be readily constructed based on the decoder description .", "label": "", "metadata": {}, "score": "46.50055"}
{"text": "Next , a reproducing ( or decoding ) processing according to the first embodiment of the present invention is described .Thereafter , a piece of compressed data is read out from the compressed data file 27 and is written in the input buffer 22A.", "label": "", "metadata": {}, "score": "46.509914"}
{"text": "The speech recognition dictionary comprises a plurality of meaning tokens each associated with one or more pronunciations of one or more vocabulary words and signifying a single meaning .The speech recognizer is configured to convert spoken input into a sequence of meaning tokens that are contained in the speech recognition dictionary and correspond to a sequence of vocabulary words that are most likely to have been spoken by a user .", "label": "", "metadata": {}, "score": "46.55638"}
{"text": "The proposed dictionary contains both a positive template set and negative samples , and during training , the LU strategy is proposed to only update its partial columns .Furthermore , the positive samples are used to update the positive part , and their negative counterparts are used to generate the learned negative part .", "label": "", "metadata": {}, "score": "46.652916"}
{"text": "For example , in a computer ( C source ) program or the like , an occurrence probability of a series of codes \" OA \" subsequent to a series of codes \" OD \" indicated according to the hexadecimal notation is high .", "label": "", "metadata": {}, "score": "46.691864"}
{"text": "We discuss how the corpus can be usefully employed in evaluating paraphrase systems automatically ( e.g. , by measuring precision , recall and F1 ) and also in developing linguistically rich paraphrase models based on syntactic structure . \" ...This work explores computing distributional similarity between sub - parses , i.e. , fragments of a parse tree , as an extension to general lexical distributional similarity techniques .", "label": "", "metadata": {}, "score": "46.708355"}
{"text": "In addition to the SMT decoder , the toolki ... \" .We describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models .", "label": "", "metadata": {}, "score": "46.77683"}
{"text": "Therefore , the redundancy of the dictionary data in the dictionary buffer 22B is reduced as compared with that in the first prior art , and the data compression efficiency can be enhanced .( 2 ) Description of a second embodiment .", "label": "", "metadata": {}, "score": "46.92967"}
{"text": "Each meaning token preferably is characterized by a unique spelling .The spelling of a meaning token preferably facilitates extraction of meaning by a language analyzer .The spelling of a meaning token may encode one or more labels identifying one or more respective application - specific categories .", "label": "", "metadata": {}, "score": "46.95328"}
{"text": "We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data . ...English sentences with a state - of - the - art statistical parser ( Collins , 1999 ) .", "label": "", "metadata": {}, "score": "47.04444"}
{"text": "These systems and methods , however , are not limited to any particular hardware or software configuration , but rather they may be implemented in any computing or processing environment , including in digital electronic circuitry or in computer hardware , firmware or software .", "label": "", "metadata": {}, "score": "47.08561"}
{"text": "The reason is as follows .In general , the agreement data having a short bit length is often found out .Therefore , a short length code is allocated to the agreement data having a short bit length , so that the coding of the length of the agreement data can be considered .", "label": "", "metadata": {}, "score": "47.188046"}
{"text": "We compare the performance of our approach to that of ALFC using the same set of parameters employed in the experimental evaluation presented in [ 4 ] .That is , we used the quantization parameters . , since this is the minimum number of bits required to represent the measurements in our datasets .", "label": "", "metadata": {}, "score": "47.212482"}
{"text": "Prior to LZMA , most encoder models were purely byte - based ( i.e. they coded each bit using only a cascade of contexts to represent the dependencies on previous bits from the same byte ) .Furthermore , compared to classic dictionary compression ( such as the one used in zip and gzip formats ) , the dictionary sizes can be and usually are much larger , taking advantage of the large amount of memory available on modern systems .", "label": "", "metadata": {}, "score": "47.257454"}
{"text": "However , the probability of occurrence of a symbol not present in the dictionary is extremely low ( see Figure 1 ) .Hence , its value can be sent uncompressed and identified by the presence of a special marker in the Huffman dictionary .", "label": "", "metadata": {}, "score": "47.297386"}
{"text": "To formulate the training and classification process , a multiple linear classifier group based on a K - combined voting ( KCV ) function is proposed .As the dictionary evolves , the models are also trained to timely adapt the target appearance variation .", "label": "", "metadata": {}, "score": "47.340824"}
{"text": "In addition , since there may be fewer word candidates with pronunciations that are similar enough to the sound input , there may be fewer hypotheses to test against the grammar , increasing the efficiency of the automatic speech recognition process .", "label": "", "metadata": {}, "score": "47.352913"}
{"text": "Therefore , because a data search can be heightened as compared with the first prior art , a data compressing speed can be heightened .Therefore , the number of pieces of dictionary data or dictionary data strings to be referred can be substantially increased .", "label": "", "metadata": {}, "score": "47.504066"}
{"text": "The rest of the paper is organized as follows .In Section 2 , details of the proposed structured dictionary learning algorithm are described .Details of the proposed visual tracking algorithm within the Bayesian inference framework are proposed in Section 3 .", "label": "", "metadata": {}, "score": "47.565754"}
{"text": "However , the method can deal with respective values of the class counters for three or more classes in other tasks .Algorithm 2 shows a pseudocode for the prediction phase .In Algorithm 2 , text sequential data ( .Expansion of Training Data .", "label": "", "metadata": {}, "score": "47.627"}
{"text": "By applying the method in [ 2 ] to the temperatures in Set 1 we obtain .with respect to the original 6 bits / symbol .Nevertheless , note that the scheme proposed in [ 2 ] uses a fixed dictionary , which can be applied to any source .", "label": "", "metadata": {}, "score": "47.79056"}
{"text": "Also , an alphabet \" u \" subsequent to an alphabet \" q \" occurs at a high probability .However , even though the input data indicating the sentence in which the word \" and \" is frequently used is input to the first apparatus , it is required to search the pieces of candidate data relating to the converted data DT one after another .", "label": "", "metadata": {}, "score": "47.817917"}
{"text": "In this data compressing apparatus , an occurrence probability of each of single characters is calculated , and a variable - length code is allocated to each of the single characters .As a result , there is a problem that a compression efficiency for each of the data strings uniformly occurring can not be heightened .", "label": "", "metadata": {}, "score": "47.86457"}
{"text": "Therefore , the number of dictionary data to be referred can be substantially increased .Also , the agreement data or the agreement data string can be found out by extending the dictionary data or the dictionary data strings to the input memory region .", "label": "", "metadata": {}, "score": "48.021126"}
{"text": "Basically , we assume that the target could be represented with a lower error by its overlapped patches in the form of the target templates ' learning results in the previous frames .The template contains a set of images , each of which is cropped from the corresponding video frame based on the latest tracking results .", "label": "", "metadata": {}, "score": "48.19066"}
{"text": "Second , grammar processes that attempt to make fine distinctions among possible utterances are complicated , inefficient , and hard to write .Simpler grammars , on the other hand , allow many incorrect utterances that can not be parsed ( or interpreted ) for meaning .", "label": "", "metadata": {}, "score": "48.199932"}
{"text": "We propose a theory that gives formal semantics to word - level alignments defined over parallel corpora .We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human ... \" .", "label": "", "metadata": {}, "score": "48.239906"}
{"text": "That is , one or more input data strings of the input memory region successively connected to the dictionary memory region in which the dictionary data string are written are regarded as one or more dictionary data strings , and the agreement data string can be found out .", "label": "", "metadata": {}, "score": "48.288704"}
{"text": "Most of the processing time is spent on the dictionary learning and classifier training part .It is certain that the processing could be several times faster in both single camera and visual sensor network cases when all the codes are re - written in C. The running speed could be higher if the processing could be paralleled or assisted with a graphic processing unit ( GPU ) coprocessor , since each patch could be independently processed before KCV without interleaving .", "label": "", "metadata": {}, "score": "48.80326"}
{"text": "The same synthesis of information can also be of interest to the patient .The summarizer predicts which medical terms used in a text will be too technical for patients , and augments it with appropriate definitions when necessary .We adopt a generation - like architecture for our summarizer .", "label": "", "metadata": {}, "score": "48.8535"}
{"text": "13 is a flow chart showing a reproducing ( or decoding ) processing of a piece of compressed data according to the second embodiment of the present invention ; .FIG .14(A ) shows a fixed dictionary of a fixed dictionary buffer according to a third embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "48.9202"}
{"text": "Therefore , the LZ77 coding method is called the slide dictionary method .PROBLEMS TO BE SOLVED BY THE INVENTION : .Therefore , the same piece of dictionary data is undesirably stored in duplicate in the dictionary buffer 2B , and a piece of dictionary data which has previously agrees with a piece of input data is necessarily expelled from the dictionary buffer 2B. As a result , a data compressing efficiency is lowered .", "label": "", "metadata": {}, "score": "48.97177"}
{"text": "BACKGROUND OF THE INVENTION .Field of the Invention .Also , the present invention relates to data processing method and apparatus in which a piece of input is compressed according to a Huffman code method and a piece of compressed data is decoded according to the Huffman code method .", "label": "", "metadata": {}, "score": "49.049633"}
{"text": "This search is continued until the current occurrence frequency of the current input code becomes lower than a compared occurrence frequency placed in another upper entry while replacing the upper entry to another one placed in an upper position moreover .Thereafter , the arrangement positions of the current input code and the upper entry are exchanged in a step P35 .", "label": "", "metadata": {}, "score": "49.125023"}
{"text": "Once the final solution of a subproblem is found , the LZMA state and least used distances for it are computed , and are then used to appropriately compute post - range - encoding sizes of its extensions .At the end of the dynamic programming optimization , the whole optimal encoding of the longest substring considered is output , and encoding continues at the first uncompressed byte not already encoded , after updating the LZMA state and least used distances .", "label": "", "metadata": {}, "score": "49.154358"}
{"text": "FIG .6 , in some embodiments , the spelling of a meaning token may encode various kinds of information that may be used by language analyzer 14 or a downstream application program .In some embodiments , a meaning token may encode one or more labels identifying one or more respective application - specific categories , such as an object category , a place category , an event category , or an action category .", "label": "", "metadata": {}, "score": "49.21439"}
{"text": "For example , when the resource is a webpage that is written using HTML ( Hyper Text Markup Language ) , the parser analyzes a structure of the acquired HTML document while referencing elements that are separated by tags .A plugin is a kind of program that is added to an application program , and by switching plugins , it is possible to realize various functions .", "label": "", "metadata": {}, "score": "49.288624"}
{"text": "Experiments o ... \" .We propose a self - supervised word segmentation technique for text segmentation in Chinese information retrieval .This method combines the advantages of traditional dictionary based , character based and mutual information based approaches , while overcoming many of their shortcomings .", "label": "", "metadata": {}, "score": "49.29543"}
{"text": "Although in this kind of scenario in which there is little variation of symbol probability , adaptive Huffman would tend to perform well ; the main drawback of this approach is that it requires maintaining one dictionary for each connection between a pair of neighboring nodes [ 8 ] .", "label": "", "metadata": {}, "score": "49.365273"}
{"text": "This is explained in detail below .[0044 ] FIG .6 illustrates a method for rewriting the DOM nodes .By setting new tags for the extracted keywords , or rewriting tags , the keyword markup module 41 creates or changes the DOM nodes .", "label": "", "metadata": {}, "score": "49.45275"}
{"text": "The transformation can be performed by incorporating additional steps as shown in Algorithm 3 between step 3 and step 4 in Algorithm 1 .In the additional steps , TD is a topic dictionary .Step 4 in Algorithm 3 transforms a topic ( w ) into evaluation objects .", "label": "", "metadata": {}, "score": "49.53713"}
{"text": "It consists of creating , for each k less than N , a hash table indexed by tuples of k bytes , where each of the buckets contains the last position where the first k bytes hashed to the hash value associated with that hash table bucket .", "label": "", "metadata": {}, "score": "49.742046"}
{"text": "Tools . by W. J. Teahan , Yingying Wen , Rodger Mcnab , Ian H. Witten - Computational Linguistics . \" ...This paper describes a general scheme for segmenting text by inferring the position of word boundaries , thus supplying a necessary preprocessing step for applications like those mentioned above .", "label": "", "metadata": {}, "score": "49.881187"}
{"text": "In the Shannon - Fano coding , the higher an occurrence probability of a character , the smaller the number of bytes of a variable - length code allocated to the character .Therefore , each of pieces of data is compressed .", "label": "", "metadata": {}, "score": "49.949524"}
{"text": "These coefficients would be the input of the SVM classifiers for training .It can be found that , without dictionary learning , the coefficient values are more globally distributed across the dimension , while with dictionary learning , the data are more aggregated .", "label": "", "metadata": {}, "score": "49.96398"}
{"text": "The encoder needs to be able to quickly locate matches in the dictionary .Since LZMA uses very large dictionaries ( potentially on the order of gigabytes ) to improve compression , simply scanning the whole dictionary would result in an encoder too slow to be practically usable , so sophisticated data structures are needed to support fast match searches .", "label": "", "metadata": {}, "score": "49.96776"}
{"text": "Thus , statistically speaking , it improves the estimation generalization during the tracking process .The maximal scheme is a nonlinear superposition process , and it creates more confidence points given limited candidates .It has been shown from the experiments that our proposed tracker is currently not sufficient for real - time processing .", "label": "", "metadata": {}, "score": "49.979256"}
{"text": "In the Huffman coding , the data compressing of pieces of data is performed by considering differences in occurrence frequency of characters .Thereafter , another data compressing method in which a concept of a dictionary is used and a data compressing is performed by considering the repetition of each of character strings .", "label": "", "metadata": {}, "score": "50.05513"}
{"text": "The automatic speech recognition system also may include an application command translator that is configured to select an action from a set of application - specific actions based upon the meaning extracted by the language analyzer , and to issue one or more commands to carry out the selected action .", "label": "", "metadata": {}, "score": "50.201473"}
{"text": "PROBLEMS TO BE SOLVED BY THE INVENTION : .Therefore , even though the order of a plurality of input data strings is predicted to some extent , it is required to search the candidate data registered in a dictionary having a connected - list structure one after another according to the dictionary searching list .", "label": "", "metadata": {}, "score": "50.226856"}
{"text": "The natural language processor attempts to make sense of the word sequences that are output by the speech recognizer .That is , the natural language processor attempts to extract from each word sequence a meaning that is relevant to a specific user application .", "label": "", "metadata": {}, "score": "50.31176"}
{"text": "Next , a sixth compressed data reproducing ( or decoding ) method according to the sixth embodiment of the present invention is described .Therefore , the code table described in FIGS . 19(A ) and 19(B ) is , for example , reproduced .", "label": "", "metadata": {}, "score": "50.33754"}
{"text": "3 - 14 , March 1995 .View at Scopus .J. Pei , J. Han , B. Mortazavi - Asl et al . , \" PrefixSpan : mining sequential patterns efficiently by prefix - projected pattern growth , \" in Proceedings of the 17th International Conference on Data Engineering , pp .", "label": "", "metadata": {}, "score": "50.34631"}
{"text": "Therefore , an identification code is arranged at a head position of the compressed data to add the position information of the agreement data found out .For example , FIG .9(A ) shows a data format in which a piece of positional information is added to a piece of compressed data in case where a memory size of the dictionary buffer 22B is 2 KB .", "label": "", "metadata": {}, "score": "50.58008"}
{"text": "This would mitigate the impact in the performance of the method seen , for example , when Set 3 was used to generate the dictionary .In fact , taking one step further , this approach might allow us to establish synthetic measurement distributions for different environmental variables ( e.g. , temperature or relative humidity ) which would allow sensor nodes to generate measurement dictionaries on - the - fly without the necessity of referring to reference datasets .", "label": "", "metadata": {}, "score": "50.595562"}
{"text": "The natural language processor identifies permissible sentences based upon the internal semantic patterns , and outputs summary structures representing permissible sentences .In task - oriented automatic speech recognition systems , an application command translator may match the summary structures with a known set of user application commands , and may return appropriate commands to a user application for processing .", "label": "", "metadata": {}, "score": "50.624504"}
{"text": "Also , when any agreement data string does not exist ( NO ) , the procedure proceeds to a step P6 , the original data string is coded as it is , and the procedure proceeds to a step P7 .To be concrete , a piece of positional information of the agreement data string and a piece of length information of the agreement data string are coded to produce a positional code and a length code .", "label": "", "metadata": {}, "score": "50.6373"}
{"text": "In order to make use of the visual information acquired as much as possible , we establish the tracking process with a shared dictionary across all the cameras , shown in Figure 13 .When the cameras are switched on , their trackers begin to work .", "label": "", "metadata": {}, "score": "50.759792"}
{"text": "The decoding operation is performed according to the configuration of a code tree used in general and shown in FIG .20(B ) .Thereafter , it is judged in a step P3 whether or not all of pieces of compressed data stored in the compressed data file 51 are decoded .", "label": "", "metadata": {}, "score": "51.001938"}
{"text": "It selects the appropriate training data to construct the prediction model .The paper shows that the method arrives at the high prediction performance .Peramunetilleke and Wong [ 12 ] propose a method that acquires classification rules .Their antecedent part is weighted keywords and their result part is classes discretizing changes for currency exchange .", "label": "", "metadata": {}, "score": "51.097977"}
{"text": "Most real - time automatic speech recognition systems include a speech recognizer and a natural language processor .The speech recognizer converts an incoming stream of sounds into a likely sequence of words .In particular , the speech recognizer receives digitized speech samples from an acoustic input device ( e.g. , a microphone ) , and converts the digitized speech samples into sequences of recognized words based upon finite state machine templates .", "label": "", "metadata": {}, "score": "51.113716"}
{"text": "In the same manner , an identification code \" 001 \" is allocated to 4 data samples to identify pieces of positional information placed in positions ranging from a second position to a fifth position , and each of the positional information has 2 bits .", "label": "", "metadata": {}, "score": "51.128468"}
{"text": "15(A ) is a constitutional view of a ring - shaped dictionary buffer of a fourth data processing apparatus according to a fourth embodiment of the present invention ; .FIGS . 15(B ) and 15(C ) respectively show a searching condition view of the ring - shaped dictionary buffer ; .", "label": "", "metadata": {}, "score": "51.309616"}
{"text": "In some embodiments , these systems preferably are implemented in a high level procedural or object oriented processing language ; however , the algorithms may be implemented in assembly or machine language , if desired .In any case , the processing language may be a compiled or interpreted language .", "label": "", "metadata": {}, "score": "51.35827"}
{"text": "In case where the fixed code does not exist in the input buffer 22A in the step P1 , a decoding operation is performed in the dictionary buffer 22B in a step P2 , and the procedure is finished .Therefore , pieces of input data to be decoded can be decoded while referring the fixed dictionary data of the fixed dictionary buffer 22D.", "label": "", "metadata": {}, "score": "51.393684"}
{"text": "Abstract .: To tackle robust object tracking for video sensor - based applications , an online discriminative algorithm based on incremental discriminative structured dictionary learning ( IDSDL - VT ) is presented .In our framework , a discriminative dictionary combining both positive , negative and trivial patches is designed to sparsely represent the overlapped target patches .", "label": "", "metadata": {}, "score": "51.45308"}
{"text": "We consider a sensor node monitoring environmental data .Let the data acquired by the sensor at time instant .The efficiency of a compression algorithm can be measured by comparing the average symbol length after compression to the source entropy .", "label": "", "metadata": {}, "score": "51.55996"}
{"text": "We compare the lifecycle of the Kalman tracking algorithm and the proposed algorithm without and with dictionary share .The result of the lifecycle is shown in Table 3 .Vertically , it can be found from the table that the proposed algorithm with dictionary sharing achieves higher values .", "label": "", "metadata": {}, "score": "51.75"}
{"text": "In Section 5 , concluding remarks and a possible direction for future research are provided .Discriminative Structured Dictionary Learning .We begin the description of the proposed dictionary learning algorithm , incremental discriminative structured dictionary learning ( IDSDL ) , with the sparse appearance modeling as follows .", "label": "", "metadata": {}, "score": "51.780544"}
{"text": "In addition , it is possible to use a TRIE ( ordered tree structure ) dictionary , a hash structure dictionary , or the like as the dictionary 42 ( see a fourth embodiment ) .[ 0040 ] When the user agent 31 acquires an HTML document ( S504 ) , and displays a webpage , the user agent 31 notifies the plugin 34 of an event , and calls up the plugin 34 ( S524 ) .", "label": "", "metadata": {}, "score": "51.81803"}
{"text": "In the above configuration , an operation in the data processing apparatus is described with reference to FIG .4 .Thereafter , the input data to be compressed or the input data string to be compressed is compared with the dictionary data or the dictionary data strings under the control of the dictionary control means 24 .", "label": "", "metadata": {}, "score": "52.003216"}
{"text": "Meaning token dictionary for automatic speech recognition US 6963832 B2 .Abstract .Systems and methods of automatic speech recognition are described .In one aspect , an automatic speech recognition system includes a speech recognition dictionary and a speech recognizer .", "label": "", "metadata": {}, "score": "52.08336"}
{"text": "As the alphabet is fixed , the complexity of the proposed approach is rather low , being no more complex than that in [ 2 ] .For instance , an implementation of the Huffman encoding and decoding for AVR microcontrollers , widely used in sensor nodes , utilizes only 468 bytes of program memory [ 24 ] .", "label": "", "metadata": {}, "score": "52.311703"}
{"text": "In the next frame , target candidates are sampled based on affine warping , and their corresponding coefficients are obtained .Then , given the learned model set , patch - based classifications are conducted to calculate the confidence set , and a K - combined voting ( KCV ) function is used to jointly locate the target .", "label": "", "metadata": {}, "score": "52.317722"}
{"text": "Also , in case where the distribution of the occurrence frequencies is one - sided , each of the input data can be converted into a piece of compressed data having a shorter bit length by increasing a level adjustment according to the offset coding .", "label": "", "metadata": {}, "score": "52.31773"}
{"text": "Figure 1 .Workflow of the proposed algorithm .The proposed dictionary learning method , incremental discriminative structured dictionary learning ( IDSDL ) , is detailed in Section 2 , while the proposed affine warping , support vector machine ( SVM ) training and classification and K - combined voting ( KCV ) are detailed in Section 3 .", "label": "", "metadata": {}, "score": "52.319286"}
{"text": "As shown in FIG .11(A ) , a ring - shaped structure type of buffer having a certain memory capacity is , for example , used for the agreement dictionary buffer 22C. The ring - shaped structure type of buffer is described in detail in a fourth embodiment of the present invention .", "label": "", "metadata": {}, "score": "52.34546"}
{"text": "As illustrated in FIG .6 , the interface module 43 creates a new DOM node that includes the keyword \" plugin \" , and performs markup by replacing the original keyword in the HTML document .[0045 ] At this time , if creating the new DOM node over an existing DOM node , the CSS of the existing DOM node may not necessarily be properly inherited .", "label": "", "metadata": {}, "score": "52.357315"}
{"text": "Comparatively , the proposed tracker is evaluated against state - of - the - art algorithms , including Frag [ 4 ] , IVT [ 3 ] , VTD [ 16 ] , L1 T [ 11 ] , TLD [ 10 ] , MIL [ 9 ] and PLS [ 18 ] .", "label": "", "metadata": {}, "score": "52.372627"}
{"text": "[ 0039 ]FIG .5 illustrates the keyword display method according to the first embodiment of the present invention .After the web browser 22 is started up on the client terminal 11 ( S502 ) , the user agent 31 calls up the plugin 34 , and the dictionary 42 is expanded in the memory of the client terminal 11 ( S522 ) .", "label": "", "metadata": {}, "score": "52.386032"}
{"text": "According to the present embodiment , when a webpage including keywords that have been specified beforehand is displayed , the webpage is displayed so that it is obvious that the keywords have been registered .Moreover , when the user clicks on a keyword , only information related to the keyword is displayed .", "label": "", "metadata": {}, "score": "52.38713"}
{"text": "They are also the most diffi - cult operations to identify automatically , with the lowest overall classifier accuracy among all operations ( 73 % and 59 % , re - spectively ) . \" ... Abstract .This paper proposes a method to automatically classify texts from different varieties of the same language .", "label": "", "metadata": {}, "score": "52.51413"}
{"text": "The dictionary 15 is referred by the searching and registering section 14A of the SOR coding section 14 to recognize whether or not a character string to be compressed is registered in the dictionary 15 .Thereafter , the character strings stored in the dictionary 15 are renewed according to a rule of a self - organization by the dictionary rearranging section 14B.", "label": "", "metadata": {}, "score": "52.53316"}
{"text": "On the other hand , the knowledge discovery task depends on features of the data and types of the knowledge .It is impossible to deal with all features and all types by only a method .It is indispensable to develop a discovery method reflecting target features and types .", "label": "", "metadata": {}, "score": "52.777702"}
{"text": "Set low to low + bound .Perform normalization 5 times .Bit - tree encoding is performed like decoding , except that bit values are taken from the input integer to be encoded rather than from the result of the bit decoding functions .", "label": "", "metadata": {}, "score": "52.816788"}
{"text": "[0065 ] The keyword markup module 41 searches only a specified category of the dictionary 42 according to extracted text , and extracts keywords for which related information is to be displayed .[ 0066 ] While the present invention has been described with reference to exemplary embodiments , it is to be understood that the invention is not limited to the disclosed exemplary embodiments .", "label": "", "metadata": {}, "score": "52.85933"}
{"text": "This keyword is regarded as a topic .This paper activates various topics in order to expand the learning data .The topics are managed by a topic dictionary with a three - layer structure .First layer is middle categories representing related topics , second layer is topics , and third layer is evaluation objects .", "label": "", "metadata": {}, "score": "52.861324"}
{"text": "The training data can be expanded without generating many meaningless patterns due to the improvement of the discovery method .The improvement can be performed by incorporating an additional step as shown in Algorithm 4 between step 16 and step 17 in Algorithm 1 .", "label": "", "metadata": {}, "score": "52.896812"}
{"text": "Therefore , because the occurrence of a series of characters or a character is predicted , a concept of the dictionary search disappears .In this case , a data compressing efficiency defined as a ratio of an input data capacity to a compressed data capacity : ( input data capacity)/(compressed data capacity ) v 100 % .", "label": "", "metadata": {}, "score": "52.971092"}
{"text": "( 3 ) Description of a third embodiment .FIGS .14(A ) to 14(C ) are explanatory views of a third data processing method utilizing a fixed dictionary according to a third embodiment of the present invention .The fixed dictionary is placed in a compression and expansion control program of the EPROM 23 and is added to the compressed data coded .", "label": "", "metadata": {}, "score": "53.007668"}
{"text": "[ 15 ] is discriminative , yet constructed without learning .Differently , in this paper , we exploit dictionary learning within the discriminative tracking framework and establish a tracking process based on patch - based classifiers .We focus on the dictionary design part rather than the optimization process .", "label": "", "metadata": {}, "score": "53.191803"}
{"text": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .Compared to the standard practice of intersecting predictions of independently - trained models , joint training provides a 32 % reduction in AER .", "label": "", "metadata": {}, "score": "53.20147"}
{"text": "News article headlines are a rich source of paraphrases ; they tend to describe the same event in various different ways , and can easily be obtained from the web .We compare two methods of aligning headlines to construct such an aligned corpus of paraphrases , one based on clustering , and the other on pairwise similarity - based matching .", "label": "", "metadata": {}, "score": "53.22431"}
{"text": "The range decoder also provides the bit - tree , reverse bit - tree and fixed probability integer decoding facilities , which are used to decode integers , and generalize the single - bit decoding described above .Non - reverse bit - tree decoding works by keeping a pointer to the tree of variables , which starts at the root .", "label": "", "metadata": {}, "score": "53.341446"}
{"text": "Third Embodiment . [0062 ]In a third embodiment , not only is a dictionary registered in the web server 13a , the plugin function described above is installed in the web server 13a .This method has the advantage in that the web server 13a can mark up keywords beforehand in an HTML document to be sent to the client terminal 11 , and decrease a load of keyword processing in the client server .", "label": "", "metadata": {}, "score": "53.397278"}
{"text": "Therefore , in case where the original data having a large capacity is compressed and coded , a significant effect of the data compressing efficiency can be expected in the fifth embodiment .As is described above , in the first data processing apparatus according to the present invention , a following operation is performed under the control of the dictionary control means 24 .", "label": "", "metadata": {}, "score": "53.41578"}
{"text": "We show for the first time that incorporating the predictions of a word sense disambigua - tion system within a typical phrase - based statistical machine translation ( SMT ) model consistently improves translation quality across all three different IWSLT Chinese - English test sets , as well as producing st ... \" .", "label": "", "metadata": {}, "score": "53.433594"}
{"text": "For example , idiomatic and context dependent word combinations , such as \" go for it \" or \" look it up , \" may be handled in the inventive meaning token dictionary rather than in the grammar .Other features and advantages of the invention will become apparent from the following description , including the drawings and the claims .", "label": "", "metadata": {}, "score": "53.441475"}
{"text": "5 is a block diagram of a meaning token dictionary associating multiple pronunciations with a single meaning token .FIG .6 is a block diagram of a meaning token dictionary associating a single pronunciation with one of multiple possible meaning tokens .", "label": "", "metadata": {}, "score": "53.619797"}
{"text": "Typical techniques include boosting [ 8 , 9 ] and support vector machine ( SVM ) [ 7 , 21 , 22 ] .For the trackers using SVM , Avidan et al .[ 7 ] propose a tracking algorithm integrating SVM to discriminate the target from its background .", "label": "", "metadata": {}, "score": "53.633537"}
{"text": "However , for the application of visual tracking , the size can not be very high for computational efficiency .On the other hand , a fixed dictionary is generally not sufficient to cope with the appearance changes of the tracking object , as well as the background .", "label": "", "metadata": {}, "score": "53.77063"}
{"text": "This paper describes a hybrid model that combines machine learning with linguistic heuristics for integrating unknown word identification with Chinese word segmentation .The model consists of two components : a position - of - character ( POC ) tagging component that annotates each character in a sentence with a POC tag that indicates its position in a word , and a merging component that transforms a POCtagged character sequence into a word - segmented sentence .", "label": "", "metadata": {}, "score": "53.849434"}
{"text": "The last row shows the average symbol length over all datasets .We can see that the average symbol length over all sets is reasonably consistent , except for that obtained using the dictionary generated by Set 3 , which is higher than the average .", "label": "", "metadata": {}, "score": "53.92778"}
{"text": "Pattern matching system 82 establishes a correspondence between the sequence of spectrum frames 86 and pre - stored ( trained ) representations of speech sounds 88 , and produces a sequence of speech units 90 .Meaning token selecting system 84 assembles the speech units 90 into sets of possible pronunciations and selects from a meaning token dictionary 92 a sequence of recognized meaning tokens 20 corresponding to a sequence of vocabulary words mostly likely to have been spoken by a user .", "label": "", "metadata": {}, "score": "54.21579"}
{"text": "t .N . ] drawn around the estimated target location and follow the same patch cropping pattern with that of target representation .For the i -th patch , the training data is made up of sparse coefficients by solving Equation ( 2 ) with .", "label": "", "metadata": {}, "score": "54.215965"}
{"text": "In the same manner , an identification code \" 01 \" is allocated to 2 data samples to identify pieces of positional information placed in a first position and a second position , and each of the positional information has 1 bit .", "label": "", "metadata": {}, "score": "54.237423"}
{"text": "A subset of the cor - pus was first manually analysed to iden - tify its transformation operations .We then built machine learning models to attempt to automatically classify segments based on such transformations .This classifica - tion could be used , e.g. , to filter out po - tentially noisy transformations .", "label": "", "metadata": {}, "score": "54.261997"}
{"text": "In the example of .In this case , if a user says to the application \" Do you have Profiles in Courage ? \"In some embodiments , meaning tokens may have longer spellings than the corresponding vocabulary words that are contained in a conventional speech recognition dictionary .", "label": "", "metadata": {}, "score": "54.284927"}
{"text": "SUMMARY OF THE INVENTION .[0010 ] The present invention is made in consideration of this kind of problem , and an object thereof is to provide a keyword display method and a keyword display system that display only information related to a keyword described on a webpage .", "label": "", "metadata": {}, "score": "54.316734"}
{"text": "Evaluation on five test sets with different standards shows that the adaptive system achieves state - of - the - art performance on all the test sets . rules ( or transformations ) are acquired automatically from application data via the TBL method ( Gao et al .", "label": "", "metadata": {}, "score": "54.384842"}
{"text": "Evaluation on five test sets with different standards shows that the adaptive system achieves state - of - the - art performance on all the test sets . rules ( or transformations ) are acquired automatically from application data via the TBL method ( Gao et al .", "label": "", "metadata": {}, "score": "54.384842"}
{"text": "making each of the compressed data by combining one of the pieces of positional information and the identification code .BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1(A ) is a constitutional view of a conventional data compressing apparatus according to a first previously proposed art ; .", "label": "", "metadata": {}, "score": "54.480934"}
{"text": "[0031 ] FIGS .3A and 3B illustrate an example of a webpage according to one embodiment of the present invention .FIG .4 illustrates an example of an HTML document that is a resource for this webpage .The HTML document specifies a structure of the document by classifying plain text into various \" elements \" and defines those elements .", "label": "", "metadata": {}, "score": "54.605637"}
{"text": "( bits / sample ) after compression using ALFC [ 4 ] and the proposed method for different packet sizes .Impact of Encoding Symbols Not Present in the Dictionary .To evaluate the impact of transmitting uncompressed symbols not present in the dictionary , we augmented each of the measurement datasets in Table 1 by inserting pairs of uncompressed symbols along with the corresponding special marker and computed the corresponding compression ratio .", "label": "", "metadata": {}, "score": "54.721115"}
{"text": "Therefore , the positional information can be rewritten at a short time by the code converting editor 43 while renewing of the occurrence frequency table in the occurrence frequency producing editor 41 , and the code table can be dynamically reconstructed .", "label": "", "metadata": {}, "score": "54.731094"}
{"text": "This improves the compression of partially or completely incompressible files and allows multithreaded compression and multithreaded decompression by breaking the file into runs that can be compressed or decompressed independently in parallel .The LZMA2 header consists of a byte indicating the dictionary size : .", "label": "", "metadata": {}, "score": "54.74302"}
{"text": "Each group corresponds to the number of bits required to represent the measurement differences .These groups are then entropy coded using a fixed compression table based on the baseline JPEG algorithm to compress the DC coefficients of an image .The compressed symbols are formed by concatenating the group number and the index of the element within the group .", "label": "", "metadata": {}, "score": "54.863823"}
{"text": "We note that the transformation can be applied to the prediction phase .That is , the additional steps as shown in Algorithm 3 are incorporated between step 5 and step 6 in Algorithm 2 .More evaluation transactions are used in order to identify attractive evaluation objects .", "label": "", "metadata": {}, "score": "54.993164"}
{"text": "Each method in each subfigure has 3 bar graphs corresponding to the precision , the recall , and the F -measure .Discussions .This section discusses the effect of the proposed method with four viewpoints : the expansion of training transactions , the discovered trend rules , the extracted evaluation objects , and the detection performance .", "label": "", "metadata": {}, "score": "55.02516"}
{"text": "11(C ) is an explanatory view showing a searching range of the agreement data in an agreement dictionary buffer ; .FIG .12 is a flow chart showing the generation of an agreement dictionary according to the second embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "55.065018"}
{"text": "That is , in case where the distribution of the occurrence frequencies is definitely one - sided , a length of a piece of compressed data converted in a first coding is adjusted .Therefore , the positional information of the compressed data can be moreover shortened , and the data compressing efficiency is enhanced .", "label": "", "metadata": {}, "score": "55.119904"}
{"text": "In the 7-zip LZMA file format , configuration is performed by a header containing the \" properties \" byte followed by the 32-bit little - endian dictionary size in bytes .In LZMA2 , the properties byte can optionally be changed at the start of LZMA2 LZMA packets , while the dictionary size is specified in the LZMA2 header as later described .", "label": "", "metadata": {}, "score": "55.198914"}
{"text": "Also , an identification code \" 011 \" is allocated to 4 data samples to identify pieces of positional information placed in positions ranging from a fourth position to a seventh position , and each of the positional information has 2 bits .", "label": "", "metadata": {}, "score": "55.2399"}
{"text": "Therefore , a data searching time can be shortened as compared with that in the first embodiment , and a data compression processing can be performed at a high speed .Table 1 shows a relationship among pieces of compressed data obtained by compressing pieces of binary data and pieces of text data according to three types of data compressing methods of the first , fourth and fifth embodiments .", "label": "", "metadata": {}, "score": "55.458103"}
{"text": "Aligning the English phrase to be paraphrased was the German - English section of the Europarl corpus , version 2 ( Koehn , 2002 ) .Because we wanted to test our method independently of the quality of word alignment algorithms , we also developed a gold standard of word alignments for the set of phrases that we wanted to paraphr ... .", "label": "", "metadata": {}, "score": "55.468075"}
{"text": "In the same manner , an identification code \" 010 \" is allocated to 2 data samples to identify pieces of positional information placed in positions ranging from a second position to a third position , and each of the positional information has 1 bits .", "label": "", "metadata": {}, "score": "55.501305"}
{"text": "Our framework is a medical digital library for physicians and patients .We describe a summarizer , which generates summaries of findings in an input set of clinical studies .When a physician is treating a specific patient , he 's looking for information relevant to the patient 's history and problems .", "label": "", "metadata": {}, "score": "55.51072"}
{"text": "Although lossy compression methods can generally achieve high compression ratios at the expense of moderate accuracy losses , in many WSN applications it may not be clear before data collection how much information can be disregarded without compromising the overall purpose of the system .", "label": "", "metadata": {}, "score": "55.549477"}
{"text": "In a first coding level of the Table 5 , for example , one of five types of identification codes is allocated to each of 256 data samples to identify 256 pieces of positional information obtained by converting the data samples .", "label": "", "metadata": {}, "score": "55.562897"}
{"text": "We present Minimum Bayes - Risk ( MBR ) decoding for statistical machine translation .This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of l ... \" .", "label": "", "metadata": {}, "score": "55.578186"}
{"text": "As a result , a data compression efficiency for the original data coded as it is ( step P6 in FIG .8) can be enhanced .Accordingly , the dictionary data expelled from the dictionary buffer 22B can be efficiently used , and a data compressing operation in which a moving window utilizing the dictionary buffer 22B and the agreement dictionary buffer 22C together is used can be performed .", "label": "", "metadata": {}, "score": "55.603783"}
{"text": "Furthermore , it does not require any alignment or correspondence among the complex and simple corpora .We evaluate the simplification according to three criteria : preservation of grammaticality , preservation of meaning , and degree of simplification .Results show that our method outperforms an established simplification baseline for both meaning preservation and simplification , while maintaining a high level of grammaticality . \" ...", "label": "", "metadata": {}, "score": "55.625973"}
{"text": "Also , as an occurrence probability of a type of a piece of input data DIN is higher , the type of the input data DIN is converted into a piece of compressed data having a shorter bit length while referring the code table .", "label": "", "metadata": {}, "score": "55.6362"}
{"text": "A compressed data format in each of the embodiments is shown in FIG .20(A ) .In FIG .20(A ) , a piece of compressed data DOUT is composed of a piece of positional information coded and an identification code added to a head of the positional information .", "label": "", "metadata": {}, "score": "55.657845"}
{"text": "Increment cache_size .Set low to the lowest 24 bits of low shifted left by 8 bits .Set range to range shifted left by 8 bits .Context - based range encoding of a bit using the prob probability variable proceeds in this way : .", "label": "", "metadata": {}, "score": "55.738754"}
{"text": "If a text includes keywords included in the first layer and the second layer , they are replaced with evaluation objects stored in their third layer .Figure 3 shows an outline of the transformation .That is , in the case of the keyword ' ' Strong yen ' ' in the first original transaction , the method extracts ' ' Strong yen ' ' in the dictionary and extracts corresponding stock brands ' '", "label": "", "metadata": {}, "score": "55.830505"}
{"text": "The first object is also achieved by the provision of a data processing method ( called a first data processing method ) , comprising the steps of : . placing the dictionary data or the dictionary data strings stored in the data writing range of the dictionary , from which the particular dictionary data or the particular dictionary data string is expelled , close together in a direction ; and .", "label": "", "metadata": {}, "score": "55.89158"}
{"text": "The feature of the second embodiment is that the correspondence of the agreement data to the agreement dictionary buffer 22C is made up without increasing the number of coded bits on condition that the above requirement is satisfied .To be concrete , as shown in FIG .", "label": "", "metadata": {}, "score": "55.904404"}
{"text": "[17 ] propose a novel online object tracking algorithm with sparse prototypes , which adopts principal component analysis ( PCA ) basis vectors and trivial templates to represent the tracked target sparsely , and solve the problem by using an iterative thresholding method .", "label": "", "metadata": {}, "score": "55.907604"}
{"text": "In the same manner , an identification code \" 001 \" is allocated to 8 data samples to identify pieces of positional information placed in positions ranging from a fourth position to an eleventh position , and each of the positional information has 3 bits .", "label": "", "metadata": {}, "score": "55.964394"}
{"text": "Because the particular dictionary data string agrees with the particular input data string , an agreement data string representing the particular dictionary data string and the particular input data string is coded .Thereafter , the ring - shaped dictionary buffer 22E is searched to judge whether or not an agreement data string exists in the ring - shaped dictionary buffer 22E.", "label": "", "metadata": {}, "score": "56.013638"}
{"text": "Furthermore , the state - of - the - art algorithms perform well when the resolution of the data collected by the sensor nodes is very high , but when the data resolution is limited to integer measurements they suffer significant performance penalties .", "label": "", "metadata": {}, "score": "56.01821"}
{"text": "Yet SMT translation qual - ity still obviously suffers from inaccurate lexical choice .In this paper , we address this problem by investigating a new strat - egy for integrating WSD into an SMT sys - tem , that performs fully phrasal multi - word disambiguation .", "label": "", "metadata": {}, "score": "56.077724"}
{"text": "Bollen et al .[ 8 ] propose a method that inductively learns relationships between DJIA and 6 kinds of emotions included in the messages described in Twitter .The method acquires the relationships by using fuzzy self - organization maps .", "label": "", "metadata": {}, "score": "56.13669"}
{"text": "Next , a reproducing ( or decoding ) processing of the compressed data according to the second embodiment of the present invention is described .FIG .13 is a flow chart showing a reproducing ( or decoding ) processing of the compressed data according to the second embodiment of the present invention , and the reproducing processing is performed according to a control algorithm stored in the EPROM 23 shown in FIG .", "label": "", "metadata": {}, "score": "56.139854"}
{"text": "[ 0034 ] Moreover , an attribute can be defined inside the start tag .In the example in FIG .4 , for an element ' a ' , an attribute is defined in the start tag .That is , the element ' a ' is linked as hypertext .", "label": "", "metadata": {}, "score": "56.16331"}
{"text": "As shown in FIG .4 , it is preferred that the data processing apparatus ( called a second data processing apparatus ) further comprise an auxiliary dictionary buffer 22C for storing the particular dictionary data or the particular dictionary data string expelled from the dictionary buffer 22B. Therefore , the number of dictionary data or dictionary data strings to be referred can be substantially increased .", "label": "", "metadata": {}, "score": "56.196327"}
{"text": "To accelerate the process , we apply a C implementation of elastic net regulation proposed by Mairal et al .[ 30 ] .We also normalize the target patch to make it more efficient for data processing .Experiment and Discussion .", "label": "", "metadata": {}, "score": "56.204895"}
{"text": "It shows that the method can improve the detection performance of evaluation objects through numerical experiments .Introduction .Recently , various kinds of sequential data are easily and cheaply collected from real world and virtual world .It is anticipated that the data includes the knowledge that brings smart life to us .", "label": "", "metadata": {}, "score": "56.20699"}
{"text": "Reverse bit - tree decoding instead decodes from least significant bit to most significant bits , and thus only supports ranges that are powers of two , and always decodes the same number of bits .It is equivalent to performing non - reverse bittree decoding with a power of two limit , and reversing the last log2 ( limit ) bits of the result .", "label": "", "metadata": {}, "score": "56.23167"}
{"text": "We demonstrate this e ect by presenting an empirical investigation of information retrieval on Chinese TREC data , using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44 % to 95 % .It appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters , while they are broken up by less accurate ( but reasonable ) segmenters , to a surprising advantage .", "label": "", "metadata": {}, "score": "56.361977"}
{"text": "Hence , there is a causality issue , as the exact distribution may not be known a priori .Moreover , the Huffman alphabet designed for a given source may perform poorly if used to compress another source with a different distribution .", "label": "", "metadata": {}, "score": "56.40207"}
{"text": "C .i . )N .K . )i . w .i . refers to the support vector initially obtained for the i -th patch , while .w .t .i .The K - combined voting can be seen as an efficient hierarchical generalization form of single voting .", "label": "", "metadata": {}, "score": "56.43112"}
{"text": "20(A ) is an explanatory view of two types of compressed data format in which a piece of positional information is added ; .FIG .20(B ) is an explanatory view of a code tree used to determine a series of code bits composing the compressed data ; .", "label": "", "metadata": {}, "score": "56.433346"}
{"text": "0004 ] 2 .Description of the Related Art .[ 0005 ] A web browser is an application program that is installed in a client terminal and acquires a resource from a web server having a specified URL ( Uniform Resource Locator ) to display it on a display device of the client terminal .", "label": "", "metadata": {}, "score": "56.51908"}
{"text": "[ 0015 ]According to the present invention , it is possible to extract and display a pre - registered keyword by changing a DOM node with respect to the keyword on a webpage .In addition , by defining an event handler in a tag of the new DOM node , it is possible to display information related to that keyword .", "label": "", "metadata": {}, "score": "56.521664"}
{"text": "We operate over . \" ...For developing a data - driven text rewriting algorithm for paraphrasing , it is essential to have a monolingual corpus of aligned paraphrased sentences .News article headlines are a rich source of paraphrases ; they tend to describe the same event in various different ways , and can easily be obtained f ... \" .", "label": "", "metadata": {}, "score": "56.537888"}
{"text": "In addition to the POC tags assigned to the characters , the merging component incorporates a number of linguistic and statistical heuristics to detect words with regular internal structures , recognize long words , and filter non - words .Experiments show that , without resorting to a separate unknown word identification mechanism , the model achieves an F - score of 95.0 % for word segmentation and a competitive recall of 74.8 % for unknown word recognition . .", "label": "", "metadata": {}, "score": "56.657818"}
{"text": "Thus , the patch - based SVMs are grouped for classifier modeling .As an elegant working model , sparse representation has recently been extensively studied and applied in pattern recognition and computer vision [ 23 , 24 ] .There are two basic problems [ 25 ] : the first one is to calculate the sparse solution of a linear system , while the second one refers to learning a suitable dictionary for approximation performance improvement .", "label": "", "metadata": {}, "score": "56.65805"}
{"text": "In our future work , we will try to improve detection performance .For example , we are planning to reconsider the transformation method of evaluation objects based on the topic dictionary .This is because the method can not identify original evaluation objects included in texts with the ones added by the topic dictionary .", "label": "", "metadata": {}, "score": "56.65912"}
{"text": "FIGS .4 and 5 , meaning token dictionary 92 maps a pronunciation 96 to a meaning token 20 having a dictionary spelling that signifies a single meaning ( or thought or idea ) .Meaning token 20 may have the same pronunciation as a vocabulary word in a conventional speech recognition dictionary , but the dictionary spelling that is associated with that vocabulary word is selected to signify a single meaning rather than a simple transcription of the words that were spoken .", "label": "", "metadata": {}, "score": "56.702003"}
{"text": "The content is then inserted into a balloon type display pane as information related to the keyword , and displayed on the webpage in the form of an overlay .The overlay on the webpage is performed in the form of inserting a DOM node that is displayed as an overlay over an existing webpage .", "label": "", "metadata": {}, "score": "56.72972"}
{"text": "In a fourth coding level of the Table 5 , one of four types of identification codes is allocated to each of 256 data samples .In detail , an identification code \" 00 \" is allocated to 2 data samples to identify pieces of positional information placed in positions ranging from a head ( or a zeroth ) position to a second position , and each of the positional information has 1 bits .", "label": "", "metadata": {}, "score": "56.760235"}
{"text": "Therefore , an offset coding for reconstructing pieces of compressed data can be performed by dynamically level - adjusting the lengths of the compressed data according to the types of the input data ( an eighth data processing method ) .That is , types of the input data to be compressed are one - sided in dependence on the types of the input data .", "label": "", "metadata": {}, "score": "56.867477"}
{"text": "It can be found that our proposed tracker could perform more favorably than the other state - of - the - art trackers comprehensively in both qualitative and quantitative evaluation .We present some justifications here .For discriminative tracking algorithms , discrimination of the target from the background is critical .", "label": "", "metadata": {}, "score": "56.880245"}
{"text": "It is possible to classify the keywords that are registered in the dictionary 42 according to category when registering the keywords .Categories for which the user desires to display related information can be set on an initial setup screen of the plugin 34 of the web browser 22 .", "label": "", "metadata": {}, "score": "56.922523"}
{"text": "In practice , to appropriately adjust the data compression by considering a type of the compressed data , it is required to set a range of the neighborhood agreement , or it is required to use properly a plurality of types of positional information .", "label": "", "metadata": {}, "score": "56.9247"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Reinhardt , D. Christin , M. Hollick , J. Schmitt , P. S. Mogre , and R. Steinmetz , \" Trimming the tree : tailoring adaptive Huffman coding to wireless sensor networks , \" Wireless Sensor Networks , vol .", "label": "", "metadata": {}, "score": "56.964905"}
{"text": "The parser 32 analyzes a structure of the acquired HTML document .At this time , the user agent 31 calls up the plugin 34 to execute a keyword display method according to one embodiment of the present invention in parallel with processing by the parser 32 and renderer 33 .", "label": "", "metadata": {}, "score": "56.98127"}
{"text": "claim 12 , wherein each meaning token is characterized by a unique spelling .The method of . claim 13 , wherein the spelling of a meaning token facilitates extraction of meaning by a language analyzer .The method of .claim 14 , wherein the spelling of a meaning token encodes the one or more labels identifying one or more respective application - specific categories .", "label": "", "metadata": {}, "score": "57.081932"}
{"text": "FIGS .16(A ) and 16(B ) are explanatory views of a fifth data compressing method according to a fifth embodiment of the present invention .A different point of the fifth embodiment from the first to fourth embodiments is that an input data string to be compressed written in an input memory region successively connected to a dictionary memory region is regarded as a dictionary data string .", "label": "", "metadata": {}, "score": "57.18171"}
{"text": "Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions . \" ...This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .", "label": "", "metadata": {}, "score": "57.207546"}
{"text": "Language analyzer 14 attempts to make sense of the meaning token sequences 20 that are output by speech recognizer 12 .That is , language analyzer 14 attempts to extract from each meaning token sequence a meaning 22 that is relevant to a specific user application .", "label": "", "metadata": {}, "score": "57.288414"}
{"text": "7(C ) is an explanatory view of the dictionary buffer and the input buffer after the data search ; .FIG .8 is a flow chart showing the data compression according to the first embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "57.292458"}
{"text": "9(A ) shows two data formats of two pieces of compressed data in which a piece of positional information is added ; .FIG .9(B ) shows a coding tree utilized to form the compressed data ; .FIG .9(C ) shows a relationship between a length of the compressed data and a bit string ; .", "label": "", "metadata": {}, "score": "57.32488"}
{"text": "13 are stored .In the third embodiment , a data compressing algorithm shown in FIG .14(B ) and a data reproducing algorithm shown in FIG .14(C ) are stored .Concrete contents of each of the control algorithms are described in each of the embodiments .", "label": "", "metadata": {}, "score": "57.40126"}
{"text": "The distance encoding starts with a 6-bit \" distance slot \" , which determines how many further bits are needed .No complete natural language specification of the compressed format seems to exist , other than the one attempted in the following text .", "label": "", "metadata": {}, "score": "57.437683"}
{"text": "The method assigns evaluation objects to the text sequential data by activating a topic dictionary .The dictionary describes keywords representing the numerical change .It can expand the amount of the training data .It is anticipated that the expansion leads to the acquisition of more valid trend rules .", "label": "", "metadata": {}, "score": "57.459473"}
{"text": "Within the particle filtering framework , most of the works cast the tracking problem as searching the most likely sampling candidate of the target via l 1 minimization .Mei and Ling [ 11 ] apply sparse representation to visual tracking and deal with occlusions via positive and negative trivial templates .", "label": "", "metadata": {}, "score": "57.48713"}
{"text": "Abstract : .To display information that is related to a keyword described on a webpage .Claims : .The keyword display method according to claim 1 , wherein words stored in the pre - registered dictionary are registered according to category , and wherein said second step extracts a word as a keyword according to pre - specified categories .", "label": "", "metadata": {}, "score": "57.494423"}
{"text": "Experiment Setup .The proposed tracking algorithm , IDSDL - VT , is implemented in MATLAB and C / C++ and runs at about 1.3 fps on a 3.4 GHz dual core PC with 8 GB of RAM .Moreover , the regularization constant , \u03bb 1 and \u03bb 2 , in Equation ( 2 ) are set to 0.01 , and the dictionary learning is processed once per frame .", "label": "", "metadata": {}, "score": "57.609123"}
{"text": "In such a case , a font size , color or the like of the newly inserted DOM node could differ from that of the existing DOM node , which could interfere with the display .As a result , when the markup process is performed , the keyword is displayed in an unintended different color ( in this case , the color red ) .", "label": "", "metadata": {}, "score": "57.638462"}
{"text": "This work explores computing distributional similarity between sub - parses , i.e. , fragments of a parse tree , as an extension to general lexical distributional similarity techniques .In the same way that lexical distributional similarity is used to estimate lexical semantic similarity , we propose using distributional similarity between subparses to estimate the semantic similarity of phrases .", "label": "", "metadata": {}, "score": "57.64791"}
{"text": "ALFC assumes that the measurements are transmitted in fixed - length packets which contain enough information so that the measurements within the packet can be decoded regardless of communication failures that may have caused previous packet losses .In order to achieve comparable results , we modify our approach so that measurements are also assumed to be contained in fixed - length packets , and the first measurement in each packet is not compressed .", "label": "", "metadata": {}, "score": "57.68994"}
{"text": "For example , in case where a piece of next input data subsequent to an operation code is a jump instruction having an address , an occurrence probability of a next byte is averaged .A type of a piece of data is judged from an extending code of a file .", "label": "", "metadata": {}, "score": "57.704735"}
{"text": "To be concrete , an initial value of a counter in which an occurrence frequency of an input code corresponding to a piece of input data having a high occurrence frequency is counted is set to one or a value higher than one .", "label": "", "metadata": {}, "score": "57.816193"}
{"text": "For example , in the first embodiment , a data compressing algorithm shown in FIG . 8 and a data reproducing algorithm shown in FIG .10 are stored .In the second embodiment , an agreement dictionary producing algorithm shown in FIG .", "label": "", "metadata": {}, "score": "57.82695"}
{"text": "A Tracking Framework Based on IDSDL and K - Combined Voting SVM Classification .The Principle of Online Visual Tracking Based on Bayesian Inference .An online visual tracking problem can be interpreted as a Bayesian recursive and sequential inference task in a Markov model with hidden state variables and is further divided into cascaded estimation of a dynamical model and observation model [ 3 ] .", "label": "", "metadata": {}, "score": "57.842"}
{"text": "Also , an identification code \" 011 \" is allocated to 32 data samples to identify pieces of positional information placed in positions ranging from a thirty - second position to a sixty - third position , and each of the positional information has 5 bits .", "label": "", "metadata": {}, "score": "57.860374"}
{"text": "The extraction is equal to the one of the learning phase .The subprocess generates an evaluation transaction composed of the evaluation objects and the attributes .Next , the subprocess 2 evaluates whether a trend rule matches the evaluation transaction .", "label": "", "metadata": {}, "score": "57.914406"}
{"text": "The percentage of symbols outside the dictionary is at most . , which would still allow the proposed method to perform quite well .It is interesting to notice that the highest percentages of symbols not present occur exactly when sets which contain a very limited number of symbols are used to generate the Huffman dictionaries ( i.e. , Sets 4 , 5 , and 8) .", "label": "", "metadata": {}, "score": "57.954613"}
{"text": "Therefore , the dictionary data or the dictionary data string expelled from the dictionary buffer 22B can be efficiently utilized , and a data compressing in a moving window utilizing the dictionary buffer 22B and the auxiliary dictionary buffer 22C together can be performed .", "label": "", "metadata": {}, "score": "57.97431"}
{"text": "Again , one must take into consideration the simplicity of the proposed method .Table 4 : Percentage of symbols not present in the dictionaries generated by the different datasets .Table 4 shows the percentage of symbols present in each dataset which can not be represented by the dictionary generated using the dataset on the top row .", "label": "", "metadata": {}, "score": "58.022133"}
{"text": "What many of these methods have in common is the fact that they make use of the correlation of the data acquired by the sensor nodes in order to achieve high compression ratios while employing computationally inexpensive algorithms .However , WSNs are generally deployed with the purpose of monitoring a particular phenomenon of interest [ 12 ] .", "label": "", "metadata": {}, "score": "58.053314"}
{"text": "Also , an identification code \" 011 \" is allocated to 32 data samples to identify pieces of positional information placed in positions ranging from a twenty - eighth position to a fifth - ninth position , and each of the positional information has 5 bits .", "label": "", "metadata": {}, "score": "58.069405"}
{"text": "9(B ) is considered .FIG .9(C ) shows a relationship between a length of the compressed data composed of a code tree and a corresponding bit string .As shown in FIG .9(C ) , these bit strings denote 4-byte agreement , 5-byte agreement , 6-byte agreement , 7-byte agreement , 8-byte agreement and 9-byte agreement .", "label": "", "metadata": {}, "score": "58.078888"}
{"text": "In Section 2 , we briefly explain some of the recent contributions on data compression for WSNs .We then describe the proposed lightweight compression method in Section 3 .In Section 4 , we show the results obtained with our approach and compare them with those obtained when employing the methods in [ 2 - 4 ] .", "label": "", "metadata": {}, "score": "58.123516"}
{"text": "It can also identify new words through MI - based token merging and dictionary updating .In addition , with the proposed Improved Bigram method IASeg can process N - grams .To evaluate the Systems and methods of automatic speech recognition are described .", "label": "", "metadata": {}, "score": "58.147636"}
{"text": "These tasks are usually conducted separately in other systems .Finally , we do not assume the existence of a universal word segmentation standard that is application - independent .Instead , we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different natural language processing applications might require different granularities of Chinese words .", "label": "", "metadata": {}, "score": "58.180767"}
{"text": "Therefore , a piece of original data is reproduced by the compressed data decoded .For example , a plurality of dictionary data stored in the dictionary buffer 22B are compared with pieces of input data to be decoded written in the input buffer 22A one after another under the control of the CPU 24 .", "label": "", "metadata": {}, "score": "58.181564"}
{"text": "FIG .11(B ) shows a plurality of data formats in which a piece of positional information is added to each of pieces of compressed data in case where a memory capacity of the agreement dictionary buffer 22C is 2 KB .", "label": "", "metadata": {}, "score": "58.203537"}
{"text": "claim 22 , wherein the plural different spoken words include different phrases of words .Description .TECHNICAL FIELD .This invention relates to a meaning token dictionary for automatic speech recognition applications , including task - oriented speech recognition applications .", "label": "", "metadata": {}, "score": "58.24964"}
{"text": "Even in the case of the datasets for which the performance of the proposed scheme does not approach the theoretical limit ( notably in datasets of extremely low entropy , such as Sets 8 and 9 ) , the reduction in symbol length is almost .", "label": "", "metadata": {}, "score": "58.25927"}
{"text": "That is , this paper contributes to the data mining research field with the following three viewpoints .Firstly , it incorporates a new function expanding learning data based on the topic dictionary into the previous discovery method of trend rules .", "label": "", "metadata": {}, "score": "58.36936"}
{"text": "These tasks are usually conducted separately in other systems .Finally , we do not assume the existence of a universal word segmentation standard which is application independent .Instead , we argue for the necessity of multiple segmentation standards due to the pragmatic fact that different NLP applications might require different granularities of Chinese words .", "label": "", "metadata": {}, "score": "58.377007"}
{"text": "Shift both range and code left by 8 bits .Read a byte from the compressed stream .Set the least significant 8 bits of code to the byte value read .Context - based range decoding of a bit using the prob probability variable proceeds in this way : .", "label": "", "metadata": {}, "score": "58.589752"}
{"text": "Also , it is preferred that the step of counting an occurrence frequency comprise the step of renewing the occurrence frequency table and the code table each time a piece of input data to be compressed is input ( called a seventh data processing method ) .", "label": "", "metadata": {}, "score": "58.598312"}
{"text": "That is , the keyword markup module 41 lists the DOM nodes using , for example , ' getElementsByTagName ' , ' createTreeWalker ' , or the like that is defined as DOM API .[ 0042 ] Next , the keyword markup module 41 extracts text from the listed DOM nodes ( S528 ) .", "label": "", "metadata": {}, "score": "58.620842"}
{"text": "Otherwise ( if code is greater or equal than range ): .Set code to code - range .Return bit 1 .Note that : .The division by 2 ^ 11 when computing bound and floor operation is done before the multiplication , not after ( apparently to avoid requiring fast hardware support for 32-bit multiplication with a 64-bit result ) .", "label": "", "metadata": {}, "score": "58.62956"}
{"text": "Our evaluation is established as shown in Figure 12 .We assume that the cameras are connected by the local area network ( LAN ) with the computing server in the back - end .The videos acquired would be transmitted to the server without any time delay .", "label": "", "metadata": {}, "score": "58.70128"}
{"text": "A piece of original data read out from the original data file 1 is written in the input buffer 2A of the data converting apparatus 2 as an input data string Din .Thereafter , the input data string Din written in the input buffer 2A and a dictionary data string stored in the dictionary buffer 2B are compared with each other under the control of the CPU 2C to perform a data search .", "label": "", "metadata": {}, "score": "58.713448"}
{"text": "9 in detail .Thereafter , the coded compressed data obtained by coding the dictionary data string is stored in the compressed data file 27 in a step P7 .Thereafter , a piece of coded input data string is moved to the dictionary buffer 22B in a step P8 .", "label": "", "metadata": {}, "score": "58.7454"}
{"text": "10 is a flow chart showing a data reproduction performed according to the first embodiment ; .FIG .11(A ) is an explanatory view of an agreement dictionary buffer arranged in a second data processing apparatus according to a second embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "58.773857"}
{"text": "0002 ] 1 .Field of the Invention .[ 0003 ] The present invention relates to a keyword display method and a keyword display system , and more particularly to a keyword display method and a keyword display system that provide information related to a keyword that is described on a webpage displayed by a web browser .", "label": "", "metadata": {}, "score": "58.808907"}
{"text": "We also illustrate its relay application in visual sensor networks .Keywords : . appearance model ; object tracking ; sparse representation ; structured dictionary learning ; Bayesian inference ; visual sensor networks .Introduction .Object tracking via video sensors is an important subject and has long been investigated in the computer vision community .", "label": "", "metadata": {}, "score": "58.87546"}
{"text": "On the other hand , the proposed method can not identify original evaluation objects directly described in news headlines with the ones based on the topic dictionary .The prediction phase may be able to extract more valid evaluation objects if different kinds of evaluation objects are identified and their difference is reflected on the prediction phase .", "label": "", "metadata": {}, "score": "58.976204"}
{"text": "2.4 Evaluation Performance of Chinese word segmenters is generally reported in terms of precision and recall .However , a comparison across systems cou ... . \" ...This paper describes a hybrid model that combines machine learning with linguistic heuristics for integrating unknown word identification with Chinese word segmentation .", "label": "", "metadata": {}, "score": "59.002235"}
{"text": "Historically , visual trackers proposed in the early years typically kept the appearance model fixed throughout an image sequence .Recently , methods proposed to track targets while evolving the appearance model in an online manner , called online visual tracking , have been popular [ 2 ] .", "label": "", "metadata": {}, "score": "59.00641"}
{"text": "Results using temperature and relative humidity measurements show that even when the proposed method does not approach the theoretical limit , it outperforms popular compression mechanisms designed specifically for wireless sensor networks .Introduction .Since data communication is generally the main factor responsible for draining the energy reserves of the network , techniques to reduce the amount of information transmitted by the sensor nodes are of great interest .", "label": "", "metadata": {}, "score": "59.118034"}
{"text": "The subprocess 3 identifies a class by referring to the change ratio of each evaluation object included in the text .Here , each class has a respective range of change ratios .The combination of evaluation objects and attributes extracted from the text is a training transaction with a class .", "label": "", "metadata": {}, "score": "59.13903"}
{"text": "4 is a constitutional view of a data processing apparatus representing a data compressing apparatus and a data reproducing apparatus according to a plurality of embodiments of the present invention .FIGS .7(A ) to 7(C ) are explanatory views of a dictionary buffer and an input buffer shown in FIG .", "label": "", "metadata": {}, "score": "59.147808"}
{"text": "Language analyzer 14 identifies permissible meanings based upon the semantic rules , and outputs summary structures representing permissible sentences .Application command translator 16 matches the summary structures with one of a set of application - specific actions 26 , and carries out the selected action by issuing one or more commands 28 to a downstream user application for processing .", "label": "", "metadata": {}, "score": "59.209812"}
{"text": "making each of the compressed data by combining one of the pieces of positional information and the identification code .Description .This application is a division of application Ser .No .08/505,760 , filed Jul. 21 , 1995 , now U.S. Pat .", "label": "", "metadata": {}, "score": "59.26258"}
{"text": "The system of .claim 7 , further comprising an application command translator configured to select an action from a set of application - specific actions based upon the meaning extracted by the language analyzer , and to issue one or more commands to carry out the selected action .", "label": "", "metadata": {}, "score": "59.3179"}
{"text": "Also , a word \" u \" subsequent to a word \" q \" frequently occurs .Therefore , a connection relationship between an objective character and a front or rear character of the objective character and another connection relationship between a front character and a rear character in a word can be grasped by obtaining those pieces of statistical information .", "label": "", "metadata": {}, "score": "59.34589"}
{"text": "A structured dictionary is defined as .u . j .A .j .j .B .t . ) j . )D .t .i .A . t . ) j . )D .t .", "label": "", "metadata": {}, "score": "59.352833"}
{"text": "12DZ2272600 .We also give our sincere thanks to the anonymous reviewers for their comments and suggestions .Conflicts of Interest .The authors declare no conflicts of interest .References .Yilmaz , A. ; Javed , O. ; Shah , M. Object tracking : A survey .", "label": "", "metadata": {}, "score": "59.383514"}
{"text": "Figure 1 : Probability distributions of the differences between consecutive measurements for each of the temperature datasets .Thus , in this paper we propose to construct a fixed alphabet obtained by the application of the Huffman algorithm to a large dataset of temperature measurements .", "label": "", "metadata": {}, "score": "59.56515"}
{"text": "In case where the reference code exists ( YES ) , the control algorithm proceeds to a step P2 , one or more pieces of dictionary data of the agreement dictionary buffer 22C are referred , and a piece of input data to be decoded is decoded .", "label": "", "metadata": {}, "score": "59.5979"}
{"text": "1 : state reset .2 : state reset , properties reset using properties byte .3 : state reset , properties reset using properties byte , dictionary reset .LZMA state resets cause a reset of all LZMA state except the dictionary , and specifically : .", "label": "", "metadata": {}, "score": "59.680923"}
{"text": "It is be shown that MIL [ 9 ] , L1 T [ 11 ] and PLS [ 18 ] do not perform well in Caviar 1 .The first two methods fail to discover the target when the target is occluded by a similar object ( e.g. , # 0130 ) , while the latter one drifts away from the target ( e.g. , # 0130 and # 0218 ) .", "label": "", "metadata": {}, "score": "59.737167"}
{"text": "Babenko , B. ; Yang , M.H. ; Belongie , S. Robust object tracking with online multiple instance learning .IEEE Trans .Pattern Anal .Mach .Intel .[ Google Scholar ] .Kalal , Z. ; Mikolajczyk , K. ; Matas , J. Tracking - learning - detection .", "label": "", "metadata": {}, "score": "59.787"}
{"text": "[21 ] present a tracking system based on an ensemble of adaptively - weighted linear SVM classifiers based on their discriminative abilities .Bai and Tang et al .[ 22 ] propose an online Laplacian ranking support vector tracker ( LRSVT ) , which incorporates the weakly labeled information to resist full occlusion and adapt to target appearance variation .", "label": "", "metadata": {}, "score": "59.85314"}
{"text": "In the same manner , an identification code \" 1 \" is allocated to 255 data samples to identify pieces of positional information placed in positions ranging from a first position to a two hundred fifty - fifth position , and each of the positional information has 8 bits .", "label": "", "metadata": {}, "score": "59.90232"}
{"text": "It is also possible to add , in an electronic shopping mall , a link to a webpage describing the definition of the term \" Valid Pixels \" as hypertext .However , by providing an explanation screen for each term listed in the product specifications , a volume of the webpage is increase , so that it is not possible to add links to all terms .", "label": "", "metadata": {}, "score": "59.93728"}
{"text": "For each packet size , the average symbol length was computed over all packets .The results show that the average symbol lengths . obtained using the proposed approach are lower than those obtained using ALFC for any dataset or packet length .", "label": "", "metadata": {}, "score": "60.023716"}
{"text": "Corresponding to the patch settings above , the selected learned dictionaries of sequence Occlusion 1 and David Indoor after 100 frames are shown in Figure 11 to demonstrate the proposed dictionary design and learning results .The values have been normalized before plotting for better illustration .", "label": "", "metadata": {}, "score": "60.032486"}
{"text": "Furthermore , in the context of data aggregation , that is , when nodes along routing paths collaborate to reduce the dimensionality of the data collected by multiple nodes , one recent and extremely promising technique is compressed sensing [ 20 ] .", "label": "", "metadata": {}, "score": "60.052048"}
{"text": "Due to the deletion , the method can avoid such risk that important patterns are hidden in the large amount of patterns .In the case of Figure 4 , the two former patterns are deleted because they include two companies which are evaluation objects , respectively .", "label": "", "metadata": {}, "score": "60.064316"}
{"text": "claim 1 , wherein the speech recognition dictionary is a data structure stored in a computer - readable physical medium .The system of .claim 1 , wherein the plural different spoken words include different phrases of words .An automatic speech recognition method , comprising : . converting spoken input into a sequence of meaning tokens contained in a speech recognition dictionary and corresponding to a sequence of vocabulary words most likely to have been spoken by a user , .", "label": "", "metadata": {}, "score": "60.07458"}
{"text": "For the Hansard corpus , we took the human annotation of word alignment ... . \" ...We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .", "label": "", "metadata": {}, "score": "60.17724"}
{"text": "Sec - ond , it combines a simplification model for splitting and deletion with a monolin - gual translation model for phrase substi - tution and reordering .When compared against current state of the art methods , our model yields significantly simpler out - put that is both grammatical and meaning preserving . \" ...", "label": "", "metadata": {}, "score": "60.37674"}
{"text": "The data processing method according to claim 7 in which the step of counting an occurrence frequency comprises the step of renewing the occurrence frequency table and the code table each time a piece of input data to be compressed is input .", "label": "", "metadata": {}, "score": "60.39781"}
{"text": "In the fourth data processing apparatus according to the present invention , the dictionary buffer 22B is formed in a ring - shaped structure .Therefore , the data compressing operation can be performed while referring the dictionary data written in the memory region formed in a no - end loop shape and while sliding an extended moving window in the dictionary buffer 22B to search the dictionary data or the dictionary data strings .", "label": "", "metadata": {}, "score": "60.428932"}
{"text": "It can be additionally anticipated that more valid identification is performed .Experiment .This section explains experiments verifying the effect of the proposed method .That is , the experimental data , the topic dictionary , the evaluation criteria , the experimental method , and the experimental results are explained in order .", "label": "", "metadata": {}, "score": "60.512867"}
{"text": "( 1 )There is a probability that the same piece of dictionary data is undesirably stored in duplicate in the dictionary buffer 2B and a data compressing efficiency is lowered .Therefore , as shown in FIG .2(C ) , the data string \" abc \" remains in duplicate in the dictionary buffer 2B after the data string \" abc \" is coded .", "label": "", "metadata": {}, "score": "60.51606"}
{"text": "In a third coding level of the Table 5 , one of the five types of identification codes is allocated to each of 256 data samples , in the same manner as in the first and second coding levels .In detail , an identification code \" 000 \" is allocated to 2 data samples to identify pieces of positional information placed in positions ranging from a head ( or a zeroth ) position to a first position , and each of the positional information has 1 bits .", "label": "", "metadata": {}, "score": "60.523636"}
{"text": "K. Sayood , Introduction to Data Compression , Morgan Kaufman , 3rd edition , 2005 . Y. Liang and W. Peng , \" Minimizing energy consumptions in wireless sensor networks via two - modal transmission , \" SIGCOMM Computer Communication Review , vol .", "label": "", "metadata": {}, "score": "60.574802"}
{"text": "References .T. Srisooksai , K. Keamarungsi , P. Lamsrichan , and K. Araki , \" Practical data compression in wireless sensor networks : a survey , \" Journal of Network and Computer Applications , vol .35 , no . 1 , pp .", "label": "", "metadata": {}, "score": "60.59378"}
{"text": "[5 ] .LZMA2 is a simple container format that can include both uncompressed data and LZMA data , possibly with multiple different LZMA encoding parameters .LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible .", "label": "", "metadata": {}, "score": "60.645294"}
{"text": "Firstly , most of them consider the classification problem on a single - patch level , which might lack flexibility and robustness when a drastic appearance occurs .Secondly , the features applied in these works are not unique enough .It could negatively influence the tracking performance when a similar object exists .", "label": "", "metadata": {}, "score": "60.648586"}
{"text": "That is , it is judged whether or not a bit length of the compressed data is longer than that of the input data having an 8-bit length .In case where a bit length of the compressed data produced by the code converting editor 43 is equal to or lower than 8 bits ( NO ) , the control algorithm proceeds to a step P35 .", "label": "", "metadata": {}, "score": "60.65973"}
{"text": "12 to describe the operation of the second data processing apparatus .FIG .12 is a flow chart showing the generation of the agreement dictionary according to the second embodiment of the present invention , and the second data processing method is performed according to a control algorithm stored in the EPROM 23 shown in FIG .", "label": "", "metadata": {}, "score": "60.659805"}
{"text": "11(B ) , 128 pieces of dictionary data are stored in the agreement dictionary buffer 22C. When a searching operation is performed , a position placed 128 bits behind the storage pointer currently placed is searched .Accordingly , the number of dictionary data is substantially increased , and a data compression utilizing a moving window while using the dictionary buffer 22B and the agreement dictionary buffer 22C together can be performed .", "label": "", "metadata": {}, "score": "60.710052"}
{"text": "We present a hybrid approach to sentence simplification which combines deep se - mantics and monolingual machine transla - tion to derive simple sentences from com - plex ones .The approach differs from pre - vious work in two main ways .", "label": "", "metadata": {}, "score": "60.73783"}
{"text": "We present a hybrid approach to sentence simplification which combines deep se - mantics and monolingual machine transla - tion to derive simple sentences from com - plex ones .The approach differs from pre - vious work in two main ways .", "label": "", "metadata": {}, "score": "60.73783"}
{"text": "The proposed approach can be applied to other environmental datasets .In order to demonstrate that , we consider a set of relative humidity measurements , whose characteristics are listed in Table 5 .Figure 6 shows the probability distribution of the differences between consecutive relative humidity measurements for each of the datasets .", "label": "", "metadata": {}, "score": "60.797485"}
{"text": "If the word is one of the proper nouns , it is regarded as an evaluation object .Otherwise , it is regarded as an attribute .The subprocess 2 calculates a change ratio for the evaluation object described in the text by referring to its numerical sequence .", "label": "", "metadata": {}, "score": "60.852844"}
{"text": "FIG .1 , in one embodiment , an automatic speech recognition system 10 includes a speech recognizer 12 , a language analyzer 14 , and an application command translator 16 .In operation , speech recognizer 12 converts an incoming stream of speech sounds 18 ( or utterances ) into a likely sequence of meaning tokens 20 .", "label": "", "metadata": {}, "score": "60.86705"}
{"text": "In a fifth coding level of the Table 5 , one of three types of identification codes is allocated to each of 256 data samples .In detail , only an identification code \" 00 \" is allocated to a data sample to identify a head ( or a zeroth ) position .", "label": "", "metadata": {}, "score": "60.993797"}
{"text": "25 .As is described in the sixth embodiment , the coding processing is performed for each of bytes or characters .Also , one of the coding levels shown in Table 5 is selected by the code converting editor 43 according to the ununiformity of a distribution of the occurrence frequency produced , and the compressed data is produced by applying a selected coding level .", "label": "", "metadata": {}, "score": "61.040092"}
{"text": "A bit width of the positional information having the identification code \" 010 \" is set to two bits , and the positional information having the identification code \" 010 \" is expressed by \" 00 \" , \" 01\",\"10 \" or \" 11 \" .", "label": "", "metadata": {}, "score": "61.073246"}
{"text": "Soc .Ser .B 2005 , 67 , 301 - 320 .[ Google Scholar ] .Mairal , J. ; Bach , F. ; Ponce , J. ; Sapiro , G. Online learning for matrix factorization and sparse coding .", "label": "", "metadata": {}, "score": "61.080166"}
{"text": "On the other hand , even in the case where a unique tag is written in the acquired HTML document , the web browser is able to recognize the tag as a DOM node and properly display it as part of the webpage .", "label": "", "metadata": {}, "score": "61.087154"}
{"text": "On the other hand , it is anticipated that the increase of training data realizes more valid inductive learning .It is important for the learning of trend rules to increase training data .This section proposes a method that activates a text not directly describing evaluation objects .", "label": "", "metadata": {}, "score": "61.093887"}
{"text": "Figure 8 shows that the number of extracted evaluation objects increases .Particularly , ' ' Topic ' ' extracts more evaluation transactions than the other methods do .This is because many evaluation objects are assigned to evaluation transactions by the topic dictionary .", "label": "", "metadata": {}, "score": "61.109665"}
{"text": "Parallel encoding is performed by dividing the file in chunks which are distributed to threads , and ultimately each encoded ( using , for instance , xz block encoding ) separately , resulting in a dictionary reset between chunks in the output file .", "label": "", "metadata": {}, "score": "61.16137"}
{"text": "We note that a training transaction is not generated when the text does not include an evaluation object .The subprocess 4 applies the set of training transactions with the same class to the discovery method of frequent patterns [ 1 , 17 ] and discovers frequent patterns for each class .", "label": "", "metadata": {}, "score": "61.16523"}
{"text": "Most existing Chinese word segmentation approaches are either statistics - based or dictionary - based .The pure statistical method has lower precision , while the pure dictionary - based method can not deal with new words beyond the dictionary .In this paper , we propose a hybrid method that is able to avoid the limitations of both types of approaches .", "label": "", "metadata": {}, "score": "61.284336"}
{"text": "In the arithmetic code section 17 , the SOR code is arithmetic - coded to produce a piece of multi - valued code data .In this case , a value of a sign bit and other values of upper and lower bits in the multi - valued code data are determined according to count values of an occurrence frequency and an accumulated frequency of each of the character strings in the arithmetic coding section 17 .", "label": "", "metadata": {}, "score": "61.321594"}
{"text": "That is , in case where the occurrence frequencies of the input data to be compressed are definitely distributed , the input data can be converted into the compressed data respectively having a piece of shorter positional information .Also , in case where the occurrence frequencies of the input data to be compressed are one - sidedly distributed , the input data can be converted into the compressed data respectively having a piece of shorter positional information by rising the selected coding level .", "label": "", "metadata": {}, "score": "61.37777"}
{"text": "Algorithm 1 shows a pseudocode for this phase .In Algorithm 1 , text sequential data ( .Prediction of Attractive Evaluation Objects .The prediction phase has 4 subprocesses as shown in Figure 2 .The prediction phase deals with the acquired trend rules and the text sequential data collected in the designated period .", "label": "", "metadata": {}, "score": "61.443474"}
{"text": "Int .J. Comput .Vis .[ Google Scholar ] .Adam , A. ; Rivlin , E. ; Shimshoni , I. Robust Fragments - Based Tracking Using the Integral Histogram .Proceedings of 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR 2006 ) , New York , NY , USA , 17 - 22 June 2006 ; pp .", "label": "", "metadata": {}, "score": "61.49777"}
{"text": "15(A ) is a constitutional view of a ring - shaped dictionary buffer of the fourth data processing apparatus , FIGS . 15(B ) and 15(C ) respectively show a searching condition view of the ring - shaped dictionary buffer .A different point of the fourth embodiment from the first and third embodiments is that the dictionary buffer 22B is formed in a ring - shaped structure .", "label": "", "metadata": {}, "score": "61.50144"}
{"text": "In contrast , in case where the selected coding level is not equal to the highest coding level ( NO ) , the control algorithm proceeds to a step P36 , and a pseudo - level adjustment is performed to adopt an appropriate coding level .", "label": "", "metadata": {}, "score": "61.518803"}
{"text": "Fixed probability integer decoding simply performs fixed probability bit decoding repeatedly , reading bits from the most to the least significant .The LZMA decoder is configured by an lclppb \" properties \" byte and a dictionary size .lc is the number of high bits of the previous byte to use as a context for literal encoding ( the default value used by the LZMA SDK is 3 ) .", "label": "", "metadata": {}, "score": "61.54561"}
{"text": "Accordingly , the input data to be compressed can be coded according to the ununiformity of an occurrence frequency distribution to shorten the bit lengths of the compressed data , and the compressed data can be stored in the compressed data file 51 .", "label": "", "metadata": {}, "score": "61.610245"}
{"text": "Unlike LEC , which always uses the same alphabet , our approach uses a reference dataset to generate a dictionary for a particular parameter under observation ( e.g. , temperature ) .We compute the frequencies of each of the symbols available in the reference dataset and use them to construct the Huffman tree that represents the compression alphabet [ 23 ] .", "label": "", "metadata": {}, "score": "61.701805"}
{"text": "It could not track the target until its foreground is re - detected again , and due to the variation of foreground area , the target is not correctly labeled in some frames .Comparatively , our proposed tracker only relies on the foreground information once for location initialization , and with the dictionary sharing across the network , it achieves a better performance .", "label": "", "metadata": {}, "score": "61.77315"}
{"text": "Even though many of the time - honored compression algorithms could be executed in modern wireless sensor nodes , they would leave few resources available for the nodes to carry out other tasks such as sensing and communication .More importantly , these nodes would have significantly fewer opportunities to enter deep sleep modes and attain the energy efficiency that motivated the use of a compression algorithm in the first place .", "label": "", "metadata": {}, "score": "61.80877"}
{"text": "In case where the bit length of the tentative compressed data is shorter than the bit length of the compressed data ( YES ) , the control algorithm proceeds to a step P38 , and the selected coding level is risen .", "label": "", "metadata": {}, "score": "61.852623"}
{"text": "The aim is to understand whether a complex - simple parallel corpus involv - ing this version of Wikipedia is appropri - ate as data source to induce simplifica - tion rules , and w ... \" .We present a study on the text simplifica - tion operations undertaken collaboratively by Simple English Wikipedia contribu - tors .", "label": "", "metadata": {}, "score": "61.883923"}
{"text": "claim 1 , wherein each meaning token is characterized by a unique spelling .The system of . claim 2 , wherein the spelling of a meaning token facilitates extraction of meaning by a language analyzer .The system of .claim 3 , wherein the spelling of a meaning token encodes the one or more labels identifying one or more respective application - specific categories .", "label": "", "metadata": {}, "score": "61.885376"}
{"text": "0012 ]The third step creates a new DOM node that includes the extracted keyword , and replaces the keyword in the HTML document therewith .Then , the third step defines an event handler in a tag of the new DOM node in order to display another webpage that has information related to the keyword .", "label": "", "metadata": {}, "score": "61.896996"}
{"text": "Also , an identification code \" 011 \" is allocated to 16 data samples to identify pieces of positional information placed in positions ranging from a fourteenth position to a twenty - ninth position , and each of the positional information has 4 bits .", "label": "", "metadata": {}, "score": "61.905178"}
{"text": "[20 ] develop a hybrid tracking method , where a sparsity - based discriminative classifier ( SDC ) and a sparsity - based generative model ( SGM ) are cascaded for target location estimation .However , investigation of the second problem in visual tracking has just started .", "label": "", "metadata": {}, "score": "61.922134"}
{"text": "Zhang , G. ; Jiang , Z. ; Davis , L.S. Online semi - supervised discriminative dictionary learning for sparse representation .Lect .Notes Comput .Sci .[ Google Scholar ] .Zou , H. ; Hastie , T. Regularization and variable selection via the elastic net .", "label": "", "metadata": {}, "score": "62.027042"}
{"text": "It is preferred that the step of converting the types of pieces of input data comprise the steps of : . rearranging pieces of data of the occurrence frequencies arranged in the occurrence frequency table in order of degree of occurrence frequency ; . defining a plurality of data positions ranging from a highest data position for a piece of data of a highest occurrence frequency to a lowest data position for a piece of data of a lowest occurrence frequency in the occurrence frequency table ; . allocating a pieces of shorter positional information to a data position in which a piece of data of an occurrence frequency is arranged as the occurrence frequency is higher ; . allocating a piece of longer positional information to a data position in which a piece of data of an occurrence frequency is arranged as the occurrence frequency is lower ; and .", "label": "", "metadata": {}, "score": "62.034714"}
{"text": "16(A ) shows a memory region divided into the dictionary buffer and the input buffer by a boundary line ; .FIG .16(B ) shows an extended dictionary buffer and the input buffer overlapping with the extended dictionary buffer obtained by moving the boundary line toward the input buffer in a data compressing method according to a fifth embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "62.045033"}
{"text": "In the fixed dictionary buffer 22D , one or more pieces of fixed data or one or more fixed data strings respectively having a high occurrence frequency are written as pieces of dictionary data .The fixed data or the fixed data string are selected from the input data to be compressed by examining occurrence frequencies of the input data in advance .", "label": "", "metadata": {}, "score": "62.085888"}
{"text": "Also , an identification code \" 010 \" is allocated to 16 data samples to identify pieces of positional information placed in positions ranging from a twelfth position to a twenty - seventh position , and each of the positional information has 4 bits .", "label": "", "metadata": {}, "score": "62.156418"}
{"text": "However , for Chinese , we nd that the relationship between segmentation and retrieval performance is in fact nonmonotonic ; that is , at around 70 % word segmentation accuracy an over - segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance .", "label": "", "metadata": {}, "score": "62.164883"}
{"text": "The finite state machine templates are defined by a set of vocabulary meaning token patterns , which are stored in a meaning token dictionary .In some embodiments , the finite state machine templates also are defined by a set of grammar rules .", "label": "", "metadata": {}, "score": "62.187233"}
{"text": "..y this involves segmenting the text into individual words . by Xiangji Huang , Fuchun Peng , Dale Schuurmans , Nick Cercone , Stephen Robertson , 2002 . \" ...We propose a self - supervised word segmentation technique for text segmentation in Chinese information retrieval .", "label": "", "metadata": {}, "score": "62.23514"}
{"text": "the values of the 4 last used distances after the packets in the solution ( computed only after the best subproblem solution has been determined ) .state .the LZMA state value after the packets in the solution ( computed only after the best subproblem solution has been determined ) .", "label": "", "metadata": {}, "score": "62.283035"}
{"text": "Therefore , 13 bits including a bit for the identification code \" 1 \" are required .Because 2 bytes ( that is , 16 bits ) are required when the input data \" r , e \" does not agree with any piece of dictionary data , the agreement data \" r , e \" is compressed by 3 bits ( 16 bits-13 bits ) .", "label": "", "metadata": {}, "score": "62.29516"}
{"text": "Also , the lower the occurrence frequency of the input data DIN , the higher the value of the code indicating the converted data DT .Therefore , the input data DIN can be coded to the coded data DOUT in the first apparatus .", "label": "", "metadata": {}, "score": "62.427345"}
{"text": "Thirdly , it applies the revised method to the prediction task in the financial field and verifies its effect .Thus , the remaining parts of this paper are composed of the followings .The second section introduces some related works in the financial field .", "label": "", "metadata": {}, "score": "62.50465"}
{"text": "Spaces are inserted into positions where their presence enables the text to be compressed more effectively .This approach means that we can capitalize on existing research in text compression to create good models for word segmentation .To build a segmenter for a new language , the only resource required is a corpus of segmented text to train the compression model ... . by", "label": "", "metadata": {}, "score": "62.54053"}
{"text": "We think that the activation of the topic dictionary contributes to the improvement of the detection performance even if the precisions more or less deteriorate .According to the above discussions , we believe that the proposed expansion method can acquire more valid trend rules .", "label": "", "metadata": {}, "score": "62.684086"}
{"text": "Accordingly , the data searching speed can be heightened as compared with that in the first prior art , the data compression processing can be performed at a high speed .Here , because a generating process of the agreement dictionary buffer 22C is not required in the third embodiment as compared with that in the second embodiment , the data processing speed can be heightened .", "label": "", "metadata": {}, "score": "62.686356"}
{"text": "The motivation for constructing such a system stems from the ... \" .This paper presents a novel method that allows a machine learning algorithm following the transformation - based learning paradigm ( Brill , 1995 ) to be applied to multiple classication tasks by training jointly and simultaneously on all elds . by Fuchun Peng , Xiangji Huang , Dale Schuurmans , Nick Cercone - Retrieval Performance in Chinese IR , Coling2002 , 2002 . \" ...", "label": "", "metadata": {}, "score": "62.686485"}
{"text": "Learn .Res .[ Google Scholar ] .Fan , R.E. ; Chang , K.W. ; Hsieh , C.J. ; Wang , X.R. ; Lin , C.J. LIBLINEAR :A library for large linear classification .J. Mach .Learn .", "label": "", "metadata": {}, "score": "62.690254"}
{"text": "The length encoded using 8 bits , gives the lengths range from 18 to 273 .As in LZ77 , the length is not limited by the distance , because copying from the dictionary is defined as if the copy was performed byte by byte , keeping the distance constant .", "label": "", "metadata": {}, "score": "62.69497"}
{"text": "In a second coding level of the Table 5 , one of the five types of identification codes is allocated to each of 256 data samples , in the same manner as in the first coding level .In detail , an identification code \" 000 \" is allocated to 4 data samples to identify pieces of positional information placed in positions ranging from a head ( or a zeroth ) position to a third position , and each of the positional information has 2 bits .", "label": "", "metadata": {}, "score": "62.70042"}
{"text": "Also , an identification code \" 11 \" is allocated to 128 data samples to identify pieces of positional information placed in positions ranging from a one hundred twenty - eighth position to two hundred fifty - fifth position , and each of the positional information has 7 bits .", "label": "", "metadata": {}, "score": "62.72634"}
{"text": "One domain which would highly benefit from tailoring summaries to both individual and class - based user characteristics is the medical domain , where physicians and patients access similar information , each with their own need ... \" .In this thesis , we present a user - sensitive approach to text summarization .", "label": "", "metadata": {}, "score": "62.74128"}
{"text": "This paper proposed a new method for the expansion of training data .The expansion can lead to the discovery of more valid trend rules from complex sequential data .The method was applied to the prediction task of attractive stock brands in the next period .", "label": "", "metadata": {}, "score": "62.750553"}
{"text": "after compression using ALFC and our proposed approach .Despite the current popularity of the 802.15.4 protocol , we did not want to constrain our analysis exclusively to protocols that only allow very small payloads ; hence we show the results for packet sizes of . bytes .", "label": "", "metadata": {}, "score": "62.796425"}
{"text": "7(A ) is an explanatory view of the data search of pieces of data in a dictionary buffer and an input buffer performed for a data compression according to a first embodiment of the present invention ; .FIG .7(B ) an explanatory view showing a piece of particular dictionary data expelled and a piece of particular input data written in the dictionary buffer ; .", "label": "", "metadata": {}, "score": "62.804623"}
{"text": "Thereafter , a code table is produced from the occurrence frequency table in a step P2 , in the same manner as in the sixth embodiment .Thereafter , an offset coding processing in which bit lengths of pieces of compressed data obtained by coding the input data are adjusted by adjusting a coding level applied to produce the compressed data with the level adjusting editor 44 in a step P3 .", "label": "", "metadata": {}, "score": "62.846832"}
{"text": "The stream is divided into packets , each packet describing either a single byte , or an LZ77 sequence with its length and distance implicitly or explicitly encoded .Each part of each packet is modeled with independent contexts , so the probability predictions for each bit are correlated with the values of that bit ( and related bits from the same field ) in previous packets of the same type .", "label": "", "metadata": {}, "score": "62.881287"}
{"text": "That is , even though the number of the dictionary data is increased , a memory capacity of the dictionary buffer 22B itself is not increased , and the number of coded bits obtained after the data compression is not increased .", "label": "", "metadata": {}, "score": "62.899857"}
{"text": "Thereafter , the control algorithm is finished without any generation of the agreement dictionary .In case where the agreement data portion exists ( YES ) , it is judged in a step P3 whether or not the agreement of the data portion is the 2-byte agreement .", "label": "", "metadata": {}, "score": "62.977848"}
{"text": "12 - 18 , 2010 .View at Google Scholar . D. Huffman , \" A method for the construction of minimum - redundancy codes , \" Proceedings of the IRE , vol .40 , no . 9 , pp .", "label": "", "metadata": {}, "score": "62.997635"}
{"text": "Lightweight Compression of Environmental Data .In this section , we define the problem of data compression in WSNs and present a simple compression approach , which takes into consideration the characteristics of the measurements acquired by the sensor nodes so that algorithmic complexity can be reduced without sacrificing compression ratios .", "label": "", "metadata": {}, "score": "63.00736"}
{"text": "Many evaluation objects can be simultaneously and respectively analyzed .Attractive evaluation objects are extracted from them .However , the method can not sufficiently explain the reason why the evaluation objects are selected as attractive ones .Thus , our another previous research [ 6 ] tackles the development of the prediction method which can represent the reason .", "label": "", "metadata": {}, "score": "63.024696"}
{"text": "The data compressing method is disclosed in a literature : Lempel Abraham and Ziv Jacob , \" A Universal Algorithm for Sequential Data Compression \" , IEEE Transaction on Information Theory , 1977 .The Lempel - Ziv coding method is improved in a Lempel - Ziv - Storer - Szymanski ( LZSS ) coding method by adding two types of alternation to the Lempel - Ziv coding method .", "label": "", "metadata": {}, "score": "63.08364"}
{"text": "Thereafter , in a step P4 of the main routine , it is judged whether or not all of the input data to be compressed stored in the input data file 50 are coded .In case where all of the input data DIN to be compressed are coded ( YES ) , the control algorithm is finished .", "label": "", "metadata": {}, "score": "63.113388"}
{"text": "T refers to the frame number of the test sequence .The results for each sequence and each method are shown in Table 1 , and it can be concluded that the proposed tracking method performs more favorably than the other methods and the single - voting case .", "label": "", "metadata": {}, "score": "63.23934"}
{"text": "Related Work .In the literature on compression methods for WSNs both lossy and lossless approaches that exploit the high temporal correlation of the sensor node data can be found .One of the first lossy methods for data compression in WSN , lightweight temporal compression ( LTC ) [ 5 ] , approximates the data collected by each sensor node in a WSN by a set of lines .", "label": "", "metadata": {}, "score": "63.27182"}
{"text": "The rules can be displayed as the reason .Also , they are used in order to predict attractive evaluation objects .The method can predict attractive evaluation objects to some extent .However , their detection performance should be revised for the more valid prediction .", "label": "", "metadata": {}, "score": "63.274734"}
{"text": "Also , in case where the bit length of the compressed data is equal to or lower than 8 bits in the step P33 , it is judged in the step P35 whether or not the selected coding level is equal to the highest coding level .", "label": "", "metadata": {}, "score": "63.308414"}
{"text": "It focuses on the expansion of the training data because many machine learning researches show that the expansion brings about better learning results .This paper activates a topic dictionary for the expansion .The dictionary describes relationships between evaluation objects and keywords related to numerical changes .", "label": "", "metadata": {}, "score": "63.379967"}
{"text": "11(A ) , a shaded portion indicates a searching range for searching the dictionary data of the agreement dictionary buffer 22C. The searching range is limited by the coding of the input data to be compressed .In the second embodiment of the present invention , a data file for storing pieces of data to be compressed on condition that the 2-byte agreement occurs at the most occurrence frequency is assumed .", "label": "", "metadata": {}, "score": "63.419495"}
{"text": "Fixed probability range decoding instead assumes a 0.5 probability , but operates slightly differently from context - based range decoding .The range decoder state consists of two unsigned 32-bit variables , range ( representing the range size ) , and code ( representing the encoded point within the range ) .", "label": "", "metadata": {}, "score": "63.420723"}
{"text": "Suitable processors include , for example , both general and special purpose microprocessors .Generally , a processor receives instructions and data from a read - only memory and/or a random access memory .Any of the foregoing technologies may be supplemented by or incorporated in specially designed ASICs ( application - specific integrated circuits ) .", "label": "", "metadata": {}, "score": "63.43692"}
{"text": "Each trend rule represents a relationship among evaluation objects , keywords , and numerical changes .The method applies the trend rules to text sequential data collected in the designated period and predicts attractive evaluation objects in the next period .It regards evaluation objects whose trends change as attractive evaluation objects .", "label": "", "metadata": {}, "score": "63.49319"}
{"text": "We apply foreground extraction based on Gaussian background modeling [ 33 ] to detect the newly appeared person in the boundary area ( 5 % of the frame height and width in this paper ) .When the foreground area is larger than a predefined threshold , the person is considered to be detected .", "label": "", "metadata": {}, "score": "63.542297"}
{"text": "0007 ] The most characteristic thing when browsing a webpage using a web browser is that it is possible to browse different webpages one after the other by following hypertext links .More specifically , in a document written using HTML , it is possible to define text ( hypertext ) that is correlated ( also referred to as \" linked \" ) with a URL .", "label": "", "metadata": {}, "score": "63.586945"}
{"text": "On test data extracted by the heuristic strategy , however , performance of the two training sets is similar , with AERs of 13.2 % and 14.7 % respectively .Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase .", "label": "", "metadata": {}, "score": "63.69523"}
{"text": "( first embodiment)dictionary data of ring - shaped 4522 bytes 1373 bytesdictionary buffer 22E arecyclically searched .( fourth embodiment)dictionary data of extended 4495 bytes 1369 bytesdictionary buffer 22B ' aresearched .( fifth embodiment ) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "63.69752"}
{"text": "For Occlusion 2 , the differences are more obvious .PLS [ 18 ] can not continuously follow the target , while MIL [ 9 ] and Frag [ 4 ] estimate the target less accurately than the proposed algorithm .Both of them comprise severe partial occlusion and scale variation from far to near , which are typical scenes in surveillance applications .", "label": "", "metadata": {}, "score": "63.722557"}
{"text": "1838 - 1845 .Tian , M. ; Zhang , W. ; Liu , F. On - line ensemble SVM for robust object tracking .Lect .Notes Comput .Sci .[ Google Scholar ] .Bai , Y. ; Tang , M. Robust Tracking via Weakly Supervised Ranking SVM .", "label": "", "metadata": {}, "score": "63.82653"}
{"text": "The pos_state and literal_pos_state values consist of respectively the pb and lp ( up to 4 , from the LZMA header or LZMA2 properties packet ) least significant bits of the dictionary position ( the number of bytes coded since the last dictionary reset modulo the dictionary size ) .", "label": "", "metadata": {}, "score": "63.908485"}
{"text": "6 , the second object is also achieved by the provision of a data processing method ( called a sixth data processing method ) , comprising the steps of : . counting an occurrence frequency of each of types of pieces of input data to be compressed in advance and producing an occurrence frequency table formed of a plurality of occurrence frequencies of the types of the input data to be compressed in a step P1 ; . predicting an occurrence of a piece of next input data subsequent to a piece of input data to be compressed in a step P2 while referring the occurrence frequency table produced ; and .", "label": "", "metadata": {}, "score": "63.919327"}
{"text": "Confidence is also important for observation estimation .Our proposed KCV voting method combines the classifiers randomly and outputs their estimated result by a maximal scheme .Since the candidates are also generated randomly , the random combination could also be viewed as a supplementary re - sampling step from the particle filtering aspect .", "label": "", "metadata": {}, "score": "63.967667"}
{"text": "Accordingly , the dictionary data or the dictionary data strings differing from each other can be always stored in the dictionary buffer 22B , and the redundancy of the dictionary data or the dictionary data strings of the dictionary buffer 22B can be reduced .", "label": "", "metadata": {}, "score": "63.9961"}
{"text": "3(B ) is a constitutional view of a second conventional data compressing apparatus according to the second previously proposed art ; .FIG .4 is a constitutional view of a data processing apparatus representing a data compressing apparatus and a data reproducing apparatus according to a plurality of embodiments of the present invention ; .", "label": "", "metadata": {}, "score": "64.019135"}
{"text": "The proposed tracker fails to continuously track the target when the target patch is normalized to 8 \u00d7 8 .This is because the details of the target are lost when the target region is interpolated on a more coarse - grained scale , and thus , the discrimination ability could not be satisfactorily maintained .", "label": "", "metadata": {}, "score": "64.031746"}
{"text": "Also , an identification code \" 10 \" is allocated to 64 data samples to identify pieces of positional information placed in positions ranging from a sixty - fourth position to one hundred twenty - seventh position , and each of the positional information has 6 bits .", "label": "", "metadata": {}, "score": "64.0912"}
{"text": "1(B ) , a coded input data string is formed within a certain memory range and is stored in a dictionary buffer 2B. The dictionary data stored in the dictionary buffer 2B is transferred to the compressed data file 3 without remaining the stored dictionary data as dictionary contents obtained after the data compressing .", "label": "", "metadata": {}, "score": "64.11086"}
{"text": "11(B ) shows a plurality of data formats of pieces of compressed data in which a piece of positional information is added to each of the compressed data in case where a memory capacity of the agreement dictionary buffer is 2 KB ; .", "label": "", "metadata": {}, "score": "64.16643"}
{"text": "FIG .8 is a flow chart showing the data compression according to a first embodiment of the present invention .FIGS .9(A ) to 9(C ) are explanatory views of a coding processing performed for a piece of agreement data in case of a data compression according to each of the embodiments .", "label": "", "metadata": {}, "score": "64.273865"}
{"text": "Some existing methods [ 7 - 9 , 12 ] analyze relationships between a specific numerical sequence and texts .The numerical sequence is the synthetic stock indexes such as DJIA or the change of specific currency exchange .The other existing methods [ 10 , 11 , 13 , 14 ] analyze relationships between texts related to limited stock brands and their numerical sequences .", "label": "", "metadata": {}, "score": "64.31633"}
{"text": "In case where all of the input data DIN are compressed ( YES ) , the control algorithm is finished .In case where all of the input data DIN are not compressed ( NO ) , the control algorithm returns to the step P3 , and the coding processing is repeatedly performed .", "label": "", "metadata": {}, "score": "64.410194"}
{"text": "As Figure 1 shows , the distribution of the measurement differences in Set 1 can be obtained by sampling a Laplacian distribution with mean 0 and scale 1 at integer points .The dictionary in Table 2 can then be generated by constructing a Huffman tree based on the probabilities of the points .", "label": "", "metadata": {}, "score": "64.41349"}
{"text": "In case where the agreement data portion does not exist in the step P2 ( NO ) , the procedure proceeds to a step P3 , and the fixed dictionary data of the fixed dictionary buffer 22D are searched .When a piece of particular fixed dictionary data agrees with a piece of particular input data to be coded , the particular fixed dictionary data is coded and stored in the compressed data file 27 .", "label": "", "metadata": {}, "score": "64.42627"}
{"text": "Its support is larger than or equal to the minimum support .Lastly , in the subprocess 5 , a class assigned to the set is combined with discovered frequent patterns .The combination is a trend rule .The subprocess 4 and the subprocess 5 generate trend rules .", "label": "", "metadata": {}, "score": "64.448456"}
{"text": "Moreover , ' getComputedValue ' that acquires a CSS style attribute is defined as DOM API .[ 0051 ] Furthermore , in the case of ( 3 ) above , the interference is avoided by instructing the web browser to force the inheritance by specifying \" inherit \" for an attribute that is likely to interfere with the display .", "label": "", "metadata": {}, "score": "64.52924"}
{"text": "The data search is performed from a head position of the dictionary data string stored in the dictionary buffer 2B , and a longest agreement data string agreeing with a piece of input data is found out in the dictionary buffer 2B. Thereafter , the input data of the input buffer 2A is transferred to the dictionary buffer 2B just after the input data agrees with the dictionary data .", "label": "", "metadata": {}, "score": "64.53698"}
{"text": "The data processing method according to claim 7 in which the step of counting an occurrence frequency comprises the step of producing the occurrence frequency table according to all pieces of input data to be compressed or a piece of input data to be compressed .", "label": "", "metadata": {}, "score": "64.60071"}
{"text": "Therefore , pieces of data stored in the dictionary buffer 22B are set to zero .Thereafter , an original data string is read out from the original data file 21 and is stored in the input buffer 22A in a step P2 .", "label": "", "metadata": {}, "score": "64.62463"}
{"text": "A bit width of the positional information having the identification code \" 1 \" is set to eight bits , and the positional information having the identification code \" 1 \" is expressed by a plurality of coded bit strings \" 00000000 \" to \" 11111111 \" .", "label": "", "metadata": {}, "score": "64.67147"}
{"text": "Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition , Ft .Collins , CO , USA , 23 - 25 June 1999 ; Volume 2 , pp .246 - 252 .Figure 1 .Workflow of the proposed algorithm .", "label": "", "metadata": {}, "score": "64.75964"}
{"text": "For example , pieces of input data respectively having an occurrence frequency written in the occurrence frequency table are compared with the next input data subsequent to the input data DIN to be compressed in the data comparing editor 42 .Therefore , the occurrence of the character \" n \" subsequent to the character \" a \" in the word \" and \" and the occurrence of the character \" u \" subsequent to the character \" q \" are predicted .", "label": "", "metadata": {}, "score": "64.78043"}
{"text": "8 , competition is avoided by specifying ' inherit ' as described below for a ' color ' attribute .[0054 ]In addition , in the CSS definition described above , the priority sequence used during the display is set depending on how the selector is selected .", "label": "", "metadata": {}, "score": "64.78152"}
{"text": "After receiving a request from a user , the user agent performs communication with a web server having a specified URL using , for example , HTTP ( Hyper Text Transfer Protocol ) to send the user request .In addition , the user agent acquires a resource corresponding to the user request from the web server .", "label": "", "metadata": {}, "score": "64.91747"}
{"text": "Also , an identification code \" 1 \" is allocated to 253 data samples to identify pieces of positional information placed in positions ranging from a third position to a two hundred fifty - fifth position , and each of the positional information has 8 bits .", "label": "", "metadata": {}, "score": "64.92758"}
{"text": "Also , an identification code \" 010 \" is allocated to 8 data samples to identify pieces of positional information placed in positions ranging from a sixth position to a thirteenth position , and each of the positional information has 3 bits .", "label": "", "metadata": {}, "score": "64.93026"}
{"text": "claim 12 , wherein the plural different spoken words include different phrases of words .A computer program for automatically recognizing speech , the computer program residing on a computer - readable medium and comprising computer - readable instructions for causing a computer to : . convert spoken input into a sequence of meaning tokens contained in a speech recognition dictionary and corresponding to a sequence of vocabulary words most likely to have been spoken by a user , .", "label": "", "metadata": {}, "score": "64.9413"}
{"text": "Also , an identification code \" 1 \" is allocated to 196 data samples to identify pieces of positional information placed in positions ranging from a sixtieth position to a two hundred fifty - fifth position , and each of the positional information has 8 bits .", "label": "", "metadata": {}, "score": "64.95383"}
{"text": "It uses Chasen [ 16 ] , which is one of representative Japanese morphological analysis engines .The engine separates a text into words and assigns their parts of speech to them .Noun words are extracted from the analyzed texts .", "label": "", "metadata": {}, "score": "65.080475"}
{"text": "v . )p .X .t .v .X .t . q .X .t .X .t .Y .t . )To avoid degeneration , the samples would be re - sampled according to the their corresponding importance weights to generate a set of equally - weighted particles .", "label": "", "metadata": {}, "score": "65.153786"}
{"text": "l .c .N .c .N .e . where N c is the frame numbers where the target is correctly tracked , and N e is the frame numbers where the target actually appears in the scene .", "label": "", "metadata": {}, "score": "65.23174"}
{"text": "Therefore , there is a problem that the data processing can not be performed at a high speed , in the same manner as in the first apparatus .Here , in a data compressing apparatus disclosed in a Published Unexamined Japanese Patent Application No .", "label": "", "metadata": {}, "score": "65.239044"}
{"text": "1(B ) explanatorily shows an arrangement view of pieces of dictionary data and pieces of input data in a data searching step ; .FIG .1(C ) explanatorily shows another arrangement view of pieces of dictionary data and pieces of input data in a data expelling step ; .", "label": "", "metadata": {}, "score": "65.28051"}
{"text": "0009 ] On the other hand , for example , suppose , on a webpage that introduces a certain product such as digital cameras , the item \" Valid Pixels \" and a numerical value indicating the number of pixels are displayed in the product specifications .", "label": "", "metadata": {}, "score": "65.30473"}
{"text": "In this paper we show that , for Chinese , the relationship between segmentation and retrieval performance is in fact nonmonotonic ; that is , at around 70 % word se ... \" .It is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in Chinese information retrieval .", "label": "", "metadata": {}, "score": "65.32977"}
{"text": "It makes attempts to find a separating hyperplane that maximizes the margin between two classes .The margin is defined as the distance of the closest point to the hyperplane .J .w . ) min .w . k . w . k . )", "label": "", "metadata": {}, "score": "65.34282"}
{"text": "Thereafter , in a step P4 , the particular dictionary data string decoded is regarded as a piece of decoded original data , and the decoded original data is written in the original data file 21 .Thereafter , in a step P5 , the particular input data string to be decoded which agrees with the particular dictionary data string is decoded , and the particular decoded input data string is moved to the dictionary buffer 22B. Thereafter , in a step P6 , it is judged whether or not all of pieces of compressed data stored in the compressed data file 27 are decoded .", "label": "", "metadata": {}, "score": "65.53111"}
{"text": "Thus , according to Equation ( 16 ) , only the candidate most voted for is chosen as the estimation result .Model Update .Once the current target location is estimated , the model is updated accordingly .In this paper , the update process is two fold .", "label": "", "metadata": {}, "score": "65.61925"}
{"text": "Also , an identification code \" 1 \" is allocated to 196 data samples to identify pieces of positional information placed in positions ranging from a sixtieth position to a two hundred fifty - fifth position , and each of the positional information has 7 bits .", "label": "", "metadata": {}, "score": "65.633095"}
{"text": "2799- 2806 , 2012 .View at Google Scholar . H. Arjmandi , M. Taki , and F. Lahouti , \" Lifetime maximized data gathering in wireless sensor networks using limited - order distributed source coding , \" Signal Processing , vol .", "label": "", "metadata": {}, "score": "65.70738"}
{"text": "In a sixth coding level of the Table 5 , one of three types of identification codes is allocated to each of 256 data samples .In detail , only an identification code \" 0 \" is allocated to a data sample to identify a head ( or a zeroth ) position .", "label": "", "metadata": {}, "score": "65.72418"}
{"text": "Those probability variables are implemented as multi - dimensional arrays ; before introducing them , a few values that are used as indices in these multidimensional arrays are defined .The state value is conceptually based on which of the patterns in the following table match the latest 2 - 4 packet types seen , and is implemented as a state machine state updated according to the transition table listed in the table every time a packet is output .", "label": "", "metadata": {}, "score": "65.76786"}
{"text": "Such a reduction is due to the strong correlation between consecutive temperature samples , making the probability distribution of the differences strongly nonuniform .Thus , it is much more promising to consider the compression of the differences of consecutive temperatures than the compression of the temperatures themselves .", "label": "", "metadata": {}, "score": "65.82854"}
{"text": "In this way , as the electronic shopping mall , by constructing the webpage hierarchically along the flow of searching products to order , a user can easily order products by simply moving a cursor with a mouse and clicking on a mouse button .", "label": "", "metadata": {}, "score": "65.87637"}
{"text": "The data processing method according to claim 7 in which the step of converting the types of pieces of input data comprises the steps of : . rearranging pieces of data of the occurrence frequencies arranged in the occurrence frequency table in order of degree of occurrence frequency ; . defining a plurality of data positions ranging from a highest data position for a piece of data of a highest occurrence frequency to a lowest data position for a piece of data of a lowest occurrence frequency in the occurrence frequency table ; . allocating a piece of shorter positional information to a data position in which a piece of data of an occurrence frequency is arranged as the occurrence frequency is higher ; . allocating a piece of longer positional information to a data position in which a piece of data of an occurrence frequency is arranged as the occurrence frequency is lower ; and .", "label": "", "metadata": {}, "score": "65.96259"}
{"text": "Here , the data compression efficiency is defined as a ratio of an original data capacity to a compressed data capacity and is expressed by ( original data capacity)/(compressed data capacity ) v 100 % .When the agreement data found out according to the first embodiment of the present invention is converted into a piece of compressed data , a piece of positional information of the agreement data found out and a piece of length information of the agreement data found out are used .", "label": "", "metadata": {}, "score": "65.97746"}
{"text": "Distance is equal to the fourth last used LZ77 distance .The length is encoded as follows : .Length code ( bit sequence ) .Description . 0 + 3 bits .The length encoded using 3 bits , gives the lengths range from 2 to 9 . 1 + 0 + 3 bits .", "label": "", "metadata": {}, "score": "65.997"}
{"text": "We take LDC 's Chinese Gigaword ... \" .Abstract .This paper proposes a method to automatically classify texts from different varieties of the same language .We show that similarity measure is a robust tool for studying comparable corpora of language variations .", "label": "", "metadata": {}, "score": "66.04497"}
{"text": "The system of .claim 1 , further comprising a language analyzer configured to extract meaning from the sequence of meaning tokens provided by the speech recognizer based upon a set of task - specific semantic rules .The system of .", "label": "", "metadata": {}, "score": "66.061356"}
{"text": "Aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms , for tasks such as summarization or paraphrasing .Aligning sentences belonging to comparable monolingual corpora has been suggested as a first step towards training text rewriting algorithms , for tasks such as summarization or paraphrasing .", "label": "", "metadata": {}, "score": "66.09756"}
{"text": "In general , the process of automatically recognizing speech is made difficult by three primary obstacles .First , most words are short .With only a few sound features , a speech recognizer usually has difficulty in clearly distinguishing among similar sounding candidates .", "label": "", "metadata": {}, "score": "66.14258"}
{"text": "The LZMA state is reset only in the first block , if the caller requests a change of properties and every time a compressed chunk is output .The LZMA properties are changed only in the first block , or if the caller requests a change of properties .", "label": "", "metadata": {}, "score": "66.18617"}
{"text": "The supports of the patterns tend to be small .On the other hand , some experimental results show that good detection performance is not given when very few trend rules are used in the prediction phase .In near future , it may be necessary to consider the method that decides the number of trend rules for the good detection performance .", "label": "", "metadata": {}, "score": "66.201294"}
{"text": "Also , an identification code \" 1 \" is allocated to 248 data samples to identify pieces of positional information placed in positions ranging from an eighth position to a two hundred fifty - fifth position , and each of the positional information has 8 bits .", "label": "", "metadata": {}, "score": "66.21139"}
{"text": "In case where all of the compressed data are not decoded ( NO ) , the control algorithm returns to the step P2 , and another piece of compressed data is read out from the compressed data file 27 and is written in the input buffer 22A. Thereafter , the steps P3 to P6 are repeatedly performed .", "label": "", "metadata": {}, "score": "66.22039"}
{"text": "Our previous research [ 15 ] proposes a ranking method of evaluation objects in order to overcome this problem .The method deals with numerical sequential data for each evaluation object and text sequential data including the contents related to evaluation objects .", "label": "", "metadata": {}, "score": "66.22174"}
{"text": "A magnetic disk apparatus or a semiconductor memory apparatus is used for the compressed data file 27 in the same manner as in the original data file 21 .( 1 ) Description of a first embodiment .A first data compressing method shown in FIG .", "label": "", "metadata": {}, "score": "66.26872"}
{"text": "In the following description , like reference numbers are used to identify like elements .Furthermore , the drawings are intended to illustrate major features of exemplary embodiments in a diagrammatic manner .The drawings are not intended to depict every feature of actual embodiments nor relative dimensions of the depicted elements , and are not drawn to scale .", "label": "", "metadata": {}, "score": "66.45798"}
{"text": "( 4 ) Description of a fourth embodiment .FIGS .15(A ) to 15(C ) are explanatory views of a fourth data processing apparatus representing a fourth data compressing apparatus and a fourth data reproducing ( or decoding ) apparatus according to a fourth embodiment of the present invention .", "label": "", "metadata": {}, "score": "66.54313"}
{"text": "Overlap rate ( OR ) evaluation for nine video clips .The proposed algorithm is compared with seven state - of - the - art methods : Frag [ 4 ] , IVT [ 3 ] , VTD [ 16 ] , L1 T [ 11 ] , MIL [ 9 ] , TLD [ 10 ] and PLS [ 18 ] .", "label": "", "metadata": {}, "score": "66.54852"}
{"text": "Overlap rate ( OR ) evaluation for nine video clips .The proposed algorithm is compared with seven state - of - the - art methods : Frag [ 4 ] , IVT [ 3 ] , VTD [ 16 ] , L1 T [ 11 ] , MIL [ 9 ] , TLD [ 10 ] and PLS [ 18 ] .", "label": "", "metadata": {}, "score": "66.54852"}
{"text": "As a webpage , web browser , or plugin is developed , a DOM ( Document Object Model ) is defined as an application interface for analyzing and controlling an HTML document .The DOM handles an HTML document as a tree structure , and defines an \" element \" defined by the HTML document as a \" DOM node \" .", "label": "", "metadata": {}, "score": "66.62027"}
{"text": "29 , no . 2 , pp . 1 - 12 , 2000 .View at Google Scholar \u00b7 View at Scopus The Lempel - Ziv - Markov chain algorithm ( LZMA ) is an algorithm used to perform lossless data compression .", "label": "", "metadata": {}, "score": "66.66266"}
{"text": "Seo et al .[14 ] develop the intelligent multiagent system for the portfolio management .The system acquires a model classifying news articles related to financial information of companies into 5 classes .It classifies news articles and evaluates information collected by agents .", "label": "", "metadata": {}, "score": "66.68179"}
{"text": "The evaluation transaction is assigned to a class of the trend rule when the trend rule matches it .The evaluation is performed for each trend rule whose number of items is larger than or equal to the minimum number of items .", "label": "", "metadata": {}, "score": "66.79418"}
{"text": "In this ... \" .This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .In the nal evaluation , the statistical approach was found to perform best among ve competing approaches .", "label": "", "metadata": {}, "score": "66.80527"}
{"text": "Abstract .This paper presents a lightweight data compression method for wireless sensor networks monitoring environmental parameters with low resolution sensors .Instead of attempting to devise novel ad hoc algorithms , we show that , given general knowledge of the parameters that must be monitored , it is possible to efficiently employ conventional Huffman coding to represent the same parameter when measured at different locations and time periods .", "label": "", "metadata": {}, "score": "66.815315"}
{"text": "10 is a diagram illustrating a method for displaying information related to a keyword .DESCRIPTION OF THE EMBODIMENTS .[ 0027 ] Embodiments of the present invention will be described in detail below with reference to the drawings .First Embodiment .", "label": "", "metadata": {}, "score": "66.83333"}
{"text": "14(B ) is a flow chart showing the reference of the fixed dictionary in a data compressing operation performed according to a third data processing method ; .FIG .14(C ) is a flow chart showing the reference of the fixed dictionary in a data reproducing ( or decoding ) operation performed according to a third data processing method ; .", "label": "", "metadata": {}, "score": "66.995026"}
{"text": "for some of the test datasets ( Sets 2 , 3 , and 6 ) .Because that approach fails to account for longer - term variations in the measurements collected by the WSN , the improvement over LEC is at most .", "label": "", "metadata": {}, "score": "67.04973"}
{"text": "These challenges make the establishment of an efficient online visual tracker a difficult task .Related Works .Appearance representation of the target is a basic , but important , task for visual tracking .Discrimination capability , computational efficiency and occlusion resistance are generally considered as the three main aspects in appearance modeling .", "label": "", "metadata": {}, "score": "67.17413"}
{"text": "However , it is applicable that the occurrence frequency table be produced each time one of the input data DIN to be compressed is input to the occurrence frequency producing editor 41 .Thereafter , a code table is produced from the occurrence frequency table in a step P2 .", "label": "", "metadata": {}, "score": "67.25476"}
{"text": "Detection Performance .Figures 9 and 10 show that ' ' Topic ' ' gives precisions which are similar to ' ' Sakurai ' ' but ' ' Topic ' ' gives more or less smaller precisions than ' ' No_topic ' ' does .", "label": "", "metadata": {}, "score": "67.27566"}
{"text": "LZMA2 compression , which is an improved version of LZMA , [ 14 ] has been introduced in version 9.04 beta , of May 30 , 2009 .[ 15 ] .The reference open source LZMA compression library is written in C++ and has the following properties : .", "label": "", "metadata": {}, "score": "67.34085"}
{"text": "[0036 ]In order to display a document described in the HTML on a display device , a text size , font , layout , and the like must be set .In order to separate the structure and format of the document , attributes that influence the appearance like this are defined by CSS ( Cascading Style Sheet ) .", "label": "", "metadata": {}, "score": "67.43697"}
{"text": "Zhou , Y. ; Snoussi , H. ; Zheng , S. Bayesian variational human tracking based on informative body parts .Opt .Eng .[ Google Scholar ] .Zulkifley , M.A. ; Rawlinson , D. ; Moran , B. Robust observation detection for single object tracking : Deterministic and probabilistic patch - based approaches .", "label": "", "metadata": {}, "score": "67.43966"}
{"text": "24(B ) .The decoding of the compressed data is performed according to the configuration of the code tree shown in FIG .20(B ) .Thereafter , it is judged in a step P3 whether or not all of the compressed data stored in the compressed data file 51 are decoded .", "label": "", "metadata": {}, "score": "67.442795"}
{"text": "The 7 types of coding levels shown in Table 5 are stored in advance in the memory 45 , and pieces of input data DIN to be compressed are coded by the code converting editor 43 while referring the 7 types of coding levels .", "label": "", "metadata": {}, "score": "67.54256"}
{"text": "Conclusions .This paper proposes an online discriminative visual tracking algorithm based on incremental discriminative structured dictionary learning and multiple linear classification based on randomly - combined voting .Not only qualitative , but also quantitative , evaluations are conducted , which demonstrate that , on challenging image sequences , the proposed tracking algorithm enjoys better performance than the state - of - the - art algorithms .", "label": "", "metadata": {}, "score": "67.555145"}
{"text": "The agreement data strings expelled from the dictionary buffer 22B are stored in the agreement dictionary buffer 22C as a plurality of auxiliary dictionaries , and the agreement data strings of the agreement dictionary buffer 22C can be referred .That is , as the flow chart is shown in FIG .", "label": "", "metadata": {}, "score": "67.593765"}
{"text": "[ 0020 ] FIG .4 is a diagram illustrating an example of an HTML document that is a resource of the webpage ; . [ 0021 ] FIG .5 is a diagram illustrating a keyword display method according to a first embodiment of the present invention ; .", "label": "", "metadata": {}, "score": "67.65661"}
{"text": "Lastly , the subprocess 4 identifies whether each evaluation object is attractive by referring to its class counter .That is , if the maximum value of the class counters assigned to the evaluation object is larger than or equal to the minimum number of transactions , the evaluation object is attractive ones .", "label": "", "metadata": {}, "score": "67.71166"}
{"text": "It differs from most previous approaches mainly in three respects .First , while theoretical linguists have defined Chinese words using various linguistic criteria , Chinese words in this study are defined pragmatically as segmentation units whose definition depends on how they are used and processed in realistic computer applications .", "label": "", "metadata": {}, "score": "67.76663"}
{"text": "w . k .c . k . s .w . k . k .l . k . ) where w is termed a support vector , which are the training patterns closest to the separating hyperplane , and c refers to the regulation term .", "label": "", "metadata": {}, "score": "67.77733"}
{"text": "Return bit 1 .Fixed - probability range decoding of a bit proceeds in this way : .If range is less than 2 ^ 24 , perform normalization .Set range to floor ( range / 2 ) .If code is less than range : .", "label": "", "metadata": {}, "score": "68.08426"}
{"text": "It differentiates from most of the previous approaches mainly in three respects .First of all , while theoretical linguists have defined Chinese words with various linguistic criteria , Chinese words in this study are defined pragmatically as segmentation units whose definition depends on how they are used and processed in realistic computer applications .", "label": "", "metadata": {}, "score": "68.136536"}
{"text": "Thereafter , the input data DIN is shifted from the input buffer 22A to the dictionary buffer 22B. The dictionary data has an n byte length .Thereafter , the dictionary buffer 22B is searched in a step P3 to judge whether or not a dictionary data string agreeing with an input data string to be compressed exists in the dictionary buffer 22B. Thereafter , it is judged in a step P4 whether or not the dictionary data string agrees with the input data string to be compressed .", "label": "", "metadata": {}, "score": "68.191635"}
{"text": "In that case , an alternative method should be employed .In most WSN applications , however , that situation is rather uncommon .In the nine temperature measurement datasets presented in Table 1 , for example , there were no symbols that were not represented in the original dictionary shown in Table 2 .", "label": "", "metadata": {}, "score": "68.25992"}
{"text": "' We can confirm that the topic dictionary expands the training transactions .The topic dictionary gets the effect of the expansion .Discovered Trend Rules .Figures 6 and 7 show that the number of trend rules decreases in ' ' Topic . '", "label": "", "metadata": {}, "score": "68.279724"}
{"text": "For example , in the smart community field , Twitter messages , amount of electrical power consumption , and communities are text sequential data , numerical sequential data , and evaluation objects .In the healthcare field , nursing texts , test values of medical examination , and patients correspond to them .", "label": "", "metadata": {}, "score": "68.312355"}
{"text": "1(B ) , a longest agreement data string \" uimad \" is specified by an agreement starting position indicated by \" 2 \" in the dictionary buffer 2B and a largest length equal to 5 bytes .In the input buffer 2A , a next character \" f \" subsequent to the longest agreement data string \" uimad \" exists .", "label": "", "metadata": {}, "score": "68.33365"}
{"text": "That is , when a piece of input data of a value ranging from 0 to 127 is coded to a piece of compressed data , the short positional information is added to the compressed data .Therefore , the long positional information is not added .", "label": "", "metadata": {}, "score": "68.52446"}
{"text": "i . w .t .i .To both timely adapt the variation of target appearance and maintain its original invariance , a progressive classification is applied .The former one is introduced to locate the target as an intermediate result based on its latest appearance model , while the latter one is used to locally refine the location with respect to its originality .", "label": "", "metadata": {}, "score": "68.53041"}
{"text": "Typically , at time t , the geometric parametrization of the target region can be realized by an affine transformation as : . p .t .G . t .p .t .t .t .G .t .", "label": "", "metadata": {}, "score": "68.55896"}
{"text": "[ Google Scholar ] .Avidan , S. Support vector tracking .IEEE Trans .Pattern Anal .Mach .Intel .[ Google Scholar ] .Grabner , H. ; Bischof , H. On - line Boosting and Vision .Proceedings of 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition , New York , NY , USA , 17 - 22 June 2006 ; Volume 1 , pp .", "label": "", "metadata": {}, "score": "68.569534"}
{"text": "claim 18 , further comprising selecting an action from a set of application - specific actions based upon the extracted meaning .The method of . claim 19 , further comprising issuing one or more commands to carry out the selected action .", "label": "", "metadata": {}, "score": "68.63176"}
{"text": "3(A ) is a constitutional view of a first conventional data compressing apparatus according to a second previously proposed art , and FIG .3(B ) is a constitutional view of a second conventional data compressing apparatus according to the second previously proposed art .", "label": "", "metadata": {}, "score": "68.68437"}
{"text": "Therefore , each of 16 pieces of compressed data respectively composed of the identification code \" 00 \" and the positional information has 6 bits .In the same manner , an identification code \" 010 \" is allocated to 16 data samples to identify pieces of positional information placed in positions ranging from a sixteenth position to a thirty - first position , and each of the positional information has 4 bits .", "label": "", "metadata": {}, "score": "68.74187"}
{"text": "J. Luo , L. Xiang , and C. Rosenberg , \" Does compressed sensing improve the throughput of wireless sensor networks ? \" in Proceedings of the IEEE International Conference on Communications ( ICC ' 10 ) , pp . 1 - 6 , May 2010 .", "label": "", "metadata": {}, "score": "68.742485"}
{"text": "t .i . refer to the horizontal and vertical center coordinates of the evaluation and ground - truth labeling results at the i -th frame , respectively , and .A . eval .i .A . g .t .", "label": "", "metadata": {}, "score": "68.771904"}
{"text": "Stable and accurate tracking of objects is fundamental to many real - world applications , such as motion - based recognition , automated surveillance , visual sensor network , video indexing , human - computer interaction , traffic monitoring , vehicle navigation , etc .", "label": "", "metadata": {}, "score": "68.861725"}
{"text": "Quantitative Evaluation .Besides qualitative evaluation , quantitative evaluation of the tracking results is also an important issue for tracking performance evaluation .Similar to other classical works , two performance measurements are applied to compare the proposed tracker with the other reference trackers .", "label": "", "metadata": {}, "score": "69.048935"}
{"text": "claim 4 , wherein an application - specific category identified by a label encoded in the spelling of a meaning token is an object category , a place category , an event category , or an action category .The system of .", "label": "", "metadata": {}, "score": "69.16399"}
{"text": "17 is a constitutional view of a data compressing and reproducing ( or decoding ) apparatus according to sixth , seventh and eighth embodiments of the present invention ; .FIG .18(A ) is a flow chart of a data compressing operation according to the sixth embodiment ; .", "label": "", "metadata": {}, "score": "69.17781"}
{"text": "We think that trend rules have changed before and after the earthquake .In order to avoid this influence , the prediction period is decided .Table 2 shows the number of news headlines for each site .About a million news headlines are used to acquire trend rules and about 0.27 million ones are used to predict attractive stock brands .", "label": "", "metadata": {}, "score": "69.27268"}
{"text": "T .t .[ .t .t .t .t .t .t .M . ]d .N .M . , where M refers to the number of the templates , and the corresponding patches .", "label": "", "metadata": {}, "score": "69.28688"}
{"text": "In the case of drastic appearance variation , the random combined voting could provide more opportunities for the invariant patches to attend the voting calculation , so that it is more likely to obtain better results .Comparison between single voting and K - combined voting has been done as the proof in the next section .", "label": "", "metadata": {}, "score": "69.291885"}
{"text": "5 .A DOM node ( HTML element ) of an extracted keyword is rewritten , and the HTML document to which an event handler has been added is sent from the web server 13a to the client terminal 11 .Fourth Embodiment .", "label": "", "metadata": {}, "score": "69.33389"}
{"text": "Kwon , J. ; Lee , K.M. Visual Tracking Decomposition .Proceedings of 2010 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Francisco , CA , USA , 13 - 18 June 2010 ; pp .1269 - 1276 .", "label": "", "metadata": {}, "score": "69.340515"}
{"text": "Abstract As the amount of online Chinese contents grows , there is a critical need for effective Chinese word segmentation approaches to facilitate Web computing applications in a range of domains including terrorism informatics .Most existing Chinese word segmentation approaches are either statistic ... \" .", "label": "", "metadata": {}, "score": "69.4425"}
{"text": "Note that the first byte output will always be 0 due to the fact that cache and low are initialized to 0 , and the encoder implementation ; the xz decoder ignores this byte .Set cache to bits 24 - 31 of low .", "label": "", "metadata": {}, "score": "69.50431"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .[0017 ]FIG .1 is a diagram illustrating a server - client system according to one embodiment of the present invention ; .[ 0018 ] FIG .2 is a block diagram illustrating a configuration of a web browser according to one embodiment of the present invention ; . [ 0019 ] FIGS .", "label": "", "metadata": {}, "score": "69.52008"}
{"text": "This is because , due to the camera view point , the person 's initial pose in Camera 2 is much different from those in other cameras .Thus , the corresponding dictionaries learned in other cameras could not provide much effective information about the target .", "label": "", "metadata": {}, "score": "69.55263"}
{"text": "Since the temperature differences only assume integer values , the distributions are actually discrete and the continuous approximations are shown simply to facilitate the visualization .More importantly , note that for all datasets , if we list the differences from the most likely to the least likely , the result is .", "label": "", "metadata": {}, "score": "69.58176"}
{"text": "When types of the fixed data strings are judged , a method for judging from an expanding code of a file or another method in which an operator indicates types of the fixed data with the keyboard 25 is adopted .As for the fixed data strings respectively having a high occurrence frequency , for example , it is examined in advance with an auxiliary tool which of the fixed data strings occurs at a high or low occurrence frequency in the 2-byte agreement .", "label": "", "metadata": {}, "score": "69.69912"}
{"text": "Equations ( 7 ) and ( 8) sequentially update each column of the dictionary , while keeping the other ones fixed under a potential constraint .D .t .i . ) j . )T .D .t .", "label": "", "metadata": {}, "score": "69.78764"}
{"text": "Also , it is preferred that the occurrence frequency table be produced by the data producing means 31 of each of the fifth , sixth and seventh data processing apparatuses by fetching all pieces of input data to be compressed or a piece of input data to be compressed .", "label": "", "metadata": {}, "score": "69.87784"}
{"text": "A sixth data compressing method according to the sixth embodiment of the present invention is described with reference to FIG .18(A ) to describe a sixth data processing apparatus .The data compressing flow chart shown in FIG .18(A ) corresponds to a control algorithm stored in the EPROM 46 shown in FIG .", "label": "", "metadata": {}, "score": "69.87842"}
{"text": "16(A ) and 16(B ) , the dictionary control is performed by moving the boundary line 22F toward the input memory region when the dictionary data string is compared with the input data string to be compressed .Therefore , the dictionary buffer 22B is extended toward the input memory region to form an extended dictionary buffer 22B ' .", "label": "", "metadata": {}, "score": "69.89845"}
{"text": "In order to evaluate the method , we computed the compression ratio obtained in several real datasets containing temperature and relative humidity measurements collected at different locations and during distinct periods of time .The compression ratios obtained using our approach vary between .", "label": "", "metadata": {}, "score": "69.93055"}
{"text": "IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , Colorado Springs , CO , USA , 20 - 25 June 2011 ; pp .1305 - 1312 .Sun , J. A fast MEANSHIFT algorithm - based target tracking system .", "label": "", "metadata": {}, "score": "69.99078"}
{"text": "Also , one or more pieces of input data DIN to be decoded are temporarily stored when the input data DIN is decoded .The EPROM 46 is made of a programmable read only memory to store a plurality of control algorithms used in the embodiments .", "label": "", "metadata": {}, "score": "70.00014"}
{"text": "7(C ) , the agreement data string \" abc \" does not remain in duplicate in the dictionary buffer 22B. In general , the number of the dictionary data is increased , a memory capacity of a dictionary buffer itself is increased , and a data compression efficiency is lowered .", "label": "", "metadata": {}, "score": "70.03865"}
{"text": "xz : a streaming implementation that contains a gzip -like command line tool , supporting both LZMA and LZMA2 in its xz file format .It made its way into several software of the Unix - like world with its high performance ( compared to bzip2 ) and small size ( compared to gzip ) .", "label": "", "metadata": {}, "score": "70.24132"}
{"text": "In FIG .23(B ) , the current input code \" 02 \" placed in the arrangement position \" 03 \" and the upper input code \" 01 \" placed in an arrangement position ( or the upper entry ) \" 02 \" are exchanged .", "label": "", "metadata": {}, "score": "70.241776"}
{"text": "However , Frag [ 4 ] can not smoothly adapt the scale changes of the person ( e.g. , # 0278 ) .In Caviar 2 , almost all the trackers evaluated , except PLS [ 18 ] and MIL [ 9 ] , can follow the target .", "label": "", "metadata": {}, "score": "70.25832"}
{"text": "Based on these figures , it can be seen that our proposed algorithm can obtain narrow ranges of fluctuations against the other algorithms ( e.g. , David Indoor and Jumping ) .Though the values of the proposed tracker are not the best all the time , they are lower in the center error and higher in the overlap rate than the other algorithms in most test frames .", "label": "", "metadata": {}, "score": "70.38497"}
{"text": "Incremental Structured Dictionary Learning for Video Sensor - Based Object Tracking .Institute of Image Communication and Network Engineering , Shanghai Jiao Tong University , Shanghai 200240 , China .Shanghai Key Laboratory of Digital Media Processing and Transmissions , Shanghai 200240 , China .", "label": "", "metadata": {}, "score": "70.405045"}
{"text": "Then , the same parameter set is used .That is , the threshold of the change ratio in the learning phase is 0.05 , and the one in the prediction phase is 0.01 , 0.02 , 0.025 , 0.03 , and 0.05 .", "label": "", "metadata": {}, "score": "70.41369"}
{"text": "It should be noted that , on the one hand , the performance with a low K would approach that in the single voting case .On the other hand , it could introduce the classification error when K is too high .", "label": "", "metadata": {}, "score": "70.50771"}
{"text": "t .j .[ .b .j .b .j .b .j .N . ]T .d .N .Therefore , in the current frame , any patch of the target candidate .p .", "label": "", "metadata": {}, "score": "70.59541"}
{"text": "[ 31 ] for training and classification .We consider the discriminative observation modeling on a patch level .p .y .t .X .t . ) arg .max .p .i .c .N .", "label": "", "metadata": {}, "score": "70.62495"}
{"text": "In case of the data reproduction , the dictionary data or the dictionary data string and one or more pieces of data to be reproduced or one or more data strings to be reproduced are temporarily stored in the memory 22 .", "label": "", "metadata": {}, "score": "70.76338"}
{"text": "S. Sakurai , K. Makino , H. Suzuki , and Y. Masaoka , \" Ranking of evaluation targets based on complex sequential data , \" in Proceedings of the 25th Annual Conference of the Japanese Society for Artificial Intelligence , 2G2 - 01 , 2011 , ( Japanese ) .", "label": "", "metadata": {}, "score": "70.81041"}
{"text": "The fourth section proposes an expansion method of training data .The fifth section explains the experimental data , the experimental method , and the experimental results .The sixth section discusses the effect of the proposed method .Lastly , the seventh section describes the summary and future works .", "label": "", "metadata": {}, "score": "70.84912"}
{"text": "7(A ) , a dictionary data string placed at the right side of a storing position \" 0 \" is shifted to the left .Thereafter , it is judged in a step P9 whether or not all of a plurality of original data strings stored in the original data file 21 are compressed .", "label": "", "metadata": {}, "score": "70.89512"}
{"text": "5 .Thereafter , the occurrence frequency table produced in the data producing means 31 is referred by the data predicting means 32 , and an occurrence of a piece of next input data subsequent to the input data DIN to be compressed is predicted by the data predicting means 32 .", "label": "", "metadata": {}, "score": "70.91875"}
{"text": "23(A ) is renewed to the data arrangement B shown in FIG .23(C ) because the entries \" 02 \" and \" 03 \" are exchanged .Also , the positional information of the input codes relating to the above renewal of the data arrangement A are rewritten by the code converting editor 43 .", "label": "", "metadata": {}, "score": "70.91971"}
{"text": "3 - 0x7f are invalid values . 0x80- 0xff denotes an LZMA chunk , where the lowest 5 bits are used as bit 16 - 20 of the uncompressed size minus one , and bit 5 - 6 indicates what should be reset .", "label": "", "metadata": {}, "score": "70.92821"}
{"text": "The keyword display system according to claim 8 , wherein said keyword markup module creates a new DOM node that includes said extracted keyword , and replaces said keyword in said HTML document therewith .The keyword display system according to claim 9 , further comprising an interface module that executes an event based on an event handler that is defined in a tag of said new DOM node .", "label": "", "metadata": {}, "score": "71.00176"}
{"text": "5 is a block diagram showing a principle view of another data processing apparatus representing a data compressing apparatus and a data reproducing apparatus according to the present invention ; .FIG .6 is a flow chart showing a principle of a data compressing method according to the present invention ; .", "label": "", "metadata": {}, "score": "71.086365"}
{"text": "[ Google Scholar ] .Wang , Q. ; Chen , F. ; Xu , W. ; Yang , M.H. Online Discriminative Object Tracking with Local Sparse Representation .Proceedinsg of 2012 IEEE Workshop on Applications of Computer Vision ( WACV ) , Breckenridge , Colorado , USA , 9 - 11 January2012 ; pp .", "label": "", "metadata": {}, "score": "71.12201"}
{"text": "The .[ xz ] format , which can contain LZMA2 data , is documented at tukaani.org , [ 7 ] while the .7z file format , which can contain either LZMA or LZMA2 data , is documented in the 7zformat.txt file contained in the LZMA SDK .", "label": "", "metadata": {}, "score": "71.15814"}
{"text": "[0008 ] For example , in an electronic shopping mall on the web server , the web browser displays a webpage that presents a catalog in which product names are listed .With the product name listed using hypertext , the product names are linked to product explanation screens for each product .", "label": "", "metadata": {}, "score": "71.20055"}
{"text": "In the following , the format of the complex sequential data and these phases are explained .Learning of Trend Rules .The learning phase is composed of 5 subprocesses as shown in Figure 1 .The subprocess 1 performs morphological analysis for the texts .", "label": "", "metadata": {}, "score": "71.27099"}
{"text": "Its current setting is established after the times of the experiments with reference to the related works .Moreover , the overlapped percentage of the neighbored patch is related to the appearance variation of the target region .Since a low percentage number would lead to lower efficiency and the benchmark video is of various kinds , a unbiased number , 0.5 , is set .", "label": "", "metadata": {}, "score": "71.30799"}
{"text": "We know that the corporate performance of export companies gets worse when their currency exchange rises .The rise tends to lead to the drop of their stock prices .Even if a text does not directly describe a specific stock brand , the text can give a big impact on the stock price of the brand .", "label": "", "metadata": {}, "score": "71.35846"}
{"text": "In case where the agreement data portion exists ( YES ) , the control algorithm proceeds to a step P3 .Also , in case where any agreement data portion does not exist ( NO ) , the control algorithm proceeds to a step P2 , pieces of dictionary data of the agreement dictionary buffer 22C are searched .", "label": "", "metadata": {}, "score": "71.4396"}
{"text": "The state value .LZMA chunks consist of : .A 16-bit big - endian value encoding the low 16-bits of the uncompressed size minus one .A properties / lclppb byte if bit 6 in the control byte is set .", "label": "", "metadata": {}, "score": "71.49399"}
{"text": "In the dictionary buffer 22B , pieces of dictionary data formed by using the input data to be compressed are stored .In the auxiliary dictionary buffer ( hereinafter , also called an agreement dictionary buffer )22C , one or more pieces of dictionary data or one or more dictionary data strings expelled from the dictionary buffer 22B are stored .", "label": "", "metadata": {}, "score": "71.540504"}
{"text": "Accordingly , the input data DIN can be coded while dynamically renewing the code table produced from the occurrence frequency table , and the compressed data obtained by coding the input data DIN can be stored in the compressed data file 51 .", "label": "", "metadata": {}, "score": "71.59094"}
{"text": "FIG .1 is a block diagram of an automatic speech recognition system that includes a speech recognizer , a language analyzer , and an application command translator .FIG .2 is a block diagram of a computer on which the automatic speech recognition system of .", "label": "", "metadata": {}, "score": "71.78705"}
{"text": "In case where all of the compressed data are not decoded ( NO ) , the control algorithm returns to the step P2 , and the decoding processing of the compressed data is continued .Therefore , all of the compressed data are decoded to produce pieces of original data , and the original data are stored in the input data file 50 .", "label": "", "metadata": {}, "score": "71.81198"}
{"text": "Therefore , the particular dictionary data string of the dictionary buffer 22B having a duplicate relationship with the particular input data to be decoded is expelled from the data writing range of the dictionary buffer 22B. As a result , a piece of dictionary data \" xyz \" which is stored in the head position of the dictionary buffer 22B and has no connection with the agreement data string is not expelled .", "label": "", "metadata": {}, "score": "71.892944"}
{"text": "8 is a diagram of a third example illustrating the method for rewriting a DOM node ; .[ 0025 ] FIG .9 is a diagram of a fourth example illustrating the method for rewriting a DOM node ; and .", "label": "", "metadata": {}, "score": "71.91084"}
{"text": "2.4 Evaluation Performance of Chinese word segmenters is generally reported in terms of precision and recall .However , a comparison across systems cou ... . by Radu Florian , Grace Ngai - Conference on Natural Language Learning , 2001 . \" ...", "label": "", "metadata": {}, "score": "71.97121"}
{"text": "For a straight forward implementation , the person entering the boundary area for the second time is considered as a disappearance , so that the tracking process in the current camera stops .Quantitatively , we evaluate the lifecycle of the target once it is detected in one camera .", "label": "", "metadata": {}, "score": "72.01602"}
{"text": "Qualitative and quantitative evaluations are presented in the rest of this section .It should be noted that the setting on a particle number and the regulation constant above is based on the setup of classical online visual tracking algorithms , for a better performance comparison [ 3 , 9 - 11 , 16 ] .", "label": "", "metadata": {}, "score": "72.076164"}
{"text": "5 is a block diagram showing a principle view of a data compressing apparatus according to the present invention .The second object is , as shown in FIG .5 , achieved by the provision of a data compressing apparatus ( called a sixth data compressing apparatus ) comprises ; . a data producing means 31 for counting an occurrence frequency of each of types of pieces of input data to be compressed and producing an occurrence frequency table formed of a plurality of occurrence frequencies of the types of the input data to be compressed ; . a data predicting means 32 for predicting an occurrence of a piece of next input data input to the data producing means 31 subsequent to a piece of input data of which a type is just counted by the data producing means 31 ; and .", "label": "", "metadata": {}, "score": "72.18926"}
{"text": "This paper presents a pragmatic approach to Chinese word segmentation .It differentiates from most of the previous approaches mainly in three respects .First of all , while theoretical linguists have defined Chinese words with various linguistic criteria , Chinese words in this study are defined pragm ... \" .", "label": "", "metadata": {}, "score": "72.21471"}
{"text": "[ 8 ] There are also third - party Python bindings for the C++ library , as well as ports of LZMA to Pascal and Go .[16 ] [ 17 ] [ 18 ] .Decompression - only code for LZMA generally compiles to around 5 KB , and the amount of RAM required during decompression is principally determined by the size of the sliding window used during compression .", "label": "", "metadata": {}, "score": "72.28382"}
{"text": "s .t .t .t . ]In a homogeneous coordinate system , Equation ( 12 ) can be equivalently expressed as : .The elements of X t are independently modeled by a Gaussian distribution around the previous state , as follows , .", "label": "", "metadata": {}, "score": "72.569595"}
{"text": "S. Sakurai and K. Ueno , \" Analysis of daily business reports based on sequential text mining method , \" in Proceedings of the IEEE International Conference on Systems , Man and Cybernetics ( SMC ' 04 ) , vol .4 , pp .", "label": "", "metadata": {}, "score": "72.78069"}
{"text": "XZ for Java member name .description .price .quantity to be minimized : number of post - range - encoding bits needed to encode the string . optPrev . uncompressed size of the substring encoded by all packets except the last one . backPrev .", "label": "", "metadata": {}, "score": "72.849625"}
{"text": "Due to this , practical implementations tend to employ non - global heuristics .The xz encoders use a value called nice_len ( the default is 64 ) : when any match of length at least nice_len is found , the encoder stops the search and outputs it , with the maximum matching length .", "label": "", "metadata": {}, "score": "72.87714"}
{"text": "15(A ) , a ring - shaped dictionary buffer 22E is arranged .Accordingly , a fourth data compressing method can be performed by using a moving window in which the dictionary buffer 22B of the first embodiment is expanded .In this case , a piece of non - coded data is moved to a head position of the input buffer 22A. Thereafter , the input data to be compressed DIN is shifted from the input buffer 22A to the ring - shaped dictionary buffer 22E.", "label": "", "metadata": {}, "score": "72.88808"}
{"text": "lzip : another LZMA implementation mostly for Unix - like systems to be directly competing with xz .[20 ] It mainly features a simpler file format and therefore easier error recovery .^ a b Lasse Collin ( 2005 - 05 - 31 ) .", "label": "", "metadata": {}, "score": "72.891174"}
{"text": "Model .t .i . , an object function , J ( w ) , is established .The training data is generated based on the samples .Y .t .[ .p .t .p .t .", "label": "", "metadata": {}, "score": "72.896614"}
{"text": "pb is the number of low bits of the dictionary position to include in pos_state ( the default value used by the LZMA SDK is 2 ) .In non - LZMA2 streams , lc must not be greater than 8 , and lp and pb must not be greater than 4 .", "label": "", "metadata": {}, "score": "72.90229"}
{"text": "X .t .X .t .N .X .t .X .t .where \u03a8 0 is a vector whose elements are the corresponding variances of the affine parameters .K - Combined Voting SVM Classification of Sparse Coefficients for Observation Modeling .", "label": "", "metadata": {}, "score": "72.96588"}
{"text": "21(A ) is a flow chart of a data compressing operation according to the seventh embodiment ; .FIG .21(B ) is a flow chart of a data reproducing ( or decoding ) operation according to the seventh embodiment ; .", "label": "", "metadata": {}, "score": "73.06763"}
{"text": "is_rep2 .IsRepG2 . state .after bit sequence 1111 .bit .LONGREP[2 ] .LONGREP[3 ] .literal .Literal .prev_byte_lc_msbs , literal_pos_state , literal_bit_mode [ bit position ] , bit - tree context .The LZMA2 container supports multiple runs of compressed LZMA data and uncompressed data .", "label": "", "metadata": {}, "score": "73.13415"}
{"text": "Figure 9 shows the average symbol length .( bits / sample ) after compression using ALFC [ 4 ] and the proposed method for different packet sizes for relative humidity datasets .Conclusions .This paper presents a lightweight compression mechanism for low resolution sensor nodes based on fixed Huffman dictionaries .", "label": "", "metadata": {}, "score": "73.19371"}
{"text": "We note that the evaluation transaction is not processed in the following subprocesses when any trend rules do not match it .The subprocess 3 accumulates 1 to class counters of evaluation objects included in the evaluation transaction where the class of the class counters is equal to the one of the evaluation object and the evaluation objects have class counters of respective classes .", "label": "", "metadata": {}, "score": "73.23081"}
{"text": "21(A ) .Thereafter , in a step P4 of the main routine , it is judged whether or not all of pieces of input data to be compressed stored in the input data file 50 are coded .In case where all of the input data DIN to be compressed are coded ( YES ) , the control algorithm is finished .", "label": "", "metadata": {}, "score": "73.34571"}
{"text": "Analysis of Complex Sequential Data .This section explains the analysis method of complex sequential data [ 6 ] .The method has two phases : the learning phase and the prediction phase .The learning phase discovers trend rules from the text sequential data and the numerical sequential data .", "label": "", "metadata": {}, "score": "73.42157"}
{"text": "In contrast , in case where all of the compressed data are not decoded ( NO ) , the control algorithm returns to the step P2 , and the decoding processing for the compressed data is continued .Therefore , all of the compressed data are decoded to pieces of original data , and the original data are stored in the input data file 50 .", "label": "", "metadata": {}, "score": "73.505455"}
{"text": "Center error ( CE ) evaluation for nine video clips .The proposed algorithm is compared with seven state - of - the - art methods : Frag [ 4 ] , IVT [ 3 ] , VTD [ 16 ] , L1 T [ 11 ] , MIL [ 9 ] , TLD [ 10 ] and PLS [ 18 ] .", "label": "", "metadata": {}, "score": "73.690704"}
{"text": "Center error ( CE ) evaluation for nine video clips .The proposed algorithm is compared with seven state - of - the - art methods : Frag [ 4 ] , IVT [ 3 ] , VTD [ 16 ] , L1 T [ 11 ] , MIL [ 9 ] , TLD [ 10 ] and PLS [ 18 ] .", "label": "", "metadata": {}, "score": "73.690704"}
{"text": "It is also possible to define an attribute in the start tag to transmit a search request including the keyword to this web version terminology dictionary .When the user uses the mouse of the client terminal 11 to click on the keyword , a webpage for the web version terminology dictionary is displayed , or a webpage that shows search results for the keyword is displayed .", "label": "", "metadata": {}, "score": "73.69236"}
{"text": "Distance is equal to the second last used LZ77 distance .1 + 1 + 1 + 1 + 0 + len .An LZ77 sequence .Distance is equal to the third last used LZ77 distance .1 + 1 + 1 + 1 + 1 + len .", "label": "", "metadata": {}, "score": "73.69437"}
{"text": "Proceedings of 2011IEEE Workshop on Applications of Computer Vision ( WACV ) , Kona , HI , USA , 5 - 7 January 2011 ; pp .642 - 649 .Zhong , W. ; Lu , H. ; Yang , M.H. Robust Object Tracking via Sparsity - Based Collaborative Model .", "label": "", "metadata": {}, "score": "73.71328"}
{"text": "The certainty can be updated for each unit time .It can flexibly evaluate relationships between evaluation objects and topics .It is anticipated that these improvements lead to the acquisition of more valid trend rules .Also , they lead to the improvement of the detection performance .", "label": "", "metadata": {}, "score": "73.71544"}
{"text": "This paper improves a method which predicts whether evaluation objects such as companies and products are to be attractive in near future .The attractiveness is evaluated by trend rules .The trend rules represent relationships among evaluation objects , keywords , and numerical changes related to the evaluation objects .", "label": "", "metadata": {}, "score": "73.73213"}
{"text": "To demonstrate the proposed improvement in the voting scheme , comparison between single voting and K - combined voting is also drawn on the benchmark sequences .The settings are the same with the ones above .It can be found that the proposed algorithm with K - combined voting is better in both center error evaluation and overlap rate evaluation .", "label": "", "metadata": {}, "score": "73.76457"}
{"text": "18(A ) or a data reproducing , algorithm shown in FIG .18(B ) is stored in the EPROM 46 in the sixth embodiment .The detail of the above control algorithms is described in the embodiments .The display 47 is used as an auxiliary tool for assisting the input and output of the keyboard 48 and the CPU 49 .", "label": "", "metadata": {}, "score": "73.88416"}
{"text": "0058 ]FIG .10 illustrates a method for displaying information related to a keyword .When the user uses the mouse of the client terminal 11 , or the like to select a keyword , the renderer 33 notifies the plugin 34 of an event that indicates that the keyword was selected .", "label": "", "metadata": {}, "score": "73.88652"}
{"text": "The prev_byte_lc_msbs value is set to the lc ( up to 4 , from the LZMA header or LZMA2 properties packet ) most significant bits of the previous uncompressed byte .The is_REP value denotes whether a packet that includes a length is a LONGREP rather than a MATCH .", "label": "", "metadata": {}, "score": "74.01578"}
{"text": "Zhang , Q. ; Li , B. Discriminative K - SVD for Dictionary Learning in Face Recognition .Proceedings of 2010 IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , San Francisco , CA , USA , 13 - 18 June 2010 ; pp .", "label": "", "metadata": {}, "score": "74.06338"}
{"text": "View at Scopus .S. Sakurai and R. Orihara , \" Discovery of important threads from bulletin board sites , \" International Journal of Information Technology and Intelligent Computing , vol .1 , no . 1 , pp .217 - 228 , 2006 .", "label": "", "metadata": {}, "score": "74.13725"}
{"text": "+ 12 bytes dictionary size .Values higher than 40 are invalid .LZMA2 data consists of packets starting with a control byte , with the following values : . 0 denotes the end of the file .1 denotes a dictionary reset followed by an uncompressed chunk .", "label": "", "metadata": {}, "score": "74.28993"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. Schoellhammer , E. Osterweil , B. Greenstein , M. Wimbrow , and D. Estrin , \" Lightweight temporal compression of microclimate datasets , \" in Proceedings of the 29th Annual IEEE International Conference on Local Computer Networks ( LCN ' 04 ) , pp .", "label": "", "metadata": {}, "score": "74.296425"}
{"text": "t .i . could be represented based on the templates by solving the optimization problem based on the elastic net regularization [ 29 ] using the least angle regression ( LARS ) method : . min .i .M .", "label": "", "metadata": {}, "score": "74.38269"}
{"text": "It was originally dual - licensed under both the GNU LGPL and Common Public License , [ 13 ] with an additional special exception for linked binaries , but was placed by Igor Pavlov in the public domain on December 2 , 2008 , with the release of version 4.62 .", "label": "", "metadata": {}, "score": "74.39075"}
{"text": "t .i . as the dictionary .p .t .i . k . i .l . k . k . s . k . i .M .N .M .d .l . k . i . , separately correspond to the positive patches and negative ones of the candidates , and the i- th classifier for .", "label": "", "metadata": {}, "score": "74.44539"}
{"text": "The former figure is a result in the case the minimum transaction is 1 and the latter one is a result in the case of 3 .The results in the cases of 5 and 10 are left out because only too fewer evaluation objects are extracted and the detection performance excessively depends on them .", "label": "", "metadata": {}, "score": "74.454384"}
{"text": "Author Contributions The work presented here was carried out in collaboration between all authors .MX , HY and SBZ defined the research topic .MX and HY designed methods , carried out the experiments , interpreted the results and wrote the paper .", "label": "", "metadata": {}, "score": "74.49182"}
{"text": "Also , in case where the agreement of the data portion is not the 2-byte agreement , the control algorithm is finished without any generation of the agreement dictionary .In case where the agreement of the data portion is the 2-byte agreement in the step P3 , it is judged in the step P4 whether or not a piece of dictionary data agreeing with the agreement data portion exists in the agreement dictionary buffer 22C. In case where the dictionary data agreeing with the agreement data portion exists ( YES ) , the control algorithm is finished without any generation of the agreement dictionary .", "label": "", "metadata": {}, "score": "74.598114"}
{"text": "When a stock brand has the value which is smaller than the minimum number of transactions , it is not regarded as an attractive stock brand .In this experiment , the total value of the class ' ' Drop ' ' and the class ' ' Rise ' ' is referred to .", "label": "", "metadata": {}, "score": "74.61896"}
{"text": "The keyword display method according to claim 3 , wherein said third step defines an event handler in a tag of said new DOM node in order to display another webpage that has information related to said keyword .The keyword display method according to claim 3 , further comprising a fourth step of displaying information over said webpage by defining an event handler in a tag of said new DOM node , the information having information related to said keyword .", "label": "", "metadata": {}, "score": "74.63484"}
{"text": "1854 - 1861 .Chen , J. ; Wang , Y. ; Wu , H. A coded aperture compressive imaging array and its visual detection and tracking algorithms for surveillance systems .Sensors 2012 , 12 , 14397 - 14415 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "74.72276"}
{"text": "Thereafter , the occurrence frequency table is renewed in a step P3 .As shown in FIG .22 , the data arrangement A of the occurrence frequency table is renewed to a data arrangement B according to a plurality of input codes ( or pieces of input data DIN ) input to the editor 41 one after another in a step P31 .", "label": "", "metadata": {}, "score": "74.83365"}
{"text": "jrsoftware.org .Retrieved 2013 - 06 - 16 .^ \" LZHAM - Lossless Data Compression Codec \" .Richard Geldreich .LZHAM is a lossless data compression codec written in C / C++ with a compression ratio similar to LZMA but with 1.5x-8x faster decompression speed .", "label": "", "metadata": {}, "score": "74.84895"}
{"text": "The transformed transaction tends to include some evaluation objects .The discovery method of frequent patterns tends to discover many frequent patterns including some stock brands .However , the patterns are not always characteristic patterns for representing the change of stock prices because the patterns only reflect on the topic dictionary .", "label": "", "metadata": {}, "score": "74.87425"}
{"text": "Information may be displayed to the user on a monitor 70 .Computer 30 also may include peripheral output devices , such as a printer .Referring to .FIG .3 , in one embodiment , speech recognizer 12 includes a spectrum analyzer 80 , a pattern matching system 82 , and a meaning token selecting system 84 .", "label": "", "metadata": {}, "score": "74.88879"}
{"text": "Suppose at time t , the target .Y .t .[ .p .t .p .t .p .t .N . ]d .N . is sampled and vectorized into N separate overlapped patches with zero mean and unit variance , where the size of each patch is d 2 \u00d7 1 .", "label": "", "metadata": {}, "score": "74.93791"}
{"text": "View at Scopus . Y. Seo , J. A. Giampapa , and K. P. Sycaratech , \" Financial news analysis for intelligent portfolio management , \" Report CMU - RI - TR-04 - 04 , Robotics Institute , Carnegie Mellon University , 2004 .", "label": "", "metadata": {}, "score": "74.96148"}
{"text": "The results concerning the utilization of the proposed scheme and of LEC are shown in Figure 8 . can be seen when compressing relative humidity datasets ) .The conclusions are very similar to those obtained for the temperature datasets .Once again the proposed scheme performs best , while approaching the theoretical limit for some of the datasets .", "label": "", "metadata": {}, "score": "75.01536"}
{"text": "FIG .6 is a diagram of a first example illustrating a method for rewriting a DOM node ; .[ 0023 ] FIG .7 is a diagram of a second example illustrating the method for rewriting a DOM node ; .", "label": "", "metadata": {}, "score": "75.0229"}
{"text": "Here , an example is described , in which the client terminal 11 acquires a document described in HTML from the web server 13a , and displays a webpage on a display device of the client terminal 11 .[ 0029 ] FIG .", "label": "", "metadata": {}, "score": "75.09119"}
{"text": "Thereafter , in a step P2 , a piece of compressed data is read out from the compressed data file 51 and is decoded .Thereafter , the occurrence frequency table is renewed each time a piece of compressed data is input to the occurrence frequency producing editor 41 .", "label": "", "metadata": {}, "score": "75.209015"}
{"text": "22 is a flow chart of the renewal of an occurrence frequency table ; .FIGS .23(A ) to 23(C ) are explanatory views respectively showing the exchange of pieces of input data listed in the occurrence frequency table in case of the renewal of the occurrence frequency table ; .", "label": "", "metadata": {}, "score": "75.23309"}
{"text": "Results .In this section we investigate the performance of the proposed scheme when the fixed Huffman alphabet in Table 2 is used to compress different datasets .First we consider the temperature datasets in Table 1 .It is important to note that the test datasets , Set 2 to Set 9 , were collected at different locations and times than those of the reference dataset Set 1 , which was used to construct the alphabet in Table 2 .", "label": "", "metadata": {}, "score": "75.26845"}
{"text": "This experiment collects their names from the home pages of each stock exchange .The number of evaluation objects arrives at 3,951 .Topic Dictionary .The table is downloaded on October 25 , 2011 .It includes 134 keywords in the first layer and 867 keywords in the second layer .", "label": "", "metadata": {}, "score": "75.32208"}
{"text": "18(B ) is a flow chart of a data reproducing ( or decoding ) operation according to the sixth embodiment ; .FIGS . 19(A ) and 19(B ) are respectively an explanatory view of a code converting editor used in a data compressing operation ; .", "label": "", "metadata": {}, "score": "75.38019"}
{"text": "Comprehensively and qualitatively speaking , the proposed algorithms perform the best .The two video sequences , Singer and Jumping , in Figure 8 are from the work by Kwon et al .[ 16 ] and TLD [ 10 ] .", "label": "", "metadata": {}, "score": "75.46554"}
{"text": "It is preferred that the occurrence frequency table be renewed by the data producing means 31 each time a piece of input data to be compressed is input to the data producing means 31 in the data processing apparatus ( called a seventh data processing apparatus ) .", "label": "", "metadata": {}, "score": "75.57352"}
{"text": "Also , it is preferred that the data processing apparatus ( called an eighth data processing apparatus ) further comprise a level adjusting means 34 for adjusting a length of each of the compressed data converted by the data converting means 33 .", "label": "", "metadata": {}, "score": "75.58393"}
{"text": "Relay tracking evaluation establishment .We assume that the cameras are connected by the local area network ( LAN ) with the computing server in the back - end .The videos acquired would be transmitted to the server without any time delay .", "label": "", "metadata": {}, "score": "75.68118"}
{"text": "Relay tracking evaluation establishment .We assume that the cameras are connected by the local area network ( LAN ) with the computing server in the back - end .The videos acquired would be transmitted to the server without any time delay .", "label": "", "metadata": {}, "score": "75.68118"}
{"text": "Sensors 2012 , 12 , 3747 - 3761 .[ Google Scholar ] .Elad , M. Sparse and Redundant Representations : From Theory to Applications in Signal and Image Processing , 1st ed . ; Springer : Heidelberg , Germany , 2010 .", "label": "", "metadata": {}, "score": "75.723076"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. P. Capo - Chichi , H. Guyennet , and J.-M. Friedt , \" K - RLE : a new data compression algorithm for wireless sensor network , \" in Proceedings of the 3rd International Conference on Sensor Technologies and Applications ( SENSORCOMM ' 09 ) , pp .", "label": "", "metadata": {}, "score": "76.000015"}
{"text": "Referring to .FIG .2 , in one embodiment , automatic speech recognition system 10 may be implemented as one or more respective software modules operating on a general - purpose computer 30 .Computer 30 includes a processing unit 34 , a system memory 36 , and a system bus 38 that couples processing unit 34 to the various components of computer 30 .", "label": "", "metadata": {}, "score": "76.1444"}
{"text": "This results in an orthogonal projection of the vector , u j , onto the constraint set , namely the l 2 - ball in this paper .Convergence to a global optimum is guaranteed , since the convex optimization problem admits separable constraints in the updated columns .", "label": "", "metadata": {}, "score": "76.238525"}
{"text": "[ Google Scholar ] .Everingham , M. ; Gool , L. ; Williams , C. ; Winn , J. ; Zisserman , A. The pascal Visual Object Classes ( VOC ) challenge .Int .J. Comput .Vis .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "76.337326"}
{"text": "Singer is challenging , as it contain illumination variation , and Deer highlights abrupt motions .Especially PLS [ 18 ] can not capture the scale variation of the target through all the frames of Singer .In Jumping , the successful trackers only include the proposed algorithms , MIL [ 9 ] and TLD [ 10 ] , while the others fail to capture the head of the person when he or she jumps up and down repeatedly .", "label": "", "metadata": {}, "score": "76.738235"}
{"text": "The occurrence frequency of each of the input data DIN is determined according to an addition or an accumulation .For example , as shown in Table 2 , 256 pieces of input data \" 00 \" to \" FF \" respectively indicated by two hexadecimal codes and occurrence frequencies indicated by a decimal notation are comparatively listed by the editor 21 .", "label": "", "metadata": {}, "score": "76.75337"}
{"text": "IEEE Trans .Image Process .[ Google Scholar ] .Wang , Q. ; Chen , F. ; Xu , W. ; Yang , M.H. Object tracking via partial least squares analysis .IEEE Trans .Image Process .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "77.108536"}
{"text": "A data compressing theory is first proposed by Mr. Claude Shannon , the Bell Telephone Laboratories in USA .He disclosed a concept \" Entropy \" of the data compressing in 1948 .Also , the same theory is disclosed by Mr. R. M. Fano , the Massachusetts Institute of Technology in USA in the almost same period .", "label": "", "metadata": {}, "score": "77.174194"}
{"text": "That is , ' ' Topic ' ' deals with additional 2,300 evaluation objects by referring to all stock brands .Also , it deals with more attributes than the previous research does because more training transactions are generated from more news headlines .", "label": "", "metadata": {}, "score": "77.29149"}
{"text": "i .d .P .t .i .i .b .t .i .b .t .M .i .b .t .M .for some scalars , .j .i .Thus , .", "label": "", "metadata": {}, "score": "77.293884"}
{"text": "The method focuses on two trends of them : ' ' Rise ' ' and ' ' Drop ' ' and inductively learns classification models .The models are used for the prediction .Mittermayer and Knolmayer [ 11 ] propose a method that automatically classifies news articles to predict the trends of stock prices .", "label": "", "metadata": {}, "score": "77.32407"}
{"text": "The data compressing apparatus according to claim 1 in which the occurrence frequency table is renewed by the data producing means each time a piece of input data to be compressed is input to the data producing means in the data processing apparatus .", "label": "", "metadata": {}, "score": "77.50378"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .C. M. Sadler and M. Martonosi , \" Data compression algorithms for energy - constrained devices in delay tolerant networks , \" in Proceedings of the 4th International Conference on Embedded Networked Sensor Systems ( SenSys ' 06 ) , pp .", "label": "", "metadata": {}, "score": "77.557"}
{"text": "11 , pp .2661 - 2666 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .K. Romer , \" Discovery of frequent distributed event patterns in sensor networks , \" in Proceedings of the 5th European Conference on Wireless Sensor Networks ( EWSN ' 08 ) , pp .", "label": "", "metadata": {}, "score": "77.57556"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Sakurai , K. Makino , and S. Matsumoto , \" A discovery method of trend rules from complex sequential data , \" in Proceedings of the 26th IEEE International Conference on Advanced Information Networking and Applications Workshops ( AINA ' 12 ) , pp .", "label": "", "metadata": {}, "score": "77.608894"}
{"text": "-1 if the first packet in the \" tail \" is LIT , 0 - 3 if it is a repetition using the last used distance number 0 - 3 , 4 + distance if it is a MATCH ( only valid if prev1IsLiteral and hasPrev2 are true ) .", "label": "", "metadata": {}, "score": "77.69316"}
{"text": "Two of the most recent and effective approaches in this category are Marcelloni and Vecchio 's lossless entropy compression ( LEC ) [ 2 , 3 ] and Kiely et al . 's adaptive linear filtering compression ( ALFC ) [ 4 ] .", "label": "", "metadata": {}, "score": "77.6957"}
{"text": "It is necessary for the recommendation system to use additional information in order to understand them .In near future , the system will be revised by referring to the knowledge in a financial engineering field .Experimental Method .This experiment discovers trend rules from the data set D1 and predicts attractive stock brands by referring to the data set D2 .", "label": "", "metadata": {}, "score": "77.878555"}
{"text": "Surv .Article No . 13 .[ Google Scholar ] .Salti , S. ; Cavallaro , A. ; Di Stefano , L. Adaptive appearance modeling for video tracking : Survey and evaluation .IEEE Trans .Image Process .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "78.33913"}
{"text": "The web browser 22 , which is one of the application programs , includes a user agent 31 , parser 32 , renderer 33 , and plugin 34 .[ 0030 ] With this kind of configuration , a user of the client terminal 11 starts up the web browser 22 , and inputs a URL of a desired resource to browse .", "label": "", "metadata": {}, "score": "78.39304"}
{"text": "This work is supported NSFC(no .61171172 and no .61102099 ) , the National Key Technology R&D Program ( no .2011BAK14B02 ) and STCSM ( Science and Technology Commission of Shanghai Municipality , China ) under no .10231204002 , no .", "label": "", "metadata": {}, "score": "78.680405"}
{"text": "That is , if some stock brands are not described in news headlines , the stock brands are excluded from the calculation of these evaluation criteria .This exclusion is performed for each day .This is because we can not understand changes of their stock prices from the news headlines .", "label": "", "metadata": {}, "score": "78.73527"}
{"text": "This section shows parts of experimental results in Figures 5 - 10 .In each figure , ' ' Sakurai ' ' shows the results in [ 6 ] and it does not use the topic dictionary . ''No_topic ' ' shows the results in the case the topic dictionary is not activated and ' ' Topic ' ' shows the one in the case the topic dictionary is activated .", "label": "", "metadata": {}, "score": "78.85968"}
{"text": "In case of the 2-byte agreement ( YES ) , the control algorithm proceeds to a step P4 .Also , in case where the judgement is not the 2-byte agreement ( NO ) , the control algorithm proceeds to a step P7 , a normal decoding processing is performed without any generation of the agreement dictionary , and the control algorithm is finished .", "label": "", "metadata": {}, "score": "78.90828"}
{"text": "Also , the occurrence frequency table is renewed in the occurrence frequency producing editor 41 of a seventh data processing apparatus each time a piece of input data DIN to be compressed is input , according to a seventh embodiment .The code converting editor 43 is an example of the data allocating means 33 , and a piece of input data DIN is converted into a piece of positional information according to the occurrence prediction of the input data DIN obtained in the data comparing editor 22 .", "label": "", "metadata": {}, "score": "78.92963"}
{"text": "t .i .T .t .i .i .i .i . 2 . . .s . . .t . . .\u03b2 .i . where \u03bb 1 and \u03bb 2 are regularization constants .", "label": "", "metadata": {}, "score": "79.137924"}
{"text": "Moreover , the average center error ( ACE ) and average overlap rate ( AOR ) are defined as : .ACE .T .i .T .c . eval .i .c . g .t .i .", "label": "", "metadata": {}, "score": "79.16641"}
{"text": "Therefore , a code table shown in Table 3 is obtained by converting the input data listed in the Table 2 into pieces of positional information coded .TABLE 3 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ occurrence positionalinput data frequencies information______________________________________02 100 00FD 100 0100 75 10003 75 10101 50 110004 50 1101FE 50 1110FF 50 1111 - - -- -- _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "79.186584"}
{"text": "Fung et al .[ 10 ] propose a method predicting the changes of stock prices in the stock market .The method segments numerical stock price data into three trends : ' ' Rise , ' ' ' ' Steady , ' ' and ' '", "label": "", "metadata": {}, "score": "79.378105"}
{"text": "This section deals with 1,680 stock brands . ''No_topic ' ' and ' ' Topic ' ' deals with all stock brands .It is anticipated that the expansion of stock brands leads to the one of training transactions .However , our preliminary experiments show that the difference of stock brands does not give a big impact on the detection performance .", "label": "", "metadata": {}, "score": "79.51464"}
{"text": "The same meaning token also may be associated with many different pronunciations .For example , \" seek \" could also be the dictionary spelling for the pronunciations of \" I want \" , \" I would like \" , \" I 'm looking for \" , \" could you please give me \" , and \" gimme \" .", "label": "", "metadata": {}, "score": "79.88831"}
{"text": "If low is greater or equal than 2 ^ 32 : .Output the byte stored in cache plus one to the compressed stream .Output cache_size - 1 bytes with 0 value .Set cache to bits 24 - 31 of low .", "label": "", "metadata": {}, "score": "80.10482"}
{"text": "On the other hand , if many recommended stock brands are not attractive , the stock traders can not believe the recommendation of the system .It is important for the system to recommend attractive stock brands with high probability .Therefore , the precision ( .", "label": "", "metadata": {}, "score": "80.32553"}
{"text": "33 - 48 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Barcelo - Llado , A. Perez , and G. Seco - Granados , \" Enhanced correlation estimators for distributed source coding in large wireless sensor networks , \" IEEE Sensors Journal , vol .", "label": "", "metadata": {}, "score": "80.43526"}
{"text": "As described above , the renderer 33 displays the webpage on the display device of the client terminal 11 according to the rewritten DOM node and CSS style sheet .Therefore , information related to a keyword is displayed by defining an event handler as an attribute in the start tag .", "label": "", "metadata": {}, "score": "80.44426"}
{"text": "D .i .N .[ .p .t .i .D .i .i . 1 . ]s . . .t. .d .j .j .This problem is not jointly convex with respect to D and \u03b2 , and it is commonly solved by alternating between the two variables .", "label": "", "metadata": {}, "score": "80.514114"}
{"text": "Moreover , based on dictionary sharing , more visual information about the target could be obtained before the target enters the specific scene .Thus , the performance with the dictionary sharing is better than the one without sharing .Horizontally , all the values in Camera 2 and Camera 4 are lower than their counterparts in other columns .", "label": "", "metadata": {}, "score": "80.57282"}
{"text": "The compressed data file 51 is a memory for storing pieces of compressed data in case of the data compressing and reproducing operations and is made of the same memory unit as that in the input data file 50 .Therefore , the data compressing apparatus and the data reproducing apparatus applied for the sixth to eighth embodiments are composed of the above elements , the input data to be compressed can be coded , and the compressed data coded can be decoded .", "label": "", "metadata": {}, "score": "80.72011"}
{"text": "19(A ) , the input data DIN listed in the occurrence frequency table T1 are rearranged in order of degree of the occurrence frequency .Thereafter , a piece of input data DIN to be compressed is read out from the input data file 50 and is coded in the code converting editor 43 in a step P3 .", "label": "", "metadata": {}, "score": "80.80467"}
{"text": "All authors read and approved the manuscript .Author to whom correspondence should be addressed ; E - Mail : silas ; Tel . : +86 - 21 - 3420 - 4002 ; Fax : +86 - 21 - 3420 - 4394 .", "label": "", "metadata": {}, "score": "80.94946"}
{"text": "A user may interact ( e.g. , enter commands or data ) with computer 40 using a keyboard 62 and a mouse 64 .A user also may interact with automatic speech recognition system 10 , which is executing on computer 30 , by speaking into a microphone 66 .", "label": "", "metadata": {}, "score": "81.06479"}
{"text": "For example , the memory 22 is composed of an input buffer 22A , a dictionary buffer 22B , an auxiliary dictionary buffer 22C and a fixed dictionary buffer 22D.A random access writable and readable memory is used for the memory 22 .", "label": "", "metadata": {}, "score": "81.10498"}
{"text": "Choudhury et al .[ 13 ] develop a model analyzing communication dynamics in the blogosphere to decide correlations with movement in stock market .The model is acquired from the data in the blogosphere by support vector machine ( SVM ) regression .", "label": "", "metadata": {}, "score": "81.174286"}
{"text": "Each bar graph is accumulated by the number of ' ' Drop \" and ' ' Rise .Figure 8 shows the number of extracted evaluation objects .Each method has 4 bar graphs .Each bar graph shows the number in the case that the minimum transaction is changed by 1 , 3 , 5 , and 10 .", "label": "", "metadata": {}, "score": "81.57692"}
{"text": "Figure 5 shows that the number of training transactions increases . ''No_topic ' ' is about 2.9 times as large as ' ' Sakurai . ' 'The expansion ratio is close to the expansion ratio of stock brands . ''", "label": "", "metadata": {}, "score": "81.58722"}
{"text": "A Quick Benchmark : Gzip vs. Bzip2 vs. LZMA \" .Retrieved 2015 - 10 - 21 . -LZMA Unix Port was finally replaced by xz which features better and faster compression ; from here we know even LZMA Unix Port was a lot better than gzip and bzip2 .", "label": "", "metadata": {}, "score": "81.693855"}
{"text": "Australasian Database Conference , vol .5 , pp .131 - 139 , 2002 .M. de Choudhury , H. Sundaram , A. John , and D. D. Seligmann , \" Can blog communication dynamics be correlated with stock market activity ? \" in Proceedings of the 19th ACM Conference on Hypertext and Hypermedia ( HT ' 08 ) , pp .", "label": "", "metadata": {}, "score": "81.83584"}
{"text": "When an occurrence frequency of a piece of input data DIN is initially measured by the measuring means 11 , the input data DIN is converted into a piece of converted data DT according to the occurrence frequency in the converting means 12 .", "label": "", "metadata": {}, "score": "81.87511"}
{"text": "The target ( the rear view of a car ) is small and easily distracted by the surroundings , including similar vehicle appearance and glare .David Indoor contains out - of - plane rotation as the person turns his or her face and scale change , due to distance variation from the cameras .", "label": "", "metadata": {}, "score": "81.93463"}
{"text": "The two latter parameters are used in order to judge whether a stock brand is attractive for each day .The first minimum number shows the minimum number of items included in trend rules .When a trend rule is composed of items whose number is smaller than the minimum number , the trend rule is not used .", "label": "", "metadata": {}, "score": "82.182526"}
{"text": "( 8) Description of an eighth embodiment .Control algorithms corresponding to the flow charts shown in FIGS .24 and 25 are stored in the EPROM 46 shown in FIG .17 .Therefore , features of the eighth embodiment of the present invention are that the length of the compressed data is adjusted and the degradation of the data compressing efficiency is prevented .", "label": "", "metadata": {}, "score": "82.4147"}
{"text": "Some keywords are compound nouns .They are separated into respective nouns .The third layer includes stock brand related to the keywords .The table has 4,842 relations between the keywords and the stock brands .Evaluation Criteria .Stock traders can not focus on all attractive stock brands because many stock brands are listed and many parts of them can be attractive .", "label": "", "metadata": {}, "score": "82.77484"}
{"text": "Through the application to the various fields , we will evaluate the effect of the proposed method in detail .Conflict of Interests .The authors declare that there is no conflict of interests regarding the publication of this paper .References .", "label": "", "metadata": {}, "score": "82.91715"}
{"text": "( 7 ) Description of a seventh embodiment .Control algorithms corresponding to the flow charts shown in FIGS .21 and 22 are stored in the EPROM 46 shown in FIG .17 .Thereafter , in a step P2 , a piece of input data DIN to be compressed is read out from the input data file 50 and is coded .", "label": "", "metadata": {}, "score": "83.05946"}
{"text": "D .t .i . ) j . ) max .u .j .u .j .where \u03b4 is a small constant , and ( \u00b7 ) ( j ) refers to the j -th column .A . t .", "label": "", "metadata": {}, "score": "83.302185"}
{"text": "10 is a flow chart showing a data reproduction .The original data file 21 a memory for storing pieces of original data in case of a data compression or a data reproduction .A magnetic disk apparatus or a semiconductor memory apparatus is used for the original data file 21 .", "label": "", "metadata": {}, "score": "83.33046"}
{"text": "The occurrence frequency producing editor 41 is an example of the data producing means 31 .Pieces of input data DIN to be compressed are input to the occurrence frequency producing editor 41 , and an occurrence frequency table in which the relationship between the input data DIN and occurrence frequencies of the input data DIN is listed is produced .", "label": "", "metadata": {}, "score": "83.398865"}
{"text": "We note that a higher change ratio is set in order to use news headlines with big impacts in the learning phase .The minimum support is 0.005 , 0.01 , 0.02 , and 0.03 .It is a criterion which evaluates whether a pattern is frequent .", "label": "", "metadata": {}, "score": "83.71126"}
{"text": "Each stock brand is judged whether it is truly attractive in the designated day by referring to the change ratios for the next day .The judged results are used as the ground truth for this experiment .The judge is performed for each day in the prediction period .", "label": "", "metadata": {}, "score": "83.82"}
{"text": "Yang , M. ; Zhang , L. ; Feng , X. ; Zhang , D. Fisher Discrimination Dictionary Learning for sparse representation .Proceedings of 2011IEEE International Conference on Computer Vision ( ICCV ) , Barcelona , Spain , 6 - 13 November 2011 ; pp .", "label": "", "metadata": {}, "score": "84.066284"}
{"text": "25 is a flow chart showing a level adjustment of a piece of positional information in case of the data compressing operation .DETAILED DESCRIPTION OF THE EMBODIMENTS .A data processing apparatus and a data processing method according to each of a plurality of embodiments of the present invention is described with reference to drawings .", "label": "", "metadata": {}, "score": "84.07901"}
{"text": "9(A ) , in case where a data length of the compressed data is 7 bits ( 0 to 63 ) , the code \" 0 \" indicating the short positional information value is written in the head position of the compressed data .", "label": "", "metadata": {}, "score": "84.10679"}
{"text": "In order to meet the assumptions of the proposed method , whenever a dataset contains measurements with resolutions higher than 1 \u00b0 C , the data is rounded before being processed by any of the algorithms under consideration .To further validate our approach , we also carry out experiments using six datasets of relative humidity measurements .", "label": "", "metadata": {}, "score": "84.262695"}
{"text": "This article presents a pragmatic approach to Chinese word segmentation .It differs from most previous approaches mainly in three respects .First , while theoretical linguists have defined Chinese words using various linguistic criteria , Chinese words in this study are defined pragmatically as segmen ... \" .", "label": "", "metadata": {}, "score": "84.70685"}
{"text": "Therefore , discovered trend rules are limitedly related to the minor stock brands and their impact is small .Figure 5 shows the number of training transactions extracted from the data set D1 .In this figure , each bar graph is accumulated by the number of ' ' Drop ' ' and ' ' Rise . ' ' Figures 6 and 7 show the number of trend rules in the case that the minimum support is 0.005 and 0.01 , respectively . ''", "label": "", "metadata": {}, "score": "85.07073"}
{"text": "24(A ) is a flow chart of a data compressing operation according to the eighth embodiment ; .FIG .24(B ) is a flow chart of a data reproducing ( or decoding ) operation according to the eighth embodiment ; and .", "label": "", "metadata": {}, "score": "85.201584"}
{"text": "In Camera 2 , there are other moving objects as the target enters the scene , and in Camera 4 , there is light variation .The foreground target could not be timely and correctly detected , which leads to a relatively poor lifecycle performance .", "label": "", "metadata": {}, "score": "85.77768"}
{"text": "Functions of the level adjusting editor 44 are described later in detail with reference to FIGS .24 and 25 .In the memory 45 , one or more pieces of input data DIN to be compressed are temporarily stored when the input data DIN is compressed .", "label": "", "metadata": {}, "score": "85.91868"}
{"text": "t . arg .max .X .t .v .p .y .t .X .t .v . )p .X .t .v .X .t .Based on the basic principle described , the details of dynamical modeling and observation modeling in the proposed framework are further described .", "label": "", "metadata": {}, "score": "86.08246"}
{"text": "In this case , though a processing speed is lowered , a data compression efficiency can be heightened .A reference pointer and a storage pointer are used in the agreement dictionary buffer 22C.The reference pointer points at a position for referring the agreement dictionary buffer 22C. The storage pointer points at a position for storing a piece of dictionary data or a dictionary data string currently expelled from the dictionary buffer 22B. The reference pointer is placed at a forward position of the storage pointer to search for a piece of updated dictionary data or an updated dictionary data string currently expelled from the dictionary buffer 22B. A memory capacity of the agreement dictionary buffer 22C is larger than that of the dictionary buffer 22B not to store a piece of dictionary data of the agreement dictionary buffer 22C and another piece of dictionary data of the dictionary buffer 22B in a duplicate relationship .", "label": "", "metadata": {}, "score": "86.41669"}
{"text": "It gives the best F -measures in the most cases .For example , when we focus on the result in Figure 9(b ) , the result shows that ' ' Topic ' ' is 14.3 % higher than ' ' Sakurai ' ' and ' ' Topic ' ' is 12.7 % higher than ' ' No_topic . ' '", "label": "", "metadata": {}, "score": "86.479546"}
{"text": "This experiment uses news headlines distributed by five sites ( Excite , Goo , Infoseek , Livedoor , and Yahoo ) as text sequential data .The ones for the learning phase ( D1 ) are collected from August 28 , 2010 , to January 31 , 2011 , and the ones for the prediction phase ( D2 ) are collected from February 1 , 2011 , to March 10 , 2011 .", "label": "", "metadata": {}, "score": "86.556274"}
{"text": "t .In the context of particle filtering , typically , a set of candidates .X .t .v .w .t .v .w .t .v .p .y .t .X .", "label": "", "metadata": {}, "score": "86.83966"}
{"text": "[ 0001 ] This application claims the benefit of Japanese Patent Application Nos .2009 - 121061 , filed May 19 , 2009 , and 2010 - 111798 , filed May 14 , 2010 , which are hereby incorporated by reference herein in their entirety .", "label": "", "metadata": {}, "score": "86.882774"}
{"text": "1 illustrates a server - client system according to one embodiment of the present invention .A client terminal 11 is connected to web servers 13a and 13b via the Internet 12 .An operating system ( OS ) 21 , web browser 22 , and other various application programs 23 are installed in a storage medium of the client terminal 11 .", "label": "", "metadata": {}, "score": "87.14667"}
{"text": "The keyboard 25 is a tool for inputting a piece of fixed data occurring at a high frequency while changing the fixed data to a control sentence .The display 16 is a tool for helping the keyboard 25 and the CPU 24 to perform the input / output operations .", "label": "", "metadata": {}, "score": "87.57269"}
{"text": "Collected business days include the periods of both D1 and D2 .Each sequence is composed of stock brand code , date , opening price , lowest price , highest price , closing price , and turnover .This experiment focuses on the sequences of opening price .", "label": "", "metadata": {}, "score": "87.774704"}
{"text": "Extending to Relay Tracking in Visual Sensor Networks .To demonstrate the potential application of the proposed algorithm , we evaluate its relay tracking performance in visual sensor networks .cat - project .at/ ) .There are four cameras , and their fields of view are slightly overlapping .", "label": "", "metadata": {}, "score": "88.46643"}
{"text": "hasPrev2 . true if the \" tail \" contains 3 packets ( only valid if prev1IsLiteral is true ) .optPrev2 . uncompressed size of the substring encoded by all packets except the \" tail \" ( only valid if prev1IsLiteral and hasPrev2 are true ) .", "label": "", "metadata": {}, "score": "88.61705"}
{"text": "An Activation Method of Topic Dictionary to Expand Training Data for Trend Rule Discovery .IT Research and Development Center , Toshiba Solutions Corporation , 3 - 22 Katamachi , Fuchu , Tokyo 183 - 8512 , Japan .Received 23 August 2013 ; Revised 28 December 2013 ; Accepted 13 January 2014 ; Published 26 February 2014 .", "label": "", "metadata": {}, "score": "88.674644"}
{"text": "The keyword ' ' Strong yen ' ' is replaced with the stock brands ' 'A company ' ' and ' ' B company . ' ' Similarly , the keyword ' ' Rare metal ' ' in the second original transaction is replaced with the stock brands ' ' C company ' ' and ' ' D company . ' ' Also , the keyword ' ' refrigerator ' ' in the third one is replaced with ' ' E company .", "label": "", "metadata": {}, "score": "89.601845"}
{"text": "In contrast , our algorithm performs well in terms of position estimation and scale adaptation .The sequences , Car 11 and David Indoor , in Figure 7 are from the work by Ross et al .[ 3 ] with the frame numbers 659 and 462 of a size of 720 \u00d7 480 and 320 \u00d7 240 .", "label": "", "metadata": {}, "score": "89.64462"}
{"text": "The data compressing apparatus according to claim 1 in which the occurrence frequency table is produced by the data producing means by fetching all pieces of input data to be compressed or a piece of input data to be compressed .The data compressing apparatus according to claim 1 in which the compressed data is rewritten by the data converting means while the occurrence frequency table is renewed by the data producing means .", "label": "", "metadata": {}, "score": "89.662575"}
{"text": "Pattern Anal .Mach .Intel .[ Google Scholar ] .Mei , X. ; Ling , H. Robust visual tracking and vehicle classification via sparse representation .IEEE Trans .Pattern Anal .Mach .Intel .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "90.40679"}
{"text": "The subprocesses 2 , 3 , and 4 predict attractive evaluation objects based on the trend rules .This paper deals with a prediction task of attractive stock brands .The task defines three classes ' ' Drop , ' ' ' ' Steady , ' ' and ' ' Rise . ' '", "label": "", "metadata": {}, "score": "90.45728"}
{"text": "View at Google Scholar Tools . by Hieu Hoang , Alexandra Birch , Chris Callison - burch , Richard Zens , Marcello Federico , Nicola Bertoldi , Chris Dyer , Brooke Cowan , Wade Shen , Christine Moran , Ond\u0159ej Bojar , Alexandra Constantin , Evan Herbst , 2007 . \" ...", "label": "", "metadata": {}, "score": "91.260475"}
{"text": "We take the challenging sequence , Car 11 as an example .The corresponding patch size is a quarter of the whole , and other parameter settings are the same as those described in the beginning of this section .The corresponding ACE and ORE are also provided .", "label": "", "metadata": {}, "score": "91.73721"}
{"text": "Zhang et al .[ 9 ] propose a method predicting stock market indexes based on tweets .It analyzes correlations between the stock market indexes and two collective emotions such as ' ' Fear ' ' and ' ' Hope . ' '", "label": "", "metadata": {}, "score": "92.20456"}
{"text": "Lightweight Data Compression in Wireless Sensor Networks Using Huffman Coding .Received 5 August 2013 ; Revised 22 November 2013 ; Accepted 10 December 2013 ; Published 23 January 2014 .Academic Editor : Wan - Young Chung .Copyright \u00a9 2014 Henry Ponti Medeiros et al .", "label": "", "metadata": {}, "score": "92.24992"}
{"text": "System memory 36 includes a read only memory ( ROM ) 40 that stores a basic input / output system ( BIOS ) containing start - up routines for computer 30 , and a random access memory ( RAM ) 42 .", "label": "", "metadata": {}, "score": "93.43057"}
{"text": "Conflict of Interests .The authors declare that there is no conflict of interests regarding the publication of this paper .Acknowledgments .This work was partially supported by CNPq , CAPES , and FAPEAM ( Brazil ) .Part of this work was presented ( in Portuguese ) in the 31st", "label": "", "metadata": {}, "score": "93.778786"}
{"text": "p .i .S .p .i .p .i . max .p .e . w .t .i .p .C .i . )e . w .i .p .C .", "label": "", "metadata": {}, "score": "93.91789"}
{"text": "TABLE 4 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ bit width of contents ofidentification position positional positionalcode ( decimal ) information information______________________________________00 0 , 1 1 bit.sup . 0 , 1010 0 to 2 2 bits 00 , 01 , 10 , 11011 6 to 13 3 bits 000 , 001 , 010 , 011 , 100 , 101 , 110 , 111,1 14 to 255 8 bits 00000000 to 11111111 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "95.25169"}
{"text": "Proceedings of 2011IEEE Conference on Computer Vision and Pattern Recognition ( CVPR ) , Colorado Springs , CO , USA , 20 - 25 June 2011 ; pp .1313 - 1320 .Li , H. ; Shen , C. ; Shi , Q. Real - Time Visual Tracking Using Compressive Sensing .", "label": "", "metadata": {}, "score": "95.32354"}
{"text": "In this table , the topic dictionary has six or more relations between keywords and evaluation objects .The relation in the second line shows that the stock price of ' 'A company ' ' is related to ' ' Strong yen ' ' and ' ' Currency exchange . ' ' Also , the relation in the fifth line shows that the one of ' ' C company ' ' is related to ' ' Nickel ' ' and ' ' Rare metal . ' ' Similarly , the eighth line shows that ' ' Eco - point ' ' and ' ' Refrigerator ' ' relate to the one of ' ' E company . ' ' Eco - point is the policy that encourages consumers to replace old home appliance with ecological new one and that was performed in Japan .", "label": "", "metadata": {}, "score": "95.7475"}
{"text": "That is , this paper focuses on two classes ' ' Drop ' ' and ' ' Rise . ' 'On the other hand , it does not care about the difference of the classes .This is because it is difficult to predict directions where stock prices change by using only limited text information and the notification of stock brands changing the prices brings useful information to the stock traders even if the directions are not identified .", "label": "", "metadata": {}, "score": "96.699425"}
{"text": "T .i .T .A . eval .i .A . g .t .i .A . eval .i .A . g .t .i . where .c . eval .i .", "label": "", "metadata": {}, "score": "96.7529"}
{"text": "Qualitative Evaluation .Qualitative analysis and discussions are provided as follows .The visual challenges include heavy occlusion , illumination change , scale change , fast motion , cluttered background , pose variation , motion blur and low contrast .The frame numbers of the sequences are 898 and 819 of a size of 320 \u00d7 240 .", "label": "", "metadata": {}, "score": "98.10701"}
{"text": "J .w .t .i . ) min .w . k . i . w . k . i . )T .w . k . i . )c . k .s . max .l . k . i . w . k . i . )", "label": "", "metadata": {}, "score": "98.72776"}
{"text": "X .t .Y .t . )p .y .t .X .t . )p .X .t .X .t .p .X .t .Y .t .d .", "label": "", "metadata": {}, "score": "99.43288"}
{"text": "This section introduces some related works based on financial data composed of numerical data and text data .Antweiler and Frank [ 7 ] investigate relationships between the stock data of 45 companies in Dow Jones Industrial Average ( DJIA ) and more than 150 million messages .", "label": "", "metadata": {}, "score": "102.26143"}
{"text": "The CPU 49 is used to control the input and output of the editors .41 to 44 , the memory 45 , the EPROM 46 , the display 47 , the keyboard 48 , the input data file 50 and the compressed data file 51 .", "label": "", "metadata": {}, "score": "103.53149"}
{"text": "t .t .i .t .i . )T .M .M .B . t .t .t .t .p .t .i .t .i . )T .d .", "label": "", "metadata": {}, "score": "104.41699"}
{"text": "Computer 30 also includes a hard drive 44 , a floppy drive 46 , and CD ROM drive 48 that are connected to system bus 38 by respective interfaces 50 , 52 , 54 .Hard drive 44 , floppy drive 46 , and CD ROM drive 48 contain respective computer - readable media disks 56 , 58 , 60 that provide non - volatile or persistent storage for data , data structures and computer - executable instructions .", "label": "", "metadata": {}, "score": "108.189896"}
{"text": "Copyright \u00a9 2014 Shigeaki Sakurai et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "117.487274"}
