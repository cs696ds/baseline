{"text": "This is a recipe to train word n - gram language models using the newswire text provided in the English Gigaword corpus ( 1200 M words of NYT , APW , AFE , XIE ) .It also prepares dictionaries needed to use the LMs with the HTK and Sphinx speech recognizers .", "label": "", "metadata": {}, "score": "31.483242"}
{"text": "This paper presents methods to combine large language models trained from diverse text sources and applies them to a stateof - art French - English and Arabic - English machine translation system .We show gains of over 2 BLEU points over a strong baseline by using continuous space language models in re - ranking . \" ...", "label": "", "metadata": {}, "score": "32.871666"}
{"text": "I. . ... andard word alignment and translation models described in Section IV - A1 .The system also used two large language models , both trained on the same data : a 4-gram language model using modified Kneser - Ney smoothing , and a suffix array language model with arbitrary history l .. \" ...", "label": "", "metadata": {}, "score": "33.86858"}
{"text": "This allows the use of a much wider range of parallel corpora for training , and can be combined with a standard phrase - table using conventional smoothing methods .Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase - based system .", "label": "", "metadata": {}, "score": "35.44203"}
{"text": "We 'll use the word alignments to create a translation grammar similar to the Chinese one shown in Step 1 .The translation grammar is created by looking for where the foreign language phrases from the test set occur in the training set , and then using the word alignments to figure out which foreign phrases are aligned .", "label": "", "metadata": {}, "score": "35.834763"}
{"text": "4.2 Collocation Correction with Phrase - based SMT We implement our approach in the fram ... . \" ...We report on efforts to build large - scale translation systems for eight European language pairs .We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation .", "label": "", "metadata": {}, "score": "36.295174"}
{"text": "We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation . \" ...In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .", "label": "", "metadata": {}, "score": "36.911034"}
{"text": "These methods use substantially less space than all known approaches and allow n - gram probabilities or counts to be retrieved in constant time , at speeds comparable to modern language modeling toolkits .Our basic ... \" .We present three novel methods of compactly storing very large n - gram language models .", "label": "", "metadata": {}, "score": "37.59468"}
{"text": "In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .We explore sourceto - target models with phrase - level as well as sentence - level scoring and target - to - source models with scoring on phrase level only .", "label": "", "metadata": {}, "score": "37.597847"}
{"text": "This paper presents an efficient low - memory method for constructing high - order approximate n - gram frequency counts .The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .", "label": "", "metadata": {}, "score": "37.805344"}
{"text": "This paper presents an efficient low - memory method for constructing high - order approximate n - gram frequency counts .The method is based on a deterministic streaming algorithm which efficiently computes approximate frequency counts over a stream of data while employing a small memory footprint .", "label": "", "metadata": {}, "score": "37.805344"}
{"text": "With this small model , there are many untranslated words , and the quality of the translations is very low .In the next steps , we 'll show you how to train a model for a new language pair , using a larger training corpus that will result in higher quality translations .", "label": "", "metadata": {}, "score": "38.2135"}
{"text": "5 ] and applied in their respective translation systems for phrase table smoothing .Chiang et al .[ 15 ] suggested morphology - based and provenance - based improvements to the Koehn - Och - Marcu method recently ...Graff , David , et al .", "label": "", "metadata": {}, "score": "38.55883"}
{"text": "Most translation models also make use of an n - gram language model as a way of assigning higher probability to hypothesis translations that look like fluent examples of the target language .Joshua provides support for n - gram language models , either through a built in data structure , or through external calls to the SRI language modeling toolkit ( srilm ) .", "label": "", "metadata": {}, "score": "39.278458"}
{"text": "The method is based on a deterministic streaming algorithm which efficiently computes ... \" .In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems .We present an efficient low - memory method for constructing high - order approximate n - gram frequency counts .", "label": "", "metadata": {}, "score": "39.99366"}
{"text": "We show that this method easily scales to billion - word monolingual corpora using a conventional ( 8 GB RAM ) desktop machine .Statistical machine translation experimental results corroborate that the resulting high - n approximate small language model is as effective as models obtained from other count pruning methods . ... count - based pruning on SMT performance using EAN corpus .", "label": "", "metadata": {}, "score": "41.587845"}
{"text": "This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "41.85122"}
{"text": "This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "41.85122"}
{"text": "One effective way to do that is to post - edit the translations produced by a vanilla RBMT system using a specially - trained statistical machine translation ( SMT ) system .Our experiments indicate that this method is just as effective as manual customization of system dictionaries in reducing the need for manual post - editing . ... entence - length feature .", "label": "", "metadata": {}, "score": "41.99852"}
{"text": "We show that our approaches are simple to implement and can easily be combined with pruning and quantization to achieve additional reductions in the size of the language model .or tasks such as machine translation and speech recognition have shown that increasing the size of the model is a constructive way of improving the performance on those tasks .", "label": "", "metadata": {}, "score": "42.228165"}
{"text": "All models are used during search , i.e. they are incorporated directly into the log - linear model combination of the decoder .Phrase table smoothing with triplet lexicon models and with discriminative word lexicons are novel contributions .We also propose a new regularization technique for IBM model 1 by means of the Kullback - Leibler divergence with the empirical unigram distribution as regularization term .", "label": "", "metadata": {}, "score": "42.44126"}
{"text": "We begin with the context of the current research , and then move to a formal problem description and an overview of the four main subproblems : translational equivalence modeling , mathematical modeling , parameter estimation , and decoding .Along the way , we present a taxonomy of some different approaches within these areas .", "label": "", "metadata": {}, "score": "42.706963"}
{"text": "I need to come up with a function that can take in document frequencies of an n - gram and all its component ( n - k)-grams and return a more meaningful measure of how much this phrase will distinguish the parent document from the rest .", "label": "", "metadata": {}, "score": "43.506744"}
{"text": "Our key assumption is that collocation errors are often caused by semantic similarity in the first language ( L1language ) of the writer .An analysis ... \" .We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora .", "label": "", "metadata": {}, "score": "43.77455"}
{"text": "This example contains 1,000 sentences of Urdu - English data ( the full dataset is available as part of the Indian languages parallel corpora with 100-sentence tuning and test sets with four references each .Running the pipeline requires two main steps : data preparation and invocation .", "label": "", "metadata": {}, "score": "44.24757"}
{"text": "The recent availability of large collections of text such as the Google 1 T 5-gram corpus ( Brants and Franz , 2006 ) and the Gigaword corpus of newswire ( Graff , 2003 ) have made it possible to build language models that incorporate counts of billions of n - grams .", "label": "", "metadata": {}, "score": "44.359863"}
{"text": "en.tok.lc 1411589 41042110 training / training .es.tok.lc 671429 16721564 training / subsampled / subsample .en.tok.lc671429 17670846 training / subsampled / subsample .es.tok.lc .Step 3 : Create word alignments .Before extracting a translation grammar , we first need to create word alignments for our parallel corpus .", "label": "", "metadata": {}, "score": "44.487568"}
{"text": "es.tok.lc.grammar .You will also need to create a small \" glue grammar \" , in a file called model / hiero .glue that contains these rules that allow hiero - style grammars to reach the goal state : .Step 6 : Run minimum error rate training .", "label": "", "metadata": {}, "score": "44.682793"}
{"text": "If you want a language model built from the target side of your training data , you 'll also need to pass in the training corpus ( --corpus ) .You can also specify an arbitrary number of additional language models with one or more --lmfile flags .", "label": "", "metadata": {}, "score": "44.70098"}
{"text": "An analysis of a large corpus of annotated learner English confirms this assumption .We evaluate our approach on real - world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance , homophones , and WordNet synonyms . ... where f denotes a foreign phrase in the L1 language .", "label": "", "metadata": {}, "score": "44.701447"}
{"text": "Before decoding the test set , you 'll need to extract a translation grammar for the foreign phrases in the test set test / newstest2009 .es.tok.lc : . /model\\ test / newstest2009 .es.tok.lc.grammar.raw \\ test / newstest2009 .es.tok.lc & .", "label": "", "metadata": {}, "score": "44.740807"}
{"text": "This paper [ Chen&Goodman ] is a pretty good summary of many of them .In particular , you sound like you might be interested in the Kneser - Ney smoothing algorithm that works in the way you suggest ( backing off to lower length n - grams ) .", "label": "", "metadata": {}, "score": "45.101112"}
{"text": "More detail can be found in Chiang ( 2007 ) [ PDF ] .SAMT grammars make use of a source- or target - side parse tree on the training data , projecting constituent labels down on the phrasal alignments in a variety of configurations .", "label": "", "metadata": {}, "score": "45.229294"}
{"text": "/usr / bin / perl # # truecase - map .cat training / training . map .Finally , recase the lowercased 1-best translation by running the SRILM disambig program , which takes the map of alternative capitalizations , creates a confusion network , and uses truecased LM to find the best path through it : .", "label": "", "metadata": {}, "score": "45.796387"}
{"text": "These steps are discussed below , after a few intervening sections about high - level details of the pipeline .Grammar options .Joshua can extract two types of grammars : Hiero - style grammars and SAMT grammars .As described on the file formats page , both of them are encoded into the same file format , but they differ in terms of the richness of their nonterminal sets .", "label": "", "metadata": {}, "score": "46.18711"}
{"text": "The grammar extraction step takes three pieces of data : ( 1 ) the source - language training corpus , ( 2 ) the target - language training corpus ( parsed , if an SAMT grammar is being extracted ) , and ( 3 ) the alignment file .", "label": "", "metadata": {}, "score": "46.698715"}
{"text": "Our basic approach generates an explicit minimal perfect hash function , that maps all n - grams in a model to distinct integers to enable storage of associated values .Extensions of this approach exploit distributional characteristics of n - gram data to reduce storage costs , including variable length coding of values and the use of tiered structures that partition the data for more efficient storage .", "label": "", "metadata": {}, "score": "46.744034"}
{"text": "The -order 3 tells srilm to produce a trigram language model .You can set this to a higher value , and srilm will happily output 4-gram , 5-gram or even higher order language models .The -kndiscount tells SRILM to use modified Kneser - Ney discounting as its smoothing scheme .", "label": "", "metadata": {}, "score": "48.126625"}
{"text": "Sentence alignment .In this exercise , we 'll start with an existing sentence - aligned parallel corpus .Download this tarball , which contains a Spanish - Engish parallel corpus , along with a dev and a test set : data.tar.gz .", "label": "", "metadata": {}, "score": "48.418743"}
{"text": "Grammar extraction with Thrax .If you jump to this step , you 'll need to provide an aligned corpus ( --alignment ) along with your parallel data .TUNE : Tuning .With this option , you need to specify a grammar ( --grammar ) or separate tune ( --tune - grammar ) and test ( --test - grammar ) grammars .", "label": "", "metadata": {}, "score": "48.571175"}
{"text": "Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase .", "label": "", "metadata": {}, "score": "48.60095"}
{"text": "en.trigram.lm .( Note : the above assumes that you are on a 64-bit machine running Mac OS X. If that 's not the case , your path to ngram - count will be slightly different . )This will train a trigram language model on the English side of the parallel corpus .", "label": "", "metadata": {}, "score": "49.088707"}
{"text": "Statistical machine translation experimental results corroborate that the resulting high - n approximate language model is as effective as conventional higher order language models .Hierarchy - based Partition Models : Using Classification Hierarchies to .We propose a novel machine learning technique that can be used to estimate probability distributions for categorical random variables that are equipped with a natural set of classification hierarchies , such as words equipped with word class hierarchies , wordnet hierarchies , and suffix and affix hierarchies .", "label": "", "metadata": {}, "score": "49.406837"}
{"text": "Both grammar formats are extracted with the Thrax software .By default , the Joshua pipeline extract a Hiero grammar , but this can be altered with the --type samt flag .Other high - level options .The following command - line arguments control run - time behavior of multiple steps : . --threads N ( 1 ) .", "label": "", "metadata": {}, "score": "49.775467"}
{"text": "We show that under certain common formulations , the batchprocessing analytic framework can be decomposed into a sequential series of updates , using as an example the task of gender classification .Once in a streaming framework , and motivated by large data sets generated by social media services , we present novel results in approximate counting , showing its applicability to space efficient streaming classification .", "label": "", "metadata": {}, "score": "50.44535"}
{"text": "I do n't really know how how you might integrate them with IDF scores , or even if that 's really what you want to do .Most of these smoothing models look like they work with moving probability mass around from term to term in elaborate ways , to make the model be able to recognize a language better .", "label": "", "metadata": {}, "score": "50.842964"}
{"text": "Experimental results on the test data of the previous campaign are presented . ... to Canadian universities for research and education purposes . \" ...Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .", "label": "", "metadata": {}, "score": "51.866108"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "51.938446"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "51.938446"}
{"text": "Up to six state - of - the - art statistical phrase - based translation systems from different project partners were combined in the experiments .Significant improvements in translation quality from Spanish to English and from English to Spanish in comparison with the best of the individual MT systems were achieved under official evaluation conditions .", "label": "", "metadata": {}, "score": "52.39839"}
{"text": "FEEDBACK .Tools . \" ...Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .", "label": "", "metadata": {}, "score": "52.42735"}
{"text": "When you are aligning tens of millions of words worth of data , the word alignment process will take several hours to complete .While it is running , you can skip ahead and complete step 4 , but not step 5 .", "label": "", "metadata": {}, "score": "52.588448"}
{"text": "When the grammar is extracted , it is compressed and placed at RUNDIR / grammar .gz .Language model .Before tuning can take place , a language model is needed .A language model is always built from the target side of the training corpus unless --no - corpus - lm is specified .", "label": "", "metadata": {}, "score": "52.833138"}
{"text": "The results show that the proposed estimator outperforms modified Kneser - Ney smoothing in terms of perplexity on unseen data .Keywords : machine learning ; categorical variables ; classification hierarchies ; language modelling ; statistical estimation We propose a novel machine learning technique that can be used to estimate probability distributions for categorical random variables that are equipped with a natural set of classification hierarchies , such as words equipped with word class hierarchies , wordnet hierarchies , and suffix and affix hierarchies .", "label": "", "metadata": {}, "score": "52.96235"}
{"text": "You can score your output using the JoshuaEval class , Joshua 's built - in scorer : .en.output \\ -ref dev / dev2006 .en.small \\ -m BLEU 4 closest Tools . \" ...This paper presents methods to combine large language models trained from diverse text sources and applies them to a stateof - art French - English and Arabic - English machine translation system .", "label": "", "metadata": {}, "score": "53.06788"}
{"text": "Feature function weights in the log - linear model are set using Och 's minimum ... . \" ...Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .", "label": "", "metadata": {}, "score": "53.393658"}
{"text": "and then rerun the pipeline with the same invocation .Memory usage is a major consideration in decoding with Joshua and hierarchical grammars .In particular , SAMT grammars often require a large amount of memory .Many steps have been taken to reduce memory usage , including beam settings and test - set- and sentence - level filtering of grammars .", "label": "", "metadata": {}, "score": "53.453663"}
{"text": "We can do recasing using SRILM , and can do detokenization with a perl script .To build a recasing model first train a language model on true cased English text : .$ SRILM / bin / macosx64/ngram - count \\ -unk \\ -order 5 \\ -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -kndiscount5 \\ -text training / training .", "label": "", "metadata": {}, "score": "53.70365"}
{"text": "In the first edition of the Arabic Gigaword corpus , a simpler three - character - code scheme was used to identify both the source and the language .The new convention allows us to distinguish data sets by source and language more naturally when a single newswire provider distributes data in multiple languages .", "label": "", "metadata": {}, "score": "54.168747"}
{"text": "( Sorry I ca n't provide you with a link to this article , but here 's the citation at least ! )Chen , Stanley F. and Goodman , Joshua .An empirical study of smoothing techniques for language modeling .", "label": "", "metadata": {}, "score": "54.445045"}
{"text": "Run the example model .To test to make sure that the decoder is installed properly , we 'll translate 5 sentences using a small translation model that loads quickly .The sentences that we will translate are contained in example / example .", "label": "", "metadata": {}, "score": "54.480526"}
{"text": "Other arguments are as follows .This determines the language model code that will be used when decoding .These implementations are described in their respective papers ( PDFs : KenLM , BerkeleyLM ) .--lmfile FILE .Specifies a pre - built language model to use when decoding .", "label": "", "metadata": {}, "score": "54.557533"}
{"text": "In a study by Chen and Goodman ( 1998 ) , the effect of varying n - gram orders on the performance of a language model was compared .As expected , they found that 4-grams and 5-grams significantly outperform trigram models when using a very large data set ( approximately 1e+06 sentences ) .", "label": "", "metadata": {}, "score": "54.76791"}
{"text": "The three numbers listed after each translation rules are negative log probabilities that signify , in order : .You can use the grammar to translate the test set by running .config.srilm \\ example / example .test.in \\ example / example .", "label": "", "metadata": {}, "score": "55.095398"}
{"text": "The small translation grammar contains 15,939 rules -- you can get the count of the number of rules by running gunzip -c example / example .The first part of the rule is the left - hand side non - terminal .", "label": "", "metadata": {}, "score": "55.134468"}
{"text": "For the 1.5 billion n - grams of Gigaword , for example , we can store full count information at a cost of 1.66 bytes per n - gram ( around 30 % of the cost when using the current stateof - the - art approach ) , or quantized counts for 1.41 bytes per n - gram .", "label": "", "metadata": {}, "score": "55.148212"}
{"text": "but the equations look pretty complicated .- adi92 Jun 11 ' 10 at 17:19 .But I guess , I could somehow adapt some of them to do something meaningful with IDFs as well .I really do n't want to do something too elaborate or complicated .", "label": "", "metadata": {}, "score": "55.370132"}
{"text": "Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .SMT has made tremendous strides in less than two decades , and many popular techniques have only emerged within the last few years .", "label": "", "metadata": {}, "score": "55.443584"}
{"text": "At the tuning step , an LM is built from the target side of the training data ( unless --no - corpus - lm is specified ) .This controls which code is used to build it .The default is a BerkeleyLM java class that computes a Kneser - Ney LM with a constant discounting and no count thresholding .", "label": "", "metadata": {}, "score": "55.453453"}
{"text": "This file contains the n - best translations , under the model .The first 10 lines that you see above are 10 best translations of the first sentence .Each line contains 4 fields .To get the 1-best translations for each sentence in the test set without all of the extra information , you can run the following command : . nbest.srilm.out \\ example / example .", "label": "", "metadata": {}, "score": "55.52234"}
{"text": "If you have an already - computed alignment , you can pass that to the script using this flag .Note that , in this case , you will want to skip data preparation and alignment using --first - step thrax ( the first step after alignment ) and also to specify --no - prepare - data so as not to retokenize the data and mess with your alignments .", "label": "", "metadata": {}, "score": "56.174576"}
{"text": "When SAMT grammars are being built ( --type samt ) , the target side of the training data must be parsed .The pipeline assumes your target side will be English , and will parse it for you using the Berkeley parser , which is included .", "label": "", "metadata": {}, "score": "56.459927"}
{"text": "es.tok.lc cat es - en / test / newstest2009 .en.tok.lc .Subsampling ( optional ) .Sometimes the amount of training data is so large that it makes creating word alignments extremely time - consuming and memory - intesive .We therefore provide a facility for subsampling the training corpus to select sentences that are relevant for a test set .", "label": "", "metadata": {}, "score": "56.584515"}
{"text": "You 'll notice that your output is all lowercased and has the punctuation split off .In order to make the output more readable to human beings ( remember us ? ) , it 'd be good to fix these problems and use proper punctuation and spacing .", "label": "", "metadata": {}, "score": "56.731293"}
{"text": "This method allows compression while retaining O(1 ) access to query the model .The first step in the compression is to encode the ranks array using ... . \" ...In this paper , we explore a streaming algorithm paradigm to handle large amounts of data for NLP problems .", "label": "", "metadata": {}, "score": "56.762596"}
{"text": "en.tok.lc -rps 1 # references per sentence -p mert / params .txt # parameter file -m BLEU 4 closest # evaluation metric and its options -maxIt 10 # maximum MERT iterations -ipi 20 # number of intermediate initial points per iteration -cmd mert / decoder_command # file containing commands to run decoder -decOut mert / news - dev2009 .", "label": "", "metadata": {}, "score": "56.943264"}
{"text": "The context of a whole corpus of automatic translations rather than a single sentence is taken into account in order to achieve high alignment quality .The confusion network is rescored with a special language model , and the consensus translation is extracted as the best path .", "label": "", "metadata": {}, "score": "57.196476"}
{"text": "The Berkeley Aligner - this software is used to align words across sentence pairs in a bilingual parallel corpus .Word alignment takes place before extracting an SCFG .After you have downloaded the srilm tar file , type the following commands to install it : . mkdir srilm mv srilm.tgz srilm/ cd srilm/ tar xfz srilm.tgz make .", "label": "", "metadata": {}, "score": "57.486996"}
{"text": "es.tok.lc \\ test / newstest2009 .output.nbest .After the decoder has finished , you can extract the 1-best translations from the n - best list using the following command : . output.nbest \\ test / newstest2009 .output.1best .", "label": "", "metadata": {}, "score": "57.90484"}
{"text": "It 's located in the tarball under the scripts directory .To use it type the following commands : . en.tok .Normalization .After tokenization , we recommend that you normalize your data by lowercasing it .The system treats words with variant capitalization as distinct , which can lead to worse probability estimates for their translation , since the counts are fragmented .", "label": "", "metadata": {}, "score": "57.969025"}
{"text": "es.tok.lc : . /model\\ mert / news - dev2009 .es.tok.lc.grammar.raw \\ dev / news - dev2009 .es.tok.lc & .Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u mert / news - dev2009 .", "label": "", "metadata": {}, "score": "58.183098"}
{"text": "These chunked files are all created in a subdirectory of RUNDIR / data / train / splits , named corpus .LANG.0 , corpus .LANG.1 , and so on .The pipeline parameters affecting alignment are : .Which aligner to use .", "label": "", "metadata": {}, "score": "58.19468"}
{"text": "conf ): .# # word - align .conf # # ---------------------- # # This is an example training script for the Berkeley # # word aligner .In this configuration it uses two HMM # # alignment models trained jointly and then decoded # # using the competitive thresholding heuristic .", "label": "", "metadata": {}, "score": "58.606407"}
{"text": "A language model built from the target side of the training data is placed at RUNDIR / lm .gz .Interlude : decoder arguments .Running the decoder is done in both the tuning stage and the testing stage .A critical point is that you have to give the decoder enough memory to run .", "label": "", "metadata": {}, "score": "58.668713"}
{"text": "Alignment .You might want to start here if you want to skip data preprocessing .PARSE : Parsing .This is only relevant for building SAMT grammars ( --type samt ) , in which case the target side ( --target ) of the training data ( --corpus ) is parsed before building a grammar .", "label": "", "metadata": {}, "score": "58.711975"}
{"text": "This breaks down , when I observed the cases above where high - idf phrases were made up very low idf terms . - adi92 Jun 11 ' 10 at 17:05 .Is your idf computed just of the corpus of documents you 're interested in ?", "label": "", "metadata": {}, "score": "58.730858"}
{"text": "Once the parsing is complete , there will be two parsed files : .RUNDIR / data / train / corpus.en.parsed : this is the mixed - case file that was parsed .RUNDIR / data / train / corpus.parsed.en : this is a leaf - lowercased version of the above file used for grammar extraction .", "label": "", "metadata": {}, "score": "59.00934"}
{"text": "TrueCase.5gram.lm \\ -keep - unk \\ -order 5 \\ -map model / lm / true - case . map \\ -text test / mt09 .output.1best.recased .Where strip - sent - tags .perl is : .Step 9 : Score the translations .", "label": "", "metadata": {}, "score": "59.161713"}
{"text": "What would be a good source to cite for trigram models being something of a standard ?What would be a good source to cite for the claim that trigram models outperform more complex models such as PCFGs ?I 've gone ahead and started a bounty for this question : even just one good reference will help ! @jlovegren Thanks , although I 'm not sure that they actually compare trigram models to the others empirically in that book , or even argue that they outperform more complex language models .", "label": "", "metadata": {}, "score": "59.162903"}
{"text": "If you have a tuned model file , you can test new corpora by passing in a test corpus with references ( --test ) .You 'll need to provide a run name ( --name ) to store the results of this run , which will be placed under test / NAME .", "label": "", "metadata": {}, "score": "59.193924"}
{"text": "All of the documents in the first edition of the Arabic Gigaword corpus can be mapped to the same documents in this edition by changing the prefix of DOC IDs and file names as below .The upper case letters are used for the DOC IDs ; the lower case letters are used for the file and directory names .", "label": "", "metadata": {}, "score": "59.22131"}
{"text": "This value is required if you start at the grammar extraction step .When alignment is complete , the alignment file can be found at RUNDIR / alignments / training . align .It is parallel to the training corpora .There are many files in the alignments/ subdirectory that contain the output of intermediate steps .", "label": "", "metadata": {}, "score": "59.405293"}
{"text": "The results show that the proposed estimator outperforms modified Kneser - Ney smoothing in terms of perplexity on unseen data .Keywords : machine learning ; categorical variables ; classification hierarchies ; language modelling ; statistical estimation .dc.contributor.corporation .Copenhagen Business School .", "label": "", "metadata": {}, "score": "59.486565"}
{"text": "es.tok.lc.grammar.raw \\ -o test / newstest2009 .es.tok.lc.grammar .Once the grammar extraction has completed , you can edit the joshua.config file for the test set .cp mert / joshua .config .ZMERT.final test / joshua . config .es.tok.lc.grammar .", "label": "", "metadata": {}, "score": "59.850853"}
{"text": "To find the foreign phrases in the test set , we first create an easily searchable index , called a suffix array , for the training data .java -Xmx500 m -cp $ JOSHUA / bin/ \\ joshua.corpus.suffix_array.Compile \\ training / subsampled / subsample .", "label": "", "metadata": {}, "score": "60.1467"}
{"text": "There are 423 files , totaling approximately 1.4 GB in compressed form ( 5,359 MB uncompressed , and 1,591,983 K - words ) .TOTAL .All text files in this corpus have been converted to UTF-8 character encoding .Owing to the use of UTF-8 , the SGML tagging within each file shows up as lines of single - byte - per - character ( ASCII ) text , whereas lines of actual text data , including article headlines and datelines , contain a mixture of single - byte and multi - byte characters .", "label": "", "metadata": {}, "score": "60.26196"}
{"text": "We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "60.807602"}
{"text": "If you successfully installed srilm in Step 1 , then you should be able to train a language model with the following command : . mkdir -p model / lm $ SRILM / bin / macosx64/ngram - count \\ -order 3 \\ -unk \\ -kndiscount1 -kndiscount2 -kndiscount3 \\ -text training / training .", "label": "", "metadata": {}, "score": "61.18832"}
{"text": "Multiple --corpora files are concatenated in the order they are specified .Multiple --tune and --test flags are not currently allowed .Normalizing punctuation and text ( e.g. , removing extra spaces , converting special quotations ) .There are a few language - specific options that depend on the file extension matching the two - letter ISO 639 - 1 designation .", "label": "", "metadata": {}, "score": "61.272297"}
{"text": "Old .New .AFA .AFP_ARB _ .ALH .HYT_ARB _ .ANN .NHR_ARB . XIA .XIN_ARB _ .Samples .For an example of the data in this corpus , please examine this screenshot which is an image of the text from a single file .", "label": "", "metadata": {}, "score": "61.428993"}
{"text": "Web Download .Philadelphia : Linguistic Data Consortium , 2006 .Introduction .Arabic Gigaword Second Edition was produced by Linguistic Data Consortium ( LDC ) catalog number LDC2006T02 and ISBN 1 - 58563 - 371 - 2 .This is a comprehensive archive of newswire text data that has been acquired from Arabic news sources by the Linguistic Data Consortium ( LDC ) , at the University of Pennsylvania .", "label": "", "metadata": {}, "score": "61.479046"}
{"text": "You can see how much the subsampling step reduces the training data , by yping wc -lw es - en / full - training / training . tok.lc es - en / full - training / subsampled / subsample . tok.lc : .", "label": "", "metadata": {}, "score": "61.581406"}
{"text": "MERT is a method for setting the weights of the different feature functions the translation model to maximize the translation quality on the dev set .Translation quality is calculated according to an automatic metric , such as Bleu .Our implementation of MERT allows you to easily implement some other metric , and optimize your paramters to that .", "label": "", "metadata": {}, "score": "61.67171"}
{"text": "I used the top 5 K , 20 K , and 64 K words occurring in the training text as vocabularies .Both verbalized punctuation ( VP ) and non - verbalized punctuation ( NVP ) LMs are built .Here are some OOV% and perplexity results measured on three different held - out evaluation test sets : I am trying to use IDF scores to find interesting phrases in my pretty huge corpus of documents .", "label": "", "metadata": {}, "score": "61.675888"}
{"text": "\" Other \" Gigaword \" corpora ( in English and Chinese ) had a fourth category , \" advis \" ( for \" advisory \" ) , which applied to DOCs that contain text intended solely for news service editors , not the news - reading public .", "label": "", "metadata": {}, "score": "62.584457"}
{"text": "Testing .For each of the tuner runs , Joshua takes the tuner output file and decodes the test set .Afterwards , by default , minimum Bayes - risk decoding is run on the 300-best output .This step usually yields about 0.3 - 0.5 BLEU points but is time - consuming , and can be turned off with the --no - mbr flag .", "label": "", "metadata": {}, "score": "62.63935"}
{"text": "TrueCase.5gram.lm .Next , you 'll need to create a list of all of the alternative ways that each word can be capitalized .This will be stored in a map file that lists a lowercased word as the key and associates it with all of the variant capitalization of that word .", "label": "", "metadata": {}, "score": "62.758125"}
{"text": "en.tok.lc \\ training / subsampled / training .en.tok.lc-es .tok.lc.align \\ model .This compiles the index that Joshua will use for its rule extraction , and puts it into a directory named model .Extract grammar rules for the dev set .", "label": "", "metadata": {}, "score": "62.765926"}
{"text": "ACL Workshop on SMT , 2007 . \" ...We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "63.34955"}
{"text": "Therefore , each file contains all the usable data received by LDC for the given month from the given news source .All text data are presented in SGML form , using a very simple , minimal markup structure .The file gigaword_a . dtd in the \" dtd \" directory provides the formal \" Document Type Declaration \" for parsing the SGML content .", "label": "", "metadata": {}, "score": "63.361206"}
{"text": "ur , and so on .Assuming no problems arise , this command will run the complete pipeline in about 20 minutes , producing BLEU scores at the end .As it runs , you will see output that looks like the following : .", "label": "", "metadata": {}, "score": "63.560905"}
{"text": "Joshua uses whitespace to delineate words .For many languages , tokenization can be as simple as separating punctation off as its own token .For languages like Chinese , which do n't put spaces around words , tokenization can be more tricky .", "label": "", "metadata": {}, "score": "64.11483"}
{"text": "Installing and running the Joshua Decoder .Note : these instructions are several years out of date .This document gives instructions on how to install and use the Joshua decoder .Joshua is an open - source decoder for parsing - based machine translation .", "label": "", "metadata": {}, "score": "64.21065"}
{"text": "Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .The consensus translation is computed by weighted majority voting on a confusion network , similarly to the well - established ROVER approach of Fiscus for combining speech recognition hypotheses .", "label": "", "metadata": {}, "score": "64.307724"}
{"text": "Given that the English side of the parallel corpus is a relatively small amount of data in terms of language modeling , it only takes a few minutes a few minutes to output the LM .The uncompressed LM is 144 megabytes large ( du -h europarl.en.trigram.lm ) .", "label": "", "metadata": {}, "score": "65.074425"}
{"text": "This is because the Berkeley aligner generally expects to test against a set of manually word - aligned data : . cd es - en / full - training/ mkdir -p example / test .After you 've created the word - align .", "label": "", "metadata": {}, "score": "65.21012"}
{"text": "input/ train .SOURCE train .TARGET tune .SOURCE tune .TARGET test .SOURCE test .TARGET .These files should be parallel at the sentence level ( with one sentence per line ) , should be in UTF-8 , and should be untokenized ( tokenization occurs in the pipeline ) .", "label": "", "metadata": {}, "score": "65.24661"}
{"text": "For those of you who are n't very familiar with Java , the arguments are the following : . -Xmx1 g -- this tells Java to use 1 GB of memory . -cp$ JOSHUA / bin -- this specifies the directory that contains the Java class files .", "label": "", "metadata": {}, "score": "65.256256"}
{"text": "This can be accomplished with the --first - step and --last - step flags , which take as argument a case - insensitive version of the following steps : .FIRST :Data preparation .Everything begins with data preparation .This is the default first step , so there is no need to be explicit about it .", "label": "", "metadata": {}, "score": "65.43489"}
{"text": "The last step .This is the default target of --last - step .We now discuss these steps in more detail .DATA PREPARATION .Data prepare involves doing the following to each of the training data ( --corpus ) , tuning data ( --tune ) , and testing data ( --test ) .", "label": "", "metadata": {}, "score": "66.02101"}
{"text": "# lm config .# tm config .# pruning config .# nbest config .# remote lm server config , we should first prepare remote_symbol_tbl before starting any jobs . /voc.remote.sym ./remote.lm.server.list .# parallel deocoder : it can not be used together with remote lm .", "label": "", "metadata": {}, "score": "66.1294"}
{"text": "The pipeline will notice that it is parsed and will not reparse it .Parsing is affected by both the --threads N and --jobs N options .The former runs the parser in multithreaded mode , while the latter distributes the runs across as cluster ( and requires some configuration , not yet documented ) .", "label": "", "metadata": {}, "score": "66.22696"}
{"text": "Inferring attributes of discourse participants has been treated as a batch - processing task : data such as all tweets from a given author are gathered in bulk , processed , analyzed for a particular feature , then reported as a result of academic interest .", "label": "", "metadata": {}, "score": "66.31842"}
{"text": "To each of these prefixes , a \" .\" is appended , followed by each of SOURCE ( --source ) and TARGET ( --target ) , which are file extensions identifying the languages .The SOURCE and TARGET files must have the same number of lines .", "label": "", "metadata": {}, "score": "66.56537"}
{"text": "- Julie Oct 10 ' 12 at 21:52 .I 've never read the book , so you 've probably gotten it right . - jlovegren Oct 11 ' 12 at 0:17 . 1 Answer 1 .The reason why trigrams can be considered powerful compared to n - grams of higher order , lies in the problem of data sparsity ; when n is higher , data becomes increasingly sparse .", "label": "", "metadata": {}, "score": "66.64956"}
{"text": "And in the current directory , you will see the following files ( among other intermediate files generated by the individual sub - steps ) .data/ train/ corpus.ur corpus.en thrax - input - file tune/ tune.tok.lc.ur tune.tok.lc.en grammar.filtered.gz grammar.glue test/ test.tok.lc.ur test.tok.lc.en grammar.filtered.gz grammar.glue alignments/ 0/ [ berkeley aligner output files ] training.align thrax - hiero . conf thrax.log grammar.gz lm.gz tune/ 1/ decoder_command joshua.config params.txt joshua.log mert.log joshua.config.ZMERT.final final - bleu .", "label": "", "metadata": {}, "score": "66.691574"}
{"text": "( xin_arb ; formally xia ) .The seven - letter codes in the parentheses above consist of the three - character source name IDs and the three - character language code ( \" arb \" ) separated by an underscore ( \" _ \" ) character .", "label": "", "metadata": {}, "score": "67.250015"}
{"text": "You cat then look at the 1-best output file by typing cat example / example .nbest.srilm.out.1best : . the goal of gene scientists is to provide diagnostic tools to found of the flawed genes , are still provide a to stop these genes treatments .", "label": "", "metadata": {}, "score": "67.32444"}
{"text": "To accommodate this kind of variation , the pipeline script allows you to specify both ( a ) the amount of memory used by the Joshua decoder instance and ( b ) the amount of memory required of nodes obtained by the qsub command .", "label": "", "metadata": {}, "score": "67.41973"}
{"text": "N ( 1 ) .This enables parallel operation over a cluster using the qsub command .This feature is not well - documented at this point , but you will likely want to edit the file $ JOSHUA / scripts / training / parallelize / LocalConfig .", "label": "", "metadata": {}, "score": "67.897835"}
{"text": "[ train - copy - ur ] cached , skipping ... ... .This indicates that the caching module has discovered that the step was already computed and thus did not need to be rerun .This feature is quite useful for restarting pipeline runs that have crashed due to bugs , memory limitations , hardware failures , and the myriad other problems that plague MT researchers across the world .", "label": "", "metadata": {}, "score": "67.93922"}
{"text": "( This mode is triggered when $ HADOOP is undefined ) .Theoretically , any grammar extractable on a full Hadoop cluster should be extractable in standalone mode , if you are patient enough ; in practice , you probably are not patient enough , and will be limited to smaller datasets .", "label": "", "metadata": {}, "score": "67.94626"}
{"text": "The default amount of memory is 3100 m , which is likely not enough ( especially if you are decoding with SAMT grammar ) .You can alter the amount of memory for Joshua using the --joshua - mem MEM argument , where MEM is a Java memory specification ( passed to its -Xmx flag ) .", "label": "", "metadata": {}, "score": "68.2036"}
{"text": "You must preprocess your dev and test sets in the same way you preprocess your training data .Run the following commands on the data that you downloaded : . cat es - en / dev / news - dev2009 .es.tok.lc cat es - en / dev / news - dev2009 .", "label": "", "metadata": {}, "score": "68.25464"}
{"text": "Step 7 : Decode a test set .When MERT finishes , it will output a file mert / joshua .config .ZMERT.final that contains the news weights for the different feature functions .You can copy this config file and use it to decode the test set .", "label": "", "metadata": {}, "score": "68.45816"}
{"text": "The default is \" 2 g \" , but you will want to increase it for larger language models .If SRILM is used , it is called with the following arguments : .$ SRILM / bin / i686-m64/ngram - count -interpolate SMOOTHING -order 5 -text TRAINING - DATA -unk -lm lm.gz .", "label": "", "metadata": {}, "score": "68.503265"}
{"text": "Again , there are language - specific tokenizations for a few languages ( English , German , and Greek ) .( Training only ) Removing all parallel sentences with more than --maxlen tokens on either side .By default , MAXLEN is 50 .", "label": "", "metadata": {}, "score": "68.56053"}
{"text": "I strongly recommend staring with the smaller set , and building an end - to - end system with it , since many steps take a very long time on the full data set .You should debug on the smaller set to avoid wasting time .", "label": "", "metadata": {}, "score": "68.65705"}
{"text": "Note the correspondences with the files defined in the first step above .The prefixes can be either absolute or relative pathnames .This particular invocation assumes that a subdirectory input/ exists in the current directory , that you are translating from a language identified \" ur \" extension to a language identified by the \" en \" extension , that the training data can be found at input / train .", "label": "", "metadata": {}, "score": "68.79739"}
{"text": "A MERT configuration file .A separate file with the list of the feature functions used in your model , along with their possible ranges .Create a MERT configuration file .In this example we name the file mert / mert . config .", "label": "", "metadata": {}, "score": "69.1443"}
{"text": "Portions \u00a9 1994 - 2004 Agence France Presse , \u00a9 1994 - 2003 Al Hayat News Agency , \u00a9 1995 - 2004 An Nahar News Agency , \u00a9 2001 - 2004 Xinhua News Agency , \u00a9 2003 - 2004 Ummah Press , \u00a9 2005 - 2006 Trustees of the University of Pennsylvania Many people in computational linguistics seem to mention the unexpected power of trigram ( or 2nd order Markov ) models for language modeling .", "label": "", "metadata": {}, "score": "69.6364"}
{"text": "The pipeline uses the Thrax grammar extractor , which is built on Hadoop .If you have a Hadoop installation , simply ensure that the $ HADOOP environment variable is defined , and the pipeline will use it automatically at the grammar extraction step .", "label": "", "metadata": {}, "score": "69.997246"}
{"text": "dc.contributor.department .Institut for Internationale Sprogstudier og Vidensteknologi ( . en_US .dc.contributor.departmentshort .ISV ( . en_US .dc.contributor.departmentuk .Department of International Language Studies and Computational Linguistics ( The Joshua Pipeline .This page describes the Joshua pipeline script , which manages the complexity of training and evaluating machine translation systems .", "label": "", "metadata": {}, "score": "69.999435"}
{"text": "Alignments are between the parallel corpora at RUNDIR / data / train / corpus .To prevent the alignment tables from getting too big , the parallel corpora are grouped into files of no more than ALIGNER_CHUNK_SIZE blocks ( controlled with a parameter below ) .", "label": "", "metadata": {}, "score": "70.29358"}
{"text": "It is not as extensive , however , as Moses ' Experiment Management System .Installation .The pipeline has no required external dependencies .However , it has support for a number of external packages , some of which are included with Joshua .", "label": "", "metadata": {}, "score": "70.37966"}
{"text": "You may also use Giza++ to create the alignments , although that program is a little unwieldy to install .To run the Berkeley aligner you first need to set up a configuration file , which defines the models that are used to align the data , how the program runs , and which files are to be aligned .", "label": "", "metadata": {}, "score": "70.66141"}
{"text": "test.in -- This is the input file containing the sentences to translate . example / example .nbest.srilm.out -- This is the output file that the n - best translations will be written to .You can inspect the output file by typing head example / example .", "label": "", "metadata": {}, "score": "70.803635"}
{"text": "When using the Berkeley aligner , you 'll want to pay attention to how much memory you allocate to it with --aligner - mem ( the default is 10 g ) .aligner - chunk - size SIZE ( 1,000,000 ) .", "label": "", "metadata": {}, "score": "70.857864"}
{"text": "The following table shows the new data that appear for the first time in the Second Edition .64,308 documents .An Nahar News Agency .16,316 documents .Ummah Press .4,641 documents .Xinhua News Agency .10,6236 documents .", "label": "", "metadata": {}, "score": "70.90819"}
{"text": "This alters the amount of memory available to Hadoop mappers ( passed via the mapred.child.java.opts options ) .--thrax - conf FILE .Use the provided Thrax configuration file instead of the ( grammar - specific ) default .The Thrax templates are located at $ JOSHUA / scripts / training / templates / thrax - TYPE .", "label": "", "metadata": {}, "score": "70.99505"}
{"text": "Lowercasing .This creates a series of intermediate files which are saved for posterity but compressed .For example , you might see .data/ train/ train.en.gz train.tok.en.gz train.tok.50.en.gz train.tok.50 .lc.en .The file \" corpus .LANG \" is a symbolic link to the last file in the chain .", "label": "", "metadata": {}, "score": "71.09972"}
{"text": "In our experience , this works fine , but you should note the following caveats : .It is of crucial importance that you have enough physical disks .We have found that having too few , or too slow of disks , results in a whole host of seemingly unrelated issues that are hard to resolve , such as timeouts .", "label": "", "metadata": {}, "score": "71.20738"}
{"text": "A name is needed to distinguish this test set from the previous ones .Output for this test run will be stored at RUNDIR / test / NAME . --joshua - config CONFIG .A tuned parameter file is required .This file will be the output of some prior tuning run .", "label": "", "metadata": {}, "score": "71.21485"}
{"text": "You can see a list of the other parameters available in our MERT implementation by running this command : . java -cp $ JOSHUA / bin joshua.zmert.ZMERT -h .Next , create a file called mert / params .txt that specifies what feature functions you are using in your mode .", "label": "", "metadata": {}, "score": "71.22025"}
{"text": "# Choose the training sources , which can either be directories or files that list files / directories trainSourcessubsampled/ .sentencesMAX .# 1-best output .competitiveThresholding .To run the Berkeley aligner , first set an environment variable saying where the aligner 's jar file is located ( this environment variable is just used for convenience in this document , and is not necessary for running the aligner in general : .", "label": "", "metadata": {}, "score": "71.54404"}
{"text": "1 Answer 1 .I take it that \" you 've never tried \" is a phrase that you do n't want to extract , but which has high IDF .The problem will be that there are going to be a vast number of n - grams that only occur in one document and so have the largest possible IDF score .", "label": "", "metadata": {}, "score": "71.68422"}
{"text": "For example , perhaps the decoder ran out of memory .This allows you to adjust the parameter ( e.g. , --joshua - mem ) and rerun the script .Of course , if you change one of the parameters a step depends on , it will trigger a rerun , which in turn might trigger further downstream reruns .", "label": "", "metadata": {}, "score": "71.87058"}
{"text": "Bold # s are not statistically significant worse than exact model .Their resulting model contained 300 million unique n - grams .It ... . \" ...Inferring attributes of discourse participants has been treated as a batch - processing task : data such as all tweets from a given author are gathered in bulk , processed , analyzed for a particular feature , then reported as a result of academic interest .", "label": "", "metadata": {}, "score": "71.90134"}
{"text": "Unlike older corpora , the present corpus uses only the information structure that is common to all sources and serves a clear function : headline , dateline , and core news content ( usually containing paragraphs ) .All sources have received a uniform treatment in terms of quality control , and have been categorized into three distinct \" types \" : . story .", "label": "", "metadata": {}, "score": "72.04763"}
{"text": "Caches the results of intermediate steps ( using robust SHA-1 checksums on dependencies ) , so the pipeline can be debugged or shared across similar runs with ( almost ) no time spent recomputing expensive steps .Allows you to jump into and out of the pipeline at a set of predefined places ( e.g. , the alignment stage ) , so long as you provide the missing dependencies .", "label": "", "metadata": {}, "score": "72.1163"}
{"text": "The pipeline script needs to be told where to find the raw training , tuning , and test data .A good convention is to place these files in an input/ subdirectory of your run 's working directory ( NOTE : do not use data/ , since a directory of that name is created and used by the pipeline itself ) .", "label": "", "metadata": {}, "score": "72.74284"}
{"text": "Run the pipeline .The following is the minimal invocation to run the complete pipeline : .$ JOSHUA / scripts / training / pipeline.pl \\ --corpus input / train \\ --tune input / tune \\ --test input / devtest \\ --source SOURCE \\ --target TARGET .", "label": "", "metadata": {}, "score": "73.22003"}
{"text": "You 'll also need to set a JAVA_HOME environment variable .For Mac OS X this usually is done by typing : .These variables will need to be set every time you use Joshua , so it 's useful to add them to your . bashrc , .", "label": "", "metadata": {}, "score": "74.078224"}
{"text": "# phrasemodel mono 0 0.5 .# wordpenalty weight . wordpenalty -2.721711092619053 .Finally , run the command to start MERT : . nohup java -cp $ JOSHUA / bin \\ joshua.zmert.ZMERT \\ -maxMem 1500 mert / mert . config & .", "label": "", "metadata": {}, "score": "74.29784"}
{"text": "Training SMT systems involves a complicated process of interacting steps that are time - consuming and prone to failure .Developing and testing new techniques requires varying parameters at different points in the pipeline .Earlier results ( which are often expensive ) need not be recomputed .", "label": "", "metadata": {}, "score": "74.91532"}
{"text": "Next , create a file called mert / decoder_command that contains the following command : . config \\ dev / news - dev2009 .es.tok.lc \\ mert / news - dev2009 .output.nbest .Next , create a configuration file for joshua at mert / joshua .", "label": "", "metadata": {}, "score": "75.51344"}
{"text": "multi . other . these DOCs clearly do not fall into any of the above types ; these are things like lists of sports scores , stock prices , temperatures around the world , and so on .The general strategy for categorizing DOCs into these three classes was , for each source , to discover the most common and frequent clues in the text stream that correlated with the \" non - story \" types .", "label": "", "metadata": {}, "score": "75.5323"}
{"text": "The pipeline takes a set of inputs ( training , tuning , and test data ) , and creates a set of intermediate files in the run directory .By default , the run directory is the current directory , but it can be changed with the --rundir parameter .", "label": "", "metadata": {}, "score": "75.915855"}
{"text": "That 's the end of the pipeline !Joshua also supports decoding further test sets .This is enabled by rerunning the pipeline with a number of arguments : . --first - step TEST .This tells the decoder to start at the test step . --name", "label": "", "metadata": {}, "score": "76.276245"}
{"text": "Tuning is run till convergence in the RUNDIR / tune directory .By default , tuning is run just once , but the pipeline supports running the optimizer an arbitrary number of times due to recent work pointing out the variance of tuning procedures in machine translation , in particular MERT .", "label": "", "metadata": {}, "score": "76.48603"}
{"text": "I am not sure what assumptions / intuitions those models leverage to perform well , and so how well they would do for IDF scores .Anybody has any better ideas ?What do you mean by ' interesting ' ?As you 've shown , a rare phrase is n't necessarily interesting .", "label": "", "metadata": {}, "score": "78.27172"}
{"text": "For example , .Also , should Thrax fail , it might be due to a memory restriction .By default , Thrax requests 2 GB from the Hadoop server .If more memory is needed , set the memory requirement with the --hadoop - mem in the same way as the --joshua - mem option is used .", "label": "", "metadata": {}, "score": "78.43505"}
{"text": "If the program finishes right away , then it probably terminated with an error .You can read the nohup.out file to see what went wrong .Common problems include a missing example / test directory , or a file not found exception .", "label": "", "metadata": {}, "score": "79.91693"}
{"text": "Each data file name consists of the seven - letter prefix , an underscore character ( \" _ \" ) , and a six - digit date ( representing the year and month during which the file contents were generated by the respective news source ) , followed by a \" .", "label": "", "metadata": {}, "score": "80.56306"}
{"text": "( If you do not have a Hadoop installation , you might consider setting one up .Hadoop can be installed in a \" pseudo - distributed \" mode that allows it to use just a few machines or a number of processors on a single machine .", "label": "", "metadata": {}, "score": "80.867615"}
{"text": "Install SRILM and set the $ SRILM environment variable to point to its installed location .Add the --lm - gen srilm flag to your pipeline invocation .More information on this is available in the LM building section of the pipeline .", "label": "", "metadata": {}, "score": "81.18225"}
{"text": "Each run can be found in a directory RUNDIR / tune / N .When tuning is finished , each final configuration file can be found at either .RUNDIR / tune / N / joshua.config .ZMERT.final RUNDIR / tune / N / joshua.config . PRO.final .", "label": "", "metadata": {}, "score": "81.790115"}
{"text": "After you successfully compile SRILM , Joshua will need to know what directory it is in .You can type pwd to get the absolute path to the sirlm/ directory that you created .Once you 've figured out the path , set an SRILM environment variable by typing : .", "label": "", "metadata": {}, "score": "82.287636"}
{"text": "If you do n't have a Hadoop installation , there are still no worries .The pipeline will unroll a standalone installation and use it to extract your grammar .This behavior will be triggered if $ HADOOP is undefined .Make sure that the environment variable $ JOSHUA is defined , and you should be all set .", "label": "", "metadata": {}, "score": "83.19947"}
{"text": "You can lowercase your tokenized data with the following script : . cat es - en / full - training / training .en.tok.lc cat es - en / full - training / training .es.tok.lc .Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "83.69602"}
{"text": "It is included with Joshua , and should compile successfully when you typed ant all from the Joshua root directory .It is not required because you can use the ( included ) Berkeley aligner ( --aligner berkeley ) .By default , the pipeline uses a Java program from the Berkeley LM package that constructs an Kneser - Ney - smoothed language model in ARPA format from the target side of your training data .", "label": "", "metadata": {}, "score": "84.64406"}
{"text": "# lm order weight .lm 1.0 .# phrasemodel owner column(0-indexed ) weight . phrasemodel pt 0 1.4037585111897322 . phrasemodel pt 1 0.38379188013385945 . phrasemodel pt 2 0.47752204361625605 .# arityphrasepenalty owner start_arity end_arity weight .# arityphrasepenalty pt 0 0 1.0 .", "label": "", "metadata": {}, "score": "86.8016"}
{"text": "Another useful flag is the --rundir DIR flag , which chdir()s to the specified directory before running the pipeline .By default the rundir is the current directory .Changing it can be useful for organizing related pipeline runs .Relative paths specified to other flags ( e.g. , to --corpus or --lmfile ) are relative to the directory the pipeline was called from , not the rundir itself ( unless they happen to be the same , of course ) .", "label": "", "metadata": {}, "score": "86.917534"}
{"text": "JoshuaDecoder -- This is the class that is run .If you want to look at the the source code for this class , you can find it in src / joshua / decoder / JoshuaDecoder.java .example / example .config.srilm -- This is the configuration file used by Joshua .", "label": "", "metadata": {}, "score": "87.78961"}
{"text": "profile file .Download and Install Joshua .Running ant will compile the Java classes and link in srilm .If everything works properly , you should see the message BUILD SUCCESSFUL .If you get a BUILD FAILED message , it may be because you have not properly set the paths to SRILM and JAVA_HOME , or because srilm was not compiled properly , as described above .", "label": "", "metadata": {}, "score": "88.010864"}
{"text": "If you already have a grammar and wish to skip this step , you can do so passing the grammar with the --grammar GRAMMAR flag .The main variable in grammar extraction is Hadoop .If you have a Hadoop installation , simply ensure that the environment variable $ HADOOP is defined , and Thrax will seamlessly use it .", "label": "", "metadata": {}, "score": "90.46835"}
{"text": "A single reference will have the format TUNE.TARGET , while multiple references will have the format TUNE.TARGET.NUM , where NUM starts at 0 and increments for as many references as there are .The following processing steps are applied to each file .", "label": "", "metadata": {}, "score": "92.344925"}
{"text": "Restarting failed runs .If the pipeline dies , you can restart it with the same command you used the first time .If you rerun the pipeline with the exact same invocation as the previous run ( or an overlapping configuration - one that causes the same set of behaviors ) , you will see slightly different output compared to what we saw above : .", "label": "", "metadata": {}, "score": "92.39274"}
{"text": "COMMON USE CASES AND PITFALLS .If the pipeline dies at the \" thrax - run \" stage with an error like the following : .JOB FAILED ( return code 1 ) hadoop / bin / hadoop : line 47 : /some / path / to / a / directory / hadoop / bin / hadoop - config .", "label": "", "metadata": {}, "score": "93.06511"}
{"text": "After tokenization and lowercasing , the file looks like this ( head -3 es - en / full - training / training .en.tok.lc ): . resumption of the session i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "93.92387"}
{"text": "Agence France Presse .( afp_arb ; formally afa ) .Al Hayat News Agency .( hyt_arb ; formally alh ) .An Nahar News Agency .( nhr_arb ; formally ann ) .Ummah Press .( umh_arb ) .", "label": "", "metadata": {}, "score": "100.08826"}
{"text": "You should really try to install physical disks that are dedicated to Hadoop scratch space .Here are some flags relevant to Hadoop and grammar extraction with Thrax : . --hadoop /path / to / hadoop .This sets the location of Hadoop ( overriding the environment variable $ HADOOP ) .", "label": "", "metadata": {}, "score": "105.19861"}
{"text": "NoClassDefFoundError: org / apache / hadoop / fs / FsShell Caused by : java.lang.ClassNotFoundException : org.apache.hadoop.fs.FsShell .This occurs if the $ HADOOP environment variable is set but does not point to a working Hadoop installation .", "label": "", "metadata": {}, "score": "105.864716"}
{"text": "Tommy Herbert Jun 11 ' 10 at 16:44 .If I had restaurant reviews ( each doc reviewing a different restaurant ) , I would expect names of their specialty dishes to come up as the most interesting phrases .If they were Wikipedia pages on cities , I would expect names of their famous landmarks and attributes to show up instead .", "label": "", "metadata": {}, "score": "116.26526"}
