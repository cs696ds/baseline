{"text": "Briefly , the filter approach relies on a probabilistic method to eliminate or rank features , similarly for example to our use of the t - test .However , according to Cover and Van Campenhout [ 29 ] , no ordering of error probabilities is guaranteed to produce the optimal feature subset or subsets .", "label": "", "metadata": {}, "score": "31.632221"}
{"text": "I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "36.446926"}
{"text": "I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "36.446926"}
{"text": "I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "36.446926"}
{"text": "If no such i exists , no hypothesis is rejected .The user can define stringency levels aided by a distribution plot and supplementary information offered by the GUI indicators .The stringency is performed by specifying a minimum fold - change cutoff , an AC test p -value cutoff , and the FDR \u03b1 .", "label": "", "metadata": {}, "score": "36.98592"}
{"text": "An increase of classifier variance corresponds to overfitting .Another interesting question is which features should be used .Given a set of N features ; how do we select an optimal subset of M features such that M .Another approach would be to replace the set of N features by a set of M features , each of which is a combination of the original feature values .", "label": "", "metadata": {}, "score": "37.428833"}
{"text": "A well known dimensionality reduction technique that yields uncorrelated , linear combinations of the original N features is Principal Component Analysis ( PCA ) .PCA tries to find a linear subspace of lower dimensionality , such that the largest variance of the original data is kept .", "label": "", "metadata": {}, "score": "38.788326"}
{"text": "Given a training sample , a maximum - margin hyper plane splits a given training sample in such a way that the distance from the closest cases ( support vectors ) to the hyper plane ( . is computed for each of the features and the feature with the smallest norm is eliminated .", "label": "", "metadata": {}, "score": "40.928642"}
{"text": "Furthermore , classifiers that tend to model non - linear decision boundaries very accurately ( e.g. neural networks , KNN classifiers , decision trees ) do not generalize well and are prone to overfitting .Therefore , the dimensionality should be kept relatively low when these classifiers are used .", "label": "", "metadata": {}, "score": "41.2408"}
{"text": "Finally , an invaluable technique used to detect and avoid overfitting during classifier training is cross - validation .Cross validation approaches split the original training data into one or more training subsets .During classifier training , one subset is used to test the accuracy and precision of the resulting classifier , while the others are used for parameter estimation .", "label": "", "metadata": {}, "score": "41.46483"}
{"text": "On the other hand , fewer holdout set instances mean that the confidence interval for the accuracy would be wider .Besides accuracy and prediction ability , the repeated holdout cross - validation was used to test the stability of a classifier .", "label": "", "metadata": {}, "score": "41.93035"}
{"text": "Again we can see that the number of parameters to be estimated grows quadratic with the number of dimensions .In an earlier article we showed that the variance of a parameter estimate increases if the number of parameters to be estimated increases ( and if the bias of the estimate and the amount of training data are kept constant ) .", "label": "", "metadata": {}, "score": "44.517727"}
{"text": "If N training samples suffice to cover a 1D feature space of unit interval size , then N^2 samples are needed to cover a 2D feature space with the same density , and N^3 samples are needed in a 3D feature space .", "label": "", "metadata": {}, "score": "44.88508"}
{"text": "Consequently , feature selection plays a key role prior to SVM classification , especially for complex datasets as in shotgun proteomics .Our GA is a stochastic heuristic that deals with massive resampling to handle the idiosyncrasies of a dataset as related to a classifier to avoid overfitting .", "label": "", "metadata": {}, "score": "45.00344"}
{"text": "Prior attempts at performing feature selection based solely on some function of the VC dimension [ 32 ] have been reported .However , our GA is based on the SRM principle that combines such a function with empirical error measures .", "label": "", "metadata": {}, "score": "45.155235"}
{"text": "Additionally , another upper bound on the generalization error depends on the machine 's number of support vectors in a way that a small number of such vectors is also preferred [ 12 ] .We use this other bound as well .", "label": "", "metadata": {}, "score": "45.300854"}
{"text": "On the other hand , wrapper methods handle the problem by relying on the classification algorithm during the feature selection process , but the algorithm becomes more prone to overfitting .nSVM is a wrapper feature selection approach that couples a nature - inspired optimization technique ( a GA ) with the state - of - the - art classifier ( an SVM ) to address the overfitting problem .", "label": "", "metadata": {}, "score": "45.436005"}
{"text": "Several types of cross - validation such as k - fold cross - validation and leave - one - out cross - validation can be used if only a limited amount of training data is available .Conclusion .In this article we discussed the importance of feature selection , feature extraction , and cross - validation , in order to avoid overfitting due to the curse of dimensionality .", "label": "", "metadata": {}, "score": "46.42804"}
{"text": "In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .Question 1 : Is it necessary to consider AIC and the BIC criteria when selecting the lag for a VAR , VECM or ARDL model OR can I use something else ?", "label": "", "metadata": {}, "score": "46.602127"}
{"text": "In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .Question 1 : Is it necessary to consider AIC and the BIC criteria when selecting the lag for a VAR , VECM or ARDL model OR can I use something else ?", "label": "", "metadata": {}, "score": "46.602127"}
{"text": "In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .Question 1 : Is it necessary to consider AIC and the BIC criteria when selecting the lag for a VAR , VECM or ARDL model OR can I use something else ?", "label": "", "metadata": {}, "score": "46.602127"}
{"text": "Finally , a false - discovery rate ( FDR ) is estimated by the Benjamini - Hochberg procedure [ 18 ] for a given fold - change cutoff .Let m be the number of identified proteins minus the number of proteins that failed to pass the fold - change cutoff test .", "label": "", "metadata": {}, "score": "46.848427"}
{"text": "Thanks a lot for this .It 's a great explanation of what the dangers are of overfitting .However , I wonder why you propose PCA as a good method to counter overfitting because you said yourself : \" However , note that the largest variance of the data not necessarily represents the most discriminative information .", "label": "", "metadata": {}, "score": "46.96117"}
{"text": "In very high dimensional settings , PCA can almost always be used to fight the curse of dimensionality .Nevertheless , regularization is always important to avoid overfitting your models , whether or not you use feature extraction and selection techniques , even in low dimensional settings .", "label": "", "metadata": {}, "score": "47.152935"}
{"text": "By the way , the mean for the 2D Gaussian case would have 2 parameters .Also , the sentence that follows the description of fitting a Gaussian ( \" Again we can see that the number of parameters to be estimated grows exponentially with the number of dimensions . \" ) is not right - the growth is quadratic .", "label": "", "metadata": {}, "score": "47.47548"}
{"text": "Furthermore , a number of minimal discriminative features can be estimated by generating a two - column list having PIDs ordered by their ranks in the first column and their achieved frequencies in the second .The set of discriminative features is then estimated by locating , in this list , the two consecutive rows that present the greatest difference in frequency values .", "label": "", "metadata": {}, "score": "47.802406"}
{"text": "Then peak appearance frequencies in the solutions were calculated and the authors showed that the most frequently selected peaks were the most discriminative .The efficiency of the algorithm was then proven on a validation set .The heuristics behind nSVM are far more computationally expensive than the one described by Li et al . , so multiple executions ( e.g. , 1000 ) would invalidate its applicability .", "label": "", "metadata": {}, "score": "47.93558"}
{"text": "Our results showed that even in simple scenarios , where the spiked concentrations can be considered relatively high , the data can still play tricks on well - founded feature selection methods .This is due to the dataset 's high dimensionality , sparseness , and lack of a known a priori probability distribution .", "label": "", "metadata": {}, "score": "47.955475"}
{"text": "However , if we project the highly dimensional classification result back to a lower dimensional space , a serious problem associated with this approach becomes evident : .Figure 6 .Using too many features results in overfitting .The classifier starts learning exceptions that are specific to the training data and do not generalize well when new data is encountered .", "label": "", "metadata": {}, "score": "48.360493"}
{"text": "Other available statistical inference methods and the result analyzer module .PatternLab offers several additional feature selection methods that are widely adopted by the proteomics community .These methods include : SVM recursive feature elimination ( SVM - RFE ) [ 22 ] , forward SVM ( the weighting used in the first step of SVM - RFE ) , Golub 's index [ 23 ] , and Student 's t - test [ 11 ] .", "label": "", "metadata": {}, "score": "48.492542"}
{"text": "The most well - known strategy to deal with multiple hypotheses is the Bonferroni , but it is in some ways too conservative and this has led to the proposal of new ones [ 18 , 26 ] .The solution we propose to this massive multiple - hypothesis test problem is to analyze the data from an FDR perspective instead of that of p -values .", "label": "", "metadata": {}, "score": "48.556835"}
{"text": "Fitness evaluation is certainly one of the most important aspects of a GA .As far as we know , this is the first time a GA takes advantage of the SRM principle [ 12 ] to drive its convergence .The SRM principle is the basis of the SVM pattern recognition method , which searches for a classification function with the \" best \" trade - off between empirical error and worst - case generalization error .", "label": "", "metadata": {}, "score": "48.60121"}
{"text": "This bias accelerates the GA in finding solutions with fewer features .In addition , a fine - tuning parameter termed mutInd1After can be set through the GUI .This parameter stands for \" Mutation Index 1 After \" , so after the algorithm has reduced the initial set of candidate proteins to a number below the one the parameter specifies , the mutation index is reduced to 1 .", "label": "", "metadata": {}, "score": "48.795555"}
{"text": "Linear kernel was used for SVM classification and feature selection .This kernel was chosen to reduce the computational complexity and eliminate the need for retuning kernel parameters for every new subset of variables .Another important advantage of choosing a linear kernel is that the norm of the weight .", "label": "", "metadata": {}, "score": "49.27487"}
{"text": "Intuitively I would say that an increase in dimensionality always increases sparsity in the data due to the fact that the difference between the largest and smallest Euclidean distances between samples in such space becomes indiscernible as the dimensionality grows ( figure 11 and equation ( 1 ) ) .", "label": "", "metadata": {}, "score": "49.653076"}
{"text": "nSVM then uses the structural risk minimization ( SRM ) principle from statistical learning theory [ 12 ] to drive the convergence of a genetic algorithm ( GA ) .Briefly , a GA is a stochastic optimization technique inspired in evolutionary biology which imitates inheritance , selection , crossover , and mutation to evolve a population of abstract genomes ( individuals ) [ 19 ] .", "label": "", "metadata": {}, "score": "50.094242"}
{"text": "The point ( or short section ) where the curve begins to rise correlates to the optimum number of variables for classifier building .Additionally , RF usually needs fewer variables to achieve the same error rate as the other three classifiers .", "label": "", "metadata": {}, "score": "50.28019"}
{"text": "I am currently trying to understand how to use cross - validation in order to choose among the ' best ' subsets of different sizes returned by the R function regsubsets .I found in p250 of Introduction to ... .In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .", "label": "", "metadata": {}, "score": "50.950405"}
{"text": "The new rows ' values for each PID are obtained by averaging the original values of the corresponding PIDs within their classes .To avoid the zero - frequency problem [ 17 ] and make fold - change calculations possible , a pseudo spectral count of 1 is then added to each PID value of the two resulting rows , including the unobserved PIDs , following the process known as Laplace 's rule of succession .", "label": "", "metadata": {}, "score": "51.222446"}
{"text": "To avoid bias , it is advisable to rank and eliminate variables one by one .Initially , the whole dataset is taken when a classifier is computed .Then , a list of variables in descending order relative to classification importance is established and the variable in the end is eliminated for subsequent analysis .", "label": "", "metadata": {}, "score": "51.539505"}
{"text": "Example : Can I pick 12 lags because the model simply ... .I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "51.558548"}
{"text": "Example : Can I pick 12 lags because the model simply ... .I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "51.558548"}
{"text": "Example : Can I pick 12 lags because the model simply ... .I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "51.558548"}
{"text": "Example : Can I pick 12 lags because the model simply ... .I understand that the permutation test on PLS can help to detect overfitting of the PLS model .Usually if the p - value is greater than a criterion , say 0.05 , it means that the model is overfitting and ... .", "label": "", "metadata": {}, "score": "51.558548"}
{"text": "Therefore , the larger the Gini score is ( ranges from 1 to 100 ) , the more important a variable is .Please refer to the appendices for the introduction of other classifiers ( PLS , SVM , and LDA ) .", "label": "", "metadata": {}, "score": "51.592247"}
{"text": "We note , finally , that the FDR estimation is conservative for massively - multiple hypothesis testing [ 28 ] and that a high stringency on false positives can imply an increase in the number of false negatives ; whence the choice of our FDR of 0.1 .", "label": "", "metadata": {}, "score": "51.626385"}
{"text": "Figure 6 showed that using a simple classifier model in a high dimensional space corresponds to using a complex classifier model in a lower dimensional space .Therefore , overfitting occurs both when estimating relatively few parameters in a highly dimensional space , and when estimating a lot of parameters in a lower dimensional space .", "label": "", "metadata": {}, "score": "51.826626"}
{"text": "The philosophy behind PatternLab is that there is no single , universally optimal feature selection technique [ 24 ] ; additionally , the existence of more than one subset of features that discriminates the data equally well [ 25 ] should be considered .", "label": "", "metadata": {}, "score": "51.888763"}
{"text": "Whereas the data was linearly separable in the 3D space , this is not the case in a lower dimensional feature space .In fact , adding the third dimension to obtain perfect classification results , simply corresponds to using a complicated non - linear classifier in the lower dimensional feature space .", "label": "", "metadata": {}, "score": "52.335075"}
{"text": "The more features we use , the higher the likelihood that we can successfully separate the classes perfectly .The above illustrations might seem to suggest that increasing the number of features until perfect classification results are obtained is the best way to train a classifier , whereas in the introduction , illustrated by figure 1 , we argued that this is not the case .", "label": "", "metadata": {}, "score": "52.43504"}
{"text": "+ 7 . x1 + 4 .Samples falling in different corners of the square are not necessarily difficult to classify - what if different corners are occupied by points with different labels ?This is actually a favorable scenario .The other reason you provide is the correct one - given a point in high dimensions , the relative distance between points far from it and close it becomes negligible .", "label": "", "metadata": {}, "score": "52.852745"}
{"text": "Variable Number Dependence .-axis ) involved in the two cases , respectively .It can be seen that with the decrease of variable number used for classifier building , all the curves keep stable initially , and then rise gradually .", "label": "", "metadata": {}, "score": "52.872383"}
{"text": "Studies have shown Student 's t - test , Fisher 's exact test , and the G - test to be trustworthy for composing putative differential marker tallies when three or more replicates are available [ 9 ] .Recently , Chen et al . increased peptide and protein identifications in complex protein mixtures by re - analyzing samples digested in the presence of different MS - compatible detergents [ 10 ] .", "label": "", "metadata": {}, "score": "52.982517"}
{"text": "I have some nitpicks though : .You can ' see ' a non - linear classification boundary , as in Fig 6 , on a lower dimension when your higher dimensions are functions of your lower dimension .If x3 is an entirely new dimension , then the non - linearity in the lower dimensions may or may not be visible .", "label": "", "metadata": {}, "score": "53.2254"}
{"text": "I would like to run a linear regression analysis and I 'm uncertain about including predictors .I have three predictor variables available .One is based on a lot of previous research .Therefore I am ... .I have a list of independent variables , some of which might be related with dependent variable ( linearly or non linearly ) .", "label": "", "metadata": {}, "score": "53.34435"}
{"text": "The processes of mating , crossover , and mutation are repeated until a population of the same size as the initial one is formed for use in the next iteration of the algorithm .The user can also configure the GA to allow \" elitism \" , permitting a specified fraction of the fittest individuals to continue on to the new population .", "label": "", "metadata": {}, "score": "53.585648"}
{"text": "The last few variables are of great potential to be biomarkers for separating the groups .Evaluation of Biomarker Selection Performance .Prediction ability and stability , overfitting , diagnosis potential , and variable number dependence are important aspects for a classifier .", "label": "", "metadata": {}, "score": "53.59604"}
{"text": "However , the experimental design described by Chen et al . introduced additional data analysis challenges , since replicate readings are not acquired .The contributions by Liu et al . and Chen et al . serve as foundations for this work .", "label": "", "metadata": {}, "score": "53.76395"}
{"text": "In fact , this depends on the amount of training data available , the complexity of the decision boundaries , and the type of classifier used .If the theoretical infinite number of training samples would be available , the curse of dimensionality does not apply and we could simply use an infinite number of features to obtain perfect classification .", "label": "", "metadata": {}, "score": "54.595642"}
{"text": "This graphical user interface offers various normalization and feature selection methods ( A ) .After applying the methods , the user can view the features ranked according to their scores .The expression from the selected feature can be graphed in the result analyzer ( B ) .", "label": "", "metadata": {}, "score": "54.85775"}
{"text": "Figure 1 .As the dimensionality increases , the classifier 's performance increases until the optimal number of features is reached .Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance .In the next sections we will review why the above is true , and how the curse of dimensionality can be avoided .", "label": "", "metadata": {}, "score": "54.92832"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "I am new to this topic and would like to understand it better .I want to build a binary classifier based on penalized logistic regression .I have 10 features and 23 observations : 16 from class \" 0 \" and ... . based on customer data I want to perform a clustering using different clustering algorithms ( K - Means , Expectation Maximization , etc . ) in R. The most attributes were engineered pursuing the goal to be ... .", "label": "", "metadata": {}, "score": "55.076992"}
{"text": "How to avoid the curse of dimensionality ?Figure 1 showed that the performance of a classifier decreases when the dimensionality of the problem becomes too large .The question then is what ' too large ' means , and how overfitting can be avoided .", "label": "", "metadata": {}, "score": "55.115105"}
{"text": "So , whether this argument is relevant for a learning problem , needs always be discussed in the light of the particular feature distributions , the data and especially the behaviour of the variances .Anyway thanx , for pushing me to read the original paper , since this insight is much more worth than unconditioned statement of formula 2 .", "label": "", "metadata": {}, "score": "55.16123"}
{"text": "The accuracy of classification is crucial for a classifier , while other classification behaviors such as prediction ability , stability , degree of overfitting and diagnostic ability are of equal significance as well .-axis denotes the error rate ( the smaller , the better ) .", "label": "", "metadata": {}, "score": "55.238987"}
{"text": "These advances allowed LC / LC / MS / MS to produce semi - quantitative data on mixtures ; however , two issues have remained open : how to normalize spectral count data for profile comparisons and how to statistically identify bona fide differences between samples ( feature selection ) .", "label": "", "metadata": {}, "score": "55.347477"}
{"text": "The VC dimension , the number of support vectors , and the number of bits having value 1 in the individual are also recorded .Finally , the fitness score for an individual is calculated as .Clearly , the lower the score , the fitter the individual .", "label": "", "metadata": {}, "score": "55.361572"}
{"text": "The conditional probability of finding a spectral count of x 2 in biological state 2 given that a spectral count of x 1 was found in biological state 1 can be estimated by the AC test .The AC test outputs a p -value related to testing a single hypothesis .", "label": "", "metadata": {}, "score": "55.765785"}
{"text": "The basic object of PLS is to find the linear ( or polynomial ) relationship between the superior variable .B. Support Vector Machine ( SVM ) .The key to the success of SVM is the kernel function which maps the data from the original space into a high dimensional ( possibly infinite dimensional ) feature space .", "label": "", "metadata": {}, "score": "56.109093"}
{"text": "As to SVM , the importance of variables is evaluated by the SVM recursive feature elimination ( SVM - RFE ) algorithm [ 25 ] .As each classifier possesses its own algorithm for variable importance ranking with its own strength and weakness , the Pearson correlation coefficient of every two ranks was used to evaluate their consistency and the rank of t -test ( by ascending order of variable . values ) was taken as an unbiased reference .", "label": "", "metadata": {}, "score": "56.148254"}
{"text": "One strategy to evaluate the effectiveness of FDR approaches is to spike protein markers with known concentrations into complex protein mixtures ( e.g. , lysates ) to perform real , but controlled , experiments , which are therefore verifiable .For example , Zhang et al .", "label": "", "metadata": {}, "score": "56.37863"}
{"text": "This approach works with a user - specified number of populations that evolve independently .Individuals will migrate , from time to time , according to a user - specified time parameter .The migration proceeds as follows .First the GA randomly chooses two populations from its pool and pauses their computations after the fitness evaluation step .", "label": "", "metadata": {}, "score": "56.41309"}
{"text": "One or more normalization methods can be applied to the sparse matrix .PatternLab currently implements : ln ( natural logarithm ) , Z [ 16 ] , Total Signal , Maximum Signal , and Row Sigma .The ln normalization is obtained by taking the natural logarithm of every value and aims at increasing the signal of the PIDs with low spectral counts with respect to the more abundant PIDs .", "label": "", "metadata": {}, "score": "56.49308"}
{"text": "Let 's say we operate in a 3D space , such that the covariance matrix is a 3\u00d73 symmetric matrix consisting of 6 unique elements ( 3 variances on the diagonal and 3 covariances off - diagonal ) .Together with the 3D mean of the distribution this means that we need to estimate 9 parameters based on our training data , to obtain the Gaussian density that represent the likelihood of our data .", "label": "", "metadata": {}, "score": "56.573853"}
{"text": "Methods with ease of interpretation tend to be more readily accepted , which is in line with the possibility of intuitive interpretation that is one of the goals of the Row Sigma normalization strategy introduced in this work .This is achieved by dividing the expression value of each protein by the mean plus three times the standard deviation taken inside its row in the sparse matrix .", "label": "", "metadata": {}, "score": "56.81607"}
{"text": "Two cases : ( a ) Normal versus CRC patients ( preoperative ) and ( b ) Preoperative versus postoperative patients were involved for analysis .Random Forest .Random forest ( RF ) , developed by Breiman [ 22 ] , is a combination of tree - structured predictors ( decision trees ) .", "label": "", "metadata": {}, "score": "56.87278"}
{"text": "The classification performance of RF as well as PLS , LDA , and SVM can be evaluated and compared using several approaches : cross - validation , .Cross - Validation : Prediction Ability and Stability . hold out were employed to estimate the prediction ability with low bias and low variance .", "label": "", "metadata": {}, "score": "56.98665"}
{"text": "On the other hand , nSVM correctly ranked the spiked markers from all sparse matrices , which justifies the extra computation time it required .The limitations of the t - test seem to be that it relies on estimates of the mean and variance that do not necessarily reflect the true values when only a few samples are available [ 30 ] .", "label": "", "metadata": {}, "score": "57.143585"}
{"text": "Table 2 shows that even though the optimal result was not always achieved , very satisfactory results were obtained .Therefore , the t - test can offer a quick \" bird 's eye \" view over changes throughout the entire experiment .", "label": "", "metadata": {}, "score": "57.288643"}
{"text": "Thirdly , individuals are selected ( as for mating , described above ) and are exchanged between the populations .Finally , the GA continues to evolve both populations from where they were stopped .PatternLab takes advantage of the recent multi - core processors by having each population \" live \" in a different computing thread .", "label": "", "metadata": {}, "score": "57.34318"}
{"text": "[ 10 ] .The ACFold analysis takes advantage of two accepted criteria in proteomics to pinpoint differences between samples : the generalized AC Test [ 11 ] and expression fold changes [ 9 ] .The algorithm first parses the project 's data as described in the Parser module section .", "label": "", "metadata": {}, "score": "57.646004"}
{"text": "Finally , the third scenario labels the 25 % , 2.5 % and 1.25 % input vectors as belonging to the positive class and the remaining 0.25 % as belonging to the negative class .Each sparse matrix was normalized according to the Z method and nSVM was applied to predict which and how many markers were spiked in the first matrix ( scenario 1 ) using the linear SVM kernel and varying some parameters of the GA ( Table 1 ) .", "label": "", "metadata": {}, "score": "57.745155"}
{"text": "To address the open issues above , we present a program termed PatternLab for proteomics .This program implements existing strategies and adds two new methods to pinpoint differences in protein profiles .The first method , ACFold , addresses experiments with less than three replicates from each state or having assays acquired by different protocols as described by Chen et al .", "label": "", "metadata": {}, "score": "57.919983"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "How does one identify the parity of predictor / feature / variable impact on response / outcome in a data mining model .Is there a standard procedure ... .the question Demonstrate the speed and accuracy of properly applied ' Random Forest ' as a variable importance selection tool especially in handling very large data against alternative approaches such ... .", "label": "", "metadata": {}, "score": "57.925014"}
{"text": "Our software , PatternLab for proteomics , or just PatternLab as referred to throughout , achieves its goal by featuring two new data analysis methods in addition to other widely adopted statistical approaches .The first method , ACFold , addresses experiments with less than three replicates from each state ( class ) or having data acquired by different protocols , as described by Chen et al .", "label": "", "metadata": {}, "score": "58.123466"}
{"text": "On the whole , RF , PLS and t -Test have good consistency with each other ( high Pearson correlation coefficients ) regardless of whether all variables ( Figure 6(a ) ) or identified metabolites ( Figure 6(b ) ) are involved .", "label": "", "metadata": {}, "score": "58.128227"}
{"text": "Conclusion .PatternLab offers an easy and unified access to a variety of feature selection and normalization strategies , each having its own niche .Additionally , graphing tools are available to aid in the analysis of high throughput experimental data .", "label": "", "metadata": {}, "score": "58.391907"}
{"text": "J Mol Diagn 2003 , 5 : 73 - 81 .View Article PubMed .Cleary JG , Teahan WJ : Experiments on the zero frequency problem .Proceedings of the Conference on Data Compression : 28 - 30 March 1995 ; Snowbird , UT ( Edited by : Storer JA , Cohn M ) .", "label": "", "metadata": {}, "score": "58.431847"}
{"text": "Maybe we can obtain a perfect classification by carefully defining a few hundred of these features ?The answer to this question might sound a bit counter - intuitive : no we can not !In fact , after a certain point , increasing the dimensionality of the problem by adding new features would actually degrade the performance of our classifier .", "label": "", "metadata": {}, "score": "59.185646"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "I am trying to analyse my data before doing multi - class classification with SVM .I have several variables .I pick one of them and study it .This is a categorical variable .It can have the value 0 or ... .", "label": "", "metadata": {}, "score": "59.28893"}
{"text": "RF includes two methods for measuring the importance of a variable or how much it contributes to predictive accuracy .The default method is the Gini score ( the method of this study ) .For any variable , the measure is the increase in prediction error if the values of that variable are permuted across the out - of - bag observations .", "label": "", "metadata": {}, "score": "59.401012"}
{"text": "Every aspect of nSVM 's GA can be customized in its graphical user interface or programmatically .A detailed explanation of each parameter can be obtained at the project 's website .Results and discussion .Two main issues characterize feature selection challenges in bioinformatics : the large input dimensionality and limitations in the dataset size .", "label": "", "metadata": {}, "score": "59.48409"}
{"text": "Anyway , I 'm not completely sure how to answer your question right now , but I 'm going to think about it for a while , thanks .( Feel free to elaborate ) .Nice article , but your parameter counting for the Gaussian distribution seems to be off .", "label": "", "metadata": {}, "score": "59.522675"}
{"text": "Consequently , RF does not over fit the data and so will give out reliable result on new samples .As to SVM , its .values on the permuted data sets are under zero and lower than those on the actual data set .", "label": "", "metadata": {}, "score": "59.631577"}
{"text": "In the above example , we showed that the curse of dimensionality introduces sparseness of the training data .The more features we use , the more sparse the data becomes such that accurate estimation of the classifier 's parameters ( i.e. its decision boundaries ) becomes more difficult .", "label": "", "metadata": {}, "score": "59.66819"}
{"text": "The Islands column indicates how many seconds were required before a migration could even occur ; a zero indicates that the island model was not applied .The No .Feat . column indicates what nSVM suggested as the minimum set of optimal features .", "label": "", "metadata": {}, "score": "59.928223"}
{"text": "Moreover , Chen et al .recently showed how to increase the number of identified proteins in shotgun proteomics by analyzing samples with different MS - compatible detergents while performing proteolytic digestion .The latter introduced new challenges as seen from the data analysis perspective , since replicate readings are not acquired .", "label": "", "metadata": {}, "score": "59.94867"}
{"text": "Details of each module and a walkthrough of PatternLab , including the two new feature selection procedures ACFold and nSVM , are described below .Parser module .Let \" project \" refer to one 's experimental data from all MudPIT assays of all biological samples from both control and case states .", "label": "", "metadata": {}, "score": "60.114967"}
{"text": "Lastly , the final report can be exported to text .ACFold 's graphical user interface .The interface above displays results from real experimental data .The plot on the right shows the distribution of the identified proteins according to log 2 ( fold change ) on the ordinate ( y ) and - log 2 ( 1- ( AC test p -value ) ) on the abscissa ( x ) .", "label": "", "metadata": {}, "score": "60.162865"}
{"text": "Hello , in your comment about cross validation , i think you should make it more clear .In k fold , The data is broken up into a number of equal subsets .one subset is chosen as the test data , the remaining subsets are used for training .", "label": "", "metadata": {}, "score": "60.214066"}
{"text": "The simplest random forest with random features is formed by selecting randomly , at each node , a small group of input variables to split on .The size of the group is fixed throughout the process of growing the forest .", "label": "", "metadata": {}, "score": "60.46171"}
{"text": "Is there any way to transform the results back to the original data space ?( To ... .I have a dataset which has dependent variable(label ) as possible destinations and independent variable(features ) as age , language , gender and many other categorical variables .", "label": "", "metadata": {}, "score": "60.923912"}
{"text": "Is there any way to transform the results back to the original data space ?( To ... .I have a dataset which has dependent variable(label ) as possible destinations and independent variable(features ) as age , language , gender and many other categorical variables .", "label": "", "metadata": {}, "score": "60.923912"}
{"text": "Benjamini Y , Hochberg Y : Controlling the false discovery rate : a practical and powerful approach to multiple testing .J R Stat Soc Ser B 1995 , 57 : 289 - 300 .Holland JH : Adaptation in Natural and Artificial Systems Ann Arbor , MI : University of Michigan Press 1974 .", "label": "", "metadata": {}, "score": "60.995033"}
{"text": "In the k -fold cross - validation , the training set was first divided into k subsets ( the folds ) of approximately equal size .( 2 ) Holdout cross - validation is similar to k -fold cross - validation except for the repeatedly ( 100 times ) random selection of the two mutually exclusive training and testing ( holdout ) subsets in accordance with a given ratio .", "label": "", "metadata": {}, "score": "61.017082"}
{"text": "RF yielded 95.4 % , a higher classification accuracy than that of LDA , SVM , and PLS , which achieved 90.8 % , 80.8 % , and 80.8 % , respectively .-axis is the classification scores .( a ) Normal versus CRC , ( b ) pre versus post .", "label": "", "metadata": {}, "score": "61.082294"}
{"text": "The score plot based on cross validation was used for classification accuracy evaluation .The cross - validation and ROC ( receiver operating characteristic ) curve were carried out to test their prediction ability and stability .The .plot was adopted for overfitting measurement .", "label": "", "metadata": {}, "score": "61.167862"}
{"text": "What matters is whether we know the exact functional relationship of a dimension , x3 , to others .Or , can conveniently determine it .Take a real - world modeling scenario where a feature \" location \" becomes available for user you want to model . \" location \" may not entirely be independent of another feature like the \" ISP \" the user uses for accessing internet .", "label": "", "metadata": {}, "score": "61.741623"}
{"text": "The features for the final classification model are selected by executing nSVM multiple times ( e.g. , 20 ) .For every nSVM execution , each time the fittest individual is replaced its genomic information is saved in a text file ( history file ) .", "label": "", "metadata": {}, "score": "61.781242"}
{"text": "Implementation .PatternLab 's current version is optimized for LC / LC / MS / MS data using spectral counts .Its architecture comprises four core modules ( parsing MS data , data normalization , feature selection , and analysis ) .", "label": "", "metadata": {}, "score": "61.877884"}
{"text": "No .Subst . column indicates how many times the fittest individual was substituted .The Time column indicates the average time and standard deviation of 1 nSVM run .These results were obtained for scenario 1 as described in the Suggestion of when to apply nSVM section .", "label": "", "metadata": {}, "score": "62.12664"}
{"text": "Here , if you are looking at hyperplane as a classifier in the new space ( i.e. one with location ) , the projection onto the original space would be a line .The case that you are talking about is something that , again typically :) , a classifier does internally by fabricating new dimensions ( basis functions ) .", "label": "", "metadata": {}, "score": "62.164543"}
{"text": "This probably caused by its dependence on kernel functions and support vectors .The ROC curve coupled with its area under the curve ( AUC ) is a common method used to estimate the diagnosis potential of a classifier in clinical applications .", "label": "", "metadata": {}, "score": "62.329456"}
{"text": "E.g. it is fulfilled for IID ( independent and identical distributed dimensions ) like independent random data uniformly distributed in every dimension ) .That 's an example of one extrem distribution .But it is not fulfilled for data distributed on a \" hyperline \" where the values of the data points in all dimensions are equal .", "label": "", "metadata": {}, "score": "62.581863"}
{"text": "That is the argument about the distance ( formula 2 ) .It is nearly the same statement as in Wikipedia , where it is also unclear , how the distances are related to the dimensions and how this argument implies that the distances become use less .", "label": "", "metadata": {}, "score": "62.687828"}
{"text": "The GUI also lists an AC FDR indicating that all blue dots satisfy the established user - selected FDR of 0.1 .nSVM feature selection .nSVM is a feature selection algorithm introduced in this work and used here to pinpoint differences in protein expression profiles when multiple replicates of each class are available .", "label": "", "metadata": {}, "score": "62.71647"}
{"text": "The training samples that do not fall within this unit circle are closer to the corners of the search space than to its center .These samples are difficult to classify because their feature values greatly differs ( e.g. samples in opposite corners of the unit square ) .", "label": "", "metadata": {}, "score": "63.16298"}
{"text": "Is n't it so , that more often than not high dimensional problems live in a lower dimensional embedded manifold ?This would mean that not every increase in dimensionality introduces necessarily more sparsity in the data .Do you know of any examination in this direction you could point me to ?", "label": "", "metadata": {}, "score": "63.178707"}
{"text": "For example , a mutation index of 2 allows the GA to perform up to two mutations in the offspring 's genome .The mutation is performed by switching the values of randomly chosen bits with a bias towards mutating them to 0 ( specifically , a 60 % chance for 0 and 40 % for 1 ) .", "label": "", "metadata": {}, "score": "63.258133"}
{"text": "The GA works by generating successive populations on the premise that the average individual fitness ( quality of the solution ) will increase for each new population .Each new solution from nSVM is produced by first selecting parents according to their quadratically normalized fitnesses .", "label": "", "metadata": {}, "score": "63.3323"}
{"text": "In the 1D case ( figure 2 ) , 10 training instances covered the complete 1D feature space , the width of which was 5 unit intervals .If we would keep adding features , the dimensionality of the feature space grows , and becomes sparser and sparser .", "label": "", "metadata": {}, "score": "63.51303"}
{"text": "This control , however , can not be obtained with the p -value alone .Label - free shotgun proteomics currently uses a random sampling process to estimate the relative quantitation of thousands of proteins .For this reason , determining the true number of differentially expressed proteins , which would require precise quantitation instead , has remained an open challenge .", "label": "", "metadata": {}, "score": "63.59194"}
{"text": "Thus far , I 've only created basic unigram language models and used these ... .I am using MATLAB function TreeBagger ( ) for Random Forest classification , for an assignment .It gives error when the number of variables of the Test data is different from the number of variables of ... .", "label": "", "metadata": {}, "score": "63.689423"}
{"text": "Thus far , I 've only created basic unigram language models and used these ... .I am using MATLAB function TreeBagger ( ) for Random Forest classification , for an assignment .It gives error when the number of variables of the Test data is different from the number of variables of ... .", "label": "", "metadata": {}, "score": "63.689423"}
{"text": "Thus far , I 've only created basic unigram language models and used these ... .I am using MATLAB function TreeBagger ( ) for Random Forest classification , for an assignment .It gives error when the number of variables of the Test data is different from the number of variables of ... .", "label": "", "metadata": {}, "score": "63.689423"}
{"text": "Thus far , I 've only created basic unigram language models and used these ... .I am using MATLAB function TreeBagger ( ) for Random Forest classification , for an assignment .It gives error when the number of variables of the Test data is different from the number of variables of ... .", "label": "", "metadata": {}, "score": "63.689423"}
{"text": "Each individual 's fitness is evaluated by how well the input vectors are \" separated \" in the feature space defined by the individual .First , the input vectors are mapped onto the feature space taking into consideration only the proteins whose PIDs have value 1 in the individual .", "label": "", "metadata": {}, "score": "63.73101"}
{"text": "The tree number of the forest in this study is set to be 200 , the number of input variables tried for each node is the square root of the number of total variables , and the minimum size of the terminal nodes is set to be 2 .", "label": "", "metadata": {}, "score": "63.90038"}
{"text": "Suggestion of when to apply ACFold . ACFold combines the AC test with fold changes to pinpoint differentially expressed proteins between classes ; this is important because conclusions drawn only from fold changes could be equivocated .For example , suppose spectral counts of 3 ( 6 ) and 30 ( 60 ) were observed for protein x ( y ) during the control ( case ) assay .", "label": "", "metadata": {}, "score": "64.20231"}
{"text": "Thus far , I 've only created basic unigram language models and used these ... .I am using MATLAB function TreeBagger ( ) for Random Forest classification , for an assignment .It gives error when the number of variables of the Test data is different from the number of variables of ...", "label": "", "metadata": {}, "score": "64.280235"}
{"text": "I used Logistic Regression as a classifier .I have six features , I want to know the important features in this classifier that influence the result more than other features .I used Information Gain ... .I applied PCA on a 12000 x 500 data set ( 12000 data points with 500 features ) .", "label": "", "metadata": {}, "score": "64.300804"}
{"text": "For RF , variables are ranked by Gini score , a measurement of average accuracy of all trees containing a particular variable [ 22 ] .For PLS , the conventional VIP ( variable importance in projection ) value is used for variable ranking .", "label": "", "metadata": {}, "score": "64.350296"}
{"text": "Accordingly , we showed that nSVM efficiently dealt with the overfitting problem on a high - dimensional and noisy dataset by correctly pinpointing the relevant features ( spiked proteins ) and outperforming the t - test filter approach , as described below .", "label": "", "metadata": {}, "score": "65.15666"}
{"text": "Clearly , fitter individuals have significantly higher chances of being selected .During the mating process , a uniformly random crossover operator is used , so the single offspring receives each gene ( bit value ) from either one of its parents with equal chances .", "label": "", "metadata": {}, "score": "65.17577"}
{"text": "However , because of the more complex nature of MudPIT 's LC / LC method and the alternating acquisition of mass spectra and tandem mass spectra , chromatographic alignment is more complicated than for LC / MS data .An important step was taken when Liu et al .", "label": "", "metadata": {}, "score": "65.191376"}
{"text": "The area under the ROC curve ( AUC ) is a statistic summary of its diagnostic potential .Variable Number Dependence .Generally , too many irrelevant variables are liable to result in overfitting decisions , whereas differences between groups can not be extracted and depicted completely if crucial variables are not concerned [ 24 ] .", "label": "", "metadata": {}, "score": "65.24887"}
{"text": "Modification in the experimental designs to isolate sub - proteomes is a solution ; however , these separations are many times not straightforward if protein content is to be disturbed only minimally .Therefore , even with all the advances in pattern recognition techniques , a set of bona fide markers requires experimental and computational validation in unseen samples to ensure the model is not a result of overfitting .", "label": "", "metadata": {}, "score": "65.30914"}
{"text": "Now let 's use a simple linear classifier and try to obtain a perfect classification .We can start by a single feature , e.g. the average ' red ' color in the image : .Figure 4 .Adding a third feature results in a linearly separable classification problem in our example .", "label": "", "metadata": {}, "score": "65.40874"}
{"text": "277 - 283 , 2007 .View at Publisher \u00b7 View at Google Scholar . A. P. Bradley , \" The use of the area under the ROC curve in the evaluation of machine learning algorithms , \" Pattern Recognition , vol .", "label": "", "metadata": {}, "score": "65.67404"}
{"text": "Receiver Operating Characteristic ( ROC ) : Diagnosis Potential .ROC analysis is a classic method from signal detection theory and is now commonly used in clinical research [ 23 ] .ROC of a classifier shows its performance as a tradeoff between specificity and sensitivity .", "label": "", "metadata": {}, "score": "65.893234"}
{"text": "I promise you that I 'll write one about this soon :) .I 'm really interested in the topic of generalization , and the differences between linear and nonlinear classifiers that you allude to .Any chance you could write a post that focuses on generalization and/or the linear / non - linear issue ?", "label": "", "metadata": {}, "score": "66.00704"}
{"text": "For example , while the output provided by univariate feature rankings can be more intuitively grasped because they analyze each feature independently , protein subgroups that could possibly interact can only be detected through multivariate techniques ( but requiring far more effort ) .", "label": "", "metadata": {}, "score": "66.237404"}
{"text": "Nevertheless , more effective and robust bioinformatics tools are in critical need for metabolomic data analysis especially when dealing with clinical samples with large individual variability due to diverse demographic and genetic background of patients and various pathological conditions or treatments .", "label": "", "metadata": {}, "score": "66.243576"}
{"text": "I applied PCA on a 12000 x 500 data set ( 12000 data points with 500 features ) .PCA gave me 12000 x 20 data ( 20 features ) .Is there any way to transform the results back to the original data space ?", "label": "", "metadata": {}, "score": "66.292915"}
{"text": "Audic S , Claverie JM : The significance of digital gene expression profiles .Genome Res 1997 , 7 : 986 - 995 .PubMed .Vapnik VN : The Nature of Statistical Learning Theory New York : Springer - Verlag 1995 .", "label": "", "metadata": {}, "score": "66.4706"}
{"text": "10 , supplement 1 , pp .65 - 76 , 2009 .View at Publisher \u00b7 View at Google Scholar . Y. Qiu , M. Su , Y. Liu , et al . , \" Application of ethyl chloroformate derivatization for gas chromatography - mass spectrometry based metabonomic profiling , \" Analytica Chimica Acta , vol .", "label": "", "metadata": {}, "score": "66.90425"}
{"text": "Figure 7 .Although the training data is not classified perfectly , this classifier achieves better results on unseen data than the one from figure 5 .Although the simple linear classifier with decision boundaries shown by figure 7 seems to perform worse than the non - linear classifier in figure 5 , this simple classifier generalizes much better to unseen data because it did not learn specific exceptions that were only in our training data by coincidence .", "label": "", "metadata": {}, "score": "66.964386"}
{"text": "References .Jessani N , Niessen S , Wei BQ , Nicolau M , Humphrey M , Ji Y , Han W , Noh DY , Yates JR 3rd , Jefferey SS , Cravatt BF : A streamlined platform for high - content functional proteomics of primary human specimens .", "label": "", "metadata": {}, "score": "67.11987"}
{"text": "We compared nSVM 's performance with and without computing the empirical error measures ; the former achieved better results on our dataset ( data not shown ) .Conclusion .The identification of trustworthy protein markers is not an easy task , since mass spectrometry based proteomics is still in development and spectral counting effectiveness can vary on the experimental setup , including mass spectrometry type and data - dependent analysis configuration .", "label": "", "metadata": {}, "score": "67.137146"}
{"text": "The latter is introduced in this work as a variation of the Maximum Signal normalization that better handles assays that obtained an exceedingly high maximum value for a protein ; further advantages are addressed in the Results and discussion section .ACFold feature selection .", "label": "", "metadata": {}, "score": "67.316574"}
{"text": "As to the other three classifiers , their performances are similar showing no significant difference .These results were validated further by more cross - validation results listed in Table 2 .Therefore , RF is the one with highest prediction ability and best stability among all the classifiers .", "label": "", "metadata": {}, "score": "67.44202"}
{"text": "Your first point actually stresses the importance of something I did not address yet in this article ( this will be part of a next article ) : The curse of dimensionality strikes hard if your features are statistically dependent .Indeed in case they are independent ( or simply uncorrelated in the Gaussian case ) , linearity is preserved when projected onto a linear subspace .", "label": "", "metadata": {}, "score": "67.47765"}
{"text": "However , if the changes between samples are expected to be minimum , applying nSVM on \" unnormalized \" data can also be considered .The widely adopted Student 's t - test was then applied to check if we could rank the spiked markers as the topmost in the three scenarios after Z and Total Signal normalization .", "label": "", "metadata": {}, "score": "68.00339"}
{"text": "View Article PubMed .Cover TM , Van Campenhout JM : On the possible orderings in the measurement selection problem .Transactions on Systems , Man and Cybernetics 1977 , 7 : 657 - 661 .View Article .Jafari P , Azuaje F : An assessment of recently published gene expression data analyses : reporting experimental design and statistical factors .", "label": "", "metadata": {}, "score": "68.03783"}
{"text": "A possible descriptor that discriminates these two classes could then consist of three number ; the average red color , the average green color and the average blue color of the image under consideration .A simple linear classifier for instance , could combine these features linearly to decide on the class label : .", "label": "", "metadata": {}, "score": "68.09303"}
{"text": "Thus far , I 've only created basic unigram language models and used these ...Affiliated with .Abstract .Background .A goal of proteomics is to distinguish between states of a biological system by identifying protein expression differences .", "label": "", "metadata": {}, "score": "68.51277"}
{"text": "The island mode and mutation index play a key role in the GA ; while apparently there are no great changes in execution time , runs using a mutation index of 2 with the island mode turned on yielded better results in our dataset .", "label": "", "metadata": {}, "score": "69.056076"}
{"text": "( To ... .I have a dataset which has dependent variable(label ) as possible destinations and independent variable(features ) as age , language , gender and many other categorical variables .How do i find which are ... .In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .", "label": "", "metadata": {}, "score": "69.62233"}
{"text": "I would like to run a linear regression analysis and I 'm uncertain about including predictors .I have three predictor variables available .One is based on a lot of previous research .Therefore I am ...I have a dataset which has dependent variable(label ) as possible destinations and independent variable(features ) as age , language , gender and many other categorical variables .", "label": "", "metadata": {}, "score": "69.7076"}
{"text": "2207 - 2217 , 2011 .View at Google Scholar . A. Statnikov , L. Wang , and C. F. Aliferis , \" A comprehensive comparison of random forests and support vector machines for microarray - based cancer classification , \" BMC Bioinformatics , vol .", "label": "", "metadata": {}, "score": "70.24849"}
{"text": "In view of these results , our choice to rely on a theoretical estimator instead of physical measurements seems justified .Figure 2 exemplifies the ACFold analysis on experimental data .The aim was to identify as many proteins as possible in a glioblastoma cell culture when exposed ( or not ) to a chemotherapeutic agent during 1.5 h [ 27 ] .", "label": "", "metadata": {}, "score": "70.34947"}
{"text": "View at Google Scholar .G. X. Xie , X. J. Zheng , X. Qi , et al . , \" Metabonomic evaluation of melamine - induced acute renal toxicity in rats , \" Journal of Proteome Research , vol .9 , no . 1 , pp .", "label": "", "metadata": {}, "score": "70.48683"}
{"text": "Our observations suggest that nSVM 's niche comprises projects targeting the selection of a minimum set of proteins ( features ) that nevertheless allows the highest rate of correctness to be achieved on unseen samples in classification problems .This selection entails the solution of the difficult bioinformatics combinatorial problem of choosing one out of the 2 n sets into which n identified proteins can be combined .", "label": "", "metadata": {}, "score": "70.5622"}
{"text": "Mach Learn 2002 , 46 : 389 - 422 .View Article .Joachims T : Making large - scale SVM learning practical .Advances in Kernel Methods : Support Vector Learning ( Edited by : Sch\u00f6lkopf B , Burges C , Smola A ) .", "label": "", "metadata": {}, "score": "70.617386"}
{"text": "Figure 8 .The amount of training data needed to cover 20 % of the feature range grows exponentially with the number of dimensions .In other words , if the amount of available training data is fixed , then overfitting occurs if we keep adding dimensions .", "label": "", "metadata": {}, "score": "70.63427"}
{"text": "View Article PubMed .Li L , Umbach DM , Terry P , Taylor JA : Application of the GA / KNN method to SELDI proteomics data .Bioinformatics 2004 , 20 : 1638 - 1640 .View Article PubMed .Fr\u00f6hlich H , Chapelle O , Sch\u00f6lkopf B : Feature selection for support vector machines using genetic algorithms .", "label": "", "metadata": {}, "score": "70.74164"}
{"text": "Yeung KY , Bumgarner RE , Raftery AE : Bayesian model averaging : development of an improved multi - class , gene selection and classification tool for microarray data .Bioinformatics 2005 , 21 : 2394 - 2402 .View Article PubMed .", "label": "", "metadata": {}, "score": "71.010574"}
{"text": "BMC Bioinformatics 2004 , 5 : 125 .View Article PubMed .Jain N , Thatte J , Braciale T , Ley K , O'Connell M , Lee JK : Local - pooled - error test for identifying differentially expressed genes with a small number of replicated microarrays .", "label": "", "metadata": {}, "score": "71.079056"}
{"text": "LDA demonstrates reasonably good performance in classification but its biomarker selection ability is open to question .SVM may be slightly overfitting regardless of its good classification accuracy and diagnosis potential .The combinational usage of multiple methods , RF , t -Test , and PLS , for example , may provide more comprehensive information for a \" global \" understanding of the metabolomics or other \" omics \" data .", "label": "", "metadata": {}, "score": "71.29262"}
{"text": "Therefore , we could decide to add some features that describe the texture of the image , for instance by calculating the average edge or gradient intensity in both the X and Y direction .We now have 5 features that , in combination , could possibly be used by a classification algorithm to distinguish cats from dogs .", "label": "", "metadata": {}, "score": "71.42085"}
{"text": "Why are the samples near the center easier to classify ?And why are the samples in the corners more difficult to classify ?Rss feed .In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .", "label": "", "metadata": {}, "score": "71.70261"}
{"text": "Thus far , I 've only created basic unigram language models and used these ...", "label": "", "metadata": {}, "score": "71.731705"}
{"text": "Figure 8 illustrates the above in a different manner .Let 's say we want to train a classifier using only a single feature whose value ranges from 0 to 1 .Let 's assume that this feature is unique for each cat and dog .", "label": "", "metadata": {}, "score": "71.7654"}
{"text": "Through the analysis of one or several kinds of biofluids including serum , urine , saliva , and tissue samples , the global and dynamic alterations in metabolism can be deciphered [ 4 ] .Univariate and/or multivariate statistical methods are routinely used in metabolomics studies , aiming at successful classification of samples with metabolic phenotypic variations and identification of potential biomarkers while minimizing the technical variations .", "label": "", "metadata": {}, "score": "71.806526"}
{"text": "The other method addresses experimental designs having multiple readings from each state and is referred to as nSVM ( natural support vector machine ) because of its roots in evolutionary computing and in statistical learning theory .Our observations suggest that nSVM 's niche comprises projects that select a minimum set of proteins for classification purposes ; for example , the development of an early detection kit for a given pathology .", "label": "", "metadata": {}, "score": "71.92942"}
{"text": "View at Google Scholar . Y. Q. Bao , T. Zhao , X. Y. Wang , et al . , \" Metabonomic variations in the drug - treated type 2 diabetes mellitus patients and healthy volunteers , \" Journal of Proteome Research , vol . 8 , no .", "label": "", "metadata": {}, "score": "72.50156"}
{"text": "The index file lists all identified proteins within the project and assigns each one a unique Protein IDentification ( PID ) integer .So , for example , the row \" +1 1:3 2:5 3:6 \" specifies an analysis from the positive class having spectral count values of 3 , 5 , and 6 for PIDs 1 , 2 , and 3 , respectively , all other PIDs having value 0 .", "label": "", "metadata": {}, "score": "72.6131"}
{"text": "Because of this , the resulting classifier would fail on real - world data , consisting of an infinite amount of unseen cats and dogs that often do not adhere to these exceptions .This concept is called overfitting and is a direct result of the curse of dimensionality .", "label": "", "metadata": {}, "score": "72.775635"}
{"text": "Classification Performance .RF as well as PLS , LDA , and SVM were applied on the dataset for the two comparative cases ( Figures 1(a ) and 1(b ) ) .In Figure 1(a ) , red and blue dots represent the normal and CRC patients , respectively .", "label": "", "metadata": {}, "score": "73.005875"}
{"text": "Specificity , on the other hand , is defined as the proportion of subjects without disease whose tests is negative , and calculated in the formula , TrueNegative/(TrueNegative+FalsePositive ) .Typically , 1-specificity is plotted on the .-axis and sensitivity is plotted on the vertical axis .", "label": "", "metadata": {}, "score": "73.01016"}
{"text": "The Total Signal normalization is achieved by dividing each value by the sum of all values in the respective row .The Maximum Signal normalization is obtained by dividing each value by the largest value in its row ; an underlying assumption is that , in each MudPIT analysis , peptide identifications were obtained at or near the capacity of the tandem MS instrument .", "label": "", "metadata": {}, "score": "73.38066"}
{"text": "2617- 2628 , 2010 .View at Publisher \u00b7 View at Google Scholar .X. Li , S. Yang , Y. Qiu , et al . , \" Urinary metabolomics as a potentially novel diagnostic and stratification tool for knee osteoarthritis , \" Metabolomics , vol .", "label": "", "metadata": {}, "score": "73.45169"}
{"text": "11 , pp .1181 - 1189 , 1999 .View at Google Scholar \u00b7 View at Scopus .T. L. Chen , G. X. Xie , X. Y. Wang , et al . , \" Serum and urinemetabolite profiling reveals potential biomarkers of human hepatocellular carcinoma , \" Molecular & Cellular Proteomics , vol .", "label": "", "metadata": {}, "score": "73.47084"}
{"text": "I have a dataset which has dependent variable(label ) as possible destinations and independent variable(features ) as age , language , gender and many other categorical variables .How do i find which are ... .In order to implementing a certain feature selection method for a classification problem I need to estimate the the interaction the interaction gain between two features and the target variable which ... .", "label": "", "metadata": {}, "score": "73.56706"}
{"text": "Anal Chem 1995 , 67 ( 8) : 1426 - 1436 .View Article PubMed .Perkins DN , Pappin DJ , Creasy DM , Cottrell JS : Probability - based protein identification by searching sequence databases using mass spectrometry data .", "label": "", "metadata": {}, "score": "73.57264"}
{"text": "Four MudPIT assays were acquired for each aliquot as described by Liu et al .[ 8 ] .Finally , we generated three sparse matrices to simulate three benchmarking scenarios ; in the first scenario , the input vectors originating from the 25 % protein spiking were labeled as belonging to the positive class and all the rest as to the negative class .", "label": "", "metadata": {}, "score": "73.81831"}
{"text": "In fact , data around the origin ( at the center of the hypercube ) is much more sparse than data in the corners of the search space .This can be understood as follows : .Imagine a unit square that represents the 2D feature space .", "label": "", "metadata": {}, "score": "73.85742"}
{"text": "In this article , we discuss the so called ' Curse of Dimensionality ' , and explain why it is important when designing a classifier .Using a clear example of overfitting due to the curse of dimensionality , I provide an intuitive explanation of this concept .", "label": "", "metadata": {}, "score": "73.97153"}
{"text": "View at Publisher \u00b7 View at Google Scholar .J. Yang , T. Chen , L. Sun , et al . , \" Potential metabolite markers of schizophrenia , \" Molecular Psychiatry , vol .18 , no . 1 , pp .", "label": "", "metadata": {}, "score": "74.16652"}
{"text": "PHS2 , ALB , CAH , and ITRA stand for the spiked protein markers , and the numbers in the respective columns indicate the ranking according to nSVM .The Z normalization was chosen because one of the steps for estimating the VC dimension , according to the SVM light algorithm [ 21 ] , is based on approximating the radius of the smallest hyphersphere that encompasses all input vectors by the norm of the largest support vector .", "label": "", "metadata": {}, "score": "74.32137"}
{"text": "For example , by specifying a cutoff of 2.0 , the number of differentially expressed proteins according to the AC test was raised to 105 ; however , because of a great increase in the number of hypotheses tested , the FDR indicated only 88 as differentially expressed .", "label": "", "metadata": {}, "score": "74.33943"}
{"text": "RF achieved the best separation with no miss - classified samples ( the accuracy is 100 % ) .The performance of SVM , LDA , and PLS was good , with descending accuracy of 90.9 % , 87.1 % , and 82.6 % , respectively .", "label": "", "metadata": {}, "score": "75.10044"}
{"text": "Operating system(s ) : Windows XP or VISTA .PatternLab is expected soon to run under Linux and Macintosh , thanks to the Mono project [ 33 ] .Programming language : C # .Declarations .Acknowledgements .The authors acknowledge CNPq , CAPES , a FAPERJ BBP grant , the Rio de Janeiro proteomic network , and NIH P41RR011823 for financial support .", "label": "", "metadata": {}, "score": "76.00835"}
{"text": "However , in the field of clinical metabolomic data analysis , it has not got enough attention and concern .In addition , no comprehensive performance evaluation about this classifier is reported .In this research , RF was used in the analysis of a GC - MS derived clinical metabolomic dataset .", "label": "", "metadata": {}, "score": "76.04911"}
{"text": "The other method addresses experimental designs that comprise multiple replicates from each state and is referred to as nSVM ( natural support vector machine ) because of its roots in evolutionary computing and statistical learning theory [ 12 ] .We benchmarked nSVM against the widely adopted Student 's t - test over a spiked marker dataset and identified its niche .", "label": "", "metadata": {}, "score": "76.14859"}
{"text": "Differently than traditional GAs , nSVM offers a new strategy to estimate which proteins are differentially expressed .Our approach is a variation of the one used by Li et al .[ 31 ] to select differentially intensified mass spectral peaks from Surface Enhanced Laser Desorption Ionization - Time Of Flight proteomic profiles .", "label": "", "metadata": {}, "score": "76.23849"}
{"text": "Figure 3 summarizes the nSVM process up to this point .nSVM 's workflow .MudPIT is applied to acquire mass spectrometry data from a biological system in different states ( 1 ) .The data are subsequently identified by SEQUEST and filtered by DTASelect ( 2 ) .", "label": "", "metadata": {}, "score": "76.625145"}
{"text": "RF outperforms the other three classifiers in the given clinical data sets , highlighting its comparative advantages as a suitable classification and biomarker selection tool for clinical metabolomic data analysis .Introduction .Metabolomics [ 1 ] or metabonomics [ 2 , 3 ] is an emerging - omics approach using nuclear magnetic resonance ( NMR ) spectroscopy or gas chromatography / liquid chromatography - mass spectrometry ( GC - MS or LC - MS ) technologies .", "label": "", "metadata": {}, "score": "76.65121"}
{"text": "All the metabolites were identified and verified by public libraries such as HMDB , KEGG , and/or reference standards available in our laboratory .All the classifiers andevaluation methods were carried out using Matlab toolbox ( Version 2009a , Mathworks ) .", "label": "", "metadata": {}, "score": "76.714355"}
{"text": "This work was financially supported by the National Basic Research Program of China ( 2007CB914700 ) , National Natural Science Foundation of China Program ( 81170760 ) , and the Natural Science Foundation of Shanghai , China ( 10ZR1414800 ) .J. K. Nicholson , J. C. Lindon , and E. Holmes , \" ' Metabonomics ' : understanding the metabolic responses of living systems to pathophysiological stimuli via multivariate statistical analysis of biological NMR spectroscopic data , \" Xenobiotica , vol .", "label": "", "metadata": {}, "score": "76.78508"}
{"text": "View Article .Copyright .\u00a9 Carvalho et al .2008 .This article is published under license to BioMed Central Ltd.I applied PCA on a 12000 x 500 data set ( 12000 data points with 500 features ) .PCA gave me 12000 x 20 data ( 20 features ) .", "label": "", "metadata": {}, "score": "76.87268"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "My data , biological data called Microarray , usually has large features but small samples - 10000 features ... .I ... .I am implementing univariate feature selection from feature selection !I have several features among which I am intending to select some features and proceed .", "label": "", "metadata": {}, "score": "77.05849"}
{"text": "plot , ROC curve , variable elimination , and Pearson correlation .RF demonstrated the best overall performance including accuracy , prediction ability , overfitting , diagnosis potential , stability , and putative biomarker selection , highlighting its potential as an excellent classification and biomarker selection tool for clinical metabolomic data analysis .", "label": "", "metadata": {}, "score": "77.146996"}
{"text": "PCC coded the software and wrote the first draft of the manuscript under the guidance of VCB and JRY .EIC and JSGF generated the MudPIT experimental , helped test the software , and suggested the inclusion of important features in it .", "label": "", "metadata": {}, "score": "77.84357"}
{"text": "Other background information such as diet and alcohol consumption was considered during sample selection to minimize the diet - induced metabolic variations .Clinical characteristics of all the samples in this study are provided in Table 1 .All the samples were chemically derivatized and subsequently analyzed by GC - MS following our previously published procedures [ 21 ] .", "label": "", "metadata": {}, "score": "78.37845"}
{"text": "Figure 7 presents some box plots of intensity in corresponding groups .-axis is the intensity of metabolites ( ( a ) for homovanillate and ( b ) for lysine ) .Conclusion .In this study , RF was applied successfully in metabolomic data analysis for clinical phenotype discrimination and biomarker selection .", "label": "", "metadata": {}, "score": "78.4707"}
{"text": "Putative Biomarker Selection .Variable number dependence section is to evaluate whether and how much the performance of RF depends on the number of variables involved .This section is to evaluate its capability on important variable ( putative biomarker ) selection .", "label": "", "metadata": {}, "score": "78.512024"}
{"text": "1623 - 1630 , 2009 .View at Publisher \u00b7 View at Google Scholar .X. Wang , J. Lin , T. Chen , M. Zhou , M. Su , and W. Jia , \" Metabolic profiling reveals the protective effect of diammonium glycyrrhizinate on acute hepatic injury induced by carbon tetrachloride , \" Metabolomics , vol .", "label": "", "metadata": {}, "score": "78.64816"}
{"text": "The ROC curves and AUC values of all the classifiers in the two cases are plotted in Figure 4 .RF outperforms the others once more with the greatest AUC values ( .Figure 4 : Receiver operating characteristic curves of 4 classifiers on 2 cases .", "label": "", "metadata": {}, "score": "78.77363"}
{"text": "Each individual 's genome is an array of bits ( 3.3 ) that corresponds to a set of proteins ( 3.1 and 3.2 ) that will be selected from the dataset ( 3.4 ) to be evaluated as a solution ( 3.5 ) according to their spectral counts .", "label": "", "metadata": {}, "score": "78.78905"}
{"text": "A goal of proteomics is to distinguish between states of a biological system by identifying protein expression differences [ 1 ] .Shotgun proteomics is a large - scale strategy for protein identification in complex mixtures that involves pre - digestion of intact proteins followed by peptide separation , fragmentation in a mass spectrometer , and database search .", "label": "", "metadata": {}, "score": "79.283905"}
{"text": "176 - 192 , 2008 .View at Publisher \u00b7 View at Google Scholar .Z. Cai , J. Zhao , X. Wang , et al . , \" A combined proteomics and metabolomics profiling of gastric cardia cancer reveals characteristic dysregulations in glucose metabolism , \" Molecular & Cellular Proteomics , vol .", "label": "", "metadata": {}, "score": "79.81501"}
{"text": "One thing in the usual argumentation regarding the cod that always leaves me wondering , is that for the curse to strike , the process of raising the dimensionality has to be such , that the data points are equally scattered around in the higher space .", "label": "", "metadata": {}, "score": "79.85164"}
{"text": "View Article PubMed .Tabb DL , McDonald WH , Yates JR III : DTASelect and Contrast : tools for assembling and comparing protein identifications from shotgun proteomics .J Proteome Res 2002 , 1 : 21 - 26 .View Article PubMed .", "label": "", "metadata": {}, "score": "80.426544"}
{"text": "109 - 118 , 2010 .View at Publisher \u00b7 View at Google Scholar .J. Wei , G. X. Xie , Z. T. Zhou , et al . , \" Salivary metabolite signatures of oral cancer and leukoplakia , \" International Journal of Cancer , vol .", "label": "", "metadata": {}, "score": "80.54338"}
{"text": "226 - 236 , 2010 .View at Publisher \u00b7 View at Google Scholar . H. W. Cho , S. B. Kim , M. K. Jeong , et al . , \" Discovery of metabolite features for the modelling and analysis of high - resolution NMR spectra , \" International Journal of Data Mining and Bioinformatics , vol .", "label": "", "metadata": {}, "score": "80.629105"}
{"text": "The dataset used for testing originated from a 12 salt step MudPIT of a whole cell yeast lysate having more than 1800 identified proteins ; this is far more complex than the average proteomic experiment .Therefore , more combinatorial possibilities were available , increasing computation time .", "label": "", "metadata": {}, "score": "80.8719"}
{"text": "All the important metabolites selected by both t -Test ( . ) as well .Consistent with previous metabolomics study , dysregulated metabolic pathways , such as glycolysis , TCA cycle , urea cycle , pyrimidine metabolism , polyamine metabolism as well as gut microbial - host cometabolism were observed [ 26 ] .", "label": "", "metadata": {}, "score": "81.37512"}
{"text": "J Proteome Res 2006 , 5 : 2909 - 2918 .View Article PubMed .Chen EI , Cociorva D , Norris JL , Yates JR III : Optimization of mass spectrometry - compatible surfactants for shotgun proteomics .J Proteome Res 2007 , 6 : 2529 - 2538 .", "label": "", "metadata": {}, "score": "81.67946"}
{"text": "Therefore , distance measures start losing their effectiveness to measure dissimilarity in highly dimensional spaces .Since classifiers depend on these distance measures ( e.g. Euclidean distance , Mahalanobis distance , Manhattan distance ) , classification is often easier in lower - dimensional spaces where less features are used to describe the object of interest .", "label": "", "metadata": {}, "score": "82.05736"}
{"text": "I understand that the article would become a bit too long if that needs to be explained as well , but I think it 's better than suggesting PCA as a good technique to reduce overfitting .Hi Jonas , good point !", "label": "", "metadata": {}, "score": "82.631805"}
{"text": "comments .Thanks !More posts will follow I 'm trying to lay out some basics in my first few posts such that I can start discussing more interesting topics ( object tracking , detection in video , etc . ) in the near future .", "label": "", "metadata": {}, "score": "82.65582"}
{"text": "Metabolomic data analysis becomes increasingly challenging when dealing with clinical samples with diverse demographic and genetic backgrounds and various pathological conditions or treatments .In this paper we comparatively evaluated the four classifiers , PLS , SVM , LDA , and RF , in the analysis of clinical metabolomic data derived from gas chromatography mass spectrometry platform of healthy subjects and patients diagnosed with colorectal cancer , where cross - validation , .", "label": "", "metadata": {}, "score": "82.73527"}
{"text": "The Curse of Dimensionality in classification .In this article , we will discuss the so called ' Curse of Dimensionality ' , and explain why it is important when designing a classifier .In the following sections I will provide an intuitive explanation of this concept , illustrated by a clear example of overfitting due to the curse of dimensionality .", "label": "", "metadata": {}, "score": "83.16887"}
{"text": "Patients enrolled in this study were not on any medication before preoperative sample collection .The postoperative samples were collected on the 7 day after surgery .Sample collection protocol was approved by the Cancer Hospital Institutional Review Board and written consents were signed by all participants prior to the study .", "label": "", "metadata": {}, "score": "84.16905"}
{"text": "Table 1 .Evaluation of nSVM results on the spiking dataset using different parameters .nSVM was executed 20 times for each of the 8 specified conditions .PHS2 , ALB , CAH , and ITRA stand for the spiked protein markers , and the numbers in the respective columns indicate the ranking .", "label": "", "metadata": {}, "score": "84.17708"}
{"text": "Carvalho PC , Carvalho MG , Degrave W , Lilla S , De NG , Fonseca R , Spector N , Musacchio J , Domont GB : Differential protein expression patterns obtained by mass spectrometry can aid in the diagnosis of Hodgkin 's disease .", "label": "", "metadata": {}, "score": "84.53209"}
{"text": "We would like to create a classifier that is able to distinguish dogs from cats automatically .To do so , we first need to think about a descriptor for each object class that can be expressed by numbers , such that a mathematical algorithm , i.e. a classifier , can use these numbers to recognize the object .", "label": "", "metadata": {}, "score": "85.15643"}
{"text": "319 - 328 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .X. Y. Wu , Z. Y. Wu , and K. Li , \" Identification of differential gene expression for microarray data using recursive random forest , \" Chinese Medical Journal , vol .", "label": "", "metadata": {}, "score": "85.3624"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.576675"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "I would like to fit a linear regression model in R for predicting motorbike prices .My dataset has 13 variables , including number of kilometers driven , colour , month of the first registration , etc . ... .I 'm running a basic language classification task .", "label": "", "metadata": {}, "score": "85.57669"}
{"text": "PubMed .Science 1999 , 286 : 531 - 537 .View Article PubMed .Yang YH , Xiao Y , Segal MR : Identifying differentially expressed genes from microarray experiments via statistic synthesis .Bioinformatics 2005 , 21 : 1084 - 1093 .", "label": "", "metadata": {}, "score": "85.796265"}
{"text": "Figure 4 exemplifies PatternLab 's GUI to access the feature selection methods and a result analyzer .In a future version of PatternLab , we intend to add new components to the result analysis module .Figure 5 exemplifies nSVM 's interface .", "label": "", "metadata": {}, "score": "85.79878"}
{"text": "Tnx a lot for your feedback and for summarizing the paper mentioned .I will change the article to include this one of these days ( do n't have time now ) .[ ... ] you are not yet familiar with this approach and its immanent problems of overfitting , etc . read \" The Curse of Dimensionality in classification \" ) .", "label": "", "metadata": {}, "score": "85.90478"}
{"text": "In the three - dimensional feature space , we can now find a plane that perfectly separates dogs from cats .This means that a linear combination of the three features can be used to obtain perfect classification results on our training data of 10 images : .", "label": "", "metadata": {}, "score": "86.38659"}
{"text": "The chromatography proceeds in cycles , each of which consists of increasing salt concentration to \" bump \" peptides off the SCX followed by a hydrophobic gradient to progressively elute peptides from the RP into the ion source .This process identifies mixture components by tandem mass spectrometry ( MS / MS ) .", "label": "", "metadata": {}, "score": "86.60336"}
{"text": "View Article PubMed .Liu H , Sadygov RG , Yates JR III : A model for random sampling and estimation of relative protein abundance in shotgun proteomics .Anal Chem 2004 , 76 : 4193 - 4201 .View Article PubMed .", "label": "", "metadata": {}, "score": "87.184166"}
{"text": "As mentioned before , instances in the corners of the feature space are much more difficult to classify than instances around the centroid of the hypersphere .For an 8-dimensional hypercube , about 98 % of the data is concentrated in its 256 corners .", "label": "", "metadata": {}, "score": "87.28245"}
{"text": "A total of 187 variables ( areas of peaks , denoting concentrations of metabolites ) , 35 metabolites were obtained from the spectral data analysis .Normalization ( to the total intensity to compensate for the overall variability during sample extraction , injection , detection , and disparity of urine volume ) , mean centering , and unit variance scaling of the data sets were performed prior to statistical analysis .", "label": "", "metadata": {}, "score": "87.28826"}
{"text": "Figure 9 .Training samples that fall outside the unit circle are in the corners of the feature space and are more difficult to classify than samples near the center of the feature space .An interesting question is now how the volume of the circle ( hypersphere ) changes relative to the volume of the square ( hypercube ) when we increase the dimensionality of the feature space .", "label": "", "metadata": {}, "score": "87.2941"}
{"text": "1145 - 1159 , 1997 .View at Publisher \u00b7 View at Google Scholar . 8 , no .10 , pp .4844 - 4850 , 2009 .View at Publisher \u00b7 View at Google Scholar I applied PCA on a 12000 x 500 data set ( 12000 data points with 500 features ) .", "label": "", "metadata": {}, "score": "87.382706"}
{"text": "The current parser can address both SEQUEST followed by DTASelect [ 15 ] and MASCOT having results exported to the DTASelect format .To use the parser , one should place the DTASelect results from the control and case analyses in different folders and then simply indicate their paths in the GUI .", "label": "", "metadata": {}, "score": "87.95923"}
{"text": "The green and pinkish structures in the upper part of the simulator represent the strong cation exchange and the reverse phase material packed in the capillary ( yellow structure ) .The semi - conical structure represents the mass spectrometer nozzle ( entrance ) and the structure below is an X - Ray of a quadrupole ion trap .", "label": "", "metadata": {}, "score": "88.086876"}
{"text": "Thank you so much for putting the effort and time into writing this post !You did an excellent job .Although I 'm not new to ML , there is something about the way you explained everything that just sort of ties together everything I know about dimensionality .", "label": "", "metadata": {}, "score": "88.40859"}
{"text": "View Article PubMed .Pang JX , Ginanni N , Dongre AR , Hefta SA , Opitek GJ : Biomarker discovery in urine by proteomics .J Proteome Res 2002 , 1 : 161 - 169 .View Article PubMed .Wang W , Zhou H , Lin H , Roy S , Shaler TA , Hill LR , Norton S , Kumar P , Anderle M , Becker CH : Quantification of proteins and metabolites by mass spectrometry without isotopic labeling or spiked standards .", "label": "", "metadata": {}, "score": "88.832794"}
{"text": "Good luck in your work , and I 'm looking forward to reading all of your posts .Thank you for your kinds words , Bryan !More articles will follow .The first few articles cover the basics of machine learning .", "label": "", "metadata": {}, "score": "90.12832"}
{"text": "[ 10 ] .Experiments that do not acquire replicate readings such as the latter or that have very few readings for each class ( one or two ) fall within ACFold 's niche .As shown in Figure 2 , 2687 proteins were identified , of which 104 were pinpointed as being differentially expressed according to ACFold ( using a minimum fold - change cutoff of 2.5 , an AC test p -value of 0.05 , and an FDR of 0.1 ) .", "label": "", "metadata": {}, "score": "90.222786"}
{"text": "Multi - dimensional Protein Identification Technology ( MudPIT ) is a shotgun proteomics technique capable of identifying thousands of proteins in proteolytically digested complex mixtures [ 2 , 3 ] .MudPIT separates peptides according to two independent physicochemical properties using two - dimensional liquid chromatography ( LC / LC ) online with the ion source of a mass spectrometer .", "label": "", "metadata": {}, "score": "90.65785"}
{"text": "thanks for your intuitive explanation about the curse of dimensionality . heard of the issue many times roughly but did n't find anything as thorough and informative as this article .thank u for your sharing .Very inspiring .I like the way you argumented .", "label": "", "metadata": {}, "score": "92.18225"}
{"text": "Figure 10 shows how the volume of this hypersphere changes when the dimensionality increases : .Figure 10 .The volume of the hypersphere tends to zero as the dimensionality increases .This shows that the volume of the hypersphere tends to zero as the dimensionality tends to infinity , whereas the volume of the surrounding hypercube remains constant .", "label": "", "metadata": {}, "score": "92.41856"}
{"text": "In the earlier introduced example of cats and dogs , let 's assume there are an infinite number of cats and dogs living on our planet .However , due to our limited time and processing power , we were only able to obtain 10 pictures of cats and dogs .", "label": "", "metadata": {}, "score": "93.9237"}
{"text": "PHS2 , ALB , CAH , and ITRA stand for the spiked protein markers , and the numbers in the respective columns indicate ranking according to Student 's t - test applied to the sparse matrix normalized by Z or Total Signal normalization .", "label": "", "metadata": {}, "score": "94.46942"}
{"text": "Besides these classification performances , the variable ranking and putative biomarker selection power of RF was examined as well by Pearson correlation .Methods .Metabolomic Data Set .Colorectal cancer ( CRC ) is one of the common types of cancer and the leading causes of cancer death in the world [ 20 ] .", "label": "", "metadata": {}, "score": "99.55528"}
{"text": "Anal Chem 2006 , 78 : 493 - 500 .View Article PubMed .Katajamaa M , Oresic M : Processing methods for differential analysis of LC / MS profile data .BMC Bioinformatics 2005 , 6 : 179 .View Article PubMed .", "label": "", "metadata": {}, "score": "100.00244"}
{"text": "View Article PubMed .Washburn MP , Wolters D , Yates JR III : Large - scale analysis of the yeast proteome by multidimensional protein identification technology .Nat Biotechnol 2001 , 19 : 242 - 247 .View Article PubMed .", "label": "", "metadata": {}, "score": "101.94573"}
{"text": "MudPIT simulator .The image displays the graphical user interface of the MudPIT simulator available on the project 's website for didactic purposes .The simulator allows one to specify MudPIT parameters and then see the two - dimensional liquid chromatography simulation proceed on the fly .", "label": "", "metadata": {}, "score": "103.33244"}
{"text": "23 proteins ( orange dots ) did not meet the fold - change cutoff but were indicated as statistically differentially expressed , therefore deserving a second look .267 proteins ( green dots ) met the fold - change cutoff ; however , the AC test indicated that this happened by chance .", "label": "", "metadata": {}, "score": "104.978546"}
{"text": "Center for Translational Medicine and Shanghai Key Laboratory of Diabetes Mellitus , Department of Endocrinology and Metabolism , Shanghai Jiao Tong University Affiliated Sixth People 's Hospital , Shanghai 200233 , China .Received 10 November 2012 ; Accepted 11 December 2012 .", "label": "", "metadata": {}, "score": "108.1469"}
{"text": "Authors ' Affiliations .Systems Engineering and Computer Science Program , COPPE , Federal University of Rio de Janeiro .Laboratory for Protein Chemistry , Chemistry Institute , and the Rio de Janeiro Proteomic Network , Federal University of Rio de Janeiro .", "label": "", "metadata": {}, "score": "109.354965"}
{"text": "24 , pp .2492 - 2496 , 2008 .View at Google Scholar \u00b7 View at Scopus .705 , no . 1 - 2 , pp .56 - 63 , 2011 .View at Publisher \u00b7 View at Google Scholar .", "label": "", "metadata": {}, "score": "113.39258"}
{"text": "Copyright \u00a9 2013 Tianlu Chen et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "125.25984"}
