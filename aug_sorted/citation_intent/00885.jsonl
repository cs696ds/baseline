{"text": "This illustrates how the evaluator may be used without an embedded classifier -- cases are just added in terms of the first - best answer and the response classification .The meat of this implementation is in pulling out the subjective sentences .", "label": "", "metadata": {}, "score": "26.320522"}
{"text": "The changing weights of sample vectors in the first phase , as well as increasing the size of classifier pool , can be easily recast as a moving classification goal , described in [ 3 ] .This is an additional reason why we only update a single weak classifier , rather than all of them , since , otherwise , the drift would be proportional to the number of classifiers added , significantly penalizing convergence rate .", "label": "", "metadata": {}, "score": "29.6442"}
{"text": "Specifically , we plan for it to be incorporated as part of an active learning and/or interactive learning process .Conclusions .This paper describes a simple sample size prediction algorithm that conducts weighted fitting of learning curves .When tested on free text and waveform classification with active and passive sampling methods , the algorithm outperformed the un - weighted algorithm described in previous literature in terms of goodness of fit measures .", "label": "", "metadata": {}, "score": "30.653036"}
{"text": "However , with a neural network classifier and much higher dimensionality features , all feature sets perform similarly in terms of classification accuracy .As just illustrated , dimensionality reduction is not necessarily advantageous in terms of accuracy for classifiers trained with enough data and the \" right \" classifier .", "label": "", "metadata": {}, "score": "30.662592"}
{"text": "These heterogeneous features can not be used all together to train a single classifier ( and even if they could - by converting all features into a vector of scalar values - such a training is unlikely to be successful ) .", "label": "", "metadata": {}, "score": "31.817219"}
{"text": "We pooled the RMSE values and conducted a paired t - test .We conducted a similar analysis comparing the MAE between the two methods and obtained similar results .Discussion .In this paper we described a relatively simple method to predict a classifier 's performance for a given sample size , through the creation and modelling of a learning curve .", "label": "", "metadata": {}, "score": "31.869297"}
{"text": "If the underlying assumption of zero mean multivariate Gaussian random variables is satisfied , then this method of feature reduction generally performs very well .The principal components are also statistically independent for the Gaussian case .The principal components \" account for \" or explain the maximum amount of variance of the original data .", "label": "", "metadata": {}, "score": "33.046036"}
{"text": "Prior studies have stated that the variance is not an unbiased estimator when a model is tested on new data [ 1 ] .Hence , our confidence intervals may sometimes be optimistic .A major limitation of the methods is that an initial set of annotated data is needed .", "label": "", "metadata": {}, "score": "33.240913"}
{"text": "Several of the learning curves used in this paper were generated using active learning techniques .Progressive sampling , on the other hand , focuses more on minimizing the amount of computation for a given performance target .For instance , Provost et al .", "label": "", "metadata": {}, "score": "33.303223"}
{"text": "In our study , the Gaussian kernel SVM generates a result that is not significantly different from its linear counterpart , but at the expense of an exhaustive grid search .From this , we recommend a linear classifier for a P300 spelling systems for patients , also since , to support its online applicability , we have to minimize the classifier 's training time .", "label": "", "metadata": {}, "score": "33.415035"}
{"text": "This work accepts only perfect ( no training error ) linear hyperplane classifiers from the above method .This criteria for accepting only perfect classification is very stringent .We first thought that this would lead to few result sets being found , and that allowing non - perfect classification would need be done to find more classifiers .", "label": "", "metadata": {}, "score": "33.929283"}
{"text": "This paper describes a simple and effective sample size prediction algorithm that conducts weighted fitting of learning curves .The algorithm outperformed an un - weighted algorithm described in previous literature .It can help researchers determine annotation sample size for supervised machine learning .", "label": "", "metadata": {}, "score": "34.17202"}
{"text": "The first model applied a linear combination on a set of characteristics so as to extract the best candidate sentence , whereas the second model was based on the predicate calculus , using another set of characteristics .Our results not only confirm that argumentation plays an important role to drive the extraction of functional descriptions in life sciences ' texts , but also show how complementary information can be directly extracted from the Gene Ontology controlled - vocabulary using a generic text categorization engine .", "label": "", "metadata": {}, "score": "34.20433"}
{"text": "Even so , 85 % is higher than our standalone simple classifier , and 86.4 % is the best hierarchical performance reported by Pang and Lee using SVMs to classify polarity stacked on top of a naive Bayes subjectivity classifier .Inspecting the Code .", "label": "", "metadata": {}, "score": "34.424324"}
{"text": "However , finding very small sets of features able to correctly classify the data is problematic as the fundamental mathematical proposition is hard .Existing methods can find \" small \" feature sets , but give no hint how close this is to the true minimum size .", "label": "", "metadata": {}, "score": "34.548923"}
{"text": "If the data are primarily clustered on curved subspaces embedded in high dimensionality feature spaces , linear transformations for feature dimensionality reduction are not well suited .For example , the data depicted in Figure 2 would be better approximated by its position with respect to a curved U - shape line rather the straight line obtained with linear PCA .", "label": "", "metadata": {}, "score": "35.010777"}
{"text": "We could also build a class that reads both models in from a file , or takes both classifiers as parameters , or any number of other solutions in - between .By constructing a hierarchical classifier in any of these ways , it may be supplied to an evaluator .", "label": "", "metadata": {}, "score": "35.295845"}
{"text": "Therefore , in addition to the parameter search described in [ 14 ] and re - used here , we also performed a coarse - grid threshold searching strategy in [ 0,1 ] with step 0.05 .Assuming that the test corpus has similar characteristic as the training one - the usual guess in the absence of further knowledge - we selected the threshold between positive and negative classes such that their ratio approximated the best the ratio measured on the training set .", "label": "", "metadata": {}, "score": "35.347176"}
{"text": "They also explored convergence detection methods for progressive sampling and selected a convergence method that used linear regression with local sampling ( LRLS ) .In LRLS , the slope of a linear regression line that has been built with r points sampled around the neighborhood of the last sample size is compared to zero .", "label": "", "metadata": {}, "score": "35.368614"}
{"text": "Certain problems are just too difficult for a given classifier to solve .In fact , the decision boundary that separates data from different classes may be too complex , or lie outside the space of functions that can be implemented by the chosen classifier model .", "label": "", "metadata": {}, "score": "35.398438"}
{"text": "However , it has been shown that a properly trained ensemble decision is usually correct if its confidence is high , and usually incorrect if its confidence is low .Using such an approach then , the ensemble decisions can be used to estimate the posterior probabilities of the classification decisions ( Muhlbaier 2005 ) .", "label": "", "metadata": {}, "score": "35.407505"}
{"text": "The combination is fully based on empirical heuristics derived from a relatively small tuning data set , avoiding overfitting phenomena .The argumentative filtering yielded effective results during the TREC-2003 challenge [ 21 ] .Combining this approach with the Gene Ontology - driven ranking module , which takes advantage of the available protein annotation , improves the lexical overlap - measured by Dice metrics - by about 10 % compared to the baseline .", "label": "", "metadata": {}, "score": "35.565964"}
{"text": "Metrics .In general , for each input text the data mining techniques yield a ranked list of candidates .Thus , sentence filtering like information extraction and text categorization may be formally evaluated by recall and precision measures .However , we must recognize that it is hard to obtain complete agreement regarding the appropriate measure that should be used for sentence comparison .", "label": "", "metadata": {}, "score": "35.604855"}
{"text": "For automatic summarization , feature selection and weighting , often based on term frequency and inverse document frequency factors ( tf.idf ) have been reported .Conclusions reached are however not always consistent [ 7 ] with respect to tf.idf .Among other interesting features , both sentence location as well as sentence length seem important [ 8 ] .", "label": "", "metadata": {}, "score": "35.620598"}
{"text": "A linear classifier , one that is capable of learning linear boundaries , can not learn this complex non - linear boundary .However , appropriate combination of an ensemble of such linear classifiers can learn any non - linear boundary .", "label": "", "metadata": {}, "score": "35.841347"}
{"text": "For future work , we intend to integrate the function to predict sample size into our NLP software .The purpose is to guide users in text mining and annotation tasks .In clinical NLP research , annotation is usually expensive and the sample size decision is often made based on budget rather than expected performance .", "label": "", "metadata": {}, "score": "35.95043"}
{"text": "Therefore , individual classifiers in an ensemble system need to make different errors on different instances .The intuition , then , is that if each classifier makes different errors , then a strategic combination of these classifiers can reduce the total error , a concept not too dissimilar to low pass filtering of the noise .", "label": "", "metadata": {}, "score": "35.998245"}
{"text": "Chen et al.5apply SVMs to document classification of biological literature .Brank et al.6study the interaction of feature ranking and selection with the learning algorithm , in particular feature selection through linear SVMs , which then are used to train the SVM classifier .", "label": "", "metadata": {}, "score": "36.141094"}
{"text": "They then increase the number of annotations if the target performance could not be reached , based on the vague but generally correct belief that performance will improve with a larger sample size .The amount of improvement though can not be known without the modelling effort we describe in this paper .", "label": "", "metadata": {}, "score": "36.31227"}
{"text": "Thus they are not likely to perform well as predictors on new samples , or in different experiment setups .Classifiers made from large data sets are more likely to be reproducible and perform well in other situations .Classifiers with large margin and zero LOO error are more likely to indicate real biological effects that would hold true on new patient samples .", "label": "", "metadata": {}, "score": "36.348106"}
{"text": "Given the purpose of predicting future performance , our method assigned higher weights to data points associated with larger sample size .The evaluation experiments were conducted on free text and waveform data , using passive and active learning algorithms .Prior studies typically used a single type of data ( e.g. microarray or text ) and a single type of sampling algorithm ( i.e. random sampling ) .", "label": "", "metadata": {}, "score": "36.39479"}
{"text": "Since such a network has .parameters to be trained , it can easily overfit the training data in the case of a large number of features ( . , and a large number of hidden layer neurons ( .To avoid this , we performed a 5-fold cross - validation with a line search for the number of hidden neurons .", "label": "", "metadata": {}, "score": "36.403755"}
{"text": "There are newer versions of these methods such as Heteroscedastic Discriminant Analysis ( HDA ) ( Kumar & Andreou , 1998 ; Saon et al . , 2000 ) .However , in all cases certain assumptions are made about the statistical properties of the original data ( such as multivariate Gaussian ) ; even more fundamentally , the transformations are restricted to be linear .", "label": "", "metadata": {}, "score": "36.504986"}
{"text": "To ensure that individual boundaries are adequately different , despite using substantially similar training data , weaker or more unstable classifiers are used as base models , since they can generate suffi - ciently different decision boundaries even for small perturbations in their training parameters .", "label": "", "metadata": {}, "score": "36.517536"}
{"text": "Other applications of ensemble learning include assigning a confidence to the decision made by the model , selecting optimal ( or near optimal ) features , data fusion , incremental learning , nonstationary learning and error - correcting .This article focuses on classification related applications of ensemble learning , however , all principle ideas described below can be easily generalized to function approximation or prediction type problems as well .", "label": "", "metadata": {}, "score": "36.517586"}
{"text": "Adding to this the fact that in most real - time applications the exact form of the data distribution is not known , we have decided to compare algorithm performance using experimental data .Possible Extensions .In this section , we discuss several possible extensions of our algorithm .", "label": "", "metadata": {}, "score": "36.657288"}
{"text": "As another powerful approach , the softmax function takes all the nodes in a layer into account and calculates the output of a node as a posterior probability .When the outputs of the network are to be used as transformed features for the HMM recognition , a linear function or a softmax function is appropriate to generate the data with a more diverse distribution , such as one that would be well - modeled with a GMM .", "label": "", "metadata": {}, "score": "36.81138"}
{"text": "Nor an improvement on the ensemble 's average performance can be guaranteed except for certain special cases ( Fumera 2005 ) .Hence combining classifiers may not necessarily beat the performance of the best classifier in the ensemble , but it certainly reduces the overall risk of making a particularly poor selection .", "label": "", "metadata": {}, "score": "37.001026"}
{"text": "Hierarchical Polarity Analysis .Pang and Lee ( 2004 ) introduce a hierarchical approach to classification .Specifically , they use the subjectivity classifier to extract subjective sentences from reviews to be used for polarity classification .Hierarchical models are quite common in the classification and general statistics and machine learning literatures .", "label": "", "metadata": {}, "score": "37.06515"}
{"text": "This section covers a second form of sentiment analysis , namely determining if a sentence is \" objective \" or \" subjective \" ( again , as defined by the database curators ) .It follows pretty much the same pattern as the last example , with a slightly different data format , the addition of the classifier evaluation framework from com.aliasi.classify , and a step to compile the model to a file for later use .", "label": "", "metadata": {}, "score": "37.08411"}
{"text": "The underlying complex decision boundary can then be approximated by an appropriate combination of different classifiers .In many applications that call for automated decision making , it is not unusual to receive data obtained from different sources that may provide complementary information .", "label": "", "metadata": {}, "score": "37.386364"}
{"text": "However , it is also possible that one of the nonlinear methods for feature reduction is more effective , that is enable higher ASR accuracy , than any of the linear methods .The comparisons of these various methods can only be done experimentally .", "label": "", "metadata": {}, "score": "37.39884"}
{"text": "Since the size of currently available training corpora do not keep up with the linguistic diversity , we see two alternatives as possible solutions .The first , computationally more economic strategy focuses on decreasing the linguistic variability using graph rewriting rules on the parse level ( see , for instance , [ 37 , 38 ] ) .", "label": "", "metadata": {}, "score": "37.422844"}
{"text": "The explicit assumption for LDA is that the within class covariance of each category is the same , which is rarely true in practice .Nevertheless , for many practical classification problems , features reduced by LDA often are as effective or even advantageous to original higher dimensional features .", "label": "", "metadata": {}, "score": "37.541412"}
{"text": "In this case , the human annotator chooses to combine different segments , taken from various sentences or hypothetically from the full - text of the article .Table 8 .Performance of each basic strategy and their combination .The top combination ( 57.29 % ) achieves 80.7 % of the theoretical upper bound ( 70.96 % ) .", "label": "", "metadata": {}, "score": "37.695152"}
{"text": "However , the recognition accuracy is often degraded .The more hidden nodes a network has , the more complex a decision surface can be formed , and thus better classification accuracy can be expected ( Meng , 2006 ) .Generally , the number of hidden nodes is empirically determined by a combination of accuracy and computational considerations , as applied to a particular application .", "label": "", "metadata": {}, "score": "37.72553"}
{"text": "This is perhaps the primary reason why ensemble based systems are used in practice : what is the most appropriate classifier for a given classification problem ?The most commonly used procedure - choosing the classifiers with the smallest error on training data - is unfortunately a flawed one .", "label": "", "metadata": {}, "score": "37.83024"}
{"text": "These observations lower the hope that novel types of kernels ( using the same input representation ) will be able to achieve a breakthrough in PPI extraction performance .We conclude that one should be rather pessimistic in terms of expecting breakthroughs in kernel - based methods to PPI extraction .", "label": "", "metadata": {}, "score": "37.92421"}
{"text": "It determines sample size based on standardized fold change , class prevalence , and number of genes or features on the arrays .Another more generic approach is to fit a classifier 's learning curve created using empirical data to inverse power law models .", "label": "", "metadata": {}, "score": "38.00664"}
{"text": "Then , a description the different methods and their combinations , as well as the metrics defined for the task .Finally , reports on results and conclusion .Background and applications .Summarization .So that , it seems simpler and more effective to view the summarization problem as a sentence extraction problem .", "label": "", "metadata": {}, "score": "38.1006"}
{"text": "We also observe that the vertical standard deviation is in general smaller than the horizontal one ( .for factor \" direction \" ) , particularly for the most accurate classifiers and , especially , after more than 5 - 6 intensification sequences .", "label": "", "metadata": {}, "score": "38.127846"}
{"text": "The high - level idea is to use LingPipe 's language classification framework to do two classification tasks : separating subjective from objective sentences , and separating positive from negative movie reviews .In the third section , we show how to build a hierarchical classifier by composing these models .", "label": "", "metadata": {}, "score": "38.180817"}
{"text": "In particular , we showed that we can transform our top performing recall - oriented categorizer into a competitive precision - oriented system , just by setting a threshold on the CSV , what resulted in a precision close to 80 % .", "label": "", "metadata": {}, "score": "38.215603"}
{"text": "This work focuses on subsets smaller than those produced by existing algorithms : all subsets of one- , two- , ( and sometimes three- ) features that can be separated by a linear surface without error .A multiplicity of error - free linear classifiers constructed from few features could facilitate the creation of cost - effective clinical tests and guide further basic research .", "label": "", "metadata": {}, "score": "38.27594"}
{"text": "Some other methods attempt to find the sample size needed to reach a target performance ( e.g. a high correlation coefficient ) [ 22 - 25 ] .Within this category we find methods that predict the sample size required for a classifier to reach a particular accuracy [ 2 , 4 , 26 ] .", "label": "", "metadata": {}, "score": "38.32372"}
{"text": "Interestingly , while sentence extraction is normally seen as a preliminary step toward assigning Gene Ontology descriptors , we observe here that such a controlled - vocabulary can be used a priori to guide the sentence selection process , from which more accurate category assignment could be expected !", "label": "", "metadata": {}, "score": "38.498413"}
{"text": "Thus , while some recent studies have made use of Kim 's method [ 9 - 12 ] , most profiling studies neither consider nor report small sized discriminatory feature subsets .The number of free parameters in this method is m + 1 , hence is very small relative to the size of the data , greatly reducing the problem of over - fitting to the training data ( see Random Data section ) .", "label": "", "metadata": {}, "score": "38.501205"}
{"text": "While these results reveal attractive performance levels when compared to other runs in the TREC-2003 genomic evaluation campaign [ 21 ] , several teams were faced with the same extraction problem yet suggested other interesting approaches .These authors suggested basing this selection on a Naive Bayes [ 35 ] machine learning approach .", "label": "", "metadata": {}, "score": "38.577324"}
{"text": "This statement is also supported by other researchers ( e.g . , in [ 12 ] ) .In light of this , and since a classifier comparison has never been performed on patients , it remains an open question what is the best classifier in this case .", "label": "", "metadata": {}, "score": "38.614113"}
{"text": "A larger margin means the data are more well separated hence is more resilient to noise and more likely biologically relevant .Example plot using two hypothetical genes .Each data point is labeled with the class , and the separating plane is computed to be positioned halfway between the two classes .", "label": "", "metadata": {}, "score": "38.66323"}
{"text": "Our comparison indicates that , in general , nonlinear classifiers perform worse or equal to linear ones .This is in accordance with other studies [ 10 - 12 ] , which were performed on healthy subjects .This could be due to the tendency of nonlinear classifiers to overfit the training data , leading to an inferior generalization performance .", "label": "", "metadata": {}, "score": "38.67746"}
{"text": "However , to generate predictive models , the supervised learning techniques need an annotated training sample .Literature suggests that the predictive power of the classifiers is largely dependent on the quality and size of the training sample [ 1 - 6 ] .", "label": "", "metadata": {}, "score": "38.840233"}
{"text": "We investigate how kernels ' input representations correlate with their similarity .Finally , to quantify the claimed advantage of kernels for PPI extraction , we compare kernels to more simple methods .We used linear , non - kernel based classifiers and a surface feature set also found in the kernel methods .", "label": "", "metadata": {}, "score": "38.99894"}
{"text": "The decisions made by each classifier can then be combined by any of the combination rules described below .Confidence Estimation .The very structure of an ensemble based system naturally allows assigning a confidence to the decision made by such a system .", "label": "", "metadata": {}, "score": "39.0185"}
{"text": "The alternative hypothesis is that the means of the RMSE and MAE of the baseline method is greater than those of our weighted fitting method .Results .Using the 3 datasets and 4 sampling methods , 12 actual learning curves are generated .", "label": "", "metadata": {}, "score": "39.02407"}
{"text": "For example , sentence filtering for protein interactions was previously described in [ 12 ] .In the same vein , functional annotation of proteins [ 13 ] and protein interactions , which were respectively the subject of BioCreative I and II , are tasks demanding information extraction at the level of short passages .", "label": "", "metadata": {}, "score": "39.08976"}
{"text": "In general , only somewhat isolated points lying close to the classifier decision boundary will be found in error during LOO testing .Thus LOO error rates tend to be very low , 0 - 3 out of the total number of points .", "label": "", "metadata": {}, "score": "39.101875"}
{"text": "On the other hand , BLDA relies on assumed distributions of the classification errors and of the used parameters .From the obtained classification results , we observe that different classifiers lead to different accuracies .On the one hand , this shows the necessity to properly choose the classifier for the intended P300 BCI application .", "label": "", "metadata": {}, "score": "39.218643"}
{"text": "However , selecting the best CONCLUSION sentence is not sufficient ( such a strategy exhibits a Dice performance of 35.2 % ) , due to the fact that 45 % of GeneRiFs in the TREC evaluation set were strictly cut and paste from the article 's title .", "label": "", "metadata": {}, "score": "39.37959"}
{"text": "This allows creation of the classifiers that compensate for the errors introduced by the previous ones , and eventual convergence to the true distribution of the samples .After the weak classifier , boosting weights are adjusted , using vector . as an input .", "label": "", "metadata": {}, "score": "39.435715"}
{"text": "This leads to highly heterogeneous evaluation results indicating that methods are strongly prone to over - fit the training corpus .The focus of this paper is to perform a cross - kernel error analysis at the instance level with the goal to explore possible ways to improve kernel - based PPI extraction .", "label": "", "metadata": {}, "score": "39.454765"}
{"text": "Many difficult false positives turned out to be misinterpreted linguistic constructs like enumerations and coreferences .Such constructs might be more appropriately dealt with by using linguistic / syntactical patterns .Note , however , that some other pairs found in sentences with such constructs ( e.g. B.d765.s0.p10 , A.d28.s234.p1 ) were correctly annotated by all kernel methods in our assessment .", "label": "", "metadata": {}, "score": "39.53487"}
{"text": "The original data [ 1 ] was re - normalized using the Intensity / local then Spatial / local methods as implemented in the BioConductor R package [ 15 ] ( this is the best performing method as outlined in , Wei Wu , unpublished 2004 ) .", "label": "", "metadata": {}, "score": "39.58542"}
{"text": "..The average precision being much higher than the accuracy tells us that we are doing a good job ranking by confidence in that we tend to be more correct when we are more confident .Macro- and Micro - Averaged Results .", "label": "", "metadata": {}, "score": "39.650623"}
{"text": "For sentence splitting , we developed a robust tool based on manually crafted regular expressions .The tool can detect sentence boundaries with more than 97 % precision on MEDLINE abstracts , and was deemed competitive with more elaborate methods [ 24 ] .", "label": "", "metadata": {}, "score": "39.69484"}
{"text": "] , which was used as input to either the linear classifier .( see further ) or the nonlinear one , .Since we use . scores as features , and since we use a balanced training set ( equal numbers of target and nontarget responses ) , the parameter . should be close to zero .", "label": "", "metadata": {}, "score": "39.698463"}
{"text": "Existing classification and feature selection techniques can be employed to ascertain the cardinality of a feature subset yielding a classifier that generalizes well , i.e . , one which makes zero ( or few ) errors in assigning the class of an unseen data point .", "label": "", "metadata": {}, "score": "39.81416"}
{"text": "If some of the measured genes reflect this mutation status we would apriori expect to find some ( possibly many ) small feature sets , and not finding any could indicate errors in the class labels ( a sample is mislabeled ) .", "label": "", "metadata": {}, "score": "39.97449"}
{"text": "Or to phrase it differently : what is the expected classification performance for a given training sample size ?Problem formulation .Our interest in sample size prediction stemmed from our experiments with active learning .Active learning is a sampling technique that aims to minimize the size of the training set for classification .", "label": "", "metadata": {}, "score": "40.135513"}
{"text": "Empirically , we found that j needed to be greater than or equal to 5 for the curve fitting to be effective .In many studies , as well as ours , the learning curves appear to be smooth because each data point on the curve is assigned the average value from multiple experiments ( e.g. 10-fold cross validation repeated 100 times ) .", "label": "", "metadata": {}, "score": "40.15074"}
{"text": "When an additional classifier is created , a new . is created and initialized with zeroes .:The number of iterations current weak classifier have been trained .Using established convergence bounds of Pegasos algorithm , can be used as a simplest form of cutoff parameter , that is , each additional weak classifier is trained on a fixed number of samples .", "label": "", "metadata": {}, "score": "40.225025"}
{"text": "In addition , a difficulty in neural network training is that the input data has a wide range of means and variances for each feature component .In order to avoid this , the input data of neural networks is often scaled so that all feature components have the same mean ( zero ) and variance ( so that range of values is approximately \u00b1 1 ) .", "label": "", "metadata": {}, "score": "40.370026"}
{"text": "This tutorial essentially reimplements the basic classifiers and then the hierarchical classification technique described in Bo Pang and Lillian Lee 's 2004 ACL paper \" A sentimental education . \"Downloading Training Corpora .Luckily for us , Lillian Lee and Bo Pang have provided annotated slices of movie review data for polarity ( both boolean and scalar ) , and subjectivity .", "label": "", "metadata": {}, "score": "40.588734"}
{"text": "This number increases when different types of decision boundaries are permissible for each value of m .The scope of the problem can be reduced and simplified if only m -feature linear classifiers ( m -LCs ) are considered .This restriction of neglecting non - linear decision surfaces is reasonable because hyperplanes can be calculated efficiently , and Support Vector Machines with linear kernels are sufficient for classification problems associated with profiling data ( see for example [ 3 - 6 ] ) .", "label": "", "metadata": {}, "score": "40.66333"}
{"text": "Given that the compute time is small enough and the results possibly important , we conclude that examining microarray data for single genes , gene pairs and maybe gene triples should be done routinely .When performed along with acquiring the biological data , the results can be used as a quality check on the experimental process .", "label": "", "metadata": {}, "score": "40.772507"}
{"text": "The first method considers that , apart from the title , the best GeneRiF candidate should appear in the article 's conclusion or purpose sections .The second extraction approach is based on a generic categorization framework , which is designed to estimate the density of Gene Ontology concepts in the selected sentences .", "label": "", "metadata": {}, "score": "40.834034"}
{"text": "Indeed , together with bringing signals , full - texts also brings massive amounts of noise .Conclusion .This paper focuses on the extraction of gene function sentences ( so - called GeneRiF ) from a MEDLINE record given a gene name , as proposed in the TREC Genomics Track in 2003 [ 21 ] .", "label": "", "metadata": {}, "score": "41.010536"}
{"text": "The first line simply breaks the review into sentences .We then create a priority queue of objects ordered by score with a maximum size of the maximum number of sentences returned .Then we just iterate over the sentences and classify them with the subjectivity classifier we created in the constructor .", "label": "", "metadata": {}, "score": "41.149147"}
{"text": "The output segment is used for comparison to the correct GeneRiF provided by LocusLink 's annotators , as explained in the next section .We have tried to apply the sentence reduction filter , either after of before the combination , without observing any change regarding the final output .", "label": "", "metadata": {}, "score": "41.191414"}
{"text": "These averages are over the one - versus - all results , which are broken out category - by - category at the end of the report ( see below ) .The thing to remember is that the macro - averaged results average the one - versus - all results with each category weighted equally : .", "label": "", "metadata": {}, "score": "41.245186"}
{"text": "Surface and parsing features generated from sentence text used for training non - kernel based classifiers .number of conj constituents along the shortest path in the syntax tree .Kullback - Leibler divergence of constituent types in the entire syntax tree .", "label": "", "metadata": {}, "score": "41.347305"}
{"text": "In his 2000 review article , Dietterich lists three primary reasons for using an ensemble based system : i ) statistical ; ii ) computational ; and iii ) representational ( Dietterich 2000 ) .Note that these reasons are similar to those listed above .", "label": "", "metadata": {}, "score": "41.361786"}
{"text": "The goal of text classification is to label a document with a predefined set of categories .Usually the problem is ap- proached as supervised learning where classifiers are learned from examples in an automated way .A fairly recent development in the machine learning world has been the Management Architecture advent of support vector machines ( SVMs).2,3Joachims4 explores the use of SVMs for learning text classifiers .", "label": "", "metadata": {}, "score": "41.36291"}
{"text": "For a detailed overview of these and other combination rules , see ( Kuncheva 2005 ) .Other applications of ensemble systems .Ensemble based systems can be used in problem domains other than improving the generalization performance of a classifier .", "label": "", "metadata": {}, "score": "41.37761"}
{"text": "For example , a series of multilayer perceptron ( MLP ) neural networks can be trained by using different weight initializations , number of layers / nodes , error goals , etc .Adjusting such parameters allows one to control the instability of the individual classifiers , and hence contribute to their diversity .", "label": "", "metadata": {}, "score": "41.426983"}
{"text": "We have also analyzed the distribution of the erroneously typed characters ( see Figure 5 ) .We have found that , for all classifiers , the misclassifications mostly occur for either a row or a column in close proximity to the ones of the intended characters ( represented at the center of the plot ) .", "label": "", "metadata": {}, "score": "41.44046"}
{"text": "What this is telling us is that the classifier was not at all confident about most of its decisions .When this is the case , classifiers tend to have a very high variance with respect to parameter settings .Although the classifier is n't very confident on average , what happens when we sort the output by confidence and return answers we are most confident in first ?", "label": "", "metadata": {}, "score": "41.49327"}
{"text": "Everything else being equal , one may be tempted to choose at random , but with that decision comes the risk of choosing a particularly poor model .Using an ensemble of such models - instead of choosing just one - and combining their outputs by - for example , simply averaging them - can reduce the risk of an unfortunate selection of a particularly poorly performing classifier .", "label": "", "metadata": {}, "score": "41.50664"}
{"text": "Such a set of classifiers is said to be diverse .Classifier diversity can be achieved in several ways .Preferably , the classifier outputs should be class - conditionally independent , or better yet negatively correlated .The most popular method is to use different training datasets to train individual classifiers .", "label": "", "metadata": {}, "score": "41.547493"}
{"text": "Finally , diversity can also be achieved by using different features , or different subsets of existing features .In fact , generating different classifiers using random feature subsets is known as the random subspace method ( Ho 1998 ) , as described later in this article .", "label": "", "metadata": {}, "score": "41.565575"}
{"text": "As shown in Figure 2 , the width of the confidence interval negatively correlates with the prediction accuracy .When the predicted value deviates more from the actual observation , the confidence interval tends to be wider .As such , the confidence interval provides an additional measure to help users make the decision in selecting a sample size for additional annotation and classification .", "label": "", "metadata": {}, "score": "41.633976"}
{"text": "Figure 3 : Experimental results .For the Covertype dataset , linear classifiers work best and approach the error rates indicated in the paper [ 12 ] , with Pegasos and our algorithm giving virtually the same results .Conclusion and Future Work .", "label": "", "metadata": {}, "score": "41.66436"}
{"text": "On the dependency tree level , the difference has a smaller extent : 4.56 ( negative ) and 4.15 ( positive ) .We conclude that according to all the three distance measures ( word , dependency tree , syntax distance ) , the farther the entities of negative pairs are located the more difficult are to classify .", "label": "", "metadata": {}, "score": "41.689278"}
{"text": "We report on the standard evaluation measures ( precision ( P ) , recall ( R ) , F 1 -score ( F ) ) .Therefore , in this study we stick to the above three measures , which actually give a better picture on the expected classification performance on new texts .", "label": "", "metadata": {}, "score": "41.758694"}
{"text": "418 - 435 , 1992 . E. Allwein , R. E. Schapire , and Y. Singer , \" Reducing Multiclass to Binary : A Unifying Approach for Margin Classifiers , \" Journal of Machine Learning Research , vol .1 , pp .", "label": "", "metadata": {}, "score": "41.804794"}
{"text": "This method models the observed classifier performance as a function of the training sample size , and uses the fitted curve to forecast the classifier 's future behaviour .Previous and related work .Sample size determination .Our method can be viewed as a type of sample size determination ( SSD ) method that determines sample size for study design .", "label": "", "metadata": {}, "score": "41.840584"}
{"text": "Several approaches to the solution exist , such as interior point [ 6 ] methods and the decomposition methods , such as SMO [ 7 ] .Also , recently , interest in gradient - based methods to solving primal SVM problem has risen drastically .", "label": "", "metadata": {}, "score": "41.876488"}
{"text": "It is also shown that speech recognition accuracy can be as high as or even higher using reduced dimensionality features versus original features , with \" properly \" trained systems .Background .Both approaches are used .Thus , for good training of model parameters , increasing dimensionality from 40 to 50 , considered a modest increase , could easily increase the need for more data by a factor of 1000 or more .", "label": "", "metadata": {}, "score": "41.88981"}
{"text": "-dimensional feature space onto a one - dimensional space .for which the ratio of the variance between the two classes ( target and nontarget ) versus the variance within the classes is maximal .This \" optimal \" projection is estimated as . , where . and . define the covariances and the means of the two classes ( target and nontarget ) that need to be separated .", "label": "", "metadata": {}, "score": "41.903496"}
{"text": "Hughey R , Krogh A : Hidden Markov models for sequence analysis : extension and analysis of the basic method .CABIOS 1996 , 12 : 95 - 107 .PubMed .Cover T : Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition .", "label": "", "metadata": {}, "score": "41.918793"}
{"text": "To do so , we simply compute the Dice distance between each candidate and the title , so that among sentences classified as CONCLUSION and PURPOSES , those lexically similar to the title would move to the top of the list .", "label": "", "metadata": {}, "score": "41.9711"}
{"text": "However , our main conclusion is pessimistic : Our results indicate that significant progress in the field of PPI extraction probably can only be achieved if future methods leave the beaten tracks .Methods .We recently performed a comprehensive benchmark of nine kernel - based approaches ( hereinafter we refer to them briefly as kernels ) [ 14 ] .", "label": "", "metadata": {}, "score": "42.008"}
{"text": "Finally , to select between the title and the best - ranked sentence from the abstract , the Dice score is again used .If the sentence score is above a given threshold , then the sentence is selected , otherwise the title is returned .", "label": "", "metadata": {}, "score": "42.04489"}
{"text": "We used the \u03c7 2 -test to check if there was a significantly higher overlap between the two sets compared to as if drawn at random .A p - value lower than 0.001 is considered significant .Table 3 .The overlap of the pairs that are the most difficult and the easiest to classify correctly by the collection of kernels using cross - validation ( CV ) and cross - learning ( CL ) settings .", "label": "", "metadata": {}, "score": "42.068604"}
{"text": "Finally , to extract important sentences from documents , documents ' titles and uppercase words such as named - entities are reported to be good predictors .A life sciences literature perspective on Information Extraction .To date and as with gene and gene products functions , descriptions of most of the biological knowledge can not be found in databanks , but only in the form of scientific summaries and articles [ 10 ] .", "label": "", "metadata": {}, "score": "42.11061"}
{"text": "Later , a loss term was added to account for noisy data .The most widely used loss function for SVM training is hinge - loss : . m . a .x . where . is called a margin parameter , . is a label of a sample , and .", "label": "", "metadata": {}, "score": "42.17739"}
{"text": "Thus , our classifier has the form .where . is the number of neurons in the hidden layer , with sigmoidal activation functions .e . x .p .the number of observed features , . and . sets of thresholds and weight coefficients , respectively .", "label": "", "metadata": {}, "score": "42.196213"}
{"text": "Given such two - class high - dimensional data , one analytical task is identifying a \" small \" subset of features able to discriminate between the classes .Tools that solve this problem would accelerate development of novel and/or improved molecular targets for diagnosis , prognosis , and therapy [ 2 ] .", "label": "", "metadata": {}, "score": "42.220642"}
{"text": "Normalization was not applied to the features in order to retain verb tense information , which is important for temporal resolution .These initial features were used to build a linear SVM model for Layer 3 classifi- cation .The features were then ranked by their weights .", "label": "", "metadata": {}, "score": "42.3534"}
{"text": "The ultimate goal of PPI extraction is the identification of PPIs in biomedical texts with unknown characteristics .This task is better reflected in the CL setting , when training and test sets are drawn from different distributions : in such cases , we train on an ensemble of four corpora and test on the fifth one .", "label": "", "metadata": {}, "score": "42.412827"}
{"text": "Notations .:An input sample vector .A single vector is provided for each iteration of the algorithm .Sample vectors are assumed to be extended with an additional constant element representing bias term .:The label provided for .", "label": "", "metadata": {}, "score": "42.42945"}
{"text": "For the plot category , recall was slightly lower than precision .The one - versus - all report for the plot category continues with histograms of rank , average rank , conditional and joint probabilities , just as before , but broken out one - versus - all : .", "label": "", "metadata": {}, "score": "42.435745"}
{"text": "Relation between sentence length , entity distance and pair difficulty .In Figure 4 we show the characteristics of sentence difficulty in terms of the average length of the sentence , the average distance between entities , and the size of the shortest path in parse tree .", "label": "", "metadata": {}, "score": "42.507797"}
{"text": "The micro - averaged results weight by case , not by category , treating all cases as equal .This is calculated by summing the one - versus - all matrices and presenting the result as a precision - recall evaluation .", "label": "", "metadata": {}, "score": "42.511745"}
{"text": "Next , objective function .The loss function is then used as a weight for training the weights of the latest weak classifier .with the Pegasos algorithm .It can be seen that , similar to AdaBoost family of boosting algorithms , our algorithm increases the weights of samples misclassified by the current strong classifier , and decreases the weights of samples classified correctly .", "label": "", "metadata": {}, "score": "42.561455"}
{"text": "Experimental Results .The graphs for the estimated error rate are shown on Figure 3 , while the resulting error rate on the test datasets is shown in Table 1 .It can be seen that for linearly separable problems our algorithms performs on par with the Pegagos algorithm , with slight increase of the error rate possibly due to the overfitting .", "label": "", "metadata": {}, "score": "42.564133"}
{"text": "The mean absolute errors were very small , generally below 1 % .Progressive sampling .Another research area related to our work is progressive sampling .Both active learning and progressive sampling start with a very small batch of instances and progressively increase the training data size until a termination criteria is met [ 31 - 36 ] .", "label": "", "metadata": {}, "score": "42.586548"}
{"text": "This is illustrated in the remaining new code .Here , a string subjReview is the result of applying the method subjectiveSentences to the review .This extracts the subjective sentences and returns them as a string .We then create a classification using the filtered intput subjReview .", "label": "", "metadata": {}, "score": "42.600056"}
{"text": "Since the original data is not multivariate Gaussian , the PCA basis vector is no longer a good way to approximate the data .In fact , since the data primarily follows a curved path in the 2-D space , no linear transform method , resulting in a straight line subspace , will be a good way to approximate the data with one dimension .", "label": "", "metadata": {}, "score": "42.616257"}
{"text": "We show that kernels using the same input representation perform similarly on these pairs and that building ensembles using dissimilar kernels leads to significant performance gain .However , our analysis also reveals that characteristics shared between difficult pairs are few , which lowers the hope that new methods , if built along the same line as current ones , will deliver breakthroughs in extraction performance .", "label": "", "metadata": {}, "score": "42.6473"}
{"text": "This measure assumes that a binary decision was made prior to computing the Dice distance : a unique candidate GeneRiF must be selected .Common pre- and post - processing strategies .We started designing the task as a ranking task .", "label": "", "metadata": {}, "score": "42.664352"}
{"text": "We also showed that even very few re - annotated pairs ( below 1 % of total ) influence the kernels ' performance : the re - trained models could generalize the information beyond the affected pairs , and showed a systematic performance gain over the original model .", "label": "", "metadata": {}, "score": "42.68443"}
{"text": "The main difference between progressive sampling and SSD of classifiers is that progressive sampling assumes there are an unlimited number of annotated samples and does not predict the sample size required to reach a specific performance target .Methods .In this section we describe a new fitting algorithm to predict classifier performance based on a learning curve .", "label": "", "metadata": {}, "score": "42.70519"}
{"text": "SVM Classifier We built a linear SVM sentence classifier from the entire corpus with the features described above with an unordered bag - of - words representation using the Weka SVM imple- mentation .Layer 2 : Classifying Nonsmoker Sentences We customized NegEx , a negation detection algorithm de- veloped by Chapman et al.7for the Layer 2 classification of the nonsmoker category .", "label": "", "metadata": {}, "score": "42.747406"}
{"text": "In the latter case , classifier outputs are often normalized to the [ 0 , 1 ] interval , and these values are interpreted as the support given by the classifier to each class , or as class - conditional posterior probabilities .", "label": "", "metadata": {}, "score": "42.762062"}
{"text": "When the evaluation loop is done , we just call the toString ( ) method on the evaluator to print out the results .Note that the category supplied to the evaluator is only used for evaluation purposes ; the classifier will be used to perform classification on the input sentence without reference to the reference category .", "label": "", "metadata": {}, "score": "42.767597"}
{"text": "The error of this hypothesis with respect to the current distribution is calculated as the sum of distribution weights of the instances misclassified by \\(h_t\\ ) ( Equation 4 ) .AdaBoost . M1 requires that this error be less than \\(1/2\\ .", "label": "", "metadata": {}, "score": "42.781853"}
{"text": "For all discriminatively based transformations , either linear or nonlinear , an implicit assumption is that training data exists which has been labeled according to category .For the case of classification , such as the experiments just described , this labeled data is needed anyway , for both training and test data , to conduct classification experiments ; thus the need for category labeled data is not any extra burden .", "label": "", "metadata": {}, "score": "42.789017"}
{"text": "We submitted three sets of results with our top performing models , which we describe in the next section .We used precision , recall , and F - score as our evaluation metrics : Because our system makes assignments to every document that is processed , totalNumberOfDocumentsClassi- fiedByTheSystem and numberOfDocumentsInTestSet are the same .", "label": "", "metadata": {}, "score": "42.863903"}
{"text": "In conclusion , the methods used in these experiments provide a general view of the gene function extraction task within the framework of TREC Genomics evaluation campaigns , as well as in previous or more recent BioCreative initiatives [ 3 ] .", "label": "", "metadata": {}, "score": "42.99963"}
{"text": "This is our informal evaluation set up for which we report results .For the final competitive evaluation , we trained our models on the data from Set 1 and Set 2 with the best configurations as determined from our informal evaluation experimenta- tion .", "label": "", "metadata": {}, "score": "43.0099"}
{"text": "Results .The data was recorded during the online typing of words / characters ( in copy spell and in free spell mode ) .In order to assess the classification performance of all classifiers considered , we opted for an offline analysis , in which case we also evaluated the performance for a smaller amount of intensification sequences .", "label": "", "metadata": {}, "score": "43.086975"}
{"text": "..One - Versus - All Evaluations .Next up are one - versus - all evaluations for each category .These indicate performance on a category - by - category basis .This is n't so interesting in our case , because both categories perform about equally well , which is typical in two - category problems with roughly balanced false positives and false negatives .", "label": "", "metadata": {}, "score": "43.105625"}
{"text": "Results .We use the brute force approach of exhaustive search through all genes , gene pairs ( and for some data sets gene triples ) .Each unique gene combination is analyzed with a few - parameter linear - hyperplane classification method looking for those combinations that form training error - free classifiers .", "label": "", "metadata": {}, "score": "43.12196"}
{"text": "Each method ranks the candidate sentences separately .From these two rankings , our aim is to identify a confidence estimator on each ranking .When both methods disagree on the top - ranked sentence , a final decision is needed .", "label": "", "metadata": {}, "score": "43.15877"}
{"text": "We expect the model fitting to be more accurate and the confidence interval to be narrower on smoother curves , though the fitting process remains the same for the less smooth curves .Although the curve fitting can be done in real time , the time to create the learning curve depends on the classification task , batch size , feature number , processing time of the machine among others .", "label": "", "metadata": {}, "score": "43.195213"}
{"text": "n .In our work , we also incorporate a bias term . by substituting .with .In this case , .To simplify notation , we assume that all input vectors have been extended in such a way , and simply use . . . .", "label": "", "metadata": {}, "score": "43.259056"}
{"text": "Elsewhere , a precision oriented - metric such as 11-point average precision has been suggested [ 23 ] .In the TREC-2003 genomics evaluation campaign , a third type of measure was used to evaluate information extraction : the Dice coefficient is shown in Equation 1 .", "label": "", "metadata": {}, "score": "43.30438"}
{"text": "Alternatively , the neural network architecture and/or training constraints could be modified so that the nonlinearly transformed features are more suitable as input features for a Hidden Markov Model .References . 2 - S. B. Davis , P. Mermelstein , 1980 Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences , IEEE Trans . on Acoustics , Speech and Signal Processing , 28 357 366 .", "label": "", "metadata": {}, "score": "43.34742"}
{"text": "Our experiments show that current methods do not seem to do very well in capturing the shared characteristics of positive PPI pairs , which must also be attributed to the heterogeneity of the ( still very few ) available corpora .Our analysis suggests that performance improvements shall be sought after rather in novel feature sets than in novel kernel functions .", "label": "", "metadata": {}, "score": "43.47348"}
{"text": "a column vector of true labels ( classification ) for all corresponding training examples .As a result , our hyperplane will have the form .This solution is equivalent to a penalized least square estimate .Linear Support Vector Machine .", "label": "", "metadata": {}, "score": "43.613327"}
{"text": "Here , we used a support vector machine with the Gaussian radial - basis function .e . x .p . , as a kernel .In our experiment , we opted for the SVMlight package [ 20 ] .The SVM 's outcome , for a new sample , is a value for . , where . are the support vectors chosen from the training set with known class labels . are Lagrange multipliers .", "label": "", "metadata": {}, "score": "43.61743"}
{"text": "Kupiec J , Pedersen J , Chen F : A Trainable Document Summarizer .SIGIR 1995 , 68 - 73 .Teufel S , Moens M : Argumentative Classification of Extracted Sentences as a First Step Towards Flexible Abstracting .Advances in Automatic Text Summarization 1999 , 155 - 171 .", "label": "", "metadata": {}, "score": "43.68283"}
{"text": "Like other classification methods that produce large numbers of genes , considering the corpus of all genes found can provide insight to the underlying biology .Experimental design and construction of a data set profoundly influences the presence of small sized classifiers .", "label": "", "metadata": {}, "score": "43.70279"}
{"text": "In this case , the classifier combination involves merging the individual ( usually weaker and/or diverse ) classifiers to obtain a single ( stronger ) expert of superior performance .Examples of this approach include bagging predictors ( Breiman 1996 ) , boosting ( Schapire 1990 ) , AdaBoost ( Freund 2001 ) and their many variations .", "label": "", "metadata": {}, "score": "43.729866"}
{"text": "Using the above similarity , we separate the boosting process into two phases : the training of each successful weak classifier , and adjusting the boosting weights that combine weak classifiers into a strong one .Both phases can then be cast as a primal SVM problem , same as ( 4 ) , with the difference that in phase 1 ( weak classifier training ) each sample is weighted according to the result provided by the current strong classifier .", "label": "", "metadata": {}, "score": "43.77877"}
{"text": "Results of the argumentative classification are given in Table 5 for these two sets .These proportions indicate that GeneRiFs are mainly classified as PURPOSE and CONCLUSION sentences ( respectively 41 % and 55 % in Set A ) .The significance of these observations is accentuated for GeneRiFs originating from the abstract ( see Set B in Table 5 ) but the trend is stable .", "label": "", "metadata": {}, "score": "43.79911"}
{"text": "N. C. Oza and K. Tumer , \" Input Decimation Ensembles : Decorrelation through Dimensio - nality Reduction , \" 2nd Int .Workshop on Multiple Classifier Systems , in Lecture Notes in Computer Science , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "43.814213"}
{"text": "If a classifier can not meet this requirement , the algorithm aborts .The normalized error \\(\\beta_t\\ , \\ ) is then computed ( Equation 5 ) so that the actual error that is in the [ 0 0.5 ] interval is mapped to [ 0 1 ] interval .", "label": "", "metadata": {}, "score": "43.814766"}
{"text": "However , the percentages of correctly classified sentences in the METHODS or RESULTS classes do not vary when the sentence position is taken into account .Table 4 .Confusion matrices for argumentative classification : the first column indeictes the expected category , while the first line provides the measured classification .", "label": "", "metadata": {}, "score": "43.844177"}
{"text": "Same as ( 1 ) , but 10 % of the labels are switched , simulating salt - and - pepper noise .( 3 ) Bayes - separable data ( Bayesian ) .This dataset is generated as described in [ 3 ] , that is , in such a way so that data is clearly separable using ideal Bayesian classifier for known class distributions .", "label": "", "metadata": {}, "score": "43.89179"}
{"text": "Such a naive selection procedure achieved a relatively high performance of 50.47 % , due to the fact that 45 % of GeneRiFs were extracted from the article 's title .On the other hand , if for each query we had an oracle that always selected the title or the sentence achieving the highest Dice score , we could obtain a performance of 70.96 % , one that represents a theoretical upper bound for our experiments .", "label": "", "metadata": {}, "score": "43.912983"}
{"text": "We describe and analyze a simple and effective two - step online boosting algorithm that allows us to utilize highly effective gradient descent - based methods developed for online SVM training without the need to fine - tune the kernel parameters , and we show its efficiency by several experiments .", "label": "", "metadata": {}, "score": "43.92858"}
{"text": "Clearly , such characteristics can be exploited in building ensembles as they allow a rationale choice of base classifiers ; we will report on using such a strategy in the discussion .Feature analysis .To assess the importance of the aforementioned features we constructed a feature space representation of all pairs .", "label": "", "metadata": {}, "score": "44.07538"}
{"text": "As an existing example , BioInfer originally also contains richer event information and was transformed to a PPI corpus using some simplifying rules [ 8 ] .Second , we built an ensemble by combining three kernels with a simple majority voting scheme .", "label": "", "metadata": {}, "score": "44.246574"}
{"text": "This experiment shows the ability of the algorithms to completely relearn a distribution .In case of the dataset with the changing distribution , the distribution at the last iteration is used for testing .and the cutoff parameter .Pegasos and Norma used parameter . , and either a linear kernel or a Gaussian RBF kernel with . , which is the same value of .", "label": "", "metadata": {}, "score": "44.256065"}
{"text": "The number of hidden nodes in the second hidden layer was varied from 1 to 39 , according to the dimensionality being evaluated .For the case of NLDA2 , the network used for dimensionality reduction was also a classifier .For the sake of consistency , the outputs of the hidden nodes from the bottleneck neural network were used as features for a classifier , using either another neural network or the MXL classifier .", "label": "", "metadata": {}, "score": "44.26965"}
{"text": "Clearly , for this example , the two basis vectors are quite different , and clearly the projection of data onto the first LDA basis vector would be more effective for separating the two categories than data projected onto the first PCA basis vector .", "label": "", "metadata": {}, "score": "44.28181"}
{"text": "Long sentences have typically more complicated sentence structure , thus deep parsers are also prone to produce more erroneous parses , which makes the PPI relation extraction task especially difficult .Characteristics of pairs by difficulty class .Characteristics of pairs by difficulty class ( average sentence length in words , average word distance between entities , average distance in the dependency graph ( DG ) and syntax tree ( ST ) shortest path ) .", "label": "", "metadata": {}, "score": "44.297123"}
{"text": "We found that the distribution of mistakes around the intended character , based on BLDA , is , in general , significantly ( .for factor \" method \" ) smaller than for any other classifier , except for nSVM ( nSVM versus BLDA has .", "label": "", "metadata": {}, "score": "44.35604"}
{"text": "The principal idea of a linear SVM is to find the separating hyperplane , between two classes , so that the distance between the hyperplane and the closest points from both classes is maximal .In other words , we need to maximize the margin between the two classes [ 18 ] .", "label": "", "metadata": {}, "score": "44.380024"}
{"text": "Theoretically , the nonlinear methods have the potential to be more \" efficient \" than linear methods , that is , give better representations with fewer dimensions .In addition , some examples are shown from experiments with Automatic Speech Recognition ( ASR ) where the nonlinear methods in fact perform better , resulting in higher ASR accuracy than obtained with either the original speech features , or linearly reduced feature sets .", "label": "", "metadata": {}, "score": "44.39552"}
{"text": "Some of these combination rules operate on class labels only , whereas others need continuous outputs that can be interpreted as support given by the classifier to each of the classes .Xu et al ( Xu 1992 ) .defines three types of base model outputs to be used for classifier combination .", "label": "", "metadata": {}, "score": "44.433517"}
{"text": "Figure 1 graphically illustrates this concept , where each classifier - trained on a different subset of the available training data - makes different errors ( shown as instances with dark borders ) , but the combination of the ( three ) classifiers provides the best decision boundary .", "label": "", "metadata": {}, "score": "44.448112"}
{"text": "Although we were not able to reproduce their results based on their TREC report , [ 36 ] report a Dice score close to 57 % , using similar classifiers , but trained on the sentence position in the abstract .[ 37 ] report on slightly weaker results using text - derived rather than Gene Ontology - derived features , which confirms that Gene Ontology features capture most information needed for functional annotation .", "label": "", "metadata": {}, "score": "44.44995"}
{"text": "In this chapter , some techniques are presented for reducing feature dimensionality while preserving category ( i.e. , phonetic for the case of speech ) discriminability .Since the techniques presented for reducing dimensionality are statistically based , these methods also are subject to \" curse of dimensionality \" issues .", "label": "", "metadata": {}, "score": "44.476273"}
{"text": "In most cases , when dealing with kernels , the task of learning a support vector machine is cast as a constrained quadratic programming problem .Methods that deal with such problem usually need access to all labeled samples at once and require about .", "label": "", "metadata": {}, "score": "44.486126"}
{"text": "These models have only 2 - 4 free parameters .Thus the problem of over - fitting the model to the data during training is not a large issue and we do n't perform any stringent generalization tests such as multi - round cross validation .", "label": "", "metadata": {}, "score": "44.552856"}
{"text": "Basic Polarity Analysis .We begin with a simple classification exercise , amounting to training and testing our basic classifiers on ( different slices of ) the boolean polarity data .The resulting classifier is able to judge whether a whole movie review is essentially positive or negative ( as defined by the data set curators ) .", "label": "", "metadata": {}, "score": "44.571278"}
{"text": "There exist several algorithms to deal with such a problem that have well - established convergence bound .Amongst them , methods using stochastic gradient descent ( or , in case of hinge - loss function , subgradient descent ) are most prominent .", "label": "", "metadata": {}, "score": "44.589554"}
{"text": "In order for this process to be effective , the individual experts must exhibit some level of diversity among themselves , as described later in this article in more detail .Within the classification context , then , the diversity in the classifiers - typically achieved by using different training parameters for each classifier - allows individual classifiers to generate different decision boundaries .", "label": "", "metadata": {}, "score": "44.612495"}
{"text": "The outputs of these classifiers on their pseudo - training blocks , along with the actual correct labels for those blocks constitute the training dataset for the Tier 2 classifier ( see Figure 7 ) .Jordan and Jacobs ' mixture of experts ( Jacobs 1991 ) generates several experts ( classifiers ) whose outputs are combined through a ( generalized ) linear rule .", "label": "", "metadata": {}, "score": "44.61548"}
{"text": "In the iterative process , users need to make a decision on when to stop / continue the data labeling and classification process .Although termination criteria is an issue for both passive and active learning , identifying an optimal termination point and training sample size may be more important in active learning .", "label": "", "metadata": {}, "score": "44.61596"}
{"text": "Generalization performance of classifiers was evaluated using a Leave - One - Out procedure .In LOO testing , one data point is removed and a classifier re - learned using the rest of the data .The resultant classifier is tested on the held out data point and if it is in error , a LOO error is counted .", "label": "", "metadata": {}, "score": "44.620823"}
{"text": "At large sample sizes , confidence intervals are very narrow and residual values very small .Both Figures 2 and 3 suggest that the confidence interval width relates to MAE and prediction accuracy .Progression of confidence interval width and MAE for predicted values .", "label": "", "metadata": {}, "score": "44.71579"}
{"text": "In summary , we compare a more extensive set of classifiers and perform our comparison on patients , instead of on healthy subjects , both of which distinguish our study from others .Methods .EEG Data Acquisition .Our recordings were performed with a prototype of a miniature EEG recording device that wirelessly communicates with a USB stick receiver ( Figures 1(a ) and 1(c ) ) .", "label": "", "metadata": {}, "score": "44.74829"}
{"text": "For this case , 2-D pseudo random data was created to lie along a U shaped curve , similar to the data depicted in Figure 2 .A neural network ( 2 - 5 - 1 - 5 - 2 ) was then trained as an identify map .", "label": "", "metadata": {}, "score": "44.81617"}
{"text": "Ideally , the targets are uncorrelated , which enables quicker convergence of weight updates .The targets can also be viewed as multidimensional vectors , with a value of \" 1 \" for the target category and \" 0s \" for the non - target categories .", "label": "", "metadata": {}, "score": "44.898914"}
{"text": "The negative sign converts the distance metric into a support value , whose largest value can be zero in case of a perfect match .Note that this output does not match any of the code words exactly , and this is where the error correcting ability of the ECOC lies .", "label": "", "metadata": {}, "score": "44.911568"}
{"text": "It can be applied to protein microarray , mass - spectra , or any data with similar characteristics .Data set discussion .We ca n't discuss all the gene sets found in all data sets : there are too many .", "label": "", "metadata": {}, "score": "45.064102"}
{"text": "The drawbacks of our algorithm include the fact that it is not as efficient in case of linearly separable problems , and that it inherits some of the sensibility to the rapid changes in target function from Pegasos .In the future , we plan to study the application of our algorithm to various image and signal processing tasks , in partiular to object tracking problem to compare its effectiveness to methods based on various online AdaBoost modifications , like the ones described in [ 1 ] .", "label": "", "metadata": {}, "score": "45.08557"}
{"text": "There are , of course , more efficient ways to write this code , and ways to refactor so that the cut - and - pasted code is also shared , but we leave those improvements as exercises to the reader .", "label": "", "metadata": {}, "score": "45.18251"}
{"text": "We also manually scrutiny some of the pairs that were found to be the most difficult ones , suspecting that the reason for the failure of kernels is in fact an incorrect annotation .We re - labeled a set of such suspicious annotations and re - evaluated if kernels were able to benefit from these modifications .", "label": "", "metadata": {}, "score": "45.24115"}
{"text": "This strategy allowed us to avoid broken / lost data frames , which might occur due to a buffer overflow .The amount of broken / lost frames can be precisely computed using the counter incorporated into each data frame .Data - Stimuli Synchronization .", "label": "", "metadata": {}, "score": "45.356937"}
{"text": "We show that kernels using the same input representation perform similarly on these pairs and that building ensembles using dissimilar kernels leads to significant performance gain .Additionally , we identify kernels that perform better on certain difficulty classes ; paving the road to more complex ensembles .", "label": "", "metadata": {}, "score": "45.429047"}
{"text": "Therefore , an arbitrary number of reduced dimensions can be obtained , independent of the input feature dimensions and the nature of the training targets .A lower dimensional representation of the input features is easily obtained by simply deploying fewer nodes in the middle layer than the input layer .", "label": "", "metadata": {}, "score": "45.524376"}
{"text": "Diversity ( DIV ) which selects instances based on their diversity / dissimilarity from instances in the training set .Diversity is measured as the simple cosine distance between the candidate instances and the already selected set of instances in order to reduce information redundancy ; and .", "label": "", "metadata": {}, "score": "45.54231"}
{"text": "The algorithms described above have their built in combination rules , such as simple majority voting for bagging , weighted majority voting for AdaBoost , a separate classifier for stacking , etc .However , an ensemble of classifiers can be trained simply on different subsets of the training data , different parameters of the classifiers , or even with different subsets of features as in random subspace models .", "label": "", "metadata": {}, "score": "45.598415"}
{"text": "Such a classifier can not learn the boundary shown in Figure 2 .Now consider a collection of circular decision boundaries generated by an ensemble of such classifiers as shown in Figure 3 , where each classifier labels the data as class O or class X , based on whether the instances fall within or outside of its boundary .", "label": "", "metadata": {}, "score": "45.641605"}
{"text": "Our method uses Pegasos , Stochastic Gradient Descent-(SGD- ) based SVM training method introduced in [ 4 ] to produce both a set of weak classifiers and boosting weights for combining them into strong classifier .Using this kind of training algorithm allows us to utilize solid theoretical background and well - defined convergence rate of SGD algorithms , as well as increased accuracy of classifier produced by boosting .", "label": "", "metadata": {}, "score": "45.682037"}
{"text": "This distinction is useful relative to our information extraction task , since we see the task as a direct application of question - answering applied to functional proteomics .Indeed , the study can be regarded as an attempt to answer typical questions as formulated by Swiss - Prot expert curators .", "label": "", "metadata": {}, "score": "45.77565"}
{"text": "In preliminary experiment we tried to establish a relation between the GeneRiFs and the argumentative sections .We selected two sets of 1000 GeneRiFs from our training data and submitted them to the argumentative classifier .Both set A and B are random sets , but for B we impose that the extract describing the GeneRiF must be found in the abstract ( as exemplified in Table 1 ) .", "label": "", "metadata": {}, "score": "45.79853"}
{"text": "The last two rows of Table 8 indicate the performance of our combined approach ( 56.14 % ) , clearly showing better overall results than those for each extraction scheme run separately .When we apply the sentence trimming procedure , the Dice score increased slightly ( 57.29 % vs. 56.14 % ) .", "label": "", "metadata": {}, "score": "45.896824"}
{"text": "By optimizing of the choice of m this might be over fitting the data , but in the Random Data section we show that we are finding many more features than chance alone would account for .Given data points specified by P features , the potential number of m -LCs is equivalent to the combinatorial problem of choosing m items out of P , i.e .", "label": "", "metadata": {}, "score": "45.93994"}
{"text": "Suffice it to say here that this kappa value is well within the rule - of - thumb range expected for \" reliable \" classification .The next basic statistics report on information - theoretic measures about the marginal and conditional distributions produced by the training data and the results : .", "label": "", "metadata": {}, "score": "45.95697"}
{"text": "Yang Y , Pedersen J : A Comparative Study on Feature Selection in Text Categorization .Ruch P , Boyer C , Chichester C , Tbahriti I , Geissbuhler A , Fabry P , Gobeill J , Pillet V , Rebholz - Schuhmann D , Lovis C , Veuthey A : Using argumentation to extract key sentences from biomedical abstracts .", "label": "", "metadata": {}, "score": "45.976013"}
{"text": "Both have small class sizes and have pre - disposed differences between the classes .The GIST experiment compares cancers from different tissue types which means there will be a very strong signal from just the tissue differences rather than just the cancers alone .", "label": "", "metadata": {}, "score": "45.976715"}
{"text": "The representations of protein pairs together with their gold standard PPI - labels are analyzed by a kernel - based learner ( mostly an SVM ) , which builds a predictive model .When analyzing a new sentence for PPIs , its candidate protein pairs are turned into the same representation , then classified by the kernel method .", "label": "", "metadata": {}, "score": "45.992157"}
{"text": "Feature selection .As mentioned earlier in this article , one way to improve diversity in the ensemble is to train individual classifiers different subsets of the available features .Selecting the feature subsets at random is known as the random subspace method , a term coined by ( Ho 1998 ) , who used it on constructing decision tree ensembles .", "label": "", "metadata": {}, "score": "46.028572"}
{"text": "is much less than the kernel expansion terms , it still grows with additional samples , which may lead to loss of effectiveness and overfitting .There are several possible extensions that may allow to avoid this .One way is to remove classifiers .", "label": "", "metadata": {}, "score": "46.07065"}
{"text": "We implement a technique that reduces a review to 5 to 25 sentences .This will be the five most subjective sentences as ranked by conditional probability of the subjectivity model , as well as up to 20 more sentences if they are 50 % or more likely to be subjective according to the subjectivity model .", "label": "", "metadata": {}, "score": "46.120705"}
{"text": "m .i .n .with respect to . , where .corresponds to the training points in the feature space , and . is the associated output ( .The regularization parameter is estimated through a line search on cross - validation results .", "label": "", "metadata": {}, "score": "46.145065"}
{"text": "The filterings ( if used ) were applied once , before LIKNON analysis , solely to reduce the number of genes and hence the runtimes .They were not used to adjust for \" good \" results .When three or more categories of samples had been defined by the original authors , two - class data sets were produced by partitioning these categories .", "label": "", "metadata": {}, "score": "46.15435"}
{"text": "Non - kernel based classifiers .We also compared kernel based classifiers with some linear , non - kernel based classifiers as implemented in Weka [ 36 ] .We used the surface feature space created for feature analysis ( see Table 15 ) .", "label": "", "metadata": {}, "score": "46.22023"}
{"text": "While the algorithm described in [ 3 ] allows truncation of the kernel expansion coefficients , this is only applicable to the SGD algorithms with constant learning rate , and it results in an accuracy penalty .The second difference is the update of vector . , which allows change in the weights different from the exponential decay of [ 3 ] .", "label": "", "metadata": {}, "score": "46.35668"}
{"text": "The results are shown in Table 4 .When the number of samples is lower than about 30 , or one class is very small relative to the other class , then the chance of finding a pair of random genes that form a perfect classifier is large enough to easily measure .", "label": "", "metadata": {}, "score": "46.359303"}
{"text": "Note that unlike phonetic recognition experiments , for the case of classification , the timing labels in the database are explicitly used for both training and testing .Thus classification is \" easier \" than recognition , and accuracies typically higher , since phone boundaries are known in advance and used .", "label": "", "metadata": {}, "score": "46.373383"}
{"text": "4 , pp .497 - 508 , 2001 .R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .", "label": "", "metadata": {}, "score": "46.402496"}
{"text": "Regarding fitting samples sizes , we can observe a rapid decrease in RMSE and MAE from 80 to 200 instances .From 200 to the end of the curves , values stay relatively constant and close to zero with a few exceptions .", "label": "", "metadata": {}, "score": "46.432167"}
{"text": "The conditional entropy statistic tells us how many additional bits we 'd need on average to encode the classification result given the reference category .This number will be 0.0 if there is perfect classification .The mutual information statistic just presents the response entropy minus the conditional entropy .", "label": "", "metadata": {}, "score": "46.525177"}
{"text": "The final bit of this section of the report includes results about average performance from a ranked , scored , conditional and joint probability perspective .The average reference rank indicates the average position on the n - best list provided by a ranked classifier of the gold standard answer .", "label": "", "metadata": {}, "score": "46.56341"}
{"text": "View at Google Scholar .S. L. Shishkin , I. P. Ganin , I. A. Basyul , A. Y. Zhigalov , and A. Ya .Kaplan , \" N1 wave in the P300 BCI is not sensitive to the physical characteristics of stimuli , \" Journal of Integrative Neuroscience , vol . 8 , no .", "label": "", "metadata": {}, "score": "46.589554"}
{"text": "While a SVM is constructed so as to maximize a margin between the two classes , the BLDA tries to maximize the probability of having training data with the correct class labels .Since both classifiers depend on some regularization parameters , their optimal choice increases the generalization accuracy .", "label": "", "metadata": {}, "score": "46.592197"}
{"text": "945 - 954 , 1995 .K. Woods , W. P. J. Kegelmeyer , and K. Bowyer , \" Combination of multiple classifiers using local accuracy estimates , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .19 , no .", "label": "", "metadata": {}, "score": "46.59372"}
{"text": "As the last step of an iteration , we calculate the cutoff parameter for adding new classifier .If the preset threshold is reached ( in this case , the number of training iterations .reaches ., the value of . is increased , and a new classifier is initialized with zero weights .", "label": "", "metadata": {}, "score": "46.59662"}
{"text": "In the suggested approach we merge two independent sentence extraction strategies .The first proposed strategy ( LASt ) uses argumentative features , inspired by discourse - analysis models .The second extraction scheme ( GOEx ) uses an automatic text categorizer to estimate the density of Gene Ontology categories in every sentence ; thus providing a full ranking of all possible candidate GeneRiFs .", "label": "", "metadata": {}, "score": "46.61025"}
{"text": "We perform several experiments , aiming to compare generalization error and convergence rates over different datasets , as well as the ability of the algorithm to adapt to the distribution with the changing parameters ( flexibility ) .We use several artificial datasets with known distributions and separation properties , and a Forest Covertype dataset ( separating class 5 from other classes ) , originally used in [ 12 ] , and also used for comparison of convergence speed in [ 4 ] .", "label": "", "metadata": {}, "score": "46.615433"}
{"text": "ISMB 1995 , 246 - 254 .Larkey L , Croft W : Combining Classifiers in Text Categorization .SIGIR 1996 , 289 - 297 .Reynar J , Ratnaparkhi A : A Maximum Entropy Approach to Identifying Sentence Boundaries .ANLP 1997 , 16 - 19 .", "label": "", "metadata": {}, "score": "46.651672"}
{"text": "This approach may also be helpful for the basic character - level models , but they 're usually more robust with respect to smoothing than token models , which is just another reason why we prefer them for most applications .Cross - Validating Factory Implementation .", "label": "", "metadata": {}, "score": "46.692482"}
{"text": "Table 7 provides an example of density estimation for two sentences extracted from the abstract in Table 1 .Table 7 .Fusion of extraction strategies .This last step attempts to combine our two extraction schemes .To achieve this goal , we used the following rules ( decision boundaries are computed on the GeneRiF training set ) : .", "label": "", "metadata": {}, "score": "46.708176"}
{"text": "Figure 1 is an example of a learning curve .Mukherjee et al . experimented with fitting inverse power laws to empirical learning curves to forecast the performance at larger sample sizes [ 1 ] .They have also discussed a permutation test procedure to assess the statistical significance of classification performance for a given dataset size .", "label": "", "metadata": {}, "score": "46.71771"}
{"text": "The probability that the sentence is subjective is then set into the variable subjProb .This is a bit tricky because we have to determine if the category quote ( meaning \" subjective \" ) is the first - best or second - best response and pull out the correct probability .", "label": "", "metadata": {}, "score": "46.795654"}
{"text": "For our comparison , we have used the same procedure as in [ 10 ] ( in the forward step , the entrance tolerance .v . a .l .u .e . ; in the backward step , the exit tolerance .", "label": "", "metadata": {}, "score": "46.801193"}
{"text": "For all datasets RMSE and MAE have similar values with RMSE sometimes being slightly larger .On Figure 2 and 5 , it can be observed that the width of the observed confidence intervals changes only slightly along the learning curves , showing that performance variance among experiments are not strongly impacted by the sample size .", "label": "", "metadata": {}, "score": "46.809002"}
{"text": "Some of the limitations as applied to the use - case are negation detection and temporal resolution .J Am Med Inform Assoc .2008;15:25 - 28 .DOI 10.1197/jamia .M2437 .Hence , it is difficult to produce comparable results , evaluate techniques , and share platforms .", "label": "", "metadata": {}, "score": "46.82165"}
{"text": "For all cases , including original features , and all versions of the transformed features , a neural network classifier with 100 hidden nodes and 10 output nodes , trained with backpropagation , was used as the classifier .In addition , a Bayesian maximum likelihood Mahalanobis distance based Gaussian assumption classifier ( MXL ) was used for evaluation .", "label": "", "metadata": {}, "score": "46.851074"}
{"text": "There is good separation between the tumor and normal samples , except for the 2 \" outlier \" examples .Most of the triples found when the \" outlier \" examples are relabeled have larger margins than those found with the original data labels .", "label": "", "metadata": {}, "score": "46.867565"}
{"text": "IG - information gain ; ST - syntax tree ; DG - dependency graph ; SP - shortest path .Italic typesetting indicates parsing tree labels .The sign after each feature indicates positive / negative correlation .This experiment justifies that pairs in longer sentences may become more distant and more likely to be negative , thus easier to predict .", "label": "", "metadata": {}, "score": "46.886505"}
{"text": "HDA suffers from two drawbacks .There is no known closed form solution for minimizing the objective function required to solve for the transformation - rather a complex numerically based gradient search is required .More fundamentally , with actual speech data , HDA alone was found to perform far worse than LDA .", "label": "", "metadata": {}, "score": "46.912483"}
{"text": "This will also allow the algorithm to adapt better to the case of changing distribution .The other way is to increase the parameter .depending on . , or to choose a different cutoff algorithm altogether .Also , to increase adaptability to changing input conditions , it is possible to change the calculation of the learning rate of the Pegasos algorithm , for example , stopping its decay on a certain threshold .", "label": "", "metadata": {}, "score": "46.95095"}
{"text": "Ruch P , Tbahriti I , Gobeill J , Aronson A : Argumentative Feedback : A Linguistically - Motivated Term Expansion for Information Retrieval .ACL 2006 .Hersh W , Bhupatiraju B : TREC Genomics Track Overview .TREC-2003 2004 , 14 - 23 .", "label": "", "metadata": {}, "score": "46.977005"}
{"text": "Writing the Model to a File .The remaining code in the train ( ) method simply compiles the model to a file : .This actually transforms the format , with the resulting model being much faster at runtime .A more robust implementation would handle the close in a finally block that made sure the file output stream was closed to ensure no dangling file pointers were held .", "label": "", "metadata": {}, "score": "46.99412"}
{"text": "G. Rogova , \" Combining the results of several neural network classifiers , \" Neural Networks , vol .7 , no .5 , pp .777 - 781 , 1994 .L. Lam and C. Y. Suen , \" Optimal combinations of pattern classifiers , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "47.004143"}
{"text": "V. Bostanov and B. Kotchoubey , \" The t - CWT : a new ERP detection and quantification method based on the continuous wavelet transform and Student 's t - statistics , \" Clinical Neurophysiology , vol .117 , no .", "label": "", "metadata": {}, "score": "47.017017"}
{"text": "This issue of category labels is described in more detail in the following two subsections .Figure 12 .Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers using 10 % of group 2 training data for training classifier .", "label": "", "metadata": {}, "score": "47.031387"}
{"text": "However , cross - experiment array comparisons are difficult and would be much easier and more broad if experimenters used more common clones and references .Is the cost of such computation worth it ?Certainly it is for single genes and pairs .", "label": "", "metadata": {}, "score": "47.059296"}
{"text": "For the E class , the entropy of edge labels in the entire syntax tree and dependency graph , and the sentence length correlate positively , while frequency of nn , appos , conj_and , dep , det , etc . correlate negatively .", "label": "", "metadata": {}, "score": "47.067642"}
{"text": "This update rule ensures that the weights of all correctly classified instances and the weights of all misclassified instances always add up to \\ ( 1/2\\ .\\ )More specifically , the requirement for the training error of the base classifier to be less than \\ ( 1/2\\ ) forces teh algorithm to correct at least one mistake made by the previous base model .", "label": "", "metadata": {}, "score": "47.08236"}
{"text": "In particular , note that it calculates the 10 percent of the data on which to test and then as each sentence is encountered , it is added as a case to the evaluator using the BaseClassifierEvaluator.handle(Classified ) .Evaluation cases consist of the reference result , in this case category , and the input , in this case sentences[j ] .", "label": "", "metadata": {}, "score": "47.08273"}
{"text": "In this experiment we determine the difficulty of protein pairs .The fewer kernel based approaches are able to classify a pair correctly , the more difficult the pair is .Different kernels ' predictions vary heavily as we have reported in [ 14 ] .", "label": "", "metadata": {}, "score": "47.1054"}
{"text": "Here , we construct a file using the specified pattern and then read all of the data from the file .We then split on newlines to derive the sentences .The number of training instances is set to 90 % of the input data .", "label": "", "metadata": {}, "score": "47.119576"}
{"text": "[ 2 ] - Most , but not all researchers , have used the recommended training and test sets .For all ASR experiments reported in this chapter , the SA sentences were removed , the recommended training and test sets were used , and the phone set was collapsed to the same 39 phones used in most ASR experiments with TIMIT .", "label": "", "metadata": {}, "score": "47.122097"}
{"text": "R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .4 , pp .59 - 72 , 2007 .", "label": "", "metadata": {}, "score": "47.124386"}
{"text": "Disagreement - if both ranker disagree on the top item , then we look at the density estimate returned by GOEx ; . - if this estimate is above an empirical threshold and if the candidate sentence is assigned a PURPOSE / CONCLUSION / Title category by the LASt classifier , then the candidate sentence provided by GOEx is selected , . - otherwise the LASt candidate sentence is returned .", "label": "", "metadata": {}, "score": "47.154175"}
{"text": "Next comes the same accuracy report as we computed by hand in the last demo .This performance is much better at 92 % accuracy than the polarity classification results we saw in the last section .After the basic accuracy report , the confusion matrix is presented in a format to provide easy inclusion into a spreadsheet or other graphing package such as gnuplot . .", "label": "", "metadata": {}, "score": "47.282425"}
{"text": "That is .Y .A .T .X , where X , Y are again column vectors as for PCA .The big difference is in how A is computed .For LDA , it has been shown that the columns of A correspond to the m largest eigenvalues of .", "label": "", "metadata": {}, "score": "47.285927"}
{"text": "So in the design of the task , the system is fed with an abstract , which a priori contain a GeneRiF. As defined by the TREC Genomics protocol , we also hypothesized that GeneRiFs are sentences or significant sentence fragments .", "label": "", "metadata": {}, "score": "47.293552"}
{"text": "Fukunaga K , Hayes R : Effects of sample size in classifier design .Pattern Analysis and Machine Intelligence , IEEE Transactions on 1989 , 11 ( 8) : 873 - 885 .View Article .Cortes C , Jackel LD , Solla SA , Vapnik V , Denker JS : Learning Curves : Asymptotic Values and Rate of Convergence .", "label": "", "metadata": {}, "score": "47.377243"}
{"text": "Methods .We designed and implemented a method that fits an inverse power law model to points of a given learning curve created using a small annotated training set .Fitting is carried out using nonlinear weighted least squares optimization .The fitted model is then used to predict the classifier 's performance and confidence interval for larger sample sizes .", "label": "", "metadata": {}, "score": "47.390102"}
{"text": "It is also important to note that while parameters . and .This is shown in our Section 3 , where our algorithm is running on the same set of parameters for datasets with different variable distributions , and often outperforming even the kernel - using algorithms with ideal kernel settings .", "label": "", "metadata": {}, "score": "47.40609"}
{"text": "As such co - occurrences are less likely to describe actual interactions , a more informed approach could benefit from taking such aspects into consideration .Third , pattern - based methods , which are capable of capturing even exotic instances , might be worth looking into again .", "label": "", "metadata": {}, "score": "47.516857"}
{"text": "Even if both decisions were right ( both were indeed plots ) , the ranked scores suffer .Usually , the conditional one - versus - all scores will be much better .These use the conditional probabilities assigned to answers to rank output .", "label": "", "metadata": {}, "score": "47.521297"}
{"text": "Classifiers performed best when trained on data sets with imbalance ratio below 10 .Conclusions We were able to achieve high sensitivity with moderate specificity for automatic case identification on two data sets of electronic medical records .Such a high - sensitive case identification system can be used as a pre - filter to significantly reduce the burden of manual record validation .", "label": "", "metadata": {}, "score": "47.546375"}
{"text": "Results and discussion .The main goal of our analysis was to better characterize kernel methods and understand their short - comings in terms of PPI extraction .We started by characterizing protein pairs : we divided them into three classes based on their difficulty .", "label": "", "metadata": {}, "score": "47.643494"}
{"text": "A similar design has been proposed by the TREC Genomics track in 2006 , which investigated passage retrieval in full - text articles [ 14 ] .TREC 2003 benchmarks .To provide a general view of the problems underlying the generation of the most appropriate GeneRiF during the TREC-2003 Genomics Track [ 21 ] , a simple example is provided in Table 1 .", "label": "", "metadata": {}, "score": "47.644276"}
{"text": "Learn + + primarily for incremental learning problems that do not introduce new classes ( Polikar2001 ) , and Learn + + .NC for those that introduce new classes with additional datasets ( Muhlbaier 2008 ) are two examples of ensemble based incremental learning algorithms .", "label": "", "metadata": {}, "score": "47.646057"}
{"text": "The neural network features also should be linearly transformed with a principal components transform in order to be effective for use by a Hidden Markov Model .For use with a multi - hidden - state Hidden Markov Model , the nonlinear transform should be trained with state - specific targets , but using \" do n't cares , \" to account for imprecise information about state boundaries .", "label": "", "metadata": {}, "score": "47.660072"}
{"text": "The features which were dimensionality reduced by PCA and LDA alone were also evaluated for the purpose of comparison .Figure 16 shows recognition accuracies of dimensionality reduced features using 1-state and 3-state HMMs with 3 mixtures per state .", "label": "", "metadata": {}, "score": "47.76755"}
{"text": "L. I. Kuncheva , J. C. Bezdek , and R. Duin , \" Decision templates for multiple classifier fu - sion : an experimental comparison , \" Pattern Recognition , vol .34 , no . 2 , pp .299 - 314 , 2001 .", "label": "", "metadata": {}, "score": "47.781425"}
{"text": "Then a neural network ( 3 - 10 - 2 - 10 - 3 ) was trained as an identity map .After training , the outputs of the neural network are plotted in the right panel of Figure 6 .Clearly the neural network \" learned \" a 2-D internal representation , at the bottleneck layer , from which it could reconstruct the original data .", "label": "", "metadata": {}, "score": "47.803154"}
{"text": "The initial set of annotated data is used to create a learning curve .The curve contains .j data points with a starting sample size of m 0 and a step size of k .The values of m 0 and k are determined by users .", "label": "", "metadata": {}, "score": "47.840637"}
{"text": "Nonlinear Dimensionality Reduction Methods for Use with Automatic Speech Recognition .Stephen A. Zahorian 1 and Hongbing Hu 1 .Introduction .For nearly a century , researchers have investigated and used mathematical techniques for reducing the dimensionality of vector valued data used to characterize categorical data with the goal of preserving \" information \" or discriminability of the different categories in the reduced dimensionality data .", "label": "", "metadata": {}, "score": "47.886063"}
{"text": "Hidden Markov Models ( HMMs ) .Left - to - right Markov models with no skip were used and a total of 48 monophone HMMs were created from the training data using the HTK toolbox ( Verion 3.4 ) ( Young et al . , 2006 ) .", "label": "", "metadata": {}, "score": "47.886787"}
{"text": "The training data subset for the second classifier \\(C_2\\ ) is chosen as the most informative subset , given \\(C_1\\ .\\ ) Specifically , \\(C_2\\ ) is trained on a training data only half of which is correctly classified by \\(C_1\\ , \\ ) and the other half is misclassified .", "label": "", "metadata": {}, "score": "47.99597"}
{"text": "The total effort invested in this project was approximately 160 hours , which includes manual sentence - level annotation , code develop- Table 2 y Best Results .Informal evaluation set up : training on Set 1 ; testing on Set 2 .", "label": "", "metadata": {}, "score": "48.000866"}
{"text": "Classification accuracy points ( y j ) , i.e. the proportion of correctly classified samples , can be calculated at each training sample size x j using an independent test set or through n - fold cross validation .Model fitting and parameter identification .", "label": "", "metadata": {}, "score": "48.047256"}
{"text": "Results .Based on the TREC-2003 Genomics collection for GeneRiF identification , the LASt extraction strategy is already competitive ( 52.78 % ) .When used in a combined approach , the extraction task clearly shows improvement , achieving a Dice score of over 57 % ( +10 % ) .", "label": "", "metadata": {}, "score": "48.057182"}
{"text": "Second , we believe that advances could be achieved if methods considered additional background knowledge , for instance by adding them as features of the pair .This encompasses detailed knowledge on the proteins under consideration ( like their function , participation in protein families , evolutionary relationships , etc . ) and on the semantics of the terms surrounding them .", "label": "", "metadata": {}, "score": "48.233887"}
{"text": "Experimental setup .For the experimental setup we follow the procedure described in [ 14 ] .In a nutshell , we applied entity blinding , resolved entity - token mismatch problems and extended the learning format of the sentences with the missing parses .", "label": "", "metadata": {}, "score": "48.29347"}
{"text": "The priority queue keeps the items in ranked order up to the specified maximum .The next batch of code creates a buffer into which to append the subjective sentences .We create an iterator over the elements in the priority queue , which returns the item in order of highest estimated probability of being subjective .", "label": "", "metadata": {}, "score": "48.31768"}
{"text": "Together , these two argumentative classes concentrate between 88 % ( Set B ) and 96 % ( Set A ) for the GeneRiFs in LocusLink ( now ENTREZ - Gene ) .Fortunately , and as shown in Table 4 , the discriminative power of the argumentative classifier is more effective for these two classes than for the RESULTS and METHODS classes .", "label": "", "metadata": {}, "score": "48.349804"}
{"text": "In this paper , we want to elucidate the reason of the slow progress by performing a detailed , cross - method study of characteristics shared by PPI instances which many methods fail to classify correctly .We concentrate on a fairly recent class of PPI extraction algorithms , namely kernel methods [ 10 , 11 ] .", "label": "", "metadata": {}, "score": "48.389107"}
{"text": "Finally , the representational reason is to address to cases when the chosen model can not properly represent the sought decision boundary , which is discussed under divide and conquer section above .Perhaps one of the earliest work on ensemble systems is Dasarathy and Sheela 's 1979 paper ( Dasarathy 1979 ) , which first proposed using an ensemble system in a divide - and - conquer fashion , partitioning the feature space using two or more classifiers .", "label": "", "metadata": {}, "score": "48.477753"}
{"text": "r .g . m . a .x . a .r .g . m . a .x .The character at the intersection of the .row and . column in the matrix was then taken as the result of the classification and presented , as a feedback , to the subject , in online mode .", "label": "", "metadata": {}, "score": "48.515415"}
{"text": "Performance prediction .In this step , the mathematical model ( Eq .In other words , the fitted curve is used to extrapolate the classifier 's performance at larger sample sizes .Additionally , the 95 % confidence interval of the estimated accuracy is also calculated by using Hessian matrix and the second - order derivatives on the function describing the curve .", "label": "", "metadata": {}, "score": "48.53416"}
{"text": "A .T .X .Let .X .^ .B .Y be an approximation to .X .Note that .X .Y and .X .^ can all be viewed as ( column ) vector - valued random variables .", "label": "", "metadata": {}, "score": "48.568466"}
{"text": "Figure 3 illustrates the width of the confidence interval and MAE at various sample sizes .When the model is fitted with a small number of annotated samples , we can observe that the confidence interval width and MAE in most of the cases have larger values .", "label": "", "metadata": {}, "score": "48.611862"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , F. Roli , J. Kittler , and T. Windeatt , Eds . , vol .3077 , pp . 1 - 15 , 2004 .L. I. Kuncheva , Combining Pattern Classifiers , Methods and Algorithms .", "label": "", "metadata": {}, "score": "48.62551"}
{"text": "set to 1 on both stages .Our algorithm uses three parameters : . and . , corresponding to the regularization parameters in the SGD algorithm , and additional parameter .that defines the length each additional classifier is trained and may be defined in several ways .", "label": "", "metadata": {}, "score": "48.67146"}
{"text": "Two recent tutorials written by the current curator of this article also provide a comprehensive overview of ensemble systems ( Polikar 2006 , Polikar 2007 ) .Diversity .The success of an ensemble system - that is , its ability to correct the errors of some of its members - rests squarely on the diversity of the classifiers that make up the ensemble .", "label": "", "metadata": {}, "score": "48.67365"}
{"text": "We have compared five linear and two nonlinear classifiers in a P300 BCI speller tested on stroke and ALS patients .We have found that the BLDA classifier performs the best , followed by the ( non)linear SVM .These results could be helpful to decide what classifier to use for stroke and ALS patients .", "label": "", "metadata": {}, "score": "48.724983"}
{"text": "Similar to bagging , boosting also creates an ensemble of classifiers by resampling the data , which are then combined by majority voting .However , in boosting , resampling is strategically geared to provide the most informative training data for each consecutive classifier .", "label": "", "metadata": {}, "score": "48.782036"}
{"text": "RMSE is the average of the square root values of the difference between the observed accuracy ( y j ) and the predicted accuracy ( ) .RMSE and MAE values of close to zero indicate a better fit .To evaluate our method , we used as baseline the non - weighted least squares optimization algorithm described by Mukherjee et al [ 1 ] .", "label": "", "metadata": {}, "score": "48.78228"}
{"text": "The effect on F - score when changing the ground truth of incorrectly annotated pairs with APG and SL kernels .Modified - using the original model with modified ground truth ; retrained - results of a model retrained on the modified ground truth ; \u0394 m - o - difference between modified and original ; \u0394 r - m - difference between retrained and modified .", "label": "", "metadata": {}, "score": "48.791603"}
{"text": "Otherwise , we append the next sentence .Finally , we return the result after trimming any residual whitespace ( probably just the final newline ; not a very efficient way to do this at all , but these data sets are miniscule ) .", "label": "", "metadata": {}, "score": "48.795197"}
{"text": "Here , we only present algorithm with our modifications for weighting sample , for detailed analysis please refer to the original article [ 4 ] .On each iteration , the Pegasos algorithm is given iteration number .( regulating how \" soft \" the resulting margin is , i.e. , whether the priority is given to low error rate over training set or larger margin between classes ) , sample vector . , used in the original Pegasos algorithm , is taken to be 1 , reducing Pegasos to an SGD algorithm with an additional projection step .", "label": "", "metadata": {}, "score": "48.845573"}
{"text": "In contrast with NLDA1 where dimensionality reduction is assigned to PCA , for NLDA2 , since the dimensionality reduction can be accomplished with the neural network only , the linear PCA is used specifically for reducing the feature correlation .Figure 9 .", "label": "", "metadata": {}, "score": "48.942467"}
{"text": "A new classifier is added if .: Objective function of a . 's weak classifier .: Objective function of a strong classifier .: A loss function for the combined strong classifier .In this work we use hinge - loss function ( see ( 1 ) ) .", "label": "", "metadata": {}, "score": "48.975952"}
{"text": "We then performed feature selection by information gain using each difficulty class as label .The ten most relevant features of the difficult ( D ) and easy ( E ) classes are tabulated in Table 16 according to an independent feature analysis .", "label": "", "metadata": {}, "score": "48.982586"}
{"text": "These results show the NLDA methods based on the state level training targets are able to form highly discriminative features in a dimensionality reduced space .MFCC experiments .For comparison , 39-dimensional MFCC features ( 12 coefficients plus energy with the delta and acceleration terms ) were reduced to 36 dimensions with the same configurations and evaluated .", "label": "", "metadata": {}, "score": "49.01371"}
{"text": "We compare our algorithm to other online training algorithms , and we show , that for most cases with unknown kernel parameters , our algorithm outperforms other algorithms both in runtime and convergence speed .Introduction .Online Training Methods .During the recent years , there has been an increasing amount of interest in the area of online learning methods .", "label": "", "metadata": {}, "score": "49.014347"}
{"text": "T.K. Ho , \" Complexity of classification problems and comparative advantages of combined classifiers , \" Int .Workshop on Multiple Classifier Systems , lecture Notes on Computer Science , Vol .1857 , pp .97 - 106 , 2000 , Springer- Verlag . F. Roli , G. Giacinto , \" Design of Multiple Classifier Systems , \" in H. Bunke and A. Kandel ( Eds . )", "label": "", "metadata": {}, "score": "49.044224"}
{"text": "Four contain thousands of gene pairs and 6 have single genes that perfectly discriminate .Conclusion .This technique discovered small sets of genes ( 3 or less ) in published data that form accurate classifiers , yet were not reported in the prior publications .", "label": "", "metadata": {}, "score": "49.09546"}
{"text": "m -feature linear classifier ( m -LC ) .Consider a two - class data set composed of N data points , .Assume that the classes are linearly separable .If this value is positive , x is identified with the +1 class , otherwise it belongs to the -1 class .", "label": "", "metadata": {}, "score": "49.11718"}
{"text": "Witten IH , Frank E : Data Mining : Practical Machine Learning Tools and Techniques , 2nd edition .San Francisco : Morgan Kaufmann ; 2005 .Miwa M , Pyysalo S , Hara T , Tsujii J : Evaluating dependency representations for event extraction .", "label": "", "metadata": {}, "score": "49.13241"}
{"text": "View Article .Brinker K : Incorporating Diversity in Active Learning with Support Vector Machines .Proceedings of the Twentieth International Conference on Machine Learning ( ICML ) : 2003 2003 , 59 - 66 .Yuan J , Zhou X , Zhang J , Wang M , Zhang Q , Wang W , Shi B : Positive Sample Enhanced Angle - Diversity Active Learning for SVM Based Image Retrieval .", "label": "", "metadata": {}, "score": "49.14589"}
{"text": "Progression of confidence interval widths for the observed values ( training set ) and the predicted values .We also compared our algorithm with the un - weighted algorithm .Table 1 shows average values of RMSE for the baseline un - weighted and our weighted method ; min and max values are also provided .", "label": "", "metadata": {}, "score": "49.19215"}
{"text": "The LOO error is included in the results available from the web site .In the end , classifiers with a larger separation ( margin ) between the classes are able to tolerate more noise without errors .Thus larger margin classifiers are more desirable for use on future data points .", "label": "", "metadata": {}, "score": "49.23561"}
{"text": "( Like the other statistics , this is thoroughly explained in the class documentation . )Even for a two - way classification problem , these reports provide some interesting insight into the classificatioin problem .Here 's the initial piece of the report for the category of objective sentences , plot : .", "label": "", "metadata": {}, "score": "49.289886"}
{"text": "The use of PCA improves accuracy on the order of 2 % to 10 % , depending on the conditions .In contrast , for NLDA2 , best performance was obtained with the nonlinearities used for both training and final transformations .", "label": "", "metadata": {}, "score": "49.364056"}
{"text": "i . g .n . can be expressed in the same form as objective function .of SVM : . where . is a vector that consists of outputs provided by a set of .weak classifiers , .That means that an SVM training algorithm can be applied for training weights . , with the same guarantees for convergence rate and generalization error bound shown in [ 5 ] .", "label": "", "metadata": {}, "score": "49.489067"}
{"text": "When the amount of training data is too large to make a single classifier training difficult , the data can be strategically partitioned into smaller subsets .Each partition can then be used to train a separate classifier which can then be combined using an appropriate combination rule ( see below for different combination rules ) .", "label": "", "metadata": {}, "score": "49.504864"}
{"text": "Related Work .Our algorithm is closely related to algorithms used for SVM training and boosting .These algorithms , taken together , can be separated into several classes : .Support Vector Machines .The support vector machines are a class of linear or kernel - based binary classifiers that attempt to maximize the minimal distance ( margin ) between each member of the class and separating surface .", "label": "", "metadata": {}, "score": "49.525665"}
{"text": "The maximum F(1)-measure indicates the best possible operating point achievable by setting a threshold .The BEP indicates the best score possible when precision is equal to recall .Note that the first set of results compares cases using their score , which is just their joint log probabilities ( divided by the number of characters , and thus expressed as negative cross - entropy rates ) .", "label": "", "metadata": {}, "score": "49.594433"}
{"text": "As illustrated in one of the experiments presented later , the state dependent targets were shown to perform better than phone level targets .The simpler approach of using fixed ratios for state boundaries was as good as using the Viterbi alignment approach .", "label": "", "metadata": {}, "score": "49.667778"}
{"text": "Typically , naive Bayes as used in classifiers is smoothed using something like add - one ( Laplace ) smoothing .This is what Pang and Lee do for their naive Bayes baseline .The way to implement add - one smoothing over LingPipe 's naive Bayes implementation is to collect all of the tokens during the first training pass in a set .", "label": "", "metadata": {}, "score": "49.743423"}
{"text": "Evaluation was carried out on 12 learning curves at dozens of sample sizes for model fitting and predictions were validated using standard goodness of fit measures .Algorithm description .The algorithm to model and predict a classifier 's performance contains three steps : .", "label": "", "metadata": {}, "score": "49.87078"}
{"text": "Incremental learning refers to the ability of an algorithm to learn from new data that may become available after a classifier ( or a model ) has already been generated from a previously available dataset .Hence , an incremental learning algorithm must learn the new information , and retain previously acquired knowledge , without having access to previously seen data .", "label": "", "metadata": {}, "score": "49.919537"}
{"text": "Bo and Jonassen surveyed a number of classifier discovery methods including linear hyperplanes .They showed that accurate two - gene classifiers exist in real world data sets and that they perform well .They only analyzed 2 data sets , did not report computer runtimes nor consider single genes or gene triples in their analysis .", "label": "", "metadata": {}, "score": "49.989655"}
{"text": "R1-R13 , 2007 .View at Google Scholar \u00b7 View at Scopus .R. F. Yazicioglu , P. Merken , R. Puers , and C. Van Hoof , \" Low - power low - noise 8-channel EEG front - end ASIC for ambulatory acquisition systems , \" in Proceedings of the 32nd European Solid - State Circuits Conference ( ESSCIRC ' 06 ) , pp .", "label": "", "metadata": {}, "score": "50.020992"}
{"text": "For our nSVM classifier , a search through pairs .( where . is the regularization parameter and .the kernel parameter ) was performed using a 5-fold cross - validation on the grid .[ . 6 . ]", "label": "", "metadata": {}, "score": "50.053307"}
{"text": "For the validation of the performance of the classifiers and their comparison , we used as features the amplitudes of the filtered EEG signals from different electrodes .This led to satisfactory results for healthy subjects ( see , e.g. , [ 17 ] ) .", "label": "", "metadata": {}, "score": "50.063553"}
{"text": "Conclusions .Nonlinear dimensionality reduction methods , based on the general nonlinear mapping abilities of neural networks , can be useful for capturing most of the information from high dimensional spectral / temporal features , using a much smaller number of features .", "label": "", "metadata": {}, "score": "50.084633"}
{"text": "Outline of the Paper .The remainder of this article is organized as follows .In Section 2 , we give the description of our algorithm .In Section 3 , we compare it to several existing online training methods in terms of computational cost and flexibility .", "label": "", "metadata": {}, "score": "50.097168"}
{"text": "We could not however obtain a better performance level due to the fact that LocusLink 's annotators may have used words that did not appear in the article 's title or in the abstract .Moreover , correct GeneRiFs may paraphrase a sentence or the article 's title , revealing the same gene function with different words or expressions .", "label": "", "metadata": {}, "score": "50.11483"}
{"text": "We investigate the achieved typing accuracy given the individual patient 's disorder , and how it correlates with the type of classifier used .We considered 7 types of classifiers , linear as well as nonlinear ones , and found that , overall , one type of linear classifier yielded a higher classification accuracy .", "label": "", "metadata": {}, "score": "50.161747"}
{"text": "CoNLL-2000 2000 , 111 - 116 .Orasan C : Patterns in Scientific Abstracts .Proceedings Corpus Linguistics 2001 , 433 - 445 .Swales J : Genre Analysis : English in Academic and Research Settings Cambridge University Press 1990 .Langley P , Iba W , Thompson K : An Analysis of Bayesian Classifiers .", "label": "", "metadata": {}, "score": "50.163372"}
{"text": "All kernels except for the very slow PT kernel are taken into consideration .Table 1 .The distribution of pairs for each corpus according to classification success level using cross - validation setting .The distribution of pairs ( total , positive and negative ) in terms of the number of kernels that classify them correctly .", "label": "", "metadata": {}, "score": "50.243538"}
{"text": "Model fitting ; .Sample size prediction ; .Learning curve creation .Assuming the target performance measure is classification , a learning curve that characterizes classification accuracy ( Y acc ) , as a function of the training set size ( X ) is created .", "label": "", "metadata": {}, "score": "50.256783"}
{"text": "E .X .X .^ . ) is minimized .That is , .X .^ should approximate X as well as possible , in a mean square error sense .As has been shown in several references ( for example , Duda et al . , 2001 ) , this seemingly intractable problem has a very straightforward solution , provided X is zero mean and multivariate Gaussian .", "label": "", "metadata": {}, "score": "50.27852"}
{"text": "It 's identical to the first demo in the way that it steps through data , so we do n't repeat that code here .Construction : Reading in the Model .The constructor in this implementation does all of the work of the one in the basic polarity demo in setting up member variables ( indicated by ellipses ( ... ) ) .", "label": "", "metadata": {}, "score": "50.297836"}
{"text": "If a vast majority of the classifiers agree with their decisions , such an outcome can be interpreted as the ensemble having high confidence in its decision .If , however , half the classifiers make one decision and the other half make a different decision , this can be interpreted as the ensemble having low confidence in its decision .", "label": "", "metadata": {}, "score": "50.30444"}
{"text": "Also unlike [ 1 ] , we only train the last weak classifier of the set rather than all of them .The reason for this is that fixing parameters of all classifiers previously added increases the accuracy of the error rate estimation for the training of additional classifier , as well as significantly decreasing the amount of calculations needed for a single update .", "label": "", "metadata": {}, "score": "50.31138"}
{"text": "For any given instance , the class chosen by most number of classifiers is the ensemble decision .Since the training datasets may overlap substantially , additional measures can be used to increase diversity , such as using a subset of the training data for training each classifier , or using relatively weak classifiers ( such as decision stumps ) .", "label": "", "metadata": {}, "score": "50.418514"}
{"text": "Table 8 depicts the overall performance measure using the Dice coefficient ( last column ) .The table 's middle columns show how the proposed GeneRiF may have originated from the article 's title or from an abstract sentence .Our baseline approach was very simple .", "label": "", "metadata": {}, "score": "50.447277"}
{"text": "Each binary classifier combined three types of features : words , word bigrams and trigrams .The log of the class frequency represented the weight of each feature , but for every category , DF thresholding [ 29 ] is applied so that rare features are not selected .", "label": "", "metadata": {}, "score": "50.462936"}
{"text": "Maximal margin means that the hyperplane is positioned halfway between the two classes .Gene sets that linearly separate the data are recorded , otherwise they are rejected .If a single gene is a classifier , it is not used during the pair checks as it would always form a classifier with any other gene .", "label": "", "metadata": {}, "score": "50.482506"}
{"text": "The original tuning of the tool is based on experiments conducted for the BioCreative tasks 2.1 [ 5 ] , where the system achieved very competitive results both for functional annotation and for passage selection - see [ 31 ] for a comparative presentation .", "label": "", "metadata": {}, "score": "50.48363"}
{"text": "Conclusions : Our study demonstrates the feasibility of establishing a biorepository of plasma , serum and DNA , with relatively rapid annotation of clinical variables using EMR - based algorithms .Figure 3 . 2-D 2-class data , along with first LDA basis vector and first PCA basis vector .", "label": "", "metadata": {}, "score": "50.49872"}
{"text": "Equation ( 1 ) describes the classifier 's accuracy ( Y acc ) as function of the training sample size \u00d7 with the parameters a , b , and c representing the minimum achievable error , learning rate and decay rate respectively .", "label": "", "metadata": {}, "score": "50.55567"}
{"text": "All gene sets that formed zero training error classifiers were saved and are available via the web site .Table 2 lists the number of such gene sets .Many data sets have single genes or pairs that form such perfect linear classifiers which is interesting as most original reports did not note their presence .", "label": "", "metadata": {}, "score": "50.578667"}
{"text": "Now , assume that each classifier has a probability p of making a correct decision .Weighted majority voting .Other combination rules .There are several other combination rules , which are arguably more sophisticated than the ones listed above .", "label": "", "metadata": {}, "score": "50.59272"}
{"text": "In layman 's terms , we 're not particularly confident about our results for the basic polarity evaluation .If we had 2000 tests rather than 200 ( ten times as much data ) , that number would be + /- 0.014 , a factor of sqrt(10 ) less due to 10 times the amount of data .", "label": "", "metadata": {}, "score": "50.656258"}
{"text": "401 - 407 , 2001 .L. I. Kuncheva , \" Switching between selection and fusion in combining classifiers : An ex - periment , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B : Cybernetics , vol .", "label": "", "metadata": {}, "score": "50.671135"}
{"text": "Dietterich and Bakiri introduced ECOC to be used within the ensemble setting ( Dietterich 1995 ) .The idea is to use a different class encoding for each member of the ensemble .The encodings constitute a binary C by T code matrix , where C and T are the number of classes and ensemble size , respectively , combined by the minimum Hamming distance rule .", "label": "", "metadata": {}, "score": "50.699696"}
{"text": "We also investigated the sentence position 's impact on the classification effectiveness through assigning a relative position to each sentence , see [ 30 ] for a comprehensive evaluation and description of the argumentative classifier .Table 3 .The classification results for the abstract shown in Table 2 ( explicit argumentative labels are removed before classification ) .", "label": "", "metadata": {}, "score": "50.701546"}
{"text": "With the re - trained model the performance of APG and SL could be further improved on both corpora ( F - score gain 0.25 - 1.0 ) .This shows that the re - annotation of corpora yield performance uplift even if only a small fraction of pairs is concerned .", "label": "", "metadata": {}, "score": "50.724968"}
{"text": "Small sets of genes ( single genes , pairs of genes , and triples of genes ) able to accurately classify two - class microarray data occur in many real - world data sets .These small sets could portend simple medical diagnostics and point to important research targets .", "label": "", "metadata": {}, "score": "50.732758"}
{"text": "The pairs that are difficult in both evaluation settings ( D ) are reasonable target for further inspection , as improving kernels to better perform on the them would benefit both scenarios ; we attempt to characterize such pairs in subsequent Section .", "label": "", "metadata": {}, "score": "50.758217"}
{"text": "Traditional Classification Statistics .The report continues with a range of standard statistics that have been applied to classification problems : .The most popular statistic here is the kappa statistic , which we present in all three forms : \" standard \" , adjusted for \" bias \" , and adjusted for \" prevalence \" .", "label": "", "metadata": {}, "score": "50.77698"}
{"text": "Determining the sample size required to achieve sufficient statistical power to reject a null hypothesis is a standard approach [ 13 - 16 ] .Cohen defines statistical power as the probability that a test will \" yield statistically significant results \" i.e. the probability that the null hypothesis will be rejected when the alternative hypothesis is true [ 17 ] .", "label": "", "metadata": {}, "score": "50.797974"}
{"text": "Argumentative representation levels and conceptual density estimation using Gene Ontology contents appear complementary for functional annotation in proteomics .Introduction .As an increasing amount of information becomes available in the form of electronic documents , the increasing need for intelligent text processing makes shallow text understanding methods such as the Information Extraction ( IE ) particularly useful .", "label": "", "metadata": {}, "score": "50.802937"}
{"text": "being the number of added classifiers , and each classifier producing output . , so the convergence rate slowly decreases as additional classifiers are added .To combat this , certain classifiers with lows weights may be removed from the pool .", "label": "", "metadata": {}, "score": "50.83819"}
{"text": "We refer to this set of features as temporal resolution features .SVM Classifier The corpus comprising of sentences indicating past smoker , current smoker , and smoker was represented as vectors using the temporal resolution features .We built a linear SVM classifier from that corpus using Weka .", "label": "", "metadata": {}, "score": "50.90863"}
{"text": "Similar vector space classifiers have been used previously for PPI extraction by [ 31 ] , however , without an in - depth comparison with existing kernels and in a different evaluation setting .These observations suggest that PPI extraction performance depends far more on the feature set than on the similarity function encoded in kernels , and that future research in the field should focus on finding more expressive features rather than more complex kernel functions .", "label": "", "metadata": {}, "score": "50.922203"}
{"text": "Suppose that the targets .( in the case of a classification problem these are .[ . . ] with an additive Gaussian noise term .Assuming further an independent generation of the examples from a data set , the likelihood of all data is .", "label": "", "metadata": {}, "score": "50.92874"}
{"text": "These proposals are regularly compared to each other regarding their overall performance on different gold standard corpora , but little is known about their respective performance on the instance level .Results .We report on a detailed analysis of the shared characteristics and the differences between 13 current methods using five PPI corpora .", "label": "", "metadata": {}, "score": "50.940567"}
{"text": "Pyysalo S , Ginter F , Heimonen J , Bjorne J , Boberg J , Jarvinen J , Salakoski T : BioInfer : a corpus for information extraction in the biomedical domain .BMC Bioinformatics 2007 , 8 : 50 .PubMed View Article .", "label": "", "metadata": {}, "score": "50.979927"}
{"text": "Confusion matrix columns correspond to predicted classes .Still , we found a number of correlations between pair difficulty and simple surface features that can not be exploited in kernels as they use a different feature set .Later on , we will show that such features already suffice to build a classifier that is almost on par with the state - of - the - art , without using any sophisticated ( and costly to compute ) kernels .", "label": "", "metadata": {}, "score": "50.993538"}
{"text": "In the remaining part of this chapter , two versions of NLDA based on this strategy are described , followed by a series of experimental evaluations for the phonetic classification and recognition tasks .Nonlinear dimensionality reduction architecture .In a previous work ( Zahorian et al . , 2007 ) , NLPCA was applied to an isolated vowel classification task , and the nonlinear method based on neural networks was experimentally compared with linear methods for reducing the dimensionality of speech features .", "label": "", "metadata": {}, "score": "51.007854"}
{"text": "Then the classifier is used to produce a classification for a review string in a single line .Next , the classification 's best category is extracted as the result category of classification .If the result category matches the test category , the number of correct classifications is incremented .", "label": "", "metadata": {}, "score": "51.070568"}
{"text": "Contact us if you need more information .For movie reviews we focus on two types of classification problem : .Subjective ( opinion ) vs. Objective ( fact ) sentences .Positive ( favorable ) vs. Negative ( unfavorable ) movie reviews .", "label": "", "metadata": {}, "score": "51.10817"}
{"text": "We first review some traditional linear methods for dimensionality reduction before proceeding to the nonlinear transformation , the main subject of this chapter .Principal Components Analysis ( PCA ) .Principal Components Analysis ( PCA ) , also known as the Karhunen - Loeve Transform ( KLT ) , has been known of and in use for nearly a century ( Fodor , 2002 ; Duda et al . , 2001 ) , as a linear method for dimensionality reduction .", "label": "", "metadata": {}, "score": "51.124523"}
{"text": "The evaluation code this time is even simpler with the use of the evaluator .Classification classification .mCategories[i ] .tok.gt9.5000 \" ) ; .The first line simply creates an evaluator from the classifier and the array of categories .", "label": "", "metadata": {}, "score": "51.1382"}
{"text": "Figure 13 .Training target vectors of the neural network .State - level targets .Due to the nonstationarity of speech signals , a speech signal varies even in a very short time interval ( e.g. a phoneme ) .For speech recognition tasks , instead of phone level training targets , state ( as in hidden states of an HMM ) dependent targets could be advantageous in training a versatile network for more highly discriminative speech features .", "label": "", "metadata": {}, "score": "51.141"}
{"text": "Classification accuracies of neural network ( top panel ) and MXL ( bottom panel ) classifiers with various types of features .The results obtained with the neural network and MXL classifiers using 10 % of the group 2 training data ( that is , 5 % of the overall training data ) are shown in Figure 12 .", "label": "", "metadata": {}, "score": "51.19718"}
{"text": "Therefore , such systems are also known as multiple classifier systems , or just ensemble systems .There are several scenarios where using an ensemble based system makes statistical sense , which are discussed below in detail .For example , we typically ask the opinions of several doctors before agreeing to a medical procedure , we read user reviews before purchasing an item ( particularly big ticket items ) , we evaluate future employees by checking their references , etc .", "label": "", "metadata": {}, "score": "51.207603"}
{"text": "Nigam K , McCallum AK , Thrun S , Mitchell T : Text Classification from Labeled and Unlabeled Documents using EM .Mach Learn 2000 , 39 ( 2 - 3 ) : 103 - 134 .View Article .Vlachos A : A stopping criterion for active learning .", "label": "", "metadata": {}, "score": "51.208347"}
{"text": "We can also see that the larger a corpus , the better CV and CL evaluations \" agree \" on the difficulty class of pairs : the strongest correlations can be observed at BioInfer and AIMed .Considering case ( 2 ) , for LLL , the intersection of difficult pairs in CV and CL happens to be empty .", "label": "", "metadata": {}, "score": "51.267292"}
{"text": "This method runs through the categories , of which there are two in this demo .It then creates a directory using the polarity data directory and the name of the category .This only works for this demo because the data is organized into directories by category .", "label": "", "metadata": {}, "score": "51.320656"}
{"text": "For both the neural network and MXL classifiers , NLDA2 clearly performs much better than the other transformations or the original features .However , the advantage of NLDA2 decreases with an increasing number of features , and as the percentage of group 2 data increases ( not shown in figure ) .", "label": "", "metadata": {}, "score": "51.341095"}
{"text": "Algorithm Description .Overview of SVM and Pegasos Algorithm .Support vector machines are a useful classification tool , developed by Cortes and Vapnik [ 11 ] , that attempt to construct a hyperplane separating the data that has the largest distance to any point in any class ( see Figure 1 ) .", "label": "", "metadata": {}, "score": "51.424683"}
{"text": "Thus , this similarity coefficient measures the lexical overlap between a candidate and the corresponding correct GeneRiF. More precisely , four Dice coefficients variants were suggested , and all were found to be highly correlated - N - grams metrics were tested but no evidence of improvement compared to word - based distance measure was observed .", "label": "", "metadata": {}, "score": "51.517548"}
{"text": "Classifier similarity is a key factor when constructing ensemble classifiers .We define the similarity of two kernels as the number of shared annotations versus the total number of annotations .Performing hierarchical clustering with this similarity measure reveals that kernels using the same parsing information group together almost perfectly , i.e. , classify pairs much more similarly to each other than to kernels using different parsing information ( see Figure 8 ) .", "label": "", "metadata": {}, "score": "51.524063"}
{"text": "The columns of B are also the same eigenvectors .Thus the \" forward \" and \" reverse \" transformations are transposes of each other .The components of Y are uncorrelated .Furthermore the expected value of this normalized mean square error between original and re - estimated X vectors can be shown to equal the ratio of the sum of \" unused \" eignevalues to the sum of all eigenvalues .", "label": "", "metadata": {}, "score": "51.547546"}
{"text": "In this example , one of RESULTS sentences ( in bold ) is misclassified as METHODS , while he INTRODUCTION sentence has been classified as PURPOSE .CONCLUSION ( 00160116 )The highly favorable pathologic stage ( RI - RII , 58 % ) and the fact that the majority of patients were alive and disease - free suggested a more favorable prognosis for this type of renal cell carcinoma .", "label": "", "metadata": {}, "score": "51.609356"}
{"text": "..Basically , this is just another precision - recall evaluation over aggregate data .Affiliated with .Abstract .Background .Molecular profiling generates abundance measurements for thousands of gene transcripts in biological samples such as normal and tumor tissues ( data points ) .", "label": "", "metadata": {}, "score": "51.633995"}
{"text": "The configuration of HMMs can be largely simplified by incorporating NLDA .Experiments with large network training .The results of the previous experiment showed large performance advantages for NLDA2 over NLDA1 and the original features , when using either a small number of features , or a \" small \" HMM .", "label": "", "metadata": {}, "score": "51.714035"}
{"text": "Fisher 's Linear Discriminant Analysis .Fisher 's linear discriminant analysis ( LDA ) is one of the most widely used classifiers in P300 BCI systems [ 10 , 15 ] .It was reported to even outperform other classifiers [ 11 ] .", "label": "", "metadata": {}, "score": "51.796875"}
{"text": "Discussion .Many of the genes sets found accurately classify the data with large separation between the classes .It is exciting to consider the possibilities for medical diagnostics if some small gene set is found to accurately and reproducibly indicate a disease state .", "label": "", "metadata": {}, "score": "51.83476"}
{"text": "Consider the basic polarity evaluation , for which accuracy is 81.5 % , or 0.815 over 200 cases .This leads to a deviation of .This is a huge deviation , primarily because there are only 200 test cases .A 95 % confidence interval is roughly plus or minus 1.6 deviations , or about + /- 0.044 .", "label": "", "metadata": {}, "score": "51.96951"}
{"text": "Relabeled means the the 2 \" outlier \" samples are re - labeled as normal . )Table 3 .Wallclock run times for 2-LCs ( pairs ) for some of the data sets listed in Table .The P -LC program is written in C and uses double precision arithmetic .", "label": "", "metadata": {}, "score": "51.997818"}
{"text": "For example , un - annotated clinical notes are abundant .To label un - annotated text corpora from the clinical domain , however , requires a group of reviewers with domain expertise and only a tiny fraction of the available clinical notes can be annotated .", "label": "", "metadata": {}, "score": "52.01255"}
{"text": "Due to the fact that the EEG signal has a constant sampling rate , and assuming a constant ( virtual ) serial port latency , the precise mapping between the timestamps and the corresponding EEG data samples is straightforward .We used this mapping for partitioning the EEG signal into signal segments , for further processing .", "label": "", "metadata": {}, "score": "52.040184"}
{"text": "As the duration of each session is known a priori , as well as the data transfer rate , it is easy to estimate the amount of data transmitted during a session .We used this estimate , increased by a . margin , as the size of the serial port buffer .", "label": "", "metadata": {}, "score": "52.072403"}
{"text": "During the three - month period before the formal evaluation , we used Set 1 as a development and training set and Set 2 as our test set .After we determined the best configuration parameters on the training / development set , we built our models from Set 1 .", "label": "", "metadata": {}, "score": "52.12625"}
{"text": "In each case , a final decision is made by combining the individual decisions of several experts .In doing so , the primary goal is to minimize the unfortunate selection of an unnecessary medical procedure , a poor product , an unqualified employee or even a poorly written and misguiding article .", "label": "", "metadata": {}, "score": "52.128532"}
{"text": "Stacked Generalization .In Wolpert 's stacked generalization ( or stacking ) , an ensemble of classifiers is first trained using bootstrapped samples of the training data , creating Tier 1 classifiers , whose outputs are then used to train a Tier 2 classifier ( meta - classifier ) ( Wolpert 1992 ) .", "label": "", "metadata": {}, "score": "52.14838"}
{"text": "Therefore we could expect that by estimating the density of other semantic categories , such as pathological functions ( e.g. drugs or diseases using Medical Subject Headings ) , could significantly help extracting passages supporting protein - diseases associations , in particular for passages related to mutated proteins [ 41 ] .", "label": "", "metadata": {}, "score": "52.16794"}
{"text": "1998 ) discusses several theoretical methods for determining these curved subspaces ( manifolds ) within higher dimensionality spaces .Another general method , and the one illustrated and explored in more detail in this chapter , is based on a \" bottleneck \" neural network ( Kramer , 1991 ) .", "label": "", "metadata": {}, "score": "52.199875"}
{"text": "59 - 72 , 2007 .R. Polikar , \" Ensemble based systems in decision making , \" IEEE Circuits and Systems Magazine , vol .6 , no.3 , pp .21 - 45 , 2006 .Muhlbaier M. , Topalis A. , Polikar R. , \" Ensemble confidence estimates posterior probability , \" 6th Int .", "label": "", "metadata": {}, "score": "52.21058"}
{"text": "Both the experts themselves and the gating network requires the input instances for training .Several mixture - of - experts models can also be further combined to obtain a hierarchical mixture of experts ( Jordan 1994 ) .Mixture of experts are particularly useful when different experts are trained on different parts of the feature space , or when heterogeneous sets of features are available to be used for a data fusion problem .", "label": "", "metadata": {}, "score": "52.217148"}
{"text": "Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector .No straight line can be a good fit to this data .Linear Discriminant Analysis ( LDA ) .Linear transforms for the purpose of reducing dimensionality while preserving discriminability between pre - defined categories have also long been known about and used ( Wang & Paliwal , 2003 ) , and are usually referred to as Linear Discriminant Analysis ( LDA ) .", "label": "", "metadata": {}, "score": "52.23582"}
{"text": "2- and 3-LCs for the data sets described in Table 1 .For the two - class partitionings shown , the Table gives the number of perfect linear classifiers ( given P initial features ) that can be constructed using one , two and for some sets 3 , genes .", "label": "", "metadata": {}, "score": "52.237488"}
{"text": "Even when similar patient populations are pooled together from multiple locations , differences in medical treatment and record systems can limit which outcome measures can be commonly analyzed .In total , these differences in medical research settings can lead to differing conclusions or can even prevent some studies from starting .", "label": "", "metadata": {}, "score": "52.259872"}
{"text": "G. Fumera and F. Roli , \" A Theoretical and Experimental Analysis of Linear Combiners for Multiple Classifier Systems , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol .27 , no .6 pp .942 - 956 , 2005 .", "label": "", "metadata": {}, "score": "52.274315"}
{"text": "Shah P , Perez - Iratxeta C , Bork P , Andrade M : Information extraction from full text scientific articles : Where are the keywords ?BMC Bioinformatics 2003 , 4 ( ) : 20 .View Article PubMed .Hakenberg J , Rutsch J , Leser U : Tuning text classification for hereditary diseases with section weighting .", "label": "", "metadata": {}, "score": "52.32403"}
{"text": "The ErrP is evoked when the subject perceives a wrong outcome of the BCI system .When the ErrP is detected , we can take the second most likely character ( e.g. , the row or the column with the second largest distance to the classification boundary ) for correcting the classifier 's outcome .", "label": "", "metadata": {}, "score": "52.351498"}
{"text": "A set of 5000 GeneRiF - abstract pairs has been collected for tuning our system before evaluating it on the TREC benchmark .Table 1 .Example of an ENTREZ - Gene record and the corresponding GeneRiF ( italic added ) .", "label": "", "metadata": {}, "score": "52.359337"}
{"text": "In our prior work on medical text classification , we have investigated and experimented with several active learning sampling methods and observed the need to predict future classification performance for the purpose of selecting the best sampling algorithm and sample size [ 10 , 11 ] .", "label": "", "metadata": {}, "score": "52.39337"}
{"text": "Note that in this usage , \" outputs \" may be from the final outputs or from one of the internal hidden layers .Figure 7 .Overview of the NLDA transformation for speech recognition .The multilayer bottleneck neural network employed in NLDA contains an input layer , hidden layers including the bottleneck layer , and an output layer .", "label": "", "metadata": {}, "score": "52.42136"}
{"text": "The positive ground truth rate vs. the length of the sentence containing the pair .The distance in words between entities in a sentence seems to be more independent from the difficulty of the pair ( see Figure 6 ) .The entities in NE pairs are closer to each other than neutral or more difficult ones , while for positive pairs no such tendency can be observed : the distance in both PE and PD pairs are shorter than at neutral ones .", "label": "", "metadata": {}, "score": "52.436535"}
{"text": "GeneRiF distribution in titles and ( \" ti \" ) and in abstracts from the 1 st to the n th sentence .Methods .As for automatic abstracting , evaluating sentence classifiers is difficult .First , establishing a consensus benchmark is clearly a more complex task .", "label": "", "metadata": {}, "score": "52.44342"}
{"text": "As a result , the classifier produces 12 ( for each row / column ) values . which describe the distance to the class boundary in the feature space , together with the sign .The row index .and the column index . of the classified character were calculated as .", "label": "", "metadata": {}, "score": "52.469254"}
{"text": "Grate L , Bhattacharyya C , Jordan M , Mian I : Simultaneous relevant feature identification and classification in high - dimensional spaces .Workshop on Algorithms in Bioinformatics ( WABI 2002 ) ( Edited by : Guig\u00f3 R , D G ) .", "label": "", "metadata": {}, "score": "52.471664"}
{"text": "Thus , in one sense , the recognizer is a hybrid neural network / Hidden Markov Model ( NN / HMM ) recognizer .However , the neural network step is used for the task of nonlinear dimensionality reduction and is independent of the HMM .", "label": "", "metadata": {}, "score": "52.66236"}
{"text": "Figure 10 .Illustrations of a linear activation function ( left ) , a unipolar sigmoid function ( middle ) and a bipolar sigmoid function ( right ) .The weights of the neural network are estimated using the backpropagation algorithm to minimize the distance between the scaled input features and target data .", "label": "", "metadata": {}, "score": "52.704872"}
{"text": "What this means is that the joint probabilities assigned to cases perform very well at ranking plots versus quotes ( 92 % accuracy ) , but not very good at ranking confidence overall .For instance , in one case , there might be a score of -1.2 for plot and -1.4 for quote , whereas in another there might be a score of -1.9 for plot and -2.1 for quote .", "label": "", "metadata": {}, "score": "52.706024"}
{"text": "The similarity score is calculated by both kernels as the number of identical subgraphs of two shortest paths with the specific node labeling .The combined kernel is the sum of the former two variants .The syntactic kernel , defined in [ 16 ] , applies exclusively the structural information from the dependency tree , that is , only the edge labels are considered at similarity score calculation .", "label": "", "metadata": {}, "score": "52.7144"}
{"text": "Our best F - score is 85.57 for this final formal evaluation ( most frequent category baseline is 60.58 ) .Our error analysis uncovered several areas for improve- ment .Currently , our negation detection does not account for nonnegated lexical items indicating nonsmoker status , e.g. , nonsmoker , nonsmoker .", "label": "", "metadata": {}, "score": "52.76469"}
{"text": "Bo T , Jonassen I : New feature subset selection procedures for classification of expression profiles .Genome Biol 2002 , 3 ( 4 ) : research0017.1 - 0017.11 .View Article .Kim S , Dougherty E , Barrera J , Chen Y , Bittner M , Trent J : Strong Feature sets from small samples .", "label": "", "metadata": {}, "score": "52.794937"}
{"text": "and the set . of corresponding labels .According to [ 21 ] , the mutual information between the set of projections .and the set of corresponding labels . can be estimated as .l .o . g .Artificial Neural Network .", "label": "", "metadata": {}, "score": "52.803566"}
{"text": "146 - 156 , 2002 .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the Margin : A New Explanation for the Effectiveness of Voting Methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "52.830696"}
{"text": "Therefore , an additional experiment was performed , using the state level targets with \" do n't cares , \" as mentioned previously , and a very large neural network for transforming features .The state targets were formed using either a constant length ratio ( ratio for 3 states : 1:4:1 ) or a Viterbi forced alignment approach , as described in Section 4.5 .", "label": "", "metadata": {}, "score": "52.846924"}
{"text": "The use of confidence or fiducial limits illustrated in the case of the binomial .Biometrica 1934;26:404 - 13 .28 SAVOVA et al . , Mayo Clinic NLP System .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .", "label": "", "metadata": {}, "score": "52.950058"}
{"text": "( 1 ) High - dimensional linearly separable data ( Linear ) .A random hyperplane is created in 50-dimensional space .Data points are generated randomly to both positive and negative sides of the hyperplane .Data points too close to the hyperplane are filtered out .", "label": "", "metadata": {}, "score": "52.976498"}
{"text": "119 - 139 , 1997 .J. Kittler , M. Hatef , R. P. W. Duin , and J. Mates , \" On combining classifiers , \" IEEE Trans . on Pattern Analysis and Machine Intelligence , vol .20 , no . 3 , pp .", "label": "", "metadata": {}, "score": "52.985985"}
{"text": "For the MXL classifier , NLDA2 features result in approximately 10 % higher classification accuracies as compared to all other features .For both the neural network and MXL classifiers , accuracy with NLPCA features was very similar to that obtained with linear PCA .", "label": "", "metadata": {}, "score": "53.016098"}
{"text": "Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 6 - 97 ) contains supplementary material , which is available to authorized users .Background .Transcriptional profiling studies can produce data in the form of abundance measurements for genes in samples assigned to one of two classes .", "label": "", "metadata": {}, "score": "53.026997"}
{"text": "The TREC-2003 Genomics Track and the BioCreative I passage retrieval task propose an extension of IE tasks to identify entities , such as functional descriptors , which are less strictly defined than gene and gene products .The 2003 Genomics Track suggested extracting gene functions as defined in the LocusLink database ( now ENTREZ - Gene ) .", "label": "", "metadata": {}, "score": "53.05884"}
{"text": "View Article .Stalbovskaya V , Hamadicharef B , Ifeachor E : Sample Size Determination using ROC Analysis .3rd International Conference on Computational Intelligence in Medicine and Healthcare ( CIMED2007 ) : 2007 2007 .Beal SL : Sample Size Determination for Confidence Intervals on the Population Mean and on the Difference Between Two Population Means .", "label": "", "metadata": {}, "score": "53.106728"}
{"text": "Ruch P , Perret L , Savoy J : Features Combination for Extracting Gene Functions from MEDLINE .ECIR ( to appear ) 2005 .Kayaalp M , Aronson A , Humphrey S , Ide N , Tanabe L , Smith L , Demner D , Loane R , Mork J , Bodenreider O : Methods for Accurate Retrieval of MEDLINE Citations in Functional Genomics .", "label": "", "metadata": {}, "score": "53.113777"}
{"text": "Although we 've used 8-gram character language model classifiers , it 's easy to plug - and - play any of LingPipe 's other classifiers .Modifying the Existing Classes .Naive Bayes can be evaluated by importing the relevant classes and setting the classifier in the constructor : .", "label": "", "metadata": {}, "score": "53.11939"}
{"text": "When the sentence position is not taken into account , 80.65 % of PURPOSE sentences are correctly classified , while 16 % are mis - classified as CONCLUSION , and 3.23 % as RESULTS .On the other hand , when the sentence position is taken into account , 93.55 % of PURPOSE sentences are correctly classified .", "label": "", "metadata": {}, "score": "53.123016"}
{"text": "From the original set , we retained 12,000 abstracts ( an example is given in Table 2 ) used for training our LASt system , and 1,200 were used for fine - tuning and evaluating the tool , following removal of explicit argumentative markers .", "label": "", "metadata": {}, "score": "53.154984"}
{"text": "We also use the cross - learning ( CL ) evaluation strategy for identifying pairs that behave similarly across various evaluation methods .In the CV setting , we train and test each kernel on the same corpus using document - level 10-fold cross - validation .", "label": "", "metadata": {}, "score": "53.221703"}
{"text": "By averaging , the activity that is time locked to a known event ( e.g. , the onset of the attended stimulus ) is extracted as an ERP , whereas the activity that is not related to the stimulus onset is expected to be averaged out .", "label": "", "metadata": {}, "score": "53.240223"}
{"text": "Introduction .Research on brain - computer interfaces ( BCIs ) has witnessed a tremendous development in recent years [ 1 ] that has even been covered in the popular media .Several noninvasive BCI paradigms have been described in the literature , but the one we concentrate on relies on event - related potentials ( ERPs , a stereotyped electrophysiological response to an internal or external stimulus [ 2 ] ) .", "label": "", "metadata": {}, "score": "53.270027"}
{"text": "5 , pp . 1651 - 1686 , 1998 .View at Google Scholar \u00b7 View at Scopus .G. Ratsch , B. Scholkopf , S. Mika , and K.-R. Muller , \" SVM and boosting : one class , \" Tech .", "label": "", "metadata": {}, "score": "53.278984"}
{"text": "This sophisticated method computes noise tolerant hyperplanes using an analytic spherical model .However , 140 hours on a supercomputer cluster were required to identify at least 11 pairs of genes , each of which separates the data .The high dimensionality of transcriptional profiles and the logistical issues associated with exhaustive enumeration of all 1- ,", "label": "", "metadata": {}, "score": "53.280422"}
{"text": "( Numbers in brackets are 95 % exact confidence intervals )Training Set Test Set No .Documents Correctly Classified by System Total No .Documents Classified by System No .Page 4 . ment of the project 's UIMA components , model building , experimentation , and discussions .", "label": "", "metadata": {}, "score": "53.28323"}
{"text": "We highlighted with bold the number pairs in the intersection of CV and CL settings .We show the p - value of Fisher 's independence \u03c7 2 -test rounded to the closest factor of 10 .Bold typesetting indicates that the size of the overlap is too low .", "label": "", "metadata": {}, "score": "53.349426"}
{"text": "Similar experiments , with all identical conditions except using either phone level targets , or state level targets without \" do n't cares \" resulted in about 2 % lower accuracies .These results imply that the use of \" do n't cares \" is able to reduce errors introduced by inaccurate determination of state boundaries .", "label": "", "metadata": {}, "score": "53.352425"}
{"text": "View at Google Scholar .J. M. Leiva - Murillo and A. Art\u00e9s - Rodr\u00edguez , \" Maximization of mutual information for supervised linear feature extraction , \" IEEE Transactions on Neural Networks , vol .18 , no .5 , pp .", "label": "", "metadata": {}, "score": "53.370495"}
{"text": "Goldstein J , Kantrowitz M , Mittal V , Carbonell J : Summarizing Text Documents .Ehrler F , Geissb\u00fchler A , Jimeno A , Ruch P : Data - poor categorization and passage retrieval for gene ontology annotation in Swiss - Prot .", "label": "", "metadata": {}, "score": "53.44718"}
{"text": "Zhu J , Wang H , Hovy E , Ma M : Confidence - based stopping criteria for active learning for data annotation .ACM Transactions on Speech and Language Processing ( TSLP ) 2010 , 6 ( 3 ) : 1 - 24 .", "label": "", "metadata": {}, "score": "53.473133"}
{"text": "In addition , to strict functional annotation , as defined by the Gene Ontology along its three axes ( molecular functions , subcellular location and biological processes ) , GeneRiFs also may describe pathological functions , drug - related information and tissue - specificity .", "label": "", "metadata": {}, "score": "53.50283"}
{"text": "Combining them leads to a performance improvement of more than 2 percentage points in F - score over the best member 's performance ( see Table 17 ) .Furthermore , we expect that even a higher performance gain can be achieved by employing more sophisticated ensemble construction methods , such as bagging or stacking [ 40 , 41 ] .", "label": "", "metadata": {}, "score": "53.586655"}
{"text": "Input and output plot of the 3-D Gaussian before ( left ) and after ( right ) using neural network for NLPCA .Nonlinear Discriminant Analysis ( NLDA ) .Fortunately , only a minor modification to NLPCA is needed to form NLDA .", "label": "", "metadata": {}, "score": "53.612755"}
{"text": "Let .X .[ .x .x .x .n . ]T be an n -dimensional ( column ) feature vector , and .Y .[ .y .y .y . m . ]T be an m -dimensional ( column ) feature vector , obtained as the linear transform of X , using the n by m transformation matrix A , i.e. .", "label": "", "metadata": {}, "score": "53.649887"}
{"text": "Neural networks .In optimizing the design of a neural network , an important consideration is the number of hidden layers and an appropriate number of hidden nodes in each layer .A neural network with no hidden layers can form only simple decision regions , which is not suitable for highly nonlinear and complex speech features .", "label": "", "metadata": {}, "score": "53.655537"}
{"text": "Parsing features were generated from both syntax and dependency parses .Scope of features are typically sentence ( s ) , before entities ( b ) , between entities ( w ) , after entities ( a ) .Table 16 .", "label": "", "metadata": {}, "score": "53.661476"}
{"text": "For such tasks , advances retrieval engines are sufficient to achieve top - precision in the range of 80 % or higher .Thus , mean reciprocal rank measures reaching 87.18 % for retrieving passages supporting protein - protein interactions has been reported for BioCreative II [ 3 ] !", "label": "", "metadata": {}, "score": "53.715797"}
{"text": "As in ( 4 ) , but the parameters of a distribution are changed slightly each iteration , simulating target movement .This experiment estimates the ability of the algorithms to adapt to gradual changes in the data distribution .( 5 ) Bayes - separable data with switching distribution ( Switching ) .", "label": "", "metadata": {}, "score": "53.76416"}
{"text": "These are the kinds of reports produced for information retrieval tasks as used , for example , in the Text Retrieval Conference ( TREC ) . ...These include cumulative statistics from the interpolated and uninterpolated precision - recall ( PR ) and receiver operating characteristic ( ROC ) curves determined as described in the scored precision - recall evaluation documentation .", "label": "", "metadata": {}, "score": "53.849915"}
{"text": "During typing , the EEG data was stored for further ( offline ) analysis based on a smaller amount . of trials ( in this case we used all .-combination of 15 trials for each typed character , for assessing the accuracy ) .", "label": "", "metadata": {}, "score": "53.89258"}
{"text": "Figures 1 and 2 show the distribution of PPI pairs in terms of success level for CV and CL evaluation aggregated across the 5 corpora , respectively .We also show the same statistics for each corpus separately ( Tables 1 and 2 ) .", "label": "", "metadata": {}, "score": "53.897854"}
{"text": "The classes would be well separated by a projection onto the first LDA basis vector , but poorly separated by a projection onto the first PCA basis vector .Heteroscedastic Discriminant Analysis ( HDA ) .Another linear transformation technique , related to linear discriminant analysis , but which accounts for the ( very common ) case where within class covariance matrices are not the same for all classes , is called Heterocscedastic Discriminant Analysis ( HDA ) ( Saon et al . , 2000 ) .", "label": "", "metadata": {}, "score": "53.907455"}
{"text": "Central to the learning and the classification phases is a so - called kernel function .Simply speaking , a kernel function is a function that takes the representation of two instances ( here , protein pairs ) and computes their similarity .", "label": "", "metadata": {}, "score": "53.922176"}
{"text": "24 , no . 2 , pp .123 - 140 , 1996 .Y. Freund and R. E. Schapire , \" Decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "53.93789"}
{"text": "Comput Intell 2011 , 27 ( 4 ) : 645 - 664 .View Article .Buyko E , Faessler E , Wermter J , Hahn U : Syntactic simplification and semantic enrichment - trimming dependency graphs for event extraction .Comput Intell 2011 , 27 ( 4 ) : 610 - 644 .", "label": "", "metadata": {}, "score": "53.95542"}
{"text": "The three classifiers are combined through a three - way majority vote .The pseudocode and implementation detail of boosting is shown in Figure 5 . \\ )Also , the ensemble error is a training error bound .Hence , a stronger classifier is generated from three weaker classifiers .", "label": "", "metadata": {}, "score": "54.014126"}
{"text": "Stepwise linear discriminant analysis ( SWLDA ) has been used in patient studies of the P300 BCI speller [ 4 , 5 ] .It can be considered as an extension of the LDA with an incorporated filter feature selection .SWLDA adds and removes terms from a linear discriminant model , based on their statistical significance in regression , thus , producing model that is adjustable to the training data .", "label": "", "metadata": {}, "score": "54.020874"}
{"text": "The features which are the direct outputs of the network without PCA processing were also evaluated .Figure 17 shows accuracies using 1-state and 3-state HMMs with a varying number of mixtures per state .NLDA2 performed better than NLDA1 for all conditions -- approximately 2 % higher accuracy .", "label": "", "metadata": {}, "score": "54.07244"}
{"text": "After sentence shortening , the system achieves a Dice score of 52.78 % ( ranked 3 out of 15 participants in the official TREC evaluation ) .Our second extraction scheme ( run labelled GOEx ) performed at near similar levels ( 52.28 % ) .", "label": "", "metadata": {}, "score": "54.10255"}
{"text": "Lee and Pang were generous enough to pre - slice the files into ten equally - sized slices which are distinguished by the third character of the file name .For instance , the file pos / cv362_15341.txt is a positive training instance in block 3 , whereas pos / cv532_6522.txt is a positive training instance in block 5 .", "label": "", "metadata": {}, "score": "54.105186"}
{"text": "s .i . g .n . :Boosting coefficients , used for combining several weak classifiers into a stronger one .With the bias term , the number of elements in .:A combined strong classifier .Using this notation , the algorithm is initialized with the following data : .", "label": "", "metadata": {}, "score": "54.13354"}
{"text": "The thousands of pairs found on small data sets are mostly due to the small sample size of the data , and would likely not maintain their perfect classification upon addition of further patient samples .Generalization performance was estimated with a LOO methodology ( see Methods ) and is often reasonably good .", "label": "", "metadata": {}, "score": "54.200176"}
{"text": "BMC bioinformatics 2009 , 10 ( 1 ) : 147 .PubMed View Article .Kalayeh HM , Landgrebe DA : Predicting the Required Number of Training Samples .Pattern Analysis and Machine Intelligence , IEEE Transactions on 1983 , 5 ( 6 ) : 664 - 667 .", "label": "", "metadata": {}, "score": "54.258186"}
{"text": "These so - called Gene Reference Into Functions ( GeneRiFs ) are usually short extracts taken from MEDLINE articles .Generalizing the use of textual passages for the three axes of the Gene Ontology ( GO ) , i.e. not only to identify functions but also to identify biological processes and cellular components , the BioCreative task 2.1 aimed at evaluating text mining tools tailored to extract short passages .", "label": "", "metadata": {}, "score": "54.289333"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , Vol .1857 , pp . 1 - 15 , 2000 , Springer - Verlag .L. K. Hansen and P. Salamon , \" Neural network ensembles , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "54.30394"}
{"text": "Stemming is applied to get a more general feature representation space .In parallel , for each sentence in the abstract , a vector is constructed .The title of the abstracts is normalized as other sentences to provide a candidate vector .", "label": "", "metadata": {}, "score": "54.309"}
{"text": "Relatively few papers have been published on the termination criteria for active learning [ 7 - 9 ] .The published criteria are generally based on target accuracy , classifier confidence , uncertainty estimation , and minimum expected error .As such , they do not directly predict a sample size .", "label": "", "metadata": {}, "score": "54.312492"}
{"text": "The first experiment was conducted to evaluate the two NLDA versions with various dimensions in the reduced feature space with and without the use of PCA .As input features , 13 DCTCs , computed with 8 ms frames and 2 ms spacing , were represented with 6 DCSCs over a 500 ms block , for a total of 78 features ( 13 DCTCs x 6 DCSCs ) .", "label": "", "metadata": {}, "score": "54.333733"}
{"text": "As usual , our main method constructs an instance using the command - line arguments , then runs it .If any errors are thrown , it prints their stack traces .Constructor to Marshal Arguments .Also following our standard operating procedure ( SOP ) , the constructor sets up the member variables using the command - line arguments : .", "label": "", "metadata": {}, "score": "54.357418"}
{"text": "Boosting was the predecessor of the AdaBoost family of algorithms - which arguably became one of the most popular machine learning algorithms in recent times .Since these seminal works , research in ensemble systems have expanded rapidly , appearing often in the literature under many creative names and ideas .", "label": "", "metadata": {}, "score": "54.46436"}
{"text": "The aim of the second experiment is a more thorough evaluation of NLDA1 and NLDA2 using a varying number of states and mixtures in HMMs .The 78 DCTC / DCSCs ( computed as mentioned in previous section ) were reduced to 36 dimensions based on the results of the previous experiment .", "label": "", "metadata": {}, "score": "54.465935"}
{"text": "But this is hard to achieve in a row / column paradigm , since in a free spelling mode we do not know a priori the character that the subject wants to communicate .The design of the proper stimulation paradigm as in , for example , [ 31 ] is the subject of further research .", "label": "", "metadata": {}, "score": "54.505276"}
{"text": "Experiment 1 .In the first experiment , all training data were used to train the transformations including LDA , PCA , NLPCA , and NLDA2 , and the classifiers .Figure 11 shows the results based on the neural network and MXL classifiers for each transformation method in terms of classification accuracy , as the number of features varies from 1 to 39 .", "label": "", "metadata": {}, "score": "54.535896"}
{"text": "Research , vol .2 , pp .263 - 286 , 1995 .T. K. Ho , \" Random subspace method for constructing decision forests , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .20 , no . 8 , pp .", "label": "", "metadata": {}, "score": "54.54151"}
{"text": "We found that predicting the difficult ( D ) class is particularly hard , with a recall of 20.8 and an F - score of 28.2 , indicating that difficult pairs share very few characteristics .Table 12 .Classification of difficulty classes based on pair surface features by decision tree .", "label": "", "metadata": {}, "score": "54.550568"}
{"text": "This code raises the possibility of either an IOException from the I / O or a ClassNotFoundException from reading in the actual classifier from the object intput stream .Training .The next step is to train the polarity classifier .It does this in exactly the same way as in the basic polarity demo using the method train ( ) .", "label": "", "metadata": {}, "score": "54.56698"}
{"text": "S. Shalev - Shwartz , Y. Singer , and N. Srebro , \" Pegasos : primal estimated sub - GrAdient sOlver for SVM , \" in Proceedings of the 24th ACM International Conference on Machine Learning ( ICML ' 07 ) , pp .", "label": "", "metadata": {}, "score": "54.567978"}
{"text": "405 - 410 , 1997 .S. B. Cho and J. H. Kim , \" Combining multiple neural networks by fuzzy integral for robust classification , \" IEEE Transactions on Systems , Man and Cybernetics , vol .25 , no . 2 , pp .", "label": "", "metadata": {}, "score": "54.58987"}
{"text": "Uncompiled naive Bayes takes almost ten minutes to complete the classification demo .The act of compiling them to a file actually precomputes almost all of the probabilities ; reading them back in computes the suffix tree backoffs .The compiled classifiers should work in exactly the same way as the classifier from which they were compiled .", "label": "", "metadata": {}, "score": "54.686485"}
{"text": "View Article .Moler E , Chow M , Mian I : Analysis of molecular profile data using generative and discriminative methods .Physiological Genomics 2000 , 4 : 109 - 126 .PubMed .Proc Natl Acad Sci 2001 , 98 : 15149 - 15154 .", "label": "", "metadata": {}, "score": "54.816116"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the margin : a new explanation for the effectiveness of voting methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "54.84388"}
{"text": "First , we proposed a method for identifying different difficulty classes of protein pairs independently from evaluation setting .Protein interactions are expressed at the linguistic level in diverse ways ; its complexity influences the performance of automated methods to classify the pairs correctly .", "label": "", "metadata": {}, "score": "54.859104"}
{"text": "Tbahriti I , Chichester C , Lisacek F , Ruch P : Using argumentation to retrieve articles with similar citations : an inquiry into improving related articles search in the MEDLINE digital library .Int J Med Inf 2006 , 75 ( 6 ) : 488 - 495 .", "label": "", "metadata": {}, "score": "54.86779"}
{"text": "Both the weak classifiers and selectors were updated each iteration .The limited number of selectors and features allowed for simplified online algorithm .Their algorithm , however , had two potential problems , one being that a limited number of selectors limited flexibility of the classifier , the second being that the effect constant updates have on the error rate of the weak classifiers was never addressed .", "label": "", "metadata": {}, "score": "54.91047"}
{"text": "497 - 501 , 1995 .L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" Fuzzy Sets and Systems , L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" vol .", "label": "", "metadata": {}, "score": "54.921223"}
{"text": "A Tutorial , Institute for Signal and Information Processing , Department of Electrical and Computer Engineering , Mississippi State University .Abstract .Background .Supervised learning methods need annotated data in order to generate efficient models .Annotated data , however , is a relatively scarce resource and can be expensive to obtain .", "label": "", "metadata": {}, "score": "54.928825"}
{"text": "Algebraic combiners .Algebraic combiners are non - trainable combiners , where continuous valued outputs of classifiers are combined through an algebraic expression , such as minimum , maximum , sum , mean , product , median , etc .Specifically .", "label": "", "metadata": {}, "score": "54.943485"}
{"text": "The classification goal is to perform binary classification to discriminate the first class of waves from the other two .Each dataset was randomly split into a training set and a testing set .Test sets for D1 and D2 contained 1,000 instances each while 2,500 instances were set apart as test set in D3 .", "label": "", "metadata": {}, "score": "54.94462"}
{"text": "Among the difficult cases it can be observed that incorrectly classified indirect PPIs among the difficult cases ( e.g. B.d14.s1.p2 , A.d78.s669.p2 ) tend to be regulatory relationships .As other types of PPIs may be less affected by this issue , the move from generic PPIs to more specific relations should allow for a higher performance for those PPI subtypes .", "label": "", "metadata": {}, "score": "54.968666"}
{"text": "5 , pp . 1651 - 1686 , 1998 .Y. S. Huang and C. Y. Suen , \" Behavior - knowledge space method for combination of mul - tiple classifiers , \" Proc . of IEEE Computer Vision and Pattern Recog .", "label": "", "metadata": {}, "score": "54.986687"}
{"text": "PubMed View Article .Cai J , Zeng D : Sample size / power calculation for case - cohort studies .Biometrics 2004 , 60 ( 4 ) : 1015 - 1024 .PubMed View Article .Algina J , Moulder BC , Moser BK : Sample Size Requirements for Accurate Estimation of Squared Semi - Partial Correlation Coefficients .", "label": "", "metadata": {}, "score": "54.99175"}
{"text": "Difficulty classes of PPs were defined based on the success level of kernels in classifying them .We showed that difficulty classes correlate with certain surface features of the pair / the sentence containing the pair , especially word distance , shortest path length between the two proteins in the dependency graph and in the syntax tree .", "label": "", "metadata": {}, "score": "55.01763"}
{"text": "For the majority of curves , the RMSE fell below 0.01 , within a relative small sample size of 200 used for curve fitting .We observed minimal differences between values of RMSE and MAE which indicates a low variance of the errors .", "label": "", "metadata": {}, "score": "55.04014"}
{"text": "The experimental protocol was approved by the ethical committee .The information about the patients , of which the recordings were further considered , that is , their diagnosis , age , and gender , is presented in Table 1 .We have used the same visual stimulus paradigm as in the first P300-based speller , introduced by Farwell and Donchin in [ 3 ] : a matrix of .", "label": "", "metadata": {}, "score": "55.042503"}
{"text": "However , the F - scores turned out similar .For the formal evaluation and the final i2b2 submission , our models were built from Set 1 and Set 2 and were run on the official I2B2 test set ( Set 3 ) .", "label": "", "metadata": {}, "score": "55.067703"}
{"text": "Compared to the PCA and LDA reduced features , the NLDA1 and NLDA2 features performed considerably better for both the 1-state and 3-state HMMs .For the case of 3-state HMMs , the transformed features reduced to 24 dimensions resulted in the highest accuracy of 69.3 % for NLDA1 .", "label": "", "metadata": {}, "score": "55.06784"}
{"text": "In Proceedings of the Seventh IEEE International Conference on Data Mining Workshops .IEEE Computer Society ; 2007:219 - 224 .View Article .Provost F , Jensen D , Oates T : Efficient progressive sampling .In Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining .", "label": "", "metadata": {}, "score": "55.09066"}
{"text": "Diversity of classifiers in bagging is obtained by using bootstrapped replicas of the training data .That is , different training data subsets are randomly drawn - with replacement - from the entire training dataset .Each training data subset is used to train a different classifier of the same type .", "label": "", "metadata": {}, "score": "55.118317"}
{"text": "As most learning algorithms of that time , AdaBoost was designed for offline ( batch ) training , with the error rate estimated over all available samples .There are , therefore , several difficulties involved in employing boosting as an online training algorithm , several of which were mentioned in [ 1 ] .", "label": "", "metadata": {}, "score": "55.19304"}
{"text": "In both cases the number of patient samples and class sizes is small and 100,000 + pairs are found .We suspect there are some biologically real pairs hidden in the large background noise due to small sample sizes .Random data tests ( see Methods ) indicate that 30 + samples with more even class sizes are needed in order to reduce the random chance noise to a very small level ( Table 4 ) .", "label": "", "metadata": {}, "score": "55.207024"}
{"text": "However , values for parameter c are expected to be negative within the range [ -1,0 ] ; values for a are expected to be much smaller than 1 .The values of Y acc fall between 0 and 1 .Y acc grows asymptotically to the maximum achievable performance , in this case ( 1-a ) .", "label": "", "metadata": {}, "score": "55.217976"}
{"text": "Bhattacharyya C , Grate L , Jordan M , Ghaoui L , Mian I : Robust sparse hyperplane classifiers : application to uncertain molecular profiling data .Journal of Computational Biology 2004 , 11 ( 6 ) : 1073 - 1089 .", "label": "", "metadata": {}, "score": "55.34099"}
{"text": "Class distribution in 1000 GeneRiFs after argumentative classification .Sets A and B are samples of GeneRiFs as in LocusLink , but Set B contains only GeneRiFs originating from the abstract .Based on these findings , the preferred sentence ranking order for GeneRiF extraction should be : CONCLUSION , PURPOSE , RESULTS , METHODS .", "label": "", "metadata": {}, "score": "55.354736"}
{"text": "L. I. Kuncheva , \" A theoretical study on six classifier fusion strategies , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .24 , no . 2 , pp .281 - 286 , 2002 .S. B. Cho and J. H. Kim , \" Multiple network fusion using fuzzy logic , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "55.36355"}
{"text": "The classification results are shown in Figure 7 , and the averaged ERP waveforms in Figure 8 , for electrode POz .The results suggest that the early ERP components should , for some of our patients , also be considered as features for decoding .", "label": "", "metadata": {}, "score": "55.434387"}
{"text": "7 , no . 3 , pp .788 - 792 , 1996 .G. Giacinto and F. Roli , \" Approach to the automatic design of multiple classifier systems , \" Pattern Recognition Letters , vol .22 , no . 1 , pp .", "label": "", "metadata": {}, "score": "55.485523"}
{"text": "Boosting algorithms that employ linear combination of weak classifiers to form confidence function for strong classifier were shown to be closely related to the primal formulation for support vector machines [ 10 ] .As in case of SVM , many boosting methods were designed for offline setting , where all of the training examples are given a priori , and share the same set of problems when dealing with larger datasets .", "label": "", "metadata": {}, "score": "55.50136"}
{"text": "The distribution of pairs according to classification success level using cross - validation setting .The distribution of pairs ( total , positive and negative ) in terms of the number of kernels that classify them correctly ( success level ) aggregated across the 5 corpora in cross - validation setting .", "label": "", "metadata": {}, "score": "55.507893"}
{"text": "471 - 485 , 2009 .View at Publisher \u00b7 View at Google Scholar .G. Townsend , B. K. LaPallo , C. B. Boulay et al . , \" A novel P300-based brain - computer interface stimulus presentation paradigm : moving beyond rows and columns , \" Clinical Neurophysiology , vol .", "label": "", "metadata": {}, "score": "55.644753"}
{"text": "For example , the NLDA2 features modeled by 3-state HMMs with 3 mixtures resulted in an accuracy of 69.4 % versus 63.2 % for the original features .Figure 16 .Figure 17 .Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures .", "label": "", "metadata": {}, "score": "55.657394"}
{"text": "A rough time estimate is 1 second per million pairs per sample .So a 2000 gene , 30 sample set would need about seconds .As expected , small data sets are found to have many thousands of gene pairs while large data sets have few .", "label": "", "metadata": {}, "score": "55.675568"}
{"text": "The combination of the classifiers is then based on the given instance : the classifier trained with data closest to the vicinity of the instance , according to some distance metric , is given the highest credit .One or more local experts can be nominated to make the decision ( Jacobs 1991 , Woods 1997 , Alpaydin 1996 , Giacinto 2001 ) .", "label": "", "metadata": {}, "score": "55.70071"}
{"text": "These classes were chosen because in scientific literature they have been found to be fairly stable [ 26 ] [ 27 ] and they are also recommended by ANSI / ISO guidelines for professionals .We obtained 19,555 explicitly structured abstracts from MEDLINE in order to train our Latent Argumentative Structuring learner - this set does not contain the MEDLINE records used during the evaluation .", "label": "", "metadata": {}, "score": "55.73239"}
{"text": "Automatic document classification of biological literature .BMC Bioinformatics 2006;7 : 370 .Brank J , Grobelnik M , Milic - Frayling N , Mladenic D. Feature selection using linear support vector machines .Microsoft Re- search Technical report MSR - TR-2002 - 63 .", "label": "", "metadata": {}, "score": "55.736282"}
{"text": "Bhalotia G , Nakov P , Schwartz A , Hearst M : BioText Team Report for the TREC 2003 Genomics Track .TREC-2003 2004 , 612 - 621 .Mitchell T : Machine Learning McGraw Hill 1997 .Jelier R , Schuemie M , van der Eijk C , Weeber M , van Mulligen E , Schijvenaars B , Mons B , Kors J : Searching for GeneRIFs : Concept - Based Query Expansion and Bayes Classification .", "label": "", "metadata": {}, "score": "55.7862"}
{"text": "This also allowed us to construct a larger amount of test data for .This was done by taking combinations of .elements from the available 15 responses for each row and column .The performance results are shown in Figure 3 for each individual patient , and the averaged performance result in Figure 4 , averaged over all subjects .", "label": "", "metadata": {}, "score": "55.81276"}
{"text": "A simple baseline is to assign the most frequent category to each report ( Unknown ) .The best F - score from our informal evaluation is 92.64 .The F - score baseline for this set is 63.31 .We limited our runs on Set 2 to three to avoid overtraining / overfitting on the test data .", "label": "", "metadata": {}, "score": "55.81968"}
{"text": "Aggregated results are shown in Figure 1 .All the 13 kernels are taken into consideration .Table 2 .The distribution of pairs for each corpus according to classification success level using cross - learning setting .The distribution of pairs ( total , positive and negative ) in terms of the number of kernels that classify them correctly .", "label": "", "metadata": {}, "score": "55.839134"}
{"text": "Classifiers and parameters .Typically , kernel functions are integrated into SVM implementations .Several freely available and extensible implementations of SVMs exist , among which SVM l i g h t [ 24 ] and LibSVM [ 25 ] probably are the most renowned ones .", "label": "", "metadata": {}, "score": "55.95894"}
{"text": "Mizuta Y , Collier N : Zone Identification in Biology Articles as a Basis for Information Extraction .COLING Workshop on Natural Language Processing in Biomedicine and its Applications ( NLPBA / BioNLP ) 2004 .Lisacek F , Chichester C , Kaplan A , Sandor A : Discovering Paradigm Shift Patterns in Biomedical Abstracts : Application to Neurodegenerative Diseases .", "label": "", "metadata": {}, "score": "55.97054"}
{"text": "4 , pp .406 - 415 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .N. V. Manyakov , R. Vogels , and M. M. Van Hulle , \" Decoding stimulus - reward pairing from local field potentials recorded from monkey visual cortex , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "55.97327"}
{"text": "There there are 1000 test cases and a 92.1 % accuracy , leading to : .So for those results , our 95 % confidence interval is the narrower ( 90.7 , 93.5 ) .Pang and Lee used paired t - tests over cross - validated slices of data for significance , but we ca n't use that tighter technique to compare our results to theirs because we do n't have access to their results for the requisite pairing .", "label": "", "metadata": {}, "score": "55.974594"}
{"text": "Recall that the process models are normalized for a given input length and do not model boundaries of strings differently than other positions .Training .The run method simply calls training then evaluation : .We consider training in this section and evaluation in the next .", "label": "", "metadata": {}, "score": "55.998245"}
{"text": "10 , pp .993 - 1001 , 1990 .R. E. Schapire , \" The Strength of Weak Learnability , \" Machine Learning , vol .5 , no . 2 , pp .197 - 227 , 1990 .R. A. Jacobs , M. I. Jordan , S. J. Nowlan , and G. E. Hinton , \" Adaptive mixtures of local ex - perts , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "56.01497"}
{"text": "Comparison of some non - kernel based and kernel based classifiers in terms of F - score ( CV evaluation ) .The first 9 are non - kernel based classifiers , the last four are kernel based classifiers .Conclusions .", "label": "", "metadata": {}, "score": "56.035885"}
{"text": "A super computer or dedicated computer cluster is not required .It seems possible that the occurrence of such small sized classifiers is a common characteristic of microarray data , thus making the effort of searching for small gene sets worth the computational cost .", "label": "", "metadata": {}, "score": "56.05773"}
{"text": "Stepping through the Code .The code for the subjectivity classifier in src / SubjectivityBasic .java is almost identical to that of the polarity classifier , so we only focus on a few differences in this section .Splitting the Training File .", "label": "", "metadata": {}, "score": "56.05938"}
{"text": "Nonlinear Principal Components Analysis ( NLPCA ) .Since the final NN outputs are created from the internal NN representations at the bottleneck layer , the bottleneck outputs can be viewed as the reduced dimensionality version of the data .This idea was tested using pseudo - random data generated so as to cluster on curved subspaces .", "label": "", "metadata": {}, "score": "56.06733"}
{"text": "Figure 8 shows a particular code matrix for a 5-class problem that uses 15 encodings .This encoding , suggested in ( Dietterich 1995 ) , is a ( pseudo ) exhaustive coding because it includes all possible non - trivial and non - repeating codes .", "label": "", "metadata": {}, "score": "56.075428"}
{"text": "Classification classification .evaluator.addClassification(category , .System.out.println ( ) ; .As in previous examples , the code that remains the same is greyed out .The new code creates a classifier evaluator with a null classifier .This is because we do n't actually have an implementation of the BaseClassifier interface ( see the next section for an illustration of how to create one ) .", "label": "", "metadata": {}, "score": "56.11108"}
{"text": "r .g . m .i .n .In the linear case , function .can be represented as . , where . denote an inner product , and the equation becomes . a .r .g . m .", "label": "", "metadata": {}, "score": "56.127796"}
{"text": "AdaBoost [ 8 ] is an algorithm that iteratively adds weighted weak classifiers .Each additional classifier is selected from the pool of available classifiers to minimize the weighted error rate over training samples .This error rate was also used to calculate the weight of .", "label": "", "metadata": {}, "score": "56.18067"}
{"text": "In one of these methods , referred to as nonlinear PCA ( NLPCA ) , the goal of the nonlinear transformation is to minimize the mean square error between features estimated from reduced dimensionality features and original features .Thus this method is patterned after PCA .", "label": "", "metadata": {}, "score": "56.2432"}
{"text": "Published transcriptional profiling data sets reexamined in this study .For each set , the Table gives the abbreviation ; dimensionality of the data points investigated after pre - processing of the features assayed in the original study ; number , description and abbreviation for samples assigned to a category .", "label": "", "metadata": {}, "score": "56.252483"}
{"text": "From this data set , 55 were mainly extracted from the article 's title , as depicted in Figure 1 .The second most frequent source of GeneRiF was the abstract 's last sentence ( see the last column in Figure 1 , following the label \" n \" ) , showing the source of 36 GeneRiFs .", "label": "", "metadata": {}, "score": "56.266136"}
{"text": "347 - 352 , 1993 .L. Xu , A. Krzyzak , and C.Y. Suen , Methods for combining multiple classifiers and their applications to handwriting recognition , IEEE Trans . on Systems , Man , and Cyb . , Vol .", "label": "", "metadata": {}, "score": "56.32042"}
{"text": "The weighted majority voting then chooses the class \\(\\omega\\ ) receiving the highest total vote from all classifiers .This result may appear to go against the conventional wisdom ( see Occam 's razor ) indicating that adding too many classifiers - beyond a certain limit - would eventually lead to overfitting of the data .", "label": "", "metadata": {}, "score": "56.322952"}
{"text": "When these stop phrases occurred they are removed from the beginning of the selected GeneRiF candidate .Part - of - speech information is used to augment the list of stopwords , thus any adverb ( e.g. finally , surprisingly , etc . ) located at the beginning of a sentence are removed .", "label": "", "metadata": {}, "score": "56.389526"}
{"text": "This can be attributed to the low probability of the positive class in edit 's prediction , which is also manifested in the below average performance on positive pairs ( PD&PE ) , and the very good results on NE pairs .", "label": "", "metadata": {}, "score": "56.40233"}
{"text": "Examples of this approach include the algorithms proposed by Mukherjee and others [ 1 , 28 - 30 ] .Since our proposed method is a variant of this approach , we will describe the prior work on learning curve fitting in more detail .", "label": "", "metadata": {}, "score": "56.409885"}
{"text": "Evaluation .Datasets .We evaluated our algorithm using three sets of data .In the first two sets ( D1 and D2 ) , observations are smoking - related sentences from a set of patient discharge summaries from the Partners Health Care 's research patient data repository ( RPDR ) .", "label": "", "metadata": {}, "score": "56.46325"}
{"text": "As expected this basis vector , represented by a straight line , is oriented along the axis with maximum data variation .Figure 1 .Scatter plot of 2-D multivariate Gaussian data and first principal components basis vector .Line is a good fit to data .", "label": "", "metadata": {}, "score": "56.47956"}
{"text": "Overall , we showed that 1 - 2 % of PPI instances are misclassified by all the 13 kernels we considered , independent of which evaluation setting ( and hence which training set ) was used .Vastly more , 19 - 30 % of PPI instances are misclassified by the majority of these kernels .", "label": "", "metadata": {}, "score": "56.505066"}
{"text": "The database used for all experiments reported in the remainder of this chapter is TIMIT .The TIMIT database was developed in the early 1980 's for expediting acoustic - phonetic ASR research ( Garofolo et al . , 1993 ; Zue et al , 1990 ) .", "label": "", "metadata": {}, "score": "56.56263"}
{"text": "Recall that only a mere 0.3 % of GT values were changed , most of them in BioInfer ( 36 ) and AIMed ( 12 ) corpora .We analyzed the performance change both using the original and the re - trained model on the re - annotated corpora ( see Table 14 ) .", "label": "", "metadata": {}, "score": "56.568687"}
{"text": "241 - 259 , 1992 .T. K. Ho , J. J. Hull , and S. N. Srihari , \" Decision combination in multiple classifier systems , \" IEEE Trans . on Pattern Analy .Machine Intel . , vol .16 , no . 1 , pp .", "label": "", "metadata": {}, "score": "56.604065"}
{"text": "To demonstrate the biological utility of the strategy , the gene pairs and triples discovered in the aforementioned LiverCancer data set were examined and found to yield new and unanticipated scientific insights .Overall , the results indicate the importance of ascertaining , as a matter of routine , the presence ( or absence ) of small distinguishing feature subsets .", "label": "", "metadata": {}, "score": "56.608772"}
{"text": "Words that appeared only once in the subcorpus were removed from the set of features .Features were chosen only from the smoking - related subcorpus to reduce those unre- lated to smoking .This assumes that sentences labeled un- known did not share useful features for classification , but simply lack the features that describe sentences labeled smoking - related .", "label": "", "metadata": {}, "score": "56.671562"}
{"text": "On the test set , this threshold results in selecting 14 sentences from the abstract and 125 from the title , out of a total of 139 queries ( see Table 6 ) .Gene Ontology - driven passage selection .The second module capitalizes on the automatic assignment of GO categories to each candidate sentence .", "label": "", "metadata": {}, "score": "56.69128"}
{"text": "Each gene alone provides some classification power , but when linearly combined form a perfect classifier , albeit with a small margin .Plus signs are the T subtype samples .E2A vs the rest .The 4 single probe sets for E2A are .", "label": "", "metadata": {}, "score": "56.72558"}
{"text": "For these online test sessions , we considered a linear SVM classifier trained on data averaged over 15 trials .Thus , each subject attempted to type characters based on 15 row / column intensifications .About 36 characters were typed by each subject .", "label": "", "metadata": {}, "score": "56.929203"}
{"text": "An average time estimate is roughly 1 second per 10 6 evaluations per sample .However , even gene sets found in small data sets can be interesting depending on the end use .For use in a medical diagnostic it is desirable that the gene set be highly accurate on large number of patient samples so the test result error rate is low .", "label": "", "metadata": {}, "score": "56.933006"}
{"text": "However , ASR accuracies obtained with a combination of LDA and MLLT were nearly as good as those obtained with HDA and MLLT .A detailed summary of HDA and MLLT are beyond the scope of this chapter ; however , these methods , either by themselves , or in conjunction with the nonlinear methods described in this chapter warrant further investigation .", "label": "", "metadata": {}, "score": "56.96225"}
{"text": "How many such pairs occur by chance ?Theory suggests [ 18 ] that the probability of a 2 class data set of N items with a in one class , being linearly separable in 1 dimension is and in 2 dimensions is approximately .", "label": "", "metadata": {}, "score": "56.995964"}
{"text": "There are 42 cases that were labeled as plots in the gold standard , but were misclassified as quotes by the classifier .On the next row , there are 463 cases that were labeled quotes in the gold standard that were correctly classified as quotes by our classifier .", "label": "", "metadata": {}, "score": "57.01799"}
{"text": "In a nutshell , they work as follows .First , they require a training corpus consisting of labeled sentences , some of which contain PPIs and/or non - interacting proteins , while others contain only one or no protein mentions .", "label": "", "metadata": {}, "score": "57.02733"}
{"text": "As shown in Figure 18 , both NLDA1 and NLDA2 using the expanded targets lead to a significant increase in accuracy .The NLDA2 accuracies are typically about 2 % higher than NLDA1 accuracies .The use of forced alignment for state boundaries resulted in the highest accuracy of 75.0 % with 64 mixtures .", "label": "", "metadata": {}, "score": "57.158985"}
{"text": "After training the classifier , each subject performed several online test sessions during which ( s)he was asked to mind - type a few words .In the case of a mistyping , ( s)he was instructed to type further without trying to correct the mistake ( \" backspace \" was not allowed ) .", "label": "", "metadata": {}, "score": "57.169014"}
{"text": "High - level architecture for the sentence classi- 26 SAVOVA et al . , Mayo Clinic NLP System .Page 3 .Data Sets and Evaluation The I2B2 challenge organizers released three data sets ( Table 1).1Set 1 and Set 2 were made available three months before the formal competitive evaluation .", "label": "", "metadata": {}, "score": "57.170616"}
{"text": "View at Publisher \u00b7 View at Google Scholar . A. Hyv\u00e4rinen , \" New approximations of differential entropy for independent component analysis and projection pursuit , \" in Proceedings of the Conference on Advances in Neural Information Processing Systems , pp .", "label": "", "metadata": {}, "score": "57.179585"}
{"text": "The results are easily visualized for single and pairs of genes by a scatter plot .For each sample the expression values for the gene(s ) set the x - y position and the point is labeled with the sample class as shown in the example plot Figure 1 .", "label": "", "metadata": {}, "score": "57.188248"}
{"text": "The first classifier is called LASt for Latent Argumentative Structuring , so called because we assume that it can reveal the underlying / latent / hidden rhetorical structure of the article .The tool started ranking abstract sentences as to their argumentative classes .", "label": "", "metadata": {}, "score": "57.19037"}
{"text": "In this section , we step through the source found in the file src / PolarityBasic . java .The program reads the training directory location from the command line , trains a classifier on the training data , then evaluates the classifier on the test data .", "label": "", "metadata": {}, "score": "57.252693"}
{"text": "A particular limitation of boosting is that it applies only to binary classification problems .This limitation is removed with the AdaBoost algorithm .AdaBoost .Arguably the best known of all ensemble - based algorithms , AdaBoost ( Adaptive Boosting ) extends boosting to multi - class and regression problems ( Freund 2001 ) .", "label": "", "metadata": {}, "score": "57.27868"}
{"text": "We found that the best surface - based classifier , BayesNet , is on par with or better than all kernel based classifiers except APG , SL and kBSPS ( see Figure 9 ) .On larger corpora , BayesNet attains 43.4 F - score on AIMed and 54.6 on BioInfer which is outperformed only by the above three kernels .", "label": "", "metadata": {}, "score": "57.297592"}
{"text": "Example of an explicitly structured abstract in MEDLINE .The 4-class argumentation model , supporting our experiments can have minor variations in abstracts as illustrated with the INTRODUCTION marker in this explicitly structured abstracts .Explicitely structured abstracts in MEDLINE account for less than 2 % of all abstracts .", "label": "", "metadata": {}, "score": "57.305473"}
{"text": "As control we used an un - weighted fitting method .Results .A total of 568 models were fitted and the model predictions were compared with the observed performances .Depending on the data set and sampling method , it took between 80 to 560 annotated samples to achieve mean average and root mean squared error below 0.01 .", "label": "", "metadata": {}, "score": "57.343483"}
{"text": "2627 - 2644 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. Gysels and P. Celka , \" Phase synchronization for the recognition of mental tasks in a brain - computer interface , \" IEEE Transactions on Neural Systems and Rehabilitation Engineering , vol .", "label": "", "metadata": {}, "score": "57.354233"}
{"text": "Note that similar characteristics were observed at the BioNLP'09 event extraction task regarding the size of minimal subgraph of the dependency graph that includes all triggers and arguments .It was shown in [ 33 ] that the size of this subgraph correlates with the class of the event : positive instances are present typically in smaller subgraphs .", "label": "", "metadata": {}, "score": "57.377026"}
{"text": "Consecutive interval centers are spaced by 25 ms . .Figure 8 : Averaged ERP to target ( blue solid line ) and nontarget ( red dashed line ) stimuli for all considered subjects .Baseline correction was performed on the basis of the 150 ms pre - stimulus interval .", "label": "", "metadata": {}, "score": "57.37905"}
{"text": "W .S .B , where S W is the within class covariance matrix and S B is the between class covariance matrix .Often S B is computed as the covariance of the category means ; alternatively , it is sometimes computed as the \" grand \" covariance matrix over all data , ignoring category labels , identical to the covariance matrix used to compute PCA basis vectors .", "label": "", "metadata": {}, "score": "57.444"}
{"text": "Typical choices include a linear activation function , a unipolar sigmoid function and a bipolar sigmoid function as illustrated in Figure 10 .The activation function should match the characteristic of the input or output data .For example , with training targets assigned the values of \" 0 \" and \" 1 \" , a sigmoid function with the outputs in the range of [ 0 , 1 ] is a good candidate for the output layer .", "label": "", "metadata": {}, "score": "57.44724"}
{"text": "This reuse of code minimized the development effort related specifically to our smoking status classifier .We report precision , recall , F - score , and 95 % exact confidence intervals for each metric .Recasting the classification task for the sentence level and reusing code from other text analysis projects allowed us to quickly build a classification system that performs with a system F - score of 92.64 based on held - out data tests and of 85.57 on the formal evaluation data .", "label": "", "metadata": {}, "score": "57.52128"}
{"text": "This reuse of code minimized the development effort related specifically to our smoking status classifier .We report precision , recall , F - score , and 95 % exact confidence intervals for each metric .Recasting the classification task for the sentence level and reusing code from other text analysis projects allowed us to quickly build a classification system that performs with a system F - score of 92.64 based on held - out data tests and of 85.57 on the formal evaluation data .", "label": "", "metadata": {}, "score": "57.52128"}
{"text": "View at Google Scholar .J. A. Blackard , \" Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables , \" Computers and Electronics in Agriculture , vol .24 , no . 3 , pp .", "label": "", "metadata": {}, "score": "57.53182"}
{"text": "79 - 87 , 1991 .M. J. Jordan and R. A. Jacobs , \" Hierarchical mixtures of experts and the EM algorithm , \" Neural Computation , vol .6 , no . 2 , pp .181 - 214 , 1994 . D. H. Wolpert , \" Stacked generalization , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "57.550323"}
{"text": "For each character , 10 intensifications for each row / column were performed .The subjects were asked to mentally count the number of intensifications of the intended character .The counting was used only to ensure that the participants paid attention .", "label": "", "metadata": {}, "score": "57.57691"}
{"text": "San Francisco , CA .USA .: Morgan Kaufmann Publishers ; 1994 .Boonyanunta N , Zeephongsekul P : Predicting the Relationship Between the Size of Training Sample and the Predictive Power of Classifiers .In Knowledge - Based Intelligent Information and Engineering Systems .", "label": "", "metadata": {}, "score": "57.583588"}
{"text": "Today the Gene Ontology categorizer [ 32 ] has significantly improved its classification power .The predictive model currently achieves top - precision of 48.93 % for a recall of 66.56 % after twenty categories .The data - poor model achieves a top precision of 29.8 % but is more adapted to density estimation than the predictive model ; therefore it is used in the experiments reported in this paper .", "label": "", "metadata": {}, "score": "57.583725"}
{"text": "6 , pp .510 - 523 , 1988 .View at Google Scholar \u00b7 View at Scopus .F. Piccione , F. Giorgi , P. Tonin et al . , \" P300-based brain computer interface : reliability and performance in healthy and paralysed participants , \" Clinical Neurophysiology , vol .", "label": "", "metadata": {}, "score": "57.63114"}
{"text": "Warmuth MK , Liao J , Ratsch G , Mathieson M , Putta S , Lemmen C : Active learning with support vector machines in the drug discovery process .J Chem Inf Comput Sci 2003 , 43 ( 2 ) : 667 - 673 .", "label": "", "metadata": {}, "score": "57.64112"}
{"text": "In particular , Swiss - Prot keywords improved the effectiveness of the categorizer .To rank the candidate vectors we compute a vector - space distance between these candidates and the reference vector .The resulting ranking expresses a lexical similarity between the annotation provided by LocusLink curators and sentences of the abstract and title .", "label": "", "metadata": {}, "score": "57.67509"}
{"text": "Erkan G , \u00d6zg\u00fcr A , Radev DR : Semi - supervised classification for extracting protein interaction sentences using dependency parsing .In Proc . of the 2007 Joint Conf . on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ( EMNLP - CoNLL ) .", "label": "", "metadata": {}, "score": "57.7519"}
{"text": "This is telling us that even though we 're not particularly confident about our decisions on average , the ranking is in fact useful .In fact , this tells us that by setting a threshold other than 0.50 for classification , we could achieve an 86.4 % f - measure on this task .", "label": "", "metadata": {}, "score": "57.77323"}
{"text": "While subjects 2 and 8 could achieve an almost perfect typing performance for already .row / column intensifications , subjects 4 and 7 achieved the worst accuracy ( around 50 % after . intensifications , with a chance level of .", "label": "", "metadata": {}, "score": "57.86342"}
{"text": "In Proc . of The 17th European Conf . on Machine Learning .Berlin , Germany ; 2006:318 - 329 .Kuboyama T , Hirata K , Kashima H , Aoki - Kinoshita KF , Yasuda H : A spectrum tree kernel .", "label": "", "metadata": {}, "score": "57.86555"}
{"text": "This correlates with the average length of sentences with positive / negative pairs being 27.6 and 37.2 words - these numbers coincide with the average length of neutral sentences .This is also in accordance with the distribution of positive and negative pairs in terms of the sentence length .", "label": "", "metadata": {}, "score": "57.867714"}
{"text": "PubMed View Article .Cohen AM , Hersh WR : A survey of current work in biomedical text mining .Brief Bioinformatics 2005 , 6 : 57 .PubMed View Article .Krallinger M , Valencia A , Hirschman L : Linking genes to literature : text mining , information extraction , and retrieval applications for biology .", "label": "", "metadata": {}, "score": "57.877472"}
{"text": "There has been a growing interest in the ERP detection problem , as witnessed by the increased availability of BCIs that rely on it .A notorious example is the P300 speller [ 3 ] , with which subjects are able to type words on a computer screen .", "label": "", "metadata": {}, "score": "57.97522"}
{"text": "Note : The basic subjectivity demo must be run first to create the model file that will be used by the hierarchical model .The hierarchical classifier is run just like the other demos , either through the Ant target hierarchical ( with the same system property setting the data directory ) , or by the command : . java -cp \" sentimentDemo.jar ; . /lingpipe-4.1.0 . jar \" PolarityHierarchical POLARITY_DIR .", "label": "", "metadata": {}, "score": "57.996902"}
{"text": "The number of characters ( here set to 128 ) , determines the amount of penalty per character for unknown words .It 's also possible to experiment with token n - grams .And , of course , each of these classifiers has a few parameters to tweak .", "label": "", "metadata": {}, "score": "58.00615"}
{"text": "The number .was equal to the number of intensification sequences ( trials ) , for each stimulus , during the testing stage .Signal amplitudes at specific time instants in the interval 100 - 750 ms after stimuli onset , of the downsampled EEG signal , were taken as features .", "label": "", "metadata": {}, "score": "58.016422"}
{"text": "We observed that for difficult ( D ) pairs , some kernels perform equally better independently of the class label : SST ( CL and CV ) and ST ( CL only ) .However , this advantage can not be easily exploited unless difficult pairs are identified in advance .", "label": "", "metadata": {}, "score": "58.042526"}
{"text": "In particular , several methods for online boosting and online support vector machine ( SVM ) training has been proposed .However , those methods have several limitations .Furthermore , kernel - using SVM usually have significant storage and computational requirements due to the large amount of kernel expansion terms .", "label": "", "metadata": {}, "score": "58.17909"}
{"text": "Main method that is used as a basis for our algorithm is Pegasos [ 4 ] , which is a modified SGD method with an added projection step , although more generic algorithms such as NORMA [ 3 ] can be easily used in its place .", "label": "", "metadata": {}, "score": "58.24156"}
{"text": "Minimum rule .\\(\\alpha\\rightarrow \\infty \\Rightarrow\\ ) maximum rule .Voting based methods .The ensemble then chooses class J that receives the largest total vote : .Majority ( plurality ) voting .Under the condition that the classifier outputs are independent , it can be shown the majority voting combination will always lead to a performance improvement .", "label": "", "metadata": {}, "score": "58.337044"}
{"text": "The next group of kernels applies dependency parse sentence representation .The most sophisticated kernel , all - path graph ( APG ; [ 23 ] ) builds both on the dependency graph and the token sequence representations of the entire sentence , and weighs connections within and outside the shortest path differently .", "label": "", "metadata": {}, "score": "58.33834"}
{"text": "Given the high dimensionality of speech feature spaces used for automatic speech recognition , typically 39 or more , it is not feasible to visualize the distribution of data in feature space .It is possible that a reduced dimensionality subspace obtained by linear methods , such as PCA or LDA , forms an effective , or at least adequate subspace for implementing automatic speech recognition systems with a reduced dimensionality feature space .", "label": "", "metadata": {}, "score": "58.35873"}
{"text": "Moreover , sentence trimming is blocked when clauses contain gene and protein names ( GPN ) .The GPN tagger is based on a very simple heuristic : any non - recognized English token is considered as a GPN .We use the UMLS SPECIALIST Lexicon and a frequency list of English words ( total of more than 400,000 items ) to separate between known and unknown words .", "label": "", "metadata": {}, "score": "58.473396"}
{"text": "Each of these segments consisted of 1000 ms of recording , starting from the stimuli onsets .Then , they were downsampled , by retaining every 25th sample , and assigned to one of two possible groups : target and nontarget , according to the stimuli that they were locked to .", "label": "", "metadata": {}, "score": "58.478645"}
{"text": "Figure 5 .Plot of input and output data for pseudo - random 2-D data .The output data ( red line ) is reconstructed data obtained after passing the input data through the trained neural network .In Figure 6 , NLPCA is illustrated by data which falls on a 2-D surface embedded in a 3-D space .", "label": "", "metadata": {}, "score": "58.52581"}
{"text": "Relatively poorly investigated by researchers in text mining , both terminology - driven density estimation and discourse - level classification look particularly promising .Declarations .Acknowledgements .This study is supported by the Swiss National Science Foundation thanks to the EAGL project ( Engine for question - Answering in Genomics 3252B0 - 105755 ) .", "label": "", "metadata": {}, "score": "58.549416"}
{"text": "Cross - validation performs multiple divisions of the data into training and test sets and then averages the results in order to bring down evaluation variance in order to tighten confidence intervals .It can be run just like the other methods either from the command line using class PolarityWhole or from Ant using the target whole .", "label": "", "metadata": {}, "score": "58.613308"}
{"text": "We 'll step through the output ( which runs to more than a page ) a bite - sized chunk at a time ; the ellipses ( ... ) indicate that the report is continued .The first line of the report indicates the name of the categories .", "label": "", "metadata": {}, "score": "58.66549"}
{"text": "We used a synchronization scheme based on the high - precision timestamps of the stimulus onsets ( during stimulation ) .The timestamps are obtained using the high - resolution system performance counter ( via QueryPerformanceCounter system call ) , which allows to achieve microsecond resolution .", "label": "", "metadata": {}, "score": "58.70706"}
{"text": "Both PCA and LDA are based on linear , i.e. matrix multiplication , transformations .For the case of PCA , the transformation is based on minimizing mean square error between original data vectors and data vectors that can be estimated from the reduced dimensionality data vectors .", "label": "", "metadata": {}, "score": "58.74233"}
{"text": "PubMed View Article .Jiroutek MR , Muller KE , Kupper LL , Stewart PW : A New Method for Choosing Sample Size for Confidence Interval - Based Inferences .Biometrics 2003 , 59 ( 3 ) : 580 - 590 .", "label": "", "metadata": {}, "score": "58.858543"}
{"text": "th channel ( electrode ) at time . , after stimulus onset , .the average of . , and .the standard deviation for all training examples of both the target and nontarget recordings of the training set .When combining all those features , we obtained a feature vector .", "label": "", "metadata": {}, "score": "58.892296"}
{"text": "Our temporal resolution component does not include an explicit one - year rule for distinguishing between past smoker and current smoker , but relies on the features and labeled data to learn the differences .The most challenging category for our system to classify is past smoker .", "label": "", "metadata": {}, "score": "58.95948"}
{"text": "Experiment 2 .To simulate lack of training data , another experiment was conducted .In this experiment , the training data was separated into two groups , with about 50 % in each group .One group of data ( group 1 ) was used for \" training \" transformations while the other data ( group 2 ) was used for training classifiers .", "label": "", "metadata": {}, "score": "59.020977"}
{"text": "Table 5 .Random data simulations of real data sets .This table compares the results found from the real data ( Real column ) to two different types of random data .The Random column contains the experimentally determined largest number of pairs found from 10 simulation runs using a random data matrix ( drawn from a uniform distribution ) where the number of genes and class sizes is the same as the indicated for the real data .", "label": "", "metadata": {}, "score": "59.05509"}
{"text": "These vectors have 48 dimensions and each vector consists of only one peak value to indicate the category .Note that , in the TIMIT case , other reasonable choices for targets would be 61 ( the number of phone label categories ) , or 39 ( the number of collapses phone categories ) .", "label": "", "metadata": {}, "score": "59.09201"}
{"text": "for factor \" method \" ) and with post hoc multiple comparison based on Turkey LSD test for pairs of all methods .We found that the accuracy of a BLDA in general is significantly ( . better than that of any other classifier except the Gaussian kernel SVM ( nSVM versus BLDA has . , since the later , for some subjects , and for some numbers of intensifications . , yielded on average better results .", "label": "", "metadata": {}, "score": "59.128017"}
{"text": "Each experiment was composed of one training and several testing stages .During both stages , columns and rows of the matrix were intensified ( see Figure 2 ) in a random manner .The duration of the intensification was fixed to 100 ms , followed by 100 ms of no intensification .", "label": "", "metadata": {}, "score": "59.158543"}
{"text": "View Article PubMed .Mottaz A , Yip YL , Ruch P , Veuthey AL : Mapping proteins to disease terminologies : from UniProt to MeSH .BMC Bioinformatics 2008 .( to appear ) .Natarajan J , Berrar D , Dubitzky W , Hack C , Zhang Y , DeSesa C , van Brocklyn J , Bremer E : Text mining of full - text journal articles combined with gene expression analysis reveals a relationship between sphingosine-1-phosphate and invasiveness of a glioblastoma cell line .", "label": "", "metadata": {}, "score": "59.27796"}
{"text": "The recognition accuracies were further improved by about 3 % with PCA reduced dimensionality features versus the NLDA features for most cases , showing the effectiveness of PCA in de - correlating the network outputs .The accuracies obtained with the original 78 features , and 3 mixture HMMs , are approximately 58 % ( 1 state models ) and 63 % ( 3 state models ) .", "label": "", "metadata": {}, "score": "59.291325"}
{"text": "If you want to see results for other slices , just swap out the 9 for any digit between 0 and 8 .Evaluation .The evaluation code follows the same structure as the training code : . if ( !+ + numTests ; .", "label": "", "metadata": {}, "score": "59.384914"}
{"text": "View Article PubMed .Liotta L , Ferrari M , Petricoin E : Clinical proteomics : Written in blood .Nature 2003 , 425 : 905 .View Article PubMed .Brown M , Grundy W , Lin D , Cristianini N , Sugnet C , Furey T , Ares M Jr , Haussler D : Knowledge - based analysis of microarray gene expression data by using support vector machines .", "label": "", "metadata": {}, "score": "59.46678"}
{"text": "238 - 247 , 2001 .G. Brown , J. Wyatt , R. Harris , X. Yao , \" Diversity Creation Methods : A Survey and Categorisation , \" Journal of Information Fusion ( Special issue on Diversity in Multiple Classifier Systems ) .", "label": "", "metadata": {}, "score": "59.47242"}
{"text": "Thus the method is patterned after LDA .In all cases , the dimensionality reduction is accomplished with a Neural Network ( NN ) , which internally encodes data with a reduced number of dimensions .The differences in the methods depend on error criteria used to train the network , the architecture of the network , and the extent to which the reduced dimensions are \" hidden \" in the neural network .", "label": "", "metadata": {}, "score": "59.498684"}
{"text": "The 10 - 15 percentage point difference in F - score between CV and CL settings reported in [ 14 ] can be most evidently seen in the slightly better performance of classifiers on difficult pairs in the CV setting .For example , pairs not classified correctly by any kernels in the CL setting ( CL00 ) are most likely correctly classified by some CV classifiers ( CV01-CV05 ) , as shown in Figure 3 .", "label": "", "metadata": {}, "score": "59.50361"}
{"text": "1109 - 1120 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Neshige , N. Murayama , W. Izumi , T. Igasaki , and K. Takano , \" Non - verbal and verbal P300 of auditory and visual stimuli in dementia , dysarthria and aphasia , \" Japanese Journal of Rehabilitation Medicine , vol .", "label": "", "metadata": {}, "score": "59.57512"}
{"text": "Stoica E , Hearst M : Predicting gene functions from text using a cross - species approach .Pac Symp Biocomput 2006 , 88 - 99 .Crangle C , Cherry JM , Hong EL , Zbyslaw A : Mining experimental evidence of molecular function claims from the literature .", "label": "", "metadata": {}, "score": "59.578808"}
{"text": "Layer 1 : Classifying Unknown and Smoking - related Sentences Feature Selection A subcorpus containing all sentences that were labeled a category other than unknown was created from the sen- tence - level training data described above .All features for the Layer 1 classifier were drawn from this subcorpus .", "label": "", "metadata": {}, "score": "59.587074"}
{"text": "We show that our theoretical convergence bounds are similar to those of earlier algorithms , while allowing for greater flexibility .Our approach may also easily incorporate additional nonlinearity in form of Mercer kernels , although our experiments show that this is not necessary for most situations .", "label": "", "metadata": {}, "score": "59.602448"}
{"text": "p .Additionally to this , we have to introduce a prior distribution over all weights as a zero - mean Gaussian .e . x .p .Using Bayes 's rule , we can define the posterior distribution .which is a Gaussian with mean . and covariance matrix .", "label": "", "metadata": {}, "score": "59.70751"}
{"text": "Method Based on Feature Extraction .Another linear classifier used in P300 BCI research [ 13 ] relies on the one - dimensional version of the linear feature extraction ( FE ) approach proposed by Leiva - Murillo and Art\u00e8s - Rodriguez in [ 21 ] .", "label": "", "metadata": {}, "score": "59.712128"}
{"text": "View Article PubMed .Strube M , Hahn U : Functional Centering .ACL 1996 , 270 - 277 .Paice C : Constructing Literature Abstracts by Computer : Techniques and Prospects .Inform Proc Manag 1990 , 26 : 171 - 86 .", "label": "", "metadata": {}, "score": "59.803947"}
{"text": "In AdaBoost . M1 , bootstrap training data samples are drawn from a distribution \\(D\\ ) that is iteratively updated such that subsequent classifiers focus on increasingly difficult instances .This is done by adjusting \\(D\\ ) such that previously misclassified instances are more likely to appear in the next bootstrap sample .", "label": "", "metadata": {}, "score": "59.828373"}
{"text": "The distribution \\(D\\ ) starts out as uniform ( Equation 3 in Figure 6 ) , so that all instances have equal probability to be drawn into the first data subset \\(S_1\\ .\\ )At each iteration t , a new training set is drawn , and a weak classifier is trained to produce a hypothesis \\(h_t\\ .", "label": "", "metadata": {}, "score": "59.8677"}
{"text": "l .u .e .The process was iterated until convergence , or until it reached a predefined number of 60 features .Bayesian Linear Discriminant Analysis .Bayesian linear discriminant analysis ( BLDA ) has been used in P300 BCI patient studies [ 7 ] .", "label": "", "metadata": {}, "score": "59.878845"}
{"text": "For linear SVMs , . is a simple inner product of input and weight vectors , while for SVMs using kernel trick for nonlinear classification : .Several methods exist for solving both primal and dual formulations of the SVM optimization problem .", "label": "", "metadata": {}, "score": "60.045513"}
{"text": "The number of current iteration .: Regularization parameter from Pegasos algorithm used for training weak classifiers .: Regularization parameter used for training strong classifier .:The number of weak classifiers in the current pool .This number is incremented each time a new classifier is added .", "label": "", "metadata": {}, "score": "60.219124"}
{"text": "Confusion Matrix reference \\ response , plot , quote plot,458,42 quote,37,463 ... .This matrix represents the count of all reference / response pairs .The reference categories are read down the left and the response categories along the top .For this demo , the reference is the \" gold standard \" defined by the database curators and the response is the first - best category produced by the classifier we just trained .", "label": "", "metadata": {}, "score": "60.31295"}
{"text": "Results For the hepatobiliary data set , we obtained a high sensitivity of 0.95 ( on a par with manual annotators , as compared to 0.91 for a baseline classifier ) with specificity 0.56 .For the acute renal failure data set , sensitivity increased from 0.69 to 0.89 , with specificity 0.59 .", "label": "", "metadata": {}, "score": "60.31662"}
{"text": "185 - 208 , MIT Press , Cambridge , Mass , USA , 1999 .View at Google Scholar . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "60.366417"}
{"text": "This is consistent with the previous experiment on PD pairs : in long sentences there are only few positive pairs , and those are difficult to classify .Class distribution of pairs depending on the number of proteins in the sentence .", "label": "", "metadata": {}, "score": "60.53914"}
{"text": "Cusick M , Yu H , Smolyar A , Venkatesan K , Carvunis A , Simonis N , Rual J , Borick H , Braun P , Dreze M , et al .: Literature - curated protein interaction datasets .Nat Methods 2008 , 6 : 39 - 46 .", "label": "", "metadata": {}, "score": "60.63226"}
{"text": "Methods : The Vascular Disease Biorepository at Mayo Clinic was established to archive DNA , plasma , and serum from patients with suspected AVD .AVD phenotypes , relevant risk factors and comorbid conditions were ascertained by electronic medical record ( EMR)-based electronic algorithms that included diagnosis and procedure codes , laboratory data and text searches to ascertain medication use .", "label": "", "metadata": {}, "score": "60.71963"}
{"text": "Figure 2 : Typing matrix of the P300 speller .Rows and columns are intensified in random order ; one trial consists of the intensifications all six rows and all six columns .The intensification of the third column ( a ) and of the second row ( b ) are shown .", "label": "", "metadata": {}, "score": "60.76754"}
{"text": "60 , pp . 1 - 12 , 2009 .View at Google Scholar . D. J. Krusienski , E. W. Sellers , F. Cabestaing et al . , \" A comparison of classification techniques for the P300 Speller , \" Journal of Neural Engineering , vol .", "label": "", "metadata": {}, "score": "60.834007"}
{"text": "View Article .Olsson F , Tomanek K : An intrinsic stopping criterion for committee - based active learning .In Proceedings of the Thirteenth Conference on Computational Natural Language Learning .Boulder , Colorado : Association for Computational Linguistics ; 2009:138 - 146 .", "label": "", "metadata": {}, "score": "61.01728"}
{"text": "We find that there are 4 genes that when paired with PLAC8 form a classifier with the low error rate of 1/90 .In addition there are many gene triples , often including PLAC8 , that form zero error classifiers and might be good biomarkers .", "label": "", "metadata": {}, "score": "61.031788"}
{"text": "The general network configuration is shown in Figure 4 .Figure 4 .Architecture of Bottleneck Neural Network .If data lies along a single curved line in a higher dimensionality space , 1 node in the bottleneck layer should be sufficient .", "label": "", "metadata": {}, "score": "61.061405"}
{"text": "a. Accessed October 25 , 2007 .Chapman WW , Bridewell W , Hanbury P , Cooper GF , Buchanan BG .A simple algorithm for identifying negated findings and diseases in discharge summaries .J Biomed Inform 2001;34 : 301 - 10 .", "label": "", "metadata": {}, "score": "61.0935"}
{"text": "All the training sentences ( 4620 sentences ) were used to extract a total of 31,300 vowel tokens for training .All the test sentences ( 1680 sentences ) were used to extract a total of 11,625 vowel tokens for testing .", "label": "", "metadata": {}, "score": "61.137703"}
{"text": "Sentiment analysis is also called opinion mining or voice of the customer .There are lots of startups in this area and conferences .This tutorial covers assigning sentiment to movie reviews using language models .There are many other approaches to sentiment .", "label": "", "metadata": {}, "score": "61.19609"}
{"text": "NLDA1 .In the first approach , which is referred to as NLDA1 , the transformed features are produced from the final output layer of the network .This approach is similar to the use of tandem neural networks used in some automatic speech recognition studies ( Hermansky & Sharma , 2000 ; Ellis et al . , 2001 ) .", "label": "", "metadata": {}, "score": "61.199604"}
{"text": "PLoS Comput Biol 2005 , 1 ( 3 ) : 179 - 81 .View Article PubMed .Bairoch A : Proteome Research : new frontiers in functional genomics Protein databases - Springer 1997 .Blaschke C , Andrade M , Ouzounis C , Valencia A : Automatic Extraction of Biological Information from Scientific Text : Protein - Protein Interactions .", "label": "", "metadata": {}, "score": "61.26438"}
{"text": "PubMed View Article .Dobbin K , Zhao Y , Simon R : How Large a Training Set is Needed to Develop a Classifier for Microarray Data ?Clinical Cancer Research 2008 , 14 ( 1 ) : 108 - 114 .", "label": "", "metadata": {}, "score": "61.35344"}
{"text": "Portland : ACL ; 2011:1 - 9 .Xu F , Uszkoreit H , Li H : A seed - driven bottom - up machine learning framework for extracting relations of various complexity .ACL'07 2007 , 584 - 591 .Liu H , Komandur R , Verspoor K : From graphs to events : a subgraph matching approach for information extraction from biomedical text .", "label": "", "metadata": {}, "score": "61.357723"}
{"text": "\" There is a substantial amount of literature on identifying and extracting information from EMRs [ 12].Schuemie et al .[ 24 ] compared several machine - learning methods for identifying patients with liver disorder from free - text medical records .", "label": "", "metadata": {}, "score": "61.384186"}
{"text": "Each data file should be downloaded and then unpacked .They are distributed in tarred / gzipped format .For instance , here 's the result of unpacking the review polarity data which was downloaded to the directory in which untarring is performed : .", "label": "", "metadata": {}, "score": "61.45304"}
{"text": "In order to play nicely with the rest of LingPipe , we should really define our hierarchical classifier to implement the interface classify .Classifier .This is actually very straightfoward , but we did n't want to confuse the earlier code with tricky Java particulars like inner class interface implementations .", "label": "", "metadata": {}, "score": "61.48698"}
{"text": "But , as it is mostly the case with BCI research , the P300 BCI has primarily been tested on healthy subjects .Only very few attempts have been made on patients [ 4 - 9 ] .Several of these tests on patients [ 4 , 9 ] deal with P300-based online typing , however , since only very few patients were tested , it is still an open question for which patient categories the P300 speller is best suited .", "label": "", "metadata": {}, "score": "61.542778"}
{"text": "The training of the neural network ( NLDA1 and NLDA2 ) requires category information for creating training targets .For the case of databases such as TIMIT , the data is labeled using 61 phone categories , and the starting point for training discriminative transformations would seem to be these phonetic labels .", "label": "", "metadata": {}, "score": "61.56781"}
{"text": "Detailed information about the three algorithms can be found in appendix 2 ( see additional file 2 ) and in literature [ 10 , 35 , 42 ] .Each experiment was repeated 100 times and Y acc averaged at each batch size over the 100 runs to obtain data points ( x j , y j ) of the learning curve .", "label": "", "metadata": {}, "score": "61.580246"}
{"text": "Int J Med Inform 2006 , 75 ( 6 ) : 413 - 417 .View Article PubMed .Ehrler F , Gobeill J , Tbahriti I , Ruch P : GeneTeam Site Report for BioCreative II : Customizing a Simple Toolkit for Text Mining in Molecular Biology .", "label": "", "metadata": {}, "score": "61.628677"}
{"text": "The category array is initialized using the directory names under txt_sentoken , which in this case are \" pos \" and \" neg \" .We set the n - gram length to the constant 8 ; this could obviously be set with a command - line argument if desired .", "label": "", "metadata": {}, "score": "61.66182"}
{"text": "Blaschke C , Andrade MA , Ouzounis C , Valencia A : Automatic extraction of biological information from scientific text : protein - protein interactions .Proc Int Conf Intell Syst Mol Biol 1999 , 7 : 60 - 67 .Ono T , Hishigaki H , Tanigami A , Takagi T : Automated extraction of information on protein - protein interactions from the biological literature .", "label": "", "metadata": {}, "score": "61.671387"}
{"text": "Copyright .\u00a9 Figueroa et al ; licensee BioMed Central Ltd. 2012 .What is Sentiment Analysis ?Sentiment analysis involves classifying opinions in text into categories like \" positive \" or \" negative \" often with an implicit category of \" neutral \" .", "label": "", "metadata": {}, "score": "61.75036"}
{"text": "Boosting is a meta - algorithm for supervised learning that combines several weak classifiers , that is , classifiers that can label examples only slightly better than random guessing , into a single strong classifier with arbitrary classification accuracy .One of the most successful and well known examples is AdaBoost [ 8 ] and its variants , like LPBoost .", "label": "", "metadata": {}, "score": "61.821026"}
{"text": "All of these are high in E2A and lower in the rest .Probe 33355_at for PBX1 is shown in Figure 7 where it clearly separates the classes and was the only single gene identified in Yeoh 's original work .The other 3 probes barely separate the data and likely failed Yeoh 's stringent cross - validation criteria .", "label": "", "metadata": {}, "score": "61.894012"}
{"text": "The four kernels differ in the information they use from the parses .The lexical kernel uses only lexical information encoded into the dependency tree , that is , nodes are the lemmas of the sentences connected by dependency relation labeled edges .", "label": "", "metadata": {}, "score": "61.916275"}
{"text": "Liu Y : Active learning with support vector machine applied to gene expression data for cancer classification .J Chem Inf Comput Sci 2004 , 44 ( 6 ) : 1936 - 1941 .PubMed View Article .Li M , Sethi IK : Confidence - based active learning .", "label": "", "metadata": {}, "score": "62.01084"}
{"text": "Thus , during the testing stage , for each trial , we had 12 segments ( from all 12 types ) of 1000 ms EEG data recorded from each electrode .The averaged EEG response for each electrode was determined for each stimulus type .", "label": "", "metadata": {}, "score": "62.068924"}
{"text": "R. Polikar , L. Udpa , S. S. Udpa , and V. Honavar , \" Learn++ : An incremental learning algo - rithm for supervised neural networks , \" IEEE Transactions on Systems , Man and Cybernetics Part C : Applications and Reviews , vol .", "label": "", "metadata": {}, "score": "62.161324"}
{"text": "View Article .Yelle LE : The Learning Curve : Historical Review and Comprehensive Survey .Decision Sciences 1979 , 10 ( 2 ) : 302 - 327 .View Article .Ramsay C , Grant A , Wallace S , Garthwaite P , Monk A , Russell I : Statistical assessment of the learning curves of health technologies .", "label": "", "metadata": {}, "score": "62.17192"}
{"text": "We only discuss the top results for the T and E2A splittings here , see the supplement [ see Additional file 1 ] for more details .Our results reinforce many of the findings in Yeoh 's work that there are single genes that accurately classify the T and E2A leukemia subtypes and extends it by identifying accurate 2 gene classifiers .", "label": "", "metadata": {}, "score": "62.174103"}
{"text": "Figure 5 : Distribution ( percentage with respect to all typed characters ) of the P300 speller outputs for . intensifications .Cells with zero coordinates correspond to correctly spelled characters , while other cells show the results of mistyping .The coordinates of those cells indicate the relative positions of the mistyped and intended characters .", "label": "", "metadata": {}, "score": "62.22942"}
{"text": "As for SWLDA and LDA , which ranked third , SWLDA performs slightly better , but not in a significant way .The worst results are obtained for the feature extraction ( FE ) method and the multilayer feed - forward neural network ( NN ) .", "label": "", "metadata": {}, "score": "62.23076"}
{"text": "Appendix 1 : Confidence Intervals .As usual , we compute 95 % confidence intervals for a given accuracy of p over N trials using the binomial distribution bionmial(p , N ) , which is just the distribution corresponding to N independent trials each with a p chance of success .", "label": "", "metadata": {}, "score": "62.23822"}
{"text": "All hidden nodes and output nodes had a bipolar sigmoidal activation function .After training with backpropagation , all data were transformed by the neural network .In Figure 5 , the original data is shown as blue symbols , and the transformed data is shown by red .", "label": "", "metadata": {}, "score": "62.310608"}
{"text": "Figure 16 .Accuracies of NLDA1 and NLDA2 with various dimensionality reduced features based on 1-state ( top panel ) and 3-state HMMs ( bottom panel ) .The NLDA1 features without PCA are always 48 dimensions .", "label": "", "metadata": {}, "score": "62.31346"}
{"text": "In SVM l i g h t , kernel functions can be defined as a real function of a pair in the corresponding instance representation .LibSVM , on the other hand , requires the user to pre - compute kernel values , i.e. , pass to the SVM learner a matrix containing the pairwise similarity of all instances .", "label": "", "metadata": {}, "score": "62.326485"}
{"text": "4 , pp .299 - 305 , 2006 .View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . F. Lotte , M. Congedo , A. L\u00e9cuyer , F. Lamarche , and B. Arnaldi , \" A review of classification algorithms for EEG - based brain - computer interfaces , \" Journal of Neural Engineering , vol .", "label": "", "metadata": {}, "score": "62.37405"}
{"text": "If the word was determined to be negated , then we labeled the sentence with nonsmoker .All sentences that were not labeled nonsmoker were passed to the Layer 3 classifier .Layer 3 : Classifying Current Smoker , Past Smoker , and Smoker Sentences Feature Selection A subcorpus containing all sentences that were labeled current smoker or past smoker was created from the sen- tence - level training data described above .", "label": "", "metadata": {}, "score": "62.38617"}
{"text": "View Article PubMed .Ruch P : Automatic assignment of biomedical categories : toward a generic approach .Bioinformatics 2006 , 22 ( 6 ) : 658 - 64 .View Article PubMed .Camon E , Barrell D , Dimmer E , Lee V , Magrane M , Maslen J , Binn D , Apweiler R : An evaluation of GO annotation retrieval for BioCreAtIvE and GOA .", "label": "", "metadata": {}, "score": "62.388817"}
{"text": "24 - 29 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .V. Vapnik , The Nature of Statistical Learning Theory , Springer , New York , NY , USA , 1995 .J. Suykens , T. Van Gestel , J. De Brabanter , B. De Moor , and J. Vanderwalle , Least Square Support Vector Machines , World Scientific , Singapore , 2002 .", "label": "", "metadata": {}, "score": "62.533943"}
{"text": "This could , probably , be due to the fact that the subject sometimes gets distracted by the flashing of a column or row adjacent to one containing the intended character .Or , it could be that the intensification of the row / column containing the intended character is immediately preceded or followed by an intensification of an adjacent row / column , leading to a decreased P300 response .", "label": "", "metadata": {}, "score": "62.551216"}
{"text": "In contrast , the nonlinear technique NLDA based on minimizing classification error was quite effective for improving accuracy .The general form of the NLDA transformer and its relationship to the HMM recognizer are depicted in Figure 7 .NLDA is based on a multilayer bottleneck neural network and performs a nonlinear feature transformation of the input data .", "label": "", "metadata": {}, "score": "62.587864"}
{"text": "In addition , mitochondria related genes have been associated with cancer progression .For this data , normal LIKNON was a useful aid in identifying outlying data examples .The two outlying tumor samples in this data set could represent a rare tumor type or development state .", "label": "", "metadata": {}, "score": "62.591972"}
{"text": "Accuracies of the NLDA1 and NLDA2 features using 1-state ( top panel ) and 3-state HMMs ( bottom panel ) with various numbers of mixtures .Figure 18 .Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \"", "label": "", "metadata": {}, "score": "62.690704"}
{"text": "It can be detected while a subject is shown two types of events with one occurring much less frequently than the other ( \" rare event \" ) .The rare event elicits an ERP consisting of an enhanced positive - going signal component with a latency of about 300 ms after stimulus onset [ 2 ] .", "label": "", "metadata": {}, "score": "62.841927"}
{"text": "The program is an adaptation of the lp_solve version of LIKNON [ 6 ] implemented in C. A modern desktop workstation ( 1.8 Ghz Athlon ) is able to evaluate many thousands of pairs per second using this LIKNON method .Table 3 lists run times for some of the data sets .", "label": "", "metadata": {}, "score": "62.953236"}
{"text": "ST , SST and PT kernels extract subtrees of the syntax parse tree that contain the analyzed protein pair .SpT uses vertex - walks , that is , sequences of edge - connected syntax tree nodes , as the unit of representation .", "label": "", "metadata": {}, "score": "62.962597"}
{"text": "3541 , pp .326 - 335 , Seaside .Monterey , CA , June 2005 .M. Muhlbaier , A. Topalis , R. Polikar , \" Learn++ .NC : Combining Ensemble of Classifiers with Dynamically Weighted Consult - and - Vote for Efficient Incremental Learning of New Classes , \" IEEE Transactions on Neural Networks , In press , 2008 .", "label": "", "metadata": {}, "score": "62.987843"}
{"text": "From the YeohALL data , E2A vs the rest , the best gene pair RPS6 and LRMP .Plus signs are the E2A subtype samples .LRMP by itself is a reasonable indicator of E2A status , but when combined with RPS6 can perfectly separate the data .", "label": "", "metadata": {}, "score": "63.045757"}
{"text": "Corpora .We use the five freely available and widely used PPI - annotated resources also described in [ 8 ] , i.e. , AIMed [ 26 ] , BioInfer [ 27 ] , HPRD50 [ 28 ] , IEPA [ 29 ] , and LLL [ 30 ] .", "label": "", "metadata": {}, "score": "63.233395"}
{"text": "Going further we wished to look for gene triples , yet using all 6605 genes would lead to excessive runtimes , so we applied a variation filter to reduce the number of genes .Only 291 of these triples were saved for further analysis .", "label": "", "metadata": {}, "score": "63.25072"}
{"text": "We define the concept of success level as the number of kernels being able to classify a given pair correctly .For CV evaluation we performed experiments with all 13 kernels , and therefore have success levels : 0, ... ,13 .", "label": "", "metadata": {}, "score": "63.28982"}
{"text": "Kim 's kernels build a proper sub - cluster within dependency - based kernels .The only kernel that does not use neither dependency nor syntax data , SL , is grouped in the cluster of dependency - based kernels .Interestingly , the outlier in this cluster is kBSPS and not SL .", "label": "", "metadata": {}, "score": "63.44209"}
{"text": "We also find small gene sets able to accurately distinguish leukemia subtypes in the large pediatric acute lymphoblastic leukemia cancer set of Yeoh et al .Methods .Transcriptional profiling data sets .The existing transcriptional profiling data sets investigated here are summarized in Table 1 .", "label": "", "metadata": {}, "score": "63.570717"}
{"text": "24 - S. A. Zahorian , T. Singh , H. Hu , 2007 Dimensionality Reduction of Speech Features using Nonlinear Principal Components Analysis .Proc .INTERSPEECH ' 07 , 1134 1137 , Antwerb , Belgium , Aug. 27 - 31 , 2007 .", "label": "", "metadata": {}, "score": "63.5738"}
{"text": "For both NLDA1 and NLDA2 , the networks were configured with 78 - 500 - 36 - 500 - 144 nodes , going from input to output .Figure 18 .Recognition accuracies of the NLDA dimensionality reduced features using the state level targets . \"", "label": "", "metadata": {}, "score": "63.62455"}
{"text": "The highly favorable pathologic stage ( RI - RII , 58 % ) and the fact that the majority of patients were alive and disease - free suggested a more favorable prognosis for this type of renal cell carcinoma .Features and heuristics .", "label": "", "metadata": {}, "score": "63.650787"}
{"text": "Table 13 .Incorrectly annotated protein pairs selected from the very hardest positive and negative pairs .Membranous staining and concomitant cytoplasmic localization of E - cadherin , and were seen in one case with abnormal beta - catenin immunoreactivity .B.d418.s0.p1 .", "label": "", "metadata": {}, "score": "63.71151"}
{"text": "It is shown here for the first time that the regulation of ABCA1 mRNA levels exploits the use of alternative transcription start sites .GeneRiF : regulation of ABC A1 mRNA levels exploits the use of alternative transcription start sites .In the TREC evaluation data , we analyzed the sentence location distribution used to produce the GeneRiF. In this case , we considered the title ( see Figure 1 , the first column labelled \" ti \" ) and the abstract 's sentence sequence .", "label": "", "metadata": {}, "score": "63.800358"}
{"text": "It was shown that SWLDA and LDA render the best overall performance .In [ 11 ] , it was shown that , among linear SVM , Gaussian kernel SVM , multi - layer perceptron , Fisher LDA , and kernel Fisher Discriminant , the best performance was achieved with LDA .", "label": "", "metadata": {}, "score": "63.809284"}
{"text": "Thus NTIMIT is more bandlimited ( approximately 300Hz to 3400 Hz ) , more noisy , but has the identical \" raw \" speech .DCTC / DCSC speech features .The modified DCTC is used for representing speech spectra , and the modified DCSC is used to represent spectral trajectories .", "label": "", "metadata": {}, "score": "63.901665"}
{"text": "On the other hand , kernels perform far better for easy pairs ( both PE&NE ) in CV than in CL setting .This shows that the more general CL models do not work so well on easy pairs than the rather corpus specific CV models ; that is , the smaller variability in training examples is also reflected in performance of the learnt model .", "label": "", "metadata": {}, "score": "63.96638"}
{"text": "D1 contains 7,016 sentences and 350 word features to distinguish between smokers ( 5,333 sentences ) and non smokers ( 1,683 sentences ) .D2 contains 8,449 sentences , 350 word features to discriminate between past smokers ( 5,109 sentences ) and current smokers ( 3,340 sentences ) .", "label": "", "metadata": {}, "score": "64.00345"}
{"text": "PubMed View Article .Marcotte EM , Xenarios I , Eisenberg D : Mining literature for protein - protein interactions .Bioinformatics 2001 , 17 ( 4 ) : 359 .PubMed View Article .Huang M , Zhu X , Hao Y , Payan DG , Qu K , Li M : Discovering patterns to extract protein - protein interactions from full texts .", "label": "", "metadata": {}, "score": "64.12452"}
{"text": "The number of DCTCs used was 13 , and number of DCS terms was varied from 4 to 7 , for a total number of features ranging from 52 to 91 .These numbers are given for each experiment .Additionally , as a control , one experiment was conducted with Mel - frequency Cepstral Coefficients ( MFCCs ) ( Davis & Mermelstein , 1980 ) , since these MFCC features are most typically used in ASR experiments .", "label": "", "metadata": {}, "score": "64.15249"}
{"text": "Cross validation type selection is typically used for training the Tier 1 classifiers : the entire training dataset is divided into T blocks , and each Tier-1 classifier is first trained on ( a different set of ) T -1 blocks of the training data .", "label": "", "metadata": {}, "score": "64.193344"}
{"text": "Triples needs tens to hundreds of hours on a single computer , which is still tractable .Quadruples are beyond single computer tractability .However , this type of algorithm is trivially parallelized over a standard network of computers leading to linear speed up .", "label": "", "metadata": {}, "score": "64.21541"}
{"text": "The database is further divided into a suggested training set ( 4620 sentences , 462 speakers ) and suggested test set ( 1680 sentences , 168 speakers ) .The training and test sets are balanced in terms of representing dialect regions and male / female speakers .", "label": "", "metadata": {}, "score": "64.21965"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. C. Panicker , S. Puthusserypady , and Y. Sun , \" Adaptation in P300 braincomputer interfaces : a two - classifier cotraining approach , \" IEEE Transactions on Biomedical Engineering , vol .", "label": "", "metadata": {}, "score": "64.22177"}
{"text": "( equivalent to filling in a triangular matrix ) , and all triples about n 3 /6 .An approximate average time is 1 second/10 6 pairs / sample .Thus , evaluating all pairs for these data sizes requires only a few minutes whereas all triples needs many hours .", "label": "", "metadata": {}, "score": "64.362946"}
{"text": "Modeling of PLAC8 .PLAC8 is noted to have a match to the PFAM model pfam04749.5 DUF614 .The PFAM model locates to the C terminus of the 1P59 crystal structure .The sequence of 1P59 is some 85 amino acids longer than PLAC8 , and the alignment hit aligns the last 110 amino acids or so .", "label": "", "metadata": {}, "score": "64.45469"}
{"text": "PubMed View Article .Kim JD , Pyysalo S , Ohta T , Bossy R , Nguyen N , Tsujii J : Overview of BioNLP shared task 2011 .Proceedings of the BioNLP Shared Task 2011Workshop , Association for Computational Linguistics 2011 , 1 - 6 .", "label": "", "metadata": {}, "score": "64.49904"}
{"text": "Just do n't be fooled by overtrained a posteriori results on a test set .You 're unlikely to be called out on this in an ACL paper , but the results are rarely achievable in practice .We 'll end with some words of warning on efficiency and performance .", "label": "", "metadata": {}, "score": "64.527596"}
{"text": "We compared different approaches to develop an automatic case identification system with high sensitivity to assist manual annotators .Methods We used four different machine - learning algorithms to build case identification systems for two data sets , one comprising hepatobiliary disease patients , the other acute renal failure patients .", "label": "", "metadata": {}, "score": "64.61638"}
{"text": "PubMed .Cohen J : Statistical Power Analysis for the Behavioural Sciences .Hillsdale , NJ : Lawrence Erlbaum Associates ; 1988 .Scheinin I , Ferreira JA , Knuutila S , Meijer GA , van de Wiel MA , Ylstra B : CGHpower : exploring sample size calculations for chromosomal copy number experiments .", "label": "", "metadata": {}, "score": "64.6705"}
{"text": "Springer Berlin / Heidelberg ; 2004:529 - 535 .View Article .Hess KR , Wei C : Learning Curves in Classification With Microarray Data .Seminars in oncology 2010 , 37 ( 1 ) : 65 - 68 .PubMed View Article .", "label": "", "metadata": {}, "score": "64.71813"}
{"text": "Dennis JE , Gay DM , Welsch RE : Algorithm 573 : NL2SOL - An Adaptive Nonlinear Least - Squares Algorithm [ E4].ACM Transactions on Mathematical Software 1981 , 7 ( 3 ) : 369 - 383 .View Article .", "label": "", "metadata": {}, "score": "64.77982"}
{"text": "Blaschke C , Leon E , Krallinger M , Valencia A : Evaluation of BioCreAtIvE assessment of task 2 .BMC Bioinformatics 2005 , 6 ( Suppl 1 ) : S16 .View Article PubMed .Hersh W , Cohen A , Rekapalli H , Roberts P : TREC 2006 Genomics Track Overview .", "label": "", "metadata": {}, "score": "64.86403"}
{"text": "We see three main possibilities to escape this situation , some of which have already proven successful in other domains or in other extraction tasks ( see references below ) .For all the three directions we provided below examples found among the 120 examined difficult cases .", "label": "", "metadata": {}, "score": "64.917984"}
{"text": "\u03a9 can be partitioned into two sub - sets : \u03a9 t to fit the model , and \u03a9 t to validate the fitted model .Please note that in real life applications only \u03a9 t will be available .We also assigned weights to the data points in\u03a9 t .", "label": "", "metadata": {}, "score": "64.952484"}
{"text": "In the samples column , the number in parenthesis is the number of positive samples .The numbers after the slash are the number of single genes found .Label shuffling leads to more pairs found \" by chance \" only for the smaller data sets .", "label": "", "metadata": {}, "score": "65.03689"}
{"text": "To incorporate this additional early information into the decoding process , we used the interval starting 100 ms after stimulus onset .The negative - going component , called N2 in [ 30 ] , was shown by these authors to be important for the P300 speller , even if the subject only covertly attended the intended target .", "label": "", "metadata": {}, "score": "65.06694"}
{"text": "In each experiment , the predicted performance was compared to the actual observed performance .Figure 2 shows the curve fitting and prediction results for the random sampling learning curve using D2 data at different sample sizes .In Figure 2a the curve was fitted using 6 data points ; the predicted curve ( blue ) deviates slightly from the actual data points ( black ) , though the actual data points do fall in the relatively large confidence interval ( red ) .", "label": "", "metadata": {}, "score": "65.1025"}
{"text": "1 , pp .47 - 56 , September 2006 .N. Oza and S. Russell , \" Online bagging and boosting , \" in Artificial Intelligence and Statistics Pages , pp .105 - 112 , Morgan Kaufmann , Boston , Mass , USA , 2001 .", "label": "", "metadata": {}, "score": "65.20211"}
{"text": "Experiments .Description .We compare our algorithm to both Pegasos [ 4 ] and Norma [ 3 ] , implemented on MATLAB for both the linear and the kernel - based case .The experiments are being run on AMD Phenom X4", "label": "", "metadata": {}, "score": "65.20699"}
{"text": "Protein - protein interaction Relation extraction Kernel methods Error analysis Kernel similarity .Background .Automatically extracting protein - protein interactions ( PPIs ) from free text is one of the major challenges in biomedical text mining [ 1 - 6 ] .", "label": "", "metadata": {}, "score": "65.24832"}
{"text": "The idea is that frequent lexical features in the GO controlled vocabulary ( such as receptor , nucleotid , transfer , regul ... ) must be downweighted .A distribution of the most frequent GO codes is given in Table 6 .", "label": "", "metadata": {}, "score": "65.298996"}
{"text": "Int .Conf . on Computational Linguistics ( Coling'10 ) .Beijing , China ; 2010:779 - 787 .Thomas P , Pietschmann S , Solt I , Tikk D , Leser U : Not all links are equal : exploiting dependency types for the extraction of protein - protein interactions from text .", "label": "", "metadata": {}, "score": "65.31218"}
{"text": "Experiments with 10 random data sets of this size shows that 1500 pairs would be expected on average with a maximum found of 2706 , and no single genes , Table 5 .Also in Table 5 are results of experiments where the real data and class labels are used , but the class labels are randomly shuffled .", "label": "", "metadata": {}, "score": "65.34331"}
{"text": "During training , the system \" knows \" exactly which one of 36 possible characters is attended by the subject at any moment of time ( copy spelling ) .Based on this information , the collected signal segments can be grouped into only two categories : target ( attended ) and nontarget ( not attended ) .", "label": "", "metadata": {}, "score": "65.38455"}
{"text": "Because of theoretical and empirical evidence showing that SVMs are well - suited for text categorization,4,5our efforts focus on building such a classifier for the task .To build a sentence - level classifier , we manually identified every sentence that we judged related to the patient 's smoking status in each document and assigned that sentence the smoking status category assigned to the document .", "label": "", "metadata": {}, "score": "65.38697"}
{"text": "In Proc . of the 11st Conf . of the European Chapter of the ACL ( EACL'06 ) .Trento : The Association for Computer Linguistics ; 2006:401 - 408 .Vishwanathan SVN , Smola AJ : Fast kernels for string and tree matching .", "label": "", "metadata": {}, "score": "65.40312"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .V. N. Vapnik , The Nature of Statistical Learning Theory , Springer , New York , NY , USA , 1995 .S. Boyd and L. Vandenberghe , Convex Optimization , Cambridge University Press , New York , NY , USA , 2004 .", "label": "", "metadata": {}, "score": "65.4151"}
{"text": "This article describes our system entry for the 2006 I2B2 contest \" Challenges in Natural Language Processing for Clinical Data \" for the task of identifying the smoking status of patients .Our system makes the simplifying assumption that patient - level smoking status determination can be achieved by accurately classifying individual sentences from a patient 's record .", "label": "", "metadata": {}, "score": "65.57074"}
{"text": "The anchor words that we used were from a small dictionary we created that contained the top 10 features as ranked by the weights in the SVM model built for Layer 1 and included words like smoke , smoker , tobacco , and cigarette .", "label": "", "metadata": {}, "score": "65.64286"}
{"text": "Two goodness of fit measurements , mean absolute error ( MAE ) ( Eq .( 2 ) ) and root mean squared error ( RMSE ) ( Eq .( 3 ) ) , were used to evaluate the fitted function on\u03a9 v .", "label": "", "metadata": {}, "score": "65.85624"}
{"text": "Visible 5 Hz oscillations are due to the stimulation rate .The analysis of the distribution of the mistyped characters ( Figure 5 ) suggests that mistakes mostly occur due to a wrongly selected row or column in the typing matrix .", "label": "", "metadata": {}, "score": "65.969925"}
{"text": "LLL has a particularly high ratio of positive examples ( 50 % compared to the average of 25 % in the other four corpora ) .Therefore , kernels predict positive pairs easier for LLL at the CV evaluation , in contrast to CL : in CV evaluation , negative pairs are difficult and in CL evaluation positive ones are difficult .", "label": "", "metadata": {}, "score": "66.0011"}
{"text": "The same tendency can be observed for easy pairs ( PE&NE ) in the opposite direction .The difference in performance between CV and CL settings reported in [ 14 ] can not be observed on ND pairs : kernels tend to create more general models in the CL setting and identify ND pairs with greater success in average .", "label": "", "metadata": {}, "score": "66.00559"}
{"text": "531 - 537 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . S. Silvoni , C. Volpato , M. Cavinato , et al . , \" P300-based brain - computer interface communication : evaluation and follow - up in amyotrophic lateral sclerosis , \" Frontiers in Neuroscience , vol .", "label": "", "metadata": {}, "score": "66.02396"}
{"text": "12 , pp .2927 - 2935 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Google Scholar .M. Thulasidas , C. Guan , and J. Wu , \" Robust classification of EEG signal for brain - computer interface , \" IEEE Transactions on Neural Systems and Rehabilitation Engineering , vol .", "label": "", "metadata": {}, "score": "66.39548"}
{"text": "Some of the limitations as applied to the use - case are negation detection and temporal resolution .Full - text .Technical Brief ?Our system makes the simplifying assumption that patient - level smoking status determination can be achieved by accurately classifying individual sentences from a patient 's record .", "label": "", "metadata": {}, "score": "66.56559"}
{"text": "For each training file , a test is done to see if it is a training file .If it is , then the text is read from the file using the LingPipe utility method Files.readFromFile , and then used to train the classifier for the specified category .", "label": "", "metadata": {}, "score": "66.58762"}
{"text": "Of the text material in the database , two dialect sentences ( SA sentences ) were designed to expose the specific variants of the speakers and were read by all 630 speakers .There are 450 phonetically - compact sentences ( SX sentences ) which provide a good coverage of pairs of phones .", "label": "", "metadata": {}, "score": "66.593285"}
{"text": "Our system has three layers of sentence classification ( Figure 1 ) .Layer 1 classifies sentences as unknown or smoking- related ( an umbrella category for nonsmoker , past smoker , current smoker , and smoker ) .All sentences labeled smok- ing - related are passed on to Layer 2 .", "label": "", "metadata": {}, "score": "66.62985"}
{"text": "Many members of the pairs discovered here have known associations with cancer and indicate the possibility of simple , accurate medical diagnostic tests based on such results .Exhaustively examining all pairs in thousand gene size datasets is easily tractable on modern computer hardware .", "label": "", "metadata": {}, "score": "66.647995"}
{"text": "12 , pp .1892 - 1902 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. Sellers , D. Krusienski , D. McFarland , and J. Wolpaw , \" Non - invasive brain - computer interface research at the Wadsworth Center , \" in Toward Brain - Computer Interfacing , G. Dornhege , J. Mill\u00e1n , T. Hinterberger , D. McFarland , and K.-R. M\u00fcller , Eds . , pp .", "label": "", "metadata": {}, "score": "66.65333"}
{"text": "Sentences not marked as nonsmoker by Layer 2 are passed to Layer 3 .Layer 3 assigns current smoker , past smoker , and the generic smoker categories by performing temporal resolution .After each sentence in a document is classified , we apply precedence rules to assign the document level smoking status .", "label": "", "metadata": {}, "score": "66.67721"}
{"text": "In T , HUMTCBYY is generally high and HLA - DRA is low .These and many other of the genes in the top pairs are identified by Yeoh as being significantly differentially expressed in T. However they did not identify any gene pairs as accurate classifiers .", "label": "", "metadata": {}, "score": "66.82687"}
{"text": "Figure 8 .Use of network outputs in NLDA1 . NLDA2 .Figure 9 illustrates the use of network outputs in NLDA2 .These two versions of NLDA were experimentally tested , with and without PCA following the neural network transformer , with some variations of the nonlinearities in the networks .", "label": "", "metadata": {}, "score": "66.97917"}
{"text": "Warning : Oracle Evaluation .We set the tuning parameter ex post facto ( after the fact ) .This is sometimes called an oracle parameter setting .Only rarely does a research paper make its tuning and evaluation methodology clear .", "label": "", "metadata": {}, "score": "67.04471"}
{"text": "The online version of this article ( doi : 10 .1186/\u200b1472 - 6947 - 12 - 8 ) contains supplementary material , which is available to authorized users .Background .The availability of biomedical data has increased during the past decades .", "label": "", "metadata": {}, "score": "67.084946"}
{"text": "IMAGE:667883 PHLDA1 pleckstrin homology - like domain , family A , member 1 Hs82101 .In all , the 291 triples make use of 168 of the genes .The fact that triples exist when using the original labelings argues that the \" outlier \" examples are correctly labeled as tumor , albeit maybe a different type of tumor ( or in a different development stage ) .", "label": "", "metadata": {}, "score": "67.139015"}
{"text": "View Article PubMed .Copyright .\u00a9 Gobeill et al ..This article is published under license to BioMed Central Ltd. Abstract .Background .Kernel - based classification is the current state - of - the - art for extracting pairs of interacting proteins ( PPIs ) from free text .", "label": "", "metadata": {}, "score": "67.1564"}
{"text": "[ Reprinted in Artificial Neural Networks : Concepts and Theory , IEEE Computer Society Press , 1992].View Article .Gruvberger S , Ringn\u00e9r M , Chen Y , Panavally S , Saal L , Borg A , Fern\u00f6 M , Peterson C , Meltzer P : Estrogen receptor status in breast cancer is associated with remarkably distinct gene expression patterns .", "label": "", "metadata": {}, "score": "67.23218"}
{"text": "B. V. Dasarathy and B. V. Sheela ( 1979 ) , \" Composite classifier system design : concepts and me - thodology , \" Proceedings of the IEEE , vol .67 , no .5 , pp .708 - 713 .", "label": "", "metadata": {}, "score": "67.36087"}
{"text": "In 2b , with 11 data points for fitting , the predicted curve closely resembles the observed data and the confidence interval is much narrower .In 2c with 22 data points , the predicted curve is even closer to the actual observations with a very narrow confidence interval .", "label": "", "metadata": {}, "score": "67.565544"}
{"text": "Portland , OR , USA ; 2011:164 - 172 .Copyright .\u00a9Tikk et al . ; licensee BioMed Central Ltd. 2013 .This article is published under license to BioMed Central Ltd.Ensemble learning is the process by which multiple models , such as classifiers or experts , are strategically generated and combined to solve a particular computational intelligence problem .", "label": "", "metadata": {}, "score": "67.69173"}
{"text": "Aggregated results are shown in Figure 2 .All but the PT kernel are considered .( PT is extremely slow and provide below average results ) .Heatmap of success level correlation in CV and CL evaluations .Correlation ranges from 2 ( cyan ) through 63 ( white ) to 1266 ( magenta ) pairs .", "label": "", "metadata": {}, "score": "67.71203"}
{"text": "The process of convergence is illustrated in Figure 2 for the case of two - dimensional dataset , where the data is classified according to whether it is inside unit circle about origin .The parameters used for this illustration , .", "label": "", "metadata": {}, "score": "67.73782"}
{"text": "This produces the following output after running for about a minute and a quarter on my desktop machine : .This result is very encouraging .We 've set LingPipe up with its default recommended n - gram length , 8 , and the resulting classification accuracy is within .014 of the best accuracy reported in Pang , Lee and Vaithyanathan 's Lee 's 2002 Thumbs up ? paper from EMNLP .", "label": "", "metadata": {}, "score": "67.73927"}
{"text": "LRMP is identified in Yeoh 's lists ( but not RPS6 ) , as are many of the genes in the other top pairs .These and the other top pairs often highly separate the data indicating they might be biologically relevant and resilient to noise .", "label": "", "metadata": {}, "score": "67.832596"}
{"text": "Figueroa RL , Zeng - Treitler Q : Exploring Active Learning in Medical Text Classification .In Poster session presented at : AMIA 2009 Annual Symposium in Biomedical and Health Informatics .San Francisco , CA , USA ; 2009 .Kandula S , Figueroa R , Zeng - Treitler Q : Predicting Outcome Measures in Active Learning .", "label": "", "metadata": {}, "score": "67.8411"}
{"text": "Table 17 .Results of some simple majority vote ensembles and comparison with best single methods in terms of F - score .Third , the identification of difficult protein pairs was found to be highly useful to spot likely incorrect annotations in the benchmark corpora .", "label": "", "metadata": {}, "score": "68.07421"}
{"text": "Figure 14 .Figure 15 .Illustration of the state level training targets with \" do n't cares . \"The -1 values are used to denote \" do n't cares . \"Two approaches are used to determine state boundaries .", "label": "", "metadata": {}, "score": "68.10149"}
{"text": "When ground truth can be considered to be known , we may further define the intuitive subclasses negative difficult ( ND ) , positive difficult ( PD ) , negative easy ( NE ) and positive easy ( PE ) , respectively .", "label": "", "metadata": {}, "score": "68.14209"}
{"text": "Classification results on the 219 PE pairs with CL evaluation ( in decreasing order according to the successfully classified pairs ) .On difficult pairs ( ND&PD ) , the measured number of true negatives ( TN ) is smaller than expected based on the class distribution of kernels ' prediction .", "label": "", "metadata": {}, "score": "68.32164"}
{"text": "String resultCategory . if ( resultCategory.equals(category ) ) .The code rendered in grey is the same as in the training loop described in the last section .The remaining code begins by setting the number of tests and number of correct answer counters to zero .", "label": "", "metadata": {}, "score": "68.41377"}
{"text": "s .i . g .n .s .i . g .n .There are several key differences between our algorithms and SVM using Mercer kernels as described in [ 3 , 4 ] .The first and possibly most important one is that the number of weak classifiers .", "label": "", "metadata": {}, "score": "68.56575"}
{"text": "This does illustrate another aspect of the nearly limitless fiddling that 's possible with these kinds of models .Evaluation .The evaluation is set up similarly to the subjectivity demo with a slight twist : . if ( !String review .", "label": "", "metadata": {}, "score": "68.69261"}
{"text": "1471 - 2105 - 6 - 97-S1.ps Additional File 1 : The expanded discussion section .A discussion of the top results for all data sets .( PS 3 MB ) .Authors ' contributions .LG carried out all work outlined in this article .", "label": "", "metadata": {}, "score": "68.7666"}
{"text": "Our HMM modeling indicates PLAC8 might have a domain like part of lP59 's crystal structure ( a Non - Covalent Endonuclease lii - Dna Complex ) .The previously identified HCC biomarker gene , glypican 3 ( GPC3 ) , is part of an accurate gene triple involving MT1E and ARHE .", "label": "", "metadata": {}, "score": "68.80582"}
{"text": "All 13 kernels are taken into consideration .The distribution of pairs according to classification success level using cross - learning setting .The distribution of pairs ( total , positive and negative ) in terms of the number of kernels that classify them correctly ( success level ) aggregated across the 5 corpora in cross - learning setting .", "label": "", "metadata": {}, "score": "69.01437"}
{"text": "The only difference is with the character set : in our case , it is the usual set of 26 Latin characters , eight digits , but with two special characters ( \" _ \" instead of space and \" .as the end of input indicator ) .", "label": "", "metadata": {}, "score": "69.046555"}
{"text": "Briggs AH , Gray AM : Power and Sample Size Calculations for Stochastic Cost - Effectiveness Analysis .Medical Decision Making 1998 , 18 ( 2 ) : S81-S92 .PubMed View Article .Carneiro AV : Estimating sample size in clinical studies : basic methodological principles .", "label": "", "metadata": {}, "score": "69.322365"}
{"text": "Thus the estimation of state boundary information is required .This boundary information may be in error due to the nature of unclear state boundaries and the lack of a reliable estimation approach .Therefore , in the discriminative training process , \" do n't cares \" were used to account for this lack of precision in determining state boundaries .", "label": "", "metadata": {}, "score": "69.370316"}
{"text": "The 10 published cancer data sets examined here are listed in Table 1 .They range in size from small ( few genes and/or a small class ) to large ( many genes with large classes ) , using a variety of microarray technologies .", "label": "", "metadata": {}, "score": "69.37558"}
{"text": "cDNA microarray data were log transformed ; Affymetrix data were used as is .Only the LiverCancer data set was subjected to advanced re - normalization procedures .The one data set with a few missing values ( LungStanford ) had them set to the appropriate class average value .", "label": "", "metadata": {}, "score": "69.49351"}
{"text": "San Francisco , CA : Morgan Kaufmann Publishers , 2000 .Joachims T. Text categorization with support vector machines : learning with many relevant features .In : Machine Learning : ECML-98 . 10th European Conference on Machine Learning .Heidelberg , Germany : Springer - Verlag , 1998:137 - 42 .", "label": "", "metadata": {}, "score": "69.56248"}
{"text": "( [ 15 ] ) and one is its modification described in [ 16 ] ; we refer to them collectively as Kim 's kernels .In this work , we investigate similarities and differences between these 13 kernels .Kernels .", "label": "", "metadata": {}, "score": "69.60791"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Affiliated with .Affiliated with .Abstract .Background .This paper describes and evaluates a sentence selection engine that extracts a GeneRiF ( Gene Reference into Functions ) as defined in ENTREZ - Gene based on a MEDLINE record .", "label": "", "metadata": {}, "score": "69.85898"}
{"text": "Running the Subjectivity Classifier .Assuming the data is in the directory POLARITY_DIR and the sentimentDemo.jar file exists ( if it does n't , run ant jar to create it ) , the demo may be run from the command line : . java -cp \" sentimentDemo.jar ; . /lingpipe-4.1.0 . jar \" SubjectivityBasic POLARITY_DIR . or through Ant with the target subjectivity and a system property determining the polarity directory 's location : .", "label": "", "metadata": {}, "score": "70.11801"}
{"text": "F .T .Intriguingly , NR1- binding is directly antagonized by Ca2+/ . A.d151.s1288.p1 .F .T .Immunoprecipitation assays also show a weak substoichiometric association of the ( TBP ) with , consistent with the previous report of a PTF - related complex ( SNAPc ) containing substoichiometric levels of TBP and a component ( SNAPc43 ) identical in sequence to the PTF gamma reported here .", "label": "", "metadata": {}, "score": "70.19426"}
{"text": "Uzuner \u00d6 , Goldstein I , Luo Y , Kohane I. Identifying patient smoking status from medical discharge records .J Am Med Inform Assoc .2008;15:xxx .Cortes C , Vapnik V. Support - vector networks .Machine Learning 1995;10:273 - 97 .", "label": "", "metadata": {}, "score": "70.30109"}
{"text": "Under the label \" TI \" , we find the article 's title and under \" AB \" its abstract , from which the GeneRiF is extracted .A preliminary study [ 21 ] showed that around 95 % of the GeneRiF snippets were extracted from the title or from the abstract of the corresponding scientific paper .", "label": "", "metadata": {}, "score": "70.48887"}
{"text": "However , normal LIKNON [ 6 ] discovers a 23 gene classifier with 2 of the tumor examples strongly mis - classified ( patient samples 108 and 109 in the raw data table from [ 1 ] ) .This suggests these 2 samples might have some sort of problem with them ( contamination with too much normal tissue , a very different type of cancer , different tumor stage , etc ) or are simply mislabeled .", "label": "", "metadata": {}, "score": "70.57353"}
{"text": "Journal of the Royal Statistical Society : Series D ( The Statistician ) 1997 , 46 ( 2 ) : 261 - 283 .View Article .Lenth RV : Some Practical Guidelines for Effective Sample Size Determination .The American Statistician 2001 , 55 ( 3 ) : 187 - 193 .", "label": "", "metadata": {}, "score": "70.588165"}
{"text": "Sung Y , Hwang S , Park M , Farooq M , Han I , Bae H , Kim J , Kim M : Glypican-3 is overexpressed in human hepatocellular carcinoma .Cancer Science 2003 , 94 : 259 - 262 .", "label": "", "metadata": {}, "score": "71.007065"}
{"text": "In Table 4 we show how the different kernels perform on the 521 ND pairs .We publish the same results for the PD , NE , and PE pairs , as well as for all four experiments for CL setting ( Tables 5 , 6 , 7 , 8 , 9 , 10 and 11 ) .", "label": "", "metadata": {}, "score": "71.044876"}
{"text": "Conclusion In this article , we described our system for identifying patient smoking status as part of the First Shared Task on Natural Language Challenges for Clinical Data .We reduce the problem of document classification to a sentence classi- fication task to discover the relevant smoking information from which the final document level assignment is derived .", "label": "", "metadata": {}, "score": "71.19433"}
{"text": "The only single gene separating T vs the rest is the same one identified by Yeoh , \" 38319_at CD3D antigen , delta polypeptide ( TiT3 complex ) Hs95327 \" .This gene has a high value in T and a low value otherwise ( Figure 5 ) and clearly separates the data .", "label": "", "metadata": {}, "score": "71.25073"}
{"text": "Tam VH , Kabbara S , Yeh RF , Leary RH : Impact of sample size on the performance of multiple - model pharmacokinetic simulations .Antimicrobial agents and chemotherapy 2006 , 50 ( 11 ) : 3950 - 3952 .PubMed View Article .", "label": "", "metadata": {}, "score": "71.57852"}
{"text": "Both the T vs the rest and E2A vs the rest splittings have single genes that perfectly separate T or E2A from the others .Five out of the 6 splittings have gene pairs , only Hyperdiploid vs the rest does not have any small gene sets .", "label": "", "metadata": {}, "score": "71.76764"}
{"text": "The top triple using the relabeled examples is .Liver cancer 3D plot of MT1E , ARHE and GPC3 .These 3 genes form a perfect classifier although the margin is small .Red are cancer samples .The web site contains an interactive plot .", "label": "", "metadata": {}, "score": "71.8287"}
{"text": "The real GIST data has more than 137000 pairs , some 50 times more than found in random data and 30 times more than when labels are randomly shuffled .It also has 74 single genes , where the random data yields none .", "label": "", "metadata": {}, "score": "72.26984"}
{"text": "Random data tests .The experimentally determined largest number of pairs found from 20 runs on random data .The total number of genes is 2000 .The second column is the number of pairs found for this 2000 gene set size , third column is the observed probability a single pair will be a perfect classifier ( \" observed \" / 2000 2 /2 ) .", "label": "", "metadata": {}, "score": "72.350235"}
{"text": "PubMed View Article .Eng J : Sample size estimation : how many individuals should be studied ?Radiology 2003 , 227 ( 2 ) : 309 - 313 .PubMed View Article .Walters SJ : Sample size and power estimation for studies with health related quality of life outcomes : a comparison of four methods using the SF-36 .", "label": "", "metadata": {}, "score": "72.36557"}
{"text": "\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Results of medical research studies are often contradictory or can not be reproduced .One reason is that there may not be enough patient subjects available for observation for a long enough time period .", "label": "", "metadata": {}, "score": "72.38721"}
{"text": "F . indirect .We conclude that Aip1p is a -associated protein that enhances the filament disassembly activity of cofilin and restricts localization to cortical actin patches .L.d35.s1.p1 .F . indirect .Our data demonstrate that the protein acts as a global repressor of the clpC operon , as well as other class III heat shock genes , by preventing unstressed transcription from either the sigmaB- or -dependent promoter and might be inactivated or dissociate under inducing stress conditions .", "label": "", "metadata": {}, "score": "72.540085"}
{"text": "T . coreference .We have identified a new TNF - related ligand , designated human ligand ( ) , and its human receptor ( hGITR ) , an ortholog of the recently discovered murine glucocorticoid - induced TNFR - related ( mGITR ) protein [ 4].", "label": "", "metadata": {}, "score": "72.83269"}
{"text": "Department of Biomedical Informatics , University of Utah .Department of Medicine , Beth Israel Deaconess Medical Center and Harvard Medical School .References .Mukherjee S , Tamayo P , Rogers S , Rifkin R , Engle A , Campbell C , Golub TR , Mesirov JP : Estimating dataset size requirements for classifying DNA microarray data .", "label": "", "metadata": {}, "score": "72.86784"}
{"text": "The reference and ground electrodes were placed on the left and right mastoids , respectively .Each experiment started with a pause of approximately 90 s , which is required for the EEG amplifier to stabilize its internal filters .During this period , the EEG signals were not recorded .", "label": "", "metadata": {}, "score": "72.88179"}
{"text": "The state training targets with \" do n't cares \" uses \" do n't care \" states for each phoneme model , so that one neural network trained with the targets can generate state dependent outputs .As illustrated in Figure 15 , the phone - specific training targets in Figure 14 are expanded to 144 dimensions by duplicating the phoneme specific target by the required number of the states .", "label": "", "metadata": {}, "score": "73.02228"}
{"text": "Competing interests .The authors declare that they have no competing interests .Authors ' contributions .QZ and RLF conceived the study .SK and RLF designed and implemented experiments .SK and RLF analyzed data and performed statistical analysis .", "label": "", "metadata": {}, "score": "73.154854"}
{"text": "Journal of the American Medical Informatics AssociationVolume 15 Number 1 Jan / Feb 2008 25 .Page 2 .We used the WEKA SVM implementation for our classification task .One of the MAWUI components provides a way to generate Weka - compliant data files suitable for training classifiers from features created by UIMA annotators .", "label": "", "metadata": {}, "score": "73.15528"}
{"text": "F .T . binds to an alternatively spliced exon of .B.d93.s0.p9 .F .T .Because H3 shares many structural features with histone H4 and is intimately associated with in the assembled nucleosome , we asked whether H3 has similar functions .", "label": "", "metadata": {}, "score": "73.30899"}
{"text": "S. Luck , An Introduction to the Event - Related Potential Technique , MIT Press , Cambridge , Mass , USA , 2005 .L. A. Farwell and E. Donchin , \" Talking off the top of your head : toward a mental prosthesis utilizing event - related brain potentials , \" Electroencephalography and Clinical Neurophysiology , vol .", "label": "", "metadata": {}, "score": "73.44389"}
{"text": "The purpose of this report is to provide open source software for multi - site clinical studies and to report on early uses of this application .At this time SHRINE implementations have been used for multi - site studies of autism co - morbidity , juvenile idiopathic arthritis , peripartum cardiomyopathy , colorectal cancer , diabetes , and others .", "label": "", "metadata": {}, "score": "73.448166"}
{"text": "T . enumeration .To determine the relationship between cell cycle regulation and differentiation , the spatiotemporal expression of cyclin A , cyclin B1 , cyclin D1 , the ( CKIs ) p27 and , and markers of differentiating podocytes in developing human kidneys was investigated by immunohistochemistry .", "label": "", "metadata": {}, "score": "73.547"}
{"text": "Cancer Research 2001 , 61 : 8624 - 8628 .PubMed .Cancer Cell 2002 , 1 : 133 - 143 .View Article PubMed .Welsh J , Sapinoso L , Su A , Kern S , Wang - Rodriguez J , Moskaluk C , Frierson J Jr , Hampton G : Analysis of gene expression identifies candidate markers and pharmacological targets in prostate cancer .", "label": "", "metadata": {}, "score": "73.59607"}
{"text": "Next we looked into the relationship between pair difficulty and number of entities in a sentence .In general , long sentences have more protein mentions , and the number of pairs increases quadratically with the number of mentions .We investigated the class distribution of pairs depending on the number of proteins in the sentence ( see Figure 7 ) .", "label": "", "metadata": {}, "score": "73.59627"}
{"text": "Potential en- hancements are the inclusion metadata information as fea- tures , e.g. , section headings , and experimenting with higher order SVMs , especially for temporal resolution .We also noticed interesting cases such as the following report , which contained the sentence \" He does drink alcohol three drinks per day , denies any current tobacco use . \" The final classification as provided by the challenge organizers is unknown despite the fact that based on the above sentence one would be tempted to assign the nonsmoker label .", "label": "", "metadata": {}, "score": "73.81538"}
{"text": "Various numbers of states and mixtures were evaluated as described in the following experiments .In all cases diagonal covariance matrices were used .For final evaluations of accuracy , some of these 48 monophones were combined to create the \" standard \" set of 39 phone categories .", "label": "", "metadata": {}, "score": "73.904015"}
{"text": "In primary human fibroblasts , upregulation of mRNA levels by oxysterols and retinoic acid increased the relative proportion of class 2 transcript compared to class 1 .Phorbol ester stimulated human macrophage - derived THP-1 cells increased the abundance of class 1 transcripts relative to class 2 .", "label": "", "metadata": {}, "score": "73.92354"}
{"text": "Additionally , we have raised the priority of the application , responsible for the visual stimulation and EEG data acquisition / processing , to the \" high \" level .Experiment Design .Twelve subjects , na\u00efve to BCI applications , participated in the experiments ( ten male and two female , aged 37 - 66 with an average age of 51.25 ) .", "label": "", "metadata": {}, "score": "73.928856"}
{"text": "Final Resolution : Discovering Smoking Status at the Document Level After each sentence in a document was classified into one of the unknown , past smoker , current smoker , smoker , or non- smoker categories , we applied additional logic to assign the final document - level smoking status .", "label": "", "metadata": {}, "score": "74.584595"}
{"text": "Kim S , Dougherty E , Shmulevich L , Hess K , Hamilton S , Trent J , Fuller G , Zhang W : Identification of combination gene sets for glioma classification .Mol Cancer Ther 2002 , 1 ( 13 ) : 1229 - 1236 .", "label": "", "metadata": {}, "score": "74.64111"}
{"text": "From the YeohALL data , E2A vs the rest , the best single gene PBX1 .This gene perfectly separates the classes with a wide margin and has higher values in E2A. Plus signs are E2A subtype samples .There are 386 pairs , the best pair is \" 35125_at RPS6 ribosomal protein S6 Hs408073 \" and \" 35974_at LRMP lymphoid - restricted membrane protein Hs124922 \" .", "label": "", "metadata": {}, "score": "74.77981"}
{"text": "A total of 1890 phonetically - diverse sentences ( SI sentences ) were selected from existing text sources to add diversity in sentence types and phonetic contexts .Each speaker read 3 of these sentences , with each text being read only by a single speaker .", "label": "", "metadata": {}, "score": "74.89711"}
{"text": "The targets were chosen as the 48 ( collapsed ) phones in the training data .The number of hidden layers was experimentally determined as well as the number of nodes included in those layers .However , most typically three hidden layers were used .", "label": "", "metadata": {}, "score": "74.965744"}
{"text": "For some of the very hardest pairs ( 60 PD and 60 ND ) , we manually investigated whether their difficulty is actually caused by annotation errors .We identified 23 PD and 28 ND pairs that we considered as incorrectly annotated ( for the list of the pair identifiers , see Table 13 ) .", "label": "", "metadata": {}, "score": "75.00894"}
{"text": "T .Furthermore , a bacterially expressed fusion protein ( glutathione S - transferase - p3722W ) also activated kinase in vitro .A.d199.s1701.p0 .F .T . in complex with a previously identified 90-kDa protein and designated protein .", "label": "", "metadata": {}, "score": "75.07517"}
{"text": "The primal form of SVM problem is a minimization problem with added regularization term .Formally , for a reproducing kernel Hilbert space .with kernel . , and a set of vectors .with corresponding labels ., find function .", "label": "", "metadata": {}, "score": "75.13979"}
{"text": "We conclude that our method for identifying the difficult and easy pairs of each class finds meaningful subsets of pairs .We identified 521 ND ( negative difficult ) , 190 PD ( positive difficult ) , 1510 NE ( negative easy ) and 219 PE ( positive easy ) pairs .", "label": "", "metadata": {}, "score": "75.65631"}
{"text": "However , a number of mammalian DNA repair proteins lack NLS clusters ; these proteins include ERCC1 , ERCC2 ( XPD ) , mouse RAD51 , and the HHR23B/ and subunits of XPC .B.d833.s0.p35 .T . functional .Within 1 hour of raising the concentration of calcium ions , , , alpha - catenin , beta - catenin , plakoglobin , vinculin and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "76.017494"}
{"text": "T . functional .The clone contains an open reading frame of 139 amino acid residues which shows greater than 40 % sequence identity in a 91 amino acid overlap to animal actin - depolymerizing factors ( ) , and destrin .", "label": "", "metadata": {}, "score": "76.20218"}
{"text": "Motor aphasia is known to deteriorate the visual verbal P300 latency more than the visual nonverbal one [ 34 ] , possibly explaining the inferior performance achieved with these patients .The effect on the P300 speller should be examined further in a study specifically designed for motor aphasia patients .", "label": "", "metadata": {}, "score": "76.23957"}
{"text": "T . functional .Since both and have been found enriched in ruffling membranes of animal cells , our in vitro findings may be relevant to the regulation of actin filaments in living cells .B.d546.s0.p20 .T . functional .Specific antibodies to isoforms ( SM1 , SM2 , ) , caldesmon , and alpha - smooth muscle actin and cDNAs for SMemb were used .", "label": "", "metadata": {}, "score": "77.02318"}
{"text": "12911_2011_460_MOESM1_ESM.pdf Additional file 1 : Appendix1 is a PDF file with the main lines of R code that implements curve fitting using inverse power models .( PDF 18 KB ) .12911_2011_460_MOESM2_ESM.pdf Additional file 2 : Appendix 2 is a PDF file that contains more details about the active learning methods used to generate the learning curves .", "label": "", "metadata": {}, "score": "77.18308"}
{"text": "Vancouver , BC , Canada ; 2002:569 - 576 .Collins M , Duffy N : Convolution kernels for natural language .In Proc . of Neural Information Processing Systems ( NIPS'01 ) .Vancouver , BC , Canada ; 2001:625 - 632 .", "label": "", "metadata": {}, "score": "77.28548"}
{"text": "T . enumeration .To determine the relationship between cell cycle regulation and differentiation , the spatiotemporal expression of cyclin A , cyclin B1 , cyclin D1 , the cyclin - dependent kinase inhibitors ( ) p27 and , and markers of differentiating podocytes in developing human kidneys was investigated by immunohistochemistry .", "label": "", "metadata": {}, "score": "77.29732"}
{"text": "Figure 1 : ( a ) Wireless 8 channel amplifier .( b )Locations of the electrodes on the scalp .( c ) USB stick receiver .( d ) Active electrode .We used an electrode cap with large filling holes and sockets for active Ag / AgCl electrodes ( ActiCap , Brain Products , Figure 1(d ) ) .", "label": "", "metadata": {}, "score": "77.35962"}
{"text": "Such small gene sets could indicate biomarkers and portend simple medical diagnostic tests .We recommend checking for small gene sets routinely .We find 4 gene pairs and many gene triples in the large hepatocellular carcinoma ( HCC , Liver cancer ) data set of Chen et al .", "label": "", "metadata": {}, "score": "77.793526"}
{"text": "Liver cancer pair PLAC8 verses BCAT2 .The two misclassified samples 108 and 109 are shown as squares .There are 3 other genes that form such pairs with PLAC8 .IMAGE:1472735 MT1E metallothionein 1E ( functional ) Hs74170 . and is shown in Figure 3 . GPC3 is a recently noted HCC cancer marker [ 16 ] , where it was elevated in 6 out of 7 patient samples .", "label": "", "metadata": {}, "score": "77.8079"}
{"text": "The supplement [ see Additional file 1 ] contains further discussion and the web site [ 14 ] provides access to all the results and plots .Liver cancer .The large liver cancer data set contains 2 classes ( tumor and normal ) with 181 patient samples and measurements for 6605 genes .", "label": "", "metadata": {}, "score": "77.86101"}
{"text": "Proc .ICASSP ' 97 , 1011 1014 , Munich , Germany , April 21 - 24 , 1997 .23 - S. A. Zahorian , A. M. Zimmer , F. Meng , 2002 Vowel Classification for Computer - based Visual Feedback for Speech Training for the Hearing Impaired , Proc .", "label": "", "metadata": {}, "score": "77.90404"}
{"text": "RLF drafted the manuscript .Both SK and QZ had full access to all of the data and made critical revisions to the manuscript .All authors read and approved the final manuscript .Authors ' Affiliations .Dep .Ing .", "label": "", "metadata": {}, "score": "77.98864"}
{"text": "Authors ' Affiliations .Life Sciences Division , Lawrence Berkeley National Laboratory .References .Chen X , Cheung S , So S , Fan S , Barry C , Higgins J , Lai K , Ji J , Dudoit S , Ng I , Van De Rijn M , Botstein D , Brown P : Gene expression patterns in human liver cancers .", "label": "", "metadata": {}, "score": "78.35556"}
{"text": "17 ] kernel does not use deep parsing information .Subtree ( ST ; [ 18 ] ) , subset tree ( SST ; [ 19 ] ) , partial tree ( PT ; [ 20 ] ) and spectrum tree ( SpT ; [ 21 ] ) kernels exploits the syntax tree representation of sentences .", "label": "", "metadata": {}, "score": "78.42838"}
{"text": "21 - S. A. Zahorian , D. Qian , A. J. Jagharghi , 1991 Acoustic - phonetic transformations for improved speaker - independent isolated word recognition .Proc .ICASSP'91 , 561 564 .Toronto , Ontario , Canada , May 14 - 17 , 1991 .", "label": "", "metadata": {}, "score": "79.00134"}
{"text": "F .T .PF4-dependent downregulation of cyclin E - cdk2 activity was associated with increased binding of the , p21(Cip1/WAF1 ) , to the -cdk2 complex .A.d157.s1329.p4 .F .T .Deletion analysis and binding studies demonstrate that a third enzyme , protein kinase C ( ) , binds at a site distinct from those bound by PKA or CaN. A.d60.s529.p0 .", "label": "", "metadata": {}, "score": "79.070755"}
{"text": "METHODS ( 00162303 )Of 250 renal cell carcinomas analyzed , 36 were classified as chromophobe renal cell carcinoma , representing 14 % of the group studied .PURPOSE ( 00156456 )In this study , we analyzed 250 renal cell carcinomas to a ) determine frequency of CCRC at our Hospital and b ) analyze clinical and pathologic features of CCRCs .", "label": "", "metadata": {}, "score": "79.1291"}
{"text": "F . indirect .These studies suggest that profilin and may control the generation of 3-OH phosphorylated phosphoinositides , which in turn may regulate the polymerization .I.d11.s28.p1 .F . coreference .The inhibitor U 71322 prevented the activation of phospholipase C by .", "label": "", "metadata": {}, "score": "79.27985"}
{"text": "Cape Town , South Africa ; 2010 .Maxwell SE , Kelley K , Rausch JR : Sample size planning for statistical power and accuracy in parameter estimation .Annual review of psychology 2008 , 59 : 537 - 563 .PubMed View Article .", "label": "", "metadata": {}, "score": "79.85289"}
{"text": "All authors read and approved the final manuscript .Authors ' Affiliations .Knowledge Management in Bioinformatics , Computer Science Department , Humboldt - Universit\u00e4t zu Berlin .Software Engineering Institute , \u00d3buda University .Department of Telecommunications and Telematics , Budapest University of Technology and Economics .", "label": "", "metadata": {}, "score": "80.16327"}
{"text": "The authors declare that they have no competing interests .Authors ' contributions .Conceived and designed the experiments : DT , IS , UL .Performed the experiments : DT , IS , PT .Analyzed the data : DT , IS , PT .", "label": "", "metadata": {}, "score": "80.20945"}
{"text": "Declarations .Acknowledgements .The authors wish to acknowledge CONICYT ( Chilean National Council for Science and Technology Research ) , MECESUP program , and Universidad de Concepcion for their support to this research .This research was funded in part by CHIR HIR 08 - 374 and VINCI HIR-08 - 204 .", "label": "", "metadata": {}, "score": "80.25129"}
{"text": "CCRC may have a slightly better prognosis than clear cell carcinoma , but outcome data are limited .PURPOSE : In this study , we analyzed 250 renal cell carcinomas to a ) determine frequency of CCRC at our Hospital and b ) analyze clinical and pathologic features of CCRCs .", "label": "", "metadata": {}, "score": "80.38933"}
{"text": "The authors are also grateful to Refet Firat Yazicioglu , Tom Torfs , and Chris Van Hoof from imec , Leuven , for providing us with the wireless EEG system .Finally , they would like to thank Prof. Philip Van Damme from the Experimental Neurology Department , Katholieke Universiteit Leuven , for his assistance in translating the patient diagnoses from Russian .", "label": "", "metadata": {}, "score": "80.390686"}
{"text": "1P59 crystal structure .Shown with the alignment hit to the liver cancer possible biomarker PLAC8 highlighted in strands at the top .Alignment generated from PFAM model pfam 04749.5 DUF 614 using the SAM HMM system and displayed in RASMOL .", "label": "", "metadata": {}, "score": "80.47667"}
{"text": "T . functional .In normal livers , E - cad , and beta - catenin , but not CD44s , CD44v5 , , CD44v7 - 8 , and CD44v10 , were expressed at the cell membrane of normal intrahepatic bile ducts .", "label": "", "metadata": {}, "score": "81.2697"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , , cadherins , alpha - catenin , , plakoglobin , vinculin and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "81.434616"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , , cadherins , alpha - catenin , beta - catenin , , vinculin and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "81.78116"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , , cadherins , alpha - catenin , beta - catenin , plakoglobin , vinculin and appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "81.82086"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , , cadherins , , beta - catenin , plakoglobin , vinculin and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "81.82562"}
{"text": "T . functional .In normal livers , E - cad , and beta - catenin , but not CD44s , , CD44v6 , CD44v7 - 8 , and CD44v10 , were expressed at the cell membrane of normal intrahepatic bile ducts .", "label": "", "metadata": {}, "score": "81.99359"}
{"text": "Web site .Declarations .Acknowledgements .The author acknowledges I Saira Mian for her support of this work .This work was supported by the National Institute on Aging , National Institute of Environmental Health Sciences , U.S. Department of Energy and California Breast Cancer Research Program .", "label": "", "metadata": {}, "score": "82.053856"}
{"text": "The authors thank Lesa Rohde for manual annotations and the JAMIA reviewers for their critiques .Correspondence : Guergana K. Savova , PhD , Biomedical Informatics Research , Mayo Clinic , 200 First Street SW , Rochester , MN 55902 ; e - mail : ? mailto:savova.guergana@mayo.edu ? .", "label": "", "metadata": {}, "score": "82.061066"}
{"text": "PE : .Positive easy .dep : . dependent .nn : . noun compound modifier .appos : . appositional modifier .conj : . conjunct .Declarations .Acknowledgements .D Tikk was supported by the Alexander von Humboldt Foundation .", "label": "", "metadata": {}, "score": "82.110275"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , , cadherins , alpha - catenin , beta - catenin , plakoglobin , and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , paxillin and talin , did not .", "label": "", "metadata": {}, "score": "82.12953"}
{"text": "F . indirect .In Acanthamoeba polymerization is regulated , at least in part , by profilin , which binds to monomers , and by capping protein , which both nucleates polymerization and blocks monomer addition at the ' barbed ' end of the filament .", "label": "", "metadata": {}, "score": "82.38107"}
{"text": "6 , no.3 , pp .21 - 45 , 2006 .Comparison of Classification Methods for P300 Brain - Computer Interface on Disabled Subjects .Laboratorium voor Neuro- en Psychofysiologie , K.U.Leuven , Campus Gasthuisberg , O&N 2 , Bus 1021 , Herestraat 49 , B-3000 Leuven , Belgium .", "label": "", "metadata": {}, "score": "82.46263"}
{"text": "The mean age of the initial 2182 patients recruited was 70.4 \u00b1 11.2 years , 62.6 % were men and 97.6 % were whites .The prevalences of AVD phenotypes were : carotid artery stenosis 48 % , abdominal aortic aneurysm 21 % and peripheral arterial disease 38 % .", "label": "", "metadata": {}, "score": "82.46601"}
{"text": "F .T .In extracts from mouse brain , and profilin II can form complexes with regulators of endocytosis , synaptic vesicle recycling and assembly .A.d141.s1189.p0 .F .T .The cyclin - dependent kinase associates with , D , and E and has been implicated in the control of the G1 to S phase transition in mammals .", "label": "", "metadata": {}, "score": "83.31625"}
{"text": "The triples found with the original labelings make no errors .Based on such data , one can imagine a simple few - gene diagnostic test based on these pairs and triples .It is perhaps not surprising that a placental gene is associated with liver cancer .", "label": "", "metadata": {}, "score": "83.679146"}
{"text": "PubMed .New England Journal of Medicine 2001 , 344 : 539 - 548 .View Article PubMed .Jazaeri A , Yee C , Sotiriou C , Brantley K , Boyd J , Liu E : Gene expression profiles of BRCA1-linked , BRCA2-linked , and sporadic ovarian cancers .", "label": "", "metadata": {}, "score": "83.706535"}
{"text": "T . functional .Within 1 hour of raising the concentration of calcium ions , integrins , cadherins , alpha - catenin , beta - catenin , plakoglobin , vinculin and alpha - actinin appeared to accumulate at cell - cell borders , whereas the focal contact proteins , and , did not .", "label": "", "metadata": {}, "score": "83.84427"}
{"text": "T . functional .In normal livers , , and beta - catenin , but not CD44s , CD44v5 , CD44v6 , CD44v7 - 8 , and CD44v10 , were expressed at the cell membrane of normal intrahepatic bile ducts .B.d267.s0.p18 .", "label": "", "metadata": {}, "score": "84.45306"}
{"text": "Competing interests .The authors declare that they have no competing interests .Authors ' contributions .Authors ' Affiliations .University and Hospitals of Geneva .Swiss - Prot Research Group , Swiss Institute of Bioinformatics .References .Chinchor N : MUC-7 Named - Entity task Definition 1997 .", "label": "", "metadata": {}, "score": "85.15764"}
{"text": "Our system was built on IBM 's UIMA , which is a framework that facilitates the construction of reusable text analysis components ( see Figure 2 , available as a JAMIA on - line data Affiliations of the authors : Biomedical Informatics , Mayo Clinic , Rochester , MN .", "label": "", "metadata": {}, "score": "85.662384"}
{"text": "Tumors were classified according to well - established histologic criteria to determine stage of disease ; the system proposed by Robson was used .RESULTS : Of 250 renal cell carcinomas analyzed , 36 were classified as chromophobe renal cell carcinoma , representing 14 % of the group studied .", "label": "", "metadata": {}, "score": "85.720695"}
{"text": "F .T .Actin - binding proteins such as and gelsolin bind to phosphatidylinositol ( PI ) 4,5-bisphosphate ( PI 4,5-P2 ) and regulate the concentration of monomeric .A.d39.s340.p0 .F . indirect .Chloramphenicol acetyltransferase assays in F9 cells showed that suppresses transactivation by /c - Jun but not by c - Jun / c - Fos heterodimers , consistent with the reported function of QM / Jif-1 .", "label": "", "metadata": {}, "score": "86.40927"}
{"text": "T . enumeration .Quantitation of the appearance of X22 banding in primary cultures of myotubes indicates that it precedes that of other myofibrillar proteins and that assembly takes place in the following order : X22 , titin , , , and desmin .", "label": "", "metadata": {}, "score": "86.57758"}
{"text": "F .T .Three actin - associated proteins , actin - binding protein , , and profilin , influence gelation , solation , and polymerization , respectively , of in vitro . B.d639.s0.p0 .F .T .The main inhibitory action of p27 , a cyclin - dependent kinase inhibitor ( ) , arises from its binding with the cyclin E/ ( Cdk2 ) complex that results in G(1)-S arrest .", "label": "", "metadata": {}, "score": "86.919334"}
{"text": "B.d506.s0.p8 .Quantitation of the appearance of X22 banding in primary cultures of myotubes indicates that it precedes that of other myofibrillar proteins and that assembly takes place in the following order : , titin , , actin , and desmin .", "label": "", "metadata": {}, "score": "87.25215"}
{"text": "The 4 genes in pairs with PLAC8 are .IMAGE:669379 GLCCI1 glucocorticoid induced transcript 1 . IMAGE:590591 ADCY6 adenylate cyclase 6 .IMAGE:260259 Transcribed sequence with moderate similarity to protein sp : P39188 ( H.sapiens ) .IMAGE:756490 BCAT2 branched chain aminotransferase 2 , mitochondrial .", "label": "", "metadata": {}, "score": "87.39139"}
{"text": "The YeohALL data set is a large multi - class pediatric acute lymphoblastic leukemia cancer set .The original data contains 7 classes , we use only 6 of them ( we do not use their \" other \" class ) .", "label": "", "metadata": {}, "score": "87.451355"}
{"text": "F .T .PF4-dependent downregulation of cyclin E - cdk2 activity was associated with increased binding of the , p21(Cip1/WAF1 ) , to the cyclin E- complex .B.d814.s0.p26 .F .T .We have shown that the Bni1p and Bnr1p are potential targets of the Rho family small GTP - binding proteins and bind to an actin - binding protein , , at their proline - rich FH1 domains to regulate reorganization of the actin cytoskeleton in the yeast Saccharomyces cerevisiae .", "label": "", "metadata": {}, "score": "87.90628"}
{"text": "Classification results on the 1510 NE pairs with CV evaluation ( in decreasing order according to the successfully classified pairs ) .Classification results on the 1510 NE pairs with CL evaluation ( in decreasing order according to the successfully classified pairs ) .", "label": "", "metadata": {}, "score": "87.92796"}
{"text": "F . indirect .Production of about 1 h earlier than normal does affect Spo0A , which when phosphorylated is an activator of transcription .A.d78.s669.p2 .F . indirect .Our data suggest that inhibits the interactions of LIGHT with HVEM / / and LTbetaR , thereby suppressing LIGHT - mediated HT29 cell death .", "label": "", "metadata": {}, "score": "88.26421"}
{"text": "( 1 Only 180 of the estrogen triples do not contain the Estrogen Receptor 1 gene .2 Ovarian BRCA B1 vs Sporadic triples were run with a reduced set of genes , variation filtered to 2109 .3 Ovarian BRCA B2 vs Sporadic triples were run with a reduced set of genes , variation filtered to 2097 .", "label": "", "metadata": {}, "score": "88.61415"}
{"text": "\" 1287_at ADPRT ADP - ribosyltransferase ( NAD+ ; poly ( ADP - ribose ) polymerase ) Hs177766 \" .\" 430_at NP nucleoside phosphorylase Hs75514 \" .\" 32063_at PBX1 pre - B - cell leukemia transcription factor 1 Hs408222 \" .", "label": "", "metadata": {}, "score": "88.76888"}
{"text": "Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable . \"Important study variables , such as smoking status[57 ] , [ 58 ] , co - morbidities[59 ] , and family disease history[60 ] are often missing from the coded record and more likely to appear in physician notes .", "label": "", "metadata": {}, "score": "88.86034"}
{"text": "Classification results on the 521 ND pairs with CL evaluation ( in decreasing order according to the number of successfully classified pairs ) .Classification results on the 190 PD pairs with CV evaluation ( in decreasing order according to the number of successfully classified pairs ) .", "label": "", "metadata": {}, "score": "89.5177"}
{"text": "CCRC may have a slightly better prognosis than clear cell carcinoma , but outcome data are limited .RESULTS ( 00155338 ) Robson staging was possible in all cases , and 10 patients were stage 1 ) 11 stage II ; 10 stage III , and five stage IV .", "label": "", "metadata": {}, "score": "89.76331"}
{"text": "Thus , the outcome of a comparison for healthy subjects might not be valid for patients .In this paper , we report on tests performed on a group of ( partially ) disabled patients suffering from amyotrophic lateral sclerosis ( ALS ) , middle cerebral artery ( MCA ) stroke , and subarachnoid hemorrhage ( SAH ) .", "label": "", "metadata": {}, "score": "89.98853"}
{"text": "View Article PubMed .Hum Mol Genet 2003 , 12 ( 17 ) : 2191 - 2199 .View Article PubMed .Cancer Res 2003 , 63 ( 1 ) : 60 - 66 .PubMed .Int J Oncol 2003 , 23 ( 3 ) : 617 - 625 .", "label": "", "metadata": {}, "score": "92.95059"}
{"text": "T . enumeration .Quantitation of the appearance of X22 banding in primary cultures of myotubes indicates that it precedes that of other myofibrillar proteins and that assembly takes place in the following order : X22 , titin , , actin , and desmin .", "label": "", "metadata": {}, "score": "93.26034"}
{"text": "F .T .The proteins purified from cells infected with EC12 or 22W viruses activated kinase from skeletal muscle in vitro .B.d180.s0.p0 .F .T . signal transduction is mediated by a complex of intracellular signaling molecules including , TRAF2 , FADD , and FLICE .", "label": "", "metadata": {}, "score": "93.45863"}
{"text": "PubMed .Copyright .\u00a9 Grate .This article is published under license to BioMed Central Ltd. Online Boosting Algorithm Based on Two - Phase SVM Training . 1 Department of Information Processing , Tokyo Institute of Technology , Tokyo 152 - 8550 , Japan 2 Imaging Science and Engineering Laboratory , Tokyo Institute of Technology , Tokyo 152 - 8550 , Japan .", "label": "", "metadata": {}, "score": "93.47232"}
{"text": "Smoking status was ascertained by NLP as described previously 6 and smokers were defined as either current or past smokers .\" [ Show abstract ] [ Hide abstract ] ABSTRACT : Background : Atherosclerotic vascular disease ( AVD ) , a leading cause of morbidity and mortality , is increasing in prevalence in the developing world .", "label": "", "metadata": {}, "score": "93.89384"}
{"text": "Robson staging was possible in all cases , and 10 patients were stage 1 ) 11 stage II ; 10 stage III , and five stage IV .The average follow - up period was 4 years and 18 ( 53 % ) patients were alive without disease .", "label": "", "metadata": {}, "score": "94.6882"}
{"text": "AB - ( ... ) .The longest ( class 1 ) transcripts were abundant in adult brain and fetal tissues .Class 2 transcripts predominated in most other tissues .The shortest ( class 3 ) transcripts were present mainly in adult liver and lung .", "label": "", "metadata": {}, "score": "94.87044"}
{"text": "B-10/1 - 2010 - 0009 .PT was supported by the German Ministry for Education and Research ( BMBF grant no 0315417B ) .A part of this work was done while D. Tikk was with the Budapest University of Technology and Economics ( Hungary ) .", "label": "", "metadata": {}, "score": "98.42639"}
{"text": "F .T . associates with the ( PDGF ) receptor after ligand stimulation , and binding of SHPTP2 to this receptor promotes tyrosine phosphorylation of SHPTP2 .B.d357.s0.p1 .F .T .( beta ) chains , for example , interact with actin - binding proteins ( e.g. and filamin ) , which form mechanical links to the cytoskeleton .", "label": "", "metadata": {}, "score": "98.55726"}
{"text": "Acknowledgments .NVM is supported by the Flemish Regional Ministry of Education ( Belgium ) ( GOA 10/019 ) .NC is supported by the European Commission ( IST-2007 - 217077 ) .AC is supported by a specialization Grant from the Agentschap voor Innovatie door Wetenschap en Technologie ( IWT , Flemish Agency for Innovation through Science and Technology ) .", "label": "", "metadata": {}, "score": "102.419014"}
{"text": "F . functional .Furthermore , the deletion of suppresses the temperature - sensitive growth defect of sac6 , a mutant in yeast , supporting a role for synaptojanin family members in actin function .Entities ( in the pair ) are highlighted with bold typeface .", "label": "", "metadata": {}, "score": "104.393196"}
{"text": "164 - 169 , 1998 .View at Google Scholar", "label": "", "metadata": {}, "score": "106.439835"}
{"text": "PubMed .Proc Natl Acad Sci 2001 , 98 : 13784 - 13789 .View Article PubMed .Nature Medicine 2002 , 8 : 816 - 824 .PubMed .Nature 2000 , 406 : 536 - 540 .View Article PubMed .", "label": "", "metadata": {}, "score": "106.89634"}
{"text": "Copyright \u00a9 2011 Nikolay V. Manyakov et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "116.748886"}
{"text": "Copyright \u00a9 2012 Vsevolod Yugov and Itsuo Kumazawa .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "118.89141"}
