{"text": "These operations can be combined with a tree - based language model to produce a noisy channel translation search .One such system is set out in Yamada and Knight , A SYNTAX - BASED STATISTICAL TRANSLATION MODEL , Proceedings of the ACL ( 2001 ) .", "label": "", "metadata": {}, "score": "30.68463"}
{"text": "In other words , a baseline SMT system is used to produce an N - best list of translations , and then a group of models , possibly including syntactic models , is used to rerank the output .One such system is described in Och et al . , A SMORGASBORD OF FEATURES FOR STATISTICAL MACHINE TRANSLATION , Proceedings of the Joint HLT / NAACL Conference ( 2004 ) .", "label": "", "metadata": {}, "score": "31.45796"}
{"text": "Beginning with a word alignment , all contiguous source and target word sequences ( contiguous on the surface strings ) are gathered as possible phrase translation pairs or alignment templates .These pairs are collected into a single translation repository .Then , a translation probability is associated with each distinct pair by using a maximum likelihood estimation model such as that set out in Vogel et al .", "label": "", "metadata": {}, "score": "31.937334"}
{"text": "This produces a list of matching treelet translation pairs and thus a list of source language treelets .However , it will not be known , for certain , what order those treelets are to be connected together to form the target language dependency tree .", "label": "", "metadata": {}, "score": "33.32281"}
{"text": "Thus , at this point , the full set of models shown in .FIG .2 have now been trained , along with the treelet translation pair database 204 .These models can now be used by a translation decoder in order to generate runtime translations for input text fragments .", "label": "", "metadata": {}, "score": "33.776707"}
{"text": "In one illustrative embodiment , the treelets retain node - level alignment information from training time .Each treelet translation is divided into a disjoint set of paired source and target minimal translation units ( MTUs ) , where each MTU corresponds to the minimum unit of alignment .", "label": "", "metadata": {}, "score": "34.144863"}
{"text": "If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them .The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work .", "label": "", "metadata": {}, "score": "34.59536"}
{"text": "For instance , the order model score for a node can not be calculated until translations for its child nodes have been selected and the order of those children has been determined .Similarly , the target language model can only apply once a complete candidate has been obtained such that the surface string can be read off of the dependency tree .", "label": "", "metadata": {}, "score": "35.54409"}
{"text": "This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word - to - word alignments from an MT system , and syntactic structure from parse - trees of source and target language sentences .", "label": "", "metadata": {}, "score": "35.6784"}
{"text": "One such system is set out in Alashawi , et al . , LEARNING DEPENDENCY TRANSLATION MODELS AS COLLECTIONS OF FINITE - STATE HEAD TRANSDUCERS , Computational Linguistics , 26(1):45 - 60 ( 2000 ) .A tangential line of research as formed at the confluence of dependency transducers and multi - text grammars .", "label": "", "metadata": {}, "score": "35.695663"}
{"text": "It will be appreciated that a source string may be translated as multiple different target strings where multiple alternative bi - fragments exist .The exemplary translation system may employ a log - linear statistical translation model for identifying statistically probable translations ( step S 112 D ) .", "label": "", "metadata": {}, "score": "35.712273"}
{"text": "While this is significantly faster , it does not always produce the best orderings .In another embodiment , the decoder 608 is made significantly faster by reducing the number of translation combinations that need to be attempted .The number of translations using a given treelet translation pair is exponential in the number of sub - trees of the input not covered by that treelet pair .", "label": "", "metadata": {}, "score": "36.33493"}
{"text": "Our expe ... \" .In this paper , we propose a novel string - todependency algorithm for statistical machine translation .With this new framework , we employ a target dependency language model during decoding to exploit long distance word relations , which are unavailable with a traditional n - gram language model .", "label": "", "metadata": {}, "score": "36.515224"}
{"text": "The next step is to identify unknown target language words that are surrounded by aligned substrings .The source language word that corresponds to the \" well - aligned \" unknown is considered to be a possible translation .A suffix tree stores in linear space all suffixes of a given string .", "label": "", "metadata": {}, "score": "37.397774"}
{"text": "We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human translation data . ...English sentences with a state - of - the - art statistical parser ( Collins , 1999 ) .", "label": "", "metadata": {}, "score": "37.706894"}
{"text": "Our results provide the first known empir - ical evidence that lexical semantics are in - deed useful for SMT , despite claims to the contrary . \" ...In this paper , we propose a novel string - todependency algorithm for statistical machine translation .", "label": "", "metadata": {}, "score": "37.728455"}
{"text": "In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] .This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language .", "label": "", "metadata": {}, "score": "37.95437"}
{"text": "The fact that n has outgoing edge e indicates there is a mismatch on the subsequent words of those two sequences .Thus , in order to extract all aligned substrings , the system 400 traverses the BST on edges labeled with word pairs , and extract all paths that end either at the leaves or at nodes that have outgoing edges labeled only with source language words .", "label": "", "metadata": {}, "score": "38.034233"}
{"text": "In one embodiment , the training is done using decision trees .It should also , of course , be noted that different features or additional features could be used as well .However , in any case , the order model is trained using a machine learning process and is trained to predict the best way to order target nodes in a translation dependency tree , given a source node of a source dependency tree .", "label": "", "metadata": {}, "score": "38.624367"}
{"text": "A flow diagram illustrating one embodiment of the same exhaustive search is illustrated by the flow diagram shown in .FIG .14D which will now be discussed .The decoder 608 starts by looking at each source node separately in a bottom - up manner and attempts to find the best translation for the whole sub - tree rooted at that source node .", "label": "", "metadata": {}, "score": "38.883595"}
{"text": "This is because the actual score for that incomplete candidate , once it is completed , is guaranteed to be less than or equal to its optimistic estimated score , and hence less than that of the already completed candidate .Instead , it is searching the space of treelet mappings ( or matching treelet translation pairs 618 ) extracted by component 604 from treelet translation pair database 204 and identified as matching portions of the source language dependency tree 614 created by component 606 from input 610 .", "label": "", "metadata": {}, "score": "38.97534"}
{"text": "This is shown in .FIG .6C .Next , the remaining words in the matching set of words on the target side , are taken in - turn , and made either pre - modifiers or post - modifiers of the right - most word \" pas \" based , again , on the original word order in the target surface string .", "label": "", "metadata": {}, "score": "39.19021"}
{"text": "This is indicated by block 720 in .FIG .14D .This process is continued until decoder 608 traverses the entire source language dependency tree to its root , and the N - best translations are then output , as desired .", "label": "", "metadata": {}, "score": "39.74539"}
{"text": "In general , phrase - based models proposed so far only deal with multi - word units that are sequences of contiguous words on both the source and the target side .In many translation systems , the quality of the resulting translation is assessed by means of a statistical translation model , which estimates the probability of observing some target - language segment of the text as the translation of the given source - language input .", "label": "", "metadata": {}, "score": "40.250664"}
{"text": "The demonstrated translation quality of these systems has not been on par with the best SMT systems .A more recent theoretical approach has been presented using multi - text grammars and generalized multi - text grammars and attempts to generalize the inversion transduction grammar approach by allowing non - contiguous translations and loosening the reordering constraints .", "label": "", "metadata": {}, "score": "40.289566"}
{"text": "This means that a recursive call to translate an uncovered sub - tree will limit the size of the returned N - best list according to the number of uncovered sub - trees in the current treelet .If a treelet covers very little of the tree , and will require several recursive calls , then those calls return smaller N - best lists .", "label": "", "metadata": {}, "score": "41.219334"}
{"text": "The basic idea is to consider alignment and translation as simultaneous parses of the source and target language .Two types of binary branching rules are allowed .Either the source and target constituents are produced in the same order , or the source and target constituents are produced in reverse order .", "label": "", "metadata": {}, "score": "41.272034"}
{"text": "In one embodiment , a word alignment of a word segmented source language sentence and a word segmented target language sentence is represented as a graph in the following manner .Each distinct source language word is a vertex in the graph , and each distinct target language word is a distinct vertex in the graph .", "label": "", "metadata": {}, "score": "41.42939"}
{"text": "Another prior art approach for employing dependency information in translation is by translating via paths in the dependency tree .One such system is described in Lin , A PATH - BASED TRANSFER MODEL FOR MACHINE TRANSLATION , Proceedings of COLLING ( 2004 ) .", "label": "", "metadata": {}, "score": "41.466568"}
{"text": "The distribution dist is used to model the conditional probability of getting specific sizes for each of the k source and l target gaps conditional on the actual use of the given gapped bi - phrase in a ( sentence - level ) translation .", "label": "", "metadata": {}, "score": "41.519962"}
{"text": "The decoder also accesses a table of treelet translation pairs and returns a target dependency tree based on the source dependency tree , based on access to the table of treelet translation pairs , and based on the application of the statistical models .", "label": "", "metadata": {}, "score": "41.804558"}
{"text": "Each of the target hypotheses includes text fragments selected from the second language .The target hypothesis is evaluated with a translation scoring function which scores the target hypothesis according to a plurality of feature functions .At least one of the feature functions includes a gap size scoring feature which favors hypotheses with statistically more probable gap sizes over hypotheses with statically less probable gap sizes .", "label": "", "metadata": {}, "score": "41.84554"}
{"text": "This vector can be translated into the other language , since we already know the translations of the seed words are already known .The lexicon builder 125 can search for the best matching context vector in the target language , and decide upon the corresponding word to construct a word mapping .", "label": "", "metadata": {}, "score": "41.99954"}
{"text": "The search terminates when all remaining hypotheses on the stack are complete , i.e. , they cover all the source words .Existing decoding procedures assume bi - fragments to be contiguous .The following method adapts a beam search procedure to handle elastic bi - fragments .", "label": "", "metadata": {}, "score": "42.179955"}
{"text": "We introduce a novel approach to syntactic reordering .This approach provides better exploitation of the information in the reordering rules and eliminates problematic biases of previous approaches .Although the approach is examined within a pre - translation reordering framework , it easily extends to other frameworks .", "label": "", "metadata": {}, "score": "42.209995"}
{"text": "We introduce a novel approach to syntactic reordering .This approach provides better exploitation of the information in the reordering rules and eliminates problematic biases of previous approaches .Although the approach is examined within a pre - translation reordering framework , it easily extends to other frameworks .", "label": "", "metadata": {}, "score": "42.209995"}
{"text": "We introduce a novel approach to syntactic reordering .This approach provides better exploitation of the information in the reordering rules and eliminates problematic biases of previous approaches .Although the approach is examined within a pre - translation reordering framework , it easily extends to other frameworks .", "label": "", "metadata": {}, "score": "42.209995"}
{"text": "Translations are produced by combining together such bi - fragments .A search for the optimal combination may be performed by means of a specially adapted , multiple - stack beam search .Within this search procedure , the potential of each candidate translation is evaluated by means of a statistical model of translation .", "label": "", "metadata": {}, "score": "42.308983"}
{"text": "From these , some high - quality lexical entries can be learned , but there will always be many words that are missing .These may be learned using the described methods .FIG .4 shows a system 400 for building a translation lexicon according to another embodiment .", "label": "", "metadata": {}, "score": "42.586494"}
{"text": "The step of extracting the matching treelet translation pairs is indicated by block 620 in .FIG .13 .Decoder 608 receives the source language dependency tree 614 and the matching treelet translation pairs 618 and generates translation tree hypotheses and scores each with the models 206 , 208 , 210 , and 212 as weighted by model weights 602 and outputs the top N translations 621 .", "label": "", "metadata": {}, "score": "42.663757"}
{"text": "Additional variables can be introduced in such a log - linear model , so as to account for selected characteristics , and the feature functions can be extended accordingly .For example , if the translation t 1 J of s 1 I is produced using bi - fragments as described here , the model can be modified to take into account the actual set of decisions that lead to t 1 J .", "label": "", "metadata": {}, "score": "42.729107"}
{"text": "The non - contiguous bi - fragments are expressed in terms of elastic bi - phrases .Each elastic bi - phrase is considered to be a pair of gapped phrases , one in the source language and one in the target language .", "label": "", "metadata": {}, "score": "42.737816"}
{"text": "For instance , instead of keeping the full list of translation candidates for each source node , only the top - scoring subset of candidates may be maintained .In addition , in order to limit the number of ordering operations conducted by decoder 608 , decoder 608 can check to see if a given word - set has been previously ordered by the decoder prior to beginning the ordering process .", "label": "", "metadata": {}, "score": "42.752686"}
{"text": "Once the root has been encountered , the decoder 608 will have found a best translation for the entire text fragment represented by that source tree .Specifically , decoder 608 first selects the lowest source node and identifies all treelet translation pairs rooted at that source node .", "label": "", "metadata": {}, "score": "42.855698"}
{"text": "A GRAMMAR FORMALISM FOR SYNTAX BASED STATISTICAL MT , In COLLING 2004 : Workshop on Recent Advances in Dependency Grammars ( 2004 ) .In yet another prior art attempt , in order to improve the problems with fluency in an SMT system , a parser has been employed in the target language .", "label": "", "metadata": {}, "score": "42.924862"}
{"text": "This new hypothesis is then pushed onto a stack of hypotheses in need of being extended .To reduce the search space , partial translation hypotheses are scored , and the stack is periodically pruned based on these scores , i.e. , only the most promising hypotheses are retained on the stack .", "label": "", "metadata": {}, "score": "42.93966"}
{"text": "Working on a string of contiguous words in the source language , such as a sentence of the input text , the translation module 22 accesses the library 20 to retrieve fragments in the target language corresponding to input fragments in the source language text string .", "label": "", "metadata": {}, "score": "42.941345"}
{"text": "FIG .3 .One embodiment of the dependency tree projection component progresses in the following manner .( 1 )One - to - Many alignments : For all distinct sets of target words T i and T k where .", "label": "", "metadata": {}, "score": "43.10839"}
{"text": "Processing again continues at block 406 where treelet pair extractor 224 enumerates all of the source language treelets of size 3 .Those are indicated at 424 , 426 and 428 in .FIG .8I .Treelets 424 and 426 have translations which contain all of the alignments from the source language words .", "label": "", "metadata": {}, "score": "43.228153"}
{"text": "Using the example discussed thus far , .FIG .4D shows the source language portion of the aligned text on the top and the target language portion of the aligned text on the bottom with the correspondences between words annotated by lines between the two .", "label": "", "metadata": {}, "score": "43.255142"}
{"text": "These conversion solutions use a proprietary model to save the document content along with all structural , layout and mark - up instructions .Although the document content is converted into a standard structure format , this solution is often insufficient from a user 's point of view , as it addresses not the document content with associated semantics , but instead addresses how the document content is to be visualized .", "label": "", "metadata": {}, "score": "43.295082"}
{"text": "The correspondences will illustratively be indicative of a determination that the target language aligned portion is a translation of the source language portion to which it is aligned .In one embodiment , the unsupervised word alignment component uses only the segmented versions of the source language corpus and target language corpus as input .", "label": "", "metadata": {}, "score": "43.29683"}
{"text": "The decoder may buffer the parts of elastic phrases in the target text which are not yet consumed , and may use a heuristic cost function that takes into account these buffered parts in assessing the value of a partial translation hypothesis .", "label": "", "metadata": {}, "score": "43.354103"}
{"text": "FIG .2 .The step of performing word alignment between the word segmented source language dependency structure and the word segmented target language text is indicated by block 260 in .FIG .3 .Therefore , in word - aligned parallel corpus 258 , the source and target language words have associations annotated in some systematic form .", "label": "", "metadata": {}, "score": "43.576485"}
{"text": "This is generally achievable where the target documents produced by human annotators respect the reading order of the source documents .Furthermore , reading order reconstruction tools are widely available .Similarity between Document Leaves .The present method addresses the issue of the acceptability of imperfect matches by assigning a level of similarity for two document leaves ( Step S 108 A ) .", "label": "", "metadata": {}, "score": "43.827374"}
{"text": "In one embodiment , the decoder performs as follows .The current partial translation is represented as : ( 1 ) a partial coverage of the words of the source sentence , and ( 2 ) a partial target sentence representation .", "label": "", "metadata": {}, "score": "43.833275"}
{"text": "These provide a good reflection of the main reordering issues of a given language pair .We examine the influence of several parameters that may have influence on the quality of the rules learned .Finally , we provide a new approach for improving automatic word alignment .", "label": "", "metadata": {}, "score": "43.879646"}
{"text": "These provide a good reflection of the main reordering issues of a given language pair .We examine the influence of several parameters that may have influence on the quality of the rules learned .Finally , we provide a new approach for improving automatic word alignment .", "label": "", "metadata": {}, "score": "43.879646"}
{"text": "These provide a good reflection of the main reordering issues of a given language pair .We examine the influence of several parameters that may have influence on the quality of the rules learned .Finally , we provide a new approach for improving automatic word alignment .", "label": "", "metadata": {}, "score": "43.879646"}
{"text": "FIG .3 shows results of the English - German implementation .\" Entries \" indicate the number of correct lexicon entries that were added to a seed lexicon of 1337 identically spelled words , and \" Corpus \" indicates how well the resulting translation lexicon performs compared to the actual word - level translations in a parallel corpus .", "label": "", "metadata": {}, "score": "43.906883"}
{"text": "For example , it may be distinguished by a special notation for convenience .In order to determine \u03bc for a gap size on the source side , consider s , and observe all sentence pairs in the training corpus which use a bi - phrase of the form ( s , t ) .", "label": "", "metadata": {}, "score": "43.967407"}
{"text": "In practice , it is to be expected that contiguous bi - fragments ( the gap size is 0 ) are somewhat more reliable than non - contiguous ones .Therefore , one aspect that the translation scoring function may take into account is the amount of \" non - contiguity \" that goes into any given translation .", "label": "", "metadata": {}, "score": "43.9731"}
{"text": "The problem here is to align the words and linguistic structures of sentences of different languages .This problem can also be seen as sequence alignment , even though the constraint of order preservation is in general not satisfied .Brown et al . describe a method that is solely based on the sentence lengths , but they consider only at maximum two sentences of one language corresponding to one of the other .", "label": "", "metadata": {}, "score": "43.998672"}
{"text": "No .11/315,043 ( hereinafter Cancedda , et al . ) discloses a computer implemented system and method for translating source text from a first language to target text in a second language different from the first language and to a method of training such a translation system .", "label": "", "metadata": {}, "score": "44.031197"}
{"text": "In one aspect , it has the ability to make use of elastic fragments of source and/or target language text .In another aspect , it includes components within a statistical translation model to model variable gap sizes .In yet another aspect , it includes procedures for efficiently computing the potential of partial translations with discontinuities with regard to the statistical model .", "label": "", "metadata": {}, "score": "44.105865"}
{"text": "A limit may be placed on the number of uncovered source language words to the left of the last covered word .Note that if the constraint completely disallows uncovered source language words to the left of the last covered word , the decoder works as a monotone decoder , meaning that reorderings are not allowed between the source and the target .", "label": "", "metadata": {}, "score": "44.137756"}
{"text": "We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .This method requires a source - language dependency parser , target language word segmentation and an unsupervised word alignment component .", "label": "", "metadata": {}, "score": "44.441235"}
{"text": "At each step , the highest scoring candidate is extracted from the incomplete queue .The next treelet translation is considered in the global list as indicated by a next treelet pair pointer in each candidate .If there are no more treelet pairs , the candidate is dropped .", "label": "", "metadata": {}, "score": "44.455162"}
{"text": "Here are some examples : .A limit may be placed on the number of gaps in any partial translation t(H ) .Note that a constraint which imposes that partial translation t(H ) may not contain gaps at all amounts to translating exclusively with contiguous bi - fragments .", "label": "", "metadata": {}, "score": "44.64814"}
{"text": "Its next treelet pair pointer is updated as is its estimated score to reflect the choices not taken .This has the effect of tightening up the estimate with each not - taken choice with no loss in the optimality of the search .", "label": "", "metadata": {}, "score": "44.751823"}
{"text": "Each candidate includes the following members : .The position of the next treelet translation to consider in the global list of matching treelet translation pairs .The set of treelet translations already selected in this candidate .A representation of the input nodes covered by this candidate .", "label": "", "metadata": {}, "score": "44.766716"}
{"text": "This also allows the decoder to thoroughly explore those treelet pairs that are likely to result in good translations - that is , those treelet pairs that already translate a lot of the tree .In one illustrative embodiment , the value of N in the N - best list is determined by dividing a seed value by the number of uncovered nodes that need to be resolved using recursive calls .", "label": "", "metadata": {}, "score": "44.894516"}
{"text": "In document alignment , the PC - DATA segmentation intervenes on levels of sentences , words , syllables and characters at the same time and is much more arbitrary .Here , the translation confidence of sentence alignment is replaced by a non - trivial leaf similarity .", "label": "", "metadata": {}, "score": "44.912373"}
{"text": "Otherwise , if no instance of a target gap of a given size is observed , translations using this configuration will not be generated automatically , even though instances with smaller and larger gaps exist in the training corpus .In the present exemplary embodiment , the gaps can assume different sizes with certain probabilities .", "label": "", "metadata": {}, "score": "45.00117"}
{"text": "This is indicated by block 704 in .FIG .14D .In the present example , treelet translation pair 700 is the only one identified and it covers the entire sub - tree rooted at \" computer . \"Therefore , it is simply scored and added to the N - best list for the source node \" computer .", "label": "", "metadata": {}, "score": "45.01641"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ .In one embodiment of the present invention , a decoder receives a dependency tree as a source language input and accesses a set of statistical models that produce outputs combined in a log linear framework .", "label": "", "metadata": {}, "score": "45.116642"}
{"text": "The following paragraphs illustrate these rules with specific examples .FIG .6 illustrates a flow diagram that shows the process by which dependencies are projected from the source language dependency tree onto the aligned target language text .Component 220 projects the dependencies from the source language dependency tree to aligned words in the target language text , in order to generate dependencies in the target language dependency tree .", "label": "", "metadata": {}, "score": "45.154633"}
{"text": "Other probability models can be used as well .The specific translation model set out in Vogel is used in combination with at least a target language model to form a classic noisy channel model .The best scoring translation is found by a simple search : a monotone decoder assumes that source phrase order is preserved and uses Viterbi decoding to find the best path through the translation lattice .", "label": "", "metadata": {}, "score": "45.23637"}
{"text": "In one approach , the context vector for each word in the lexicon may consist of co - occurrence counts in respect to a number of peripheral tokens ( basically , the most frequent words ) .These counts may be collected for each position in an n - word window around the word in focus .", "label": "", "metadata": {}, "score": "45.262863"}
{"text": "Treelet pair extractor 224 then considers all source language treelets of size 4 .There is only one , and it is shown at 430 in .FIG .8I .This is a well - formed treelet translation pair and it is thus extracted and placed in the treelet translation pair database 204 .", "label": "", "metadata": {}, "score": "45.4412"}
{"text": "In another aspect , a training method is provided which is based on the maximization of translation accuracy , for example , as measured with the National Institute of Standards and Technology ( NIST ) evaluation metric .The translations may be produced by means of a beam - search decoder or other suitable optimizing function which takes into account the probabilities of various hypothetical translations being found in practice .", "label": "", "metadata": {}, "score": "45.507717"}
{"text": "We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser . \" ...Previous work has used monolingual parallel corpora to extract and generate paraphrases .", "label": "", "metadata": {}, "score": "45.526817"}
{"text": "Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] .", "label": "", "metadata": {}, "score": "45.533806"}
{"text": "14B ( the list of pairs in .FIG .14B has been shortened for clarity of illustration .In reality even a short sentence such as this is likely to have dozens of matching treelet translation pairs ) .The job of decoder 608 is now to find the best combination and ordering of the treelet translation pairs in .", "label": "", "metadata": {}, "score": "45.55443"}
{"text": "The procedure is similar to ( Chiang , 2007 ) except that we maintain tree structures on the target side , instead of strings .We use a statistical CFG parser to parse the English side of the training data , and extract dependency trees with Magerman 's rules ( 1995 ) .", "label": "", "metadata": {}, "score": "45.575172"}
{"text": "While this type of system does appear to incorporate larger memorized patterns ( like phrasal SMT ) in combination with a dependency analysis , the statistical modeling in the system is extremely limited .Only a direct maximum likelihood estimation translation model is used .", "label": "", "metadata": {}, "score": "45.598602"}
{"text": "Each of the retrieved text fragments is linked in a bi - phrase in the library to a text fragment from the source text .At least one target hypothesis is generated .Each of the target hypotheses comprises text fragments selected from the second language .", "label": "", "metadata": {}, "score": "45.66433"}
{"text": "Accordingly , it is appropriate to ignore it .The model presupposes that , for each bi - phrase , the empirical statistics of its observed gap sizes in the training corpus are available .In the exemplary embodiment , a static library of bi - chunks constructed according to the method of Cancedda , et al . is used as the information source for obtaining these statistics .", "label": "", "metadata": {}, "score": "45.69809"}
{"text": "a processor which executes the instructions .A machine translation system for translating source text from a first language to target text in a second language , comprising : .The machine translation system of . claim 17 , wherein translation scoring function further includes at least one of : . a second feature function which evaluates bi - fragment discontinuities ; and .", "label": "", "metadata": {}, "score": "45.740185"}
{"text": "FIG .3 .A treelet translation pair is defined as connected subgraph of the source language dependency tree and the corresponding connected subgraph of the target language dependency tree .However , in accordance with one embodiment of the present invention , the definition of \" connected subgraph \" treats siblings from any node in the tree as being connected through a dummy parent node .", "label": "", "metadata": {}, "score": "45.940884"}
{"text": "However , SMT appears more promising in current research , so this discussion will focus primarily on SMT and not EBMT .Typically the transfer - based systems incorporate linguistic information using a parser , and the SMT systems do not .", "label": "", "metadata": {}, "score": "45.958614"}
{"text": "The system may use various clues , e.g. , context and frequency , to identify and score other possible translation pairs , using the seed lexicon as a basis .An alternative system may use a small bilingual lexicon in addition to non - parallel corpora to learn translations of unknown words and to generate a parallel corpus .", "label": "", "metadata": {}, "score": "46.037506"}
{"text": "Each edge connects a vertex representing a source language word to a vertex representing a target language word : the graph is a bipartite graph .Other embodiments of word alignments may use different representations , such as a function from a source language word to the set of target language words to which it is aligned , but this functional representation can be easily recast in terms of the graph representation above .", "label": "", "metadata": {}, "score": "46.0522"}
{"text": "No .6,304,841 , log - linear features are used to score word hypotheses depending on their lexical context .The following references relate to phrase - based statistical machine translation methods .U.S. Pat .No . 6,182,026 by Tillmann , et al . , entitled \" METHOD AND DEVICE FOR TRANSLATING A SOURCE TEXT INTO A TARGET USING MODELING AND DYNAMIC PROGRAMMING , \" discloses a method and device for translating a source text into a target using modeling and dynamic programming .", "label": "", "metadata": {}, "score": "46.087807"}
{"text": "However , in order to make these types of processes computationally efficient , a number of severely limiting simplifying assumptions must be made .This significantly reduces the modeling power of such systems .In addition , this type of translation model acts only at the level of a single lexical item at a time ( i.e. , at the word level ) and phrasal combinations are not modeled directly .", "label": "", "metadata": {}, "score": "46.12181"}
{"text": "Note that every leaf appears in this candidate list , and that it appears in its most likely group as the matrix traversal is guided by similarity .Note furthermore , that the M - N candidate list can also be obtained by cutting the resulting traditional alignment : s 1 //s 2 s 3 //s 4 . - //s 5 . - //s 6 - //t 1 . - //t 2 t 3 //t 4 t 5 //t 6 .", "label": "", "metadata": {}, "score": "46.162342"}
{"text": "We investigate unsupervised techniques for acquiring monolingual sentence - level paraphrases from a corpus of temporally and topically clustered news articles collected from thousands of web - based news sources .Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy ... \" .", "label": "", "metadata": {}, "score": "46.164734"}
{"text": "The system includes a processor which executes instructions for receiving the source text in the first language , accessing the library to retrieve text fragments from the second language , scoring hypotheses according to a plurality of features including the gap scoring feature .", "label": "", "metadata": {}, "score": "46.323757"}
{"text": "Its complexity grows with the factorial of the number of nodes to be ordered .It is also called for each possible combination of translation choices .Therefore , in one illustrative embodiment , a greedy ordering strategy is employed instead of the exhaustive ordering step described above .", "label": "", "metadata": {}, "score": "46.38771"}
{"text": "We present Minimum Bayes - Risk ( MBR ) decoding for statistical machine translation .This statistical approach aims to minimize expected loss of translation errors under loss functions that measure translation performance .We describe a hierarchy of loss functions that incorporate different levels of l ... \" .", "label": "", "metadata": {}, "score": "46.41484"}
{"text": "Pruning away low scoring treelet translation pairs before the search starts allows the decoder to spend more time inspecting combinations and orderings of high quality treelet pairs .A number of heuristics can be used to prune , and those include : .", "label": "", "metadata": {}, "score": "46.444725"}
{"text": "A cutoff of 0.01 is believed to be useful .Given a set of treelet translation pairs where the source treelets are identical , only keep those pairs whose maximum likelihood estimation probability is within some ratio of the best pair .", "label": "", "metadata": {}, "score": "46.50805"}
{"text": "2 shows a flowchart describing a method 200 for building a translation lexicon from non - parallel corpora .A word comparator 120 may be used to collect pairs of identical words ( block 205 ) .In the English German implementation described above , 977 identical words were found .", "label": "", "metadata": {}, "score": "46.539536"}
{"text": "Keep only the top N treelet translation pairs with the same input node , as ranked first by size , then by maximum likelihood estimation channel model score , then by another model score .The estimated scores are optimistic such that once a candidate has been completed , the decoder can perform fairly severe pruning .", "label": "", "metadata": {}, "score": "46.56105"}
{"text": "Recent developments have , however , seen improvements in translation quality following from syntax - based reordering .One such development is the pre - translation approach that adjusts the source sentence to resemble target language word order prior to translation .", "label": "", "metadata": {}, "score": "46.689667"}
{"text": "Two techniques are employed : ( 1 ) simple string edit distance , and ( 2 ) a heuristic strategy that pairs initial ( presumably summary ) sentences from different news stories in the same cluster .We evaluate both datasets using a word alignment algorithm and a metric borrowed from machine translation .", "label": "", "metadata": {}, "score": "46.75491"}
{"text": "These transducers are limited in scope .They rely only on very local context , such that the end result is a fundamentally word - based ( as opposed to phrase - based ) decoder .The transducer induction process is also likely complicated by data sparsity problems .", "label": "", "metadata": {}, "score": "46.872017"}
{"text": "6F is a flow diagram illustrating how nodes in the target language dependency tree are re - attached .FIG .7 is a flow diagram illustrating assignment of dependencies to unaligned target words .FIGS .7A-7F illustrate projection of dependencies from a source language dependency tree to a target language dependency tree where a word in the target sentence is unaligned .", "label": "", "metadata": {}, "score": "46.923073"}
{"text": "Given a fixed pre and post modifier count , the order model is capable of evaluating a single ordering decision independently from other ordering decisions .In decoding , then , the step that considers all possible orderings in the algorithm shown in table 1 is replaced with the algorithm shown in table 2 .", "label": "", "metadata": {}, "score": "46.949112"}
{"text": "Dagan , et al .enhance this model by considering the word positions within an alignment .See Dagan , K. Church , and W. Gale , \" Robust Word Alignment for Machine Aided Translation , \" in Proceedings of the Workshop on Very Large Corpora , pp . 1 - 8 , Columbus , Ohio , USA ( June 1993 ) .", "label": "", "metadata": {}, "score": "47.066967"}
{"text": "The alignment shown in .FIG .4D is fairly straightforward , because it is a one - two - one alignment .In another words , each word in the source language text is aligned with a single word in the target language text .", "label": "", "metadata": {}, "score": "47.2022"}
{"text": "Therefore , decoder 608 retrieves the best translations for the uncovered part of the sub - tree root at \" installed . \" In this case , it retrieves the best translations for the sub - tree rooted at \" computer .", "label": "", "metadata": {}, "score": "47.22762"}
{"text": "Hypotheses deemed equivalent may be merged together .With elastic bi - fragments , one conditions is verified before adding an edge from a hypothesis H : .The source language fragment matches source language words , without overlapping source language words from previous bi - fragments ( i.e. , bi - fragments in decisions leading to H ) .", "label": "", "metadata": {}, "score": "47.229904"}
{"text": "In order to filter out such cases , the system 400 uses two simple heuristics : length and word content .The translation candidate must also be an open - class word .The algorithm 1000 for learning translations of unknown words is summarized in .", "label": "", "metadata": {}, "score": "47.325005"}
{"text": "Referring again to the overall training system diagram shown in .FIG .2 , the translation probability table 210 is simply a conventional channel model which predicts the probability of a word in a source language being translated to a word in a target language .", "label": "", "metadata": {}, "score": "47.37677"}
{"text": "Attempts have also been made to combine different aspects of the two types of machine translation systems into a single , hybrid system .However , these attempts have still suffered from disadvantages .Let us briefly survey the state - of - the - art in SMT as well as some prior art attempts to combine syntax and SMT .", "label": "", "metadata": {}, "score": "47.381016"}
{"text": "FIG .7 illustrates traversal of the matrix of .FIG .6 , in which the path of minimum costs is indicated by the connected blocks ; .FIG .8 illustrates the source and target documents after alignment ; and .", "label": "", "metadata": {}, "score": "47.387596"}
{"text": "We propose a theory that gives formal semantics to word - level alignments defined over parallel corpora .We use our theory to introduce a linear algorithm that can be used to derive from word - aligned , parallel corpora the minimal set of syntactically motivated transformation rules that explain human ... \" .", "label": "", "metadata": {}, "score": "47.413826"}
{"text": "2 In what follows we compare two strategies for unsupervised construction of such a corpus , one employing string similarity and the other associating sentences that may overlap very little at the s .. \" ...We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation .", "label": "", "metadata": {}, "score": "47.433365"}
{"text": "To handle this , in accordance with one embodiment of the present invention , heuristics are employed that look at the set of words on the target side that are aligned to the single word on the source side to determine its dependency .", "label": "", "metadata": {}, "score": "47.521976"}
{"text": "Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work .The first section ( \" Translation equivalents \" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use .", "label": "", "metadata": {}, "score": "47.5806"}
{"text": "This corpus - based approach has since been augmented in two dimensions .[ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them .While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al .", "label": "", "metadata": {}, "score": "47.63672"}
{"text": "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with paraphrases extracted from automatic alignments . ... ol .", "label": "", "metadata": {}, "score": "47.638638"}
{"text": "In one embodiment , a measure of reordering is used which relies on the \" source coverage vector \" V of the sequence of decisions d 1 . . .d K .The coverage vector can be computed using the procedure of Cancedda , et al .", "label": "", "metadata": {}, "score": "47.781994"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system for building a translation lexicon according to an embodiment .FIG .2 is a flowchart describing a method for building a translation lexicon from non - parallel corpora .", "label": "", "metadata": {}, "score": "47.833885"}
{"text": "Accordingly , the word comparator 120 may restrict the word length to be able to increase the accuracy of the collected word pairs .For instance , by relying only on words at least of length six , 622 word pairs were collected with 96 % accuracy .", "label": "", "metadata": {}, "score": "47.91684"}
{"text": "i ( i ) and the concatenation of the contents of the target leaves t 1 ( i ) , . . ., t n .i i .The present method provides a way to approach this optimal solution by making assumptions which make processing easier and less time consuming .", "label": "", "metadata": {}, "score": "48.183167"}
{"text": "FIG .8J .In this initial pass , where the size of the source language treelets being enumerated is 1 , every node in the source language dependency tree is enumerated .For the word aligned parallel dependency tree structure shown in .", "label": "", "metadata": {}, "score": "48.21985"}
{"text": "This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well .Parsing .Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference .", "label": "", "metadata": {}, "score": "48.29265"}
{"text": "1 .A software module called an XML processor is used to read XML documents and provide access to their content and structure ( Step S 104 ) .String Alignment .To provide an understanding of alignment techniques in general , a description of string alignment will first be given .", "label": "", "metadata": {}, "score": "48.30126"}
{"text": "For a gap size on the target side , an analogous procedure may be used .One further simplification may be made , namely to assume that all the components in a gapped phrase are of length one , that is , correspond exactly to one single word .", "label": "", "metadata": {}, "score": "48.3028"}
{"text": "FIG .6 .The first such error discussed will be that just mentioned ( where a target word - the word \" ne\"-appears out of order in the translation read from the final target language dependency tree shown at .FIG .", "label": "", "metadata": {}, "score": "48.329193"}
{"text": "8J .The treelets \" empty - vider \" and \" the - le \" are both well - formed and are thus extracted and placed in the treelet translation table .However , the entire treelet translation pair for the terms \" bin \" and , \" recycle \" is shown in .", "label": "", "metadata": {}, "score": "48.471806"}
{"text": "FIG .9A .FIG .9B shows the alternate structure indicating the same thing ( dependencies and word alignments ) .With this example , when source language treelets of size 1 are enumerated along with their alignments , this will produce , among other treelets , that shown in .", "label": "", "metadata": {}, "score": "48.479618"}
{"text": "It finds particular application in the translation of non - contiguous bi - fragments of text .A recent development in statistical machine translation has entailed the step from word - based models to phrase - based models .While in traditional word - based statistical models , the atomic unit that translation operates on is the word , phrase - based methods acknowledge the significant role played in language by multi - word expressions , thus incorporating , in a statistical framework , the insight behind Example - Based Machine Translation .", "label": "", "metadata": {}, "score": "48.50963"}
{"text": "FIG .14D .Decoder 608 then selects another source node , moving up the tree .The next source node is \" on \" and there are no matching treelet translation pairs in .FIG .14B which are rooted at the node \" on . \"", "label": "", "metadata": {}, "score": "48.51082"}
{"text": "8A-8I illustrate projection of dependencies from a source language tree to a target language tree and the extraction of treelet translation pairs where multiple words in the source language input are aligned to a single word in the target language input .", "label": "", "metadata": {}, "score": "48.562374"}
{"text": "For example , blocks in the flowcharts may be skipped or performed out of order and still produce desirable results .Also , the heuristics described herein may be combined with the alignment method described herein .Accordingly , other embodiments are within the scope of the following claims .", "label": "", "metadata": {}, "score": "48.59128"}
{"text": "This is introduced to compensate for h bf 's strong tendency to overestimate the probability of rare bi - fragments .This feature may be computed as in equation ( 7 ) : .Pr . t . s .s .", "label": "", "metadata": {}, "score": "48.60436"}
{"text": "Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from .And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts .", "label": "", "metadata": {}, "score": "48.62821"}
{"text": "It is used to decompose the overall problem into smaller sub - problems ( partitions ) to reduce complexity ( .FIG .5 ) .Matrix match : A minimum edit distance algorithm may then be used to produce 1 - 1 , 1 - 0 , and 0 - 1 pairs ( .", "label": "", "metadata": {}, "score": "48.77677"}
{"text": "In other words , two treelet translation pairs that overlap on one or more nodes , will be considered , so long as those nodes are not inconsistent .This may be , for instance , treelet size or channel model score .", "label": "", "metadata": {}, "score": "48.79888"}
{"text": "The bi - fragments may be learned from a large aligned bilingual corpus comprising source text and its translation into target text .The statistics of bi - phrases can be established through observation of a corpus .It is to be appreciated that direct observation of a bi - phrase in a given sentence pair is not necessary , but rather the presence of such a bi - phrase can be indirectly induced through phrase - level alignment mechanisms .", "label": "", "metadata": {}, "score": "48.84549"}
{"text": "^ .arg .max .t .J . m .M . m .h .m .s .I . t .J . )Exemplary Features Suitable for Treatment of Elastic Bi - Phrases .For example , the present model can include the following feature functions as illustrated in .", "label": "", "metadata": {}, "score": "48.988464"}
{"text": "6 .In the present example , that word is \" ne \" .Component 220 then identifies the lowest point above the identified word ( above \" ne \" ) in the target language dependency tree for reattachment , so that the original surface string order is preserved .", "label": "", "metadata": {}, "score": "49.04515"}
{"text": "The translation system includes a library of bi - fragments .Each of the bi - fragments includes a text fragment from the first language and a text fragment from the second language .At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "49.048424"}
{"text": "The reordering is only selected if it leads to a translation that looks like a much better sentence than the alternative .Recent developments have , however , seen improvements in translation quality following from syntax - based reordering .One such development is the pre - translation approach that adjusts the source sentence to resemble target language word order prior to translation .", "label": "", "metadata": {}, "score": "49.170395"}
{"text": "The reordering is only selected if it leads to a translation that looks like a much better sentence than the alternative .Recent developments have , however , seen improvements in translation quality following from syntax - based reordering .One such development is the pre - translation approach that adjusts the source sentence to resemble target language word order prior to translation .", "label": "", "metadata": {}, "score": "49.170395"}
{"text": "We show for the first time that incorporating the predictions of a word sense disambigua - tion system within a typical phrase - based statistical machine translation ( SMT ) model consistently improves translation quality across all three different IWSLT Chinese - English test sets , as well as producing st ... \" .", "label": "", "metadata": {}, "score": "49.234642"}
{"text": "In the subsequent structured learning process , systems and methods adopt a learning methodology which first learns the structural transformations from available examples of transformations , and then applies these transformations to a set of legacy documents .FIG .9 shows an exemplary embodiment of an alignment apparatus 200 suitable for performing the document alignment method described herein , according to one aspect of the exemplary embodiment .", "label": "", "metadata": {}, "score": "49.251976"}
{"text": "However , the sheer computational complexity of the problem was a difficult obstacle to overcome , and it proved difficult to capture local context in a word - to - word statistical model .Thus the resulting systems were often rather slow and produced only moderate quality translations .", "label": "", "metadata": {}, "score": "49.262802"}
{"text": "The method of .claim 1 , wherein the translation scoring function utilizes a log - linear scoring model of the form : .Pr .t .J .d .K .s .I . )Z .f .", "label": "", "metadata": {}, "score": "49.365124"}
{"text": "This means that they assume the common XML structure and provide a description language to say how the elements are laid out and are related to each other .An important part of developing a system for automated conversion of documents from one format to another is the automatic learning of document transformations .", "label": "", "metadata": {}, "score": "49.388298"}
{"text": "Translating with Elastic Bi - Fragments .The translation of a source sentence f is produced by combining together bi - fragments so as to entirely cover the source sentence , and produce a well - formed target - language sentence , i.e. , a sequence without gaps .", "label": "", "metadata": {}, "score": "49.413803"}
{"text": "The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .Building a translation lexicon from comparable , non - parallel corpora US 8234106 B2 .Abstract .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .", "label": "", "metadata": {}, "score": "49.437378"}
{"text": "In addition to the SMT decoder , the toolki ... \" .We describe an open - source toolkit for statistical machine translation whose novel contributions are ( a ) support for linguistically motivated factors , ( b ) confusion network decoding , and ( c ) efficient data formats for translation models and language models .", "label": "", "metadata": {}, "score": "49.470688"}
{"text": "The results of this automatic processing go far beyond the construction of a single lexicon .I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use .As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text .", "label": "", "metadata": {}, "score": "49.473907"}
{"text": "The basic algorithm has a complexity of O(l 1 l 2 ) where l 1 and l 2 are the lengths of the sequences .In the case of document alignment , both of these lengths can easily be larger than 10,000 .", "label": "", "metadata": {}, "score": "49.512165"}
{"text": "If it passes the threshold tests but does not yet cover the entire input , it is placed back in the incomplete queue .If the new candidate now covers the entire input , the overlapping treelets are merged and all possible orderings that are consistent with the selected treelets are explored .", "label": "", "metadata": {}, "score": "49.555283"}
{"text": "8E , each of the source language nodes is enumerated on the left side of .FIG .8F .Then , the alignments for each of the enumerated treelets are identified .This is shown on the right half of .", "label": "", "metadata": {}, "score": "49.639034"}
{"text": "In the decoder of Cancedda , et al . , the states ( sometimes called \" merged hypotheses \" ) are organized in stacks , where two states are put in the same stack if they have the same number of source words covered .", "label": "", "metadata": {}, "score": "49.64906"}
{"text": "In a GST of a set of strings , each path from the root to a leaf represents a suffix in one or more strings from the set .FIG .6 shows the GST 600 for a corpus of two sentences .", "label": "", "metadata": {}, "score": "49.6548"}
{"text": "All words in T i and T j are aligned to source words s k and s l , and .( b )No other target words are aligned to s k and s l , and .( c ) t i is the representative element of T i , and .", "label": "", "metadata": {}, "score": "49.702072"}
{"text": "Yet SMT translation qual - ity still obviously suffers from inaccurate lexical choice .In this paper , we address this problem by investigating a new strat - egy for integrating WSD into an SMT sys - tem , that performs fully phrasal multi - word disambiguation .", "label": "", "metadata": {}, "score": "49.713745"}
{"text": "In the worst case , the initial match is empty , so no decomposition is done and no gain whatsoever is achieved .In general , by selection of k , a significant reduction in complexity can be achieved , reducing computational time .", "label": "", "metadata": {}, "score": "49.781628"}
{"text": "In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure .[ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research .", "label": "", "metadata": {}, "score": "49.799995"}
{"text": "One adaptation may include lower - bounding the value of h ELAST based on a consideration of the buffer .For each gap between two words e 1 and e 2 in a target phrase which is deployed in the current partial translation , one of three cases can occur : .", "label": "", "metadata": {}, "score": "49.84481"}
{"text": "The effect of the addition of the new state may then be to eject the last state of the current stack .This is because , as with the method of Cancedda , et al ., the stacks are generally pruned to remove the lowest ( highest cost ) states , once they reach a preselected number of states .", "label": "", "metadata": {}, "score": "49.852737"}
{"text": "When given a new segment of text to translate , these systems search the database to extract all relevant bi - fragments , i.e. , items in the database whose source - language fragment matches some portion of the new input .", "label": "", "metadata": {}, "score": "49.88428"}
{"text": "The one approach that uses a separate parser is very limited in scope , combines paths in an arbitrary order and has not employed a combination of statistical models which severely limits possible translation quality .SUMMARY OF THE INVENTION .Given the present state of the technology , a context - free constituency analysis ( as opposed to a dependency analysis ) may seem to be a natural starting point in developing a statistical machine translation system .", "label": "", "metadata": {}, "score": "49.885567"}
{"text": "The paths are combined in an arbitrary order .Finally the restriction imposed by this approach that the \" phrases \" extracted from the dependency trees be linear paths is quite detrimental .Not only does it lose promising treelet translations in a non - linear branching configuration , but it also can not model certain common phrases that are contiguous in the surface string but non - linear in the dependency tree .", "label": "", "metadata": {}, "score": "50.01992"}
{"text": "For each occurrence of a target word , the counts may be collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions .The counts may be collected separately for each position and then entered into a context vector with a dimension for each context word in each position .", "label": "", "metadata": {}, "score": "50.11447"}
{"text": "j .m .j .m .j . ) . . .The result is shown in .FIG .7 .This figure also shows the traversal of the matrix , which is done as before by starting at the end point and following the path of the minimal cost .", "label": "", "metadata": {}, "score": "50.126804"}
{"text": "Suitable candidates for these matches are apparent from the traversal of the matrix in .FIG .7 .Every point in the traversal corresponds to a match between a source leaf 11 and a target leaf 13 or an unmatched pair .", "label": "", "metadata": {}, "score": "50.31669"}
{"text": "3 , it can be seen that processing has advanced through the projection of source language dependencies onto aligned target language text in block 264 to obtain the word - aligned parallel dependency tree corpus 400 .The next step is to extract from corpus 400 treelet translation pairs and to place them in treelet translation pair database 204 .", "label": "", "metadata": {}, "score": "50.408928"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a simulated distribution of gaps in a target phrase of a bi - phrase for a selected number of gaps in the corresponding source phrase .FIG .2 is a block diagram of a system for machine translation of text which utilizes non - contiguous bi - fragments according to one aspect of the exemplary embodiment ; .", "label": "", "metadata": {}, "score": "50.417477"}
{"text": "t .t . s .s .Pr . t .s . )A \" target language \" feature function h tl illustrated at 38 .This is based on an N - gram language model of the target language .", "label": "", "metadata": {}, "score": "50.452915"}
{"text": "3 is a table showing results of an experiment utilizing the system of .FIG .1 .FIG .4 is a block diagram of a system for building a translation lexicon according to another embodiment .FIG .5 is a suffix tree .", "label": "", "metadata": {}, "score": "50.46971"}
{"text": "Using this convention , contiguous and discontinuous phrases can be treated under a single type , with has the advantage of avoiding the need to learn separately different combinations of contiguous / non - contiguous components with the associated sparseness issues .", "label": "", "metadata": {}, "score": "50.474106"}
{"text": "One reason for this is that it does not differentiate between two hypotheses with the same coverage , but at different stages of buffer consumption .Where the decoder allows relatively large stacks , it may be reasonable to ignore the problem .", "label": "", "metadata": {}, "score": "50.475533"}
{"text": "FIG .8C .FIG .8D illustrates both the source and target language dependency structures and the word segmented word alignment between the source and target language strings .The structure shown in .FIG .8D can also be redrawn as shown in .", "label": "", "metadata": {}, "score": "50.508373"}
{"text": "In another embodiment , a system may align text segments in comparable , non - parallel corpora , matching strings in the corpora , and using the matched strings to build a parallel corpus .The system may build a Bilingual Suffix Tree ( BST ) and traverse edges of the BST to identify matched strings .", "label": "", "metadata": {}, "score": "50.549164"}
{"text": "In particular it can be used to model the empirical distribution of gaps for a given bi - phrase , without any smoothing / generalization , by just recording a histogram of the observed gap sizes in the training corpus .It also allows the modeling of conditional dependencies between gaps .", "label": "", "metadata": {}, "score": "50.653713"}
{"text": "SMT systems perform well in learning translations of domain - specific terminology and fixed phrases , but simple grammatical generalizations are poorly captured and often confused during the translation process .Transfer - based systems , by contrast , often succeed in producing grammatical and fluent translations , but are highly time consuming to develop .", "label": "", "metadata": {}, "score": "50.67116"}
{"text": "We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \" computational lexicography \" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s .This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with .", "label": "", "metadata": {}, "score": "50.694588"}
{"text": "Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis .We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek .", "label": "", "metadata": {}, "score": "50.732807"}
{"text": "For each bi - fragment identified from the corpus , a probability distribution ( or a mean of the probability distribution ) representing the statistical probabilities of number of gaps in the target fragment for each observed number of gaps in the source may be stored and used to develop distributions .", "label": "", "metadata": {}, "score": "50.740337"}
{"text": "3 .( In languages such as Chinese that do not use spaces to separate words , the word segmentation task is more complex than it is in languages such as English or German . )After the source language dependency parse is generated and the target language word segmentation has been performed , the source language dependency parse is placed adjacent the target language word segmentation to form word segmented parallel corpus 256 in .", "label": "", "metadata": {}, "score": "50.75177"}
{"text": "This is a first step that is now refined to account for partial ( M - N ) matches ( Step S 114 ) .The M - N Match .As already pointed out , the standard alignment algorithm produces only 1 - 1 , 1 - 0 , or 0 - 1 matches .", "label": "", "metadata": {}, "score": "50.871307"}
{"text": "This computation is performed when e 2 has just been moved from a suffix in the buffer to the sequence , using the sequence index of e 1 , which it is assumed has been maintained on the suffix .At this point , the index on the suffix is updated to the sequence index of e 2 .", "label": "", "metadata": {}, "score": "50.91443"}
{"text": "Where a decision results in consumption of a buffered word , the sequence remains in the same stack .Provided that this new hypothesis is not pruned , it is reconsidered in that stack .Eventually , one or more of the stacks may reach or exceed its maximum beam size and is culled by eliminating hypotheses which are statistically less favored to bring the number of hypotheses to the maximum permitted by the beam .", "label": "", "metadata": {}, "score": "50.918114"}
{"text": "FIG .1 illustrates the hypertext mark - up language fragment of a book represented as a tree with internal nodes containing different layout instructions and leads containing fragments of the document content in a first XML format and a second XML format ; .", "label": "", "metadata": {}, "score": "50.99743"}
{"text": "Such assumptions are generally not applicable to document alignment .Another difference between document alignment and sentence alignment is that the total content ( ignoring the order ) of the leaves of the target document 12 is substantially identical to the total content of the leaves of the source document 10 .", "label": "", "metadata": {}, "score": "51.015495"}
{"text": "With each iteration , the tuning process repeats the steps until it reaches an optimized translation quality .Reordering has been an important topic in statistical machine translation ( SMT ) as long as SMT has been around .State - of - the - art SMT systems such as Pharaoh ( Koehn , 2004a ) still employ a simplistic model of the reordering process to do non - local reordering .", "label": "", "metadata": {}, "score": "51.042114"}
{"text": "In the case of document alignment , it frequently happens that one leaf 13 of the target document 12 corresponds to several leaves 11 of the source document 10 .The typical leaf 11 of the source document is a line of text determined by the layout of the document .", "label": "", "metadata": {}, "score": "51.112587"}
{"text": "In this way , rule - based morphological , syntactic and/or semantic information is combined with knowledge extracted from bilingual texts which is then re - used in the translation process .Such methods are often collectively referred to as \" phrase - based methods \" .", "label": "", "metadata": {}, "score": "51.164146"}
{"text": "In aspects of the exemplary embodiment , gaps in source and target phrases are modeled utilizing one or more of the following simplifying assumptions : .For a given bi - phrase ( s , t ) , the gap sizes ( source and/or target ) are mutually independent .", "label": "", "metadata": {}, "score": "51.307922"}
{"text": "The next step takes each partition and identifies candidate ( 1 - 0 ) , ( 0 - 1 ) , and ( 1 - 1 ) matches by applying an algorithm which determines the minimum edit distance ( Step S 112 ) .", "label": "", "metadata": {}, "score": "51.345116"}
{"text": "SMT systems , the evaluation set is typically limited to a few hundred sentences , in part because each iteration of the weight - optimization procedure requires translation of the entire evaluation set .Other features are optionally included that calibrate certain dependencies between gap sizes ignored by the simple model of equation ( 6 ) .", "label": "", "metadata": {}, "score": "51.347767"}
{"text": "Note that the parallel corpus belongs to a different domain than the comparable corpus .Also the parallel corpus is extremely small .For low density languages , such a corpus can be built manually .When given as input the comparable corpora described above and the bilingual lexicon of 6,900 entries , the algorithm 1000 found 33,926 parallel sequences , with length between three and seven words .", "label": "", "metadata": {}, "score": "51.39078"}
{"text": "At step S 108 , source and target leaves from the same block ( e.g. , set 22 and set 28 in block 16 ) are examined to identify any direct matches which have a high level of confidence of being an actual match due to their similarity .", "label": "", "metadata": {}, "score": "51.42739"}
{"text": "8 G .Because both the single node treelet \" recycle \" and the single node treelet \" bin \" are aligned to the same target language word \" corbeille \" , extracting either of them independently of the other would not generate a well - formed treelet translation pair .", "label": "", "metadata": {}, "score": "51.464302"}
{"text": "For example , SUB or WORD may be used in the initial stages of the algorithm ( for the initial match and the matrix match ( Steps S 108 and S 112 ) , and RMBL may be used for refinement in the post - processing phase ( Step 116 ) .", "label": "", "metadata": {}, "score": "51.479427"}
{"text": "FIG .2 .Example structures indicating how this is done are shown in .FIGS .8E-8I and a flow diagram illustrating how this is done is shown in .FIG .8J .Treelet pair extractor 224 first starts out by enumerating all source language treelets of size 1 ( the size being indicated by the number of nodes contained in the treelet ) .", "label": "", "metadata": {}, "score": "51.49185"}
{"text": "All of the source language treelets of size 2 are enumerated in .FIG .8H .It will be noted that , because children of a node are considered to form a connected subgraph , treelet 420 is enumerated as well .", "label": "", "metadata": {}, "score": "51.843998"}
{"text": "During the supervised learning process , it is important that a correspondence between the leaves of the source document and the leaves of the sample target document is established .This enables the learning method to assign a target class to the leaves in the source document .", "label": "", "metadata": {}, "score": "51.86651"}
{"text": "Specifically , the method includes identifying , from the matches identified by the alignment algorithm , groups of two ( or more ) matches wherein each match in the group has a leaf in common .In rarer cases , where a pair is both horizontally aligned with an adjacent pair and vertically aligned with another adjacent pair in the traversal , this indicates that more than one source leaf could be matched with more than one target leaf .", "label": "", "metadata": {}, "score": "51.93148"}
{"text": "( more than one source leaf is matched with one leaf in the target document ) , such as the pair ( [ s 2 , s 3 ] , [ t 1 ] ) .1-n ( more than one target leaf is matched with one leaf in the source document ) , such as the pair ( [ s 4 ] , [ t 2 , t 3 ] ) .", "label": "", "metadata": {}, "score": "52.010887"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ , 160 - 167 .Taskar , B. , et al . , \" A Discriminative Matching Approach to Word Alignment , \" In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing ( Vancouver , BC , Canada , Oct. 6 - 8 , 2005 ) .", "label": "", "metadata": {}, "score": "52.091843"}
{"text": "Or , the similarity values of pairs of leaves may be ranked and a preselected number of the highest ranking pairs selected .These initial match pairs may then be used to decompose the overall alignment problem .The basic minimum edit distance algorithm has the complexity O(l 1 l 2 ) where l 1 and l 2 are the lengths of the two documents ( in terms of number of leaves ) , in other words quadratic .", "label": "", "metadata": {}, "score": "52.108543"}
{"text": "In any case , once the source input is segmented by parser 214 , a head word in each text fragment is identified .Then , dependencies between all of the words in the sentence are identified as well .Therefore , the output 262 of dependency parser component 214 is a set of words with the head word identified and with dependencies between words noted .", "label": "", "metadata": {}, "score": "52.16294"}
{"text": "These steps will be described in greater detail below .As will be appreciated , at step S 112 A , the library 20 may identify an elastic bi - fragment which is retrieved by the processing component and which may ultimately used in translation of the text string .", "label": "", "metadata": {}, "score": "52.169907"}
{"text": "corpus preparation .SMT .Corpus preparation is the general process to extract , transform , categorize various documents from their original purpose to and align the resulting data into a parallel corpus for training a translation model .The evaluation process uses a translation model of components created in the training process and configured with the tuning process to translate several thousand source language sentences in the eval set .", "label": "", "metadata": {}, "score": "52.20718"}
{"text": "2 is a flow diagram of an exemplary method for aligning leaves of a source document and a target document ; .FIG .3 is a schematic view of a sequence of leaves in a source document in process of being aligned with a sequence of leaves in a target document ; .", "label": "", "metadata": {}, "score": "52.26113"}
{"text": "s .I . t .J .d .K . ) k .K .EL . target .d . k . ) source .d . k . ) where EL represents a logarithm of a probability of observing a certain configuration of gap sizes in the target and source fragments of a bi - fragment .", "label": "", "metadata": {}, "score": "52.28586"}
{"text": "156 - 163 , Philadelphia , 2002 .S.Kumar , W.Byrne , A Weighted Finite State Transducer Implementation of the Alignment Template Model for Statistical Machine Translation , In HLT - NAACL 2003 , Main Papers , pp .63 - 70 , Edmonton , Canada , May - Jun .", "label": "", "metadata": {}, "score": "52.30394"}
{"text": "A method for aligning documents which may be in different XML formats includes inputting source and target leaves of a source and documents in first and second tree structured formats and assigning a cost to each of a plurality of matches .", "label": "", "metadata": {}, "score": "52.36453"}
{"text": "For uniformity of the notation here , it is retained .When solving the machine translation problem ( Eqn . 1 ) with this type of model , the value of Z f i . 1 need not be computed explicitly , because it depends only on s 1 I , and so the problem reduces to : . t .", "label": "", "metadata": {}, "score": "52.368446"}
{"text": "However , instead of training the model to predict some word given some number of previous words in a string , component 228 descends each of the target language dependency trees and trains the model to predict a child given its parent .", "label": "", "metadata": {}, "score": "52.480762"}
{"text": "6A ) starts with the word \" I \" .It can be seen from the source language dependency structure that the word \" I \" depends on the word \" speak \" as a pre - modifier ( as a modifier that occurs before the word \" speak \" and in the surface string ) .", "label": "", "metadata": {}, "score": "52.4852"}
{"text": "Non - source languages are referred to as \" target \" languages .For Moses SMT , parallel data takes the form of one source and one target language text file where both files contain corresponding translation of sentences line by line .", "label": "", "metadata": {}, "score": "52.495384"}
{"text": "FIG .5 illustrates , by way of example , how the stacks utilized in the beam search procedure are developed .The string 60 to be translated is matched to the bi - phrase library 20 .It will be appreciated that only some of the possible library matches are illustrated .", "label": "", "metadata": {}, "score": "52.57426"}
{"text": "It is understood that an actual document may have many more leaves .The method begins at step S 100 .At Step S 102 , a pair of source and target documents 10 , 12 are input to a suitable automated document alignment apparatus , which performs the following steps .", "label": "", "metadata": {}, "score": "52.595634"}
{"text": "Schemas describe what types of nodes may appear in documents and which hierarchical relationships such nodes may have .A schema is typically represented by an extended context - free grammar .A tree is an instance of this schema if it is a parse tree of that grammar .", "label": "", "metadata": {}, "score": "52.635704"}
{"text": "6 is a flow diagram illustrating projection of dependencies from a source language dependency tree to a target language dependency tree .FIGS .6A-6E illustrate structures that show projection of dependencies from the source language dependency tree to the target language dependency tree .", "label": "", "metadata": {}, "score": "52.641792"}
{"text": "A machine translation method includes receiving source text in a first language and retrieving text fragments in a target language from a library of bi - fragments to generate a target hypothesis .Each bi - fragment includes a text fragment from the first language and a corresponding text fragment from the second language .", "label": "", "metadata": {}, "score": "52.691154"}
{"text": "Other Features Which May be Used .In addition to the two features described above , the log linear model may include analogous features to those described in Cancedda , et al , along with their associated heuristics .These may include one or more of : .", "label": "", "metadata": {}, "score": "52.743896"}
{"text": "9 ( ii ) shows the reverse BST 910 , and in bold , the path we are interested in .When d and y are surrounded by aligned sequences , we hypothesize that they are translations of each other .For a pair of words from the two corpora , we use the terms \" right alignment \" and \" left alignment \" to refer to the aligned sequences that precede and respectively succeed the two words in each corpus .", "label": "", "metadata": {}, "score": "52.763138"}
{"text": "Reordering has been an important topic in statistical machine translation ( SMT ) as long as SMT has been around .State - of - the - art SMT systems such as Pharaoh ( Koehn , 2004a ) still employ a simplistic model of the reordering process to do non - local reordering .", "label": "", "metadata": {}, "score": "52.78765"}
{"text": "Such conversion processes involve the automatic learning of document transformations .The subsequent learning phases , which will not be described in detail here , may include both a supervised learning and an automated learning phase of document transformations .The learning phase may include transforming ordered trees of the first format or \" grammar \" into ordered trees of the second format or \" grammar .", "label": "", "metadata": {}, "score": "52.79119"}
{"text": "Or the text may be input from a database or word document .The text may include one or more sentences , such as a paragraph or an entire document comprising multiple paragraphs .The translation system 10 includes a processing component 16 and a memory component 18 .", "label": "", "metadata": {}, "score": "52.859955"}
{"text": "FIG .4 , words in bold correspond to word of the source sentence which are covered in the decision and words of the target added to the sequence or buffer .Words in the source sentence covered in a prior decision are underlined .", "label": "", "metadata": {}, "score": "52.87086"}
{"text": "The exemplary embodiment may differ in this respect from the method disclosed in Cancedda , et al . .A gap size scoring feature ( h ELAST ) , illustrated at 34 , corresponding to the probability of the gap sizes assigned to ( s , t ) in the translation , relative to the distribution dist associated with ( s , t ) in the training phase .", "label": "", "metadata": {}, "score": "52.874466"}
{"text": "The target language is the language the source language text should be translated to .Tuning is a process that finds the optimized configuration file settings for a translation model when used a specific purpose .The tuning process translates thousands of source language phrases in the tuning set with a translation model , compares the model 's output to a set of reference human translations , and adjusts the settings with the intention to improve the translation quality .", "label": "", "metadata": {}, "score": "52.903656"}
{"text": "SMT .A \" phrase table \" is a statistical description of a parallel corpus of source - target language sentence pairs .The frequencies that n - grams in a source language text co - occur with n - grams in a parallel target language text represent the probability that those source - target paired n - grams will occur again in other texts similar to the parallel corpus .", "label": "", "metadata": {}, "score": "52.97368"}
{"text": "Abstract .In one embodiment of the present invention , a decoder receives a dependency tree as a source language input and accesses a set of statistical models that produce outputs combined in a log linear framework .The decoder also accesses a table of treelet translation pairs and returns a target dependency tree based on the source dependency tree , based on access to the table of treelet translation pairs , and based on the application of the statistical models .", "label": "", "metadata": {}, "score": "52.98922"}
{"text": "The translated vector can be compared to other vectors in the second language .The lexicon builder 125 may perform a greedy search for the best matching similarity vectors and add the corresponding words to the lexicon .Another clue is based on the assumption that in comparable corpora , the same concepts should occur with similar frequencies .", "label": "", "metadata": {}, "score": "52.992325"}
{"text": "FIG .9 .FIG .9 ( i ) shows a branch 905 of the BST corresponding to the comparable corpus in the same figure .The path defined by the bold edges shows that sequences xyz and abc are aligned , and diverge ( i.e. , have a mismatch ) at characters y and d , respectively .", "label": "", "metadata": {}, "score": "53.003937"}
{"text": "Then , the minimum edit distance in the illustrated example is three , as the first two x 's are inserted into t 1 and the c in s 1 is replaced by the third x in t 1 .FIG .", "label": "", "metadata": {}, "score": "53.018364"}
{"text": "8J is a flow diagram illustrating the extraction of treelet translation pairs .FIGS .9A-9C illustrate extraction of treelet translation pairs from a pair of sentences in which multiple words in the target language sentence are aligned to a single word in the source language sentence .", "label": "", "metadata": {}, "score": "53.0801"}
{"text": "3 is a flow diagram illustrating the overall operation of the system shown in .FIG .2 .FIGS .4A-4D illustrate generation of a dependency structure and word alignment of a pair of sentences .FIGS .5A and 5B illustrate a word alignment and source dependency structure for a pair of sentences .", "label": "", "metadata": {}, "score": "53.08995"}
{"text": "Based on these alignments , the system 400 may generate a parallel corpus 420 and identify translations 425 of words from the source language which are not in the lexicon .For example , consider the following two sentences where the only unknown French word is \" raison \" : .", "label": "", "metadata": {}, "score": "53.136044"}
{"text": "FIG .10A .FIG .10B shows two illustrative treelet translation pairs which reside in treelet translation pair database 204 , having been extracted by extractor 224 from the training corpus .Because the word \" your \" is a dependent of the word \" bin \" in the source language dependency tree shown in .", "label": "", "metadata": {}, "score": "53.152756"}
{"text": "In this case , the mode ( maximum ) of the distribution for the corresponding gap is known a priori .The mode can be used as the basis for the optimistic evaluation of the contribution of the gap to the h ELAST feature . log .", "label": "", "metadata": {}, "score": "53.2809"}
{"text": "FIG . 8 .In the source document , the following modifications may be made : .The content 15 is rearranged so as to match the content of the target document .For an m - n match , the m source leaves are replaced by n leaves .", "label": "", "metadata": {}, "score": "53.29255"}
{"text": "FIG .6 .For instance , component 220 examines the words on the source side , to see what they depend on , and projects those same dependencies to the aligned words on the target side .In the present example , assume that component 220 ( after it has identified the root \" parle \" shown in .", "label": "", "metadata": {}, "score": "53.55024"}
{"text": "While in the embodiment , gaps are considered to be represented by a Poisson distribution , other distributions may be used .Such a distribution favors small gaps over large gaps .Another aspect of the modeling of the gap distribution may include the handling of outliers .", "label": "", "metadata": {}, "score": "53.559265"}
{"text": "Where the mean has already been exceeded , the h ELAST feature favors consuming the next buffered word immediately .In this description , a conceptually simple model may be used which assumes that there are gaps between any two words .", "label": "", "metadata": {}, "score": "53.614426"}
{"text": "Each element in one language matches the corresponding element in the other language(s ) .The elements , sometimes called segments , can be block - aligned , paragraph - aligned , sentence - aligned , phrase - aligned or token - aligned . alignment process .", "label": "", "metadata": {}, "score": "53.614616"}
{"text": "At step S 100 , source text to be decoded is input to the system 10 .At step S 110 , the processing component 16 identifies a string of the source text to be translated , such as a sentence .", "label": "", "metadata": {}, "score": "53.65034"}
{"text": "FIG .15 illustrates how model weights are trained in accordance with one embodiment of the present invention .DETAILED DESCRIPTION OF ILLUSTRATIVE EMBODIMENTS .The present invention deals with machine translation using syntactic dependency trees .However , prior to describing the present invention in greater detail , one illustrative environment in which the present invention can be used will be discussed .", "label": "", "metadata": {}, "score": "53.677002"}
{"text": "Additionally , since treelets are ordered , each treelet node implies a partial ordering among its children .One additional item should be considered in accordance with one embodiment of the present invention , and that is training the values for model weights 602 which are applied in the log - linear framework to the model scores .", "label": "", "metadata": {}, "score": "53.68277"}
{"text": "The reordering table is translation model components .source language .SMT .The source language is the language of the text that is to be translated .Typically , this is the authored language of the text .The source language is the same as the TMX specification \" srclang \" attribute of the tag .", "label": "", "metadata": {}, "score": "53.737846"}
{"text": "A buffer 46 , illustrated herein as located in volatile memory 30 , stores unconsumed portion(s ) of a target phrase in a partial translation .FIG .3 illustrates steps in an exemplary method for translation of text comprising non - contiguous bi - fragments .", "label": "", "metadata": {}, "score": "53.75332"}
{"text": "8F , and is indicated by block 408 in .FIG .8J .Treelet pair extractor 224 then extracts from the enumerated treelets and corresponding alignments , well - formed treelet translation pairs .This is indicated by block 410 in .", "label": "", "metadata": {}, "score": "53.780354"}
{"text": "Dependency analysis , in contrast to constituency analysis , tends to bring more semantically related elements together .For instance , verbs become directly contiguous to all their arguments , not just objects .In addition , dependency trees are better suited to heavily lexicalized operations , which have proven quite effective in phrasal statistical machine translation .", "label": "", "metadata": {}, "score": "53.82684"}
{"text": "60/625,489 , filed Nov. 4 , 2004 , the content of which is hereby incorporated by reference in its entirety .BACKGROUND OF THE INVENTION .The present invention deals with machine translation .More specifically , the present invention deals with a machine translation system that uses syntactic dependency treelets .", "label": "", "metadata": {}, "score": "53.84461"}
{"text": "For the learning process it is important that a correspondence between the leaves of the source document and the leaves of the sample target document is established .This enables the learning method to assign a target class to the leaves in the source document .", "label": "", "metadata": {}, "score": "53.88043"}
{"text": "[ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure .In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it .", "label": "", "metadata": {}, "score": "53.94718"}
{"text": "If the monolingual corpora are somewhat comparable , it can be assumed that a word that occurs in a certain context should have a translation that occurs in a similar context .The context may be defined by the frequencies of context words in surrounding positions .", "label": "", "metadata": {}, "score": "53.960007"}
{"text": "The final BLEU score evaluation report shows how well the machine translations match the reference translations .A training corpus with each phrase annotated with the hierarchical structure of the language , such as parts of speech , word function , etc . .", "label": "", "metadata": {}, "score": "53.994858"}
{"text": "FIG .7 shows two corpora 705 , 710 , a bilingual lexicon 715 , and the corresponding BST 720 .Edges drawn with dotted lines mark ends of alignment paths through the tree .Their labels are ( unaligned ) continuations of the source language substrings from the respective paths .", "label": "", "metadata": {}, "score": "53.999073"}
{"text": "An alignment between leaves of the target document and leaves of the source document which includes the probable matches is output .From the matrix alignments , candidate matches are identified in which a leaf of at least one of the source and target documents matches a combination of leaves of the other of the source and target documents .", "label": "", "metadata": {}, "score": "54.005463"}
{"text": "FIG .8H shows that three of the treelets of size 2 have either the word \" bin \" or the word \" recycle \" without the other .Therefore , from the discussion of .FIGS .8F and 8 G , these can not spawn well - formed treelet translation pairs .", "label": "", "metadata": {}, "score": "54.013367"}
{"text": "within each of the blocks , identifying any matches wherein a similarity measure between a leaf of the source document and a leaf of the target document exceeds a threshold value ; and .subdividing the leaves of the source document and the leaves of the target document into partitions bounded by the identified matches wherein the similarity measure between the leaf of the source document and the leaf of the target document exceeds a threshold value .", "label": "", "metadata": {}, "score": "54.014275"}
{"text": "Furthermore , the source and the target documents 10 , 12 may be modified accordingly to make supervised learning with them more feasible ( Step S 120 ) .The method ends at Step S 122 .These steps will now be described in greater detail .", "label": "", "metadata": {}, "score": "54.018875"}
{"text": "The extractions of all parallel phrases and of the translations took about 2 hours each .The experiments were run on a Linux \u00ae system 400 with an Intel \u00ae Pentium \u00ae 3 processor of 866 Mhz .A number of embodiments have been described .", "label": "", "metadata": {}, "score": "54.13427"}
{"text": "Munich : K. G. Saur , 2006 .Tufis et al .2004 Tufis , Dan , Radu Ion , and Nancy Ide . \" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) .", "label": "", "metadata": {}, "score": "54.135174"}
{"text": "With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts .In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections .", "label": "", "metadata": {}, "score": "54.16887"}
{"text": "Finally , the source and target documents may be modified to facilitate the supervised learning process ( Step S 120 ) .The leaves 11 , 13 of the source and/or target documents 10 , 12 or the document as a whole may be annotated such that the alignment of all the leaves may be retrieved .", "label": "", "metadata": {}, "score": "54.2104"}
{"text": "Using elastic bi - fragments in phrase - based translation methods raises a number of issues for the statistical translation model , which the present method addresses by incorporation of \" feature functions \" into a log - linear statistical translation model .", "label": "", "metadata": {}, "score": "54.21526"}
{"text": "The lexicon builder 125 extracts potential translation word pairs based on one or more clues .These clues may include similar spelling , similar context , preserving word similarity , and word frequency .When words are adopted into another language , their spelling might change slightly in a manner that can not be simply generalized in a rule , e.g. , \" website \" and \" Webseite .", "label": "", "metadata": {}, "score": "54.215405"}
{"text": "A machine translation method for translating source text from a first language to target text in a second language , comprising : . receiving the source text in the first language ; . retrieving text fragments from the second language from the library corresponding to text fragments in the source text ; . generating , by a processor , at least one target hypothesis , each of said target hypotheses comprising text fragments selected from the second language ; and .", "label": "", "metadata": {}, "score": "54.29296"}
{"text": "However , since the possibility also exists for these N - M matches to be combinations of a 1 - 1 match with one or more 1 - 0 and/or 0 - 1 matches , the method applies one or more tests to distinguish probable M - N matches from probable combinations of matches ( Step S 116 ) .", "label": "", "metadata": {}, "score": "54.31456"}
{"text": "FIG .2 , a block diagram of an automated natural language translation system 10 for translating text comprising elastic bi - fragments is illustrated .Text is input to the system by an input device 12 .Input text may be directly input into the natural language translation system 10 ( for example , as with a person typing sentences into a computer using a keyboard ) .", "label": "", "metadata": {}, "score": "54.316677"}
{"text": "In one embodiment , the representative of a set of target nodes is always the rightmost node in that set .In other potential embodiments , the representative may be selected by corpus statistics regarding which should be the head , or by hand - crafted rules .", "label": "", "metadata": {}, "score": "54.31993"}
{"text": "8D and 8E , and such as those shown in .FIGS .9A and 9B .The present discussion of the order model will proceed with respect to the exemplary structures shown in .FIGS .8D and 8E .Assume that at runtime an input sentence is \" Empty your recycle bin \" .", "label": "", "metadata": {}, "score": "54.370506"}
{"text": "To avoid this , the system 400 appends an end - of - string marker \" $ \" that appears nowhere else in the string .For clarity , the drawings only show the $ marker when necessary .Each monolingual corpus given as input to the system 400 may be divided into a set of sentences .", "label": "", "metadata": {}, "score": "54.39974"}
{"text": "There are two alignment processes .In corpus preparation , the alignment process creates aligned data .During training , the alignment process uses a program such as MGIZA++ to create word alignment files .BLEU score .SMT .BLEU stands for B i-", "label": "", "metadata": {}, "score": "54.4014"}
{"text": "reord .s .I . t .J .d .K . ) k .K .i .i .d . k . )I .V .i . k . )Beam Search Procedure .Beam - search procedures are described , for example , in Cancedda , et al . , incorporated herein by reference .", "label": "", "metadata": {}, "score": "54.421574"}
{"text": "As discussed above , these contiguous bi - fragments may also be modeled as elastic bi - fragments .Bi - Fragment Library .To produce translations , the method relies on a collection of bi - fragments , which is referred to as a bi - fragment library 20 .", "label": "", "metadata": {}, "score": "54.432514"}
{"text": "Each dependency tree is descended , looking at each level independently , in order to calculate these probabilities .Referring again to the overall operation of the training system 200 , as illustrated in .FIGS . 2 and 3 , agreement model training component 228 also accesses corpus 400 to train agreement model 208 .", "label": "", "metadata": {}, "score": "54.461056"}
{"text": "This indication would become stronger if , for example , the sequences following d and y in the two corpora would also be aligned .One way to verify this is to reverse both strings , build a BST for the reversed corpora ( a reverse BST ) , and look for a common path that diverges at the same d and y. .", "label": "", "metadata": {}, "score": "54.484947"}
{"text": "At step S 114 , candidate ( 1-n ) , ( m-1 ) , and ( m - n ) pairs are identified from the algorithm in step S 112 by identifying groups of adjacent matches which have a leaf in common .", "label": "", "metadata": {}, "score": "54.51156"}
{"text": "Such alignments have been used , for example , in DNA sequencing .In the present case , the sequences are composed of document leaves and their contents .For strings , many algorithms have been proposed .The method described here adapts a string alignment method which calculates the Levenstein distance ( minimum edit distance ) between two strings .", "label": "", "metadata": {}, "score": "54.51687"}
{"text": "No .5,477,451 by Brown , et al .entitled \" METHOD AND SYSTEM FOR NATURAL LANGUAGE TRANSLATION , \" and U.S. Pat .No .6,304,841 by Berger , et al . , entitled \" AUTOMATIC CONSTRUCTION OF CONDITIONAL EXPONENTIAL MODELS FROM ELEMENTARY FEATURES \" describe word - based statistical machine translation methods and systems for natural language translation .", "label": "", "metadata": {}, "score": "54.622986"}
{"text": "U.S. Pat .No .6,950,815 entitled \" CONTENT MANAGEMENT SYSTEM AND METHODOLOGY FEATURING QUERY CONVERSION CAPABILITY FOR EFFICIENT SEARCHING \" by Tijare , et al . , describes a content management system which employs a hierarchical item type tree - based structure including tables at different levels to store metadata for items .", "label": "", "metadata": {}, "score": "54.636055"}
{"text": "Thus , order model 206 is trained to generate a probabilistic prediction of the order in which the treelets are to appear in the target language dependency tree .Recall that the word - aligned parallel dependency tree corpus includes word - aligned parallel dependency trees for training data such as those shown in .", "label": "", "metadata": {}, "score": "54.697166"}
{"text": "However , less common discontiguous \" phrases \" will be nearly impossible to learn , given practical limits on the size of the training data set .For these reasons , and others , some researchers have attempted to incorporate syntactic information into statistical machine translation processes .", "label": "", "metadata": {}, "score": "54.721123"}
{"text": "FIG .8 shows some alignments we can extract from the BST in .FIG .7 , a portion of which is shown in .FIG .8 .As can be seen in .For alignment extraction , we are interested in edges of the third type , because they mark ends of alignments .", "label": "", "metadata": {}, "score": "54.72267"}
{"text": "One alternative is to allow the stack to increase in size , beyond its predetermined maximum number of hypotheses , where a consumption decision would otherwise result in the hypothesis being expelled from the stack .Another approach is to give a heuristic handicap to hypotheses with a large buffer size , as compared to other hypotheses with the same source coverage .", "label": "", "metadata": {}, "score": "54.746952"}
{"text": "A \" language model \" or \" lm \" is a statistical description of one language that includes the frequencies of token - based n - grams occurrences in a corpus .The \" lm \" is trained from a large monolingual corpus and saved as a file .", "label": "", "metadata": {}, "score": "54.780487"}
{"text": "6 - 7 ) .The algorithm is applied to each of the partitions formed in the initial match .The results of the sub - problems are then combined to form an overall result .M - N match : From the matrix match , candidates for m - n , m-1 , and 1-n correspondences are identified .", "label": "", "metadata": {}, "score": "54.93623"}
{"text": "Merge zero - similarity pairs with their neighbors ( Substep S 116 B 1 ) .Split m - n match candidates into minimal components ( Substep S 116 B 2 ) .( c ) Split candidates that are contradictory are removed and the split is performed .", "label": "", "metadata": {}, "score": "54.940193"}
{"text": "No .11/137,566 , filed May 26 , 2005 , entitled \" METHOD AND APPARATUS FOR DETERMINING LOGICAL DOCUMENT STRUCTURE , \" by Jean - Luc Meunier .U.S. application Ser .No .10/986,490 , filed Nov. 10 , 2004 , entitled \" SYSTEM AND METHOD FOR TRANSFORMING LEGACY DOCUMENTS INTO XML DOCUMENTS , \" by Boris Chidlovskii .", "label": "", "metadata": {}, "score": "54.956696"}
{"text": "\" The decoder 608 constructs all hypotheses at the node under consideration , scores each hypothesis with all of the models , applying the weights associated with each model , and summing them to obtain a single score for each hypothesis at this node .", "label": "", "metadata": {}, "score": "54.98811"}
{"text": "FIG .14B as having a source sub - tree rooted at the node \" computer . \" Selecting the source node and identifying all treelet translation pairs rooted at that source node is indicated by block 702 in .FIG .", "label": "", "metadata": {}, "score": "55.0562"}
{"text": "In one embodiment , gaps on the source side are estimated independently of the target side .The pooled estimates can than replace standard estimates , or they can be used as a back - off when too few occurrences of a bi - phrase are observed , and the standard estimates are likely to be unreliable .", "label": "", "metadata": {}, "score": "55.067627"}
{"text": "The scoring may include maximizing a log - linear scoring function in which weights are assigned to each of the feature functions .In another aspect , a machine translation system translates source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "55.072205"}
{"text": "The model then determines whether a given one of the pair is closer or further than the other of the pair .The model iterates through all of the pre - modifiers at that level thus finding the closest , the next closest , etc .", "label": "", "metadata": {}, "score": "55.10028"}
{"text": "The method of .claim 3 wherein adjusting the target language dependency structure comprises : . re - arranging the target language dependency structure until lexical items in the target language text string read from the target language dependency structure are the same order that they appear in the target language text fragment .", "label": "", "metadata": {}, "score": "55.103058"}
{"text": "14A .Matching treelet translation pairs component 604 then extracts treelets from treelet translation pair database 204 that have source language treelets which match treelets in the source language dependency tree 614 .Assume , for example , that the treelet translation pairs extracted by component 604 and generated as output 618 are those shown in .", "label": "", "metadata": {}, "score": "55.118607"}
{"text": "All these similarity measures return values between 0 and the length of the shorter of the two contents .It has been found that the similarity measures SUB and WORD are much more efficient to compute than the others although in general , RMBL gives the best results .", "label": "", "metadata": {}, "score": "55.131626"}
{"text": "Then : .For each pair of leaves in each of the k blocks the similarity is calculated ( e.g. , SUB or WORD ) ( Substep S 108 A ) .If this similarity passes a certain threshold , then the pair of leaves is added to the initial match ( Substep S 108 A ) .", "label": "", "metadata": {}, "score": "55.153877"}
{"text": "The source language dependency trees are represented by numeral 262 in .FIG .2 .Dependency tree projection component 220 then projects dependencies from the source language dependency tree representation onto the aligned target language word segmented input to obtain a target language dependency tree ( or dependency structure ) .", "label": "", "metadata": {}, "score": "55.163536"}
{"text": "A computer readable medium storing computer readable instructions which , when executed by a computer , cause a computer to perform a method for generating a target language dependency structure , the method comprising : . accessing a training data corpus having a plurality of pairs of parallel text fragments , each pair of parallel text fragments comprising a source language text fragment and a corresponding target language text fragment ; . obtaining an aligned structure in which lexical items in a source language dependency structure , generated based on a source language text fragment , are aligned with lexical items in a corresponding target language text fragment ; and . identifying a parent node for the plurality of lexical items in the target language dependency structure ; . identifying a right - most one of the plurality of lexical items in the target language text fragment ; . assigning the right - most lexical item as dependent from the parent node ; and . assigning a remainder of the plurality of lexical items in the target language text fragment as dependent from the right - most lexical item ; . determining whether the target language dependency structure is properly indicative of the target language text fragment ; and . if not , adjusting the target language dependency structure so it is properly indicative of the target language text fragment .", "label": "", "metadata": {}, "score": "55.170303"}
{"text": "A recaser model is a special translation model translates lower cased data to \" natural \" cased text ( upper and lower casing ) .reordering table .SMT .A \" reordering table \" contains the statistical frequencies that describe the changes in word order between source and target languages , such as \" big house \" versus \" house big \" .", "label": "", "metadata": {}, "score": "55.29427"}
{"text": "u . s .I . t .J .d .K . )i .J . log .Pr .t .j .t .j .N .j .\" Word - count \" and \" bi - fragment \" count feature functions h wc and h bc , illustrated at 40 and 42 , respectively .", "label": "", "metadata": {}, "score": "55.306496"}
{"text": "The sequence 52 is a contiguous string of target words , and the buffer 46 comprises a set of target phrase suffixes 54 .Each target phrase suffix 54 constitutes the non - consumed portion of a target phrase for which the ongoing hypothesis has established that a gap exists between a first portion and a second portion of the target phrase .", "label": "", "metadata": {}, "score": "55.381683"}
{"text": "State - of - the - art SMT systems such as Pharaoh ( Koehn , 2004a ) still employ a simplistic model of the reordering process to do non - local reordering .This model penalizes any reordering no matter the words .", "label": "", "metadata": {}, "score": "55.404167"}
{"text": "A \" reordering \" feature function h reord ( s 1 I , t 1 J , d 1 K ) , illustrated at 46 , measures the difference in order between source and target fragments .Such feature functions are a standard component in most existing phrase - based machine translation systems .", "label": "", "metadata": {}, "score": "55.44966"}
{"text": "10 .An advantage of the algorithm over previous approaches is that we do not provide as input to the algorithm a list of unknown words .Instead , the system automatically learns from the corpus both the unknown words and their translation , upon discovery of appropriate context alignments .", "label": "", "metadata": {}, "score": "55.480507"}
{"text": "FIG .6F is a flow diagram illustrating how component 220 corrects this error .First , component 220 identifies a word in the target string that is read from the target dependency tree that is out of order .This is indicated by block 300 in .", "label": "", "metadata": {}, "score": "55.510475"}
{"text": "The logical connections depicted in .FIG .1 include a local area network ( LAN ) 171 and a wide area network ( WAN ) 173 , but may also include other networks .Such networking environments are commonplace in offices , enterprise - wide computer networks , intranets and the Internet .", "label": "", "metadata": {}, "score": "55.547768"}
{"text": "Bannard , C. and Callison - Burch , C. , \" Paraphrasing with Bilingual Parallel Corpora , \" In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ( Ann Arbor , MI , Jun. 25 - 30 , 2005 ) .", "label": "", "metadata": {}, "score": "55.571693"}
{"text": "10 is psuedocode describing an algorithm for learning translations of unknown words .DETAILED DESCRIPTION .FIG .1 shows a system 100 for building a translation lexicon 105 according to an embodiment .The system may use non - parallel monolingual corpora 110 , 115 in two languages to automatically generate one - to - one mapping of words in the two languages .", "label": "", "metadata": {}, "score": "55.58175"}
{"text": "FIG .3 .Agreement model training component 228 accesses all of the target language dependency trees in corpus 400 .The agreement model 208 attempts to predict each child given its head .These are predicted , regardless of the order of the children .", "label": "", "metadata": {}, "score": "55.616096"}
{"text": "Fewer or more matches may be employed than those listed above .Each of these matches is described in greater detail below .In the exemplary embodiment , the sequences to be aligned are sequences of leaves 11 , 13 of tree - structured XML - documents 10 , 12 , such as the two examples shown above in .", "label": "", "metadata": {}, "score": "55.66806"}
{"text": "Also that various presently unforeseen or unanticipated alternatives , modifications , variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims .P. F. Brown , J. C. Lai , R. L. Mercer , \" Aligning Sentences in Parallel Corpora \" Proceedings of the Annual Meeting of the Association for Computional Linguistics ( ACL ) , pp . 169 - 176 , Berkeley , CA ( Jun. 1991 ) .", "label": "", "metadata": {}, "score": "55.698086"}
{"text": "The method of .claim 1 , wherein the identifying of the set of matches for which a total cost is minimal comprises : . from the matrix alignments , identifying candidate matches in which a leaf of at least one of the source and target documents matches a combination of leaves of the other of the source and target documents ; . refining the candidate matches to identify the probable matches ; and .", "label": "", "metadata": {}, "score": "55.710648"}
{"text": "( 3 ) e 1 is already in the sequence but e 2 is still in the buffer .In this case , the minimum size m of the gap between e 1 and e 2 is known ( namely , the number of words in the sequence which are to the right of e 1 .", "label": "", "metadata": {}, "score": "55.758934"}
{"text": "Decoding , as this procedure is called , is done by expanding a search graph on the fly .Each node H in the graph is uniquely defined by the sequence of decisions d 1 k that connect it to an initial \" null hypothesis \" node H 0 ( a starting hypothesis in which there are no words translated ) .", "label": "", "metadata": {}, "score": "55.777843"}
{"text": "The correctness of word mappings acquired in this fashion may depend highly on word length .While identical three - letter words were only translations of each other 60 % of the time , this was true for 98 % of ten - letter words .", "label": "", "metadata": {}, "score": "55.80411"}
{"text": "If the sum of these scores is lower than the lowest score currently in the N - best list , then the final candidate is bound to fall off the N - best list , since adding the order model probability will only drop the overall score .", "label": "", "metadata": {}, "score": "55.81259"}
{"text": "FIG .14D .The retrieved best translation pairs will include translation pair 700 .Now , decoder 608 must determine how to connect the target language portion of translation pair 700 onto the target language portion of translation pair 710 .", "label": "", "metadata": {}, "score": "55.823975"}
{"text": "If the lexicon is probabilistic , each matching between two words will be weighted by the corresponding translation probability .The paths in the resulting bilingual tree will also have weights associated with them , defined as the product of the matching probabilities of the words along the path .", "label": "", "metadata": {}, "score": "55.913517"}
{"text": "Our results show that MBR decoding can be used to tune statistical MT performance for specific loss functions . \" ...This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .", "label": "", "metadata": {}, "score": "55.931858"}
{"text": "3 is a flow diagram of a method for machine translation of text which utilizes elastic bi - fragments according to one aspect of the exemplary embodiment ; .FIG .4 is an example of the combination of bi - fragments to produce a translation ; .", "label": "", "metadata": {}, "score": "56.005775"}
{"text": "The format of a second or target document , with which the first document is to be aligned , may be a semantically rich extended attribute grammar , such as an Extended Markup Language ( XML ) format .By way of example , documents in semantically poor XML format are considered as the first format .", "label": "", "metadata": {}, "score": "56.04212"}
{"text": "Searching by selectional preference .The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \" means . \"In building the lexicon , we automatically assign an argument structure to all of the verbs .", "label": "", "metadata": {}, "score": "56.045357"}
{"text": "In addition , as mentioned above , prior art phrasal statistical machine translation systems are currently limited to phrases that are contiguous .By this , the prior art systems meant that the phrases are contiguous in both the source and target surface strings .", "label": "", "metadata": {}, "score": "56.23378"}
{"text": "If t l is lower in the dependency tree , create a dependency between t i and t l ; otherwise create a dependency between t i and t r .( 4 ) Unaligned Root : If the root of the source dependency tree is unaligned , then after applying all the above steps there will be multiple target dependency trees , each with a single root .", "label": "", "metadata": {}, "score": "56.2445"}
{"text": "A General Method Applicable to the Search For Similarities In The Amino Acid Sequence Of Two Proteins .\" J. Molecular Biol . , 48:443 - 453 ( 1970 ) , T. F. Smith and M. S. Waterman , \" Identification of Common Molecular Subsequences , \" J. Molecular Biol .", "label": "", "metadata": {}, "score": "56.24939"}
{"text": "An exemplary outlier is illustrated in .FIG .1 .In one embodiment , outliers are ignored in determining the distribution .As will be appreciated , this occurs because the alignments are constructed , and not directly observed .In such cases , it may be appropriate to discard the few suspect instances and force the distribution to have mean 0 .", "label": "", "metadata": {}, "score": "56.262417"}
{"text": "\u00c9.Gaussier , Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora , In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Intl Conference on Computational Linguistics , pp .444 - 450 , San Francisco , CA , 1998 .", "label": "", "metadata": {}, "score": "56.272953"}
{"text": "Fox , H. , \" Phrasal Cohesion and Statistical Machine Translation \" Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , Jul. 2002 , pp .304 - 311 .Association for Computational Linguistics .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :", "label": "", "metadata": {}, "score": "56.275284"}
{"text": "As a consequence , the model is able to consider an extended and more representative class of potential translations and to evaluate them more accurately .The exemplary embodiment introduces the notion of elastic bi - phrase , and provides a decoding mechanism for producing translations using the resulting model .", "label": "", "metadata": {}, "score": "56.295578"}
{"text": "33 , No . 2 , pp . 111 - 124 , 2005 .K.Papineni , et al . , Bleu : A Method for Automatic Evaluation of Machine Translation , Proc . of the 40th Annual Meeting of the Assn . for Computational Linguistics ( ACL ) , Philadelphia , pp . 311 - 318 , Jul. 2002 .", "label": "", "metadata": {}, "score": "56.323944"}
{"text": "The content of the buffer may vary during the extension of a hypothesis as new suffixes are added and consumed suffixes are removed .Where a suffix includes more than one word , the words may be removed from the buffer one at a time .", "label": "", "metadata": {}, "score": "56.447697"}
{"text": "We calculate the final matrix as before .Specifically , each new matrix element m(i , j ) is calculated from the initial cost m 0 ( ij ) as follows : . m .j . ) m .j . ) min .", "label": "", "metadata": {}, "score": "56.475563"}
{"text": "This is a complex task , because the number of possible translations typically grows exponentially with the size of the input , and so not all solutions can be examined in practice .INCORPORATION BY REFERENCE .The following references , the disclosures of which are incorporated herein in their entireties by reference , are mentioned : .", "label": "", "metadata": {}, "score": "56.529423"}
{"text": "s .i . t .j . )l .i .j .However , this definition is independent of the length of the contents .Thus , a content consisting of just one character is a perfect match ( resulting in zero cost ) for another ( possibly very long ) content containing that one character .", "label": "", "metadata": {}, "score": "56.594322"}
{"text": "P . k . ) k . k . which is fully determined by a single parameter of the distribution , its mean \u03bc , as illustrated in .FIG .1 .In this way , gap sizes for which no observed examples are found in the corpus may nevertheless be assigned a non - zero probability of being found in a translation .", "label": "", "metadata": {}, "score": "56.647076"}
{"text": "In the string scenario , one letter corresponds precisely to one letter or to a gap .There are sometimes different alignments of equal quality , but in each one of them there are only 1 - 0 , 0 - 1 , and 1 - 1 correspondences .", "label": "", "metadata": {}, "score": "56.654392"}
{"text": "m .M . m .h .m .s .I . t .J .d .K . )I is a normalization constant and where at least one of the feature functions h m is selected from : .", "label": "", "metadata": {}, "score": "56.659218"}
{"text": "The diagonal move has priority in case of equality of values .If a diagonal move is about to be made , this indicates that the two current string elements are aligned .If a move to the left is about to be made , this indicates that an element of t 1 is aligned with a gap ( meaning a gap has to be inserted into s 1 .", "label": "", "metadata": {}, "score": "56.66311"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "56.66471"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "56.66471"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "56.66471"}
{"text": "We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .Compared to the standard practice of intersecting predictions of independently - trained models , joint training provides a 32 % reduction in AER .", "label": "", "metadata": {}, "score": "56.677635"}
{"text": "It functions as a sophisticated dictionary between the source and target languages .Phrase tables and reordering tables are translation model components .pipeline .SMT .A \" pipeline \" is a toolchain of processes connected by standard streams , so that the output of each process ( stdout ) feeds directly as input ( stdin ) to the next one . recaser model .", "label": "", "metadata": {}, "score": "56.695442"}
{"text": "an unmatched target leaf ; . a processing module which identifies matches for which a total cost is minimal , wherein each of the leaves is in at least one of the identified matches ; . a processing module which identifies , from the identified matches , groups of matches wherein each match in the group has a leaf in common ; . a processing module which identifies , from the groups , probable matches in which more than one target leaf is matched with at least one source leaf and probable matches where more than one source leaf is matched with a target leaf ; and .", "label": "", "metadata": {}, "score": "56.75006"}
{"text": "Verbs , adjectives , adverbs and other part of speech may be handled in a similar way .They might also provide useful context information that is beneficial to building a noun lexicon .These methods may be also useful given a different starting point .", "label": "", "metadata": {}, "score": "56.773052"}
{"text": "I . t .J .d .K . ) k .K . log .Pr . target .d . k . ) source .d . k . )The h bf feature estimates the log of the probability of observing the bi - phrase configuration ( s , t ) given the source phrase configuration s in the training corpus through the ratio of respective counts of these configurations .", "label": "", "metadata": {}, "score": "56.799084"}
{"text": "Machine translation has typically been attempted using one of two different approaches .The first is a knowledge engineered approach , typically using a linguistic parser and hand - crafted transfer rules .Almost all commercial translation systems ( such as Systran ) are of this type .", "label": "", "metadata": {}, "score": "56.85178"}
{"text": "applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar spelling .", "label": "", "metadata": {}, "score": "56.880493"}
{"text": "In the illustrated embodiment , at extension step S 201 , the consumed portion of a target phrase do is highlighted .In the buffer , the non - consumed portion want of the phrase is stored along with the index I 2 , indicating that the last word of the consumed portion of the phrase is in the second position of the sequence .", "label": "", "metadata": {}, "score": "56.8935"}
{"text": "The first and second text fragments may be a word or a phrase .The first and second fragments are associated in a word - aligned corpora , such as a library stored in memory , whereby a target fragment corresponding to an input source fragment may be retrieved .", "label": "", "metadata": {}, "score": "56.92678"}
{"text": "For example , in an implementation , an English - German translation lexicon was generated from a 1990 - 1992 Wall Street Journal corpus on the English side and a 1995 - 1996 German news wire ( DPA ) on the German side .", "label": "", "metadata": {}, "score": "57.018383"}
{"text": "The document alignment method of .claim 1 , wherein at least one of the first and second tree structured formats is an extended mark up language format .The document alignment method of .claim 1 , wherein the target document is in a semantically richer extended mark up language format than the source document .", "label": "", "metadata": {}, "score": "57.022163"}
{"text": "In order to predict this , the order model 206 generates a probability shown in Equation 2 .For the present example , it can be seen that the order model predicts one pre - modifier and two post - modifiers on the level of the target language dependency tree currently under consideration .", "label": "", "metadata": {}, "score": "57.025806"}
{"text": "5 illustrates the development of hypothesis stacks for a partial translation of the sentence illustrated in .FIG .4 . DETAILED DESCRIPTION .Aspects of the exemplary embodiment relate to a method for translating source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "57.075943"}
{"text": "The system also found translations for thirty unknown French words .Of these , nine were correct , which means a precision of 30 % .For each of the two corpora , building the monolingual GST took only 1.5 minutes .", "label": "", "metadata": {}, "score": "57.25393"}
{"text": "This is indicated by block 320 in .FIG .7 .Of course , in the present example , component 220 identifies the word \" de \" .Next , component 220 identifies the closest dependency arc above the unaligned word in the target dependency structure .", "label": "", "metadata": {}, "score": "57.31096"}
{"text": "INCORPORATION BY REFERENCE .The following references , the disclosures of which are incorporated herein in their entireties by reference , are mentioned : .The system treats the conversion as transforming ordered trees of one schema into ordered trees of another schema .", "label": "", "metadata": {}, "score": "57.31636"}
{"text": "The 0 stack , by convention contains a null hypothesis .Stack 1 ( identified as 62 ) contains hypotheses in which the source projection is a string of one word , such as Je and danser .The beam search starts with an hypothesis in the first stack and adds a bi - phrase or consumes a buffered word using the principles outlined above .", "label": "", "metadata": {}, "score": "57.41335"}
{"text": "Our approach learns from hand aligned data how to combine several automatic word alignments to one superior word alignment .The automatic word alignments are created from the same data that has been preprocessed with different tokenization schemes .Thus utilizing the different strengths that different tokenization schemes exhibit in word alignment .", "label": "", "metadata": {}, "score": "57.434925"}
{"text": "Our approach learns from hand aligned data how to combine several automatic word alignments to one superior word alignment .The automatic word alignments are created from the same data that has been preprocessed with different tokenization schemes .Thus utilizing the different strengths that different tokenization schemes exhibit in word alignment .", "label": "", "metadata": {}, "score": "57.434925"}
{"text": "Our approach learns from hand aligned data how to combine several automatic word alignments to one superior word alignment .The automatic word alignments are created from the same data that has been preprocessed with different tokenization schemes .Thus utilizing the different strengths that different tokenization schemes exhibit in word alignment .", "label": "", "metadata": {}, "score": "57.434925"}
{"text": "Therefore , those two enumerated treelets of size 1 are discarded .Extractor 224 then determines whether there are any larger treelets to be considered , as indicated by block 412 .If so , then the size of the treelets to be considered is increased by 1 at block 414 and processing continues at block 406 .", "label": "", "metadata": {}, "score": "57.435944"}
{"text": "7A .It can readily be seen that the word \" de \" in the target language string has no alignment to the source language string .FIG .7B shows the word alignments of .FIG .7A , along with a representation of the dependency tree ( the arcs on the source language string show the dependencies on the source language input ) .", "label": "", "metadata": {}, "score": "57.438168"}
{"text": "4B .The structure shown in .FIG .4B is referred to as a dependency tree where each word comprises a node of the tree and the uppermost node comprises the head word or root of the tree .Therefore , the dependency tree in .", "label": "", "metadata": {}, "score": "57.459175"}
{"text": "Afterwards , the results are combined to form the global result ( step S 118 ) .In the case described above , the similarity measures between two leaves s i and t j always return a value sim(s i , t j ) between 0 and the length of the shorter of the two contents , which is denoted l ij .", "label": "", "metadata": {}, "score": "57.488487"}
{"text": "claim 14 , wherein the translation scoring function outputs a translation for which the log - linear scoring model is maximized .A machine translation system for translating source text from a first language to target text in a second language , comprising : . memory which stores instructions for performing the method of .", "label": "", "metadata": {}, "score": "57.502777"}
{"text": "The notations s j ( i ) and t k ( i ) are used to denote both leaves and their contents ; the correct semantics should be clear from the context .i ( i ) and likewise t ( i ) for the target side .", "label": "", "metadata": {}, "score": "57.50518"}
{"text": "11/315,043 , filed Dec. 22 , 2005 , entitled MACHINE TRANSLATION USING NON - CONTIGUOUS FRAGMENTS OF TEXT , by Nicola Cancedda , et al . , the disclosure of which is incorporated herein by reference in its entirety .BACKGROUND .", "label": "", "metadata": {}, "score": "57.517754"}
{"text": "Different types of alignment can be identified .For example , six different types of matches may be identified : . 1 - 0 ( unmatched source leaf ) , such as the pair ( [ s 1 ] , [ ] ) in the above example . 0 - 1 ( unmatched target leaf ) , which could be represented by a pair such as ( [ ] , [ t n ] ) .", "label": "", "metadata": {}, "score": "57.596527"}
{"text": "The computer readable medium of .claim 22 wherein adjusting the target language dependency structure comprises : . re - arranging the target language dependency structure until lexical items in the target language text string read from the target language dependency structure are the same order that they appear in the target language text fragment .", "label": "", "metadata": {}, "score": "57.611687"}
{"text": "Sinclair 1987 Sinclair , John M. ( ed . )Looking Up : an account of the COBUILD project in lexical computing .Collins , 1987 .Singh and Husain 2005 Singh , Anil Kumar and Samar Husain .\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) .", "label": "", "metadata": {}, "score": "57.633316"}
{"text": "Using alignment techniques from phrasebased statistical machine translation , we show how paraphrases ... \" .Previous work has used monolingual parallel corpora to extract and generate paraphrases .We show that this task can be done using bilingual parallel corpora , a much more commonly available resource .", "label": "", "metadata": {}, "score": "57.648716"}
{"text": "an unmatched target leaf ; . identifying matches for which a total cost is minimal , wherein each of the leaves is in at least one of the identified matches ; . identifying , from the identified matches , groups of matches wherein each match in the group has a leaf in common ; . identifying , from the groups , probable matches in which more than one target leaf is matched with at least one source leaf and probable matches where more than one source leaf is matched with a target leaf ; . outputting an alignment between leaves of the target document and leaves of the source document which includes the probable matches .", "label": "", "metadata": {}, "score": "57.693726"}
{"text": "The cost thus tends to decrease as the length of the shorter content increases ( for constant similarity measures ) , thereby allowing the algorithm to favor matches between such pairs .The weighting factor .l .i .j .", "label": "", "metadata": {}, "score": "57.72283"}
{"text": "The metadata is accessed and a view of the accessed metadata is created in a second data format .The second format is compatible with a query language , such as XML .U.S. Pat .U.S. Pat .An extended attribute coupling grammar couples the first and second extended attribute grammars .", "label": "", "metadata": {}, "score": "57.78581"}
{"text": "Such a decision produces a new state , which is placed on its proper stack , at a position determined by its heuristic cost , which can be an optimistic estimate of the best complete sequence of decisions passing through this state .", "label": "", "metadata": {}, "score": "57.824535"}
{"text": "At step S 112 , the processing component builds a target language string comprising target fragments corresponding to fragments in the source language .Steps S 112 A , 112 B , 112 C , 112 D , 112 E may be repeated multiple times in the process of generating a target language text string .", "label": "", "metadata": {}, "score": "57.873657"}
{"text": "9C .Thus , the treelet translation pair database will contain treelet translation pairs to indicate that \" ne . . .pas \" may be translated as \" not \" .Referring back again to the overall training system 200 shown in .", "label": "", "metadata": {}, "score": "57.880203"}
{"text": "Instead of attempting to model the translation of each word independently , phrasal statistical machine translation attempts to model how chunks of words translate together .This captures an important intuition of foreign language learning - that is , small idioms and common phrases are both idiosyncratic and important for both fluency and fidelity .", "label": "", "metadata": {}, "score": "57.883614"}
{"text": "ELAST .s .I . t .J .d .K . ) k .K .EL . target .d . k . ) source .d . k . ) where EL may be a function of the probabilities of gaps in the target and source fragments for a bi - fragment .", "label": "", "metadata": {}, "score": "57.892853"}
{"text": "An alignment between leaves of the target document and leaves of the source document is output which includes the probable matches . inputting source leaves of a source document in first tree structured format , the first tree structured format comprising nodes which are ultimately connected with the source leaves by paths , text content of the source document being distributed among the source leaves ; . inputting target leaves of a target document in second tree structured format , the second tree structured format comprising nodes which are ultimately connected with the target leaves by paths , text content of the target document being distributed among the target leaves ; . assigning a cost to each of a plurality of matches based on text content of the leaves , each match comprising elements selected from the group consisting of : . a source leaf and a target leaf , . an unmatched source leaf , and .", "label": "", "metadata": {}, "score": "57.89991"}
{"text": "The method of .claim 1 , wherein the identifying of probable matches in which more than one target leaf is matched with at least one source leaf and probable matches where more than one source leaf is matched with a target leaf includes : . merging a pair of a group with another pair where the similarity between the elements of one of the pairs is 0 .", "label": "", "metadata": {}, "score": "57.908474"}
{"text": "6F .Thus , the word \" ne \" is detached from being a pre - modifier of the word \" pas \" and is reattached as being a pre - modifier of the word \" parle \" .Another problem which can be encountered in the present invention arises when a word in the word segmented target string is unaligned to any word in the source string .", "label": "", "metadata": {}, "score": "57.93348"}
{"text": "There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \" context \" is most appropriate itself .One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al .", "label": "", "metadata": {}, "score": "57.941967"}
{"text": "An apparatus comprising : . a lexicon builder operative to be executed to expand the seed lexicon by identifying possible translations of words in the first and second corpora using one or more clues .The apparatus of .claim 24 , wherein the lexicon builder is configured to use the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}, "score": "58.030388"}
{"text": "The document alignment apparatus of .claim 18 , further comprising : . a processing module for subdividing the source and target document into blocks , each block comprising a plurality of target leaves and a plurality of target leaves .The document alignment apparatus of .", "label": "", "metadata": {}, "score": "58.050873"}
{"text": "The method takes as input a document in the first format and a document in the second format .Both documents have a tree structure in which nodes are labeled with labels and are connected by paths to other nodes and ultimately to leaves , which are also labeled with labels .", "label": "", "metadata": {}, "score": "58.102943"}
{"text": "For matches where a leaf of the source document is not paired with a leaf of a target document , or vice versa ( unmatched pairs ) , a similarity measure of 0 is automatically assigned .The initial match and matrix match can both use these similarity measures , as will be described below .", "label": "", "metadata": {}, "score": "58.206764"}
{"text": "7E .This is indicated by block 324 in .FIG .7 .The dependency structure shown in .FIG .7E can , of course , be re - written as the dependency tree shown in .FIG .7F .", "label": "", "metadata": {}, "score": "58.215717"}
{"text": "4C .Therefore , the output of target language word segmentation component 216 is simply a list of words in the target sentence part of the aligned pair of sentences .Performing word segmentation on the target language input is indicated by block 254 in .", "label": "", "metadata": {}, "score": "58.245728"}
{"text": "FIG .3 ): .Buffer consumption : one word from a buffered suffix ( step S 112 E ) and appending it to the end of the sequence ( step S 112 G ) .The process ends when both the coverage of the source is complete and the buffer is empty .", "label": "", "metadata": {}, "score": "58.27559"}
{"text": "8E .FIG .8E shows both the source and target language dependency structures as dependency trees , and the word alignments are indicated by the dashed lines between the two dependency trees .Thus , the output of dependency tree projection component 220 yields a corpus of word aligned parallel dependency tree structures ( or dependency structures ) such as those shown in .", "label": "", "metadata": {}, "score": "58.281723"}
{"text": "Pairs of elements , such as illustrated at 48 , which lie on the path of the minimum edit distance are said to be aligned .From .FIG .4 , the algorithm obtains the following alignment : .Many variations of this basic algorithm exist , notably in the domain of bioinformatics .", "label": "", "metadata": {}, "score": "58.323975"}
{"text": "FIG .14A , decoder 608 starts with source node \" your . \"However , there are no treelet translation pairs in .FIG .14B which are rooted at the node \" your . \"Therefore , the decoder simply moves up to the next level and analyzes the node \" computer . \"", "label": "", "metadata": {}, "score": "58.363136"}
{"text": "See I. D. Melamed , Bitext Maps and Alignment Via Pattern Recognition , \" Computational Linguistics , 25(1):107 - 130 ( 1999 ) .There are several differences between the sentence alignment problem and the problem of tree - structured document alignment .", "label": "", "metadata": {}, "score": "58.593483"}
{"text": "The alignment method includes identifying these correspondences and lack of correspondences allowing the transformation rules which map them to be determined .As used herein , a document can comprise a portion of a larger document , such as a chapter or page of a larger document .", "label": "", "metadata": {}, "score": "58.709633"}
{"text": "This is repeated until no further improvement to the objective function is produced .In one prior art system , Och , MINIMUM ERROR RATE TRAINING AND STATISTICAL MACHINE TRANSLATION , in Proceedings of the ACL ( 2003 ) a method is described for optimizing an objective function directly ( such as the BLEU score ) .", "label": "", "metadata": {}, "score": "58.728622"}
{"text": "The exemplary method includes aligning leaves of a source document in the first format with leaves of a target document in the second format .Once the two documents are aligned in this way , a set of transformations can be identified which result in the alignment .", "label": "", "metadata": {}, "score": "58.750473"}
{"text": "13 is a flow diagram illustrating the overall operation of the runtime environment shown in .FIG .12 .FIGS .14A-14C illustrate the operation of a dynamic programming decoder in accordance with one embodiment of the present invention .FIG .", "label": "", "metadata": {}, "score": "58.782997"}
{"text": "Moses uses language model to select the most \" probably \" target language sentence from a large set of \" possible \" translations it generated using the phrase table and reordering table .language model types .SMT .Language model files contain statistical data generated by one of various programs .", "label": "", "metadata": {}, "score": "58.81997"}
{"text": "This is a powerful resource that can give us much more information about a text than simple search engines currently allow .Conclusion .In this a dynamic lexicon fills a gap left by traditional reference works .By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not .", "label": "", "metadata": {}, "score": "58.83304"}
{"text": "This may be used to determine a cost function to a given hypothesis in a scoring model .In general , the cost decreases as the hypothesis moves toward a number of gaps which corresponds to the statistically determined most likely number of gaps .", "label": "", "metadata": {}, "score": "58.90884"}
{"text": "gaps .s .I . t .J .d .K . )i .I .gapCount .b .d .i . ) where gapCount(b ) counts the number of gaps in the source and target fragments of b. .", "label": "", "metadata": {}, "score": "58.955116"}
{"text": "Beyond the lexicon .These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage .This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry .", "label": "", "metadata": {}, "score": "59.02012"}
{"text": "Adding such a feature is thus an alternative to using more complex parametric models for the distribution of gaps in a bi - phrase .Heuristics .In adopting the h ELAST feature 34 discussed above , only a few adaptations to the admissible heuristics described in Cancedda , et al .", "label": "", "metadata": {}, "score": "59.02432"}
{"text": "First , the elementary similarity measure between two basic entities ( letters vs. leaves ) is more complex in the document scenario .As discussed above , a notion of soft similarity is generally needed in the case of document alignment because the leaf content of two matched leaves is not always identical .", "label": "", "metadata": {}, "score": "59.099346"}
{"text": "10A-10D illustrate how an order model is trained in accordance with one embodiment of the present invention .FIGS .11A-11C illustrate the operation of a second embodiment of an order model .FIG .12 is a block diagram illustrating a runtime environment in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "59.119595"}
{"text": "In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years .Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica .", "label": "", "metadata": {}, "score": "59.183258"}
{"text": "Two trees are considered to be equal if they have the same tree structure and lexical choices after sorting each parent 's children into a canonical order .Another way of restricting the search space is to drop candidates early in the decoding process , and this may be done before the relatively large and expensive ordering space is explored .", "label": "", "metadata": {}, "score": "59.27394"}
{"text": "Statistical Model .Many existing machine translation systems are based on probability models , that assign to any target language sequence t 1 J a probability of being a translation of a source language sequence s 1 I .The general approach in statistical machine translation is to find the most probable translation for s 1 I : . t .", "label": "", "metadata": {}, "score": "59.317184"}
{"text": "The ordering that produces the highest combined order and target score is chosen .At this point , the candidate score includes no estimates , but instead the actual scores for all models , including the order and target models .The completed candidate is then placed in the completed queue .", "label": "", "metadata": {}, "score": "59.44233"}
{"text": "Thus , a similarity measure based on minimum edit distance alone may lead to inaccurate results .The calculation of the minimum edit distance is also quite computationally expensive .A more desirable approach is to view a substring match as an acceptable match and provide a weighting factor which takes this into account .", "label": "", "metadata": {}, "score": "59.487377"}
{"text": "We therefore must concentrate on two problems .First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ?And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ?", "label": "", "metadata": {}, "score": "59.51091"}
{"text": "For example , the left alignment xyzabc , the right alignment xzy - acb and the words y and d in .FIG .9 ( iii ) make up a context alignment 915 .Given a comparable corpus , this procedure will yield many context alignments which correspond to incorrect translations , such as that between the words \" canadien \" and \" previous \" : . tout canadien serieux .", "label": "", "metadata": {}, "score": "59.517548"}
{"text": "At step S 118 , the matches identified in each partition 102 are added to those of the other partitions and output as the matches for the two documents 10 , 12 .It will be appreciated that for longer documents , the alignment may not be precisely the optimal alignment .", "label": "", "metadata": {}, "score": "59.564377"}
{"text": "McDonald et al .2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\u010d .\" Non - projective Dependency Parsing using Spanning Tree Algorithms \" , Proceedings of HLT / EMNLP ( 2005 ) .Mel'\u010duk 1988 Mel'\u010duk , Igor A. Dependency Syntax : Theory and Practice .", "label": "", "metadata": {}, "score": "59.646008"}
{"text": "For example , RMBL is used as the similarity measure and the costs determined according to Eqn . 2 , above .The result is shown in .FIG .6 .In this case , the final alignment is clear from the initialization .", "label": "", "metadata": {}, "score": "59.654358"}
{"text": "Therefore , the word aligned with \" empty \" is \" vider \" and is made the root of the target language dependency tree shown in .FIG .8C .Next recall that the dependencies in the source language dependency tree are now projected onto the aligned words in target language input to obtain the target language dependency tree .", "label": "", "metadata": {}, "score": "59.703865"}
{"text": "No .11/170,542 , filed Jun. 29 , 2005 , entitled \" A PROBABILISTIC LEARNING METHOD FOR XML ANNOTATION OF DOCUMENTS , \" by Boris Chidlovskii , et al . .U.S. application Ser .No .11/156,776 , filed Jun. 20 , 2005 , entitled \" A METHOD FOR CLASSIFYING SUB - TREES IN SEMI - STRUCTURED DOCUMENTS , \" by Boris Chidlovskii , et al . .", "label": "", "metadata": {}, "score": "59.770424"}
{"text": "7E ) or the actual depending tree structure such as that shown in .FIG .7F .A \" dependency tree \" will thus refer to both of those structures interchangeably .Therefore , the term \" word aligned parallel dependency trees \" can be used to refer to both types of structures shown in .", "label": "", "metadata": {}, "score": "59.802635"}
{"text": "The general case of the m - n match , which is rare in practice , occurs if the traversal through the matrix takes the shape of a staircase .Postprocessing .The goal of the postprocessing step ( Step S 116 ) is to transform a candidate M - N match into a real match .", "label": "", "metadata": {}, "score": "59.80287"}
{"text": "An alignment of the leaves of the first document with the leaves of the second document is output which includes matches of at least some of the leaves of the first document with at least some of the leaves of the second document .", "label": "", "metadata": {}, "score": "59.854004"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of one exemplary environment in which the present invention can be used .FIG .2 is a block diagram of a training system in accordance with one embodiment of the present invention .", "label": "", "metadata": {}, "score": "60.00798"}
{"text": "On test data extracted by the heuristic strategy , however , performance of the two training sets is similar , with AERs of 13.2 % and 14.7 % respectively .Analysis of 100 pairs of sentences from each set reveals that the edit distance data lacks many of the complex lexical and syntactic alternations that characterize monolingual paraphrase .", "label": "", "metadata": {}, "score": "60.06653"}
{"text": "In this ... \" .This paper is based on the work carried out in the framework of the Verbmobil project , which is a limited - domain speech translation task ( German - English ) .In the nal evaluation , the statistical approach was found to perform best among ve competing approaches .", "label": "", "metadata": {}, "score": "60.092068"}
{"text": "An algorithm is used to compute the minimum edit distance or Levenstein distance , which makes use of the alignment of the strings .The minimum edit distance is the minimum cost which is incurred in alignment of the two strings in terms of basic edit operations ( insert , delete , replace ) which are needed to transform one string into the other .", "label": "", "metadata": {}, "score": "60.136543"}
{"text": "14B and make a binary decision , whether to chose that translation pair or not .Each such decision represents a branch in the search space .The estimated part of the score for each candidate is calculated by applying the applicable models to all of the treelet translation pairs that might possibly apply to the uncovered portion of the source language dependency tree .", "label": "", "metadata": {}, "score": "60.1739"}
{"text": "First , parser component 214 identifies words in the input text fragment ( that is , segments the input sentence into words ) .In doing this , the parser may optionally treat multiword inputs as a single word ( such as White House ) , and may also split up terms .", "label": "", "metadata": {}, "score": "60.2038"}
{"text": "12 is illustrated by a flow diagram illustrated in .FIG .13 .System 600 first receives an input sentence ( or other text fragment ) 610 .This is indicated by block 612 in .FIG .13 .Dependency parser component 606 parses this source - input sentence 610 into a source language dependency tree .", "label": "", "metadata": {}, "score": "60.209133"}
{"text": "Additionally , even though the pairs are considered to be a match , they do not always match exactly ( e.g. , the removal of quotes in the target document ) .A measure of \" soft similarity \" is thus incorporated into the matching process .", "label": "", "metadata": {}, "score": "60.22975"}
{"text": "First , the lexicon builder 125 searches for the highest score for any word pair .This is added to the lexicon ( block 230 ) , and word pairs that include either the German and English word are dropped from further search .", "label": "", "metadata": {}, "score": "60.23115"}
{"text": "s .I . t .J .d .K . ) k .K . log .Pr . target .d . k . ) source .d . k . )( 2 ) a gap size scoring feature ( h ELAST ) , of the general form : . h .", "label": "", "metadata": {}, "score": "60.318504"}
{"text": "Vector comparison is done by adding all absolute differences of all components .Alternatively , the lexicon builder 125 may count how often another word occurs in the same sentence as the target word .The counts may then be normalized by a using the tf / idf method , which is often used in information retrieval .", "label": "", "metadata": {}, "score": "60.36082"}
{"text": "This is also shown in .FIG .6D .Component 220 then computes the order in the target dependency tree .In this case , the order of the dependency tree shown in .FIG .6D would lead to the French string \" Je parle ne pas Francais \" .", "label": "", "metadata": {}, "score": "60.54513"}
{"text": "For the Hansard corpus , we took the human annotation of word alignment ... . \" ...We present an unsupervised approach to symmetric word alignment in which two simple asymmetric models are trained jointly to maximize a combination of data likelihood and agreement between the models .", "label": "", "metadata": {}, "score": "60.559387"}
{"text": "Nat .6.90 , etc .For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL .This is what we might consider a manual form of \" lemmatized searching .\" The search results are thus significantly diluted by a large number of false positives .", "label": "", "metadata": {}, "score": "60.567055"}
{"text": "The method of .The method of .claim 1 , wherein the generating of a target hypothesis includes performing a multiple stack beam - search .The method of .claim 1 , wherein the generating of at least one target hypothesis includes generation of a plurality of partial translation hypotheses , scoring the partial hypotheses , and optionally pruning partial translation hypotheses according to their scores .", "label": "", "metadata": {}, "score": "60.62519"}
{"text": "For each position , the tokens are compared in frequency and the frequency count is replaced by the frequency rank , e.g. , the most frequent token count is replaced with 1 and the least frequent by n. R . a .", "label": "", "metadata": {}, "score": "60.637802"}
{"text": "Identifying this point is indicated by block 302 in .FIG .6 .Component 320 then reattaches the word \" ne \" to that point .This is shown in .FIG .6E and is indicated by block 304 in .", "label": "", "metadata": {}, "score": "60.679596"}
{"text": "Matrix Match .The matrix match ( Step S 112 , .FIGS .6 - 7 ) is calculated in a similar manner to that described for string alignment ( .FIG .4 ) , by incorporating a factor which takes into account the similarity measure between the leaves .", "label": "", "metadata": {}, "score": "60.76259"}
{"text": "A standard formalism for encoding this meta - information and data exchange is extendable mark - up language ( XML ) .The conversion process has two main steps .The first main step involves design of a rich and highly structured document model .", "label": "", "metadata": {}, "score": "60.764084"}
{"text": "2 .Thus , one of the representations from .FIG .4A or 4 B is associated with the representation shown in .FIG .4C .Next , unsupervised word alignment component 218 finds correspondences between words in the parallel aligned word segmented corpus 256 .", "label": "", "metadata": {}, "score": "60.79664"}
{"text": "Because the two documents to be aligned are basically the same , it is unlikely that a leaf at the beginning of one document matches another leaf at the end of the other document ( in fact , such a match would most likely be a mismatch .", "label": "", "metadata": {}, "score": "60.900436"}
{"text": "Since the XML files of both the source text and its translations are marked up with the same reference points , \" chapter 1 , section 1 \" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) .", "label": "", "metadata": {}, "score": "60.9168"}
{"text": "For example , if the sequences of leaves are arranged in matrix form 50 ( .FIG .5 ) as it is done for the basic algorithm ( .FIG .4 ) , then the correct match has a high probability of being found along the diagonal from the top - left to the bottom - right .", "label": "", "metadata": {}, "score": "60.934685"}
{"text": "s .t . )i . k . log .Poisson .[ . i . ] gs .i . ) j .l . log .Poisson .[ .j . ] gt .j . )This feature estimates the log of the probability of observing certain values of the gap sizes as the sum of the logs of the individual Poisson distributions associated with each gap .", "label": "", "metadata": {}, "score": "60.985092"}
{"text": "14B in order to cover the source language dependency tree 614 and to generate a target language dependency tree .In one illustrative embodiment , decoder 608 is a dynamic programming decoder .In that embodiment , decoder 608 performs an exhaustive decoding search .", "label": "", "metadata": {}, "score": "61.11188"}
{"text": "6,377,945 , the disclosure of which is incorporated herein by reference .In the case of strings , the alignment process is relatively simple because a letter is either present in one string or it is not present .The method may involve calculation of four matches : .", "label": "", "metadata": {}, "score": "61.1221"}
{"text": "10C is more likely .In one illustrative embodiment , order model 206 simply enumerates all possibilities , scores each , and chooses the one with the highest score .To accomplish this , order model 206 predicts the order of children , given their parent , in the dependency tree .", "label": "", "metadata": {}, "score": "61.14158"}
{"text": "11B .The order model 206 is used to predict the order of the modifiers of \" homme \" since there are a plurality of children which immediately depend from that node , and all of which reside on the same level in the target language dependency tree .", "label": "", "metadata": {}, "score": "61.15013"}
{"text": "Component 606 generates a source language dependency tree 614 such as that shown in .FIG .14A .Performing the dependency parse on the input text is illustrated by block 616 in .FIG .13 .Matching treelet translation pairs component 604 receives source language dependency tree 614 and accesses treelet translation pair database 204 .", "label": "", "metadata": {}, "score": "61.16014"}
{"text": "The document alignment method of .claim 7 , wherein each cost is also a function of a length of a shorter of the two leaves .The method of .claim 1 , wherein when two elements have no greater than a threshold number of characters in common , a maximum cost value is assigned .", "label": "", "metadata": {}, "score": "61.228302"}
{"text": "FIG .5 illustrates the initial match process in matrix form with the leaves of the target document arranged in sequence from left to right in the x direction and the leaves of the source document arranged in sequence from top to bottom in the y direction .", "label": "", "metadata": {}, "score": "61.315662"}
{"text": "Consider an input sentence \" Click the selected button .\" Assume we have translations for \" click button \" and \" selected button \" that agree on the translation of \" button .\" It would likely be detrimental to force a choice between these two translations , instead of allowing their translational preferences to mutually reinforce one another .", "label": "", "metadata": {}, "score": "61.419453"}
{"text": "4 , the minimum edit distance 46 is found ( in this case , 3 ) .To obtain an optimal alignment , the matrix is traversed backwards .Specifically , the matrix is traversed by starting at the bottom right corner and moving in one of three directions : left , up , or diagonally left - up .", "label": "", "metadata": {}, "score": "61.427147"}
{"text": "claim 1 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .The method of .claim 3 , wherein said identifying substantially identical words comprises .", "label": "", "metadata": {}, "score": "61.466698"}
{"text": "A.Kempe , J.M.Champarnaud , J.Eisner , A Note on Join and Auto - Intersection of n - ary Rational Relations , In B. Watson and L. Cleophas , editors , Proceedings Eindhoven FASTAR Days , No . 04- 40 in TU / e CS TR , pp .", "label": "", "metadata": {}, "score": "61.47914"}
{"text": "No .11/137,566 , incorporated herein by reference , adapts string alignment methods to the alignment of words in translation of natural languages .Document Alignment .In the case of document alignment , rather than aligning letters , the alignment seeks to align leaves , with all of the characters ( and spaces between characters ) of a leaf being treated as a unit to be aligned .", "label": "", "metadata": {}, "score": "61.561977"}
{"text": "In the illustrated embodiment ( .FIG .5 ) , seven initial matches 104 , 106 , etc are obtained , allowing the document to be divided into eight sub - problems 102 .The reduction in the complexity is the difference between the area of the rectangle 108 bounding the blocks 16 , 18 , 20 and the total area of each of the partitions 102 .", "label": "", "metadata": {}, "score": "61.59877"}
{"text": "However , there is a much broader peak 802 which has a BLEU score that is not quite as high as that at peak 800 , but yet would tend to indicate that a whole range of sentences performed much better at that value of \u03bb .", "label": "", "metadata": {}, "score": "61.651665"}
{"text": "The availability of monolingual corpora has been enhanced greatly due to the digital revolution and widespread use of the World Wide Web .Methods for processing such resources can therefore greatly benefit the field .SUMMARY .In an embodiment , a system may be able to build a translation lexicon from comparable , non - parallel corpora .", "label": "", "metadata": {}, "score": "61.656822"}
{"text": "Building the suffix tree of a string takes time and space linear in the length of the string .Building a GST for a set of strings takes time and space linear in the sum of the lengths of all strings in the set .", "label": "", "metadata": {}, "score": "61.728683"}
{"text": "BRIEF DESCRIPTION .Aspects of the exemplary embodiment relate to a method and an apparatus for document alignment .In one aspect , a document alignment method includes inputting source leaves of a source document in first tree structured format and inputting target leaves of a target document in second tree structured format .", "label": "", "metadata": {}, "score": "61.82325"}
{"text": "One or more of the following methods may be used to determine a measure of the similarity between leaves : .FIG .4 ) , the minimal possible value being zero .e.g. , in the example of .FIG .", "label": "", "metadata": {}, "score": "61.856384"}
{"text": "Description .CROSS REFERENCE TO RELATED PATENTS AND APPLICATIONS .The following copending applications , the disclosures of which are incorporated herein in their entireties by reference , are mentioned : .U.S. application Ser .No .11/222,881 , filed Sep. 9 , 2005 , entitled \" METHOD FOR DOCUMENT CLUSTERING BASED ON PAGE LAYOUT ATTRIBUTES , \" by Andr\u00e9 Bergholz .", "label": "", "metadata": {}, "score": "61.953606"}
{"text": "When human interaction is enabled , an operator may review the alignment or portions thereof and accept or reject one or more matches or select one from a plurality of candidate alignments .An advantage of the exemplary embodiment is that it reduces processing time for generating aligned documents and provides well - aligned documents in computer readable form ready for structured learning or for other uses .", "label": "", "metadata": {}, "score": "61.95672"}
{"text": "In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) .A predicate 's selectional preference specifies the type of argument it generally appears with .The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically .", "label": "", "metadata": {}, "score": "62.057426"}
{"text": "( b )For both sides the longest possible ascending sequence is calculated .The longest ascending sequence is not identified by the length of the offset sequence , but by the lengths of the contents of the corresponding leaves .The target side is treated analogously .", "label": "", "metadata": {}, "score": "62.151318"}
{"text": "Each of the k gaps allows one word from another phrase to be inserted between two of the words of the elastic phrase .Similarly each target phrase includes l gaps and l+1 components .Both k and l can independently assume a positive natural number , including 0 , such as 0 , 1 , 2 , 3 , 4 , 5 etc .", "label": "", "metadata": {}, "score": "62.271355"}
{"text": "Translated strings of text and the entire text , once translated , may be stored in volatile or non - volatile memory 30 .Alternatively , the processing component 16 accesses a remote database which stores the bi - fragment library , for example , via the internet or via a network link .", "label": "", "metadata": {}, "score": "62.30361"}
{"text": "Structural aspects are treated in the learning phase of the document transformation process , which are described elsewhere .Hence , tree alignment methods need not be considered in detail here .The exemplary alignment method ( Step S 112 ) involves sequence alignment .", "label": "", "metadata": {}, "score": "62.311043"}
{"text": "M . m .h .m .s .I . t .J .d .K . ) 1 I is a normalization constant .The h m are thus real - valued features representing different aspects of a translation hypothesis , and the \u03bb m are weights associated with these features .", "label": "", "metadata": {}, "score": "62.323578"}
{"text": "6 is a Generalized Suffix Tree ( GST ) .FIG .7 is a Bilingual Suffix Tree ( BST ) .FIG .8 is a portion of a BST showing example alignments .FIG .9 are portions of a BST describing left and right alignments .", "label": "", "metadata": {}, "score": "62.44867"}
{"text": "Similarly , if the treelet is compatible but adds no new information ( i.e. , it does not cover any new input nodes ) to the already chosen treelets , it is also skipped .The new score may be subjected to one or more threshold tests and the candidate is discarded if it fails any of them .", "label": "", "metadata": {}, "score": "62.458313"}
{"text": "2 : .A \" bi - fragment \" feature h bf , illustrated at 32 .It represents the probability of producing t 1 J using some set of bi - fragments , under the assumption that each source fragment produces a target fragment independently of the others : . h . bf .", "label": "", "metadata": {}, "score": "62.601883"}
{"text": "FIG .5 shows the suffix tree 500 of string xyzyxzy .Note that if a suffix of a string is also a prefix of another suffix ( as would be the case for suffix zy of string xyzyxzy ) , a proper suffix tree can not be built for the string .", "label": "", "metadata": {}, "score": "62.664047"}
{"text": "claim 1 , wherein bi - fragments comprising contiguous text fragments are modeled as elastic bi - phrases .The method of .claim 1 , wherein the generation of a target hypothesis includes generating a partial hypothesis comprising a sequence of words without gaps from the text fragments selected from the second language and storing any unconsumed words of the text fragments used in generating the sequence in a buffer .", "label": "", "metadata": {}, "score": "62.70172"}
{"text": "Marcu , Daniel , \" Building Up Rhetorical Structure Trees , \" 1996 , Proc . of the National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference , vol .2 , pp .1069 - 1074 .Och , F. , \" Minimum Error Rate Training in Statistical Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "62.78438"}
{"text": "Generally , it performs like a sequential optimization .For instance , assume that there are only two models in the decoder , each having a weight \u03bb 1 and \u03bb 2 .The above mentioned reference operates by holding one of the weights steady and optimizing along all values of the other weight , and then picking an optimum weight .", "label": "", "metadata": {}, "score": "62.789406"}
{"text": "Two strings ( i.e. , sequences of words ) match if the corresponding words are translations of each other according to a bilingual lexicon .In order to perform the matching operation , all paths that correspond to an exhaustive traversal of one of the trees ( the source tree ) are traversed in the other ( the target tree ) , until a mismatch occurs .", "label": "", "metadata": {}, "score": "62.81115"}
{"text": "The system of . claim 13 and further comprising : . a training data corpus having a plurality of pairs of parallel text fragments , each pair of parallel text fragments comprising a source language text fragment and a corresponding target language text fragment .", "label": "", "metadata": {}, "score": "62.835106"}
{"text": "A well - known technique referred to as maximum BLEU training can be used as a specific way to train the weights for log - linear model combination .At a relatively high level , the maximum BLEU algorithm operates much like any multi - dimensional function optimization approach .", "label": "", "metadata": {}, "score": "62.994522"}
{"text": "FIG .4 ) .While calculation of minimum edit distance between their contents 15 is one approach , it suffers from a drawback .The content 15 of one leaf is frequently a substring of the content of the other leaf ( e.g. , an m-1 or 1-n match ) .", "label": "", "metadata": {}, "score": "62.997196"}
{"text": "The initial match ( Step S 108 ) is an optional step which is used to reduce the overall run - time of the overall algorithm ( step S 112 ) .The initial match uses one or more of the similarity measures discussed above to identify pairs of leaves which have high similarity values and thus which have a high probability of being an actual 1 - 1 match .", "label": "", "metadata": {}, "score": "63.17525"}
{"text": "2 , order model training component 226 also trains an order model 206 based on the word - aligned parallel dependency tree corpus .This is indicated by block 480 shown in .FIG .3 .The problem addressed by order model 206 is that , at runtime ( which is discussed later in the specification ) , a source language input is received and parsed into a source language dependency structure .", "label": "", "metadata": {}, "score": "63.17594"}
{"text": "4A .The lower portion of the structure shown in .FIG .5B includes the target language input \" Je ne parle pas Francais \" with its words aligned to words in the source language input .Thus , .FIG .", "label": "", "metadata": {}, "score": "63.203823"}
{"text": "Therefore , \" ne \" is made a dependent of the word \" pas \" .Since it precedes the word \" pas \" , it is placed in the dependency tree as a pre - modifier of the word \" pas \" .", "label": "", "metadata": {}, "score": "63.342285"}
{"text": "To accommodate for this error , and to correct a number of other potential problems as discussed below , component 220 performs a second pass , if necessary , through the target language dependency tree to clean up target language dependency tree errors .", "label": "", "metadata": {}, "score": "63.43211"}
{"text": "each decision results in at least a new source word being covered , and therefore the new state is placed on a stack strictly \" to the right \" of the current stack .In consequence , once the current stack has been traversed , it is never seen again and the decoder can move to the next stack .", "label": "", "metadata": {}, "score": "63.477295"}
{"text": "Head - Driven Statistical Models for Natural Language Parsing \" , Ph.D. thesis .Philadelphia : University of Pennsylvania , 1999 .Freund 1840 Freund , Wilhelm ( ed . )W\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde .", "label": "", "metadata": {}, "score": "63.53463"}
{"text": "How do we get there ?We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] .Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and .", "label": "", "metadata": {}, "score": "63.550293"}
{"text": "When calculating a new element , three neighboring elements are considered : the one directly to the left , the one on top , and the one on the diagonal to the upper - left .The new matrix element m(i , j ) is calculated as follows : . m .", "label": "", "metadata": {}, "score": "63.634605"}
{"text": "8D or 8 E. For the present discussion , the term \" dependency tree \" will be used to refer to either the type of structure which shows dependencies by arcs along a word string ( such as that shown in .", "label": "", "metadata": {}, "score": "63.71528"}
{"text": "The method of .claim 1 , further comprising , .annotating leaves of at least one of the source document and the target document with an attribute , the attribute identifying a matching leaf from the other of the source document and the target document .", "label": "", "metadata": {}, "score": "63.760384"}
{"text": "System 600 includes matching treelet translation pair component 604 , source language dependency parser 606 ( which can be the same as source language dependency parser component 214 shown in .FIG .2 ) and decoder 608 .The overall operation of system 600 shown in .", "label": "", "metadata": {}, "score": "63.895947"}
{"text": "Aligning the English phrase to be paraphrased was the German - English section of the Europarl corpus , version 2 ( Koehn , 2002 ) .Because we wanted to test our method independently of the quality of word alignment algorithms , we also developed a gold standard of word alignments for the set of phrases that we wanted to paraphr ... .", "label": "", "metadata": {}, "score": "63.951767"}
{"text": "It has been found that the number 12 makes a relatively good seed value as it results in a progression that falls off quickly , but still differentiates between interesting cases .In one embodiment , it has been found that use of variable - size N - best lists has made it possible for the exhaustive decoder to outperform the greedy decoder .", "label": "", "metadata": {}, "score": "63.96627"}
{"text": "The method of . claim 21 , wherein a maximum cost value is assigned to unmatched pairs in which a target leaf has no corresponding source leaf or a source leaf has no corresponding target leaf .The method of .claim 1 , wherein each assigned cost is a function of a similarity measure for the elements in the pair and wherein each cost is also a function of a length of a shorter of the two leaves .", "label": "", "metadata": {}, "score": "63.974167"}
{"text": "FIG .10C .Order model 206 predicts the order of dependency of each level of the target language dependency tree , independently of other levels .In the present example , order model 206 will predict which dependency tree shown in .", "label": "", "metadata": {}, "score": "63.994045"}
{"text": "FIG .3 ) .In .The solid lines indicate the partitions 102 ( analogous to partitions 34 , 36 , 38 in .FIG .3 ) .Each partition 102 is bounded by nodes 104 , 106 , etc . comprising first and second pairs of leaves which passed the initial match ( step S 108 ) .", "label": "", "metadata": {}, "score": "64.04025"}
{"text": "Rada Mihalcea et al . , An evaluation exercise for word alignment .In Rada Mihalcea and Ted Pederson , editors , HLT - NAACL 2003 Workshop : Building and Using Parallel Texts : Data Driven Machine Translation and Beyond , pp . 1 - 10 , Edmonton , Alberta , Canada , May 31 , 2003 .", "label": "", "metadata": {}, "score": "64.11601"}
{"text": "BRIEF DESCRIPTION .Aspects of the exemplary embodiment relate to a machine translation method and system for machine translation .In one aspect , a machine translation method is provided for translating source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "64.191086"}
{"text": "The highest score is picked as the optimistic estimate for the agreement model portion of the estimated score for that candidate .This is repeated for that candidate , for each applicable model , to derive the entire estimated score for that candidate .", "label": "", "metadata": {}, "score": "64.26794"}
{"text": "Matches for which a total cost is minimal are identified , wherein each of the leaves is in at least one of the identified matches .From the identified matches , groups of matches wherein each match in the group has a leaf in common are identified .", "label": "", "metadata": {}, "score": "64.27434"}
{"text": "15 .This takes into account not only the height of the peaks of the step - wise function but also their width .Thus , when looking at the moving average of the BLEU scores , the value of \u03bb will be set much more closely to the value associated with the broad peak 802 of the step function , yielding a more stable system .", "label": "", "metadata": {}, "score": "64.497284"}
{"text": "Similarly , the arrows from the two words \" the \" and \" old \" to the word \" man \" indicate that those two words depend from the word \" man \" .Another representation of the words and dependencies output from dependency parser component 214 is shown in .", "label": "", "metadata": {}, "score": "64.58568"}
{"text": "Miller et al .1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \"A Semantic Concordance \" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) .Moore 2002 Moore , Robert C. \" Fast and Accurate Sentence Alignment of Bilingual Corpora \" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) .", "label": "", "metadata": {}, "score": "64.645325"}
{"text": "FIG .7D , that is above the unaligned word \" de \" is the arc originating at \" livre \" and terminating at the word \" ai \" .Identifying this arc is indicated by block 322 in .FIG .", "label": "", "metadata": {}, "score": "64.65608"}
{"text": "In another embodiment , two h ELAST features 34 A , 34 B may be utilized , one for the target side , and the other for the source side .Considering only a target side h ELAST feature may not be beneficial as it would not penalize \" spurious \" versions of source side phrases with unlikely gap sizes .", "label": "", "metadata": {}, "score": "64.74156"}
{"text": "The purpose of this step is to retain only those pairs for which there is a high level of confidence that an actual match exists .The initial match then is used to decompose the matrix problem into partitions 34 , 36 , 38 ( Step S 110 ) .", "label": "", "metadata": {}, "score": "64.78053"}
{"text": "In one method , a number k is selected and the leaves of the two documents divided into k blocks , in sequence , each block having the same number of leaves 11 , 13 of each document ( .FIG .", "label": "", "metadata": {}, "score": "64.8176"}
{"text": "Every leaf is annotated with an attribute 140 indicating the class of the leaf .The class is derived from the corresponding target leaf .Unmatched source leaves get a specific class : None .Leaves are only removed for the purpose of rearrangement .", "label": "", "metadata": {}, "score": "64.847626"}
{"text": "Again , at block 702 , decoder 608 locates , in the list shown in .FIG .14B treelet translation pair 710 which is rooted at the node \" installed . \"At block 704 , decoder 608 determines that the treelet translation pair 710 does not cover the part of the source subtree from \" computer \" and below .", "label": "", "metadata": {}, "score": "64.85538"}
{"text": "pas \" which reside on either side of the verb \" parle \" .Alignment component 218 thus aligns the two sentences as shown in .FIG .5A .Note that there is no alignment to the source language word \" do \" and there are two words aligned with the source language word \" not \" , those being \" ne \" and \" pas \" .", "label": "", "metadata": {}, "score": "64.893326"}
{"text": "The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network .In a distributed computing environment , program modules may be located in both local and remote computer storage media including memory storage devices .", "label": "", "metadata": {}, "score": "64.89689"}
{"text": "Therefore , component 220 must decide which dependency to project onto the target word .In accordance with one embodiment of the present invention , component 220 uses the dependency of the source word that is highest in the source language dependency tree structure .", "label": "", "metadata": {}, "score": "65.0502"}
{"text": "Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . )Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) .Miller 1995 Miller , George .", "label": "", "metadata": {}, "score": "65.139435"}
{"text": "BACKGROUND .Machine translation ( MT ) concerns the automatic translation of natural language sentences from a first language ( e.g. , French ) into another language ( e.g. , English ) .Systems that perform MT techniques are said to \" decode \" the source language into the target language .", "label": "", "metadata": {}, "score": "65.2516"}
{"text": "However , the source language treelet which shows \" recycle \" as a pre - modifier of \" bin \" is translated as \" corbeille \" and neither of the source language terms in that source language treelet have alignments which are outside of the treelet translation pair .", "label": "", "metadata": {}, "score": "65.29675"}
{"text": "The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval .By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies .", "label": "", "metadata": {}, "score": "65.31825"}
{"text": "The conversion process not only transforms legacy documents from an existing format into a new one , such as , for example , from Microsoft Word \u2122 into extended mark - up language , but also customizes information which is not explicitly encoded in the legacy documents .", "label": "", "metadata": {}, "score": "65.431076"}
{"text": "l .i .j . )l .i .j .To illustrate this , consider the example of .FIG .1 and treat it as a whole ( without any decomposition ) .Both documents consist of six leaves .", "label": "", "metadata": {}, "score": "65.49123"}
{"text": "claim 12 , further comprising adding the matching scores .The method of . claim 13 , further comprising weighting the matching scores .A non - transitory computer readable medium having embodied thereon a program , the program being executable by a processor for performing a method for building a translation lexicon from non - parallel corpora , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}, "score": "65.575195"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "65.66591"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "65.66591"}
{"text": "For instance , in one embodiment , the model predicts the order at each level by predicting the order for each word in turn .A probability can be predicted for each word having a given offset from its head .The model can use those factors listed in Equation 1 or different factors .", "label": "", "metadata": {}, "score": "65.66902"}
{"text": "Gale et al .1992a Gale , William , Kenneth W. Church and David Yarowsky . \"Using bilingual materials to develop word sense disambiguation methods \" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) .", "label": "", "metadata": {}, "score": "65.78652"}
{"text": "This is indicated by block 484 in .FIG .3 and is shown as target language model 212 in .FIG .2 .Training component 222 uses conventional language model training techniques to train an n - gram language model in the target language .", "label": "", "metadata": {}, "score": "65.82225"}
{"text": "The system 400 may use a suffix tree data structure in order to identify the alignments .The suffix tree of a string uniquely encodes all the suffixes of that string ( and thus , implicitly , all its substrings too ) .", "label": "", "metadata": {}, "score": "65.834076"}
{"text": "8A illustrates that two source language words \" recycle \" and \" bin \" are aligned with the target language word \" corbeille \" .The dependency tree associated with the source language string is shown in .FIG . 8B .", "label": "", "metadata": {}, "score": "65.83633"}
{"text": "No .11/032,814 , filed Jan. 10 , 2005 , entitled \" VERSATILE DETECTION OF A TABLE OF CONTENTS AND REFERENCE DETERMINATION , \" by Herv\u00e9 D\u00e9jean , et al . .BACKGROUND .The present invention is directed to the field of electronic document format conversion .", "label": "", "metadata": {}, "score": "65.84397"}
{"text": "Corpus 202 is illustratively a large parallel data corpus .Dependency parser component 214 accesses corpus 202 and performs a dependency parse on each of the source language sentences .This is indicated by block 252 in .FIG .3 .", "label": "", "metadata": {}, "score": "65.855606"}
{"text": "Refined matching block length ( RMBL ) : The MBL is refined by imposing certain constraints whose violation leads to a similarity of 0 .Optionally , the result obtained by any of the measures above may be normalized .For example , in the case of WORD , the similarity may be normalized by multiplying the result by the length of the shorter content and dividing by the number of characters that are part of a word ( 52/44 in this case ) .", "label": "", "metadata": {}, "score": "65.86908"}
{"text": "The apparatus of 26 , the apparatus comprising : . an alignment module operative to be executed to align text segments in two nonparallel corpora , the corpora including a source language corpus and a target language corpus .The apparatus of .", "label": "", "metadata": {}, "score": "65.89902"}
{"text": "FIG .6D .Finally , component 220 arrives at the word \" French \" and determines that it is aligned to a single target word \" Francais \" and that the word \" French \" depends from the word \" speak \" .", "label": "", "metadata": {}, "score": "65.91649"}
{"text": "A BLEU score indicates how closely the token sequences in one set of data , such as machine translation output , correlate with ( match ) the token sequences in another set of data , such as a reference human translation .", "label": "", "metadata": {}, "score": "66.07089"}
{"text": "Mediae Latinitatis Lexicon Minus .Leiden : Brill , 1976 .Nivre et al .2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \"MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) .", "label": "", "metadata": {}, "score": "66.07453"}
{"text": "Although the total content 15 of the leaves 13 of the target document is substantially the same , it is distributed differently among the leaves .In the exemplary embodiment , both documents contain six leaves 11 , 13 , although it will be appreciated that it is not necessary for each document to have the same number of leaves .", "label": "", "metadata": {}, "score": "66.12813"}
{"text": "claim 23 wherein re - arranging comprises : . identifying a node in the target language dependency tree that is out of order ; and . re - attaching the identified node at a lowest level in the target language dependency tree that yields a target language string with lexical items in the same order that they appear in the target language text fragment .", "label": "", "metadata": {}, "score": "66.145584"}
{"text": "The exemplary embodiment includes a method to find one such correspondence .With reference to .FIG .1 , to illustrate the exemplary method , as an example , a source document 20 comprising leaves 11 may be a layout - oriented source document describing a book , a portion of which may be represented as follows : .", "label": "", "metadata": {}, "score": "66.25356"}
{"text": "Specifically , the current stack is traversed from top to bottom , then the decoder moves to the next stack .Extending Hypotheses in the Beam Search Procedure .FIG .4 illustrates this with the example sentence Je ne veux plus danser le tango .", "label": "", "metadata": {}, "score": "66.29882"}
{"text": "Given an initial bilingual lexicon 405 and two texts 410 , 415 in each of the languages , the system 400 may identify parts of the texts which can be aligned ( i.e. , are mutual translations of each other according to the lexicon ) .", "label": "", "metadata": {}, "score": "66.35404"}
{"text": "Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . of the 38th Annual Meeting of the Association for Computational Lingustics , ' Online !Oct. 2 - 6 , 2000 , pp .440 - 447 , XP002279144", "label": "", "metadata": {}, "score": "66.423904"}
{"text": "FIG .7C .This can also be represented as shown in .FIG .7D , as discussed in previous examples .Of course , this indicates that the word \" de \" does not have a dependency associated with it .", "label": "", "metadata": {}, "score": "66.59082"}
{"text": "Thus , different gap sizes can result in different hypotheses , each one being separately analyzed .For example , the bi - phrase : .As will be appreciated , where there are gaps on both the source and target sides , a substantial number of potential hypotheses may be generated corresponding to different numbers of gaps on both the source and target sides .", "label": "", "metadata": {}, "score": "66.61275"}
{"text": "The leaves in the longest possible ascending sequences form the match .All other leaves are eliminated .In the example that would be s 1 .Thus the combinations ( [ s 2 , s 3 ] , [ t 1 ] ) and ( [ s 1 ] , [ ] ) are added to the final match .", "label": "", "metadata": {}, "score": "66.74914"}
{"text": "12 is a block diagram illustrating a translation system 600 in accordance with one embodiment of the present invention .Translation system 600 has access to the statistical models 206 , 208 , 210 , and 212 along with a set of model weights 602 .", "label": "", "metadata": {}, "score": "66.78049"}
{"text": "expanding the seed lexicon by the machine translation system by identifying possible translations of words in the first and second corpora using one or more clues .The method of .claim 1 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}, "score": "66.81866"}
{"text": "In one embodiment , the cost is a function of the similarity measure sim(s i , t i ) and the value l ij For example , cost may be computed as : . cost .s .i . t .", "label": "", "metadata": {}, "score": "67.00306"}
{"text": "The machine translation system of .claim 18 , wherein the second feature function counts gaps in the target text of the hypothesis and corresponding source text .The machine translation system of . claim 17 , wherein the library associates each of a plurality of elastic bi - fragments with a corresponding parameter representative of a probability distribution of gaps in at least one of the source text fragment and the target text fragment .", "label": "", "metadata": {}, "score": "67.01015"}
{"text": "Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\u010duk 1988 ] .", "label": "", "metadata": {}, "score": "67.15097"}
{"text": "U.S. Published Application No . 2004/0024581 by Koehn , et al . , entitled \" STATISTICAL MACHINE TRANSLATION , \" discloses a phrase - based statistical machine translation method using syntactic markers for contiguous phrases .U.S. Published Application No .2004/0030551 by Marcu , et al . , entitled \" PHRASE TO PHRASE JOINT PROBABILITY MODEL FOR STATISTICAL MACHINE TRANSLATION \" discloses a phrase to phrase joint probability model for statistical machine translation .", "label": "", "metadata": {}, "score": "67.27115"}
{"text": "For instance , in the French language , negation is typically split up into two words .One of the words resides before the verb in the sentence and the other word resides after the verb in the sentence .Therefore , assume that the parallel aligned sentences that are being processed are the English sentence \" I do not speak French \" and the corresponding French translation of that sentence \" Je ne parle pas Francais \" .", "label": "", "metadata": {}, "score": "67.29042"}
{"text": "Therefore , \" ordinateur \" must depend on \" sur \" based on the dependency in the source language dependency tree shown in .FIG .14A .However , decoder 608 simply does not know if \" ordinateur \" is a pre - modifier or a post - modifier of \" sur . \" In one illustrative embodiment , decoder 608 enumerates all possible ways to connect the two sub - trees and scores each possible way with the complete set of models ( or all those that apply ) to determine the most likely way of connecting the two .", "label": "", "metadata": {}, "score": "67.295456"}
{"text": "2 .It is a corpus of structures such as those shown in .FIG .8D or 8 E , or both and those structures are generated for the entire sentence - aligned parallel corpus 202 .Referring back now to .", "label": "", "metadata": {}, "score": "67.313774"}
{"text": "It returns a list of positions j for which the similarity of x with the part of y starting at position j and having length len(x ) is maximal .For example , assume the matrix match had returned the slightly wrong pair ( [ s 1 , s 2 , s 3 ] , [ t 1 ] ) .", "label": "", "metadata": {}, "score": "67.41893"}
{"text": "claim 1 , wherein each assigned cost is a function of a similarity measure for the elements in the pair .The document alignment method of .claim 7 , further comprising : . determining the similarity measure for each of the matches in which a leaf of the target document is matched with a leaf of the source document , including computing at least one of : . a length of the leaf with the shortest content minus the minimum edit distance between the two contents ; . if the content of one leaf is a substring of the other one , the length of the former , otherwise 0 ; . a sum of the lengths of words common to the two leaves ; and .", "label": "", "metadata": {}, "score": "67.506256"}
{"text": "The memory component stores a library of bi - fragments 20 , including non - contiguous bi - fragments .The processing component 16 executes instructions for performing the method of .FIG .3 , which may be stored in an article of manufacture comprising a program storage medium readable by a computer comprising instructions for executing the method , such as memory 18 .", "label": "", "metadata": {}, "score": "67.519066"}
{"text": "8D and 8E interchangeably Other embodiments of the dependency projection tree component may use a different set of rules , heuristics or statistics than those described above .In any case , the plurality of word aligned parallel dependency tree structures generated by component 220 is shown as the word aligned parallel dependency tree corpus 400 in the system diagram of .", "label": "", "metadata": {}, "score": "67.5874"}
{"text": "[ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE .For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) .", "label": "", "metadata": {}, "score": "67.6358"}
{"text": "The statistical models may be trained on parallel corpora .Parallel corpora contain large amounts of text in one language along with their translation in another .Unfortunately , such corpora are available only in limited amounts and cover only in specific genres ( Canadian politics , Hong Kong laws , etc ) .", "label": "", "metadata": {}, "score": "67.70691"}
{"text": "A \" non - contiguous phrase \" refers to two or more words in either the source or target language wherein two of the words are separated by a gap of at least one word .Each non - contiguous phrase includes a first portion comprising at least one word , which is separated from a second portion , comprising at least one word , by a gap of at least one word .", "label": "", "metadata": {}, "score": "67.75117"}
{"text": "The lexicon builder 125 may combine different clues by adding up the matching scores .The scores can be weighted .For example , when using the spelling clue in combination with others , it may be useful to define a cutoff .", "label": "", "metadata": {}, "score": "67.80289"}
{"text": "4 illustrates an exemplary matrix for aligning two strings , the alignment is shown by the connected blocks ; .FIG .5 illustrates an exemplary matrix for alignment of source and target documents according to the exemplary embodiment ; .FIG .", "label": "", "metadata": {}, "score": "67.89545"}
{"text": "FIG .15 , the present invention uses a moving average , of the BLEU scores as indicated by the dashed line in .FIG .15 .By using a moving average , where the averaging window includes a plurality of scores , then the result is the dashed line in .", "label": "", "metadata": {}, "score": "67.91176"}
{"text": "aligning lexical items in the source language dependency structure with lexical items in the target language text fragment .The computer readable medium of .claim 25 wherein projecting comprises : . when an unaligned lexical item in the source language dependency structure is not aligned with a lexical item in the target language text fragment , then projecting no dependency from the unaligned lexical item in the source language dependency structure to the target language text fragment .", "label": "", "metadata": {}, "score": "67.97719"}
{"text": "The method of .claim 1 , wherein , when a plurality of lexical items in the source language dependency structure are aligied with one lexical item in the target language text fragment , then projecting comprises : . identifying one of the plurality of lexical items in the source language dependency structure that is highest in the source language dependency structure ; and . assigning a dependency associated with the identified one of the plurality of items in the source language dependency structure to the one lexical item in the target language text fragment .", "label": "", "metadata": {}, "score": "68.053116"}
{"text": "FIG .14D .The two possibilities are shown in .FIG .14C .Equation 4 indicates one illustrative way of combining the scores of each of the models to obtain an overall score for each hypothesis .The channel model probability will be the probability of \" installe sur \" being translated as \" installed on \" , multiplied by the probability of \" votre ordinateur \" being translated as \" your computer .", "label": "", "metadata": {}, "score": "68.21466"}
{"text": "Consider , for example , the source string \" Empty the recycle bin \" .The French translation of that string is \" Vider le corbeille \" .The word alignment for these two strings is shown in .FIG .8A , along with the dependency structure corresponding to the source language string .", "label": "", "metadata": {}, "score": "68.310684"}
{"text": "The first copy is completed by evaluating it 's attribute with respect to the extended attribute coupling grammar .The first copy is then a partially attributed tree of the second document .The partially attributed tree is completed to form a second tree based on the second extended attribute grammar .", "label": "", "metadata": {}, "score": "68.376625"}
{"text": "The computer readable medium of . claim 21 wherein an unaligned lexical item in the target language text fragment is unaligned to a lexical item in the source language dependency structure , and wherein projecting comprises : . identifying a lexical item located proximate the unaligned lexical item in the target language text fragment ; and . assigning the unaligned lexical item as dependent from the identified lexical item .", "label": "", "metadata": {}, "score": "68.37958"}
{"text": "This is shown in .FIG .6B .Component 220 next encounters the word \" do \" .Since it is not aligned to any words in the target language text , component 220 simply skips the word \" do \" and moves on the word \" not \" .", "label": "", "metadata": {}, "score": "68.48154"}
{"text": "Some of the benefits of electronic documents over paper documents include enhanced document processing capabilities and easier manipulation of documents , such as creation , editing , updating , storage , access , and delivery of documents .A key enabler for such enhancement in known systems is their ability to represent not only the contents of documents but also various meta - information about the contents .", "label": "", "metadata": {}, "score": "68.48971"}
{"text": "FIG .4 illustrates the partial coverage of the source sentence Je ne veux plus danser le tango and the corresponding partial coverage of the target sentence in a series of hypothesis extension steps S 201 , S 202 , S 203 , S 204 , S 206 .", "label": "", "metadata": {}, "score": "68.62048"}
{"text": "The method of .claim 1 , wherein said one or more clues includes similar spelling .The method of .claim 1 , wherein said identifying comprises identifying cognates .The method of .claim 1 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .", "label": "", "metadata": {}, "score": "68.64604"}
{"text": "The method of .claim 1 wherein projecting dependencies comprises : . determining whether the target language dependency structure is properly indicative of the target language text fragment ; and . if not , adjusting the target language dependency structure so it is properly indicative of the target language text fragment .", "label": "", "metadata": {}, "score": "68.71146"}
{"text": "The system of .The system of .The system of . claim 13 wherein the corpus processing system comprises : . a parser configured to parse the source language text fragment to obtain a source language dependency structure .The system of . claim 13 wherein the corpus processing system comprises : . a segmenter configured to segment target language text fragments into lexical items .", "label": "", "metadata": {}, "score": "68.73576"}
{"text": "The updated score is subjected to one or more threshold tests and the candidate is discarded if it fails any of them , and otherwise it is inserted back into the incomplete queue based on its updated score .The process repeats until the incomplete queue is empty , or a specified amount of effort on the search has been expanded .", "label": "", "metadata": {}, "score": "68.7682"}
{"text": "Bayesian classification is most commonly found in spam filtering .By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other .", "label": "", "metadata": {}, "score": "68.802986"}
{"text": "claim 1 , wherein the assigning a cost to each of the plurality of matches is not based on the tree structure of the source and target documents .The method of . claim 5 , wherein the leaves of the source and target documents are subdivided into an equal number of blocks .", "label": "", "metadata": {}, "score": "68.93817"}
{"text": "Busa 2004 Busa , Roberto .\" Foreword : Perspectives on the Digital Humanities \" , Blackwell Companion to Digital Humanities .Oxford : Blackwell , 2004 .Charniak 2000 Charniak , Eugene .\" A Maximum - Entropy - Inspired Parser \" , Proceedings of NAACL ( 2000 ) .", "label": "", "metadata": {}, "score": "69.06138"}
{"text": "j .if .l .i .j .if .l .i .j .This expression assigns a maximum cost ( 1 in the illustrated embodiment ) for all matches where the length of the shorter of the two contents is no greater than a predetermined value , 2 in the illustrated embodiment .", "label": "", "metadata": {}, "score": "69.06769"}
{"text": "Still , these words , often called \" cognates , \" maintain a very similar spelling .This can be defined as differing in very few letters .This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .", "label": "", "metadata": {}, "score": "69.08562"}
{"text": "The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts .Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated .", "label": "", "metadata": {}, "score": "69.200775"}
{"text": "Matches are identified for which a total cost is minimal , wherein each of the leaves is in at least one of the identified matches .From the identified matches , groups of two or more matches are identified which have a leaf in common .", "label": "", "metadata": {}, "score": "69.27644"}
{"text": "i .b .i . )n .n .The result is a matrix with similarity scores between all German words , and a second matrix with similarity scores between all English words .For a new word , the lexicon builder 125 may look up its similarity scores to seed words , thus creating a similarity vector .", "label": "", "metadata": {}, "score": "69.304756"}
{"text": "The book title is spread over two lines in the source document , and thus appears in two text leaves .In the target document , the title is all in one leaf .The source document contains quotes which are absent in the target document .", "label": "", "metadata": {}, "score": "69.392654"}
{"text": "FIG .5B illustrates a structure that shows how both of these things can be shown in a single structure .The upper portion of .FIG .5B shows the source language input sentence \" I do not speak French \" with the dependency structure formed by the arrows as discussed above with respect to .", "label": "", "metadata": {}, "score": "69.49467"}
{"text": "FIG . 8Bshows that the word \" bin \" is highest in the dependency tree structure and therefore its dependence will be projected onto the word \" corbeille \" .Since bin is a post - modifier of the word \" empty \" , the word \" corbeille \" will be a post - modifier of the word \" vider \" in the target language dependency tree .", "label": "", "metadata": {}, "score": "69.58263"}
{"text": "Computational Linguistics and Classical Lexicography .Abstract .Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials .", "label": "", "metadata": {}, "score": "69.851135"}
{"text": "Companies and organizations that own data and documents in electronic form frequently face a problem of migrating legacy documents , often in proprietary formats , into new document formats that allow performance of such operations in a most cost effective and efficient manner .", "label": "", "metadata": {}, "score": "70.07575"}
{"text": "Each of the clues provides a matching score between two words ( block 220 ) , e.g. , a German word and an English word .The likelihood of these two words being actual translations of each other may correlate to these scores .", "label": "", "metadata": {}, "score": "70.12543"}
{"text": "Communication media typically embodies computer readable instructions , data structures , program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media .The term \" modulated data signal \" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal .", "label": "", "metadata": {}, "score": "70.17328"}
{"text": "claim 1 , wherein the target document has substantially the same content as the source document .The method of .claim 1 , wherein the source document is a natural language document and the target document is a natural language document in the same language and wherein the source and target leaves include text in the natural language .", "label": "", "metadata": {}, "score": "70.24308"}
{"text": "2 is a block diagram illustrating a training system 200 .Training system 200 is shown receiving an input which is a large sentence - aligned parallel corpus 202 .Training system 200 is also shown providing an output that includes treelet translation pair database 204 , order model 206 , agreement model 208 , translation probability table 210 and target language model 212 .", "label": "", "metadata": {}, "score": "70.39404"}
{"text": "The following ascending offset lists are obtained : [ 0,44 ] , [ 0,45 ] , [ 1,44 ] , and [ 1,45].They all yield the longest ascending sequence [ s 2 , s 3 ] .For the target side , the longest ascending sequence is trivially [ t 1 ] .", "label": "", "metadata": {}, "score": "70.69296"}
{"text": "By way of example only , .The drives and their associated computer storage media discussed above and illustrated in .FIG .1 , provide storage of computer readable instructions , data structures , program modules and other data for the computer 110 .", "label": "", "metadata": {}, "score": "70.8575"}
{"text": "For German to English , this includes replacing the letters k and z by c and changing the ending -t\u00e4t to -ty .Both these rules can be observed in the word pair Elektrizitat and electricity .The lexicon builder 125 may utilize these rules to expand the seed lexicon .", "label": "", "metadata": {}, "score": "70.90451"}
{"text": "m .j .cost . replace .cost . replace . if .s .s .j . ) m .j .cost .insert .m .j . ) cost . delete .At the bottom right corner of the matrix in .", "label": "", "metadata": {}, "score": "70.94937"}
{"text": "4B shows that \" man \" is a pre - modifier of the word \" ate \" and the two words \" the \" and \" old \" are both pre - modifiers of the word \" man \" .Conversely a line sloping downwards to the right would indicate a word that is a post - modifier of its head .", "label": "", "metadata": {}, "score": "71.1171"}
{"text": "In determining whether to add a feature , consideration should be given to balancing the modeling precision gained by adding new features with the increase in the size of the evaluation set on which the weights of the features are trained .", "label": "", "metadata": {}, "score": "71.29465"}
{"text": "In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text .Word Sense Induction .So , for example , the Greek word arch\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 .", "label": "", "metadata": {}, "score": "71.31204"}
{"text": "Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same .", "label": "", "metadata": {}, "score": "71.36818"}
{"text": "Of course , if a parent ( or head ) has more then one child , then the position integer may be greater than 1 for those children , if a plurality of them reside on one side of the parent .", "label": "", "metadata": {}, "score": "71.45639"}
{"text": "Digital methods also let us deal well with scale .For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond .", "label": "", "metadata": {}, "score": "71.73274"}
{"text": "3 is a flow diagram illustrating the overall operation of system 200 shown in .FIG .2 .First , system 200 accesses the sentence - aligned parallel corpus 202 .This is indicated by block 250 shown in .FIG .", "label": "", "metadata": {}, "score": "71.945206"}
{"text": "Substring length ( SUB ) : If one content ( or a sequence thereof ) is a substring of the other one , the similarity is defined as the length of the former , otherwise 0 .Common words number ( WORD ) : The similarity is the sum of the lengths of the common words between the two contents .", "label": "", "metadata": {}, "score": "71.96785"}
{"text": "The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}, "score": "72.20355"}
{"text": "9 is a block diagram of an exemplary processing system for performing the alignment method of .FIG .2 . DETAILED DESCRIPTION .The exemplary embodiment relates to the conversion of documents from one format to another .Aspects of the exemplary embodiment relate to a method and a system for document alignment .", "label": "", "metadata": {}, "score": "72.203606"}
{"text": "The method of . assigning the dependency between the first and second source lexical items to the first and second target lexical items .The method of .claim 1 wherein a lexical item in the source language dependency structure is aligned with a plurality of lexical items in the target language text fragment , and wherein projecting comprises : . identifying a parent node for the plurality of lexical items in the target language dependency structure ; . identifying a right - most one of the plurality of lexical items in the target language text fragment ; . assigning the right - most lexical item as dependent from the parent node ; and . assigning a remainder of the plurality of lexical items in the target language text fragment as dependent from the right - most lexical item .", "label": "", "metadata": {}, "score": "72.42226"}
{"text": "s 1 is the \" vertical \" string , and t 1 the \" horizontal \" string .A dummy row 42 and a dummy column 44 are inserted at the beginning .They denote the empty string , their purpose is to initialize the matrix .", "label": "", "metadata": {}, "score": "72.45325"}
{"text": "2 illustrates the steps of an exemplary method of aligning two documents 10 , 12 illustrated schematically in .FIG .3 .It will be appreciated that fewer or more steps may be employed and that the order of steps may differ from that illustrated .", "label": "", "metadata": {}, "score": "72.624954"}
{"text": "; \" and .\" It is for this reason that the party has proposed . . . \" .Since \" Ce est pour cette \" can be aligned with \" It is for this \" and \" que le \" with \" that the , \" it is a reasonable assumption that \" raison \" can be translated by \" reason .", "label": "", "metadata": {}, "score": "72.66744"}
{"text": "claim 3 , wherein the generation of a hypothesis includes expanding an existing partial hypothesis by at least one of : . adding a word of a new text fragment to the sequence of words ; and . retrieving an unconsumed word from the buffer to the sequence of words .", "label": "", "metadata": {}, "score": "72.70123"}
{"text": "For instance , assume that the target language sentence that is aligned with \" the old man ate \" shown in .FIGS .4A and 4B is the German language sentence \" der alte mann isst \" .In that example , component 216 splits the target language sentence into the words shown in .", "label": "", "metadata": {}, "score": "72.70848"}
{"text": "Due to cultural exchange , a large number of words that originate in one language may be adopted by others .Recently , this phenomenon can be seen with words such as \" Internet U or \" Aids u. \" These terms may be adopted verbatim or changed by well - established rules .", "label": "", "metadata": {}, "score": "72.94081"}
{"text": "Studies in Honour of Jarmila Panevov\u00e1 .Prague : Charles University Press , 1999 .Kilgarriff et al .2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \"The Sketch Engine \" , Proceedings of EURALEX ( 2004 ) .", "label": "", "metadata": {}, "score": "72.94362"}
{"text": "Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation .This method uses the same techniques we used to create a sense inventory to disambiguate words in context .", "label": "", "metadata": {}, "score": "73.05643"}
{"text": "The non - transitory computer readable medium of .claim 15 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The non - transitory computer readable medium of . claim 15 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}, "score": "73.11566"}
{"text": "Brants and Franz 2006 Brants , Thorsten and Alex Franz .Web 1 T 5-gram Version 1 .Philadelphia : Linguistic Data Consortium , 2006 .Brown et al .1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer .", "label": "", "metadata": {}, "score": "73.134224"}
{"text": "claim 9 , further comprising generating a context vector .The method of .claim 1 , wherein said identifying comprises identifying frequencies of occurrence of words in the first and second first corpora .The method of .claim 1 , further comprising : generating matching scores for each of a plurality of clues .", "label": "", "metadata": {}, "score": "73.13838"}
{"text": "SRILM , RandLM and IRSTLM toolkits include tools that train the new language model files .KenLM , however , only reads ARPA standard language model files which can be created by SRILM , IRSTLM .A linguistic corpus of two or more languages where each element in one language corresponds to an element with the same meaning in the other language(s ) .", "label": "", "metadata": {}, "score": "73.2046"}
{"text": "FIG .1 , an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer 110 .Components of computer 110 may include , but are not limited to , a processing unit 120 , a system memory 130 , and a system bus 121 that couples various system components including the system memory to the processing unit 120 .", "label": "", "metadata": {}, "score": "73.25417"}
{"text": "While this type of system is an improvement over other types of systems where no reordering is allowed , the reordering model used in this type of system is limited in terms of linguistic generalizations .For instance , when translating English to Japanese , the English subject - verb - object clauses generally become Japanese subject - object - verb clauses , and English post - modifying prepositional phrases become Japanese pre - modifying prepositional phrases .", "label": "", "metadata": {}, "score": "73.70405"}
{"text": "Now to better illustrate the fact that siblings of a node are considered to form a connected subgraph , consider the earlier example of the source language sentence \" I do not speak French \" and the corresponding French translation \" Je ne parle pas Francais \" .", "label": "", "metadata": {}, "score": "73.78572"}
{"text": "FIG .15 .This solid line shows that for a particular very narrow range of \u03bb 's , the BLEU score is the highest at a \u03bb value designated by number 800 .However , if the weight were chosen at the corresponding value for the peak 800 , this would be a relatively unstable system , because the peak is so narrow .", "label": "", "metadata": {}, "score": "73.84219"}
{"text": "claim 1 wherein obtaining an aligned structure comprises : . parsing the source language text fragment to obtain a source language dependency structure ; . segmenting the corresponding target language text fragment into lexical items ; and .aligning lexical items in the source language dependency structure with lexical items in the target language text fragment .", "label": "", "metadata": {}, "score": "73.91936"}
{"text": "163 , No . 4 , pp .845 - 8 ( 1965 ) ; also Cybernetics and Control Theory , Vol . 10 , No . 8 , pp .707 - 10 , ( 1966 ) , as discussed , for example , in U.S. Pat .", "label": "", "metadata": {}, "score": "73.92354"}
{"text": "R\u00e9sum\u00e9 .A method for aligning documents which may be in different XML formats includes inputting source and target leaves of a source and documents in first and second tree structured formats and assigning a cost to each of a plurality of matches .", "label": "", "metadata": {}, "score": "74.44334"}
{"text": "RAM 132 typically contains data and/or program modules that are immediately accessible to and/or presently being operated on by processing unit 120 .By way of example , and not limitation , .FIG .1 illustrates operating system 134 , application programs 135 , other program modules 136 , and program data 137 .", "label": "", "metadata": {}, "score": "74.50255"}
{"text": "Treelet 428 , however , again only has the word \" bin \" without the word \" recycle \" .Since \" corbeille \" is assigned to both \" bin \" and \" recycle \" , it has an alignment which would extend outside any treelet translation pair generated using treelet 428 .", "label": "", "metadata": {}, "score": "74.61192"}
{"text": "Assume , for example , the source language input string of \" I have books \" .The proper French translation is \" Je ai de livre \" .The alignment between words in the source language and target language word segmented inputs is shown in .", "label": "", "metadata": {}, "score": "74.67705"}
{"text": "It will be appreciated that various of the above - disclosed and other features and functions , or alternatives thereof , may be desirably combined into many other different systems or applications .Also that various presently unforeseen or unanticipated alternatives , modifications , variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims .", "label": "", "metadata": {}, "score": "74.70119"}
{"text": "FIG .4A .The source language input ( or surface string ) is \" The old man ate \" .FIG .4A shows that the surface string simply includes arrows wherein the vertical , downward pointing arrow to the word \" ate \" identifies \" ate \" as the head word in the text fragment .", "label": "", "metadata": {}, "score": "74.73268"}
{"text": "The method of .claim 1 , wherein the evaluation of the at least one hypothesis includes selecting a hypothesis from a plurality of hypotheses which maximizes a log - linear scoring function in which weights are assigned to each of the feature functions .", "label": "", "metadata": {}, "score": "74.77316"}
{"text": "This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) .Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task .", "label": "", "metadata": {}, "score": "74.9577"}
{"text": "Also , agreement model 208 does not consider the number of words between the modifier and the word it modifies .All that is considered is that it is a dependent of a parent and thus modifies the parent .This can be advantageous over surface string language models .", "label": "", "metadata": {}, "score": "75.02386"}
{"text": "Great advances have been made in the sciences on which lexicography depends .Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography .Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown .", "label": "", "metadata": {}, "score": "75.10418"}
{"text": "claim 1 , wherein said one or more clues includes similar context .The method of .claim 1 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a target word .", "label": "", "metadata": {}, "score": "75.11093"}
{"text": "The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information .Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish .", "label": "", "metadata": {}, "score": "75.149734"}
{"text": "1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz .\" Building a Large Annotated Corpus of English : The Penn Treebank \" , Computational Linguistics 19.2 ( 1993 ) .McCarthy et al .2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll .", "label": "", "metadata": {}, "score": "75.18322"}
{"text": "However , there are two post - modifiers .Therefore , the order model must predict which is closest to the head .Thus , in addition to the probability shown in .FIG .2 , the order model must predict \" fatigu\u00e9 \" wins a challenge with respect to the post - modifier rooted at \" de \" as to which is closest to the head .", "label": "", "metadata": {}, "score": "75.40788"}
{"text": "Equation .Thus , Equation 3 calculates the probability that the term \" fatigu\u00e9 \" will win a challenge with respect to the remaining post modifiers given a variety of features .In one illustrative embodiment , the features considered when generating this probability are the same as those illustrated in Equation 1 .", "label": "", "metadata": {}, "score": "75.595375"}
{"text": "This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents .A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \" that glory would know my old age \" - would look like the following : .", "label": "", "metadata": {}, "score": "75.827576"}
{"text": "FIG .1 the total content differs , for example , in the removal of quotation marks , the removal word , \" by \" , and the like .The majority of the document content 15 ( at least 90 % and typically , at least 98 % ) of the total content is retained .", "label": "", "metadata": {}, "score": "75.91202"}
{"text": "The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty .What we need is a resource to combine the best of both .Where do we want to be ?The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated .", "label": "", "metadata": {}, "score": "76.00932"}
{"text": "FIG .13 .To follow through on the example , in which the source language input text 610 is \" the files installed on your computer \" , source language dependency parser component 606 generates source language dependency tree 614 , as shown in .", "label": "", "metadata": {}, "score": "76.0735"}
{"text": "An article of manufacture comprising a program storage medium readable by a computer comprising instructions for executing the method of . claim 1 .Description .CROSS REFERENCE TO RELATED PATENTS AND APPLICATIONS .Cross - reference is made to U.S. application Ser .", "label": "", "metadata": {}, "score": "76.24004"}
{"text": "Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a Continuation of U.S. patent application Ser .No .10/401,124 , filed on Mar. 26 , 2003 , now U.S. Pat .No .7,620,538 , which claims priority to U.S. Provisional Application Ser .", "label": "", "metadata": {}, "score": "76.48385"}
{"text": "The invention is operational with numerous other general purpose or special purpose computing system environments or configurations .The invention may be described in the general context of computer - executable instructions , such as program modules , being executed by a computer .", "label": "", "metadata": {}, "score": "76.487495"}
{"text": "The Hansard Corpus includes parallel texts in English and Canadian French , drawn from official records of the proceedings of the Canadian Parliament .A small bilingual lexicon of 6,900 entries was built using 5,000 sentences pairs ( 150,000 words for each language ) .", "label": "", "metadata": {}, "score": "76.67906"}
{"text": "Oxford Latin Dictionary .Oxford : Oxford University Press , 1968 - 1982 .Grozea 2004 Grozea , Christian .\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) .", "label": "", "metadata": {}, "score": "76.74686"}
{"text": "claim 1 , wherein each assigned cost is a function of a similarity measure for the elements in the pair .A document alignment method comprising : . inputting source leaves of a source document in first tree structured format , the first tree structured format comprising nodes which are ultimately connected with the source leaves by paths , each source leaf comprising text content ; . inputting target leaves of a target document in second tree structured format , the second tree structured format comprising nodes which are ultimately connected with the target leaves by paths , each target leaf comprising text content ; . subdividing the leaves of the source document and the leaves of the target document into blocks , each block including a set of the source leaves and a set of the target leaves ; . identifying a set of matches for which a total cost is minimal , wherein each of the input source and target leaves is in at least one of the identified matches ; . identifying , from the set of identified matches , groups of matches wherein each match in the group has a leaf in common ; . identifying , from the groups , probable matches in which more than one target leaf is matched with at least one source leaf and probable matches where more than one source leaf is matched with a target leaf ; and . outputting an alignment between leaves of the target document and leaves of the source document which includes the probable matches .", "label": "", "metadata": {}, "score": "77.01166"}
{"text": "^ .arg .max .t .J .Pr .t .J .s .I . )Pr .t .J .d .K .s .I . )Z .f .I . exp .", "label": "", "metadata": {}, "score": "77.02099"}
{"text": "A \" non - contiguous bi - fragment , \" as used herein , refers to a bi - fragment in which at least one of the text fragments comprises a non - contiguous phrase .It should be noted that in many cases discussed herein , both fragments of a bi - fragment are phrases ( i.e. , a bi - phrase ) although they need not be so .", "label": "", "metadata": {}, "score": "77.23834"}
{"text": "Even within a given time frame , spelling can vary , especially from poetry to prose .By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era .", "label": "", "metadata": {}, "score": "77.73454"}
{"text": "The basic schema for XML is the DTD ( Document Type Definition ) .Other XML schema definitions are also being developed , such as DCD ( Document Content Definition ) , XSchema , etc .DTD uses a different syntax from XML , while DCD and XSchema specify an XML schema language in XML itself .", "label": "", "metadata": {}, "score": "77.94182"}
{"text": "As shown in the example in .FIG .5 , not all of the phrases in the library which include two or more words need to be modeled as elastic bi - phrases , such as the phrases danser le tango and to tango .", "label": "", "metadata": {}, "score": "78.16074"}
{"text": "s .i . t .j . )l .i .j .log .l .i .j . )l .i .j . sim .s .i . t .j . )l .", "label": "", "metadata": {}, "score": "78.495995"}
{"text": "Combinations of any of the above should also be included within the scope of computer readable media .The system memory 130 includes computer storage media in the form of volatile and/or nonvolatile memory such as read only memory ( ROM ) 131 and random access memory ( RAM ) 132 .", "label": "", "metadata": {}, "score": "78.506134"}
{"text": "A lexicon builder 125 may expand the seed lexicon into the larger translation lexicon 105 by applying rules based on clues which indicate probable translations .The lexicon builder 125 may use seed lexicon to bootstrap these methods , using the word pairs in the seed lexicon as correct translations .", "label": "", "metadata": {}, "score": "78.75396"}
{"text": "The order model probability of the second possible order in .FIG .14C will be the probability that \" sur \" is a post - modifier of \" installe \" in position +1 multiplied by the probability that \" ordinateur \" is a post - modifier of \" sur \" in position +1 .", "label": "", "metadata": {}, "score": "79.07519"}
{"text": "Description .The present application is a divisional of and claims priority of U.S. patent application Ser .No .11/014,503 , filed Dec. 16 , 2004 , which is based on and claims the benefit of U.S. provisional patent application Ser .", "label": "", "metadata": {}, "score": "79.82437"}
{"text": "Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique .In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways .", "label": "", "metadata": {}, "score": "79.829994"}
{"text": "No .11/116,100 , filed Apr. 27 , 2005 , entitled \" METHOD FOR STRUCTURING DOCUMENTS BASED ON THEIR TOC , \" by Herv\u00e9 D\u00e9jean , et al . .U.S. application Ser .No . 11/032,817 , filed Jan. 10 , 2005 , entitled \" GLOBAL APPROACH FOR DETECTING PAGINATION CONSTRUCTS IN A DOCUMENT , \" by Herv\u00e9 D\u00e9jean , et al . .", "label": "", "metadata": {}, "score": "79.96978"}
{"text": "The swap / challenge model has some similarities to the model described above , except that it does not predict the probability of an offset but instead predicts two things separately .In other words , the model will predict whether a word will stay in the same direction relative to the head , or whether it will swap relative to the head .", "label": "", "metadata": {}, "score": "80.066795"}
{"text": "Tools . by Hieu Hoang , Alexandra Birch , Chris Callison - burch , Richard Zens , Marcello Federico , Nicola Bertoldi , Chris Dyer , Brooke Cowan , Wade Shen , Christine Moran , Ond\u0159ej Bojar , Alexandra Constantin , Evan Herbst , 2007 . \" ...", "label": "", "metadata": {}, "score": "80.38135"}
{"text": "Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own .", "label": "", "metadata": {}, "score": "80.47038"}
{"text": "In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching .This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) .", "label": "", "metadata": {}, "score": "80.67162"}
{"text": "la vaisselle \" in the sentence \" On peut faire beaucoup d'argent en important en france de la vaisselle chinoise \" ) .However , other features could be utilized to account for such unlikely gap sizes .As will be appreciated , an object of any translation model is to provide accurate translations and each added feature may contribute to this accuracy to greater or lesser extents .", "label": "", "metadata": {}, "score": "80.907036"}
{"text": "In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \" where are we now ? \" and \" where do we want to be ?\" : where are our colleagues already ?", "label": "", "metadata": {}, "score": "81.34955"}
{"text": "claim 1 , wherein the gap size scoring feature models gap sizes observed in a training corpus as a distribution which is expressed in terms of a parameter of the distribution .The method of . claim 5 , wherein the distribution comprises a Poisson distribution and the parameter of the distribution comprises a mean value of the Poisson distribution .", "label": "", "metadata": {}, "score": "81.35896"}
{"text": "The \" scholarship of thirty years ago \" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\u00f6rterbuch published only a decade before .Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide .", "label": "", "metadata": {}, "score": "81.61305"}
{"text": "FIG .1 , for example , hard disk drive 141 is illustrated as storing operating system 144 , application programs 145 , other program modules 146 , and program data 147 .Note that these components can either be the same as or different from operating system 134 , application programs 135 , other program modules 136 , and program data 137 .", "label": "", "metadata": {}, "score": "81.80905"}
{"text": "This is consistent both for a very close language pair , English - Danish , and a very distant language pair , English - Arabic .We also propose automatic reordering rule learning based on a rich set of linguistic information .", "label": "", "metadata": {}, "score": "82.14603"}
{"text": "This is consistent both for a very close language pair , English - Danish , and a very distant language pair , English - Arabic .We also propose automatic reordering rule learning based on a rich set of linguistic information .", "label": "", "metadata": {}, "score": "82.14603"}
{"text": "This is consistent both for a very close language pair , English - Danish , and a very distant language pair , English - Arabic .We also propose automatic reordering rule learning based on a rich set of linguistic information .", "label": "", "metadata": {}, "score": "82.14603"}
{"text": "Order model 206 provides a probability for each of the words at that level , falling in the annotated positions .Such a probability can be one such as that shown in Equation 1 .Equation 1 shows that the order model calculates the probability of the position of the word \" corbeille \" being plus 1 given a variety of factors .", "label": "", "metadata": {}, "score": "82.84652"}
{"text": "An output device 234 outputs the modified documents .The processing modules may any suitable device , circuit , or routine that is capable of performing the functions of the respective module .Bus 232 may be any device , circuit or routine that is capable of providing an interconnection between the elements 210 - 234 .", "label": "", "metadata": {}, "score": "82.96086"}
{"text": "claim 28 wherein identifying comprises : . identifying one lexical item , of a set of target language lexical items adjacent the unaligned lexical item in the target language text fragment , that is lowest on the target language dependency structure ; and . assigning the unaligned lexical item as dependent from the identified lexical item .", "label": "", "metadata": {}, "score": "83.33284"}
{"text": "For example , the phrase \" take the train . . .from . . .to \" is considered as take . . .the . . .train . . .from . . .to , where it is assumed that gaps exist between all the words .", "label": "", "metadata": {}, "score": "83.57559"}
{"text": "A Systematic Comparison of Various Statistical Alignment Models \" , Computational Linguistics 29.1 ( 2003 ) .Pinkster 1990 Pinkster , Harm .Latin Syntax and Semantics .London : Routledge , 1990 .Sch\u00fctz 1895 Sch\u00fctz , Ludwig .Thomas - Lexikon .", "label": "", "metadata": {}, "score": "83.58673"}
{"text": "10A , the word \" votre \" will depend from the word \" corbeille \" , but there are a number of different possibilities for attaching these nodes together .For instance , it is not known whether \" votre \" comes before \" corbeille \" , or whether it comes after .", "label": "", "metadata": {}, "score": "84.41731"}
{"text": "In all the above measures , a higher similarity number is indicative of a greater degree of similarity between the two leaves 11 , 13 in question , although it is to be appreciated that the measures may give different results which do not always agree .", "label": "", "metadata": {}, "score": "84.7236"}
{"text": "By way of example , and not limitation , .FIG .1 illustrates remote application programs 185 as residing on remote computer 180 .It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used .", "label": "", "metadata": {}, "score": "84.756645"}
{"text": "Busa 1974 - 1980 Busa , Roberto .Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI .", "label": "", "metadata": {}, "score": "85.23682"}
{"text": "Bamman and Crane 2007 Bamman , David and Gregory Crane . \"The Latin Dependency Treebank in a Cultural Heritage Digital Library \" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) .Bamman and Crane 2008 Bamman , David and Gregory Crane .", "label": "", "metadata": {}, "score": "85.944275"}
{"text": "Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \"An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) .Bourne 1916 Bourne , Ella . \"", "label": "", "metadata": {}, "score": "86.31389"}
{"text": "Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata .These citations essentially function as an index into the textual collection .If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic .", "label": "", "metadata": {}, "score": "86.39582"}
{"text": "Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation .For instance , the Latin word spiritus has several senses , including spirit and wind .", "label": "", "metadata": {}, "score": "86.63661"}
{"text": "However , they span different time periods and have a different orientation : the World Street Journal covers mostly business news , the German news wire mostly German politics .The system 100 may use clues to find translations of words in the monolingual corpora .", "label": "", "metadata": {}, "score": "86.70528"}
{"text": "2006 Klosa , Annette , Ulrich Schn\u00f6rch , and Petra Storjohann .\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\u00fcr deutsche Sprache , Mannheim \" , Proceedings of the 12th Euralex International Congress ( 2006 ) .", "label": "", "metadata": {}, "score": "86.78119"}
{"text": "claim 1 wherein an unaligned lexical item in the target language text fragment is unaligned to a lexical item in the source language dependency structure , and wherein projecting comprises : . identifying a lexical item located proximate the unaligned lexical item in the target language text fragment ; and . assigning the unaligned lexical item as dependent from the identified lexical item .", "label": "", "metadata": {}, "score": "87.55025"}
{"text": "Component 220 then makes the unaligned word a dependent of the origin of the identified arc .Therefore , in the present invention , component 220 makes the word \" de \" dependent from the word \" livre \" .This is shown in .", "label": "", "metadata": {}, "score": "87.60412"}
{"text": "claim 9 wherein identifying comprises : . identifying one lexical item , of a set of target language lexical items adjacent the unaligned lexical item in the target language text fragment , that is lowest on the target language dependency structure ; and . assigning the unaligned lexical item as dependent from the identified lexical item .", "label": "", "metadata": {}, "score": "87.96013"}
{"text": "Andrews 1850 Andrews , E. A. ( ed . )A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . .", "label": "", "metadata": {}, "score": "88.78242"}
{"text": "Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) .Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . )", "label": "", "metadata": {}, "score": "90.06947"}
{"text": "60/368,070 , filed on Mar. 26 , 2002 , and U.S. Provisional Application Ser .No .60/368,447 , filed on Mar. 27 , 2002 , the disclosures of which are incorporated by reference .ORIGIN OF INVENTION .The research and development described in this application were supported by Defense Advanced Research Project Agency ( DARPA ) under grant number N66001 - 00 - 1 - 8914 .", "label": "", "metadata": {}, "score": "90.10481"}
{"text": "The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) .The original English is you wished me to be safe .As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection .", "label": "", "metadata": {}, "score": "90.30734"}
{"text": "In addition to the monitor , computers may also include other peripheral output devices such as speakers 197 and printer 196 , which may be connected through an output peripheral interface 195 .The computer 110 may operate in a networked environment using logical connections to one or more remote computers , such as a remote computer 180 .", "label": "", "metadata": {}, "score": "90.948235"}
{"text": "The exemplary scoring module 24 includes instructions for computing a plurality of features 32 , 34 , 36 , 37 , 38 , 40 , 42 , 44 .It is to be appreciated that the scoring module may employ fewer , more , or different features than the features shown .", "label": "", "metadata": {}, "score": "91.97743"}
{"text": "When used in a WAN networking environment , the computer 110 typically includes a modem 172 or other means for establishing communications over the WAN 173 , such as the Internet .The modem 172 , which may be internal or external , may be connected to the system bus 121 via the user input interface 160 , or other appropriate mechanism .", "label": "", "metadata": {}, "score": "93.10076"}
{"text": "A user may enter commands and information into the computer 110 through input devices such as a keyboard 162 , a microphone 163 , and a pointing device 161 , such as a mouse , trackball or touch pad .Other input devices ( not shown ) may include a joystick , game pad , satellite dish , scanner , or the like .", "label": "", "metadata": {}, "score": "93.26497"}
{"text": "\" The lexicon builder 125 may measure the spelling similarity between every German and English word , and sort possible word pairs accordingly .This may be done in a greedy fashion , i.e. , once a word is assigned to a word pair , the lexicon builder 125 does not look for another match .", "label": "", "metadata": {}, "score": "93.63234"}
{"text": "Standard robust statistics may be used for treating \" atypical \" instances as outliers .For example , the target sentence \" He did not catch the train to Paris this week because he had no desire to travel by train anymore \" may be observed as including the non - contiguous phrase not_anymore with a gap size of 16 .", "label": "", "metadata": {}, "score": "93.78644"}
{"text": "Computer 110 typically includes a variety of computer readable media .Computer readable media can be any available media that can be accessed by computer 110 and includes both volatile and nonvolatile media , removable and non - removable media .By way of example , and not limitation , computer readable media may comprise computer storage media and communication media .", "label": "", "metadata": {}, "score": "94.046616"}
{"text": "dc.contributor.corporation .Copenhagen Business School .CBS . en_US .dc.contributor.department .Institut for Internationale Sprogstudier og Vidensteknologi ( . en_US .dc.contributor.departmentshort .ISV ( . en_US .dc.contributor.departmentuk .Department of International Language Studies and Computational Linguistics ( Syntactic reordering in statistical machine translation .", "label": "", "metadata": {}, "score": "94.618576"}
{"text": "An example may be helpful .Assume an input sentence is \" The tired man from the sea \" .A source language dependency tree structure generated for that sentence is shown in .FIG .11A .Possible matching treelet translation pairs are shown in .", "label": "", "metadata": {}, "score": "97.53485"}
{"text": "When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) .If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin .", "label": "", "metadata": {}, "score": "98.49486"}
{"text": "1 illustrates an example of a suitable computing system environment 100 on which the invention may be implemented .The computing system environment 100 is only one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of the invention .", "label": "", "metadata": {}, "score": "99.90003"}
{"text": "For instance , .FIG .10D illustrates the offset position of \" corbeille \" from \" vider \" and the offset position of \" le \" from \" corbeille \" . \"Corbeille \" has the offset position of plus 1 from \" vider \" , since it is a post - modifier ( or comes after ) \" vider \" by one position . \"", "label": "", "metadata": {}, "score": "105.63514"}
{"text": "The best word matches may be collected in a greedy fashion .Another clue is based on the assumption that pairs of words that are similar in one language should have translations that are similar in the other language .For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .", "label": "", "metadata": {}, "score": "126.17669"}
{"text": "Oxford : Clarendon Press , 1879 .Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . )A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones .Oxford : Clarendon Press , 1940 .", "label": "", "metadata": {}, "score": "128.82076"}
