{"text": "In these cases , use the function nltk.classify.apply_features , which returns an object that acts like a list but does not store all the feature sets in memory : . 1.2Choosing The Right Features .Selecting relevant features and deciding how to encode them for a learning method can have an enormous impact on the learning method 's ability to extract a good model .", "label": "", "metadata": {}, "score": "33.27261"}
{"text": "In addition , our discriminative approach integrally admits features beyond local tree configurations .We present a multi - scale training method along with an efficient CKY - style dynamic program .On a variety of domains and languages , this method produces the best published parsing accuracies with the smallest reported grammars .", "label": "", "metadata": {}, "score": "35.866264"}
{"text": "There are largely two kinds of transformation methods , that is , PCA and ICA .PCA projects the data into a new space spanned by the principal components [ 26 ] .In contrast to PCA , ICA decomposes an input dataset into components so that each component is statistically as independent from the others as possible .", "label": "", "metadata": {}, "score": "36.27539"}
{"text": "We present methods to control the lexicon size when learning a Combinatory Categorial Grammar semantic parser .Existing methods incrementally expand the lexicon by greedily adding entries , considering a single training datapoint at a time .We propose using corpus - level statistics for lexicon learning decisions .", "label": "", "metadata": {}, "score": "36.533714"}
{"text": "Now that we 've defined a feature extractor , we need to prepare a list of examples and corresponding class labels .Next , we use the feature extractor to process the names data , and divide the resulting list of feature sets into a training set and a test set .", "label": "", "metadata": {}, "score": "36.645485"}
{"text": "This situation is accomplished iff .Thus , this procedure detects the minimum set of genes required to provide the best classification coverage for a given training set .In addition , genes are descendingly ordered by number of 1 bits within the minimum set , .", "label": "", "metadata": {}, "score": "37.393856"}
{"text": "Because the parameters for Maximum Entropy classifiers are selected using iterative optimization techniques , they can take a long time to learn .This is especially true when the size of the training set , the number of features , and the number of labels are all large .", "label": "", "metadata": {}, "score": "37.824486"}
{"text": "When training a supervised classifier , you should split your corpus into three datasets : a training set for building the classifier model ; a dev - test set for helping select and tune the model 's features ; and a test set for evaluating the final model 's performance .", "label": "", "metadata": {}, "score": "38.33345"}
{"text": "( b )During prediction , the same feature extractor is used to convert unseen inputs to feature sets .These feature sets are then fed into the model , which generates predicted labels .In the rest of this section , we will look at how classifiers can be employed to solve a wide variety of tasks .", "label": "", "metadata": {}, "score": "38.421364"}
{"text": "Scaling Up to Large Datasets .Python provides an excellent environment for performing basic text processing and feature extraction .If you plan to train classifiers with large amounts of training data or a large number of features , we recommend that you explore NLTK 's facilities for interfacing with external machine learning packages .", "label": "", "metadata": {}, "score": "38.49252"}
{"text": "return features . words ( ' pos / cv957_8737 .The reason that we compute the set of all words in a document in , rather than just checking if word in document , is that checking whether a word occurs in a set is much faster than checking whether it occurs in a list ( 4.7 ) .", "label": "", "metadata": {}, "score": "38.698822"}
{"text": "Once a model is deemed sufficiently accurate , it can then be used to automatically predict information about new language data .These predictive models can be combined into systems that perform many useful language processing tasks , such as document classification , automatic translation , and question answering .", "label": "", "metadata": {}, "score": "38.87774"}
{"text": "As we are concerned with a collection of features and data ( instances ) , a natural way to split the variables would be to form two groups ( . )This split would be legitimate if the dimensionality of both subsets was quite similar .", "label": "", "metadata": {}, "score": "38.909058"}
{"text": "Unlike existing preordering models , we train feature - rich discriminative classifiers that directly predict the target - side word order .Our approach combines the strengths of lexical reordering and syntactic preordering models by performing long - distance reorderings using the structure of the parse tree , while utilizing a discriminative model with a rich set of features , including lexical features .", "label": "", "metadata": {}, "score": "38.92248"}
{"text": "One solution to this problem is to stop dividing nodes once the amount of training data becomes too small .Another solution is to grow a full decision tree , but then to prune decision nodes that do not improve performance on a dev - test .", "label": "", "metadata": {}, "score": "38.97811"}
{"text": "Once an initial set of features has been chosen , a very productive method for refining the feature set is error analysis .First , we select a development set , containing the corpus data for creating the model .This development set is then subdivided into the training set and the dev - test set .", "label": "", "metadata": {}, "score": "39.323753"}
{"text": "Our best results show a 26-fold speedup compared to a sequential C implementation .We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data .We first demonstrate that delexicalized parsers can be directly transferred between languages , producing significantly higher accuracies than unsupervised parsers .", "label": "", "metadata": {}, "score": "39.408978"}
{"text": "Second , how can we efficiently infer optimal structures within them ?Hierarchical coarse - to - fine methods address both questions .Coarse - to - fine approaches exploit a sequence of models which introduce complexity gradually .At the top of the sequence is a trivial model in which learning and inference are both cheap .", "label": "", "metadata": {}, "score": "39.569733"}
{"text": "In addition , the error rates of the corresponding classifier with the full set of features , without feature selection , are reported in the last column of Tables 4 , 5 and 6 .A similar comparison scheme is performed in [ 39 ] .", "label": "", "metadata": {}, "score": "39.867363"}
{"text": "Instead of just passing in the word to be tagged , we will pass in a complete ( untagged ) sentence , along with the index of the target word .This approach is demonstrated in 1.6 , which employs a context - dependent feature extractor to define a part of speech tag classifier . def pos_features ( sentence , i ) : . return features .", "label": "", "metadata": {}, "score": "40.355095"}
{"text": "Unlike previous work , our final model does not require any additional resources at run - time .Compared to a state - of - the - art approach , we achieve more than 20 % relative error reduction .Additionally , we annotate a corpus of search queries with part - of - speech tags , providing a resource for future work on syntactic query analysis .", "label": "", "metadata": {}, "score": "40.42028"}
{"text": "We present a novel approach which employs a randomized sequence of pruning masks .Formally , we apply auxiliary variable MCMC sampling to generate this sequence of masks , thereby gaining theoretical guarantees about convergence .Because each mask is generally able to skip large portions of an underlying dynamic program , our approach is particularly compelling for high - degree algorithms .", "label": "", "metadata": {}, "score": "40.456924"}
{"text": "Note .Most classification methods require that features be encoded using simple value types , such as booleans , numbers , and strings .But note that just because a feature has a simple type , this does not necessarily mean that the feature 's value is simple to express or compute .", "label": "", "metadata": {}, "score": "40.68313"}
{"text": "The ideas of feature and data reduction as well as hybrid approaches have been discussed in the realm of fuzzy modeling .Table 1 offers a snapshot at the diversity of the existing approaches and the advantages gained by completing the reduction processes .", "label": "", "metadata": {}, "score": "40.769753"}
{"text": "Transformational joint classifiers work by creating an initial assignment of labels for the inputs , and then iteratively refining that assignment in an attempt to repair inconsistencies between related inputs .The Brill tagger , described in ( 1 ) , is a good example of this strategy .", "label": "", "metadata": {}, "score": "40.836876"}
{"text": "Their features are specific , which allowed them to achieve good performance in the i2b2 challenge , but they are not common features used for NER in the biomedical domain and may not be generalizable to other tasks .Combining classifiers ' output in ensemble classifiers .", "label": "", "metadata": {}, "score": "41.00347"}
{"text": "It is capable of handling different measurement scales as well as both continuous and categorical variables .Advantages of C4.5 include easy to implement , no underlying assumptions and ability to generate non - parametric and non - linear models , and ability to reduce the dimensionality of input feature space in a way interpretable to the analysts .", "label": "", "metadata": {}, "score": "41.143143"}
{"text": "In contrast , the multi - disciplinary research field of data mining focuses on the non - trivial extraction of implicit , previously unknown , potentially useful knowledge about data [ 19 , 20 ] .Their advantages over signal processing methods include better adaptation to complex systems ; better accommodation to uncertainty ; and the fact they do not require calculation of characteristic defect frequencies from the physics of the components .", "label": "", "metadata": {}, "score": "41.15235"}
{"text": "When large amounts of annotated data are available , it is common to err on the side of safety by using 10 % of the overall data for evaluation .Another consideration when choosing the test set is the degree of similarity between instances in the test set and those in the development set .", "label": "", "metadata": {}, "score": "41.157936"}
{"text": "The framework used by supervised classification is shown in 1.1 .Figure 1.1 : Supervised Classification .( a )During training , a feature extractor is used to convert each input value to a feature set .These feature sets , which capture the basic information about each input that should be used to classify it , are discussed in the next section .", "label": "", "metadata": {}, "score": "41.180794"}
{"text": "Effect of each feature on system performance I. The first column shows the values when all features were used in the SVM learning ( word , POS , orthography ( orth . ) , prefix ( pre . ) , suffix ( suf . ) , dictionary matching ( dic . ) , and preceding class ( pc . ) )", "label": "", "metadata": {}, "score": "41.31659"}
{"text": "3.5 Cross - Validation .In order to evaluate our models , we must reserve a portion of the annotated data for the test set .As we already mentioned , if the test set is too small , then our evaluation may not be accurate .", "label": "", "metadata": {}, "score": "41.422653"}
{"text": "We used partial dictionary matching , and the matches found in the dictionary became features used by SVM .Here we report the performance of our system in the BioCreAtIvE competition , analyze its features , and discuss the effect of the parameters on SVM learning .", "label": "", "metadata": {}, "score": "41.538403"}
{"text": "The learning algorithm is applied to the original gene datasets as well as each newly obtained dataset containing only the selected genes , and in each case the final overall accuracy is measured .Figure 1 summarizes the learning accuracy of decision tree classifier on different feature sets .", "label": "", "metadata": {}, "score": "41.645702"}
{"text": "Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .We use graph - based label propagation for cross - lingual knowledge transfer and use the projected labels as features in an unsupervised model ( Berg - Kirkpatrick et al .", "label": "", "metadata": {}, "score": "41.651024"}
{"text": "Starting from a mono - phone model , we learn increasingly refined models that capture phone internal structures , as well as context - dependent variations in an automatic way .Our approaches reduces error rates compared to other baseline approaches , while streamlining the learning procedure .", "label": "", "metadata": {}, "score": "41.899773"}
{"text": "We present an automatic approach to tree annotation in which basic nonterminal symbols are alternately split and merged to maximize the likelihood of a training treebank .Starting with a simple Xbar grammar , we learn a new grammar whose nonterminals are subsymbols of the original nonterminals .", "label": "", "metadata": {}, "score": "41.986496"}
{"text": "In the notation of the previous section , we would have : .Such a model is trained by taking each tag occurrence and its conditioning context in the training data as a separate training instance [ 17 ] .This training process is simpler than that for CRFs as it does not require forward - backward computations .", "label": "", "metadata": {}, "score": "42.05413"}
{"text": "A combination of the extraction results from two or all three channels is also possible by using boolean functions ( such as AND , OR , XOR ) where applicable .This method is very effective with the right images ( e.g. , it can identify and separate overlapping cells ) .", "label": "", "metadata": {}, "score": "42.206738"}
{"text": "For example , noun phrases might be split into subcategories for subjects and objects , singular and plural , and so on .This splitting process admits an efficient incremental inference scheme which reduces parsing times by orders of magnitude .Furthermore , it produces the best parsing accuracies across an array of languages , in a fully language - general fashion .", "label": "", "metadata": {}, "score": "42.503223"}
{"text": "In all ensemble experiments a classification tree [ 16 ] was exploited as the base learner because it is sensitive to the changes in its training data .In order to provide a fair comparison , for all utilized ensemble techniques 100 trees are trained to constitute the corresponding ensemble classifiers .", "label": "", "metadata": {}, "score": "42.5048"}
{"text": "In contrast , the Maximum Entropy classifier model leaves it up to the user to decide what combinations of labels and features should receive their own parameters .In particular , it is possible to use a single parameter to associate a feature with more than one label ; or to associate more than one feature with a given label .", "label": "", "metadata": {}, "score": "42.676758"}
{"text": "We present several models to this end ; in particular a partially observed conditional random field model , where coupled token and type constraints provide a partial signal for training .Averaged across eight previously studied Indo - European languages , our model achieves a 25 % relative error reduction over the prior state of the art .", "label": "", "metadata": {}, "score": "42.7649"}
{"text": "At each set size , the best method of the compared techniques is identified and the log ratio between its error rate and corresponding error rate of the POS method is reported .Figure 5 shows the results with each classifier .", "label": "", "metadata": {}, "score": "42.823162"}
{"text": "This allows feature values to interact , but can be problematic when two or more features are highly correlated with one another .Maximum Entropy classifiers use a basic model that is similar to the model used by naive Bayes ; however , they employ iterative optimization to find the set of feature weights that maximizes the probability of the training set .", "label": "", "metadata": {}, "score": "42.830994"}
{"text": "In this study , as stated earlier , instead of approaching feature selection and instances selection separately , we focus on the integration of feature selection and instances selection in the construction of the fuzzy models .Both processes are applied simultaneously to the initial dataset , in order to obtain a suitable subset of feature and data to construct the parameters for the fuzzy model .", "label": "", "metadata": {}, "score": "43.02557"}
{"text": "The second set of results are for the system that also contains features extracted from external lexicons .These results are presented in the row Lexicons .Discussion .Adding the ABGene lexicons made a significant improvement to both precision and recall .", "label": "", "metadata": {}, "score": "43.217236"}
{"text": "Do you find any of them surprising ?Using the same training and test data , and the same feature extractor , build three classifiers for the task : a decision tree , a naive Bayes classifier , and a Maximum Entropy classifier .", "label": "", "metadata": {}, "score": "43.342445"}
{"text": "The essence of the cooperative version of PSO is essentially a parallel search for optimal subset of features and its optimal subset of instances .The cooperative strategy is achieved by dividing the candidate solution vector into components , called subswarm , where each subswarm represents a small part of the overall optimization processes .", "label": "", "metadata": {}, "score": "43.396343"}
{"text": "C5.0 is another enhancement of C4.5 with additional effectiveness , efficiencies and supports boosting .However , the C4.5 and C5.0 decision tree algorithms produce models with similar predictive accuracies [ 57 ] .While C4.5 decision trees naturally handle multiple classes , they can only examine a single feature at a time and do not perform well when the features are highly interdependent .", "label": "", "metadata": {}, "score": "43.440823"}
{"text": "This is exactly what the Maximum Entropy classifier does as well .In particular , for each joint - feature , the Maximum Entropy model calculates the \" empirical frequency \" of that feature - i.e. , the frequency with which it occurs in the training set .", "label": "", "metadata": {}, "score": "43.462914"}
{"text": "Many methods refer to a comparison to a so called \" ground truth \" in the first place .For a user doing a binary segmentation , this is basically a user - created binarization of an example image to test the extraction \" quality \" .", "label": "", "metadata": {}, "score": "43.59204"}
{"text": "First , we define a function that maps a single state at input position j to a set of allowed next states at position j + 1 , T j ( s ) .These values are calculated by the following recurrences : .", "label": "", "metadata": {}, "score": "43.608315"}
{"text": "Because of the potentially complex interactions between the effects of related features , there is no way to directly calculate the model parameters that maximize the likelihood of the training set .Therefore , Maximum Entropy classifiers choose the model parameters using iterative optimization techniques , which initialize the model 's parameters to random values , and then repeatedly refine those parameters to bring them closer to the optimal solution .", "label": "", "metadata": {}, "score": "43.67793"}
{"text": "In the OVO or OAO strategy , classifiers are trained to discriminate between two of the k possible classes .One classifier is trained with a data subset of each possible pair of classes , then the output of the ( . k . classifiers are combined for prediction .", "label": "", "metadata": {}, "score": "43.697277"}
{"text": "By taking the best immediate , or local , solution in finding an answer , the algorithm selects an attribute to split on at any given node with any given data set , decides whether to stop branching and what branches to form .", "label": "", "metadata": {}, "score": "43.718815"}
{"text": "Our methods result in state - of - the - art performance on the task of executing sequences of natural language instructions , achieving up to 25 % error reduction , with lexicons that are up to 70 % smaller and are qualitatively less noisy .", "label": "", "metadata": {}, "score": "43.778168"}
{"text": "However , any changes to a maximum entropy tagging model can be trivially applied to the corresponding CRF model , including changes in model order and feature choice .For all the tagging tasks that we have attempted , CRF models can be trained in under 20 hours ( and in most cases under 10 hours ) , which is quite practical since training is done only a few times .", "label": "", "metadata": {}, "score": "43.785576"}
{"text": "Unlike previous work on projecting syntactic resources , we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers .The projected parsers from our system result in state - of - the - art performance when compared to previously studied unsupervised and projected parsing systems across eight different languages .", "label": "", "metadata": {}, "score": "43.79045"}
{"text": "If we want to generate a probability estimate for each label , rather than just choosing the most likely label , then the easiest way to compute P(features ) is to simply calculate the sum over labels of P(features , label ) : . 5.2 Zero Counts and Smoothing .", "label": "", "metadata": {}, "score": "43.790695"}
{"text": "The simple algorithm for selecting decision stumps described above must construct a candidate decision stump for every possible feature , and this process must be repeated for every node in the constructed decision tree .A number of algorithms have been developed to cut down on the training time by storing and reusing information about previously evaluated examples .", "label": "", "metadata": {}, "score": "43.824837"}
{"text": "Statistical features are extracted from the time domain and the frequency domain .Prediction performance of the proposed strategy is compared with that of a simple multi - class classification , as well as that of random guess and worst - case classification .", "label": "", "metadata": {}, "score": "44.04523"}
{"text": "These include iterative scaling techniques [ 10 ] and gradient - based techniques [ 11 , 12 ] .All these methods require the calculation of the empirical and model expectations at each iteration .The empirical expectations are trivially calculated by counting in the training data the number of times each feature occurs .", "label": "", "metadata": {}, "score": "44.085743"}
{"text": "Here , we utilized 8 publicly available benchmark datasets [ 32 ] .A brief overview of these datasets is summarized in Table 1 .Preprocessing is an important step for handling gene expression data .This includes two steps : filling missing values and normalization .", "label": "", "metadata": {}, "score": "44.123802"}
{"text": "It becomes clear that one is able to reduce the input data in terms of the number features and instances .Moreover , the flexibility of choosing the reduction level helps the user focus on the most essential subsets of data and features ( variables ) .", "label": "", "metadata": {}, "score": "44.240482"}
{"text": "An extensive study in [ 68 ] has shown that the two binarization strategies with an appropriate combination strategy are simple and useful ensemble methods to improve classifier performances , even when the classification algorithm itself can handle multiple classes .C4.5 decision tree used in this paper is an example of classification algorithms that can handle multiple classes .", "label": "", "metadata": {}, "score": "44.289032"}
{"text": "Thus , a researcher generally provides some degree of verification , explains its rationale , and acknowledges its limitations when commenting on the quality of a binarization .A related issue is training the user .The \" Threshold Check \" , [ 1 ] is a trial to make it easier for the untrained user , to decide for one or the other threshold .", "label": "", "metadata": {}, "score": "44.328114"}
{"text": "Since the number of irrelevant documents far outweighs the number of relevant documents , the accuracy score for a model that labels every document as irrelevant would be very close to 100 % .It is therefore conventional to employ a different set of measures for search tasks , based on the number of items in each of the four categories shown in 3.1 : .", "label": "", "metadata": {}, "score": "44.329838"}
{"text": "Each time the error analysis procedure is repeated , we should select a different dev - test / training split , to ensure that the classifier does not start to reflect idiosyncrasies in the dev - test set .But once we 've used the dev - test set to help us develop the model , we can no longer trust that it will give us an accurate idea of how well the model would perform on new data .", "label": "", "metadata": {}, "score": "44.35857"}
{"text": "The process can then be repeated until all of the inputs have been labeled .This strategy is demonstrated in 1.7 .First , we must augment our feature extractor function to take a history argument , which provides a list of the tags that we 've predicted for the sentence so far .", "label": "", "metadata": {}, "score": "44.368237"}
{"text": "Clustering such items is a viable algorithmic approach .Running K - means or fuzzy C - means produces clusters ( group ) of variables that are used in the individual PSO .( c )In case both subsets are large , the clustering is realized both for the features and data , and the resulting , structure ( partition ) is used to run cooperative PSO .", "label": "", "metadata": {}, "score": "44.388027"}
{"text": "Significance tests .In order to compare performance between different systems , we used an approximate randomization approach for testing significance as described in [ 9 ] .We then combined A and B to a superset C .Then we randomly drew j entries from C without resampling and created the pseudo set of entries A i ; the remainder of C forms the pseudo set of entries B i .", "label": "", "metadata": {}, "score": "44.391068"}
{"text": "The model is formally a latent variable CRF grammar over trees , learned by iteratively splitting grammar productions ( not categories ) .Different regions of the grammar are refined to different degrees , yielding grammars which are three orders of magnitude smaller than the single - scale baseline and 20 times smaller than the split - and - merge grammars of Petrov et al .", "label": "", "metadata": {}, "score": "44.407368"}
{"text": "Although the proposed methodology is of general nature , we concentrate on rule - based models , which are commonly present in fuzzy modeling , to help offer a detailed view of the overall design process .An Overall Reduction Process .", "label": "", "metadata": {}, "score": "44.42639"}
{"text": "The algorithm uses a similarity graph to encourage similar n - grams to have similar POS tags .We demonstrate the efficacy of our approach on a domain adaptation task , where we assume that we have access to large amounts of unlabeled data from the target domain , but no additional labeled data .", "label": "", "metadata": {}, "score": "44.450172"}
{"text": "Given a set of training samples , the SVM training phase tries to find the optimal hyperplane , which maximizes the distance of the training samples nearest to it ( called support vectors ) .SVM takes as input a vector and maps it into a feature space using a kernel function [ 18 , 19 ] .", "label": "", "metadata": {}, "score": "44.502777"}
{"text": "Firstly , the simultaneous feature and instances selection is easily adapted to construct the structure of the fuzzy model .Secondly , the best selected subset of data obtained with this framework is capable of representing the original large data set .", "label": "", "metadata": {}, "score": "44.52716"}
{"text": "Our first- , second- , and third - order models achieve accuracies comparable to those of their unpruned counterparts , while exploring only a fraction of the search space .We observe speed - ups of up to two orders of magnitude compared to exhaustive search .", "label": "", "metadata": {}, "score": "44.544006"}
{"text": "Here , the size of the selected features ( . ) is provided in advance by the user .After the PSO meets the maximum generation , the process is stopped , and the last best subset of features and data is the best subset of data for constructing the fuzzy model .", "label": "", "metadata": {}, "score": "44.562542"}
{"text": "Following transformation , the axes are rotated optimally .Despite the conventional approach of choosing some directions for good discriminate capability , the rotation mainly contributes to the generation of diversity among the classifiers without weakening the individual classifiers .Thus , an acceptable trade - off between diversity and accuracy can be maintained simultaneously .", "label": "", "metadata": {}, "score": "44.57537"}
{"text": "Even though the individual folds might be too small to give accurate evaluation scores on their own , the combined evaluation score is based on a large amount of data , and is therefore quite reliable .A second , and equally important , advantage of using cross - validation is that it allows us to examine how widely the performance varies across different training sets .", "label": "", "metadata": {}, "score": "44.663177"}
{"text": "Implementation .Presented here is an outline of conditional random fields and the implementation specifics of the model we use .Conditional random fields .Gene identification as a tagging problem .A sample tagging of a sentence using the beginning , inside and outside tag labels .", "label": "", "metadata": {}, "score": "44.763145"}
{"text": "To determine which lexicons gave the best performance , we conducted experiments examining the effect of adding each type of lexicon individually to the model and tested the model on the development data .These results are outlined in Table 2 .", "label": "", "metadata": {}, "score": "44.775238"}
{"text": "A somewhat better approach is to ensure that the training set and test set are taken from different documents : .If we want to perform a more stringent evaluation , we can draw the test set from documents that are less closely related to those in the training set : .", "label": "", "metadata": {}, "score": "44.77903"}
{"text": "In the OVA or OAA strategy , classifiers are trained to determine whether an instance belongs to one of the k classes or not .Only k classifiers are needed in this strategy , but the training set for each classifier is much larger and the number of instances for the negative class can be much larger than the positive class .", "label": "", "metadata": {}, "score": "44.819046"}
{"text": "The goal of this chapter is to answer the following questions : .How can we identify particular features of language data that are salient for classifying it ?How can we construct models of language that can be used to perform language processing tasks automatically ?", "label": "", "metadata": {}, "score": "44.890694"}
{"text": "In addition , a cooperative PSO is developed in order to overcome the limitation of using standard PSO when dealing with a high - dimensional search space .The size of the selected features and data used to construct the fuzzy model can be adjusted based upon the feedback provided in terms of the performance of the model constructed for the currently accepted .", "label": "", "metadata": {}, "score": "44.96627"}
{"text": "Nonetheless , the resulting grammars encode many linguistically interpretable patterns and give the best published parsing accuracies on three German treebanks .We demonstrate that log - linear grammars with latent variables can be practically trained using discriminative methods .Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be efficiently approximated in a gradient - based procedure .", "label": "", "metadata": {}, "score": "45.055264"}
{"text": "Section 2 is a brief summary on the background of techniques used in this paper , namely support vector machine ( SVM ) ; C4.5 decision tree ; and class binarization .Section 3 introduces the hypothesis and logic behind the problem formulation used in modeling .", "label": "", "metadata": {}, "score": "45.060894"}
{"text": "We highlight the use of this resource via two experiments , including one that reports competitive accuracies for unsupervised grammar induction without gold standard part - of - speech tags .We present an online learning algorithm for training structured prediction models with extrinsic loss functions .", "label": "", "metadata": {}, "score": "45.068657"}
{"text": "System description .The concept of our system is shown in Figure 1 .We use SVM as the machine learning algorithm .The training data is a set of feature vectors with a binary value ( +1 for a positive example and -1 for a negative example ) .", "label": "", "metadata": {}, "score": "45.072876"}
{"text": "The R packages ' class ' and ' e1071 ' are used to perform the k Nearest Neighbor and Support Vector Machine classifiers respectively .The parameter k for k NN classifier is chosen to be rounded to the nearest odd number , where N is the total number of observations ( tissue samples ) .", "label": "", "metadata": {}, "score": "45.14193"}
{"text": "Use the dev - test set to check your progress .Once you are satisfied with your classifier , check its final performance on the test set .How does the performance on the test set compare to the performance on the dev - test set ?", "label": "", "metadata": {}, "score": "45.148552"}
{"text": "The following notations will be used in both Sections 2.1 and 2.2 .The rest of this section is a brief overview of the SVM algorithm ; the C4.5 decision tree and the class binarization strategy to be used in the proposed method .", "label": "", "metadata": {}, "score": "45.198463"}
{"text": "However , as far as we know there is no research that has been done to simultaneously select the best subset of features and input data for constructing the fuzzy model .Most of the research is focused on reducing the fuzzy rules , and the process of simplifying the system is done once the design has been completed .", "label": "", "metadata": {}, "score": "45.2389"}
{"text": "The analysed relationship between classification accuracies yielded by three different classifiers and stability confirms that the POS method can provide a good trade\u2010off between stability and classification accuracy .In the future , one can investigate the possibility of extending POS method to handle multi\u2010class situations .", "label": "", "metadata": {}, "score": "45.259193"}
{"text": "Specifically , an ini- tial hypothesis lattice is constrcuted using local features .Candidate sentences are then assigned syntactic language model scores .These global syntactic scores are combined with local low - level scores in a log - linear model .", "label": "", "metadata": {}, "score": "45.264595"}
{"text": "Results .Our system was initially trained on 7500 annotated MEDLINE sentences with a development set of 2500 sentences .Training with feature induction took approximately 15 hours , which is substantially longer than training without feature induction .Once trained , the system can annotate sentences in less than a second .", "label": "", "metadata": {}, "score": "45.369225"}
{"text": "In some situations , one of the subsets ( either the data or the features ) might be significantly larger than the other one .We often encounter a large number of data , but in some situations , a large number of features might be present ( for instance , in microarray data analysis ) .", "label": "", "metadata": {}, "score": "45.389576"}
{"text": "In most cases , the standard deviations of error produced by the CPSO are smaller than the results obtained for the standard PSO ( see Table 11 ) .Figure 12 shows the subsets of the features selected for different percentages of the features used in construction of the fuzzy model .", "label": "", "metadata": {}, "score": "45.448334"}
{"text": "Most evaluation techniques calculate a score for a model by comparing the labels that it generates for the inputs in a test set ( or evaluation set ) with the correct labels for those inputs .This test set typically has the same format as the training set .", "label": "", "metadata": {}, "score": "45.456028"}
{"text": "Evolutionary algorithms have also been used for building compact fuzzy rules [ 9 - 12 ] .An evolutionary algorithm is used to tune the structure and the rules ' parameter of the fuzzy systems [ 13 , 14 ] .However , in numerous cases , some variables are not crucial to the realization of the fuzzy model .", "label": "", "metadata": {}, "score": "45.501526"}
{"text": "We show that the automatically induced latent variable grammars of Petrov et al .2006 vary widely in their underlying representations , depending on their EM initialization point .We use this to our advantage , combining multiple automatically learned grammars into an unweighted product model , which gives significantly improved performance over state - of - the - art individual grammars .", "label": "", "metadata": {}, "score": "45.520653"}
{"text": "Three training sessions were done in a pair - wise fashion , i.e. ( B vs. I ) , ( B vs. O ) , and ( I vs. O ) , and three hyperplanes were formed .In the test data , the optimal class of each token was the class that had the maximum value in the three hyperplane functions .", "label": "", "metadata": {}, "score": "45.59831"}
{"text": "Once we have a decision tree , it is straightforward to use it to assign labels to new input values .What 's less straightforward is how we can build a decision tree that models a given training set .But before we look at the learning algorithm for building decision trees , we 'll consider a simpler task : picking the best \" decision stump \" for a corpus .", "label": "", "metadata": {}, "score": "45.612194"}
{"text": "Continuing on , the classifier checks if the word ends in \" s \" .1.5 Exploiting Context .By augmenting the feature extraction function , we could modify this part - of - speech tagger to leverage a variety of other word - internal features , such as the length of the word , the number of syllables it contains , or its prefix .", "label": "", "metadata": {}, "score": "45.61875"}
{"text": "This is a single probabilistic tagging model with no application - specific pre- or post - processing steps or voting over multiple classifiers .This makes the model quite general in that it may be extended to various other biological entities , provided appropriate lexicons are available .", "label": "", "metadata": {}, "score": "45.75051"}
{"text": "One solution to this problem is to perform multiple evaluations on different test sets , then to combine the scores from those evaluations , a technique known as cross - validation .In particular , we subdivide the original corpus into N subsets called folds .", "label": "", "metadata": {}, "score": "45.830887"}
{"text": "However , combining different classifiers to further improve the performance of clinical entity recognition systems has not been investigated extensively .Combining classifiers into an ensemble classifier presents both challenges and opportunities to improve performance in such NLP tasks .Methods .", "label": "", "metadata": {}, "score": "45.862183"}
{"text": "Because each refinement introduces only limited complexity , both learning and inference can be done in an incremental fashion .In this dissertation , we describe several coarse - to - fine systems .In the domain of syntactic parsing , complexity is in the grammar .", "label": "", "metadata": {}, "score": "45.87091"}
{"text": "We also show that our techniques can be applied to full - scale parsing applications by demonstrating its effectiveness in learning state - split grammars .Treebank parsing can be seen as the search for an optimally refined grammar consistent with a coarse training treebank .", "label": "", "metadata": {}, "score": "45.893383"}
{"text": "Consider , for instance , the difference between global and local thresholds in binary segmentation .During global thresholding , the image as a whole is taken to determine the cut - off value and this value is applied to each pixel in the image .", "label": "", "metadata": {}, "score": "45.940372"}
{"text": "We present experiments with sequence models on part - of - speech tagging and named entity recognition tasks , and with syntactic parsers on dependency parsing and machine translation reordering tasks .Low - latency solutions for syntactic parsing are needed if parsing is to become an integral part of user - facing natural language applications .", "label": "", "metadata": {}, "score": "45.954918"}
{"text": "But there 's a lot to be learned from taking a closer look at how these learning methods select models based on the data in a training set .An understanding of these methods can help guide our selection of appropriate features , and especially our decisions about how those features should be encoded .", "label": "", "metadata": {}, "score": "46.062195"}
{"text": "Consequently , the bits of ones corresponding to the classification of still uncovered samples are only considered .Note that represents updated mask of gene i at the k th iteration such that is its original gene mask whose elements are computed according to equation 8 .", "label": "", "metadata": {}, "score": "46.26586"}
{"text": "The overall scheme of the reduction process outlining a role of the PSO - guided reduction is illustrated in Figure 2 .The scheme can be divided into two important parts and can be described as follows .( a ) Reduction process via PSO : a reduction process tackles both feature reduction and data reduction simultaneously .", "label": "", "metadata": {}, "score": "46.334053"}
{"text": "In summary , descriptive models provide information about correlations in the data , while explanatory models go further to postulate causal relationships .Most models that are automatically constructed from a corpus are descriptive models ; in other words , they can tell us what features are relevant to a given pattern or construction , but they ca n't necessarily tell us how those features and patterns relate to one another .", "label": "", "metadata": {}, "score": "46.35466"}
{"text": "Second , we integrate the concept of feature selection and data selection together in the unified form to further refine ( reduce ) the fuzzy models .In this regard , the PSO technique is applied in order to search for the best subset of data .", "label": "", "metadata": {}, "score": "46.361015"}
{"text": "While rule - based approaches use existing biomedical knowledge / resources , supervised machine learning based approaches rely heavily on annotated training data and domain dictionaries .The advantage of rule - based approaches is that they are easily customized to new vocabulary and author styles , while supervised machine learning approaches often report better results when the task domain does not change and training data is plentiful .", "label": "", "metadata": {}, "score": "46.39464"}
{"text": "In this case , the classifier will make its decisions based only on information about which of the common suffixes ( if any ) a given word has .Now that we 've defined our feature extractor , we can use it to train a new \" decision tree \" classifier ( to be discussed in 4 ): .", "label": "", "metadata": {}, "score": "46.61271"}
{"text": "In this paper , we used the fuzzy C - means ( FCMs ) to construct the information granules .Therefore , the number of decomposition groups is actually the number of the clusters ( . ) used in the FCM .", "label": "", "metadata": {}, "score": "46.61553"}
{"text": "Being based on image - intrinsic properties does not mean automated segmentation methods are strictly objective and bias free .Obviously , since the user has a choice of algorithms , the final decision on which algorithm to apply under specific conditions or a specific experiment is necessarily subjective .", "label": "", "metadata": {}, "score": "46.646782"}
{"text": "To avoid bias , gene selection algorithms have been performed only on the training sets .The expressions of the selected genes as well as the class labels of the training samples have then been used to construct the considered classifiers .", "label": "", "metadata": {}, "score": "46.668823"}
{"text": "Previous sentence segmentation systems have typically been very local , using low - level prosodic and lexical features to independently decide whether or not to segment at each word boundary position .In this work , we leverage global syntactic information from a syn- tactic parser , which is better able to capture long distance depen- dencies .", "label": "", "metadata": {}, "score": "46.67439"}
{"text": "We extend and improve upon recent work in structured training for neural network transition - based dependency parsing .We do this by experimenting with novel features , additional transition systems and by testing on a wider array of languages .In particular , we introduce set - valued features to encode the predicted morphological properties and part - of - speech confusion sets of the words being parsed .", "label": "", "metadata": {}, "score": "46.75732"}
{"text": "Recently , Sutskever et al .( 2014 ) presented a task - agnostic method for learning to map input sequences to output sequences that achieved strong results on a large scale machine translation problem .In this work , we show that precisely the same sequence - to - sequence method achieves results that are close to state - of - the - art on syntactic constituency parsing , whilst making almost no assumptions about the structure of the problem .", "label": "", "metadata": {}, "score": "46.78663"}
{"text": "1.6 Sequence Classification .In order to capture the dependencies between related classification tasks , we can use joint classifier models , which choose an appropriate labeling for a collection of related inputs .In the case of part - of - speech tagging , a variety of different sequence classifier models can be used to jointly choose part - of - speech tags for all the words in a given sentence .", "label": "", "metadata": {}, "score": "46.787025"}
{"text": "In particular , for each consecutive word index i , a score is computed for each possible current and previous tag .This same basic approach is taken by two more advanced models , called Maximum Entropy Markov Models and Linear - Chain Conditional Random Field Models ; but different algorithms are used to find scores for tag sequences .", "label": "", "metadata": {}, "score": "46.79323"}
{"text": "Besides the algorithm(s ) to use , the way how data are utilized in modeling is another critical factor for the success of data - driven methods in modeling a problem .For example , when a multi - class real - world problem is modeled with a binary class classification algorithm , as in SVM , different class binarization strategies can be used .", "label": "", "metadata": {}, "score": "46.817207"}
{"text": "In our method the first , monolingual view consists of supervised predictors learned separately for each language .The second , bilingual view consists of log - linear predictors learned over both languages on bilingual text .Our training procedure estimates the parameters of the bilingual model using the output of the monolingual model , and we show how to combine the two models to account for dependence between views .", "label": "", "metadata": {}, "score": "46.85484"}
{"text": "The spatial methods use higher - order probability distribution and/or correlation between pixels .Local methods adapt the threshold value on each pixel to the local image characteristics .This list might not be comprehensive but gives a good idea about the extensive possibilities available .", "label": "", "metadata": {}, "score": "46.86628"}
{"text": "While these models are easy to train , they are independently normalized at each position j ( equation 4 ) .In contrast , conditional random fields have a single combined normalizing denominator of Z ( o ) for the entire tag sequence t ( equations 1 and 2 ) .", "label": "", "metadata": {}, "score": "46.876926"}
{"text": "Ideally , independent comparisons to different image - intrinsic measures ( e.g. , object edges , known angles or ratios between structural components , numbers of particular structural components , exclusion of known artifacts , entropy , etc . ) should also be included when verifying the quality of a binarization .", "label": "", "metadata": {}, "score": "46.878387"}
{"text": "Machine learning - based methods .In the case of named entity recognition , the labels incorporate two concepts : the type of the entity and the position of the token within the entity .In this project , we used a simple representation for token position called BIO .", "label": "", "metadata": {}, "score": "47.021473"}
{"text": "In contrast to the existing studies , which are focused predominantly on feature selection ( namely , a reduction of the input space ) , a position advocated here is that a reduction has to involve both data and features to become efficient to the design of fuzzy model .", "label": "", "metadata": {}, "score": "47.101852"}
{"text": "To begin with , they 're simple to understand , and easy to interpret .This is especially true near the top of the decision tree , where it is usually possible for the learning algorithm to find very useful features .", "label": "", "metadata": {}, "score": "47.1202"}
{"text": "Now , we have two disjoint groups ( one for each class ) of ranked genes .The topmost gene is selected from each group in a round\u2010robin fashion to compose the gene ranking list .The resulting final set includes the minimum subset of genes regardless of their POS values , because these genes allow the considered classifier to correctly classify the maximum number of training samples .", "label": "", "metadata": {}, "score": "47.173782"}
{"text": "The pseudo code of our procedure is reported in Algorithm 1 .Its inputs are the matrix of gene masks , M ; the aggregate mask of genes , ; and POS scores .It produces the minimum set of genes , , as output .", "label": "", "metadata": {}, "score": "47.193916"}
{"text": "[ 2 ] , is used to assess genes and to identify informative ones based on their contribution in the process of classifying samples when large number of classification trees have been constructed .The contribution of a particular gene to the relative importance measure is defined by a weighted scale of the overall number of splits made on that gene in all constructed trees .", "label": "", "metadata": {}, "score": "47.21031"}
{"text": "Although it can be possible to gain insight by studying them , it typically takes a lot more work .But all explicit models can make predictions about new \" unseen \" language data that was not included in the corpus used to build the model .", "label": "", "metadata": {}, "score": "47.216763"}
{"text": "Standard inference can be used at test time .Our approach is able to scale to very large problems and yields significantly improved target domain accuracy .It is well known that parsing accuracies drop significantly on out - of - domain data .", "label": "", "metadata": {}, "score": "47.289845"}
{"text": "But note that history will only contain tags for words we 've already classified , that is , words to the left of the target word .Thus , while it is possible to look at some features of words to the right of the target word , it is not possible to look at the tags for those words ( since we have n't generated them yet ) .", "label": "", "metadata": {}, "score": "47.304077"}
{"text": "Individual features make their contribution to the overall decision by \" voting against \" labels that do n't occur with that feature very often .In particular , the likelihood score for each label is reduced by multiplying it by the probability that an input value with that label would have the feature .", "label": "", "metadata": {}, "score": "47.31785"}
{"text": "Feature sets were extracted from common lexical features as described in [ 3 , 5 , 7 , 25 ] and semantic features from MedEx .Evaluation for this setting was done using 10-fold cross - validation .In the second setting , we compared our system with the system proposed by Patrick and Li [ 10 ] , the top - ranked system in the 2009 i2b2 challenge .", "label": "", "metadata": {}, "score": "47.321636"}
{"text": "Despite its simplicity , a product of eight automatically learned grammars improves parsing accuracy from 90.2 % to 91.8 % on English , and from 80.3 % to 84.5 % on German .Pruning can massively accelerate the computation of feature expectations in large models .", "label": "", "metadata": {}, "score": "47.351486"}
{"text": "This is used , for instance , to identify , separate , and analyze features of overlapping cells .This method is very useful when used with the RoiManager and in macros or plugins for automating tasks .How do I check the quality of a binarization ?", "label": "", "metadata": {}, "score": "47.359367"}
{"text": "As a result , instead of having only two subswarms , we introduce more subswarms that represent different groups of data .Figure 5 presents the process of constructing the subswarms for cooperative PSO by decomposing the instances into several subswarms .", "label": "", "metadata": {}, "score": "47.398346"}
{"text": "With 100 K unlabeled and 2 K labeled questions , uptraining is able to improve parsing accuracy to 84 % , closing the gap between in - domain and out - of - domain performance .We study self - training with products of latent variable grammars in this paper .", "label": "", "metadata": {}, "score": "47.42659"}
{"text": "The intuition that motivates Maximum Entropy classification is that we should build a model that captures the frequencies of individual joint - features , without making any unwarranted assumptions .An example will help to illustrate this principle .Suppose we are assigned the task of picking the correct word sense for a given word , from a list of ten possible senses ( labeled A - J ) .", "label": "", "metadata": {}, "score": "47.431015"}
{"text": "We ran this procedure ten times ; each time we picked a different subset for testing and the nine remaining subsets for training .Each time we built a classifier with the training sets and evaluated on the testing set .Reported results for SVM in this paper use the same parameter settings of SVM .", "label": "", "metadata": {}, "score": "47.492073"}
{"text": "The shaded area in Figure 2 shows the elements of the feature vectors for the current word , i.e. \" NF - kappaB \" .The information from the two preceding and two following tokens is used for each vector .YamCha counts the number of features , and changes each feature into a unique positive integer .", "label": "", "metadata": {}, "score": "47.574665"}
{"text": "Thus , when using a more powerful model , we end up with less data that can be used to train each parameter 's value , making it harder to find the best parameter values .As a result , a generative model may not do as good a job at answering questions 1 and 2 as a conditional model , since the conditional model can focus its efforts on those two questions .", "label": "", "metadata": {}, "score": "47.696053"}
{"text": "The information given by the PSO is used to represent the subset of features and data to construct the data - driven fuzzy models .Then , the numerical data are represented in terms of a collection of information granules ( a fuzzy sets ) produced through some clustering ( fuzzy clustering ) .", "label": "", "metadata": {}, "score": "47.711975"}
{"text": "The features and parameters we used in the BioCreAtIvE competition are shown in Tables 1 and 4 .We tested three methods for the dictionary matching ( 1 st , 2 nd , and 3 rd runs ) .Table 4 .", "label": "", "metadata": {}, "score": "47.76644"}
{"text": "See the NLTK webpage for a list of recommended machine learning packages that are supported by NLTK .3 Evaluation .In order to decide whether a classification model is accurately capturing a pattern , we must evaluate that model .The result of this evaluation is important for deciding how trustworthy the model is , and for what purposes we can use it .", "label": "", "metadata": {}, "score": "47.91839"}
{"text": "Of course , this assumption is unrealistic ; features are often highly dependent on one another .We 'll return to some of the consequences of this assumption at the end of this section .This simplifying assumption , known as the naive Bayes assumption ( or independence assumption ) makes it much easier to combine the contributions of the different features , since we do n't need to worry about how they should interact with one another .", "label": "", "metadata": {}, "score": "47.940907"}
{"text": "Note that , while m i j is the j th mask element of gene i ( see equation 8 ) .In this definition , the samples that belong to the set categorized into their target classes are only considered for each class .", "label": "", "metadata": {}, "score": "47.942707"}
{"text": "First , we present a novel coarse - to - fine method in which a grammar 's own hierarchical projections are used for incremental pruning , including a method for efficiently computing projections of a grammar without a treebank .In our experiments , hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy .", "label": "", "metadata": {}, "score": "47.94317"}
{"text": "However , a search for the subset of informative genes presents an additional layer of complexity in the learning process .In depth reviews of feature selection methods in the microarray domain can be found in [ 13 ] .One of the differences among various feature selection procedures is the way they perform the search in the feature space .", "label": "", "metadata": {}, "score": "47.99301"}
{"text": "Applying the trained model uses exactly the same Viterbi algorithm for CRFs as for maximum - entropy classification .It would be useful to compare directly the two approaches for gene and protein identification , although there are already results for similar tagging tasks [ 12 ] .", "label": "", "metadata": {}, "score": "48.003483"}
{"text": "In Bagging , each base classifier is constructed on a bootstrap sample of the original training data , that is , a random sample of instances drawn with replacement and having the same size as the original training data .Ensemble classification is achieved by means of majority voting , where an unlabeled unseen data is assigned the class with the highest number of votes among the individual classifiers ' predictions [ 21 ] .", "label": "", "metadata": {}, "score": "48.007866"}
{"text": "Results and discussion .For evaluating different feature selection methods , one can assess the accuracy of a classifier applied after the feature selection process .Thus , the classification is based only on selected gene expressions .Such an assessment can verify the efficiency of identification of discriminative genes .", "label": "", "metadata": {}, "score": "48.0118"}
{"text": "A Large number of data points or instances in a continuous input - output domain exhibit a significant impact on fuzzy models .It is well known that more training data will not always lead to a better performance for data - driven models .", "label": "", "metadata": {}, "score": "48.022644"}
{"text": "Since the number of fuzzy sets determines the family of realizable approximation functions , larger datasets present the possibility of over - fitting the training data [ 1 , 4 ] .Thus , the effectiveness of the fuzzy models relies on the quality of the training data .", "label": "", "metadata": {}, "score": "48.02796"}
{"text": "The classification accuracies of both methods are higher than the accuracy of random guess among four single - defect classes ( 25 % ) .Next we compare the performance for concurrent defects , using the COMB data .When a multi - class classification is used , there is no way for the classifier to give the correct classification because this class does not exist in the training data and hence the classifier model .", "label": "", "metadata": {}, "score": "48.06461"}
{"text": "The CPSO 1 contains two subswarms that cover the data and features , respectively .In CPSO 2 , we used three subswarms to represent data point ; in the data used here , the number of data is larger than the number of features , so a better balance of the dimensionality of the spaces is achieved .", "label": "", "metadata": {}, "score": "48.071053"}
{"text": "[ 3 ] propose an adaptive filter method based on the decomposition of the probability density function of gene expression means or variances into a mixture of Gaussian components .They determine thresholds to filter genes via tuning the proportion between the pools sizes of removed and retained genes .", "label": "", "metadata": {}, "score": "48.072884"}
{"text": "Some iterative optimization techniques are much faster than others .When training Maximum Entropy models , avoid the use of Generalized Iterative Scaling ( GIS ) or Improved Iterative Scaling ( IIS ) , which are both considerably slower than the Conjugate Gradient ( CG ) and the BFGS optimization methods .", "label": "", "metadata": {}, "score": "48.074013"}
{"text": "In general , all pre - processing should be quantitated and recorded to ensure work is reproducible .Post - processing .After being binarized through any thresholding method , an image may require additional binary operations to make the final pattern useable .", "label": "", "metadata": {}, "score": "48.12992"}
{"text": "First , it provides a better probabilistic model of the data , which can better identify where the data concentrate in .-dimensional space .Second , it can find a not necessarily orthogonal basis , which may reconstruct the data better than PCA in the presence of noise .", "label": "", "metadata": {}, "score": "48.314003"}
{"text": "however , the process of constructing the antecedent and the consequent parts of the fuzzy model is realized using the best subset of input data .In this paper , a comprehensive framework is proposed to construct fuzzy models from the subset of numerical input - output data .", "label": "", "metadata": {}, "score": "48.341743"}
{"text": "Along the way we will study some important machine learning techniques , including decision trees , naive Bayes ' classifiers , and maximum entropy classifiers .We will gloss over the mathematical and statistical underpinnings of these techniques , focusing instead on how and when to use them ( see the Further Readings section for more technical background ) .", "label": "", "metadata": {}, "score": "48.41742"}
{"text": "Only those candidates causing the highest gain are included into the current set of model features .Intuitively , features causing high gain provide strong evidence for many decisions .Thus , feature induction tends to discard infrequent features or non - discriminating features since , independently , their overall effect on the likelihood of the entire training set is usually marginal .", "label": "", "metadata": {}, "score": "48.465195"}
{"text": "The information gain ratio is the default splitting criterion of C4.5 and C5.0 decision trees , which attempts to correct this bias by scaling information gain with the entropy of X j : . C4.5 decision tree starts modeling with the whole training set D , and each time chooses the attribute with maximum information gain ration to split the data set .", "label": "", "metadata": {}, "score": "48.632683"}
{"text": "We therefore adjust our feature extractor to include features for two - letter suffixes : .Rebuilding the classifier with the new feature extractor , we see that the performance on the dev - test dataset improves by almost 2 percentage points ( from 76.5 % to 78.2 % ) : .", "label": "", "metadata": {}, "score": "48.680725"}
{"text": "In other words , f 2 is an exact copy of f 1 , and contains no new information .When the classifier is considering an input , it will include the contribution of both f 1 and f 2 when deciding which label to choose .", "label": "", "metadata": {}, "score": "48.75128"}
{"text": "Assume that the interactions between different defect types are small , the defect types can be formulated as independent non - exclusive classes .As mentioned , the OVA class binarization strategy is adopted on all single - defect classes and samples of the healthy class are added to the relabeled class .", "label": "", "metadata": {}, "score": "48.774117"}
{"text": "That is , the base learners utilized in a robust ensemble classifier should be of high classification accuracy and avoid making coincident misclassification errors which in turn necessitate the diverse learners .Thus , a sample misclassified by a base learner will be corrected by others , so the fused outputs are more accurate than that of the best individual classifier [ 37 ] .", "label": "", "metadata": {}, "score": "48.795"}
{"text": "One solution is to make use of a lexicon , which describes how different words relate to one another .Using WordNet lexicon , augment the movie review document classifier presented in this chapter to use features that generalize the words that appear in a document , making it more likely that they will match words found in the training data .", "label": "", "metadata": {}, "score": "48.819168"}
{"text": "Limited - memory quasi - Newton methods [ 14 ] approximate the Hessian by storing a history of update directions previously taken by the algorithm and combines them linearly with the current gradient to create the new search direction .These methods have been shown to be quite effective [ 11 , 12 ] for training log - linear models like CRFs .", "label": "", "metadata": {}, "score": "48.833534"}
{"text": "So there was no consistent relationship between the classification accuracy and . was the optimum choice to establish a proper balance between the overall classification accuracy and the diversity of the based learners for most examined gene datasets .To find the most promising transformation method and preserve both diversity and accuracy of the base classifiers , we conduct experiments with two well - known transforms , that is , PCA and ICA .", "label": "", "metadata": {}, "score": "48.83396"}
{"text": "9 Further Reading .Many of the machine learning algorithms discussed in this chapter are numerically intensive , and as a result , they will run slowly when coded naively in Python .For information on increasing the efficiency of numerically intensive algorithms in Python , see ( Kiusalaas , 2005 ) .", "label": "", "metadata": {}, "score": "48.84812"}
{"text": "The popular C4.5 decision tree is an example of statistical and data mining methods that do not normally use with binarization , yet produces high diagnostics accuracies with our proposed OVA diagnostic approach .The Data Set .The data set used in this empirical analysis was collected from a machine fault simulator ( MFS ) as shown in Figure 7 .", "label": "", "metadata": {}, "score": "48.86757"}
{"text": "Further combinations of classifiers will be investigated in the future .Abbreviations .SVM : .Support vector machine .CRF : .Conditional random field .NER : .Named entity recognition .POS : .Part - of - speech .", "label": "", "metadata": {}, "score": "48.88571"}
{"text": "To manage this complexity , we translate into target language clusterings of increasing vocabulary size .This approach gives dramatic speed - ups while additionally increasing final translation quality .The intersection of tree transducer - based translation models with n - gram language models results in huge dynamic programs for machine translation decoding .", "label": "", "metadata": {}, "score": "48.915672"}
{"text": "This label bias problem [ 6 ] motivated the development of CRFs , and has been shown to adversely affect accuracy in realistic tagging tasks [ 12 ] .It might be argued that the lower accuracy of maximum entropy taggers is compensated by their faster training , which allows more complex models to be considered , for instance models with higher Markov order or more feature types .", "label": "", "metadata": {}, "score": "49.093742"}
{"text": "Furthermore , we observe that in most cases , the reduced feature spaces exhibit an interesting \" nesting \" property , meaning that the extended feature space constructed subsumes the one formed previously .For example , for the Housing data , we obtain the following subsets of features : .", "label": "", "metadata": {}, "score": "49.188087"}
{"text": "This was primarily due to the availability of MALLET [ 15 ] , which includes effcient implementations of both conditional random fields and feature induction .Once the basic tagger was implemented , the remaining effort focused on testing various spelling , contextual and lexicon features on the development data to improve performance .", "label": "", "metadata": {}, "score": "49.19728"}
{"text": "Multiple fault identification ability : The ability to identify multiple faults , which is difficult due to the interacting nature of most faults .Adaptability : The ability to adapt to changes in external inputs or structural changes .Reasonable storage and computational requirement : Achieve a balance in the trade - off between the computational complexity and system performance .", "label": "", "metadata": {}, "score": "49.22589"}
{"text": "Classification , the most familiar and most popular data mining task , is a mapping from the database to the set of predefined , non - overlapping classes that partition the entire database .SVM is a kernel method which builds a model by constructing a hyperplane that best separates two classes .", "label": "", "metadata": {}, "score": "49.24062"}
{"text": "Analysis of recognition error .Some gene / protein names are compound tokens .The average number of tokens per name was 2.131 tokens in the training data .In the first run , the average number of tokens per name were 1.998 ( TP ) , 2.406 ( FP ) , and 2.298 tokens ( FN ) .", "label": "", "metadata": {}, "score": "49.24595"}
{"text": "In the following section we describe the use of SVM and CRF for assigning a label to each token ( word ) from the discharge summaries .Support vector machines .A Support Vector Machine ( SVM ) is a machine - learning method that is widely used in many NLP tasks such as phrase chunking , Part - of - Speech ( POS ) tagging , and NER .", "label": "", "metadata": {}, "score": "49.255005"}
{"text": "Gene selection is then performed on the training set , and the goodness of selected genes is assessed from the unseen test set [ 31 ] .However , due to the small number of instances in gene microarray datasets , such an approach can lead to unreliable results .", "label": "", "metadata": {}, "score": "49.263107"}
{"text": "Thus , the input will never be assigned this label , regardless of how well the other features fit the label .In particular , just because we have n't seen a feature / label combination occur in the training set , does n't mean it 's impossible for that combination to occur .", "label": "", "metadata": {}, "score": "49.363243"}
{"text": "We employ a diverse feature set containing standard orthographic features combined with expert features in the form of gene and biological term lexicons to achieve a precision of 86.4 % and recall of 78.7 % .An analysis of the contribution of the various features of the model is provided .", "label": "", "metadata": {}, "score": "49.48808"}
{"text": "Computing this sum directly is impractical because the number of possible tag sequences is exponential on training instance length .This is not a problem for maximum entropy models [ 13 ] because they model only single - label decisions so their expectations are just over the next label , not whole label sequences .", "label": "", "metadata": {}, "score": "49.496117"}
{"text": "history.append(tag ) .history.append(tag ) .return zip(sentence , history ) .1.7 Other Methods for Sequence Classification .One shortcoming of this approach is that we commit to every decision that we make .For example , if we decide to label a word as a noun , but later find evidence that it should have been a verb , there 's no way to go back and fix our mistake .", "label": "", "metadata": {}, "score": "49.498985"}
{"text": "You might think of determining a fixed manual threshold and then applying the determined cut - off values to all your images .In this case , you face the decision of choosing one from 256 values .This would at least meet the criterion to treat equally all the images that you need to compare , but will not make you happy in the end .", "label": "", "metadata": {}, "score": "49.627342"}
{"text": "Our new procedure is applied on eleven publicly available gene expression datasets with different characteristics .Then , the prediction models of three different classifiers : Random Forest ; k Nearest Neighbor ; Support Vector Machine are constructed with the selected features .", "label": "", "metadata": {}, "score": "49.63085"}
{"text": "entries ) is used to optimize the subset of features .The particle is decoded as follows .Each substring is processed ( decoded ) separately .The real number entries are ranked .The result is a list of integers viewed as the indexes of the data .", "label": "", "metadata": {}, "score": "49.630917"}
{"text": "During training , we use the annotated tags to provide the appropriate history to the feature extractor , but when tagging new sentences , we generate the history list based on the output of the tagger itself . def pos_features ( sentence , i , history ) : . return features class ConsecutivePosTagger ( nltk .", "label": "", "metadata": {}, "score": "49.68793"}
{"text": "In this paper , we propose a novel strategy to accomplish this by OVA class binarization .The SVM used in this paper is built using sequential minimal optimization ( SMO ) [ 52 ] .Although a SVM model depends on only a subset of the training data , it is well - known for its excellent performance in generalization and the classification of high - dimensional data [ 46 ] .", "label": "", "metadata": {}, "score": "49.765186"}
{"text": "We could then use those interactions to adjust the contributions that individual features make .To make this more precise , we can rewrite the equation used to calculate the likelihood of a label , separating out the contribution made by each feature ( or label ) : .", "label": "", "metadata": {}, "score": "49.789112"}
{"text": "To check how reliable the resulting classifier is , we compute its accuracy on the test set .And once again , we can use show_most_informative_features ( ) to find out which features the classifier found to be most informative . 1.4 Part - of - Speech Tagging .", "label": "", "metadata": {}, "score": "49.796085"}
{"text": "We participated in Task 1A of the BioCreAtIvE competition using the SVM algorithm .We evaluated the effect of several features on SVM learning .We also introduced information from external resources ( gene / protein name databases ) as a feature .", "label": "", "metadata": {}, "score": "49.830643"}
{"text": "import math def entropy ( labels ) : .Once we have calculated the entropy of the original set of input values ' labels , we can determine how much more organized the labels become once we apply the decision stump .", "label": "", "metadata": {}, "score": "49.84766"}
{"text": "expressed over all elements of original instances .The quality of the reduced space is assessed by quantifying the performance of the fuzzy model operating over the original , non - reduced space .The same performance index as used in the construction of the fuzzy model in the reduced space is used to describe the quality of the fuzzy model : .", "label": "", "metadata": {}, "score": "49.94335"}
{"text": "Different types of features for the SVM - based and CRF - based NER systems were investigated .For the SVM - based NER system , we used 1 ) words ; 2 ) POS tags ; 3 ) morphological features ; 4 ) orthographies of words ; 5 ) semantic tags determined by MedEx ; and 6 ) history features .", "label": "", "metadata": {}, "score": "49.96226"}
{"text": "On the other hand , if scores vary widely across the N training sets , then we should probably be skeptical about the accuracy of the evaluation score .4 Decision Trees .In the next three sections , we 'll take a closer look at three machine learning methods that can be used to automatically build classification models : decision trees , naive Bayes classifiers , and Maximum Entropy classifiers .", "label": "", "metadata": {}, "score": "50.063484"}
{"text": "Finally , we conduct a multi - lingual evaluation that demonstrates the robustness of the overall structured neural approach , as well as the benefits of the extensions proposed in this work .Our research further demonstrates the breadth of the applicability of neural network methods to dependency parsing , as well as the ease with which new features can be added to neural parsing models .", "label": "", "metadata": {}, "score": "50.0996"}
{"text": "The corpus data is divided into two sets : the development set , and the test set .The development set is often further subdivided into a training set and a dev - test set .Having divided the corpus into appropriate datasets , we train a model using the training set , and then run it on the dev - test set .", "label": "", "metadata": {}, "score": "50.100906"}
{"text": "However , a necessary and sufficient condition for an ensemble to outperform its individual members is that the base classifiers should be accurate and diverse [ 18 ] .An accurate classifier is one that has an error rate of better than randomly guessing classes for new unseen samples .", "label": "", "metadata": {}, "score": "50.131508"}
{"text": "In addition , the user can put more effort analyzing only the best subset of data that give more impact to the overall prediction .Conclusions .In this paper , we proposed a simple framework for constructing fuzzy modeling from high - dimensional and large data .", "label": "", "metadata": {}, "score": "50.211082"}
{"text": "Consequently , the POS feature selection approach is more able to adapt to different pattern of data and to different classifiers than the other techniques , whose performance is more affected by varying the data characteristics and the used classifier .A method which is more able to minimize the dependency within its selected candidates can reach a particular level of accuracy using a smaller set of genes .", "label": "", "metadata": {}, "score": "50.23146"}
{"text": "The proposed OVA approach helps speed up this process by eliminating the need of data collection of all different types of concurrent faults of a component / structure .Technique Overview .Algorithms in data mining are often grouped by the type of model generated or the most usual way of formulating a problem .", "label": "", "metadata": {}, "score": "50.28939"}
{"text": "All the subswarms share the same basic particles definition illustrated in Figure 4 .Figure 4 : The particle scheme of the \" standard \" PSO ( a ) and cooperative PSO ( b ) .In general , the dimensionality for the data ( instances ) selection is higher than that of the feature selection .", "label": "", "metadata": {}, "score": "50.30842"}
{"text": "We also evaluated the effect of various features ( both individually and combined ) : word , POS , orthography , prefix , suffix , dictionary matching , and preceding class features .In Table 8 , one feature is removed at a time .", "label": "", "metadata": {}, "score": "50.36449"}
{"text": "According to our gene mask definition ( see equation 8 ) they are the samples with 1 bits in the corresponding gene mask .Afterwards , the proportion of the class 's samples to its total sample size has been evaluated .", "label": "", "metadata": {}, "score": "50.434937"}
{"text": "Baralis et al .[ 25 ] have proposed a method that is somewhat similar to our procedure for detecting a minimum subset of genes from microarray data .The main differences are that [ 25 ] use the expression range to define the intervals which are employed for constructing gene masks , and then apply a set\u2010covering approach to obtain the minimum feature subset .", "label": "", "metadata": {}, "score": "50.466866"}
{"text": "Additionally , we consider the features of matching against dictionaries to be external resource features .We investigated and evaluated the effect of these features as well as the effect of tuning the parameters of the SVM algorithm .We found that the dictionary matching features contributed slightly to the improvement in the performance of the f - score .", "label": "", "metadata": {}, "score": "50.508545"}
{"text": "It is therefore essential to develop general approaches and robust methods that are able to overcome the limitation of the small number of training instances and reduce the influence of uncertainties so as to produce reliable classification results .The motivation for this study is to utilize robust ensemble methods that are less sensitive to the selection of genes and are capable of removing the uncertainties of gene expression data .", "label": "", "metadata": {}, "score": "50.528145"}
{"text": "Characterization of genes using training sample masks with their overlapping scores allow the detection of the minimum set of genes that provides the best classification coverage on training samples .A final gene set is then provided by combining the minimum gene subset with the top ranked genes according to the overlapping score .", "label": "", "metadata": {}, "score": "50.599274"}
{"text": "Alternatively , various search schemes have been proposed e.g. , best individual genes [ 9 ] , Max\u2010Relevance and Min\u2010Redundancy based approaches [ 8 ] , Iteratively Sure Independent Screening [ 12 ] and MaskedPainter approach [ 7 ] .A way to improve prediction accuracy , as well as interpretation of the biological relationship between genes and the considered clinical outcomes , is to use a supervised classification based on expressions of discriminative genes identified by an effective gene selection technique .", "label": "", "metadata": {}, "score": "50.61502"}
{"text": "Latent variable grammars take an observed ( coarse ) treebank and induce more fine - grained grammar categories , that are better suited for modeling the syntax of natural languages .Estimation can be done in a generative or a discriminative framework , and results in the best published parsing accuracies over a wide range of syntactically divergent languages and domains .", "label": "", "metadata": {}, "score": "50.628746"}
{"text": "However , if we instead evaluate the classifier on a more balanced corpus , where the most frequent word sense has a frequency of 40 % , then a 95 % accuracy score would be a much more positive result .( A similar issue arises when measuring inter - annotator agreement in 2 . ) 3.3 Precision and Recall .", "label": "", "metadata": {}, "score": "50.628757"}
{"text": "A major challenge is the problem of dimensionality ; tens of thousands of genes ' expressions are observed in a small number , tens to few hundreds , of samples .Given an input of gene expression data along\u2010with samples ' target classes , the problem of gene selection is to find among the entire dimensional space a subspace of genes that best characterizes the response target variable .", "label": "", "metadata": {}, "score": "50.644928"}
{"text": "How do you think that your results might be different if you used a different feature extractor ?What features are relevant in this distinction ?Build a classifier that predicts when each word should be used .However , dialog acts are highly dependent on context , and some sequences of dialog act are much more likely than others .", "label": "", "metadata": {}, "score": "50.657486"}
{"text": "Each binary classifier has one vote and the final output is the class with the maximum number of votes .These parameters have been used in many biomedical NER tasks such as [ 3 , 5 , 7 ] .Conditional random field .", "label": "", "metadata": {}, "score": "50.689537"}
{"text": "A ) System containing no lexicon features and does not use feature induction .B ) Same as A , except feature induction is used .C ) Same as B , except features using the infrequent trigram lexicon are used .", "label": "", "metadata": {}, "score": "50.692924"}
{"text": "Once we 've picked a feature , we can build the decision stump by assigning a label to each leaf based on the most frequent label for the selected examples in the training set ( i.e. , the examples where the selected feature has that value ) .", "label": "", "metadata": {}, "score": "50.716175"}
{"text": "The information gain is then equal to the original entropy minus this new , reduced entropy .The higher the information gain , the better job the decision stump does of dividing the input values into coherent groups , so we can build decision trees by selecting the decision stumps with the highest information gain .", "label": "", "metadata": {}, "score": "50.743862"}
{"text": "Collier et al .[ 6 ] and Shen et al .[ 7 ] investigated the automated recognition of biomedical named entities based on the hidden Markov model .Kazama et al .[ 8 ] , Lee et al .", "label": "", "metadata": {}, "score": "50.770115"}
{"text": "The process aims to discard irrelevant and/or redundant features [ 24 ] .In general , the FS algorithms can be classified into three main categories : filters , wrappers , and embedded methods .The filter method selection criterion is independent of the learning algorithm .", "label": "", "metadata": {}, "score": "50.82669"}
{"text": "Finally , we present multilingual experiments which show that parsing with hierarchical state - splitting is fast and accurate in multiple languages and domains , even without any language - specific tuning .This work describes systems for detecting semantic categories present in news video .", "label": "", "metadata": {}, "score": "50.86475"}
{"text": "The observation list for each token will include a predicate for every regular expression that token matches .Even with this very simple set of predicate - based features , performance on the development data was reasonable ( see Table 2 row A ) .", "label": "", "metadata": {}, "score": "50.868195"}
{"text": "Classification and regression are the most common data mining formulations , where the classes ( categorical in classification and continuous in regression ) are utilized in the modeling ( supervised learning ) .Five of the ten popular algorithms are designed for classification and regression .", "label": "", "metadata": {}, "score": "50.90807"}
{"text": "The gene subset with the highest evaluation is selected as the final set on which to run this particular model .The wrapper methods are computationally expensive since they need a new model to be fitted for each gene subset .Genetic algorithm based feature selection techniques are representative examples for wrapper methods [ 13 ] .", "label": "", "metadata": {}, "score": "50.987495"}
{"text": "This finding demonstrates that the available data come with some evident redundancy , which exhibits a negative impact on the designed model .For the PM10 data , there is a significantly reduced performance of the model when , for a low percentage of data , the number of features starts growing .", "label": "", "metadata": {}, "score": "51.02733"}
{"text": "The first step in creating a classifier is deciding what features of the input are relevant , and how to encode those features .For this example , we 'll start by just looking at the final letter of a given name .", "label": "", "metadata": {}, "score": "51.028442"}
{"text": "These ensemble techniques have the advantage to alleviate the small sample size problem by averaging and incorporating over multiple classification models to reduce the potential for overfitting the training data [ 16 ] .In this way the training data set may be used in a more efficient way , which is critical to many bioinformatics applications with small sample size .", "label": "", "metadata": {}, "score": "51.050404"}
{"text": "To be more specific , in a typical microarray dataset , there are thousands of gene features .Then if RotBoost ensemble classifier is applied to classify such dataset directly , a rotation matrix with thousands of dimensions is required for each tree , which greatly increases the computational complexity .", "label": "", "metadata": {}, "score": "51.12909"}
{"text": "The method scores candidate features f with their log - likelihood gain : . where F is the current set of model features , the log - likelihood of the data using feature set F and the log - likelihood of the data with the model extended with feature f .", "label": "", "metadata": {}, "score": "51.15076"}
{"text": "In other words , a statistically significant difference was seen in those cases .The effects of each feature on our system are shown in Tables 8 and 9 .The parameters are shown in Table 4 .The definition of \" base \" is the same as in Table 7 .", "label": "", "metadata": {}, "score": "51.19757"}
{"text": "Specifying a SVMs classifier requires two parameters , that is , the kernel function and the regularization parameter .Table 4 : Mean classification accuracy of each classification method against 8 different gene datasets .Table 4 summarizes the mean classification accuracy of each classification method on the considered datasets .", "label": "", "metadata": {}, "score": "51.258545"}
{"text": "This ' universal ' treebank is made freely available in order to facilitate research on multilingual dependency parsing .We consider the construction of part - of - speech taggers for resource - poor languages .Recently , manually constructed tag dictionaries from Wiktionary and dictionaries projected via bitext have been used as type constraints to overcome the scarcity of annotated data in this setting .", "label": "", "metadata": {}, "score": "51.263885"}
{"text": "The test set serves in our final evaluation of the system .For reasons discussed below , it is important that we employ a separate dev - test set for error analysis , rather than just using the test set .The division of the corpus data into different subsets is shown in 1.3 .", "label": "", "metadata": {}, "score": "51.34447"}
{"text": "Combining multiple grammars that were self - trained on disjoint sets of unlabeled data results in a final test accuracy of 92.5\\% on the WSJ test set and 89.6\\% on our Broadcast News test set .This work shows how to improve state - of - the - art monolingual natural language processing models using unannotated bilingual text .", "label": "", "metadata": {}, "score": "51.414433"}
{"text": "The predictors used in the OVA formulation are the same as that in the multi - class classification .The 18 features are fed into three SVMs that model binary decisions of whether each of them belongs to each single - defect , as in Table 2 .", "label": "", "metadata": {}, "score": "51.429184"}
{"text": "The first column shows the values when only the word and preceding class features were used in the SVM learning .The other columns shows the values when the word and preceding class features plus one other feature were used in the learning .", "label": "", "metadata": {}, "score": "51.435387"}
{"text": "Table 2 also shows the performance of the system without lexicons and feature induction .An examination of system errors on the development data shows that a primary source of error came from properly labeled mentions that are off by one or more tokens .", "label": "", "metadata": {}, "score": "51.484127"}
{"text": "The resulting grammars are extremely compact com- pared to other high - performance parsers , yet the parser gives the best published accuracies on several languages , as well as the best generative parsing numbers in English .In addi- tion , we give an associated coarse - to - fine inference scheme which vastly improves inference time with no loss in test set accuracy .", "label": "", "metadata": {}, "score": "51.503304"}
{"text": "Having captured the spot intensities , the obtained intensities undergo a normalization preprocessing stage to remove systematic errors within the data [ 2 ] .Early application of microarrays to the study of human disease conditions rapidly revealed their potential as a medical diagnostic tool [ 3 , 4 ] .", "label": "", "metadata": {}, "score": "51.50619"}
{"text": "This is obviously also biased , with a good training ( potentially by different experts ) the feature extraction contains a lower bias , since the same trained classifier is applied to the different images .Relevant ImageJ plugins available are the SIOX : Simple Interactive Object Extraction and the Trainable WEKA Segmentation .", "label": "", "metadata": {}, "score": "51.521545"}
{"text": "Clustering - based methods , where the gray - level samples are clustered in two parts as background and foreground ( object ) , or alternately are modelled as a mixture of two Gaussians .Entropy - based methods result in algorithms that use the entropy of the foreground and background regions , the cross - entropy between the original and binarized image , etc . .", "label": "", "metadata": {}, "score": "51.528435"}
{"text": "SVM is a kernel method first introduced in [ 43 ] , which builds a model by constructing a hyperplane that best separates two classes .A detail review of the algorithms , including the SVM for classification and the support vector regression ( SVR ) for regression can be found in [ 44 ] .", "label": "", "metadata": {}, "score": "51.53859"}
{"text": "This paper is a study to propose ways of reducing the number of scenarios for data collection , on different combination of defect types , while maintaining or even improving the diagnostic performances .Two popular statistical and data mining methods , namely SVM and C4.5 , are used in this study .", "label": "", "metadata": {}, "score": "51.55931"}
{"text": "The annotations are produced automatically with statistical models that are specifically adapted to historical text .The corpus will facilitate the study of linguistic trends , especially those related to the evolution of syntax .Syntactic analysis of search queries is important for a variety of information- retrieval tasks ; however , the lack of annotated data makes training query analysis models difficult .", "label": "", "metadata": {}, "score": "51.559654"}
{"text": "POS approach for binary class problems .Each sample is also characterized by a target class label , y j , representing the phenotype of the tissue sample being studied .Let be the vector of class labels such that its j th element , y j , has a single value c which is either 1 or 2 .", "label": "", "metadata": {}, "score": "51.562218"}
{"text": "CPSO 2 : swarms located in the instance ( data ) space .The number of subswarms used in the optimization method is also shown in Table 3 .The PSO method comprises only a single swarm whose individuals concatenate features and instances .", "label": "", "metadata": {}, "score": "51.63759"}
{"text": "We apply POS , along\u2010with four widely used gene selection methods , to several benchmark gene expression datasets .The experimental results of classification error rates computed using the Random Forest , k Nearest Neighbor and Support Vector Machine classifiers show that POS achieves a better performance .", "label": "", "metadata": {}, "score": "51.64885"}
{"text": "Otherwise , your evaluation results may be unrealistically optimistic .Decision trees are automatically constructed tree - structured flowcharts that are used to assign labels to input values based on their features .Although they 're easy to interpret , they are not very good at handling cases where feature values interact in determining the proper label .", "label": "", "metadata": {}, "score": "51.665485"}
{"text": "Thus , darker areas in an image might be extractable comparably as well as very bright areas , but this would not be possible with a global threshold , where specifically very dark areas shift into the background or might be under - extracted .", "label": "", "metadata": {}, "score": "51.70979"}
{"text": "The second phase of the process , which concerns the regular production use of the process , involves the fault detection ; isolation ; diagnosis ; prognosis and remaining useful life prediction ; and maintenance decisions .During the transition period before samples of all fault types are collected , the fault detection and isolation can still be performed .", "label": "", "metadata": {}, "score": "51.801704"}
{"text": "We report on the features that we used in our system to recognize gene / protein names .Training data and test data .Training data ( 7500 sentences ) and development test data ( 2500 sentences ) were prepared for system development in Task 1 .", "label": "", "metadata": {}, "score": "51.851295"}
{"text": "It has been defined to provide higher scores for longer overlapping intervals .Genes are then ranked in ascending order according to their scores .This simplified measure has been extended by Apiletti et al .[ 7 ] using another factor , i.e. the number of overlapped samples , in the analysis .", "label": "", "metadata": {}, "score": "51.92006"}
{"text": "The resulting likelihood score can be thought of as an estimate of the probability that a randomly selected value from the training set would have both the given label and the set of features , assuming that the feature probabilities are all independent .", "label": "", "metadata": {}, "score": "51.92804"}
{"text": "For example , consider the part - of - speech tagging task .At one extreme , we could create the training set and test set by randomly assigning sentences from a data source that reflects a single genre ( news ) : .", "label": "", "metadata": {}, "score": "51.989067"}
{"text": "The application of the OVO binarization strategy splits the original problem into .classifiers each with two classes , i.e. , C1 versus C2 ; C2 versus C3 ; C3 versus C4 .These class binarization strategies can also be useful in other situations .", "label": "", "metadata": {}, "score": "52.0316"}
{"text": "For example , in multi - class classification , each instance may be assigned multiple labels ; in open - class classification , the set of labels is not defined in advance ; and in sequence classification , a list of inputs are jointly classified .", "label": "", "metadata": {}, "score": "52.03164"}
{"text": "Figure 2 shows the potential effect of expression outliers on extending the underlying intervals , if the range of training expressions are considered .Based on the defined core intervals , we present the following definitions : Non\u2010outlier samples set , , for gene i is defined as the set of samples whose expression values fall inside their own target classes core interval .", "label": "", "metadata": {}, "score": "52.076912"}
{"text": "However , conditional models can not be used to answer the remaining questions 3 - 6 .However , this additional power comes at a price .Because the model is more powerful , it has more \" free parameters \" which need to be learned .", "label": "", "metadata": {}, "score": "52.127045"}
{"text": "This allows a wide range of classification techniques to be applied to the prognostics problem .Diagnosis with a regression method [ 42 ] is also computationally possible , though there is no physical meaning to a decimal regression estimate in between different defect types .", "label": "", "metadata": {}, "score": "52.12915"}
{"text": "We measured Precision , Recall , and F - score metrics using the standard CoNLL evaluation script ( the Perl program conlleval.pl ) [ 28 ] .For the whole system , we used micro - averaging of the Precision , Recall , and F - score .", "label": "", "metadata": {}, "score": "52.18119"}
{"text": "They reported that using the POS feature greatly increased the f - score .We trained a tagger using 500 abstracts from GENIA corpus 3.02p .After the training , the accuracy of the POS tagging increased from 79.95 to 92.81 % on the GENIA corpus .", "label": "", "metadata": {}, "score": "52.188118"}
{"text": "Based on this feature extractor , we can create a list of labeled featuresets by selecting all the punctuation tokens , and tagging whether they are boundary tokens or not : . ? ! ' ] Using these featuresets , we can train and evaluate a punctuation classifier : .", "label": "", "metadata": {}, "score": "52.196053"}
{"text": "Transformation Methods .As it was already mentioned , the purpose of rotation - based ensemble classifiers such as Rotation Forest and RotBoost is to increase the individual classifier performance and the diversity within the ensemble .Thus , a full feature set is obtained with all the transformed features for each considered tree in the ensemble .", "label": "", "metadata": {}, "score": "52.199684"}
{"text": "The contribution from each feature is then combined with this prior probability , to arrive at a likelihood estimate for each label .The label whose likelihood estimate is the highest is then assigned to the input value .5.1 illustrates this process .", "label": "", "metadata": {}, "score": "52.214035"}
{"text": "The authors are not aware of any work in the literature on the application of binarization strategies to the specific context of machinery fault diagnostics for our purpose .In this paper , the OVA class binarization strategy is adopted on all single - defect classes and samples of the healthy class are added to the relabeled class , to predict concurrent defects from normal and single - defect training data .", "label": "", "metadata": {}, "score": "52.220963"}
{"text": "In this definition , we assume that the j th input token is represented by a set o j of predicates that hold of the token or its neighborhood in the input sequence .Each feature function f i specifies an association between the predicates that hold at a position and the state for that position , and the feature weight \u03bb i specifies whether that association should be favored or disfavored .", "label": "", "metadata": {}, "score": "52.23557"}
{"text": "So what happens when we ignore the independence assumption , and use the naive Bayes classifier with features that are not independent ?One problem that arises is that the classifier can end up \" double - counting \" the effect of highly correlated features , pushing the classifier closer to a given label than is justified .", "label": "", "metadata": {}, "score": "52.255196"}
{"text": "The results are shown in Table 3 .Entities were correctly identified by the system if and only if all and only the tokens of the entity were correctly detected .Table 3 .Precision and recall numbers for the system on the unseen evaluation data .", "label": "", "metadata": {}, "score": "52.289314"}
{"text": "In order to identify the selected data in each decomposed group , we use the information granules ( membership degrees ) values to identify the index of the instances in each group .Here , we employ a winner - takes - all scheme to determine a single group for each granule , that is , the index of the instances in each of the decomposition group related to the information granule that gets the highest degrees of activation .", "label": "", "metadata": {}, "score": "52.301075"}
{"text": "Third , we develop a flexible setup to cope with the optimization of variables and data to be used in the design of the fuzzy model .The proposed approach allows the user to choose the predetermined fraction of variables and data that can be used to construct the fuzzy models .", "label": "", "metadata": {}, "score": "52.3173"}
{"text": "Here , we can see that the classifier begins by checking whether a word ends with a comma - if so , then it will receive the special tag \" , \" .Next , the classifier checks if the word ends in \" the \" , in which case it 's almost certainly a determiner .", "label": "", "metadata": {}, "score": "52.344254"}
{"text": "To generate a labeled input , the model first chooses a label for the input , then it generates each of the input 's features based on that label .Every feature is assumed to be entirely independent of every other feature , given the label .", "label": "", "metadata": {}, "score": "52.367256"}
{"text": "5.3 Non - Binary Features .We have assumed here that each feature is binary , i.e. that each input either has a feature or does not .Label - valued features ( e.g. , a color feature which could be red , green , blue , white , or orange ) can be converted to binary features by replacing them with binary features such as \" color - is - red \" .", "label": "", "metadata": {}, "score": "52.514076"}
{"text": "A mixture grammar fit with the EM algorithm shows improvement over a single PCFG , both in parsing accuracy and in test data likelihood .We argue that this improvement comes from the learning of specialized grammars that capture non - local correlations .", "label": "", "metadata": {}, "score": "52.54782"}
{"text": "But by the time the decision tree learner has descended far enough to use these features , there is not enough training data left to reliably determine what effect they should have .If we could instead look at the effect of these features across the entire training set , then we might be able to make some conclusions about how they should affect the choice of label .", "label": "", "metadata": {}, "score": "52.549496"}
{"text": "This is the approach taken by Hidden Markov Models .Hidden Markov Models are similar to consecutive classifiers in that they look at both the inputs and the history of predicted tags .However , rather than simply finding the single best tag for a given word , they generate a probability distribution over tags .", "label": "", "metadata": {}, "score": "52.580482"}
{"text": "Classifiers can help us to understand the linguistic patterns that occur in natural language , by allowing us to create explicit models that capture those patterns .Typically , these models are using supervised classification techniques , but it is also possible to build analytically motivated models .", "label": "", "metadata": {}, "score": "52.633305"}
{"text": "We learn the neural network representation using a gold corpus augmented by a large number of automatically parsed sentences .Given this fixed network representation , we learn a final layer using the structured perceptron with beam - search decoding .On the Penn Treebank , our parser reaches 94.26 % unlabeled and 92.41 % labeled attachment accuracy , which to our knowledge is the best accuracy on Stanford Dependencies to date .", "label": "", "metadata": {}, "score": "52.677494"}
{"text": "For example , the Expected Likelihood Estimation for the probability of a feature given a label basically adds 0.5 to each count(f , label ) value , and the Heldout Estimation uses a heldout corpus to calculate the relationship between feature frequencies and feature probabilities .", "label": "", "metadata": {}, "score": "52.76518"}
{"text": "The authors are not aware of any work in the literature that studies this practical problem .A strategy based on one- versus -all ( OVA ) class binarization is proposed to improve fault diagnostics accuracy while reducing the number of scenarios for data collection , by predicting concurrent defects from training data of normal and single defects .", "label": "", "metadata": {}, "score": "52.842728"}
{"text": "But the principle itself introduces bias .To understand negative consequences of this sort of bias , imagine a scenario in pathology , in which automated segmentation is developed to yield a certain result based on current expert pathologist experience , yet completely eliminates a cell feature that is later learned to be a critical indicator .", "label": "", "metadata": {}, "score": "52.855576"}
{"text": "There are two broad categories for feature selection algorithms , filter model or wrapper [ 28 ] .The filter model relies on general characteristics of the training data to choose best features without involving any learning algorithm .The wrapper models , on the contrary , depends on feature addition or deletion to compose subset features and uses an evaluation function with a predetermined learning algorithm to estimate the subset features .", "label": "", "metadata": {}, "score": "52.871513"}
{"text": "Each row provides the average classification error rate at a specific number of selected genes ( reported in the first column ) .( PDF 14 KB ) .12859_2014_6543_MOESM3_ESM.pdf Additional file 3 : Classification error rates obtained by Support Vector Machine Classifier .", "label": "", "metadata": {}, "score": "52.884872"}
{"text": "Note that our definition for gene masks allows to report in advance which samples should be covered by the minimum subset of genes .Therefore , there would be at least one gene mask which has at least one bit set to 1 if that condition is to hold .", "label": "", "metadata": {}, "score": "52.913757"}
{"text": "As the word already suggests , with a binarization you divide your image in two parts usually referred to as foreground and background .This is the simplest method of image segmentation .Simple does not refer to easily achieving good results it is just often perceived by the user as being easier .", "label": "", "metadata": {}, "score": "52.975677"}
{"text": "Data mining , first appeared as knowledge discovery in databases [ 15 ] , and emerged from a diverse background of databases , statistics and machine learning [ 16 , 17 ] .Another related research field is pattern recognition , emerged from engineering disciplines , which can be viewed as another facet of the same field as machine learning , emerged from computer science [ 18 ] .", "label": "", "metadata": {}, "score": "52.97998"}
{"text": "Microarray technology , as well as other high\u2010throughput functional genomics experiments , have become a fundamental tool for gene expression analysis in recent years .For a particular classification task , microarray data are inherently noisy since most genes are irrelevant and uninformative to the given classes ( phenotypes ) .", "label": "", "metadata": {}, "score": "52.99373"}
{"text": "Sensors 2013 , 13 , 12663 - 12686 .[ Google Scholar ] .Quinlan , J. Induction of decision trees .Mach .Learn .[ Google Scholar ] .Shannon , C.E. A mathematical theory of communication .Bell Syst .", "label": "", "metadata": {}, "score": "52.997868"}
{"text": "Precision is measured by the fraction of predicted gene mentions that are correct and recall by the fraction of actual gene mentions that were identified .Two system results are provided .The first is for the system that contains only features extracted from the training data .", "label": "", "metadata": {}, "score": "53.00386"}
{"text": "However , just like any methods , SVM methods may not be the most accurate algorithm for all situations .In particular , it does not naturally handle multiple classes or generate proper probability estimates .In the formal statistical sense , the SVM do not belong to techniques of the ' Bayesian ' approach .", "label": "", "metadata": {}, "score": "53.04676"}
{"text": "Natural language processing .Declarations .Acknowledgments .De - identified clinical records used in this research were provided by the i2b2 National Center for Biomedical Computing , funded by U54LM008748 , and were originally prepared for the Shared Tasks for Challenges in NLP for Clinical Data organized by Dr. \u00d6zlem Uzuner , i2b2 and SUNY .", "label": "", "metadata": {}, "score": "53.08593"}
{"text": "We used the features and parameters shown in Tables 1 and 4 .A statistically significant difference is not seen .We also show the case in which the stop words were not ignored in the dictionary matching .A statistically significant difference was seen in this case , so stop words should be ignored in dictionary matching .", "label": "", "metadata": {}, "score": "53.093536"}
{"text": "A classifier can then use these selected genes to enhance its classification performance and prediction accuracy .A procedure specifically designed to select genes based on their overlapping degree across different classes was recently proposed [ 6 ] .This procedure , named Painter 's feature selection method , proposes a simplified version of a measure calculating an overlapping score for each gene .", "label": "", "metadata": {}, "score": "53.099876"}
{"text": "For example , \" translocation \" , \" of \" , \" the \" , \" NF - kappaB \" , \" transcription \" , \" factor \" and \" , \" are the tokens in above sample phrase .BIO representation .", "label": "", "metadata": {}, "score": "53.139236"}
{"text": "It contains one leaf for each possible feature value , specifying the class label that should be assigned to inputs whose features have that value .In order to build a decision stump , we must first decide which feature should be used .", "label": "", "metadata": {}, "score": "53.14962"}
{"text": "A novel gene selection method , POS , is proposed .POS analyzes the expressions overlap across classes taking into account the proportions of overlapping samples .It robustly defines a mask for each gene that allows it to minimize the effect of expression outliers .", "label": "", "metadata": {}, "score": "53.174488"}
{"text": "Participating teams developed their systems based on the training set , and they were allowed to annotate additional notes .251 notes were randomly picked by the organizers as the test data set and were annotated by participating teams , as well as the organizers , and they served as the gold standard for evaluating the performance of systems [ 9 , 15 ] .", "label": "", "metadata": {}, "score": "53.19484"}
{"text": "we built a regular expression tagger that chooses a part - of - speech tag for a word by looking at the internal make - up of the word .However , this regular expression tagger had to be hand - crafted .", "label": "", "metadata": {}, "score": "53.21289"}
{"text": "Thus , gene masks can represent the capability of genes to classify correctly each sample , i.e. it represents a gene 's classification power .For a particular gene i , element j of its mask is set to 1 if the corresponding expression value x i j belongs only to core expression interval of the single class c j , i.e. if sample j is a member of the set .", "label": "", "metadata": {}, "score": "53.229286"}
{"text": "In general , simple classifiers always treat each input as independent from all other inputs .In many contexts , this makes perfect sense .For example , decisions about whether names tend to be male or female can be made on a case - by - case basis .", "label": "", "metadata": {}, "score": "53.238087"}
{"text": "The \" full(A ) \" and \" full(B ) \" columns show the cases using the original and GENIA trained POS tagger , respectively .The difference between the \" full \" cases was small , but like with \" word+pc .", "label": "", "metadata": {}, "score": "53.256443"}
{"text": "The best performing technique among the compared methods is not always the same for neither all selected gene set sizes , all datasets nor all classifiers .Hence , the POS algorithm could keep its better performance for large as well as small sets of selected genes with Random Forest and Support Vector Machine classifiers on individual datasets .", "label": "", "metadata": {}, "score": "53.27105"}
{"text": "The returned dictionary , known as a feature set , maps from feature names to their values .Feature names are case - sensitive strings that typically provide a short human - readable description of the feature , as in the example ' last_letter ' .", "label": "", "metadata": {}, "score": "53.273254"}
{"text": "We can systematically evaluate the classifier on a much larger quantity of unseen data : .Finally , we can examine the classifier to determine which features it found most effective for distinguishing the names ' genders : .This listing shows that the names in the training set that end in \" a \" are female 33 times more often than they are male , but names that end in \" k \" are male 32 times more often than they are female .", "label": "", "metadata": {}, "score": "53.273262"}
{"text": "Workshop on Natural Language Processing in the Biomedical Domain 2002 , 1 - 8 .View Article .Genome Biol 2008 , 9 ( Suppl 2 ) : S2 .PubMed View Article .Takeuchi K , Collier N : Bio - medical entity extraction using Support Vector Machines .", "label": "", "metadata": {}, "score": "53.281094"}
{"text": "These numbers represent how light or some other type of signal entered the instrument we are using and interacted with its environment and eventually triggered sensors in the instrument that further processed the information into a digital output .For example , when you record an image using a light gathering device such as a confocal microscope , the values you get at a certain coordinate or pixel are not color values , but relate to photon counts .", "label": "", "metadata": {}, "score": "53.29842"}
{"text": "The difference between using and not using semantic tags was significant according to the approximate randomization test described above .Similar results ( not shown ) were observed for the CRF - based system .Table 4 .Performance of the SVM - based system for different feature combinations in 10-fold cross - validation .", "label": "", "metadata": {}, "score": "53.329754"}
{"text": "In this article , we propose a new gene selection method , called POS , that can be described as follows : .POS utilizes the interquartile range approach to robustly detect the minimum subset of genes that maximizes the correct assignment of training samples to their corresponding classes i.e. , the minimum subset that can yield the best classification accuracy on a training set avoiding the effects of outliers .", "label": "", "metadata": {}, "score": "53.349552"}
{"text": "Training Bias .Moreover , an equally momentous problem is that segmentation methods are generally inherently biased in that they are trained in the first place against an initial human perception of what the final information extracted from an image should be .", "label": "", "metadata": {}, "score": "53.352867"}
{"text": "Syst .Signal Process .[ Google Scholar ] .F\u00fcrnkranz , J. Round robin classification .J. Mach .Learn .Res .[ Google Scholar ] .Lorena , A.C. ; de Carvalho , A.C. ; Gama , J.M. A review on the combination of binary classifiers in multiclass problems .", "label": "", "metadata": {}, "score": "53.357334"}
{"text": "It is certainly a goal of digital image processing to establish protocols that eliminate variability , but currently a lofty goal , and even the most diligent of efforts to replicate circumstances generally fail to produce sets of images that can be thresholded with one fixed value .", "label": "", "metadata": {}, "score": "53.382618"}
{"text": "We describe experiments on learning latent variable grammars for various German treebanks , using a language - agnostic statistical approach .In our method , a minimal initial grammar is hierarchically refined using an adaptive split - and - merge EM procedure , giving compact , accurate grammars .", "label": "", "metadata": {}, "score": "53.390846"}
{"text": "Proceedings of ACL , workshop on Natural Language Processing in Biomedicine 2003 , 65 - 72 .View Article .Zhou G , Shen D , Zhang J , Su J , Tan S : Recognition of protein / gene names from text using an ensemble of classifiers .", "label": "", "metadata": {}, "score": "53.397766"}
{"text": "Abbasion et al .[47 ] discussed multi - fault diagnosis of normal and three fault types in two different bearings at different locations of a system .Sugumaran et al .[ 48 ] discussed the use of a multi - class SVM built from one - class SVMs to reduce the computational efforts required by the traditional binary SVM .", "label": "", "metadata": {}, "score": "53.410553"}
{"text": "Wrapper methods evaluate gene subsets using a predictive model which is run on the dataset partitioned into training and testing sets .Each gene subset is used with training dataset to train the model , which is then tested on the test set .", "label": "", "metadata": {}, "score": "53.423847"}
{"text": "We used the MALLET [ 15 ] implementation of CRFs and limited - memory quasi - Newton training .In addition , we relied on MALLET 's feature induction capability [ 16 ] , which is discussed in the next section .", "label": "", "metadata": {}, "score": "53.47452"}
{"text": "Although it 's often possible to get decent performance by using a fairly simple and obvious set of features , there are usually significant gains to be had by using carefully constructed features based on a thorough understanding of the task at hand .", "label": "", "metadata": {}, "score": "53.514908"}
{"text": "Spatial resolution can always be downsampled after the fact - but never upsampled .Furthermore , if your objects of interest are described by too few pixels , the error of many statistical computations will be prohibitively high , and some forms of analyses will not be possible at all .", "label": "", "metadata": {}, "score": "53.551193"}
{"text": "As was mentioned before , there are several methods for identifying the most informative feature for a decision stump .One popular alternative , called information gain , measures how much more organized the input values become when we divide them up using a given feature .", "label": "", "metadata": {}, "score": "53.556076"}
{"text": "For all data , the CPSO performed better than the standard PSO .Although both algorithms show the same tendency when the percentage of feature is 100 % however , the RMSE produced by the CPSO is lower than the one obtained when running the PSO .", "label": "", "metadata": {}, "score": "53.58112"}
{"text": "Each method obtains its particular minimum at different size of selected gene set .Tables 4 , 5 and 6 summarizes these results for RF , k NN and SVM classifiers respectively .Each row shows the minimum error rate ( along\u2010with its corresponding size , shown in brackets ) obtained by all methods for a specific dataset , reported in the first column .", "label": "", "metadata": {}, "score": "53.58832"}
{"text": "View at Google Scholar .J. R. Cano , F. Herrera , and M. Lozano , \" Using evolutionary algorithms as instance selection for data reduction in KDD : an experimental study , \" IEEE Transactions on Evolutionary Computation , vol .", "label": "", "metadata": {}, "score": "53.630142"}
{"text": "State - of - the - art natural language processing models are anything but compact .Syntactic parsers have huge grammars , machine translation systems have huge transfer tables , and so on across a range of tasks .With such complexity come two challenges .", "label": "", "metadata": {}, "score": "53.63318"}
{"text": "Simple and easy to implement , majority voting provides an effective way to combine classifiers and has been shown to deliver good performance in a number of applications , for example , in recognizing protein / gene names from research articles [ 8 ] .", "label": "", "metadata": {}, "score": "53.655228"}
{"text": "The PSO - Based Representation of the Search Space .The reduction of the data and feature spaces involves a selection of a subset of the data and a subset of the features .Therefore , the problem is combinatorial in its nature .", "label": "", "metadata": {}, "score": "53.65689"}
{"text": "Because of the impact of different uncertainties together with the lack of labeled training samples , the conventional machine learning techniques face complicated challenges to develop reliable classification models .Quite often selecting only a few genes can discriminate a majority of training instances correctly [ 14 ] .", "label": "", "metadata": {}, "score": "53.682102"}
{"text": "As mentioned above , we used the Brill tagger [ 15 ] for POS tagging .The tagger was trained on newswire articles .Shen et al .[ 7 ] suggested that a POS tagger should be trained using this domain .", "label": "", "metadata": {}, "score": "53.70276"}
{"text": "they are fast ( no fiddling ) and can be automated ( e.g. , in macros , plugins , batch jobs , etc . ) .In ImageJ and Fiji , there are so far 16 Global Auto Thresholds and 9 Auto Local Thresholds implemented ( by Gabriel Landini ) .", "label": "", "metadata": {}, "score": "53.70868"}
{"text": "For all tables , \" ALL \" means the set of all named entities with values corresponding to micro - averaged Precision / Recall / F - score .Table 4 shows that the best F - score achieved was 90.54 % when using all features for the SVM - based NER system .", "label": "", "metadata": {}, "score": "53.74576"}
{"text": "Information extraction from biomedical text has attracted increasing research interest over the past few years .Several large scale annotated corpora have been developed [ 1 ] or are being developed [ 2 ] to facilitate this process .The first step in most information extraction systems is to identify the named entities that are relevant to the concepts , relations and events described in the text .", "label": "", "metadata": {}, "score": "53.81298"}
{"text": "Feature set .Feature - based models like CRFs are attractive because they reduce each problem to that of finding a feature set that adequately represents the task at hand .These predicates help the system recognize informative substrings ( e.g. ' homeo ' or ' ase ' ) in words that were not seen in training .", "label": "", "metadata": {}, "score": "53.81636"}
{"text": "This may lead to inconsistent results even though it might visually look good in the first place .Specific tools based on machine learning might be helpful .Here , the user is required to do some training on representative example images .", "label": "", "metadata": {}, "score": "53.860657"}
{"text": "J R Stat Soc : Series B ( Stat Methodol ) 2008 , 70 ( 5 ) : 849 - 911 .View Article .M\u00fcssel C , Lausser L , Maucher M , Kestler HA : Multi\u2010objective parameter selection for classifiers .", "label": "", "metadata": {}, "score": "53.86236"}
{"text": "For example , decision trees can be very effective at capturing phylogeny trees .However , decision trees also have a few disadvantages .One problem is that , since each branch in the decision tree splits the training data , the amount of training data available to train nodes lower in the tree can become quite small .", "label": "", "metadata": {}, "score": "53.867455"}
{"text": "The same tendency is noticeable for the other data sets .In this case , one would be better off to consider a suitable reduced set of features .In all cases experimented with , we noted an optimal combination of features and data that led to the best performance of the model .", "label": "", "metadata": {}, "score": "53.872227"}
{"text": "We performed a 10-fold cross validation using 90 % of the combined data set as training data and the remainder as test data .This combined data set was also used in the analyses shown in Tables 8 , 9 , 10 , 11 .", "label": "", "metadata": {}, "score": "53.880096"}
{"text": "They discussed in detail some data mining methods such as ANN , fuzzy sets and fuzzy logic , statistical learning theory and kernel methods such as SVM and relevance vector machine ( RVM ) , simulated annealing , GA , Gaussian process , graphical methods and deep belief networks .", "label": "", "metadata": {}, "score": "53.904995"}
{"text": "x i is the feature vector of the i - th sample and has a dimension of n .k is the total number of vectors .The y i is the class of the vector ; it is either a positive example ( +1 ) or negative example ( -1 ) .", "label": "", "metadata": {}, "score": "53.98356"}
{"text": "All datasets are publicly available , see section ' Availability of supporting data ' .Fifty repetitions of 10\u2010fold cross validation analysis were performed for each combination of dataset , feature selection algorithm , and a given number of selected genes , up to 50 , with the considered classifiers .", "label": "", "metadata": {}, "score": "53.9897"}
{"text": "On average across all considered datasets , POS approach improves the best performance of the compared methods by up to 26 % of the misclassification error rates achieved using their selections at different set sizes .The stability of the selections yielded by the compared feature selection methods using the cross validation technique has been highlighted .", "label": "", "metadata": {}, "score": "54.0429"}
{"text": "The experiment results showed that the proposed fuzzy modeling framework is able to handle high dimensionality and a large data set simultaneously .Moreover , the curse of dimensionality problem in fuzzy modeling was substantially reduced .In the future work one could concentrate on improving the cooperative PSO by fine - tuning the parameters of the method such as , for example , the cognitive and social parameter .", "label": "", "metadata": {}, "score": "54.05732"}
{"text": "The embedded method incorporates feature selection as part of the training process .The reader can refer to [ 23 - 25 ] for more details .Instances selection ( IS ) , another category of reduction approaches , is concerned with the selection of the relevant data ( instances ) reflective of the knowledge pertinent to the problem at hand [ 26 , 27 ] .", "label": "", "metadata": {}, "score": "54.080635"}
{"text": "Meanwhile , Graphics Processor Units ( GPUs ) have become widely available , offering the opportunity to alleviate this bottleneck by exploiting the fine - grained data parallelism found in the CKY algorithm .In this paper , we explore the design space of parallelizing the dynamic programming computations carried out by the CKY algorithm .", "label": "", "metadata": {}, "score": "54.101074"}
{"text": "Selecting minimum subset of genes .Selecting a minimum subset of genes is one of the POS method stages in which the information provided by the constructed gene masks and the POS scores are analyzed .This subset is designated to be the minimum one that correctly classify the maximum number of samples in a given training set , avoiding the effects of expression outliers .", "label": "", "metadata": {}, "score": "54.155983"}
{"text": "Following the branch that describes our input value , we arrive at a new decision node , with a new condition on the input value 's features .We continue following the branch selected by each node 's condition , until we arrive at a leaf node which provides a label for the input value .", "label": "", "metadata": {}, "score": "54.17934"}
{"text": "We begin by selecting the overall best decision stump for the classification task .We then check the accuracy of each of the leaves on the training set .Leaves that do not achieve sufficient accuracy are then replaced by new decision stumps , trained on the subset of the training corpus that is selected by the path to the leaf .", "label": "", "metadata": {}, "score": "54.23691"}
{"text": "An example of a ' Bayesian ' technique is the relevance vector machine ( RVM ) [ 53 ] . C4.5 Decision Tree .C4.5 decision tree is the most popular method in the ID3 decision tree family .The ID3 decision tree [ 54 ] family chooses attributes on the basis of information gain ( reduction in entropy ) .", "label": "", "metadata": {}, "score": "54.244095"}
{"text": "Pre - processing .Since the basis of any cut - off value during thresholding is pixel values , any change in those after image acquisition will also influence the final binarization result .Thus , the user needs to apply wisely methods that adjust brightness and contrast or any image filters , to not degrade but instead enhance the features of interest , fostering the binarization process .", "label": "", "metadata": {}, "score": "54.251633"}
{"text": "All procedures described in this manuscript have been programmed into an R package named ' propOverlap ' .Conclusion .The idea of selecting genes based on analysing the overlap of their expressions across two phenotypes , taking into account the proportions of overlapping samples , is considered in this article .", "label": "", "metadata": {}, "score": "54.251987"}
{"text": "Across various hierarchical encoding schemes and for multiple language pairs , we show speed - ups of up to 50 times over single - pass decoding while improving BLEU score .Moreover , our entire decoding cascade for trigram language models is faster than the corresponding bigram pass alone of a bigram - to - trigram decoder .", "label": "", "metadata": {}, "score": "54.26631"}
{"text": "Some popular techniques in the signal processing approach for bearing fault diagnostics include fast Fourier transform ( FFT ) , wavelet methods , spectral kurtosis ( SK ) , amplitude demodulation by envelope analysis or Hilbert transform , described in papers such as [ 12 - 14 ] .", "label": "", "metadata": {}, "score": "54.296963"}
{"text": "Apiletti et al .[ 7 ] demonstrate that the MaskedPainter method has outperformed many widely used gene selection methods available in [ 9 ] .The mRMR technique , proposed in [ 18 ] , is intensively used in microarray data analysis e.g. , [ 19 , 37 ] .", "label": "", "metadata": {}, "score": "54.30641"}
{"text": "Let 's begin by finding out what the most common suffixes are : .Next , we 'll define a feature extractor function which checks a given word for these suffixes : . endswith(suffix ) ... return features .Feature extraction functions behave like tinted glasses , highlighting some of the properties ( colors ) in our data and making it impossible to see other properties .", "label": "", "metadata": {}, "score": "54.314392"}
{"text": "Xue , H. ; Yang , Q. ; Chen , S. SVM : Support Vector Machines .In The Top Ten Algorithms in Data Mining ; Wu , X. , Kumar , V. , Eds . ; Chapman & Hall / CRC : London , UK , 2009 ; pp .", "label": "", "metadata": {}, "score": "54.31777"}
{"text": "In addition , a novel score , named as the Proportional Overlapping Score ( POS ) , is proposed by which a gene 's overlapping degree is estimated .We then utilized the constructed gene masks along\u2010with the gene scores to assign the minimum subset of genes that provide the maximum number of correctly classified samples in a training set .", "label": "", "metadata": {}, "score": "54.331657"}
{"text": "that is dynamically modified according to its own experience and results in its local best ( lb ) performance .It is also affected by others particles flying experience resulting in the best value , global best ( gb ) .The underlying expression for the update of the velocity in successive generations reads as follows : . are numbers drawn from a uniform distribution over the unit interval that brings some component of randomness to the search process .", "label": "", "metadata": {}, "score": "54.33221"}
{"text": "Having calculated the model expectations it is then possible to calculate the gradient of the objective function .This allows for the use of many gradient based optimization algorithms , the most simple of which is gradient ascent : .The gradient provides a search direction and a step size \u03b7 , which can be chosen statically or be maximized dynamically by a line search .", "label": "", "metadata": {}, "score": "54.346886"}
{"text": "Their limitations notwithstanding , the advantages of automatic binarization methods are : .they are fully reproducible ( on the same image they will always lead to the same binarization result ) .they introduce no user bias during thresholding ( this is not related to bias associated with the choice of specific automatic algorithm , which does exist ) .", "label": "", "metadata": {}, "score": "54.42015"}
{"text": "This can be seen clearly when using the CPSO method to search for the best subset of feature and instances .For example , in Table 13 if we use the CPSO method , the RMSE for using 70 % of data is 3.413 , whereas the RMSE for the standard method is 8.312 .", "label": "", "metadata": {}, "score": "54.465977"}
{"text": "We include the multiplier \" 4 \" in equation 9 to scale POS score to be within the closed interval [ 0,1].In this way , a lower score denotes gene with higher discriminative power .Once the gene mask is defined and POS index is computed , we assign each gene to its relative dominant class ( RDC ) .", "label": "", "metadata": {}, "score": "54.49961"}
{"text": "Rotation Forest is an ensemble classification approach which is built with a set of decision trees .For each tree , the bootstrap samples extracted from the original training set are adopted to construct a new training set .Then the feature set of the new training set is randomly split into some subsets , which are transformed individually .", "label": "", "metadata": {}, "score": "54.57493"}
{"text": "j . ) log .y .i .x .j . )v .D .x .j .v .D .H .Y .x .j .v . )Information gain Gain ( Y , X j ) , or average mutual information I ( X j ; Y ) , is a measure of reduction in uncertainty about Y due to the knowledge on a feature X j , quantified by the change in entropy : .", "label": "", "metadata": {}, "score": "54.58358"}
{"text": "In practice , it appeared to be difficult to define a single measure of diversity and even more difficult to relate that measure to the ensemble performance in a neat and expressive dependency .Here , to investigate the ability of the proposed ICA - based RotBoost ensemble to build accurate and diverse base learners efficiently , the pairwise diversity measure is utilized [ 40 ] .", "label": "", "metadata": {}, "score": "54.608208"}
{"text": "The highest improvement measured by log ratio , 0.150 , is obtained at the selected sets composed of a single gene .For SVM classifier , improvements over the best method of the compared techniques are achieved by the POS method at most set sizes .", "label": "", "metadata": {}, "score": "54.608208"}
{"text": "It appears that it is relatively easy to find pieces of text mentioning genes , but much harder to determine the exact boundaries of that mention .This hurts the system performance significantly since the scoring metric requires exact matches .However , entity tagging primarily exists to give some structure to text for higher level information extraction systems such as relation detection , fact generation and question answering .", "label": "", "metadata": {}, "score": "54.654793"}
{"text": "Table 1 summarizes the characteristics of the datasets .The estimated classification error rate is based on the Random Forest classifier with the full set of features , without pre\u2010selection , using 50 repetitions of 10\u2010fold cross validation .Eight of the datasets are bi\u2010class , while three , i.e. Srbct , GSE14333 and GSE27854 , are multi\u2010classes .", "label": "", "metadata": {}, "score": "54.701168"}
{"text": "In this figure , two possible separating patterns are shown : a small margin and a large margin .The two dashed lines and margin d are given by .An SVM finds the values for the variables w and b which maximizes the margin for the training data .", "label": "", "metadata": {}, "score": "54.72897"}
{"text": "The use of OVA class binarization in our proposed way increases the overall bearing fault diagnostic accuracy , from 33.3 % to 66.7 % for normal and single defect types , and from 0 % to 100 % for COMB .The overall test accuracy of 75 % of the proposed method is also much better than the random guess among eight classes ( 12.5 % ) and the worst - case classification of classifying all cases to the majority class , i.e. , Normal ( 37.5 % ) .", "label": "", "metadata": {}, "score": "54.746773"}
{"text": "View Article .Cortes C , Vapnik V : Support\u2010vector networks .Mach Learn 1995 , 20 ( 3 ) : 273 - 297 .Baralis E , Bruno G , Fiori A : Minimum number of genes for microarray feature selection .", "label": "", "metadata": {}, "score": "54.78998"}
{"text": "Dimension of polynomial kernel ( natural number ) : We can use only a polynomial kernel in YamCha .Range of window ( integer ) : The SVM can use the information on tokens surrounding the token of interest as illustrated in Figure 2 .", "label": "", "metadata": {}, "score": "54.795486"}
{"text": "We present a nonparametric Bayesian model of tree structures based on the hierarchical Dirichlet process ( HDP ) .Our HDP - PCFG model allows the complexity of the grammar to grow as more training data is available .In addition to presenting a fully Bayesian model for the PCFG , we also develop an efficient variational inference procedure .", "label": "", "metadata": {}, "score": "54.817917"}
{"text": "M. Setnes , R. Babu\u0161ka , U. Kaymak , and H. R. Van Nauta Lemke , \" Similarity measures in fuzzy rule base simplification , \" IEEE Transactions on Systems , Man , and Cybernetics B , vol .28 , no . 3 , pp .", "label": "", "metadata": {}, "score": "54.82657"}
{"text": "We apply our method to train parsers that excel when used as part of a reordering component in a statistical machine translation system .We use a corpus of weakly - labeled reference reorderings to guide parser training .Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress .", "label": "", "metadata": {}, "score": "54.842506"}
{"text": "For the thresholding of the brightness , the standard Auto Thresholds can be applied , thus reducing the bias slightly .Similar to the method described above but more replicable , the image can be split into the respective channels of different color spaces ( HSB , CIELAB , ... ) and those channels can then be automatically thresholded since all of them are based on a single intensity scale .", "label": "", "metadata": {}, "score": "54.86106"}
{"text": "In Table 3 , we also list the numeric values of the parameters of the PSO and CPSO environment .As to the size of the population and the number of generations , we used a larger population and a larger number of generations in the generic version of the PSO than in the CPSO because of the larger search space this algorithm operates in .", "label": "", "metadata": {}, "score": "54.88025"}
{"text": "Note that in this percentage we included all different combinations of the features ' percentages and the data percentages being used .The percentage of the improvement is higher when dealing with a smaller percentage of features and data .For example , the percentage of improvement is 34 % for 10 % of the instances and 50 % of the features selected while the percentage of improvement is less than 10 % for 60 % of instances and features used .", "label": "", "metadata": {}, "score": "54.895035"}
{"text": "If the token has more than one feature , then we used the feature listed first in Table 2 ( left side comes before the right side in the table ) .Prefix : Uni- , bi- , and tri - grams ( in letters ) of the beginning letters of the current token .", "label": "", "metadata": {}, "score": "54.92802"}
{"text": "The extent to which explicit models can give us insights into linguistic patterns depends largely on what kind of model is used .Some models , such as decision trees , are relatively transparent , and give us direct information about which factors are important in making decisions and about which factors are related to one another .", "label": "", "metadata": {}, "score": "54.94226"}
{"text": "So what is the alternative ?An alternative goal is to develop segmentation methods that overcome the variability .Indeed , segmentation algorithms have been a goal in biology and other areas for decades .The multifarious algorithms that have been developed are based on a vast array of different image properties ( see below ) but they all have in common that the determination of the threshold is based on image - intrinsic properties and not on subjective , real - time user decisions .", "label": "", "metadata": {}, "score": "54.997475"}
{"text": "To avoid degeneracy when some features are perfectly correlated and to reduce overfitting for rarely occurring features , we penalize the likelihood with a spherical Gaussian prior over feature weights [ 9 ] : .The variance hyper - parameter \u03c3 2 determines the modeling trade - off between fitting exactly the observed feature frequencies and the squared norm of the weight vector .", "label": "", "metadata": {}, "score": "55.01566"}
{"text": "In order to reduce the computational complexity of using the standard PSO , we employed cooperative PSO method to simultaneously solve the two optimization tasks .The motivation behind the use of cooperative PSO , as advocated in [ 33 ] , is to deal effectively with the dimensionality of the search space , which becomes a serious concern when a large number of data with a large dimensionality are involved .", "label": "", "metadata": {}, "score": "55.049026"}
{"text": "def rte_features ( rtepair ) : . return features .Example 2.2 ( code_rte_features . py ) : Figure 2.2 : \" Recognizing Text Entailment \" Feature Extractor .The RTEFeatureExtractor class builds a bag of words for both the text and the hypothesis after throwing away some stopwords , then calculates overlap and difference .", "label": "", "metadata": {}, "score": "55.108555"}
{"text": "Some studies have been reported on the application of microarray gene expression data analysis for molecular cancer classification [ 5 , 6 ] .Usually , microarray classification process comprised of two successive steps , that is , feature selection and classification .", "label": "", "metadata": {}, "score": "55.203865"}
{"text": "The c -value controls the tradeoff between errors of the SVM on training data and margin maximization .10-fold cross - validation was implemented as follows .All 268 discharge summaries were randomly divided into ten subsets of almost equal size .", "label": "", "metadata": {}, "score": "55.225197"}
{"text": "POS(A ) means the original , newswire trained POS tagger .POS(B ) means the POS tagger with trained on GENIA corpus 3.02p . \" full(A ) \" means using all features and the original , newswire trained POS tagger .\" full(B ) \" means using all features and the POS tagger with trained on GENIA corpus 3.02p The parenthesized values are p - values .", "label": "", "metadata": {}, "score": "55.252205"}
{"text": "Stability scores at different sizes of features sets that selected by Wil\u2010RS , mRMR , MP and POS methods on ' GSE24514 ' dataset .A stable selection does not guarantee the relevancy of the selected features to the considered response of the target class labels .", "label": "", "metadata": {}, "score": "55.26289"}
{"text": "The rate at which each swarm converges to the solution is significantly higher than the rate of convergence of the standard PSO .The essence of the cooperative version of PSO is to split the data into several groups so that each group is handled by a separate PSO .", "label": "", "metadata": {}, "score": "55.264885"}
{"text": "Methods .The support vector machine ( SVM ) , introduced by Vapnik [ 18 ] , is a learning algorithm for solving two - class pattern recognition problems and is known for its good performance .SVM is used for many types of natural language processing ( e.g. text classification and named entity recognition ) .", "label": "", "metadata": {}, "score": "55.269913"}
{"text": "Each row provides the average classification error rate at a specific number of selected genes , reported in the first column .The aggregate average error value and the minimum error rate for each method with each classifier are provided in the last two rows .", "label": "", "metadata": {}, "score": "55.315796"}
{"text": "Takeuchi et al .[ 10 ] also used an SVM - based system to evaluate the effects of feature sets and found a range of -1 + 1 was better than -3 + 3 , which is the opposite of our findings .", "label": "", "metadata": {}, "score": "55.32419"}
{"text": "In other words , a total of 18 features from the vibration data are used .Three classification algorithms , namely SVM , C4.5 and naive Bayes are applied with the proposed procedure and the results of the best performing method is reported below .", "label": "", "metadata": {}, "score": "55.341164"}
{"text": "We used the orthographic features described in [ 25 ] and modified some specifically for medication information such as \" digit \" and \" percent . \" In total , we had 21 labels for orthographic features .History features ( or preceding class feature ) : class assignment of the preceding word .", "label": "", "metadata": {}, "score": "55.36873"}
{"text": "It 's important to understand what we can learn about language from an automatically constructed model .One important consideration when dealing with models of language is the distinction between descriptive models and explanatory models .Descriptive models capture patterns in the data but they do n't provide any information about why the data contains those patterns .", "label": "", "metadata": {}, "score": "55.38976"}
{"text": "Make use of this fact to build a consecutive classifier for labeling dialog acts .Be sure to consider what features might be useful .See the code for the consecutive classifier for part - of - speech tags in 1.7 to get some ideas .", "label": "", "metadata": {}, "score": "55.416588"}
{"text": "Therefore , during the last decade , feature selection methods in conjunction with constructing fuzzy models for reducing the curse of dimensionality were developed [ 15 - 22 ] .This process reduces the fuzzy rule search space and increases the accuracy of the model .", "label": "", "metadata": {}, "score": "55.46804"}
{"text": "Expert Syst .Appl .[ Google Scholar ] .Galar , M. ; Fern\u00e1ndez , A. ; Barrenechea , E. ; Bustince , H. ; Herrera , F. An overview of ensemble methods for binary classifiers in multi - class problems : Experimental study on one - vs - one and one - vs - all schemes .", "label": "", "metadata": {}, "score": "55.503304"}
{"text": "10 Exercises .Find out what type and quantity of annotated data is required for developing such systems .Why do you think a large amount of data is required ?Begin by splitting the Names Corpus into three subsets : 500 words for the test set , 500 words for the dev - test set , and the remaining 6900 words for the training set .", "label": "", "metadata": {}, "score": "55.558167"}
{"text": "In our experiment , the ISIS technique has been applied using the ' SIS ' R package .For large enough input feature sets , effective classifier algorithms may have more ability to mitigate the potential effects of noisy and uninformative features by focusing more on the informative ones .", "label": "", "metadata": {}, "score": "55.56626"}
{"text": "In this paper , for the sake of convenience , we refer to the term \" named entities \" or \" entities \" as fields which have the same meaning as [ 10 , 13 ] .The first task was an NER task , for which both the Sydney and the Wisconsin teams used CRF to detect fields , while Halgrim et al .", "label": "", "metadata": {}, "score": "55.58438"}
{"text": "In previous work , various strategies have been proposed to integrate multiple classifiers , e.g. simple majority voting [ 6 , 8 ] , bootstrapping [ 26 ] and boosting [ 27 ] .In this paper , ensembles were constructed using simple voting strategies .", "label": "", "metadata": {}, "score": "55.633213"}
{"text": "To evaluate the performance of the various methods and features we used two experimental settings .In the first setting , 268 annotated discharge summaries from the 2009 i2b2 challenge were used ( 17 from the training set and 251 from the test set ) .", "label": "", "metadata": {}, "score": "55.65288"}
{"text": "( the i th row of M ) such that gene mask element m i j is defined as : .Figure 2 shows the constructed core expression intervals I i , 1 and I i , 2 associated with a particular gene i along\u2010with its gene mask .", "label": "", "metadata": {}, "score": "55.706207"}
{"text": "Modeling the linguistic data found in corpora can help us to understand linguistic patterns , and can be used to make predictions about new language data .Supervised classifiers use labeled training corpora to build models that predict the label of an input based on specific features of that input .", "label": "", "metadata": {}, "score": "55.722996"}
{"text": "The panel on right bottom of Figure 5 shows the averages of log ratios across all considered datasets for each classifier .Log ratio between the error rates of the best compared method and the POS .Log ratios measure the improvement / deterioration achieved by the proposed method over the best compared method for three different classifiers ; RF , k NN and SVM .", "label": "", "metadata": {}, "score": "55.757935"}
{"text": "For non - linear classification tasks , the application of kernel functions is required to transform the problem into a higher dimensional feature space to make linear separation possible .The choice of the appropriate kernel function is very important .A common choice is a polynomial kernel with the kernel parameters and the penalty margin C selected by cross - validation .", "label": "", "metadata": {}, "score": "55.790947"}
{"text": "In this way , we can define this set as : . to represent its size .Gene masks .For each gene , we define a mask based on its observed expression values and constructed core intervals presented in subsection ' Definition of core intervals ' .", "label": "", "metadata": {}, "score": "55.791313"}
{"text": "The study is concerned with data and feature reduction in fuzzy modeling .As these reduction activities are advantageous to fuzzy models in terms of both the effectiveness of their construction and the interpretation of the resulting models , their realization deserves particular attention .", "label": "", "metadata": {}, "score": "55.7952"}
{"text": "And since the number of branches increases exponentially as we go down the tree , the amount of repetition can be very large .A related problem is that decision trees are not good at making use of features that are weak predictors of the correct label .", "label": "", "metadata": {}, "score": "55.810753"}
{"text": "It does not stop branching until it correctly classifies all instances , or each leaf node contains a minimum number of records .Note that this split is a greedy choice and the effect of future decisions is not taken into account .", "label": "", "metadata": {}, "score": "55.830116"}
{"text": "The training set and test set are taken from the same genre , and so we can not be confident that evaluation results would generalize to other genres .What 's worse , because of the call to random.shuffle ( ) , the test set contains sentences that are taken from the same documents that were used for training .", "label": "", "metadata": {}, "score": "55.8422"}
{"text": "The log - likelihood function of conditional random fields , which generalizes the well - known case of logistic regression , is easily seen to be concave [ 6 ] , as is the penalized likelihood ( 3 ) .To maximize the penalized log - likelihood , we compute its partial derivatives with respect to the weights : . is the empirical feature count of feature f i and E [ f i ] is the model expectation of feature f i .", "label": "", "metadata": {}, "score": "55.862934"}
{"text": "Experimental studies are presented in Section 5 , and conclusions are provided in Section 6 .Selected Approaches to Data and Space Reduction .In general , reduction processes involve feature selection ( FS ) , instances ( data ) selection ( IS ) , and a combination of these two reduction processes : feature and Instances selection ( FIS ) .", "label": "", "metadata": {}, "score": "55.922417"}
{"text": "We can treat RTE as a classification task , in which we try to predict the True / False label for each pair .Although it seems likely that successful approaches to this task will involve a combination of parsing , semantics and real world knowledge , many early attempts at RTE achieved reasonably good results with shallow analysis , based on similarity between the text and hypothesis at the word level .", "label": "", "metadata": {}, "score": "55.9828"}
{"text": "17 ] use factor analysis models rather than principle component analysis to identify informative genes .A comparison between some algorithms for identifying informative genes in microarray data can be found in [ 15 , 20 ] .Analyzing the overlap between gene expression measures for different classes can be another important criterion for identifying discriminative genes which are relevant to the considered classification task .", "label": "", "metadata": {}, "score": "56.041237"}
{"text": "The authors are not aware of any work in the literature that studies this practical problem .In this paper , a strategy based on one- versus -all ( OVA ) class binarization is proposed to improve fault diagnostics accuracy while reducing the number of scenarios for data collection , by predicting concurrent defects from training data of normal and single defects .", "label": "", "metadata": {}, "score": "56.1409"}
{"text": "The problem of identification of these discriminative genes for their use in classification has been investigated in many studies [ 1 ] \u2010 [ 9 ] .Assessment of maximally selected genes or prognostic factors \u2010 equivalently selected by the minimum p\u2010value approach \u2010 have been discussed in [ 10 , 11 ] using data from clinical cancer research and gene expression .", "label": "", "metadata": {}, "score": "56.148438"}
{"text": "3.2 Accuracy .The simplest metric that can be used to evaluate a classifier , accuracy , measures the percentage of inputs in the test set that the classifier correctly labeled .The function nltk.classify.accuracy ( ) will calculate the accuracy of a classifier model on a given test set : . format(nltk.classify.accuracy(classifier , test_set ) ) ) 0.75 .", "label": "", "metadata": {}, "score": "56.15094"}
{"text": "The system already has tens of thousands of singleton features , making it infeasible to create all such conjunctions .Even if it were computationally feasible to train a model with all conjunctions , it would be difficult to gather sufficient statistics on them since most conjunctions occur rarely if ever .", "label": "", "metadata": {}, "score": "56.17068"}
{"text": "@@14477 ... the / DT secretion / NN of / IN PTH / NEWGENE and / CC CT / NEWGENE ... ./CD 59/CD,/ , p / NN .Effects of tuning parameters and features .We analyzed the effects of the tuning parameters and features on system performance .", "label": "", "metadata": {}, "score": "56.197933"}
{"text": "POS and NEWGENE were tagged in the training and test data for development .The tags were not shown in the test data for evaluation .We tagged using the Brill tagger in the training and two test data sets because we wanted to use POS as a feature .", "label": "", "metadata": {}, "score": "56.221443"}
{"text": "Conversely , if there is information found in the hypothesis that is absent from the text , then there will be no entailment .Not all words are equally important - Named Entity mentions such as the names of people , organizations and places are likely to be more significant , which motivates us to extract distinct information for word s and ne s ( Named Entities ) .", "label": "", "metadata": {}, "score": "56.222927"}
{"text": "We call these values w[label ] and w[f , label ] the parameters or weights for the model .Using the naive Bayes algorithm , we set each of these parameters independently : .However , in the next section , we 'll look at a classifier that considers the possible interactions between these parameters when choosing their values .", "label": "", "metadata": {}, "score": "56.24859"}
{"text": "\" I usually define a manual threshold to extract my objects ... , is this ok ? \"As a recommendation ...Whenever possible , try to avoid manual thresholding ! ! ! tedious and time - consuming fiddling - around finding an \" appropriate \" cut - off value .", "label": "", "metadata": {}, "score": "56.281727"}
{"text": "In contrast to the search space for CPSO , the large search space is decomposed into multiple subswarms that reduce the dimensionality of the original search space .Table 12 : Percentage of improvement of the RMSE obtained when using CPSO over the results formed by the PSO ; Housing data set .", "label": "", "metadata": {}, "score": "56.352318"}
{"text": "In Table 9 , the evaluations were performed by using each feature one at a time in addition to the word and preceding class features .As shown in the table , each feature had a significant effect when few other features were present .", "label": "", "metadata": {}, "score": "56.425304"}
{"text": "One way to capture this intuition that distribution ( i ) is more \" fair \" than the other two is to invoke the concept of entropy .In the discussion of decision trees , we described entropy as a measure of how \" disorganized \" a set of labels was .", "label": "", "metadata": {}, "score": "56.467415"}
{"text": "In this context , POS presents a novel generalized version , called POS score , of the overlapping score ( OS ) measure , proposed in [ 7 ] .POS provides genes categorization into the target class labels based on their relative dominant classes i.e. , POS assigns each gene to the class label that has the highest proportion of correctly assigned samples relative to class sizes .", "label": "", "metadata": {}, "score": "56.49143"}
{"text": "PubMed View Article .Breiman L , Friedman J , Stone C , Olshen R : Classification and regression trees .New York : Chapman & Hall / CRC ; 1984 .Ultsch A , Pallasch C , Bergmann E , Christiansen H : A comparison of algorithms to find differentially expressed genes in microarray data .", "label": "", "metadata": {}, "score": "56.52234"}
{"text": "At that point , we can use the test set to evaluate how well our model will perform on new input values . 1.3 Document Classification .In 1 , we saw several examples of corpora where documents have been labeled with categories .", "label": "", "metadata": {}, "score": "56.553123"}
{"text": "Here , the information granularity process deals only with the subset of the data and features ( .constructed from the fuzzy models is use to evaluate the performance of the selected data and features .At this stage we access the performance of the constructed fuzzy model in terms of their capability to fit the model by using the all instances in the original data set . is reduced , and in this Cartesian product a fuzzy model , denoted by FM , is designed in the usual way ( we elaborate on the form of the fuzzy model in the subsequent section ) .", "label": "", "metadata": {}, "score": "56.55349"}
{"text": "The \" word+POS(A)+pc .\" column shows the case using the original POS tagger .The \" word+POS(B)+pc . \" column shows the case using the POS tagger trained on the GENIA corpus .The word and preceding class features were also used in both cases .", "label": "", "metadata": {}, "score": "56.554718"}
{"text": "These parameters affect the support vectors in SVM learning .1 st run : Exact pattern matching between GPD1 and words in the training and test data .2 nd run : Regular expression pattern matching between GPD1 and words in the training and test data .", "label": "", "metadata": {}, "score": "56.568253"}
{"text": "View Article PubMed .Collier N , Nobata C , Tsujii J : Extracting the Names of Genes and Gene Production with a Hidden Marcov Model .Proceedings of the 18thInternational Conference on Computational Linguistics ( COLING'2000 ) 2000 , 201 - 207 .", "label": "", "metadata": {}, "score": "56.621597"}
{"text": "Word features : words only .POS features : Part - of - Speech tags of words .To obtain POS information , we used a generic POS tagger in the NLTK package [ 24 ] .Morphologic features : suffix / prefix of up to 3 characters within a word .", "label": "", "metadata": {}, "score": "56.666306"}
{"text": "We propose a statistical method for selecting genes based on overlapping analysis of expression data across classes .This method results in a novel measure , called proportional overlapping score ( POS ) , of a feature 's relevance to a classification task .", "label": "", "metadata": {}, "score": "56.69906"}
{"text": "Which automatic methods do exist for binarization ?Generally , there are different groups of algorithms for image binarization .( The following classification of methods is taken from Sezgin and Sankur , Survey over image thresholding techniques and quantitative performance evaluation , Journal of Electronic Imaging 13(1 ) , 146 - 165 ( January 2004 ) . )", "label": "", "metadata": {}, "score": "56.7006"}
{"text": "The 2009 i2b2 NLP challenge aimed to extract medication information ( i.e. , medication name , dosage , mode , frequency , duration , and reason ) from de - identified hospital discharge summaries [ 9 ] .The different types of information were called fields and are described in Table 1 .", "label": "", "metadata": {}, "score": "56.74843"}
{"text": "Spatial resolution refers to the number ( or density , if you prefer ) of samples in the image .Digital detectors such as cameras and PMTs can produce sample matrices ranging from 256 x 256 pixels or fewer , up to 128 megapixels or more .", "label": "", "metadata": {}, "score": "56.748566"}
{"text": "The error rates have been measured by 50 repetations of 10\u2010fold cross validation for three different classifiers : Random Forest ( RF ) ; k Nearest Neighbor ( k NN ) ; Support Vector Machine ( SVM ) .Stability\u2010accuracy plot for ' GSE27854 ' dataset .", "label": "", "metadata": {}, "score": "56.84191"}
{"text": "Normalization is performed so that every gene expression has mean equal to 0 and variance equal to 1 .In summary , the 8 datasets had between 2 and 5 distinct diagnostic categories , 60 - 253 instances , and 2000 - 24481 genes .", "label": "", "metadata": {}, "score": "56.961617"}
{"text": "For classification tasks that have a small number of well - balanced labels and a diverse test set , a meaningful evaluation can be performed with as few as 100 evaluation instances .But if a classification task has a large number of labels , or includes very infrequent labels , then the size of the test set should be chosen to ensure that the least frequent label occurs at least 50 times .", "label": "", "metadata": {}, "score": "57.015205"}
{"text": "Ono T , Hishigaki H , Tanigami A , Takagi T : Automatic extraction of information on protein - protein interactions from biomedical literature .Bioinformatics 2001 , 17 ( 2 ) : 155 - 161 .View Article PubMed .Blaschke C , Valencia A : Can bibliographic pointers for known biological data be found automatically ?", "label": "", "metadata": {}, "score": "57.030502"}
{"text": "[19 ] developed an R package , named ' mRMRe ' , by which an ensemble version of mRMR has been implemented .The authors of [ 19 ] use two different strategies to select multiple features sets , rather than a single set , in order to mitigate the potential effect of the low sample\u2010to\u2010dimensionality ratio on the stability of the results .", "label": "", "metadata": {}, "score": "57.064713"}
{"text": "We will learn more about the naive Bayes classifier later in the chapter .For now , let 's just test it out on some names that did not appear in its training data : .Observe that these character names from The Matrix are correctly classified .", "label": "", "metadata": {}, "score": "57.094513"}
{"text": "E ) Same as B , except features using the gene lexicon are used .F ) Same as B , except features using all lexicons are used .A straightforward method of integrating these post - processing steps into our model is to create predicates indicating whether a token occurs in one of the ABGene lexicons .", "label": "", "metadata": {}, "score": "57.107742"}
{"text": "A CRF assigns probabilities to possible class labels y of input sequences x .For the problem at hand , x is the sequence of tokens and y is the corresponding label sequence .In a linear chain CRF , which is used for sequence modeling tasks , the label y i at position i depends on its predecessor y i -1 and successor y i +1 .", "label": "", "metadata": {}, "score": "57.119026"}
{"text": "Using the test data in the challenge , Doan and Xu [ 14 ] investigated using output from the MedEx rule - based system as features for SVM algorithms and showed that those features could substantially improve an SVM - based NER system .", "label": "", "metadata": {}, "score": "57.137253"}
{"text": "On the other hand , if the input values have a wide variety of labels , then there are many labels with a \" medium \" frequency , where neither P(l ) nor log 2P(l ) is small , so the entropy is high .", "label": "", "metadata": {}, "score": "57.142113"}
{"text": "We use only binary features , for instance : .The weight \u03bb i for each feature should ideally be highly positive if the feature tends to be on for the correct labeling , highly negative if the feature tends to be off for the correct labeling , and around zero if the feature is uninformative .", "label": "", "metadata": {}, "score": "57.174934"}
{"text": "We will call xml_posts ( ) to get a data structure representing the XML annotation for each post : .Next , we 'll define a simple feature extractor that checks what words the post contains : .Finally , we construct the training and testing data by applying the feature extractor to each post ( using post.get ( ' class ' ) to get a post 's dialogue act type ) , and create a new classifier : . 2.3 Recognizing Textual Entailment .", "label": "", "metadata": {}, "score": "57.209415"}
{"text": "A total of 72 features of the vibration data from the two ( horizontal and vertical ) sensors are fed into C4.5 decision tree together for training and testing .Three classification algorithms , namely SVM , C4.5 and naive Bayes are applied with the proposed procedure and the results of the best performing method is reported below .", "label": "", "metadata": {}, "score": "57.23082"}
{"text": "However , the problem can be somewhat alleviated by making certain independence assumptions on the parameters as well as only including statistics on positions in the sequence that are mislabeled by the current parameter settings .McCallum [ 16 ] describes the procedure in more detail .", "label": "", "metadata": {}, "score": "57.2538"}
{"text": "An effective feature selection technique is expected to produce stable outcomes across several sub\u2010samples of the considered dataset .This property is particularly desirable for biomarker selections within a diagnostic setting .A stable feature selection method should yield a set of biological informative markers that are selected quite often , and randomly chosen features that are selected rarely or never .", "label": "", "metadata": {}, "score": "57.25421"}
{"text": "The results are for a 10-fold cross validation .The results in Table 5 were for the evaluation test data ; cross validation was not used . \"GPD1 \" means the results using GPD1 in dictionary matching .They correspond to the 1 st run in Table 5 . \"", "label": "", "metadata": {}, "score": "57.421722"}
{"text": "Such a case is more likely to happen in situations with unbalanced class\u2010size distributions .As a result , a biased selection could result .Assigning the dominant class on a relative basis , as proposed in subsection ' The proposed POS measure and relative dominant class assignments ' , and taking these assignments into account during the gene ranking process allows us to overcome this problem .", "label": "", "metadata": {}, "score": "57.429882"}
{"text": "\" But greetings , questions , answers , assertions , and clarifications can all be thought of as types of speech - based actions .Recognizing the dialogue acts underlying the utterances in a dialogue can be an important first step in understanding the conversation .", "label": "", "metadata": {}, "score": "57.444466"}
{"text": "After the support vectors are selected , the rest of features are not required .With consideration of the not completely separable case , the SVM modeling can be expressed as an optimization problem with slack variables \u03be i and error penalty C : .", "label": "", "metadata": {}, "score": "57.465286"}
{"text": "We can then examine individual error cases where the model predicted the wrong label , and try to determine what additional pieces of information would allow it to make the right decision ( or which existing pieces of information are tricking it into making the wrong decision ) .", "label": "", "metadata": {}, "score": "57.467358"}
{"text": "Post - processing binary operations might be necessary to correct further measurements of area or object counts .Watershed ( or related ) separation techniques may be necessary when close particles fuse to form clumps or aggregates .Segmented ROIs for additional processing .", "label": "", "metadata": {}, "score": "57.47728"}
{"text": "The penalty for a recognition failure is often small : if two con- figurations are confused , they are often similar to each other , and the illusion works well enough , for instance , to drive a graphics animation of the moving hand .", "label": "", "metadata": {}, "score": "57.496357"}
{"text": "One way to harness the advantages of both these approaches is to combine them into an ensemble classifier [ 4 , 6 , 8 ] .Zhou et al .[ 8 ] investigated the combination of three classifiers , including one SVM and two discriminative Hidden Markov Models , into an ensemble classifier with a simple majority voting strategy .", "label": "", "metadata": {}, "score": "57.527092"}
{"text": "Tsui , K. ; Chen , V. ; Jiang , W. ; Aslandogan , Y. ; Pham , H. Data Mining Methods and Applications .In Springer Handbook of Engineering Statistics ; Springer : London , UK , 2006 ; pp .", "label": "", "metadata": {}, "score": "57.561615"}
{"text": "Table 3 : Classification results obtained by RotBoost ensemble learning against typical 8 gene datasets in terms of PCA / ICA transformation methods .As it can be noted from Table 3 , in 5 cases out of 8 , ICA - based RotBoost learners could outperform their PCA - based counterparts in terms of higher classifications accuracies and lower standard deviations .", "label": "", "metadata": {}, "score": "57.635166"}
{"text": "Once again , there are many distributions that are consistent with this new piece of information , such as : .But again , we will likely choose the distribution that makes the fewest unwarranted assumptions - in this case , distribution ( v ) .", "label": "", "metadata": {}, "score": "57.669273"}
{"text": "1157 - 1182 , 2003 .View at Google Scholar . H. Liu and H. Motoda , Instance Selection and Construction for Data Mining , Kluwer Academic Publishers , Boston , Mass USA , 2001 . H. Ishibuchi , T. Nakashima , and M. Nii , \" Genetic - Algorithm - Based instance and feature selection , \" in Instance Selection and Construction for Data Mining , H. Lui and H. Motoda , Eds . , pp .", "label": "", "metadata": {}, "score": "57.694607"}
{"text": "This is coincident with the observation previously given that the diversity usually conflicts with the accuracy of the base learners .However our experiments indicate that the ICA - based RotBoost ensemble could establish a proper balance between the diversity and the accuracy of the constituted base learners .", "label": "", "metadata": {}, "score": "57.8369"}
{"text": "Such a strategy has been applied in many studies including [ 7 ] and [ 8 ] .In this article , our experiment is conducted using eleven gene expression datasets in which the POS method is validated by comparison with five well\u2010known gene selection techniques .", "label": "", "metadata": {}, "score": "57.926888"}
{"text": "The error rates have been measured by 50 repetations of 10\u2010fold cros validation for three different classifiers : Random Forest ( RF ) ; k Nearest Neighbor ( k NN ) ; Support Vector Machine ( SVM ) .Genomic experiments are representative examples for high\u2010dimensional datasets .", "label": "", "metadata": {}, "score": "57.9645"}
{"text": "The predicate set o j used to represent the j th input token picks out useful properties of the token and its context .The tag sequence uses the possible tags B - GENE , I - GENE and O , representing the beginning , inside and outside of a gene mention respectively .", "label": "", "metadata": {}, "score": "57.98501"}
{"text": "For example , features 6 and 13 were selected when using both 30 % and 70 % of data .In contrast to the selection made with the PSO algorithm , the subset of the features selected here is not as stable , especially when using only 30 % of data .", "label": "", "metadata": {}, "score": "58.05226"}
{"text": "This is often referred to as a threshold ( value ) .Separating image features and objects by there intensity is often suitable if the intensity is the parameter which directly relates to the spatial characteristics of the objects and defines those .", "label": "", "metadata": {}, "score": "58.057976"}
{"text": "Table 2 rows C through F summarizes the effect of adding these lexicons to the system .Rows C through F assume the use of feature induction , which is explored in the next section .Feature induction .So far we have only described features over a single predicate .", "label": "", "metadata": {}, "score": "58.120842"}
{"text": "To justify why certain hypotheses are proposed and why certain others are not , thus help the operator evaluate and act upon his / her experience .The focus of this paper lies in the multiple fault identification ability with statistical and data mining techniques .", "label": "", "metadata": {}, "score": "58.135445"}
{"text": "3 rd run : Exact pattern matching between GPD2 and words in the training and test data .Our final result in the BioCreAtIvE competition is shown as case 1 in Table 5 .The best \" balanced f - score \" was that for the 2 nd run .", "label": "", "metadata": {}, "score": "58.19051"}
{"text": "McCallum A , Freitag D , Pereira F : Maximum entropy Markov models for information extraction and segmentation .Proceedings of ICML 2000 .Kudo T , Matsumoto Y : Chunking with Support Vector Machines .Proc NAACL 2001 ACL 2001 .", "label": "", "metadata": {}, "score": "58.269524"}
{"text": "PubMed View Article .Fan J , Samworth R , Wu Y : Ultrahigh dimensional feature selection : beyond the linear model .J Mach Learn Res 2009 , 10 : 2013 - 2038 .PubMed Central PubMed .Saeys Y , Inza I , Larra\u00f1aga P : A review of feature selection techniques in bioinformatics .", "label": "", "metadata": {}, "score": "58.275486"}
{"text": "It is also necessary to point out that all these experiments have been accomplished on the best discriminative genes already selected by FCBF algorithm .It is well known that no algorithm can hold a general advantage in terms of generalization capability over another one across all possible classification tasks .", "label": "", "metadata": {}, "score": "58.283775"}
{"text": "11 , pp .1341 - 1347 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" A decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "58.284332"}
{"text": "5.5 The Cause of Double - Counting .The reason for the double - counting problem is that during training , feature contributions are computed separately ; but when using the classifier to choose labels for new inputs , those feature contributions are combined .", "label": "", "metadata": {}, "score": "58.288666"}
{"text": "Statistical analysis was performed using commercial software ( SPSS for Windows , version 11.0J , SPSS Inc. ) .Yeh [ 16 ] pointed out that \" both precision and f - score are complicated nonlinear functions of random variables \" and the randomization test was recommended .", "label": "", "metadata": {}, "score": "58.368217"}
{"text": "15 , pp .2429 - 2437 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .I. Kononenko , \" Estimating attributes : analysis and extensions of RELIEF , \" in Proceedings of the European Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "58.374237"}
{"text": "It 's common to start with a \" kitchen sink \" approach , including all the features that you can think of , and then checking to see which features actually are helpful .We take this approach for name gender features in 1.2 .", "label": "", "metadata": {}, "score": "58.41655"}
{"text": "The naive Bayes classification method , which we 'll discuss next , overcomes this limitation by allowing all features to act \" in parallel . \"5 Naive Bayes Classifiers .In naive Bayes classifiers , every feature gets a say in determining which label should be assigned to a given input value .", "label": "", "metadata": {}, "score": "58.435684"}
{"text": "Firstly , the PSO is divided into .PSO - Integrated Feature and Data Reduction in Fuzzy - Rule - Based Models .As the problem of feature data reduction is inherently combinatorial nature , PSO provides an interesting and computationally viable optimization alternative .", "label": "", "metadata": {}, "score": "58.438004"}
{"text": "Electronic supplementary material .12859_2014_6543_MOESM1_ESM.pdf Additional file 1 : Classification error rates obtained by Random Forest Classifier .Each row provides the average classification error rate at a specific number of selected genes ( reported in the first column ) .( PDF 16 KB ) .", "label": "", "metadata": {}, "score": "58.455635"}
{"text": "The proposed method achieved the highest averaged generalization ability compared to its counterparts and denoted an acceptable level of diversity among the learners for majority of the analyzed benchmark datasets .J. Khan , J. S. Wei , M. Ringn\u00e9r et al . , \" Classification and diagnostic prediction of cancers using gene expression profiling and artificial neural networks , \" Nature Medicine , vol .", "label": "", "metadata": {}, "score": "58.552544"}
{"text": "Participants were to build a single parsing system that is robust to domain changes and can handle noisy text that is commonly encountered on the web .There was a constituency and a dependency parsing track and 11 sites submitted a total of 20 systems .", "label": "", "metadata": {}, "score": "58.58273"}
{"text": "( PDF 15 KB ) .Competing interests .The authors declare that they have no competing interests .Authors ' contributions .OM designed , developed and implemented the POS method , conducted the experiments , developed the R code and wrote the manuscript .", "label": "", "metadata": {}, "score": "58.60035"}
{"text": "The POS score alone can rank genes according to their overlapping degree , without taking into account the class that has more correctly assigned samples by each gene ( which can be addressed as the dominant class of that gene ) .", "label": "", "metadata": {}, "score": "58.684566"}
{"text": "Widodo , A. ; Yang , B.S. Support vector machine in machine condition monitoring and fault diagnosis .Mech .Syst .Signal Process .[ Google Scholar ] .Abbasion , S. ; Rafsanjani , A. ; Farshidianfar , A. ; Irani , N. Rolling element bearings multi - fault classification based on the wavelet de - noising and support vector machine .", "label": "", "metadata": {}, "score": "58.68872"}
{"text": "Here we require more particles to capture the large search space of instances selection for using the standard PSO .As a result we can find the best solution faster than using a smaller particles size .On the other hand , the number of particle is decreased when we implement the CPSO method .", "label": "", "metadata": {}, "score": "58.73423"}
{"text": "Introduction .Previous studies have shown that gene microarray data analysis is a powerful and revolutionary tool for biological and medical researches by allowing the simultaneous monitoring of the expression levels of tens of thousands of genes [ 1 ] .This is done by measuring the signal intensity of fluorescing molecules attached to DNA species that are bound to complementary strands of DNA localized to the surface of the microarray .", "label": "", "metadata": {}, "score": "58.785583"}
{"text": "To cope with these challenges and to develop a more robust and accurate learning method , ensemble learning methodology is utilized .The datasets are first preprocessed , and then to reduce the computational complexity and select the most informative genes , FCBF is applied to these datasets .", "label": "", "metadata": {}, "score": "58.889023"}
{"text": "They are less computationally expensive than the wrapper methods .An example of this category is a classification tree based classifier [ 14 ] .Filter methods assess genes by calculating a relevant score for each gene .The low\u2010relevant genes are then removed .", "label": "", "metadata": {}, "score": "58.930252"}
{"text": "If other parameters than the intensity define the structures outline or area a simple threshold does often not lead to satisfying results or even fails completely doing the job .Many users are aware of a method which allows them to manually define such a threshold value but this comes with several limitations ... .", "label": "", "metadata": {}, "score": "59.03135"}
{"text": "Discussion .As described above , the system we developed for automated gene / protein name recognition uses the SVM algorithm and an SVM - based chunker , YamCha .YamCha performs pre - processing and set - up for the SVM .", "label": "", "metadata": {}, "score": "59.212627"}
{"text": "J. 1948 , 27 .[ Google Scholar ] .Ramakrishnan , N. C4.5 .In The Top Ten Algorithms in Data Mining ; Wu , X. , Kumar , V. , Eds . ; Chapman & Hall / CRC : London , UK , 2009 ; pp . 1 - 19 .", "label": "", "metadata": {}, "score": "59.216576"}
{"text": "Despite the much simplified training process , our acoustic model achieves state - of - the - art results on phone classification ( where it outperforms almost all other methods ) and competitive performance on phone recognition ( where it outperforms standard CD triphone / subphone / GMM approaches ) .", "label": "", "metadata": {}, "score": "59.242805"}
{"text": "The best balanced f - score was the case where all features were used .This suggests that the effect of each feature was small when the features were combined .Kudo et al .[14 ] suggested that an SVM has a high generalization performance independent of the dimension of the feature vectors .", "label": "", "metadata": {}, "score": "59.294662"}
{"text": "The mechanism of information sharing of CPSO is shown in Figure 1 .The cooperative search between one subswarm and other is achieved by sharing the information of the global best position ( . ) across all subswarm .Here the algorithm has the advantage of taking two steps forward because the candidate solution comes from the best position for all subswarm except only for the current subswarms being evaluated .", "label": "", "metadata": {}, "score": "59.304764"}
{"text": "Note .Your Turn : Modify the gender_features ( ) function to provide the classifier with features encoding the length of the name , its first letter , and any other features that seem like they might be informative .Retrain the classifier with these new features , and test its accuracy .", "label": "", "metadata": {}, "score": "59.381836"}
{"text": "For the ' Srbct ' data , the best stability scores among the compared methods are yielded by POS at most set sizes , see Table 7 .Table 7 .Stability scores of the feature selection techniques over 50 repetitions of 10\u2010fold cross validation for ' Srbct ' dataset .", "label": "", "metadata": {}, "score": "59.38842"}
{"text": "The Relief family methods are especially attractive because they may be applied in all situations , have low bias , include interaction among features , and may capture local dependencies that other methods miss [ 35 ] .The CFS method is based on test theory concepts and relies on a set of heuristics to assess the adequacy of subsets of features .", "label": "", "metadata": {}, "score": "59.395065"}
{"text": "1205 - 1224 , 2004 .View at Google Scholar .T. Li , C. Zhang , and M. Ogihara , \" A comparative study of feature selection and multiclass classfication methods for tissue classification based on gene expression , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "59.434685"}
{"text": "The listing in 2.1 shows how this can be done . def segment_sentences ( words ) : . sents.append(words[start:i+1 ] ) . sents.append(words[start : ] ) .return sents . 2.2 Identifying Dialogue Act Types .When processing dialogue , it can be useful to think of utterances as a type of action performed by the speaker .", "label": "", "metadata": {}, "score": "59.454926"}
{"text": "However , an examination of the generated model shows that only two of the four types are modeled in the decision tree , probably because the training size is too small .Performance Evaluation - Machine Fault Simulator ( MFS ) .", "label": "", "metadata": {}, "score": "59.47888"}
{"text": "These features indicate that all important words in the hypothesis are contained in the text , and thus there is some evidence for labeling this as True .The module nltk.classify.rte_classify reaches just over 58 % accuracy on the combined RTE test data using methods like these .", "label": "", "metadata": {}, "score": "59.56027"}
{"text": "In this case , we will have a harder time coming up with an appropriate distribution by hand ; however , we can verify that the following distribution looks appropriate : .Furthermore , the remaining probabilities appear to be \" evenly distributed .", "label": "", "metadata": {}, "score": "59.56763"}
{"text": "x .i .b . ) for .i .n .The optimal separating hyperplane is the separating hyperplane with greatest distance between the plane and the nearest data points on both sides , i.e. , the boundary in the middle of the maximum geometrical margin between the two classes .", "label": "", "metadata": {}, "score": "59.583855"}
{"text": "Measurement 2013 , 46 , 1551 - 1564 .[ Google Scholar ] .Boser , B.E. ; Guyon , I.M. ; Vapnik , V.N. A training algorithm for optimal margin classifiers .Proceedings of the Fifth Annual Workshop on Computational Learning Theory ( ACM ) , Pittsburgh , PA , USA , 27 - 29 July 1992 ; pp .", "label": "", "metadata": {}, "score": "59.591724"}
{"text": "Unfortunately , the number of possible tag sequences is quite large .Given a tag set with 30 tags , there are about 600 trillion ( 30 10 ) ways to label a 10-word sentence .In order to avoid considering all these possible sequences separately , Hidden Markov Models require that the feature extractor only look at the most recent tag ( or the most recent n tags , where n is fairly small ) .", "label": "", "metadata": {}, "score": "59.62558"}
{"text": "Our objective is to search for the minimum subset , denoted by , for which equals to the aggregate mask of the set of genes , .In other words , our minimum set of genes should satisfy the following statement : .", "label": "", "metadata": {}, "score": "59.66788"}
{"text": "Sub - swarm 1 represents the features ' column and subswarm 2 represents the instances ' row of the particular data set .Figure 4 illustrates the main difference between standard PSO and cooperative PSO .The standard PSO contains one swarm with a large dimension of search space .", "label": "", "metadata": {}, "score": "59.70009"}
{"text": "Conclusion .Overall , our experiments show that CRF models with carefully designed features can identify gene and protein mentions with fairly high accuracy even without features containing domain specific knowledge .However , such features , which in our case take the form of lexicon membership , can lead to improved system performance .", "label": "", "metadata": {}, "score": "59.707348"}
{"text": "M. P. S. Brown , W. N. Grundy , D. Lin et al . , \" Knowledge - based analysis of microarray gene expression data by using support vector machines , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "59.71325"}
{"text": "Average classification error rates yielded by Random Forest , k Nearest Neighbors and Support Vector Machine classifiers on ' GSE24514 ' dataset over all the 50 repetitions of 10\u2010fold cross validation .Boldface numbers indicate the minimum average of classification error rates ( the highest accuracy ) achieved with the corresponding classifier at each size of selected gene sets , reported in the first column .", "label": "", "metadata": {}, "score": "59.71845"}
{"text": "Here , for the sake of comparison , we experiment with both PCA and ICA transformation methods and will report on their efficiency for our gene microarray classification task later on .Gene Selection .Available training data sets for classification of cancer types generally have a fairly small sample size compared to the number of genes involved .", "label": "", "metadata": {}, "score": "59.764904"}
{"text": "99 , no .10 , pp .6562 - 6566 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Abstract .Background .Microarray technology , as well as other functional genomics experiments , allow simultaneous measurements of thousands of genes within each sample .", "label": "", "metadata": {}, "score": "59.809235"}
{"text": "Data mining techniques are grouped under different names in the machine diagnostics literature .In a more recent overview [ 22 ] , data mining techniques used in mechanical systems research were grouped under the term natural computing if they were motivated or suggested by biological systems or processes .", "label": "", "metadata": {}, "score": "59.854965"}
{"text": "The effect of dictionary matching feature was significant by itself .But when the features were combined , the effect was not so significant .Evaluation of different types of dictionary matching ( Table 10 ) showed that the differences between the types were small .", "label": "", "metadata": {}, "score": "59.860924"}
{"text": "Sugumaran , V. ; Ramachandran , K. Effect of number of features on classification of roller bearing faults using SVM and PSVM .Expert Syst .Appl .[ Google Scholar ] .Schwabacher , M. ; Goebel , K. A Survey of Artificial Intelligence for Prognostics .", "label": "", "metadata": {}, "score": "59.864872"}
{"text": "How likely is a given input value with a given label ?What is the most likely label for an input that might have one of two values ( but we do n't know which ) ?The Maximum Entropy classifier , on the other hand , is an example of a conditional classifier .", "label": "", "metadata": {}, "score": "59.89483"}
{"text": "Then , two measures are assigned for each gene : proportional overlapping score ( POS ) and relative dominant class ( RDC ) .Analogously to [ 7 ] these two novel measures are exploited in the ranking phase to produce the final set of ranked genes .", "label": "", "metadata": {}, "score": "59.92215"}
{"text": "In a real application such as machinery fault diagnostics and prognostics in a factory , the first phase i.e. , the initial establishment of the process involves identifying how many and which critical components to monitor , based on expert knowledge ; maintenance history ; costs and budget .", "label": "", "metadata": {}, "score": "59.940742"}
{"text": "The best accuracies were in the 80 - 84\\% range for F1 and LAS ; even part - of - speech accuracies were just above 90\\% .Coarse - to - fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy .", "label": "", "metadata": {}, "score": "59.962288"}
{"text": "Biomedical researchers may be interested in identifying small sets of genes that could be used as genetic markers for diagnostic purposes in clinical researches .This typically involves obtaining the smallest possible subset of genes that can still provide a good predictive performance , whilst removing redundant ones [ 21 ] .", "label": "", "metadata": {}, "score": "59.98533"}
{"text": "1183 - 1200 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Alcal\u00e1 - Fd\u00e9z , R. Alcal\u00e1 , and F. Herrera , \" A fuzzy associative classification system with genetic rule selection for high - dimensional problems , \" in Proceedings of the 4th International Workshop on Genetic and Evolutionary Fuzzy Systems ( GEFS ' 10 ) , pp .", "label": "", "metadata": {}, "score": "60.01895"}
{"text": "Our generative self - trained grammars reach F scores of 91.6 on the WSJ test set and surpass even discriminative reranking systems without self - training .Additionally , we show that multiple self - trained grammars can be combined in a product model to achieve even higher accuracy .", "label": "", "metadata": {}, "score": "60.022896"}
{"text": "This in turn improves classification accuracy with predominant selected features .-correlation .Using the sorted feature list , redundant features are eliminated one by one in a descending order .The remaining feature subset thus contains the predominant features with zero redundant features in terms of .", "label": "", "metadata": {}, "score": "60.04078"}
{"text": "To facilitate future research in unsupervised induction of syntactic structure and to standardize best - practices , we propose a tagset that consists of twelve universal part - of - speech categories .In addition to the tagset , we develop a mapping from 25 different treebank tagsets to this universal set .", "label": "", "metadata": {}, "score": "60.108147"}
{"text": "We define the parameters used in the 1 st run in the BioCreAtIvE competition as the \" base \" ( see sub - section BioCreAtIvE Competition and Table 4 ) .The other parameters did not have much effect on the results .", "label": "", "metadata": {}, "score": "60.14516"}
{"text": "However , as Tsuruoka et al .[ 12 ] reported , dictionary pattern matching can result in low recall in a biomedical domain due to spelling variations .We thus investigated and evaluated the performance of our system when using an additional feature of making partial dictionary pattern matches .", "label": "", "metadata": {}, "score": "60.251995"}
{"text": "Many forms of imaging require some form of illumination .You should ensure that this illumination is as evenly distributed as possible , rather than attempting to correct for it after acquisition .If you must tolerate an uneven illumination for some reason , try to acquire a background image so that you can use a background subtraction - but there may still be issues such as reflection artifacts .", "label": "", "metadata": {}, "score": "60.322502"}
{"text": "Primary acoustic , speech , and vision systems were trained to discriminate instances of the categories .Higher - level systems exploited correlations among the categories , incorporated sequential context , and combined the joint evidence from the three information sources .", "label": "", "metadata": {}, "score": "60.334076"}
{"text": "return features .Example 1.2 ( code_gender_features_overfitting .py ) : Figure 1.2 : A Feature Extractor that Overfits Gender Features .The feature sets returned by this feature extractor contain a large number of specific features , leading to overfitting for the relatively small Names Corpus .", "label": "", "metadata": {}, "score": "60.439568"}
{"text": "There are many probability distributions that we could choose for the ten senses , such as : .Although any of these distributions might be correct , we are likely to choose distribution ( i ) , because without any more information , there is no reason to believe that any word sense is more likely than any other .", "label": "", "metadata": {}, "score": "60.467392"}
{"text": "385 - 397 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. E. Gaweda , J. M. Zurada , and R. Setiono , \" Input selection in data - driven fuzzy modeling , \" in Proceedings of the 10th IEEE International Conference on Fuzzy Systems , vol .", "label": "", "metadata": {}, "score": "60.48779"}
{"text": "K. Kira and L. Rendell , \" A practical approach to feature selection , \" in Proceedings of the 9th International Workshop on Machine Learning , pp .249 - 256 , 1992 .C. Ding and H. Peng , \" Minimum redundancy feature selection from microarray gene expression data , \" in Proceedings of the 2nd IEEE Computational Systems Bioinformatics , pp .", "label": "", "metadata": {}, "score": "60.538864"}
{"text": "Each feature was represented as either Y ( matching ) or N ( not matching ) .The dictionary was constructed based on the gene / protein names from the SWISS - PROT and TrEMBL databases .The SWISS - PROT database is a protein knowledge base including amino acid sequences and other properties currently known about the proteins .", "label": "", "metadata": {}, "score": "60.544933"}
{"text": "The two semantic schemas are different but compatible .When the Vanderbilt team applied MedEx to the i2b2 challenge , they customized and extended MedEx to label medication - related fields as required by i2b2 [ 11 ] .There are three main customizations : 1 ) developing dedicated rules to combine entities recognized by MedEx into i2b2 fields , 2 ) adding a new section tagger and a spell checker , and 3 ) developing a post - processing program to convert MedEx outputs into the i2b2 challenge outputs .", "label": "", "metadata": {}, "score": "60.603104"}
{"text": "10 , pp .906 - 914 , 2000 .View at Google Scholar \u00b7 View at Scopus .N. Friedman , M. Linial , I. Nachman , and D. Pe'er , \" Using Bayesian networks to analyze expression data , \" in Proceedings of the 4th Annual International Conference on Computational Molecular Biology ( RECOMB ' 00 ) , pp .", "label": "", "metadata": {}, "score": "60.610863"}
{"text": "According to equation 9 , values of POS measure are and for genes i 1 and i 2 in Figures 3 a and 3 b respectively .Larger overlapping intervals or higher numbers of overlapping samples results in an increasing POS value .", "label": "", "metadata": {}, "score": "60.6129"}
{"text": "Experiments were run in a Linux machine with 4 GB RAM and 4 cores of Intel Xeon 2.0GHz .Results for the first setting : 10-fold cross - validation .First , we evaluated the effectiveness of the features as well as their combinations .", "label": "", "metadata": {}, "score": "60.671253"}
{"text": "1284 - 1294 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . F. Wan , H. Shang , L. X. Wang , and Y. X. Sun , \" How to determine the minimum number of fuzzy rules to achieve given accuracy : a computational geometric approach to SISO case , \" Fuzzy Sets and Systems , vol .", "label": "", "metadata": {}, "score": "60.68672"}
{"text": "The most popular ensemble methods utilize a base classification algorithm to differently permutated training sets .Examples of these techniques include AdaBoost , Bagging , Random Subspace , Random Forest , and Rotation Forest [ 19 ] .AdaBoost has become a very popular choice for its simplicity and adaptability [ 20 ] .", "label": "", "metadata": {}, "score": "60.693012"}
{"text": "As it can be seen , the number of selected genes for each processed gene dataset is different and depends on the choice of a feature selection algorithm .It should be noted that both mRMR and GSNR algorithms provide an ordered list of the initial genes ( features ) according to the genes importance and discrimination power .", "label": "", "metadata": {}, "score": "60.796318"}
{"text": "This work was supported in part by NSF grant ITR 0205448 .Authors ' Affiliations .Department of Computer and Information Science , University of Pennsylvania , Levine Hall .References .Ohta T , Tateisi Y , Kim J , Lee S , Tsujii J : GENIA corpus : A semantically annotated corpus in molecular biology domain .", "label": "", "metadata": {}, "score": "60.84504"}
{"text": "It is worth to note that the feature selection is then carried out using only the training samples .Finally , the test error ( classification accuracy ) is estimated on the unseen test samples using ( 3 ) .Table 3 presents the RotBoost mean classification accuracy against the considered 8 gene datasets when transformation matrix is chosen to be either PCA or ICA where the values following \" \u00b1 \" denote the related standard deviations .", "label": "", "metadata": {}, "score": "60.858826"}
{"text": "When considering each field separately , significant differences in performance between the two systems have been observed for the dosage , frequency , and duration fields : the SVM - based system performed better for dosage and frequency while the CRF - based system performed better for duration .", "label": "", "metadata": {}, "score": "60.861893"}
{"text": "149 - 186 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .F. J. Berlanga , A. J. Rivera , M. J. del Jesus , and F. Herrera , \" GP - COACH : genetic Programming - based learning of COmpact and ACcurate fuzzy rule - based classification systems for High - dimensional problems , \" Information Sciences , vol .", "label": "", "metadata": {}, "score": "60.884697"}
{"text": "In the DIC column , the first three items show the results ( Y or N ) of the uni- , bi- , and tri - gram ( in token ) matching for the protein names in the dictionary .The last item shows the result of the uni - gram ( in token ) matching for the gene names in the dictionary .", "label": "", "metadata": {}, "score": "60.914085"}
{"text": "Of the various types of kernels available , we used a d - th polynomial kernel : .Authors ' Affiliations .Graduate School of Information Science , Nara Institute of Science and Technology .National Institute of Information and Communications Technology .", "label": "", "metadata": {}, "score": "60.950333"}
{"text": "Three different classification strategies illustrated with a 3-class problem ( a ) Simple multi - class classification ; ( b ) One- versus -one ( OVO ) class binarization ; ( c ) One- versus -all ( OVA ) class binarization .", "label": "", "metadata": {}, "score": "60.972965"}
{"text": "For document topic identification , we can define a feature for each word , indicating whether the document contains that word .To limit the number of features that the classifier needs to process , we begin by constructing a list of the 2000 most frequent words in the overall corpus .", "label": "", "metadata": {}, "score": "61.0009"}
{"text": "The Maximum Entropy classifier uses a model that is very similar to the model employed by the naive Bayes classifier .But rather than using probabilities to set the model 's parameters , it uses search techniques to find a set of parameters that will maximize the performance of the classifier .", "label": "", "metadata": {}, "score": "61.149277"}
{"text": "In other words , there are interactions between different bearing defect types and this complicates the bearing fault diagnosis process .However , in terms of problem formulation , if the interaction between defect types is small enough , combined defects can be identified from single defect classifiers , with an OVA class binarization strategy .", "label": "", "metadata": {}, "score": "61.16588"}
{"text": "This decoding stage can be done in an efficient way using the Viterbi algorithm .In this study , we used the CRF++ tool which was developed at NAIST and is freely available [ 23 ] .Below we describe the features of the CRF and SVM models .", "label": "", "metadata": {}, "score": "61.16795"}
{"text": "The concurrent - defect classification performance is then evaluated on the potential to enable multiple - defect bearing diagnostics from single - defect training data .The diagnosis accuracy of random guess classification and worst - case classification are also considered .", "label": "", "metadata": {}, "score": "61.183525"}
{"text": "Sotiris , V. ; Tse , P. ; Pecht , M. Anomaly detection through a bayesian support vector machine .IEEE Trans .Reliab .[ Google Scholar ] .Samanta , B. ; Al - Balushi , K. ; Al - Araimi , S. Artificial neural networks and support vector machines with genetic algorithm for bearing fault detection .", "label": "", "metadata": {}, "score": "61.202793"}
{"text": "[ Google Scholar ] .Yang , Z. ; Cai , L. ; Gao , L. ; Wang , H. Adaptive redundant lifting wavelet transform based on fitting for fault feature extraction of roller bearings .Sensors 2012 , 12 , 4381 - 4398 .", "label": "", "metadata": {}, "score": "61.203384"}
{"text": "Wu , X. ; Kumar , V. The Top Ten Algorithms in Data Mining ; Chapman & Hall / CRC : London , UK , 2009 ; Volume 9 .[ Google Scholar ] .Wang , G. ; Liu , C. ; Cui , Y. Clustering diagnosis of rolling element bearing fault based on integrated Autoregressive / Autoregressive Conditional Heteroscedasticity model .", "label": "", "metadata": {}, "score": "61.24242"}
{"text": "The difference between a generative model and a conditional model is analogous to the difference between a topographical map and a picture of a skyline .Although the topographical map can be used to answer a wider variety of questions , it is significantly more difficult to generate an accurate topographical map than it is to generate an accurate skyline .", "label": "", "metadata": {}, "score": "61.248077"}
{"text": "Section ' Methods ' explains the proposed method .The results of our approach are compared with some other feature selection techniques in section ' Results and discussion ' .Section ' Conclusion ' concludes the paper and suggests future directions .", "label": "", "metadata": {}, "score": "61.2711"}
{"text": "Future work includes data collection and further analysis of the proposed approach with other combined defects such as BPFI BSF combined ; BPFO BSF combined ; and all three types combined .Acknowledgments .This research was supported in part by the Centre for Systems Informatics Engineering ( CSIE ) ( CityU # 9360144 ) ; HK RGC CRF ( # 8730029 ) ; HK RGC GRFs ( # 9041682 and # 9041578 ) ; and CityU grant SRG - Fd ( # CityU122613 ) .", "label": "", "metadata": {}, "score": "61.304245"}
{"text": "Knowl .Data Eng .[ Google Scholar ] .Bishop , C.M. ; Nasrabadi , N.M. Pattern Recognition and Machine Learning ; Springer : New York , NY , USA , 2006 .[ Google Scholar ] .Adriaans , P. ; Zantinge , D. Data Mining ; Addison - Wesley : Harlow , UK , 1996 .", "label": "", "metadata": {}, "score": "61.34073"}
{"text": "Proceedings of Pacific Symposium on Biocomputing 2003 .Kazama J , Makino T , Ohta Y , Tsujii J : Tuning Support Vector Machines for Biomedical Named Entity Recognition .Proceedings of Natural Language Processing in the Biomedical Domain , ACL 2002 .", "label": "", "metadata": {}, "score": "61.35878"}
{"text": "View at Google Scholar \u00b7 View at Scopus . H. Wang , S. Kwong , Y. Jin , W. Wei , and K. F. Man , \" Multi - objective hierarchical genetic algorithm for interpretable fuzzy rule - based knowledge extraction , \" Fuzzy Sets and Systems , vol .", "label": "", "metadata": {}, "score": "61.36116"}
{"text": "Abstract .The gene microarray analysis and classification have demonstrated an effective way for the effective diagnosis of diseases and cancers .However , it has been also revealed that the basic classification techniques have intrinsic drawbacks in achieving accurate gene classification and cancer diagnosis .", "label": "", "metadata": {}, "score": "61.365406"}
{"text": "Table 2 .Average classification error rates yielded by Random Forest , k Nearest Neighbors and Support Vector Machine classifiers on ' Leukaemia ' dataset over all the 50 repetitions of 10\u2010fold cross validation .Boldface numbers indicate the minimum average of classification error rates ( the highest accuracy ) achieved with the corresponding classifier at each size of selected gene sets , reported in the first column .", "label": "", "metadata": {}, "score": "61.365677"}
{"text": "Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm .", "label": "", "metadata": {}, "score": "61.442226"}
{"text": "Results for the second setting : The 2009 i2b2 challenge with Sydney training data .In order to compare our results to the first - ranked system ( the Sydney team from the 2009 i2b2 challenge ) , we used the same training dataset from their system and applied it to the standard test set .", "label": "", "metadata": {}, "score": "61.453438"}
{"text": "Behavior Control : Finally we show how all these elements can be incorporated into a goal keeping robot .We develop simple behaviors that can be used in a layered architecture and enable the robot to block most balls that are being shot at the goal .", "label": "", "metadata": {}, "score": "61.50749"}
{"text": "Lyon : IEEE ; 2007:4227 - 4230 .View Article .Apiletti D , Baralis E , Bruno G , Fiori A : Maskedpainter : feature selection for microarray data analysis .Intell Data Anal 2012 , 16 ( 4 ) : 717 - 737 .", "label": "", "metadata": {}, "score": "61.524994"}
{"text": "YamCha is a general purpose SVM - based chunker .YamCha takes in the training and test data and formats it for the SVM .The format of YamCha for a sample phrase is shown in Figure 2 .The phrase is written vertically in the WORD column .", "label": "", "metadata": {}, "score": "61.572556"}
{"text": "[ 10 ] investigated automated recognition based on a support vector machine ( SVM ) .Features for recognizing named entities were proposed in these investigations , e.g. word , part - of - speech ( POS ) , and orthography .", "label": "", "metadata": {}, "score": "61.58837"}
{"text": "This last statement can be understood in terms of a single image from which an experimenter may want to extract overall cell shape in one investigation but nuclear texture in another .Every basic method needs to be tested to determine if any of its implementations do a good job for every single new question to be answered .", "label": "", "metadata": {}, "score": "61.59369"}
{"text": "However , the smaller \u03b7 is , the slower the algorithm will converge .Furthermore , gradient ascent does not take into account the curvature of the function , which also fundamentally slows its convergence speed .In order to consider the function 's curvature , we require second order derivative information in the form of a Hessian .", "label": "", "metadata": {}, "score": "61.59481"}
{"text": "The new Viewer adds three features for more powerful search : wildcards , morphological inflections , and capitalization .These additions allow the discovery of patterns that were previously difficult to find and further facilitate the study of linguistic trends in printed text .", "label": "", "metadata": {}, "score": "61.657833"}
{"text": "Introduction .In scientific image processing and image analysis , an image is something different than a regular digital photograph of a beautiful scene you shot during your latest vacation .In the context of science , digital images are samples of information , sampled at vertex points of n -dimensional grids .", "label": "", "metadata": {}, "score": "61.725918"}
{"text": "Ultsch et al .[ 15 ] propose an algorithm , called ' PUL ' , in which the differentially expressed genes are identified based on a measure for retrieval information named PUL\u2010score .Ding et al .[ 18 ] propose a framework , named ' minimal redundancy maximal relevance ( mRMR ) ' based on a series of intuitive measures of relevance , to the response target , and redundancy , between genes being selected .", "label": "", "metadata": {}, "score": "61.752396"}
{"text": "Gao , J. Digital Analysis of Remotely Sensed Imagery ; McGraw - Hill Professional : New York , NY , USA , 2008 .[ Google Scholar ] .Chen , M. ; Zheng , A. ; Lloyd , J. ; Jordan , M. ; Brewer , E. Failure diagnosis using decision trees .", "label": "", "metadata": {}, "score": "61.790558"}
{"text": "L. Kuncheva and J. Rodriguez , \" An experimental study on rotation forest ensembles , \" in Multiple Classifier Systems , vol .4472 of Lecture Notes on Computer Science , pp .459 - 468 , 2007 .View at Google Scholar .", "label": "", "metadata": {}, "score": "61.794716"}
{"text": "Each combination of labels and features that receives its own parameter is called a joint - feature .Note that joint - features are properties of labeled values , whereas ( simple ) features are properties of unlabeled values .Note .", "label": "", "metadata": {}, "score": "61.835835"}
{"text": "Baseline : Multi - Class Classification .Table 9 shows the prediction results of the multi - class classification with C4.5 .The correct fault diagnoses are highlighted in bold .Multiple Defects : One- Versus -All ( OVA ) Class Binarization Performance .", "label": "", "metadata": {}, "score": "61.849148"}
{"text": "Most of the population - based algorithm approaches in FS and IS are based on GAs .Some recent studies [ 28 , 29 , 31 ] have employed population - based optimization techniques to carry out search for the best subset of variables and data for solving the application problems , but all of them were carried out to solve the classification problem .", "label": "", "metadata": {}, "score": "61.852036"}
{"text": "The experiments were repeated 10 times , and the reported results are the average RMSE values .We also report the values of the standard deviation of the performance index to offer a better insight into the variability of the performance .", "label": "", "metadata": {}, "score": "61.877815"}
{"text": "[16 ] propose another criterion to identify the informative genes in which principle component analysis has been used to explore the sources of variation in the expression data and to filter out genes corresponding to components with less variation .Tallon et al .", "label": "", "metadata": {}, "score": "61.904373"}
{"text": "Proceedings of the National Conference on Artificial Intelligence AAAI Press 1994 , 722 - 727 .Yeh A : More accurate tests for the statistical significance of result differences . 18thInternational Conference on Computational Linguistics ( COLING 2000 ) 2000 , 947 - 953 .", "label": "", "metadata": {}, "score": "61.904896"}
{"text": "Other ensembles were constructed based on characteristics of NE fields and our results of single CRF - based or SVM - based NER systems for individual fields in the first experimental setting ( described below ) , especially for duration and reason .", "label": "", "metadata": {}, "score": "61.96254"}
{"text": "Figure 2 illustrates these histograms against the typical Lung cancer dataset .Figure 2 : Kappa error diagrams for the Lung dataset using different ensemble algorithms .We perform these experiments on all 8 gene datasets , and overall the ICA and PCA RotBoost methods perform best in terms of accuracy with average classification accuracy about 98.0 % and 96.3 % , respectively .", "label": "", "metadata": {}, "score": "61.981354"}
{"text": "Let be a set containing all genes ( i.e. , ) .Also , let be its aggregate mask which is defined as the logical disjunction ( logic OR ) between all masks corresponding to genes that belong to the set .", "label": "", "metadata": {}, "score": "61.982063"}
{"text": "Keywords .Feature selection Gene ranking Microarray classification Proportional overlap score Gene mask Minimum subset of genes .Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1471 - 2105 - 15 - 274 ) contains supplementary material , which is available to authorized users .", "label": "", "metadata": {}, "score": "62.000256"}
{"text": "Table 5 .NS means \" not significant different \" .Among the six NE fields , duration and reason proved to be the most difficult , with F - scores not exceeding 69 % and 58 % respectively by all experimented methods .", "label": "", "metadata": {}, "score": "62.01097"}
{"text": "Proceedings of the ACL 2003 Workshop on NLP in Biomedicine 2003 , 49 - 56 .Kazama J , Makino T , Ohta Y , Tsujii J : Tuning Support Vector Machines for Biomedical Named Entity Recognition .Proceedings of the Natural Language Processing in the Biomedical Domain ( ACL2002 ) 2002 , 1 - 8 .", "label": "", "metadata": {}, "score": "62.028927"}
{"text": "Here the standard fuzzy model is constructed without using any feature and instances selection and the holdout method is used to select the data based on the percentage given .The experiment for using the standard fuzzy modeling is repeated for 50 times .", "label": "", "metadata": {}, "score": "62.048244"}
{"text": "The \" one - vs .-rest \" method was used for solving multi - class problems .The parenthesized values are the p - values .The values in bold font have a statistically significant difference from the base value .", "label": "", "metadata": {}, "score": "62.171227"}
{"text": "In this method , the vector definition with individual staining components is an important step ; it is nicely implemented in the tool and should be done for the sake of consistency and accuracy .A limitation inheres in using the definition by ROIs , which again introduces low reproducibility due to reliance on biased , user defined areas of \" a color \" .", "label": "", "metadata": {}, "score": "62.172104"}
{"text": "Effects of tuning parameters on system performance .The \" base \" parameters are shown in Table 4 .The \" deg . \" means the degree of the polynomial kernel , the \" win .\" is the window range , and \" cv \" is the cost of constraint violation .", "label": "", "metadata": {}, "score": "62.186592"}
{"text": "17 , pp .4963 - 4967 , 2002 .View at Google Scholar \u00b7 View at Scopus .X. Wang , M. J. Hessner , Y. Wu , N. Pati , and S. Ghosh , \" Quantitatative quality control in microarray experiments and the application in data filtering , normalization and false positive rate prediction , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "62.205845"}
{"text": "In this table , the combination of the word and preceding class feature is defined as the \" base \" .None of the features affected precision much , while all of them affected recall .Effect of dictionary matching methods .", "label": "", "metadata": {}, "score": "62.21014"}
{"text": "The proposed POS measure and relative dominant class assignments .A novel overlapping score is developed to estimate the overlapping degree between different expression intervals .Figures 3 a and 3 b represent examples of 2 different genes , i 1 and i 2 , with the same length of overlap interval , , length of total core interval , , and total number of overlapped samples , .", "label": "", "metadata": {}, "score": "62.233494"}
{"text": "RotBoost offers a potential computational advantage over AdaBoost in that it has the ability to execute in parallel .In fact , each subensemble classifier formed by AdaBoost can be learned independently of the other ones .Pseudocode 1 illustrates this algorithm . . .", "label": "", "metadata": {}, "score": "62.478966"}
{"text": "The numbers are the overlap between the row / column items .@@11506 With / IN a / DT cutoff / NN level / NN for / IN TSST-1/NEWGENE of / IN ... were / VBD positive / JJ for / IN TSST-1/NEWGENE .", "label": "", "metadata": {}, "score": "62.51338"}
{"text": "The POS approach provides improvements over the best method of the compared techniques for most datasets with all classifiers , see panels of RF , k NN and SVM in Figure 5 .On average across all datasets , POS achieves an improvement over the best compared techniques at all set sizes for RF classifier by between 0.055 and 0.720 , measured by the log ratio of the error rates .", "label": "", "metadata": {}, "score": "62.56273"}
{"text": "Due to limitations of the R package ' mRMRe ' [ 19 ] , mRMR selections could not be conducted for datasets having more than ' 46340 ' features .Therefore , mRMR method is excluded from the analysis of the ' GSE14333 ' and ' GSE27854 ' datasets .", "label": "", "metadata": {}, "score": "62.59081"}
{"text": "Settles B : Biomedical named entity recognition using conditional random fields and rich feature sets .Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ( NLPBA / BioNLP )2004 , 104 - 107 .", "label": "", "metadata": {}, "score": "62.60318"}
{"text": "In both data sets , the gene / protein names were tagged with NEWGENE .Other tokens were tagged with POS tags .An example is the following : \" translocation / NN of / IN the / DT NF - kappaB / NEWGENE transcription / NEWGENE factor / NEWGENE,/ , \" , where NN is a noun or singular mass , IN is a preposition or subordinating conjunction , and DT is a determiner .", "label": "", "metadata": {}, "score": "62.65718"}
{"text": "In this study , we use a technique of particle swarm optimization ( PSO ) as an optimization vehicle of forming a subset of features and data ( instances ) to design a fuzzy model .Given the dimensionality of the problem ( as the search space involves both features and instances ) , we discuss a cooperative version of the PSO along with a clustering mechanism of forming a partition of the overall search space .", "label": "", "metadata": {}, "score": "62.67165"}
{"text": "Rule - based method .MedEx was originally developed at Vanderbilt University for extracting medication information from clinical text [ 16 ] .It has two main components : a semantic tagger which uses a lexicon and rules to recognize entities , and a parser which uses a semantic grammar to output structured medication information .", "label": "", "metadata": {}, "score": "62.681366"}
{"text": "First , we construct a list of documents , labeled with the appropriate categories .For this example , we 've chosen the Movie Reviews Corpus , which categorizes each review as positive or negative .words(fileid ) ) , category ) ... for category in movie_reviews . categories ( ) ... for fileid in movie_reviews .", "label": "", "metadata": {}, "score": "62.689125"}
{"text": "Extraction of clinical information such as medications or problems from clinical text is an important task of clinical natural language processing ( NLP ) .Rule - based methods are often used in clinical NLP systems because they are easy to adapt and customize .", "label": "", "metadata": {}, "score": "62.68992"}
{"text": "O : Current token is outside of any chunk .The resulting chunking representation of the above sample phrase is \" translocation / O of / O the / O NF - kappaB / B transcription / I factor / I,/O \" .", "label": "", "metadata": {}, "score": "62.6959"}
{"text": "The relative dominant class ( RDC ) of a gene is the class that has the highest proportion , relative to class sizes , of correctly assigned samples .Definition of core intervals .For a certain gene i , by considering the expression values x i j with a class label c j for each sample j , we can define two expression intervals , one for each class , for that gene .", "label": "", "metadata": {}, "score": "62.80639"}
{"text": "We briefly elaborate on the selected approaches to data and feature space reduction in Section 2 , and then in Section 3 , we recall the main algorithmic features of PSO and its cooperative version , CPSO , which is of interest in problems of high - dimensionality .", "label": "", "metadata": {}, "score": "62.881676"}
{"text": "Early detection and diagnosis ability : Achieve a balance in the trade - off between quick response and high false alarm rate .Fault isolation ability : Be able to discriminate between different failures at different locations and different levels .Robustness : To maintain performance at an acceptable level under noise and uncertainty .", "label": "", "metadata": {}, "score": "62.884956"}
{"text": "ACM SIGKDD Explor .Newsl .[ Google Scholar ] .Provost , F. ; Kolluri , V. A survey of methods for scaling up inductive algorithms .Data Min .Knowl .Discov .[ Google Scholar ] .Roddick , J. ; Spiliopoulou , M. A survey of temporal knowledge discovery paradigms and methods .", "label": "", "metadata": {}, "score": "62.928295"}
{"text": "When performing classification tasks with three or more labels , it can be informative to subdivide the errors made by the model based on which types of mistake it made .A confusion matrix is a table where each cell [ i , j ] indicates how often label j was predicted when the correct label was i .", "label": "", "metadata": {}, "score": "62.934357"}
{"text": "The \" -word \" column in Table 8 shows the case in which the word feature was ignored .The other columns have a similar meaning .The suffix ( preceding class ) feature greatly affected recall ( precision ) .The other features had little effect .", "label": "", "metadata": {}, "score": "62.938004"}
{"text": "In the table we used a smaller size of generation compared to particles size .This is because in [ 34 ] Shi and Eberhart mentioned that the population size does not exhibit any significant impact on the performance of the PSO method .", "label": "", "metadata": {}, "score": "62.953133"}
{"text": "PubMed View Article .Ding C , Peng H : Minimum redundancy feature selection from microarray gene expression data .J Bioinform Comput Biol 2005 , 3 ( 02 ) : 185 - 205 .PubMed View Article .De Jay N , Papillon\u2010Cavanagh S , Olsen C , El\u2010Hachem N , Bontempi G , Haibe\u2010Kains B : mrmre : an r package for parallelized mrmr ensemble feature selection .", "label": "", "metadata": {}, "score": "62.964424"}
{"text": "The best system for 10-fold cross validation is the CRF - based voting system and the best system for held - out testing set is the majority voting .Since the training and testing data in the two experimental settings are different , it is difficult to choose the best voting method for all data .", "label": "", "metadata": {}, "score": "62.97845"}
{"text": "Introduction .In fuzzy modeling , the two main approaches for generating the rules rely on knowledge acquisition from human experts and knowledge discovery from data [ 1 , 2 ] .In recent years , knowledge discovery from data or data - driven fuzzy modeling has become more important [ 2 - 4 ] .", "label": "", "metadata": {}, "score": "63.033257"}
{"text": "There was surprisingly a drop in the balanced f - score when switching from the original POS tagger to the GENIA trained POS tagger .One of the reasons may be that the GENIA corpus is smaller than the other training corpus .", "label": "", "metadata": {}, "score": "63.0507"}
{"text": "Throughout the paper , we compare our results against the top - ranked state - of - the - art system in the i2b2 challenge task from the Sydney group .Methods .Data sets .At the beginning of the 2009 i2b2 challenge , a data set of 696 notes was provided by the organizers .", "label": "", "metadata": {}, "score": "63.110268"}
{"text": "Results and Discussion .The structure of this subsection is the same as Section 4.2 for the other data set , except that the worst - case classification accuracy for this data set is the same as the random guess classification accuracy .", "label": "", "metadata": {}, "score": "63.137978"}
{"text": "Effective naming schemes are easy to read by both humans and computers .The following examples give an overview over the principles of creating good naming schemes : .Example A : .Sample preparation .In certain cases ( Examples ... ) it is very helpful to add markers to the slides .", "label": "", "metadata": {}, "score": "63.140923"}
{"text": "Table 2 summarizes how the data are used in training each of the OVA binarized classifier .Note that applying the same strategy to normal cases simply generates the fault detection classifier that handles Normal versus notNormal .Table 3 illustrates how outputs of three classifiers , namely isBPFO ; isBPFI ; isBSF , can be interpreted together to determine the state - of - health of a bearing with all possible single defects and multiple defects .", "label": "", "metadata": {}, "score": "63.16606"}
{"text": "p is ' NEWGENE ' if t is part of a gene / protein name , otherwise , p gives the part - of - speech for t : .Table 6 .Overlap of gene / protein names between any two data items .", "label": "", "metadata": {}, "score": "63.20691"}
{"text": "More extensive survey of these methods can be found in the machine prognostics literature such as [ 35 , 36 ] .Fault Diagnostic Systems for Industrial Applications .No matter which approach is used in the fault diagnosis , some common requirements exist for an ideal fault diagnostic system .", "label": "", "metadata": {}, "score": "63.21258"}
{"text": "Typically , the joint - features that are used to construct Maximum Entropy models exactly mirror those that are used by the naive Bayes model .In particular , a joint - feature is defined for each label , corresponding to w [ label ] , and for each combination of ( simple ) feature and label , corresponding to w [ f , label ] .", "label": "", "metadata": {}, "score": "63.242477"}
{"text": "[ Google Scholar ] .Li , Z. ; Wu , Z. ; He , Y. ; Fulei , C. Hidden Markov model - based fault diagnostics method in speed- up and speed - down process for rotating machinery .Mech .", "label": "", "metadata": {}, "score": "63.304356"}
{"text": "A data sample for a test is the result of interest from one of the 10 trials in a 10-fold cross validation .Each sample was the difference between a trial 's result for the case being compared and the corresponding result for the \" base \" case ( a matched pair ) .", "label": "", "metadata": {}, "score": "63.314182"}
{"text": "For GSE24514 dataset , Figure 7 depicts the stability scores of compared feature selection techniques at different set sizes .Unlike the mRMR and the MP approaches , both the Wil\u2010RS and the POS methods keep their stability degree for different sizes of feature sets .", "label": "", "metadata": {}, "score": "63.31434"}
{"text": "In Advances in Kernel Methods - Support .Vector Learning ; Schoelkopf , B. , Burges , C. , Smola , A. , Eds . ; MIT Press : Cambridge , MA , USA , 1998 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "63.44342"}
{"text": "The mRMR criterion computes both the redundancy between features and the relevance of each feature .Redundancy is computed by the mutual information ( MI ) between pairs of features whereas relevance is measured by the MI between each feature and the class labels .", "label": "", "metadata": {}, "score": "63.48051"}
{"text": "[ Google Scholar ] .Barkov , A. ; Barkova , N. Condition assessment and life prediction of rolling element bearings .Sound Vib .[ Google Scholar ] .Figure 1 .Three different classification strategies illustrated with a 3-class problem ( a ) Simple multi - class classification ; ( b ) One- versus -one ( OVO ) class binarization ; ( c ) One- versus -all ( OVA ) class binarization .", "label": "", "metadata": {}, "score": "63.539314"}
{"text": "This process is illustrated in 5.2 and 5.3 .Figure 5.2 : Calculating label likelihoods with naive Bayes .Naive Bayes begins by calculating the prior probability of each label , based on how frequently each label occurs in the training data .", "label": "", "metadata": {}, "score": "63.545578"}
{"text": "We note that these features are different from those of the Sydney team when they used CRF - based methods .The Sydney team used six features corresponding to six NE classes and a window size of five .In order to extract those features , they used heuristic methods such as dictionary or gazetteer look - up for NE phrases [ 10 ] .", "label": "", "metadata": {}, "score": "63.54663"}
{"text": "Sensors 2012 , 12 , 5919 - 5939 .[ Google Scholar ] .Zhang , S. ; Mathew , J. ; Ma , L. ; Sun , Y. Best basis - based intelligent machine fault diagnosis .Mech .Syst .", "label": "", "metadata": {}, "score": "63.58419"}
{"text": "In other words , selecting a large number of features may allow a classifier to compensate for potential feature selection shortcomings .For the purpose of comparing the effectiveness of the considered feature selection techniques in improving the classification accuracy , the experiment is designed to focus on small sets of selected features , up to 50 genes .", "label": "", "metadata": {}, "score": "63.609142"}
{"text": "Smith et al .[ 4 ] showed that most of the top NER systems in the BioCreAtIvE II challenge for gene mention tagging combined results from multiple classifiers using simple heuristic rules .In a similar way , Torii et al .", "label": "", "metadata": {}, "score": "63.699024"}
{"text": "A of BioCreAtIvE , a competition for automated gene / protein name recognition .Results .In the work presented here , our recognition system uses the feature set of the word , the part - of - speech ( POS ) , the orthography , the prefix , the suffix , and the preceding class .", "label": "", "metadata": {}, "score": "63.704693"}
{"text": "Here , tokens is a merged list of tokens from the individual sentences , and boundaries is a set containing the indexes of all sentence - boundary tokens .Next , we need to specify the features of the data that will be used in order to decide whether punctuation indicates a sentence - boundary : . isupper ( ) , ... ' prev - word ' :", "label": "", "metadata": {}, "score": "63.72152"}
{"text": "The 3 rd run was the worst .The results without dictionary matching are shown as case 2 in Table 5 .The score went up about 2.5 % when dictionary matching was used .Table 5 .Evaluation results .Case 1 is using the dictionary feature .", "label": "", "metadata": {}, "score": "63.776134"}
{"text": "Real - world problems often involve multiple classes , but some classification algorithms , such as SVM , are inherently binary .Class binarization strategies reduce a k - class problem into a series of binary problems for classification .Two most common strategies in the literature are one- versus -one ( OVO ) and one- versus -all ( OVA ) [ 64 ] , also named one - against - one ( OAO ) and one - against - all ( OAA ) [ 65 ] .", "label": "", "metadata": {}, "score": "63.87675"}
{"text": "When the number of features becomes very large , the filter model is usually chosen due to its computational efficiency .Here , we utilize fast correlation - based filter ( FCBF ) as previous experiments [ 30 ] suggest that FCBF is an efficient and fast feature selection algorithm for classification of high dimensional data .", "label": "", "metadata": {}, "score": "63.877125"}
{"text": "The idea is that a certain gene i can assign samples ( patients ) to class c because their gene i expression interval in that class is not overlapping with gene i intervals of the other class .In other words , gene i has the ability to correctly classify samples for which their gene i expressions fall within the expression interval of a single class .", "label": "", "metadata": {}, "score": "63.887947"}
{"text": "Keywords : . bearing ; multiple defects ; fault diagnostics ; class binarization ; support vector machine ( SVM ) ; decision tree .Introduction .In bearing diagnostics using a data - driven modeling approach , a concern is the need for data from all possible scenarios to build a practical model for all operating conditions .", "label": "", "metadata": {}, "score": "63.967438"}
{"text": "Those ones are mostly based on statistical approaches .Some are implemented in Fiji such as the \" Statistical Region Merging \" plugin or the \" Linear Kuwahara filter \" .For true color images there exist even more sophisticated classification methods often based on machine learning and training by the user with a set of exemplary test images .", "label": "", "metadata": {}, "score": "63.999847"}
{"text": "On full - scale treebank parsing experiments , the discriminative latent models outperform both the comparable generative latent models as well as the discriminative non - latent baselines .We present a maximally streamlined approach to learning HMM - based acoustic models for automatic speech recognition .", "label": "", "metadata": {}, "score": "64.10092"}
{"text": "Wang , H. ; Chen , P. A feature extraction method based on information theory for fault diagnosis of reciprocating machinery .Sensors 2009 , 9 , 2415 - 2436 .[ Google Scholar ] .Xia , Z. ; Xia , S. ; Wan , L. ; Cai , S. Spectral regression based fault feature extraction for bearing accelerometer sensor signals .", "label": "", "metadata": {}, "score": "64.10991"}
{"text": "The microarray data measures the expressions of tens of thousands of genes , producing a feature vector that is high in dimensionality and that contains much irrelevant information .This dimensionality degrades classification performance .Moreover , datasets typically contain few samples for training ( e.g. , lung dataset [ 12 ] contains 12535 genes and only 181 samples ) , leading to the curse of dimensionality problem .", "label": "", "metadata": {}, "score": "64.138626"}
{"text": "A bearing diagnostic method that is able to identify both single and multiple defects is more useful in real - world applications .In classification , training data for all system states to be recognized by a model are usually necessary [ 38 ] .", "label": "", "metadata": {}, "score": "64.16829"}
{"text": "The best F - score is underlined .Second , we compared the performance of MedEx , CRF - based , and SVM - based NER systems ( the CRF - based and SVM - based systems used all six types of features ) .", "label": "", "metadata": {}, "score": "64.22808"}
{"text": "The visualization of the results in the form of a series of heat maps , see Figures 6 , 7 , and 8 , helps us arrive at a number of qualitative observations as well as to look at some quantitative relationships .", "label": "", "metadata": {}, "score": "64.23175"}
{"text": "Studies in Classification , Data Analysis , and Knowledge Organization .Edited by : Fink A , Lausen B , Seidel W , Ultsch A. Berlin Heidelberg : Springer ; 2010:685 - 697 .Lu J , Kerns RT , Peddada SD , Bushel PR : Principal component analysis\u2010based filtering improves detection for affymetrix gene expression arrays .", "label": "", "metadata": {}, "score": "64.260155"}
{"text": "For example ase occurring at the end of a word is much more informative then just knowing ase is contained in the word ( i.e laser vs. kinase ) .We include predicates that indicate whether the current token occurs within brackets or inside quotations .", "label": "", "metadata": {}, "score": "64.30178"}
{"text": "Experimental Studies .The main objective of these experiments is to show the abilities of the proposed approach , quantify the performance of the selected subsets of features and instances , and arrive at some general conclusions .A concise summary of the data sets used in the experiment is presented in Table 2 .", "label": "", "metadata": {}, "score": "64.31215"}
{"text": "For example , for medication names , we define the B class as \" B - m , \" and the I class as \" I - m . \"Therefore , we have 13 possible labels for each token ( including the O ) .", "label": "", "metadata": {}, "score": "64.312836"}
{"text": "Ties are randomly distributed on both classes .Genes are assigned to their RDC in order to associate each gene with the class it is more able to distinguish .As a result , the number of selected genes could be balanced per class at our final selection process .", "label": "", "metadata": {}, "score": "64.35976"}
{"text": "Intell . Rev. 2008 , 30 , 19 - 37 .[ Google Scholar ] .Rifkin , R. ; Klautau , A. In defense of one- vs -all classification .J. Mach .Learn .Res .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "64.376114"}
{"text": "Results and Discussion .The testing performance in terms of prediction accuracy of the proposed method is compared with that of a simple multi - class classification using the same data split .The confusion matrices are shown in the corresponding subsections , followed by discussions of the results .", "label": "", "metadata": {}, "score": "64.405304"}
{"text": "In the baseline method , multiple classes are handled by pairwise classification as in typical SVM .The data are neither normalized nor standardized .Table 6 shows the prediction results of the multi - class classification with SVM .The correct fault diagnoses are highlighted in bold .", "label": "", "metadata": {}, "score": "64.419266"}
{"text": "2.1 Sentence Segmentation .Sentence segmentation can be viewed as a classification task for punctuation : whenever we encounter a symbol that could possibly end a sentence , such as a period or a question mark , we have to decide whether it terminates the preceding sentence .", "label": "", "metadata": {}, "score": "64.43544"}
{"text": "In this project , we used the 145 notes from the Sydney training data to customize the MedEx system .The changes were 1 ) adding new phrases for NEs from the Sydney training data into MedEx 's dictionary , and 2 ) extending the post - processing program used in the 2009 i2b2 challenge .", "label": "", "metadata": {}, "score": "64.49475"}
{"text": "As in any modeling problems , the actual formulation used in the diagnosis / prognosis depends on the data available and how the analyst formulates the problem .For example , bearing diagnostics can also be modeled as a clustering problem [ 40 ] for unsupervised learning .", "label": "", "metadata": {}, "score": "64.57625"}
{"text": "1251 - 1254 , December 2001 .View at Scopus .View at Scopus . Y. Zhang , X. B. Wu , Z. Y. Xing , and W. L. Hu , \" On generating interpretable and precise fuzzy systems based on Pareto multi - objective cooperative co - evolutionary algorithm , \" Applied Soft Computing Journal , vol .", "label": "", "metadata": {}, "score": "64.61473"}
{"text": "In terms of generalization accuracy , ICA - based RotBoost ensemble in conjunction with fast correlation - based filter demonstrated superior average performance over all considered ensemble classifiers and is therefore recommended as an efficient classification technique for the prediction of new gene microarray class labels .", "label": "", "metadata": {}, "score": "64.6882"}
{"text": "Local SVM - based voting .Duration and reason are those from the SVM model , whereas the remaining mentions are those from the CRF outputs .Specifically , for a given word , if the SVM predicts duration or reason then use its label for the word ; otherwise , use the CRF prediction .", "label": "", "metadata": {}, "score": "64.70857"}
{"text": "In the cooperative PSO , the formation of the search space is realized in a more sophisticated way .The cooperative facet involves mainly exchanging information about the best positions found by the different subswarms .Here , we present a new cooperative PSO ( CPSO ) algorithm for the data and feature reduction process .", "label": "", "metadata": {}, "score": "64.733055"}
{"text": "Proceedings of Computational Linguistics and Intelligent Text Processing ( CICLing 2004 )2004 , 172 - 175 .Tsuruoka Y , Tsujii J : Boosting Precision and Recall of Dictionary - Based Protein Name Recognition .Proceedings of the ACL 2003 Workshop on NLP in Biomedicine 2003 , 41 - 48 .", "label": "", "metadata": {}, "score": "64.752075"}
{"text": "Fukuda et al .[ 4 ] and Frenz\u00e9n et al .[5 ] developed automated recognition systems based on hand - crafted rules .They identified key terms for recognizing protein names , which they termed \" core terms \" ( e.g. capital letters and special symbols ) and \" feature terms \" ( e.g. \" protein \" and \" receptor \" ) .", "label": "", "metadata": {}, "score": "64.78935"}
{"text": "[ 1 ] and Blaschke et al .[ 2 ] demonstrated the automated extraction of protein - protein interactions ( PPIs ) from biomedical literature .They identified key words that express these interactions , and demonstrated automated extraction based on these key words and some heuristic rules .", "label": "", "metadata": {}, "score": "64.799324"}
{"text": "Typical NER systems use dictionary lookup methods to determine semantic categories of a word ( e.g. , gene names in a dictionary ) .In this study , we used MedEx , the best rule - based medication extraction system in the 2009 i2b2 challenge , to assign medication specific categories into words .", "label": "", "metadata": {}, "score": "64.81878"}
{"text": "[ Google Scholar ] .Elwany , A. ; Gebraeel , N. Sensor - driven prognostic models for equipment replacement and spare parts inventory .IIE Trans .[ Google Scholar ] .Chen , N. ; Tsui , K.L. Condition monitoring and remaining useful life prediction using degradation signals : Revisited .", "label": "", "metadata": {}, "score": "64.87126"}
{"text": "Our experimental results using the 2009 i2b2 challenge datasets showed that ensemble classifiers that combine individual classifiers into a voting system could achieve better performance than a single classifier in recognizing medication information from clinical text .It suggests that simple strategies that can be easily implemented such as majority voting could have the potential to significantly improve clinical entity recognition .", "label": "", "metadata": {}, "score": "64.87388"}
{"text": "To assess the efficiency of the RotBoost , other nonensemble / ensemble techniques including Decision Trees , Support Vector Machines , Rotation Forest , AdaBoost , and Bagging are also deployed .Experimental results have revealed that the combination of the fast correlation - based feature selection method with ICA - based RotBoost ensemble is highly effective for gene classification .", "label": "", "metadata": {}, "score": "64.89086"}
{"text": "Table 5 shows the list of samples in this data set .The first eight samples were used in training and the last eight samples were used in testing .In other words , part of the bearing data collected at normal state and each of the single defect states are selected randomly for modeling training .", "label": "", "metadata": {}, "score": "64.8917"}
{"text": "EMBS 2008 .30th Annual International Conference of the IEEE .Vancouver : IEEE ; 2008:5692 - 5695 .View Article .Jirapech\u2010Umpai T , Aitken S : Feature selection and classification for microarray data analysis : Evolutionary methods for identifying predictive genes .", "label": "", "metadata": {}, "score": "64.98891"}
{"text": "The gene(s ) with the highest number of mask bits set to 1 is ( are ) chosen to form the set ( line 6 ) .This set could not be empty as long as the loop condition is still satisfied , i.e. .", "label": "", "metadata": {}, "score": "65.046524"}
{"text": "The names classifier that we have built generates about 100 errors on the dev - test corpus : .Looking through this list of errors makes it clear that some suffixes that are more than one letter can be indicative of name genders .", "label": "", "metadata": {}, "score": "65.07954"}
{"text": "6 , pp .673 - 679 , 2001 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Bhaskar , D. C. Hoyle , and S. Singh , \" Machine learning in bioinformatics : a brief survey and recommendations for practitioners , \" Computers in Biology and Medicine , vol .", "label": "", "metadata": {}, "score": "65.08185"}
{"text": "1 Supervised Classification .Classification is the task of choosing the correct class label for a given input .In basic classification tasks , each input is considered in isolation from all other inputs , and the set of labels is defined in advance .", "label": "", "metadata": {}, "score": "65.09052"}
{"text": "To the best of our knowledge , this is the first study on investigating ensemble classifiers in recognizing medication relevant entities in clinical text .Ensemble classifiers are built based on different combination methods and are evaluated using the challenge data set .", "label": "", "metadata": {}, "score": "65.096535"}
{"text": "Gene selection filter\u2010based methods can scale easily to high\u2010dimensional datasets since they are computationally simple and fast compared with the other approaches .Various examples for filter\u2010based approaches have been proposed in earlier papers [ 2 , 3 , 15 ] \u2010 [ 17 ] .", "label": "", "metadata": {}, "score": "65.12756"}
{"text": "Examples for expression values of 2 genes distinguishing between 2 classes : ( a ) gene i 1 has overlapping samples distributed as 1:1 , ( b ) gene i 2 has its overlapping samples distributed as 5:1 for class1:class2 .", "label": "", "metadata": {}, "score": "65.14647"}
{"text": "In our experiments , we choose four representative feature selection algorithms , that is , ReliefF , correlation - based filter selection ( CFS ) , minimum redundancy maximum relevance ( mRMR ) , and general signal to noise ratio ( GSNR ) in comparison with FCBF .", "label": "", "metadata": {}, "score": "65.23793"}
{"text": "119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .C.-H. Yang , L.-Y. Chuang , and C.-H. Yang , \" IG - GA : a hybrid filter / wrapper method for feature selection of microarray data , \" Journal of Medical and Biological Engineering , vol .", "label": "", "metadata": {}, "score": "65.28366"}
{"text": "In Information theory , a probabilistic method for quantifying information is developed by Shannon [ 55 ] .The entropy H(Y ) of the distribution p(y ) , a measure of the uncertainty about Y , is defined as : .The entropy ranges from zero to infinity .", "label": "", "metadata": {}, "score": "65.29109"}
{"text": "Computers fail to track these in fast video , but sleight of hand fools humans as well : what happens too quickly we just can not see .We show a 3D tracker for these types of motions that relies on the recognition of familiar configurations in 2D images ( classification ) , and fills the gaps in - between ( interpolation ) .", "label": "", "metadata": {}, "score": "65.39031"}
{"text": "12 , pp .6745 - 6750 , 1999 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. Blanco , P. Larra\u00f1aga , I. Inza , and B. Sierra , \" Gene selection for cancer classification using wrapper approaches , \" International Journal of Pattern Recognition and Artificial Intelligence , vol .", "label": "", "metadata": {}, "score": "65.47424"}
{"text": "PubMed ignores the stop words in search queries .Preceding class : Class ( i.e. B , I , or O ) of the token(s ) preceding the current token .The number of preceding tokens is dependent on the window size ( described later on ) .", "label": "", "metadata": {}, "score": "65.61524"}
{"text": "11 , no . 2 , pp .2419 - 2443 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. G. Di Nuovo , M. Palesi , and V. Catania , \" Multi - objective evolutionary fuzzy clustering for high - dimensional problems , \" in Proceedings of the IEEE International Conference on Fuzzy Systems , pp . 1 - 6 , July 2007 .", "label": "", "metadata": {}, "score": "65.66655"}
{"text": "Tse , P. ; Peng , Y. ; Yam , R. Wavelet analysis and envelope detection for rolling element bearing fault diagnosis their effectiveness and flexibilities .J. Vib .Acoust .[ Google Scholar ] .Peng , Z. ; Tse , P.W. ; Chu , F. A comparison study of improved Hilbert - Huang transform and wavelet transform : Application to fault diagnosis for rolling bearing .", "label": "", "metadata": {}, "score": "65.67541"}
{"text": "Bioinformatics 2002 . , 18(8 ) : .Lafferty J , McCallum A , Pereira F : Conditional random fields : Probabilistic models for segmenting and labeling sequence data .Proceedings of ICML 2001 .McCallum A : Effciently inducing features of conditional random fields .", "label": "", "metadata": {}, "score": "65.730446"}
{"text": "Comput .Chem .Eng .[ Google Scholar ] .Ko\u015bcielny , J.M. Methods of Pattern Recognition .In Fault Diagnosis Models , Artificial Intelligence , Applications ; Springer : Berlin , Germany , 2004 ; pp .89 - 91 .", "label": "", "metadata": {}, "score": "65.743126"}
{"text": "ABGene is a hybrid model that uses a statistical part - of - speech tagger to identify candidate genes by labeling them with a special part - of - speech ' GENE ' .Once the candidate genes are found a series of post processing rules are initiated to improve the results .", "label": "", "metadata": {}, "score": "65.78815"}
{"text": "This in turn leads to less computational cost in experiments .From Table 2 , it is obvious that FCBF achieves the highest level of dimensionality reduction by selecting the least number of discriminative genes .This is consistent with the theoretical analysis about FCBF 's ability to identify and ignore redundant features .", "label": "", "metadata": {}, "score": "65.81644"}
{"text": "Proceedings of the ACL 2003 Workshop on NLP in Biomedicine 2003 , 33 - 40 .Takeuchi K , Collier N : Bio - Medical Entity Extraction using Support Vector Machine .Proceedings of the ACL 2003 Workshop on NLP in Biomedicine 2003 , 57 - 64 .", "label": "", "metadata": {}, "score": "65.851685"}
{"text": "In many cases , it is not possible to avoid objects which overlap .But they are harder to analyze and measure , since many algorithms have difficulty distinguishing between objects .It may be possible to mitigate these difficulties by preparing the environment somehow - e.g . , staining cell membranes with a fluorescent dye .", "label": "", "metadata": {}, "score": "65.90275"}
{"text": "The relationships between the percentage of data used and the resulting RMSE values are displayed in Figures 9 and 10 .Some interesting tendencies are worth noting .A critical number of data are required to form a fuzzy model .Increasing the number of data does not produce any improvement as the curves plotted on Figures 9(a ) , 9(c ) , and 9(a ) achieve a plateau or even some increase of the RMSE is noticeable .", "label": "", "metadata": {}, "score": "65.906906"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Chen , B. Yang , A. Abraham , and L. Peng , \" Automatic design of hierarchical Takagi - Sugeno type fuzzy systems using evolutionary algorithms , \" IEEE Transactions on Fuzzy Systems , vol .", "label": "", "metadata": {}, "score": "66.033424"}
{"text": "We present here a method for identifying gene and protein mentions in text with conditional random fields ( CRFs ) [ 6 ] , which are discussed in the next section .Narayanaswamy et al .[ 3 ] suggest that rule - based systems make decisions on sets of textual indicator features and that these features may easily be exploited by supervised statistical approaches .", "label": "", "metadata": {}, "score": "66.0522"}
{"text": "3 , pp .1945 - 1950 , 1999 .View at Google Scholar Not all data is created equal and thus the analysis of certain images can be easily automated , while others pose a bigger challenge .The goal of this section is to collect information on image acquisition principles that ease the automation of image analysis .", "label": "", "metadata": {}, "score": "66.118"}
{"text": "Syst .Signal Process .[ Google Scholar ] .Tran , V. ; Yang , B. ; Oh , M. ; Tan , A. Fault diagnosis of induction motor based on decision trees and adaptive neuro - fuzzy inference .Expert Syst .", "label": "", "metadata": {}, "score": "66.13995"}
{"text": "It is clear that gene i 1 is relevant for discriminating samples between the target classes , because their values are falling in non\u2010overlapping ranges .Figure 1 b , on the other hand , shows expression values for another gene i 2 , which looks less useful for distinguishing between these target classes , because their expression values have a highly overlapping range .", "label": "", "metadata": {}, "score": "66.2349"}
{"text": "The GSNR is a measure of the ratio between intergroup and intragroup variations .Higher GSNR values indicate higher discrimination power for the gene .GSNR selects .In order to reduce the computational complexity of the problem at hand and select the most informative genes , we run all 5 feature selection algorithms against each dataset and obtain the number of selected features for each algorithm .", "label": "", "metadata": {}, "score": "66.24356"}
{"text": "To avoid the curse of dimensionality problem , gene selection plays a crucial role in DNA microarray analysis .Another important reason to reduce dimensionality is to help biologists to identify the underlying mechanism that relates gene expression to diseases .Indeed , the microarray data is associated with various uncertainties such as microarray data , gathering process which include fabrication , hybridization and image processing .", "label": "", "metadata": {}, "score": "66.26964"}
{"text": "Precision , which indicates how many of the items that we identified were relevant , is TP/(TP+FP ) .Recall , which indicates how many of the relevant items that we identified , is TP/(TP+FN ) .The F - Measure ( or F - Score ) , which combines the precision and recall to give a single score , is defined to be the harmonic mean of the precision and recall : ( 2 \u00d7 Precision \u00d7 Recall ) / ( Precision + Recall ) .", "label": "", "metadata": {}, "score": "66.270996"}
{"text": "Since stability degree increases from the bottom to the top on the vertical axis and the classification error increases to the right on the horizontal axis , the best method is the one whose dots are depicted in the upper\u2010left corner of the plot .", "label": "", "metadata": {}, "score": "66.32575"}
{"text": "Proceedings of Human Language Technology and North American Chapter of the Association for Computatitonal Linguists 2003 .Copyright .\u00a9 McDonald and Pereira 2005 .This article is published under license to BioMed Central Ltd.Syntactic parsing is a fundamental problem in computational linguistics and natural language processing .", "label": "", "metadata": {}, "score": "66.33316"}
{"text": "PubMed View Article .Doan S , Xu H : Recognizing medication related entities in hospital discharge summaries using support vector machine .Proceedings of the 23thConference on Computational Linguistics 2010 , 259 - 266 .Uzuner \u00d6 , Solti I , Xia F , Cadag E : Community annotation experiment for ground truth generation for the i2b2 medication challenge .", "label": "", "metadata": {}, "score": "66.36427"}
{"text": "These scores were higher than those of the two single classifiers CRF and SVM .According to randomization tests , the local CRF - based voting system performed significantly better than the single CRF system for overall , dosage and was significantly less accurate than the single CRF system in recognizing duration .", "label": "", "metadata": {}, "score": "66.53344"}
{"text": "Lausser L , M\u00fcssel C , Maucher M , Kestler HA : Measuring and visualizing the stability of biomarker selection techniques .Comput Stat 2013 , 28 ( 1 ) : 51 - 65 .View Article .Croner RS , St\u00fcrzl M , Rau TT , Metodieva G , Geppert CI , Naschberger E , Lausen B , Metodiev MV : Quantitative proteome profiling of lymph node\u2010positive vs.\u2010negative colorectal carcinomas pinpoints mx1 as a marker for lymph node metastasis .", "label": "", "metadata": {}, "score": "66.55513"}
{"text": "Here , we address the gene classification issue using RotBoost ensemble methodology .This method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "66.625984"}
{"text": "Comparative and Functional Genomics 2001 , 2 : 196 - 206 .View Article PubMed .Temkin JM , Gilder MR : Extraction of protein interaction information from unstructured text using a context - free grammar .Bioinformatics 2003 , 19 ( 16 ) : 2046 - 2053 .", "label": "", "metadata": {}, "score": "66.632965"}
{"text": "While JPEG compression throws away information that the eye can not detect easily , this results in considerable image artifacts ( variable \" blockiness \" of the image in groups of 8x8 pixels ) .Any attempts to process or quantify those images will be affected in uncontrollable ways by the presence of the artifacts .", "label": "", "metadata": {}, "score": "66.69058"}
{"text": "In particular , entropy is defined as the sum of the probability of each label times the log probability of that same label : .Figure 4.2 : The entropy of labels in the name gender prediction task , as a function of the percentage of names in a given set that are male .", "label": "", "metadata": {}, "score": "66.741455"}
{"text": "1373 - 1390 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Cho and H. Won , \" Machine learning in DNA microarray analysis for cancer classification , \" in Proceedings of the 1st Asia - Pacific Bioinformatics Conference on Bioinformatics , pp .", "label": "", "metadata": {}, "score": "66.745964"}
{"text": "Its value is at its maximum if and only if all possible values for Y are equally probable .H .Y .X .j . ) j .d .i .n .p .y .i .", "label": "", "metadata": {}, "score": "66.75001"}
{"text": "Pattern Anal Mach Intell IEEE Trans 2005 , 27 ( 8) : 1226 - 1238 .View Article .Su Y , Murali T , Pavlovic V , Schaffer M , Kasif S : Rankgene : identification of diagnostic genes based on expression data .", "label": "", "metadata": {}, "score": "66.77454"}
{"text": "The percentages of names over 4 or more tokens long in the FP and FN were higher than the one for TP , suggesting that recognizing longer names is more difficult than recognizing shorter ones .Percentage of n - grams of gene / protein names in test data for evaluation and TP , FP , and FN datasets .", "label": "", "metadata": {}, "score": "66.81796"}
{"text": "Kulick S , Bies A , Liberman M , Mandel M , McDonald R , Palmer M , Pancoast E , Schein A , Ungar L , White P , Winters S : Integrated annotation for biomedical information extraction .Proceedings of Biolink 2004 2004 .", "label": "", "metadata": {}, "score": "66.858315"}
{"text": "View at Scopus .G. J. Gordon , R. V. Jensen , L.-L. Hsiao et al . , \" Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma , \" Cancer Research , vol .", "label": "", "metadata": {}, "score": "66.87393"}
{"text": "[ 3 ] demonstrated the automated extraction of PPIs using a context - free grammar .In each of these studies , protein name recognition was the first step .Next , protein name dictionaries were constructed .Finally , protein name recognition was performed based on pattern matching using the dictionaries .", "label": "", "metadata": {}, "score": "66.889755"}
{"text": "Conditional random fields are closely related to other conditional formalisms in the literature .The strongest connection is between CRFs and maximum entropy classification models [ 13 ] .Maximum entropy models give the conditional probability of a class given an observation by : .", "label": "", "metadata": {}, "score": "66.98603"}
{"text": "Table 6 .The minimum error rates yielded by Support Vector Machine classifier with feature selection methods along\u2010with the classification error without selection .The numbers in brackets represent the size , average size for ISIS method , of the gene set that corresponding to the minimum error rate .", "label": "", "metadata": {}, "score": "67.14772"}
{"text": "The p - values are shown in parentheses .We defined the null hypothesis as the case where there was no statistically significant difference between two values .The alternative hypothesis was defined as the opposite case .We set the statistical significance level at 0.05 .", "label": "", "metadata": {}, "score": "67.149765"}
{"text": "-rest method .In the latter , the B , I , and O classes are learned as ( B vs. other ) , ( I vs. other ) , and ( O vs. other ) .Cost of constraint violation ( floating number ) : There is a trade - off between the training error and the soft margin of the hyper plane .", "label": "", "metadata": {}, "score": "67.17764"}
{"text": "Named Entity Recognition ( NER ) is an important step in natural language processing ( NLP ) .It has many applications in the general language domain such as identifying person names , locations , and organizations .NER is crucial for biomedical literature mining as well [ 1 , 2 ] ; many studies have focused on biomedical entities , such as gene / protein names .", "label": "", "metadata": {}, "score": "67.18474"}
{"text": "Figure 6a shows a temporal signal of a BPFI bearing data sample and Figure 6b shows the corresponding frequency spectra .The BPFI characteristic defect frequency and harmonics are not as obvious as the BPFO , but there are numerous sidebands equal to the shaft rotation frequency ( 1\u00d7 ) in the spectrum , which are caused by modulation .", "label": "", "metadata": {}, "score": "67.22536"}
{"text": "The confusion matrix indicates that common errors include a substitution of NN for JJ ( for 1.6 % of words ) , and of NN for NNS ( for 1.5 % of words ) .Note that periods ( . ) indicate cells whose value is 0 , and that the diagonal entries - which correspond to correct classifications - are marked with angle brackets .", "label": "", "metadata": {}, "score": "67.25894"}
{"text": "Krauthammer M , Nenadic G : Term identification in the biomedical literature .J Biomed Inform 2004 , 37 : 512 - 526 .PubMed View Article .Kazama J , Makino T , Ohta Y , Tsujii T : Tuning Support Vector Machines for biomedical named entity recognition .", "label": "", "metadata": {}, "score": "67.273094"}
{"text": "For the sake of comparison , the results from the Sydney team , the customized MedEx , the single classifiers based on SVMs and CRFs , and the three combination methods are shown in Table 7 .We also present the results of statistical significance tests for differences in performance between the methods in Table 8 .", "label": "", "metadata": {}, "score": "67.36175"}
{"text": "A of BioCreAtIvE was a competition involving automated gene / protein name recognition .The system described here was developed for that competition .It uses the SVM algorithm as a learning method for gene / protein name recognition .This algorithm has achieved good performance in many classification tasks , and we have previously showed that it performs well for protein name recognition [ 11 ] .", "label": "", "metadata": {}, "score": "67.3725"}
{"text": "[ Google Scholar ] .Jardine , A. ; Lin , D. ; Banjevic , D. A review on machinery diagnostics and prognostics implementing condition - based maintenance .Mech .Syst .Signal Process .[ Google Scholar ] .Worden , K. ; Staszewski , W. ; Hensman , J. Natural computing for mechanical systems research : A tutorial overview .", "label": "", "metadata": {}, "score": "67.40212"}
{"text": "[ Google Scholar ] .Di Maio , F. ; Ng , S. ; Tsui , K. ; Zio , E. Na\u00efve bayesian classifier for on - line remaining useful life prediction of degrading bearings .Proceedings of the 7th International Conference on Mathematical Methods in Reliability ( MMR ) , Beijing , China , 1 - 12 January 2011 .", "label": "", "metadata": {}, "score": "67.4331"}
{"text": "The Maximum Entropy classifier model is a generalization of the model used by the naive Bayes classifier .Like the naive Bayes model , the Maximum Entropy classifier calculates the likelihood of each label for a given input value by multiplying together the parameters that are applicable for the input value and label .", "label": "", "metadata": {}, "score": "67.56285"}
{"text": "A sound guideline is to keep the related ( associated ) variables within the same group .Obviously , such relationships are not known in advance .Several possible methods are available for addressing this issue in more detail in the context of the problem at hand .", "label": "", "metadata": {}, "score": "67.604225"}
{"text": "Competing interests .The authors declare that they have no competing interests .Authors ' contributions .SD , XH , and TMP developed the idea for this study .The project was supervised by XH , TMP , and NC .", "label": "", "metadata": {}, "score": "67.676025"}
{"text": "For the Support Vector Machine classifier , POS outperformed all other methods on ' Leukaemia ' , ' Breast ' , ' Srbct ' , ' Lung ' and ' GSE24514 ' .While the MaskedPainter provides the minimum error rates on ' GSE4045 ' and ' GSE14333 ' .", "label": "", "metadata": {}, "score": "67.70684"}
{"text": "Image acquisition .It is indispensable that during imaging the field of view be equally lit , and that this is checked and adjusted if necessary .Besides other disadvantages ( e.g. , no reliable intensity measurements ) , unequal lighting profoundly influences the outcome of thresholding methods , especially global ones .", "label": "", "metadata": {}, "score": "67.72536"}
{"text": "In this study , we investigated the combination of NER outputs from a rule - based system and two supervised machine learning systems into an ensemble classifier using different voting methods to improve recognition of medication entities from discharge summaries .Our experimental results using the 2009 i2b2 challenge datasets showed that ensemble classifiers that combine individual classifiers into a voting system could achieve better performance than a single classifier in recognizing medication related entities from clinical text .", "label": "", "metadata": {}, "score": "67.734886"}
{"text": "But , there is an element which differs in those examples and it may also affect the overlap degree between classes .This element is the distribution of overlapping samples by classes .Gene i 1 has six overlapped samples from each class , whereas gene i 2 has ten and two overlapping samples from class 1 and 2 respectively .", "label": "", "metadata": {}, "score": "67.78763"}
{"text": "6 , pp .561 - 575 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Kennedy and R. Eberhart , \" Particle swarm optimization , \" in Proceedings of the IEEE International Conference on Neural Networks , pp .", "label": "", "metadata": {}, "score": "67.79164"}
{"text": "Proceedings of the Human Language Technology Conference ( HLT 2002 ) 2002 .Vapnik VN : The Nature of Statistical Learning Theory Springer 1995 .Copyright .\u00a9 Mitsumori et al 2005 .This article is published under license to BioMed Central Ltd. Abstract .", "label": "", "metadata": {}, "score": "67.831604"}
{"text": "It is denoted by g k .The set is updated by adding the selected gene , g k ( line 8) .All gene masks are also updated by performing the logical conjunction ( logic AND ) with negated aggregate mask of set ( line 10 ) .", "label": "", "metadata": {}, "score": "67.90307"}
{"text": "Moreover , one of the most widely used approaches in fuzzy modeling is the fuzzy C - means ( FCM ) algorithm for constructing the antecedents of the rules associated with the curse of dimensionality [ 5 , 6 ] .The dimensionality problem can be addressed by reducing the constructed fuzzy rules .", "label": "", "metadata": {}, "score": "67.91235"}
{"text": "It can be combined partially with automatic intensity thresholding and is mentioned here out of completeness since it might lead to good results ( e.g. , for some histological staining ) .The \" Color Threshold \" also implemented by Gabriel Landini gives access to different color space representations of a true color image .", "label": "", "metadata": {}, "score": "67.921814"}
{"text": "Select only the instances where inst.attachment is N : .Using this sub - corpus , build a classifier that attempts to predict which preposition is used to connect a given pair of nouns .For example , given the pair of nouns \" team \" and \" researchers , \" the classifier should predict the preposition \" of \" .", "label": "", "metadata": {}, "score": "67.99774"}
{"text": "\" Lossy compression \" means that in the process of file size reduction certain amount of image information is discarded .In the case of JPEGs , this might not be readily obvious to an observer , but it will be important for image processing purposes . \" Loss - less or non - lossy compression \" means that the file size is reduced , but the data stored is exactly the same as in the original .", "label": "", "metadata": {}, "score": "68.02634"}
{"text": "262 - 267 , 2000 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. S. Furey , N. Cristianini , N. Duffy , D. W. Bednarski , M. Schummer , and D. Haussler , \" Support vector machine classification and validation of cancer tissue samples using microarray expression data , \" Bioinformatics , vol .", "label": "", "metadata": {}, "score": "68.03403"}
{"text": "Bagging also presents the second best diversity but low classification accuracy .Table 5 : Kappa error diagram for Lung dataset ( the centroids of ensembles ) .When outlining the kappa - error diagrams , the diagrams of different ensembles are greatly overlapping , and the distances between the centroids are small .", "label": "", "metadata": {}, "score": "68.081985"}
{"text": "Table 5 .The minimum error rates yielded by k Nearest Neighbor classifier with feature selection methods along\u2010with the classification error without selection .The numbers in brackets represent the size , average size for ISIS method , of the gene set that corresponding to the minimum error rate .", "label": "", "metadata": {}, "score": "68.08589"}
{"text": "This combination is quite convincing .where the corresponding features include : 1 : the concentration of PM10 ( particles ) , 7 : hour of experiment per day , 6 : wind direction , and 2 : the number of cars per hour .", "label": "", "metadata": {}, "score": "68.23561"}
{"text": "A decision tree is a simple flowchart that selects labels for input values .This flowchart consists of decision nodes , which check feature values , and leaf nodes , which assign labels .To choose the label for an input value , we begin at the flowchart 's initial decision node , known as its root node .", "label": "", "metadata": {}, "score": "68.24305"}
{"text": "5.4 The Naivete of Independence .The reason that naive Bayes classifiers are called \" naive \" is that it 's unreasonable to assume that all features are independent of one another ( given the label ) .In particular , almost all real - world problems contain features with varying degrees of dependence on one another .", "label": "", "metadata": {}, "score": "68.26329"}
{"text": "Each line shows one vector .A +1(-1 ) means a positive example ( negative example ) .The positive integer on the left side of the colon is the unique number of each feature .A \" 1 \" on the right side of the colon shows that the vector includes the feature presented by the unique number .", "label": "", "metadata": {}, "score": "68.28581"}
{"text": "and the results for which a significant difference with RotBoost was found are marked with a bullet or an open circle next to them .A bullet next to a result indicates that RotBoost is significantly better than the corresponding method .", "label": "", "metadata": {}, "score": "68.33563"}
{"text": "BMC Bioinformatics 2006 , 7 ( 1 ) : 3 .PubMed Central PubMed View Article .Breiman L : Random forests .Mach Learn 2001 , 45 ( 1 ) : 5 - 32 .View Article .Cover T , Hart P : Nearest neighbor pattern classification .", "label": "", "metadata": {}, "score": "68.386024"}
{"text": "For example , fault feature extraction from bearing accelerometer sensor signals is discussed in [ 31 - 33 ] .The effect of number of features used in bearing fault diagnostics with SVM and proximal support vector machine ( PSVM ) is discussed in [ 34 ] .", "label": "", "metadata": {}, "score": "68.4428"}
{"text": "For the SVM classifier , POS provides a good classification performance on all used datasets .In particular on Breast and Lung datasets , the classification average error rates on the test sets are less than all other feature selection techniques at all selected genes set sizes , see Figure 4 in the manuscript and Additional file 3 : Table S3 .", "label": "", "metadata": {}, "score": "68.45775"}
{"text": "For the k Nearest Neighbor classifier , POS outperformed all other methods on ' Leukaemia ' , ' Breast ' , ' Lung ' and ' GSE27854 ' .While it shows a comparable performance to the MaskedPainter method on the ' Srbct ' .", "label": "", "metadata": {}, "score": "68.54677"}
{"text": "Indeed , to verify the efficiency of the proposed method on gene - related data , 8 publically available gene microarray benchmark datasets are analyzed .To this end , we accomplish a comparative study of RotBoost efficiency against several other ensemble and single classifier systems including AdaBoost , Bagging , Rotation Forest single tree , and support vector machines ( SVMs ) .", "label": "", "metadata": {}, "score": "68.55756"}
{"text": "Takeuchi et al .[ 10 ] reached to the same conclusion from their results .As noted above , Tsuruoka et al .[ 12 ] reported that dictionary pattern matching can result in low recall in a biomedical domain due to spelling variations .", "label": "", "metadata": {}, "score": "68.57159"}
{"text": "Then , RotBoost was employed on the selected genes .Here , we experimented with 2 different transformation matrixes , that is , PCA and ICA towards RotBoost implementation .To assess the efficiency of RotBoost algorithm different ensemble / nonensemble techniques including Rotation Forest , AdaBoost , Bagging single tree , and SVMs were also deployed .", "label": "", "metadata": {}, "score": "68.61923"}
{"text": "These posts have all been labeled with one of 15 dialogue act types , such as \" Statement , \" \" Emotion , \" \" ynQuestion \" , and \" Continuer .\" We can therefore use this data to build a classifier that can identify the dialogue act types for new instant messaging posts .", "label": "", "metadata": {}, "score": "68.64061"}
{"text": "In particular , the diagnosis accuracy of combined fault samples using C4.5 with OVA is much better than that using SVM with on the same data set ( which performed worse than the worst case classification accuracy on the COMB samples ) .", "label": "", "metadata": {}, "score": "68.68103"}
{"text": "[40 ] is used to measure the stability of the compared method at different set sizes of features .Table 7 and Figures 6 and 7 show the stability scores of different feature selection methods for the ' Srbct ' , ' GSE27854 ' and ' GSE24514 ' datasets respectively .", "label": "", "metadata": {}, "score": "68.69356"}
{"text": "Nucleic Acids Research 2003 , 31 : 365 - 370 .View Article PubMed .Kudo T , Matsumoto Y : Chunking with Support Vector Machines .Proceedings of Second Meeting of North American Chapter of the Association for Computational Linguistics(NAACL ) 2001 , 192 - 199 .", "label": "", "metadata": {}, "score": "68.81541"}
{"text": "Example : a section of the famous Mandrill image .From left to right , you see the original ( with an 8-bit colormap ) , the hue channel of the original , and the hue channel after saving as a JPEG with ImageJ 's default options -- note in particular the vertical and horizontal artifacts : .", "label": "", "metadata": {}, "score": "68.827225"}
{"text": "Note that if most input values have the same label ( e.g. , if P(male ) is near 0 or near 1 ) , then entropy is low .In particular , labels that have low frequency do not contribute much to the entropy ( since P(l ) is small ) , and labels with high frequency also do not contribute much to the entropy ( since log 2", "label": "", "metadata": {}, "score": "68.82915"}
{"text": "What should I do with true color images ( e.g. histological staining ) with different colors and not a single intensity scale ?If converting to grayscale damages or distorts the information desired from a pattern , many different possibilities exist : .", "label": "", "metadata": {}, "score": "68.9655"}
{"text": "Affiliated with .Affiliated with .Abstract .Background .Automated information extraction from biomedical literature is important because a vast amount of biomedical literature has been published .Recognition of the biomedical named entities is the first step in information extraction .", "label": "", "metadata": {}, "score": "69.00163"}
{"text": "In our example , we chose distribution ( i ) because its label probabilities are evenly distributed - in other words , because its entropy is high .In general , the Maximum Entropy principle states that , among the distributions that are consistent with what we know , we should choose the distribution whose entropy is highest .", "label": "", "metadata": {}, "score": "69.00658"}
{"text": "More details of the RF classifier 's results can be found in the Additional file 1 .For the k NN classifier , POS provides a good classification performance .Its classification average error rates are less than all other compared methods on Leukaemia and Breast datasets for most selected set sizes , see Table 2 and Figure 4 .", "label": "", "metadata": {}, "score": "69.03633"}
{"text": "PubMed Central PubMed View Article .Drami\u0144ski M , Rada\u2010Iglesias A , Enroth S , Wadelius C , Koronacki J , Komorowski J : Monte carlo feature selection for supervised classification .Bioinformatics 2008 , 24 ( 1 ) : 110 - 117 .", "label": "", "metadata": {}, "score": "69.08917"}
{"text": "NS means \" not significant different \" .As in the first experimental setting , customized MedEx was behind the machine learning and ensemble systems and received the lowest overall F - score of 87.17 % ( column \" All \" in table 7 ) .", "label": "", "metadata": {}, "score": "69.12793"}
{"text": "Considering that the 8 gene microarray datasets include different characteristics in terms of number of samples , genes , classes and the type of the cancer to which these data is related to ; overall , ICA - based RotBoost classifier seems to be more effective than PCA - based RotBoost .", "label": "", "metadata": {}, "score": "69.15964"}
{"text": "On the GSE24514 dataset , Wil\u2010RS technique has outperformed all methods for set sizes that are more than eight , whereas for smaller sets , the POS was the best .While , on Srbct and GSE4045 datasets , POS shows a comparable and a worse performance respectively compared with the best techniques , MP and Wil\u2010RS respectively .", "label": "", "metadata": {}, "score": "69.23253"}
{"text": "Two dictionaries were constructed , one from SWISS - PROT ( GPD1 ) and the other from SWISS - PROT and TrEMBL ( GPD2 ) .When used for matching , each of these dictionaries is divided into 2 ( sub-)dictionaries , one with the protein names and the other with the gene names .", "label": "", "metadata": {}, "score": "69.244675"}
{"text": "Recent examples of research papers applying SVM to bearing fault diagnostics include [ 46 - 48 ] .SVM methods have also been applied in other bearing PHM formulations such as one - class anomaly detection [ 49 ] , two - class fault detection [ 50 ] , prognostics and RUL prediction [ 51 ] .", "label": "", "metadata": {}, "score": "69.25435"}
{"text": "In this case , the interactions between different defect types have to be modeled and vibration data of different combinations of defects need to be collected , if fault diagnostics of multiple defects ( which occur at a late stage of bearing degradation ) is desired .", "label": "", "metadata": {}, "score": "69.34474"}
{"text": "Performance Evaluation .The prediction performance of normal and single - defect bearings is first compared .When one- versus -all ( OVA ) formulation is used , the diagnosis accuracy increases from 93.8 % to 100 % .The classification accuracies of both methods are much higher than the accuracy of random guess among four single - defect classes ( 25 % ) .", "label": "", "metadata": {}, "score": "69.376785"}
{"text": "Table 1 .Number of fields and descriptions with examples from the i2b2 2009 dataset .\" Prn , \" \" As needed , \" \" Three times a day as needed , \" \" As needed three times a day , \" \" x3 before meal , \" \" x3 a day after meal as needed \" .", "label": "", "metadata": {}, "score": "69.37917"}
{"text": "Ball Tracking :The reliable tracking of the ball is vital in robot soccer .Therefore a Kalman - filter based system for estimating the ball position and velocity in the presence of occlusions is developped . -Sensor Fusion : The robot perceives its environment through several independent sensors ( camera , odometer , etc . ) , which have different delays .", "label": "", "metadata": {}, "score": "69.446594"}
{"text": "If we are concerned with rule - based models , the high - dimensionality of the feature space along with the topology of the rules gives rise to the curse of dimensionality [ 1 , 4 ] .The number of rules increases exponentially and is equal to .", "label": "", "metadata": {}, "score": "69.49335"}
{"text": "199 - 209 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .I. Guyon and A. Elisseeff , \" An introduction to variable and feature selection , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "69.54433"}
{"text": "Syst .Signal Process .[ Google Scholar ] .Sugumaran , V. ; Sabareesh , G. ; Ramachandran , K. Fault diagnostics of roller bearing using kernel based neighborhood score multi - class support vector machine .Expert Syst .Appl .", "label": "", "metadata": {}, "score": "69.70333"}
{"text": "These include general biological terms , amino acids , restriction enzymes , cell lines , organism names and non - biological terms meant to identify tokens that have been mislabeled as ' GENE ' .To recover false negatives , ABGene utilizes large gene lexicons coupled with context lists to identify possible mentions .", "label": "", "metadata": {}, "score": "69.733475"}
{"text": "Data analysis was conducted by SD , PHD , and TMP .The manuscript was prepared by SD with additional contributions by all authors .All authors read and approved the final manuscript .Authors ' Affiliations .National Institute of Informatics , Hitotsubashi .", "label": "", "metadata": {}, "score": "69.889366"}
{"text": "For .Table 5 summarizes the kappa - error values for typical Lung cancer dataset with FCBF gene selection method in terms of the centroids of different ensembles .From this table it is clear that the ICA - based RotBoost provides the highest pairwise accuracy , and the second best accuracy is achieved by PCA - based RotBoost .", "label": "", "metadata": {}, "score": "69.908455"}
{"text": "\" vs. \" word+POS(B)+pc . \" and \" full(A ) \" vs. \" full(B ) \" .The values in bold have a statistically significant difference in the comparison .A difference is labeled statistically significant when the p - value is less than 0.05 on the Wilcoxon signed - ranks sum test ( two - sided ) .", "label": "", "metadata": {}, "score": "69.99881"}
{"text": "For instance , the following feature would be useful : .This is because the token p-53 can either be in a gene mention ( when it is followed by the word ' mutant ' ) or be in a mutation mention ( when it is followed by the word ' mutations ' ) .", "label": "", "metadata": {}, "score": "70.06805"}
{"text": "These methods are also referred to as ' intelligent ' bearing fault diagnostics in some papers [ 23 , 24 ] .Data mining techniques applied to vibration - based bearing diagnostics include Bayesian inference [ 25 ] , HMM [ 26 ] , hidden semi - Markov model [ 27 ] , SVM [ 28 ] , ANN and GA [ 29 ] , PCA and decision tree [ 30 ] .", "label": "", "metadata": {}, "score": "70.293045"}
{"text": "The minimum error rates yielded by Random Forest classifier with feature selection methods along\u2010with the classification error without selection .The numbers in brackets represent the size , average size for ISIS method , of the gene set that corresponding to the minimum error rate .", "label": "", "metadata": {}, "score": "70.34716"}
{"text": "As shown in Figures 8b , 9b , 10b , the frequency range consists of a number of different distributions .Consequently , features are extracted from two frequency ranges , namely low frequency between the 4th to 10th harmonics of the bearing characteristics frequencies ( Table 8 ) , and high frequency between 7 - 10 kHz in the resonant frequency bands where sidebands may occur .", "label": "", "metadata": {}, "score": "70.356316"}
{"text": "There is a growing interest in genome research , and a vast amount of biomedical literature related to it has been published .Collecting and maintaining various databases of this information in computer accessible format require automatically extracting information .Various automated information extraction systems for biomedical literature have been reported .", "label": "", "metadata": {}, "score": "70.45921"}
{"text": "As shown , using all features , CRF and SVM give significantly higher F - scores ( the differences are significant according to randomization tests ) than the customized MedEx system which served as the baseline in this experiment .This can be explained because CRF and SVM can harness the advantages both from the rule - based system ( i.e. , features from MedEx ) and from machine - learning algorithms .", "label": "", "metadata": {}, "score": "70.512535"}
{"text": "Dictionary matching : Matching gene / protein names dictionary entries against uni- , bi- , and tri - grams ( in tokens ) of words starting at the current token .For example , in Figure 2 , the uni - gram ( the current token ) is \" NF - kappaB \" , the bi - gram is \" NF - kappaB transcription \" and the tri - gram is \" NF - kappaB transcription factor \" .", "label": "", "metadata": {}, "score": "70.64211"}
{"text": "Marczyk M , Jaksik R , Polanski A , Polanska J : Adaptive filtering of microarray gene expression data based on gaussian mixture decomposition .BMC Bioinformatics 2013 , 14 ( 1 ) : 101 .PubMed Central PubMed View Article .", "label": "", "metadata": {}, "score": "70.83888"}
{"text": "Signal Process .[ Google Scholar ] .Lee , H. ; Nguyen , N. ; Kwon , J. Bearing Diagnosis Using Time - Domain Features and Decision Tree .In Advanced Intelligent Computing Theories and Applications .With Aspects of Artificial Intelligence ; Springer : Berlin / Heidelberg , Germay , 2007 ; Volume 4682 , pp . 952 - 960 .", "label": "", "metadata": {}, "score": "70.91455"}
{"text": "We show that dependency parsers have more difficulty parsing questions than constituency parsers .In particular , deterministic shift - reduce dependency parsers , which are of highest interest for practical applications because of their linear running time , drop to 60 % labeled accuracy on a question test set .", "label": "", "metadata": {}, "score": "70.99424"}
{"text": "Here , we implement particle swarm optimization ( PSO ) techniques to intelligently search for the best subset of features and data ( instances ) .PSO , developed by Kennedy and Eberhart , inspired by the collective behavior of birds or fish [ 32 ] , is a population - based algorithm where each individual , referred to as a particle , represents a candidate solution .", "label": "", "metadata": {}, "score": "70.999756"}
{"text": "Indeed , from individual accuracy values , we observe that for all the datasets except SRBCT , FCBF can highly increase the overall gene classification accuracy .On the other hand , CFS method achieves the second best classification accuracy , and both relief and GSNR accomplish more than 90 % average accuracy .", "label": "", "metadata": {}, "score": "71.14132"}
{"text": "Syst .Signal Process .[ Google Scholar ] .Lei , Y. ; He , Z. ; Zi , Y. Application of an intelligent classification method to mechanical fault diagnosis .Expert Syst .Appl .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "71.15378"}
{"text": "Proc Natl Acad Sci 2001 , 98 ( 9 ) : 5116 - 5121 .PubMed Central PubMed View Article .Zou C , Gong J , Li H : An improved sequence based prediction protocol for dna\u2010binding proteins using svm and comprehensive feature analysis .", "label": "", "metadata": {}, "score": "71.156975"}
{"text": "PubMed View Article .Liu H\u2010C , Peng P\u2010C , Hsieh T\u2010C , Yeh T\u2010C , Lin C\u2010J , Chen C\u2010Y , Hou J\u2010Y , Shih L\u2010Y , Liang D\u2010C : Comparison of feature selection methods for cross\u2010laboratory microarray analysis .IEEE / ACM Trans Comput Biol Bioinformatics / IEEE , ACM 2013 , 10 ( 3 ) : 593 - 604 .", "label": "", "metadata": {}, "score": "71.18421"}
{"text": "View Article .Talloen W , Clevert D\u2010A , Hochreiter S , Amaratunga D , Bijnens L , Kass S , G\u00f6hlmann HW : I / ni\u2010calls for the exclusion of non\u2010informative genes : a highly effective filtering tool for microarray data .", "label": "", "metadata": {}, "score": "71.28995"}
{"text": "Mech .Syst .Signal Process .[ Google Scholar ] .Lei , Y. ; He , Z. ; Zi , Y. ; Hu , Q. Fault diagnosis of rotating machinery based on multiple ANFIS combination with GAs .Mech .", "label": "", "metadata": {}, "score": "71.35854"}
{"text": "23 - 28 , 2010 .View at Google Scholar \u00b7 View at Scopus .L. Yu and H. Liu , \" Efficient feature selection via analysis of relevance and redundancy , \" Journal of Machine Learning Research , vol .5 , no .", "label": "", "metadata": {}, "score": "71.42879"}
{"text": "Expression values of two different genes ( i 1 , i 2 ) each of which with 36 samples belonging to 2 classes , 18 samples for each class : ( a ) expression values of gene i 1 , ( b ) expression values of gene i 2 .", "label": "", "metadata": {}, "score": "71.434845"}
{"text": "6.3 Generative vs Conditional Classifiers .An important difference between the naive Bayes classifier and the Maximum Entropy classifier concerns the type of questions they can be used to answer .The naive Bayes classifier is an example of a generative classifier , which builds a model that predicts P(input , label ) , the joint probability of a ( input , label ) pair .", "label": "", "metadata": {}, "score": "71.54785"}
{"text": "Figures 8a , 9a , 10a show a temporal signal of a Normal ; BPFO ; and BPFI bearing data sample respectively .The two vibration signals from vertical and horizontal plane are transformed by FFT to the frequency spectra .Figures 8b , 9b , 10b show the corresponding frequency spectra of the three samples .", "label": "", "metadata": {}, "score": "71.79862"}
{"text": "Affiliated with .Affiliated with .Abstract .Background .We present a model for tagging gene and protein mentions from text using the probabilistic sequence tagging framework of conditional random fields ( CRFs ) .The mechanics of CRFs and their relationship to maximum entropy are discussed in detail .", "label": "", "metadata": {}, "score": "71.86852"}
{"text": "Conclusions .In real - world applications of bearing diagnostics , multiple defect types may occur at the same time .While many data - driven modeling methods naturally handle mutually exclusive groups of data , no discussions on the handling of non - exclusive concurrent faults in bearing diagnostics are found in the literature .", "label": "", "metadata": {}, "score": "71.98661"}
{"text": "View Article .Statnikov A , Aliferis CF , Tsamardinos I , Hardin D , Levy S : A comprehensive evaluation of multicategory classification methods for microarray gene expression cancer diagnosis .Bioinformatics 2005 , 21 ( 5 ) : 631 - 643 .", "label": "", "metadata": {}, "score": "72.032104"}
{"text": "The problem of bearing fault diagnostics is formulated to diagnose multiple defects from normal and single defect training data .The accuracy of the proposed OVA strategy on bearing diagnosis of unseen observations is evaluated with vibration data collected from laboratory .", "label": "", "metadata": {}, "score": "72.037125"}
{"text": "Simple majority voting .The system assigns to each word the label that receives the most votes from the three individual systems ( i.e. if at least two individual systems output the same label y for a given word x , y will be chosen as the final label for x ) .", "label": "", "metadata": {}, "score": "72.126205"}
{"text": "Conclusion .During SVM learning , each feature alone had a marginally positive effect on system performance .This supports the fact that the SVM algorithm is robust on the high dimensionality of the feature vector space and means that feature selection is not required .", "label": "", "metadata": {}, "score": "72.16509"}
{"text": "107 - 114 .Heng , A. ; Zhang , S. ; Tan , A. ; Mathew , J. Rotating machinery prognostics : State of the art , challenges and opportunities .Mech .Syst .Signal Process .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "72.19656"}
{"text": "Burges CJC : A Tutorial on support vector machines for pattern recognition .Data Mining and Knowledge Discovery 1998 , 2 ( 2 ) : 121 - 167 .View Article .Pre - publication history .Copyright .\u00a9 Doan et al ; licensee BioMed Central Ltd. 2012 .", "label": "", "metadata": {}, "score": "72.24229"}
{"text": "Core intervals with gene mask .An example for core expression intervals of a gene with 18 and 14 samples belonging to class 1 , in red colour , and class 2 , in green colour , respectively with its associated mask elements .", "label": "", "metadata": {}, "score": "72.50485"}
{"text": "TIFF or PNG formats available in ImageJ / Fiji are non - lossy formats .TIFF preserves any calibrations applied to your images , but images are not compressed .PNG files are smaller as non - lossy compression is applied but they do not store calibration data .", "label": "", "metadata": {}, "score": "72.510635"}
{"text": "Original data should be saved in a way that preserves the exact sample values .Do not store raw image data in file formats such as JPEG which use lossy compression .See Why ( lossy ) JPEGs should not be used in imaging below for details .", "label": "", "metadata": {}, "score": "72.52273"}
{"text": "Average classification error rates for ' Srbct ' and ' Breast ' data based on 50 repetitions 10\u2010fold CV using ISIS , Wil\u2010RS , mRMR , MP and POS methods .The proportional overlapping scores ( POS ) approach yields a good performance with different classifiers on all datasets .", "label": "", "metadata": {}, "score": "72.53073"}
{"text": "w .C .i .n .i .Subject to .y .i . w .T .x .i .b . )i .i . for .i .n . where \u03be i measures the distance between a data point x i and the boundary , for any x i that lies on the wrong side of the margin .", "label": "", "metadata": {}, "score": "72.69424"}
{"text": "As mentioned in the Background section , the corpus includes six types of fields , i.e. , medication , dosage , mode , frequency , duration , and reason .The number of fields and descriptions of the corpus are shown in Table 1 and statistics summarizing the corpus are shown in Table 3 .", "label": "", "metadata": {}, "score": "72.736786"}
{"text": "Reason . \"Dizziness , \" \" Dizzy , \" \" Fever , \" \" Diabetes , \" \" frequent PVCs , \" \" rare angina \" .The medical reason for which the medication is stated to be given .From the perspective of supervised machine learning , the medication extraction task in the 2009 i2b2 challenge can be divided into two steps : 1 ) identifying fields , and 2 ) determining the relationships between the detected medication names and the other fields .", "label": "", "metadata": {}, "score": "72.7719"}
{"text": "In this case , a limitation to specific color tones using the hue value slider is possible with a rather low bias ( if I want only blue , I exclude all the other colors , which is easier to determine than an intensity cut - off value ) .", "label": "", "metadata": {}, "score": "72.799416"}
{"text": "The time between impulses in the vibration plot is the period of the defect which is the reciprocal of the characteristic defect frequency .The same information is shown as peaks at the characteristic defect frequency and its harmonics ( marked with green arrows in Figure 5b .", "label": "", "metadata": {}, "score": "72.90813"}
{"text": "The two single CRF - based , SVM - based systems as well as two local CRF - based and SVM - based voting systems are not significantly different , except for some variations in F - scores for the duration field .", "label": "", "metadata": {}, "score": "73.00936"}
{"text": "PubMed View Article .Doan S , Bastarache L , Klimkowski S , Denny JC , Xu H : Integrating existing natural language processing tools for medication extraction from discharge summaries .J Am Med Inform Assoc 2010 , 17 ( 5 ) : 528 - 531 .", "label": "", "metadata": {}, "score": "73.05318"}
{"text": "PubMed View Article .Lausen B , Hothorn T , Bretz F , Schumacher M : Assessment of optimal selected prognostic factors .Biom J 2004 , 46 ( 3 ) : 364 - 374 .View Article .Altman DG , Lausen B , Sauerbrei W , Schumacher M : Dangers of using \" optimal \" cutpoints in the evaluation of prognostic factors .", "label": "", "metadata": {}, "score": "73.05561"}
{"text": "Fukuda K , Tamura A , Tsunoda T , Takagi T : Toward Information Extraction : Identifying protein names from biological papers .Proceedings of the Pacific Symposium on Biocomputing 1998 , 707 - 718 .Franz\u00e9n K , Eriksson G , Asker FOL , Lid\u00e9n P , C\u00f6ster J : Protein names and how to find them .", "label": "", "metadata": {}, "score": "73.07791"}
{"text": "[ Google Scholar ] .Farrar , C.R. ; Worden , K. Support Vector Machines .In Structural Health Monitoring : A Machine Learning Perspective ; John Wiley & Sons : Chichester , UK , 2013 ; pp .377 - 380 .", "label": "", "metadata": {}, "score": "73.11644"}
{"text": "Total core interval , I i , for gene i is given by the region between the global minimum and global maximum boundaries of core intervals for both classes .It is defined as : .The overlap region , , for gene i is defined as the interval yielded by the intersection between core expression intervals of both target classes .", "label": "", "metadata": {}, "score": "73.23848"}
{"text": "Data and Feature Reduction in Fuzzy Modeling through Particle Swarm Optimization . 1 Department of Electrical and Computer Engineering , University of Alberta , Edmonton , Canada T6 G 2G7 2 Systems Research Institute , Polish Academy of Sciences , 01 - 447 Warsaw , Poland .", "label": "", "metadata": {}, "score": "73.5128"}
{"text": "Of course , we do n't usually build naive Bayes classifiers that contain two identical features .However , we do build classifiers that contain features which are dependent on one another .For example , the features ends - with(a ) and ends - with(vowel ) are dependent on one another , because if an input value has the first feature , then it must also have the second feature .", "label": "", "metadata": {}, "score": "73.56981"}
{"text": "Understanding the properties of SVM - based and CRF - based systems related to duration and reason , we constructed ensembles that benefit those two sets of NEs as described below .Local CRF - based voting .In this model , duration and reason mentions are identified by the CRF , while the remaining mentions come from the SVM .", "label": "", "metadata": {}, "score": "73.60083"}
{"text": "Halgrim S , Xia F , Solti I , Cadag E , Uzuner \u00d6 : Statistical extraction of medication information from clinical records .Proceedings of AMIA Summit on Translational Bioinformatics 2010 , 10 - 12 .Halgrim S , Xia F , Solti I , Cadag E , Uzuner \u00d6 : A cascade of classifiers for extracting medication information from discharge summaries .", "label": "", "metadata": {}, "score": "73.6272"}
{"text": "For smaller sizes , the performance ratio decreases , but the POS approach still provides the best accuracy , see Figure 5 .For k NN and SVM classifiers , the averages of improvements across Leukaemia , Breast , Srbct , Lung , GSE24514 , GSE4045 , GSE14333 and GSE27854 have been depicted at different set sizes up to 50 genes .", "label": "", "metadata": {}, "score": "73.69121"}
{"text": "To date , there have been four RTE Challenges , where shared development and test data is made available to competing teams .Here are a couple of examples of text / hypothesis pairs from the Challenge 3 development dataset .The label True indicates that the entailment holds , and False , that it fails to hold .", "label": "", "metadata": {}, "score": "73.76268"}
{"text": "Support from the Ministry of Higher Education ( MOHE ) Malaysia and Universiti Teknikal Malaysia Melaka ( UTeM ) is gratefully acknowledged .References .W. Pedrycz and F. Gomide , Fuzzy Systems Engineering , John Wiley & Sons , Hoboken , NJ , USA , 2007 .", "label": "", "metadata": {}, "score": "73.815254"}
{"text": "We obtain improvements of up to 1.4 BLEU on language pairs in the WMT 2010 shared task .For languages from different families the improvements often exceed 2 BLEU .Many of these gains are also significant in human evaluations .We present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages : German , English , Swedish , Spanish , French and Korean .", "label": "", "metadata": {}, "score": "74.00806"}
{"text": "Department of Applied Statisitcs , Helwan University .References .Chen K\u2010H , Wang K\u2010J , Tsai M\u2010L , Wang K\u2010M , Adrian AM , Cheng W\u2010C , Yang T\u2010S , Teng N\u2010C , Tan K\u2010P , Chang K\u2010S :Gene selection for cancer identification : a decision tree model empowered by particle swarm optimization algorithm .", "label": "", "metadata": {}, "score": "74.03276"}
{"text": "In this work , we addressed RotBoost ensemble classification method to cope with gene microarray classification problems .This ensemble classifier method is a combination of Rotation Forest and AdaBoost techniques which in turn preserve both desirable features of an ensemble architecture , that is , accuracy and diversity .", "label": "", "metadata": {}, "score": "74.09051"}
{"text": "Gender Identification .In 4 we saw that male and female names have some distinctive characteristics .Names ending in a , e and i are likely to be female , while names ending in k , o , r , s and t are likely to be male .", "label": "", "metadata": {}, "score": "74.18271"}
{"text": "76 names overlapped between the training data and FP of the test data , which was 8.1 % of the names in the FP data .50 names overlapped between the TP and FP of the test data , which was 5.3 % of the names in the FP data .", "label": "", "metadata": {}, "score": "74.343216"}
{"text": "Syst .Signal Process .[ Google Scholar ] .Wang , D. ; Tse , P.W. ; Tsui , K.L. An enhanced Kurtogram method for fault diagnosis of rolling element bearings .Mech .Syst .Signal Process .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "74.36935"}
{"text": "PubMed View Article .Uzuner \u00d6 , Solti I , Cadag E : Extracting medication information from clinical text .J Am Med Inform Assoc 2010 , 17 ( 5 ) : 514 - 518 .PubMed View Article .Patrick J , Li M : High accuracy information extraction of medication information from clinical notes : 2009 i2b2 medication extraction challenge .", "label": "", "metadata": {}, "score": "74.42232"}
{"text": "If the interactions between different defect types are small enough to be ignored in modeling , a combined defect of BPFO and BPFI shall give positive for classifiers isBPFO and isBPFI , and negative for isBSF .In this case , classifiers for combined defects will not be needed and hence training data of combined defects need not be collected .", "label": "", "metadata": {}, "score": "74.47093"}
{"text": "10 , pp .1104 - 1125 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . U.Alon , N. Barka , D. A. Notterman et al . , \" Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "74.54428"}
{"text": "View at Scopus .F. van den Bergh and A. P. Engelbrecht , \" A cooperative approach to particle swarm optimization , \" IEEE Transactions on Evolutionary Computation , vol . 8 , no . 3 , pp .225 - 239 , 2004 .", "label": "", "metadata": {}, "score": "74.55232"}
{"text": "In contrast , explanatory models attempt to capture properties and relationships that cause the linguistic patterns .For example , we might introduce the abstract concept of \" polar verb \" , as one that has an extreme meaning , and categorize some verb like adore and detest as polar .", "label": "", "metadata": {}, "score": "74.58957"}
{"text": "They correspond to the 2 nd run in Table 5 . \" GPD2 \" means the results using GPD2 in dictionary matching .They correspond to the 3 rd run in Table 5 .\" GDP1-stop words \" means the results when the stop words were not ignored in dictionary matching on the 1 st run .", "label": "", "metadata": {}, "score": "74.68065"}
{"text": "The word feature is NF - kappaB. The POS feature is NNP .The orthographic feature is Greek .The prefix features are N , NF , and NF- .The suffix features are B , aB , and paB. The value for uni - gram ( NF - kappaB ) matching with the protein name dictionary is Y. The value for bi - gram ( NF - kappaB transcription ) matching is N. The value for tri - gram ( NF - kappaB transcription factor ) matching is N. The value for uni - gram ( NF - kappaB ) matching with the gene name dictionary is N. The preceding class features are \" O \" for \" the \" ( preceding first ) and \" O \" for \" of \" ( preceding second ) .", "label": "", "metadata": {}, "score": "74.8619"}
{"text": "\" TSST-1 \" is marked as part of a gene / protein name in test set sentence 11506 , but not in sentence 10931 .Similarly , \" PTH \" is marked as part of a gene / protein name in sentence 14477 , but not in sentence 12212 .", "label": "", "metadata": {}, "score": "74.86731"}
{"text": "When the OVA formulation is used , eight of the 20 COMB test samples are correctly classified as BPFO + BPFI .The diagnosis accuracy increases from 0 % for the baseline to 40 % , also much higher than the random guess among eight classes ( 12.5 % ) .", "label": "", "metadata": {}, "score": "75.02782"}
{"text": "The existence of a fault is indicated by the occurrence of the impulses at the characteristic defect frequency , and the impulse amplitude provides some information about how serious the fault is .The same information also occurs as sidebands in the high resonant frequency bands .", "label": "", "metadata": {}, "score": "75.12466"}
{"text": "In this paper , we have proposed a formulation strategy to improve diagnostics performance and reduce the number of scenarios needed in the training data , with focus on multiple defects that may occur concurrently .Better formulation in bearing diagnostics helps build more practical classifiers with less data , for improved performance at more operating conditions .", "label": "", "metadata": {}, "score": "75.21201"}
{"text": "The parenthesized values are the p - values .The values in bold have a statistically significant difference from the base value .A difference is labeled statistically significant when the p - value is less than 0.05 on the Wilcoxon signed - ranks sum test ( two - sided ) .", "label": "", "metadata": {}, "score": "75.32999"}
{"text": "But contextual features often provide powerful clues about the correct tag - for example , when tagging the word \" fly , \" knowing that the previous word is \" a \" will allow us to determine that it is functioning as a noun , not a verb .", "label": "", "metadata": {}, "score": "75.43276"}
{"text": "Either of the bearings on the left and right of the shaft can be at fault and we assume that the faulty bearing has been located in the isolation stage .Vibration data were collected from the accelerometers at vertical and horizontal direction of the bearing either on the left or the right of the rotors , i.e. , either positions C and D , or E and F in Figure 7 .", "label": "", "metadata": {}, "score": "75.45874"}
{"text": "In this article , we develop a new score , called proportional overlapping score ( POS ) , that estimates the overlapping degree of a gene taking into account this element , i.e. proportion of each class 's overlapped samples to the total number of overlapping samples .", "label": "", "metadata": {}, "score": "75.46826"}
{"text": "Signal Process .[ Google Scholar ] .Dong , M. ; He , D. A segmental hidden semi - Markov model ( HSMM)-based diagnostics and prognostics framework and methodology .Mech .Syst .Signal Process .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "75.66707"}
{"text": "We present a new edition of the Google Books Ngram Corpus , which describes how often words and phrases were used over a period of five centuries , in eight languages ; it reflects 6 % of all books ever published .", "label": "", "metadata": {}, "score": "75.80971"}
{"text": "For the GSE14333 data , patients with colorectal cancer of I and II tumor ' Union Internationale Contre le Cancer ( UICC ) ' stages are combined in a single class representing non\u2010invasive tumors , against patients with stage III , which represents invasive tumors .", "label": "", "metadata": {}, "score": "75.94967"}
{"text": "In Section 2 , the framework of RotBoost is described in detail .In Section 3 , the experimental results and corresponding discussions are presented .Section 4 concludes the paper .Materials and Methods .The Description of RotBoost Ensemble Classification .", "label": "", "metadata": {}, "score": "75.9861"}
{"text": "The score of majority voting was significantly higher than any single method including the method from the Sydney team , as determined by the statistical tests ( Table 8 ) .When considering each field separately , majority voting consistently outperformed the single methods in recognizing duration and reason .", "label": "", "metadata": {}, "score": "76.03617"}
{"text": "In particular , the identity of the previous word is included as a feature .It is clear that exploiting contextual features improves the performance of our part - of - speech tagger .For example , the classifier learns that a word is likely to be a noun if it comes immediately after the word \" large \" or the word \" gubernatorial \" .", "label": "", "metadata": {}, "score": "76.04498"}
{"text": "Figure 4a shows a temporal signal of a normal bearing data sample and Figure 5a shows the plot of a BPFO bearing data sample .For each sample in this data set , the vibration signal is transformed by FFT to the frequency spectrum .", "label": "", "metadata": {}, "score": "76.04982"}
{"text": "Considering a fixed percentage of the data used , we look at the nature of the feature sets .Tables 8 , 9 and 10 displays the best feature for PM10 data , Body fat data , and Housing data , respectively .", "label": "", "metadata": {}, "score": "76.086395"}
{"text": "The occurrence of impulses is pseudo - cyclostationary [ 10 ] , i.e. , they can be modeled as cyclostationary , although in reality the impulse due to a particular fault may not be excited in every cycle of rotation .Since the rotation of the machinery is periodic , impulses and the damped signals due to a particular defect also tend to be generated periodically .", "label": "", "metadata": {}, "score": "76.14665"}
{"text": "Our grammars automatically learn the kinds of linguistic distinctions exhibited in previous work on manual tree annotation .On the other hand , our grammars are much more compact and substantially more accurate than previous work on automatic annotation .Despite its simplicity , our best grammar achieves an F1 of 89.9 % on the Penn Treebank , higher than most fully lexicalized systems .", "label": "", "metadata": {}, "score": "76.27234"}
{"text": "The relation between the accuracy and stability has been outlined by Figures 8 and 9 for the ' Lung ' and ' GSE27854 ' respectively .The stability scores were combined with corresponding error rates yielded by three different classifiers : RF ; k NN ; SVM .", "label": "", "metadata": {}, "score": "76.3694"}
{"text": "JPEG stands for Joint Photographic Experts Group who were the creators of a commonly used method of lossy compression for photographic images .This format is commonly used in web pages and supported by the vast majority of digital photographic cameras and image scanners because it can store images in relatively small files at the expense of image quality .", "label": "", "metadata": {}, "score": "76.4249"}
{"text": "PubMed View Article .Gordon GJ , Jensen RV , Hsiao L\u2010L , Gullans SR , Blumenstock JE , Ramaswamy S , Richards WG , Sugarbaker DJ , Bueno R : Translation of microarray data into clinically relevant cancer diagnostic tests using gene expression ratios in lung cancer and mesothelioma .", "label": "", "metadata": {}, "score": "76.43204"}
{"text": "In practice , vibration signals due to defect impacts mix together with those generated by normal rotation of the same component , other components of the same rotating machinery and other random vibrations .Another complication is that the actual frequency is affected by the tightness - of - fit of the components and there are generally some differences between the measured frequencies and the theoretical characteristic defect frequency values due to random slip [ 8 ] .", "label": "", "metadata": {}, "score": "76.447655"}
{"text": "The TrEMBL database consists of computer - annotated entries derived from the translation of all coding sequences in the nucleotide sequence database .The sequences are not yet represented in the SWISS - PROT database .The TrEMBL database also contains protein sequences extracted from the literature and protein sequences submitted directly by the user community .", "label": "", "metadata": {}, "score": "76.71514"}
{"text": "The values in bold have a statistically significant difference from the base value .A difference is labeled statistically significant when the p - value is less than 0.05 on the Wilcoxon signed - ranks sum test ( two - sided ) .", "label": "", "metadata": {}, "score": "76.7973"}
{"text": "[ Google Scholar ] .Tandon , N. ; Choudhury , A. A review of vibration and acoustic measurement methods for the detection of defects in rolling element bearings .Tribol .Int .[ Google Scholar ] .Mitchell , J. From vibration measurements to condition - based maintenance .", "label": "", "metadata": {}, "score": "76.82965"}
{"text": "An accelerometer was connected at vertical direction of the bearing .Vibration data were collected at a sampling rate of 4,000 Hz .Two samples were extracted for each experimental condition .Each sample consists of one temporal signal of 0.5 s length .", "label": "", "metadata": {}, "score": "77.01165"}
{"text": "[ Google Scholar ] .McFadden , P. ; Smith , J. Vibration monitoring of rolling element bearings by the high - frequency resonance technique - A review .Tribol .Int .[ Google Scholar ] .McInerny , S. ; Dai , Y. Basic vibration signal processing for bearing fault detection .", "label": "", "metadata": {}, "score": "77.29085"}
{"text": "It will help build more practical classifiers with less data , for improved performance under more operating conditions .These help provide advanced failure warning and reduce unexpected failures in real applications .This paper is a study on vibration - based bearing diagnostics with the data - driven approach .", "label": "", "metadata": {}, "score": "77.41048"}
{"text": "Three voting methods were proposed and evaluated using the annotated data sets from the 2009 i2b2 NLP challenge : simple majority , local SVM - based voting , and local CRF - based voting .Results .Evaluation on 268 manually annotated discharge summaries from the i2b2 challenge showed that the local CRF - based voting method achieved the best F - score of 90.84 % ( 94.11 % Precision , 87.81 % Recall ) for 10-fold cross - validation .", "label": "", "metadata": {}, "score": "77.4218"}
{"text": "In this figure , triangles are positive examples and black dots are negative examples .The hyperplane is given by .Two possible hyper planes and margins between positive and negative examples .White triangles are positive examples .Black dots are negative examples .", "label": "", "metadata": {}, "score": "77.71748"}
{"text": "Duration . \"x10 days , \" \" 10-day course , \" \" For ten days , \" \" For a month , \" \" During spring break , \" \" Until the symptom disappears , \" \" As long as needed \" .", "label": "", "metadata": {}, "score": "77.76229"}
{"text": "The values in bold have a statistically significant difference from the 1 st value .A difference is labeled statistically significant when the p - value is less than 0.05 on the Wilcoxon signed - ranks sum test ( two - sided ) .", "label": "", "metadata": {}, "score": "78.334564"}
{"text": "As an example , consider the multi - class bearing fault diagnosis of normal and three fault types ( BPFI , BPFO , BSF ) .Applying the proposed OVA diagnostic approach to the three fault types , the 4-class classification problem among Normal , BPFI , BPFO , BSF is transformed into three 2-class classification sub - problems .", "label": "", "metadata": {}, "score": "78.36872"}
{"text": "Use those formats instead .A format called JPEG2000 supported by various slide scanners and used in \" virtual slide \" products was created to improve image quality and compression rates , however both lossy and non - lossy versions the JPEG2000 format exist .", "label": "", "metadata": {}, "score": "78.410545"}
{"text": "It contains data for four words : hard , interest , line , and serve .Choose one of these four words , and load the corresponding data : .Using this dataset , build a classifier that predicts the correct sense tag for a given instance .", "label": "", "metadata": {}, "score": "78.44357"}
{"text": "Particle Swarm Optimization and Its Cooperative Version .Population - based algorithms provide interesting solutions since any constructive method can be used to generate the initial population , and any local search technique can be used to improve each solution in the population [ 30 ] .", "label": "", "metadata": {}, "score": "78.60764"}
{"text": "Two dashed lines are the boundaries between the positive and negative examples .An SVM finds the optimal hyperplane , which is the one with the maximum margin .The two dashed lines in Figure 4 are the boundaries between the positive and negative examples .", "label": "", "metadata": {}, "score": "78.81998"}
{"text": "On Leukaemia and GSE4045 datasets , POS is outperformed by other methods for set sizes more than five and 20 respectively .More details of the SVM classifier 's results can be found in the Additional file 3 .The improvement / deterioration in the classification accuracy is analyzed in order to investigate the quality performance of our proposal against the other techniques when the size of the selected gene set varies .", "label": "", "metadata": {}, "score": "78.89148"}
{"text": "View Article .Torii M , Hu Z , Wu CH , Liu H : BioTagger - GM : a gene / protein name recognition system .J Am Med Inform Assoc 2009 , 16 : 247 - 255 .PubMed View Article .", "label": "", "metadata": {}, "score": "78.91823"}
{"text": "Appl .Artif .Intell .[ Google Scholar ] .Kim , H.E. ; Tan , A.C. ; Mathew , J. ; Choi , B.K. Bearing fault prognosis based on health state probability estimation .Expert Syst .Appl .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "79.24427"}
{"text": "For ' GSE27854 ' data with the k NN classifier , POS provides a better trade\u2010off between accuracy and stability than other compared methods .Whereas with the RF and SVM classifiers , POS is outperformed by Wil\u2010RS . Stability\u2010accuracy plot for ' Lung ' dataset .", "label": "", "metadata": {}, "score": "79.29664"}
{"text": "The challenge asked that participating systems be used to extract the text corresponding to each of the fields for each medication mention .Among the top ten systems achieving the best performance in the 2009 i2b2 challenge , there were two machine learning based systems : the Sydney team ranked first while the Wisconsin team ranked tenth in the final evaluation [ 9 ] .", "label": "", "metadata": {}, "score": "79.48037"}
{"text": "Table 7 shows the results of the OVA formulation and the correct fault diagnoses are highlighted in bold .Performance Evaluation .The prediction performance of normal and single - defect bearings is first compared .When one- versus -all ( OVA ) formulation is used , the number of correct classification is 66.7 % .", "label": "", "metadata": {}, "score": "79.6129"}
{"text": "Is there one superior automatic algorithm ?For all of the reasons outlined above , NO !But this is not really a problem .It is akin to asking the question \" Is there one superior food ? \" As is the case with biological protocols in general , most algorithms are developed serving a specific purpose or solving a specific extraction problem .", "label": "", "metadata": {}, "score": "79.87399"}
{"text": "Kikuchi A , Ishikawa T , Mogushi K , Ishiguro M , Iida S , Mizushima H , Uetake H , Tanaka H , Sugihara K : Identification of nucks1 as a colorectal cancer prognostic marker through integrated expression and copy number analysis .", "label": "", "metadata": {}, "score": "79.93504"}
{"text": "Overlapping samples set , , for gene i is the set containing the samples whose expression values fall within the overlap interval , defined in the overlap region definition ( see equation 5 ) .The overlapping sample set can be defined as : . where represents the non\u2010overlapping samples set which is defined as follows .", "label": "", "metadata": {}, "score": "79.977875"}
{"text": "Human visual perception is very good at certain tasks , such as contrast correction and detecting subtle differences in bright colors , but notoriously bad with other things , such as discerning dark colors , or classifying colors without appropriate reference .", "label": "", "metadata": {}, "score": "80.15993"}
{"text": "A One- Versus -All Class Binarization Strategy for Bearing Diagnostics of Concurrent Defects .Department of Systems Engineering and Engineering Management ( SEEM ) , City University of Hong Kong , Tat Chee Avenue , Kowloon , Hong Kong , China .", "label": "", "metadata": {}, "score": "80.18578"}
{"text": "The literature concerning the use of acceleration signals obtained from piezoelectric sensors in bearing diagnostics is so huge that a review is not the most useful format in summarizing recent research developments .Below is an overview of the logic behind vibration - based bearing fault diagnostics and some literature highlights of two approaches to the analysis , with focus on the statistical and data mining approach .", "label": "", "metadata": {}, "score": "80.29571"}
{"text": "Balderston , H. Incipient Failure Detection : Incipient Failure Detection in Ball Bearings ; Technical report ; Defense Technical Information Center : Ft .Belvoir , VA , USA , 1969 .[ Google Scholar ] .Gupta , P. Dynamics of rolling - element bearings - Part III , IV .", "label": "", "metadata": {}, "score": "80.54691"}
{"text": "In rotating machinery , whenever a defect makes contact with another surface , a high - level short - duration vibration impulse is excited .The effect is a damped signal which comprises of a sharp rise corresponding to the impact and approximately exponential decay [ 8 , 9 ] .", "label": "", "metadata": {}, "score": "80.70462"}
{"text": "PubMed Central PubMed View Article .Science 1999 , 286 ( 5439 ) : 531 - 537 .PubMed View Article .Michiels S , Koscielny S , Hill C : Prediction of cancer outcome with microarrays : a multiple random validation strategy .", "label": "", "metadata": {}, "score": "80.98764"}
{"text": "Deciding whether an email is spam or not .Deciding what the topic of a news article is , from a fixed list of topic areas such as \" sports , \" \" technology , \" and \" politics . \"Deciding whether a given occurrence of the word bank is used to refer to a river bank , a financial institution , the act of tilting to the side , or the act of depositing something in a financial institution .", "label": "", "metadata": {}, "score": "81.0755"}
{"text": "Such a framework could be effective in selecting the discriminative genes with a low degree of dependency .Declarations .Acknowledgements .We thank the referees and the editor for valuable comments which improved the paper considerably .The first author , OM , was financially supported by The Egyptian Ministry of Higher Education and Egyptian Cultural and Education Bureau ( ECEB ) in London , United Kingdom .", "label": "", "metadata": {}, "score": "81.33571"}
{"text": "Examples of research papers applying C4.5 decision trees to bearing fault diagnostics include [ 30 , 48 , 59 , 60 ] .They have also been applied in other machinery fault diagnostics problems such as motor [ 61 ] , pump [ 62 ] and shaft rotor [ 63 ] .", "label": "", "metadata": {}, "score": "81.3859"}
{"text": "[ Google Scholar ] .Sakthivel , N. ; Sugumaran , V. ; Babudevasenapati , S. Vibration based fault diagnosis of monoblock centrifugal pump using decision tree .Expert Syst .Appl .[ Google Scholar ] .Sun , W. ; Chen , J. ; Li , J. Decision tree and PCA - based fault diagnosis of rotating machinery .", "label": "", "metadata": {}, "score": "81.73727"}
{"text": "Multichannel images in particular are harmed by the JPEG format : since the multiple channels are misinterpreted as red , green and blue ( while most channels integrate more than just one wavelength ) , JPEG will shift the colors imperceptibly to improve compression .", "label": "", "metadata": {}, "score": "81.74433"}
{"text": "Conflicts of Interest .The authors declare no conflict of interest .References .Nandi , S. ; Toliyat , H.A. ; Li , X. Condition monitoring and fault diagnosis of electrical motors - a review .IEEE Trans .Energy Convers .", "label": "", "metadata": {}, "score": "81.78857"}
{"text": "The BPFO and BPFI combined defect is used in the subsequent empirical analyses in this paper .Performance Evaluation - Bearing Fault Motor .In this section , the proposed OVA strategy is evaluated with experimental data from a bearing motor test bed .", "label": "", "metadata": {}, "score": "81.7906"}
{"text": "Notterman DA , Alon U , Sierk AJ , Levine AJ : Transcriptional gene expression profiles of colorectal adenoma , adenocarcinoma , and normal tissue examined by oligonucleotide arrays .Cancer Res 2001 , 61 ( 7 ) : 3124 - 3130 .", "label": "", "metadata": {}, "score": "82.11409"}
{"text": "Technol .[ Google Scholar ] .McFadden , P. ; Smith , J. The vibration produced by multiple point defects in a rolling element bearing .J. Sound Vib .[ Google Scholar ] .McFadden , P. ; Smith , J. Model for the vibration produced by a single point defect in a rolling element bearing .", "label": "", "metadata": {}, "score": "82.163376"}
{"text": "For example , consider a classifier that determines the correct word sense for each occurrence of the word bank .If we evaluate this classifier on financial newswire text , then we may find that the financial - institution sense appears 19 times out of 20 .", "label": "", "metadata": {}, "score": "82.80935"}
{"text": "PubMed View Article .Ma C , Dong X , Li R , Liu L : A computational study identifies hiv progression\u2010related genes using mrmr and shortest path tracing .PLOS ONE 2013 , 8 ( 11 ) : 78057 .View Article .", "label": "", "metadata": {}, "score": "83.09883"}
{"text": "More details of how this procedure is applied are discussed in the next section .The Proposed Methodology .In practice , incipient faults first detected from vibration signals are usually BPFO ; BPFI ; or BSF .Multiple types of defects can develop on the same bearing concurrently i.e. , the defect types are non - exclusive classes .", "label": "", "metadata": {}, "score": "83.26964"}
{"text": "Finally , this paper closes with the conclusions in Section 6 .Bearing Fault Diagnostics .Bearings are one of the most widely used components in machines and bearing failure is one of the most frequent reasons for machine breakdown .According to [ 1 ] , almost 40%-50 % of motor failures are bearing - related .", "label": "", "metadata": {}, "score": "83.36778"}
{"text": "f .X . ) w .T .x .b .i .n .w .i .x .i .b .y .i .f .x .i . )y .i . w .", "label": "", "metadata": {}, "score": "83.563416"}
{"text": "Words ending in -ed tend to be past tense verbs ( 5 . )Frequent use of will is indicative of news text ( 3 ) .These observable patterns - word structure and word frequency - happen to correlate with particular aspects of meaning , such as tense and topic .", "label": "", "metadata": {}, "score": "83.79131"}
{"text": "In the training corpus , most documents are automotive , so the classifier starts out at a point closer to the \" automotive \" label .But it then considers the effect of each feature .In this example , the input document contains the word \" dark , \" which is a weak indicator for murder mysteries , but it also contains the word \" football , \" which is a strong indicator for sports documents .", "label": "", "metadata": {}, "score": "84.06639"}
{"text": "For the Random Forest classifier , POS performed better than the compared feature selection methods on ' Leukaemia ' , ' Breast ' , ' GSE24514 ' and ' GSE4045 ' datasets at all gene set sizes that have been investigated .", "label": "", "metadata": {}, "score": "84.1019"}
{"text": "Chiaretti S , Li X , Gentleman R , Vitale A , Vignetti M , Mandelli F , Ritz J , Foa R : Gene expression profile of adult t\u2010cell acute lymphocytic leukemia identifies distinct subsets of patients with different response to therapy and survival .", "label": "", "metadata": {}, "score": "84.42662"}
{"text": "On the Srbct , All and Lung datasets , the POS method provides lower error rates than all other methods on most set sizes .While , on the Prostate dataset , POS shows a comparable performance with the best technique ( MP ) .", "label": "", "metadata": {}, "score": "84.45604"}
{"text": "Our system based on majority voting achieved a better F - score of 89.65 % ( 93.91 % Precision , 85.76 % Recall ) than the previously reported F - score of 89.19 % ( 93.78 % Precision , 85.03 % Recall ) by the first - ranked system in the challenge .", "label": "", "metadata": {}, "score": "84.51825"}
{"text": "The vibration data were collected at a sampling rate of 32,768 Hz .Ten samples were extracted for each combination of experimental conditions .Each sample consists of two temporal signals of one second length , collected from the sensors along the vertical and horizontal directions .", "label": "", "metadata": {}, "score": "84.54979"}
{"text": "AH , AP , AG , ZK and MM contributed to the design of the study and edited the manuscript .All authors read and approved the final manuscript .Authors ' Affiliations .Department of Mathematical Sciences , University of Essex .", "label": "", "metadata": {}, "score": "84.61375"}
{"text": "C. Zhang and J. Zhang [ 19 ] proposed a novel ensemble classifier generation method RotBoost through combining Rotation Forest and AdaBoost .In this new ensemble method , the base classifier in Rotation Forest algorithm is replaced with AdaBoost .The experimental results show that RotBoost performs better than either Rotation Forest or AdaBoost when using some non - microarray gene - related data sets from the UCI repository .", "label": "", "metadata": {}, "score": "85.19385"}
{"text": "The Data Set .The data set used in this empirical analysis was collected from a bearing fault motor as shown in Figure 3 .The bearing physical parameters and the bearing characteristic defect frequencies calculation are shown in Table 4 .", "label": "", "metadata": {}, "score": "85.591034"}
{"text": "36 - 43 .Sugumaran , V. ; Ramachandran , K. Automatic rule learning using decision tree for fuzzy classifier in fault diagnosis of roller bearing .Mech .Syst .Signal Process .[ Google Scholar ] .Sugumaran , V. ; Muralidharan , V. ; Ramachandran , K. Feature selection using Decision Tree and classification through Proximal Support Vector Machine for fault diagnostics of roller bearing .", "label": "", "metadata": {}, "score": "86.07118"}
{"text": "True negatives are irrelevant items that we correctly identified as irrelevant .False positives ( or Type I errors ) are irrelevant items that we incorrectly identified as relevant .False negatives ( or Type II errors ) are relevant items that we incorrectly identified as irrelevant .", "label": "", "metadata": {}, "score": "86.18776"}
{"text": "Ann Surg 2008 , 247 ( 5 ) : 803 - 810 .PubMed View Article .Copyright .\u00a9 Mahmoud et al . ; licensee BioMed Central Ltd. 2014 .This article is published under license to BioMed Central Ltd.Learning to Classify Text .", "label": "", "metadata": {}, "score": "86.25931"}
{"text": "PubMed Central PubMed View Article .Apiletti D , Baralis E , Bruno G , Fiori A : The painter 's feature selection for gene expression data .In Engineering in Medicine and Biology Society , 2007 .EMBS 2007 .29th", "label": "", "metadata": {}, "score": "86.31235"}
{"text": "The signal processing approach constitutes a major part of the literature in vibration - based bearing fault diagnostics .This approach has been developed for decades with a huge literature and is not the focus of this paper .A useful review in this classic approach is given in [ 11 ] , which summarizes various topics in bearing condition monitoring , from the underlying science to signal processing techniques that can be used with signals from accelerometers .", "label": "", "metadata": {}, "score": "86.567665"}
{"text": "For an extreme example , consider the string interleukin-1 [ IL-1 ] , tumor necrosis factor - alpha [ TNF - alpha ] , which the tagger incorrectly returns as being one entity .The gold standard identifies 4 different entities within this string , interleukin-1 , IL-1 , tumor necrosis factor - alpha and TNF - alpha .", "label": "", "metadata": {}, "score": "86.96462"}
{"text": "Educ .[ Google Scholar ] .Randall , R. ; Antoni , J. Rolling element bearing diagnostics - A tutorial .Mech .Syst .Signal Process .[ Google Scholar ] .Howard , I. A Review of Rolling Element Bearing Vibration \" Detection , Diagnosis and Prognosis \" ; Defence Science and Technology Oganisation : Melbourne , Australia , 1994 .", "label": "", "metadata": {}, "score": "87.91458"}
{"text": "PubMed View Article .Xu H , Stenner SP , Doan S , Johnson KB , Waitman LR , Denny JC : MedEx : a medication information extraction system for clinical narratives .J Am Med Inform Assoc 2010 , 17 ( 1 ) : 19 - 24 .", "label": "", "metadata": {}, "score": "89.17525"}
{"text": "For example , when classifying documents into topics ( such as sports , automotive , or murder mystery ) , features such as hasword(football ) are highly indicative of a specific label , regardless of what other the feature values are .", "label": "", "metadata": {}, "score": "89.36023"}
{"text": "Results of the Experiments .In the experiments , we looked at the performance - an average root mean squared error ( RMSE)-obtained for the selected combinations of the number of features and data ( instances ) .The results obtained for the Housing data , PM10 data , and Parkinson 's data for .", "label": "", "metadata": {}, "score": "90.16138"}
{"text": "Received : 20 November 2013 ; in revised form : 25 December 2013 / Accepted : 7 January 2014 / Published : 13 January 2014 .Abstract . :In bearing diagnostics using a data - driven modeling approach , a concern is the need for data from all possible scenarios to build a practical model for all operating conditions .", "label": "", "metadata": {}, "score": "90.939224"}
{"text": "Int J Cancer 2012 , 130 ( 7 ) : 1558 - 1566 .PubMed View Article .Oncogene 2007 , 26 ( 2 ) : 312 - 320 .PubMed View Article .Clinical Cancer Res 2009 , 15 ( 24 ) : 7642 - 7651 .", "label": "", "metadata": {}, "score": "93.21654"}
{"text": "An Efficient Ensemble Learning Method for Gene Microarray Classification .Department of Computer Engineering , Islamic Azad University , Dezful Branch , Dezful 313 , Iran .Received 30 April 2013 ; Accepted 12 July 2013 .Copyright \u00a9 2013 Alireza Osareh and Bita Shadgar .", "label": "", "metadata": {}, "score": "93.49298"}
{"text": "This Master 's thesis describes parts of the control software used by the soccer robots of the Free University of Berlin , the so called FU - Fighters .The FU - Fighters compete in the Middle Sized League of RoboCup and reached the semi - finals during the 2004 RoboCup World Cup in Lisbon , Portugal .", "label": "", "metadata": {}, "score": "93.65416"}
{"text": "Challenge 3 , Pair 81 ( False ) .T :According to NC Articles of Organization , the members of LLC company are H. Nelson Beavers , III , H. Chester Beavers and Jennie Beavers Stewart .H : Jennie Beavers Stewart is a share - holder of Carolina Analytical Laboratory .", "label": "", "metadata": {}, "score": "94.69821"}
{"text": "Acknowledgements .The authors would like to thank our collaborators Mark Liberman , Andy Schein , Pete White and Scott Winters for useful discussions and suggestions .We would also like to thank Lorraine Tanabe for making the ABGene lexicons available to us .", "label": "", "metadata": {}, "score": "96.5762"}
{"text": "Department of Computer Science , Posts and Telecommunications Institute of Technology .References .Hirschman L , Morgan AA , Yeh AS : Rutabaga by any other name : extracting biological names .J Biomed Inform 2002 , 35 : 247 - 259 .", "label": "", "metadata": {}, "score": "97.86047"}
{"text": "T : Parviz Davudi was representing Iran at a meeting of the Shanghai Co - operation Organisation ( SCO ) , the fledgling association that binds Russia , China and four former Soviet republics of central Asia together to fight terrorism .", "label": "", "metadata": {}, "score": "102.94263"}
{"text": "The difference between those training data was that the Sydney team chose the 145 longest notes while the Wisconsin team randomly selected 147 notes from the training data [ 10 ] .The second best system , the Vanderbilt team , used a rule - based system which extended their MedEx system [ 11 ] .", "label": "", "metadata": {}, "score": "102.99146"}
{"text": "When compared with Rotation Forest , the statistically significant difference is favorable in 6 datasets , although the Rotation Forest could surpass the RotBoost when working on Breast and Ovarian datasets .Indeed , RotBoost is seen to outperform AdaBoost in most cases even though the advantage of RotBoost is not significant in 1 dataset and tie is occurred on the remaining 2 datasets .", "label": "", "metadata": {}, "score": "103.13759"}
{"text": "Academic Editor : Miin - Shen Yang .Copyright \u00a9 2012 S. Sakinah S. Ahmad and Witold Pedrycz .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "115.29297"}
