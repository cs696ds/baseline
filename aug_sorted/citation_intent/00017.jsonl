{"text": "The recognizer determines that there are more frames to be considered when two conditions are met .First , more frames must be available .Upon receiving the list of words from the pre - filtering procedure , the recognizer uses the list of words to create hypotheses or to expand any hypothesis for which a word has ended ( step 1280 ) .", "label": "", "metadata": {}, "score": "32.747017"}
{"text": "In generating the score for a hypothesis , the recognizer uses acoustic scores for words of the hypothesis , a language model score that indicates the likelihood that words of the hypothesis are used together , and scores provided for each word of the hypothesis by the pre - filtering procedure .", "label": "", "metadata": {}, "score": "33.077446"}
{"text": "The recognizer then retrieves the next frame ( step 1210 ) and repeats the procedure .If there are no more speech frames to process , then the recognizer 215 provides the most likely hypotheses to the control / interface module 220 as recognition candidates ( step 1285 ) .", "label": "", "metadata": {}, "score": "33.39821"}
{"text": "The lexical tree pre - filtering procedure is discussed in detail in U.S. Pat .No .5,822,730 , titled \" LEXICAL TREE PRE - FILTERING IN SPEECH RECOGNITION \" and issued Oct. 13 , 1998 , which is incorporated by reference .After the pre - filtering procedure responds with the requested list of words , the recognizer initiates a hypothesis for each word from the list and compares acoustic models for the word to the frames of parameters representing the utterance .", "label": "", "metadata": {}, "score": "33.702354"}
{"text": "That is , while a TDNN performs temporal i .. \" ...In this paper we describe the NPen + + system for writer independent on - line handwriting recognition .This recognizer needs no training for a particular writer and can recognize any common writing style ( cursive , hand - printed , or a mixture of both ) .", "label": "", "metadata": {}, "score": "33.979614"}
{"text": "In one example carried out by the inventors , a generic recognizer provided by a commercial vendor was trained on a large and varied native speech corpus .The acoustic model was adapted using batch - mode MAP adaptation .The adaptation corpus contained about 2,000 responses having high scores in previous ELPT tests .", "label": "", "metadata": {}, "score": "34.416763"}
{"text": "First , the n - best list generated by the speech recognizer is expanded with additional candidates , based on confusability information captured via user click statistics .In the second stage , this expanded list is rescored and pruned to produce a more accurate and compact n - best list .", "label": "", "metadata": {}, "score": "34.96814"}
{"text": "The recognizer then compares the frame to acoustic models 235 for the last word of the hypothesis ( step 1225 ) and , based on the comparison , updates a score associated with the hypothesis ( step 1230 ) .After updating the score ( step 1230 ) , the recognizer determines whether the user was likely to have spoken the word or words corresponding to the hypothesis ( step 1235 ) .", "label": "", "metadata": {}, "score": "35.413143"}
{"text": "The parameters of the acoustic Markov models may be estimated from a known uttered training text by , for example , the Forward - Backward Algorithm .( See , for example , L. R. Bahl , et al . \"A Maximum Likelihood Approach to Continuous Speech Recognition . \"", "label": "", "metadata": {}, "score": "35.471687"}
{"text": "A speech recognition method as claimed in claim 19 , further comprising the steps of : . storing a source vocabulary of words in the source language ; . comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "35.865562"}
{"text": "The proposed model works in two stages .First , the n - best list generated by the speech recognizer is expanded with additiona ... \" .We describe a novel n - best correction model that can leverage implicit user feedback ( in the form of clicks ) to improve performance in a multi - modal speech - search application .", "label": "", "metadata": {}, "score": "36.384277"}
{"text": "This work also demonstrates that in the case where the recognizer can be tuned to the new task , environment , or speaker , the post - processor can also contribute to performance improvements .ome training data .We adopt statistical techniques ( some of them from statistical machine translation ) for modeling that channel in order to correct some of the errors introduced there .", "label": "", "metadata": {}, "score": "36.576637"}
{"text": "Accordingly , a lower score indicates a better match ( a high probability ) while a higher score indicates a less likely match ( a lower probability ) , with the likelihood of the match decreasing as the score increases .After processing the utterance , the recognizer provides the best - scoring hypotheses to the control / interface module 220 as a list of recognition candidates , where each recognition candidate corresponds to a hypothesis and has an associated score .", "label": "", "metadata": {}, "score": "37.473415"}
{"text": "Advantageously , the accuracy of each subordinate message recognizer sharing a language model that includes a semantic model according to various aspects of the present invention may be improved by user identification of context .A message recognizer ( e.g. , speech recognizer 162 of system 100 or message recognizer 310 of system 200 ) may perform a context - sensitive hypothesis test of text segments against message input in accordance with an adapted semantic model .", "label": "", "metadata": {}, "score": "37.502865"}
{"text": "The Multi - State Time Delay Neural Network ( MS - TDNN ) integrates a nonlinear time alignment procedure ( DTW ) and the highaccuracy phoneme spotting capabilities of a TDNN into a connectionist speech recognition system with word - level classification and error backpropagation .", "label": "", "metadata": {}, "score": "37.72201"}
{"text": "The Multi - State Time Delay Neural Network ( MS - TDNN ) integrates a nonlinear time alignment procedure ( DTW ) and the highaccuracy phoneme spotting capabilities of a TDNN into a connectionist speech recognition system with word - level classification and error backpropagation .", "label": "", "metadata": {}, "score": "37.72201"}
{"text": "No .730,714 , filed on Jul. 16 , 1991 , entitled \" Past Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition .\"Alternatively , all acoustic feature vectors generated by the utterance of a training text and which correspond to a given elementary model may be clustered by K - means Euclidean clustering or K - means Gaussian clustering , or both .", "label": "", "metadata": {}, "score": "38.253723"}
{"text": "In this paper we describe the NPen + + system for writer independent on - line handwriting recognition .This recognizer needs no training for a particular writer and can recognize any common writing style ( cursive , hand - printed , or a mixture of both ) .", "label": "", "metadata": {}, "score": "38.429535"}
{"text": "Each of the target hypotheses includes text fragments selected from the second language .The target hypothesis is evaluated with a translation scoring function which scores the target hypothesis according to a plurality of feature functions .At least one of the feature functions includes a gap size scoring feature which favors hypotheses with statistically more probable gap sizes over hypotheses with statically less probable gap sizes .", "label": "", "metadata": {}, "score": "38.668743"}
{"text": "The words or phrases corresponding to the best matching acoustic models are referred to as recognition candidates .The processor may produce a single recognition candidate for an utterance , or may produce a list of recognition candidates .SUMMARY .In one general aspect , a method of computer - implemented speech recognition includes performing speech recognition on an utterance to produce a recognition result for the utterance , and determining if the word closely corresponds to a portion of the phrase .", "label": "", "metadata": {}, "score": "38.69214"}
{"text": "The system includes a processor which executes instructions for receiving the source text in the first language , accessing the library to retrieve text fragments from the second language , scoring hypotheses according to a plurality of features including the gap scoring feature .", "label": "", "metadata": {}, "score": "38.99389"}
{"text": "In the speech recognition system and method according to the present invention , information about the source sentence being translated is used to estimate the probability that each speech hypothesis would be uttered .This probability is estimated with the aid of a translation model .", "label": "", "metadata": {}, "score": "39.010372"}
{"text": "We achieve recognition rates up to 99.5 % on writer independent , single character recognition tasks and up to 98.1 % on writer dependent , cursive handwriting tasks .1 Introduction Several different preprocessing techniques both for optical character recognition ( OCR ) and on - line character ... . ... t recognizer [ 1 ] , which is well suited for handling temporal sequences of patterns as provided by this kind of input representation .", "label": "", "metadata": {}, "score": "39.334938"}
{"text": "A more detailed description of the use of word posterior probabilities and their application to the Hub5 task can be found in [ 1 ] .This section gives an overview of the complete system as used in the March 2000 evaluation .The system operates in multiple passes through the data : initial passes are used to generate word lattices and then these lattices are rescored using four different sets of adapted acoustic models .", "label": "", "metadata": {}, "score": "39.4024"}
{"text": "The list includes text segments that each received a high score ( but not the maximum score ) during hypothesis testing of the message recognizer .Two lists of alternative text of the type that may be produced at step 1430 are shown by way of example in TABLE IV and TABLE V below .", "label": "", "metadata": {}, "score": "39.462082"}
{"text": "During message recognition in accordance with the first exemplary semantic model , when implemented by the equation above , a ratio is computed for each semantic cluster C 1 . . .C M in the model .The probability of the tested text segment w i occurring in each semantic cluster is divided by the combined probability of user identified stop segments s 1 . . .", "label": "", "metadata": {}, "score": "39.586533"}
{"text": "A speech recognition system as claimed in claim 1 , further comprising : . means for storing a source vocabulary of words in the source language ; . means for comparing each word in the source text with each word in the source vocabulary to identify each word in the source text which is not in the source vocabulary ; and .", "label": "", "metadata": {}, "score": "39.771294"}
{"text": "2 , a recognizer 215 receives and processes the frames of an utterance to identify text corresponding to the utterance .The recognizer entertains several hypotheses about the text and associates a score with each hypothesis .The score reflects the probability that a hypothesis corresponds to the user 's speech .", "label": "", "metadata": {}, "score": "39.851337"}
{"text": "After adding a word to the list of words ( step 1125 ) , the node - processing procedure saves the score associated with the word as a reseeding score for use in reseeding the tree ( step 1130 ) .Production of a word by the lexical tree means that the current frame may correspond to the last frame of the word ( with the probability of such a correspondence being reflected by the score associated with the word ) .", "label": "", "metadata": {}, "score": "40.007317"}
{"text": "Next , the pre - filtering procedure determines whether more words may be added to the word list for the requested time ( step 735 ) .A word produced by the lexical tree is added to the list of words corresponding to the start time of the word and to lists of words corresponding to times that precede and follow the start time of the word .", "label": "", "metadata": {}, "score": "40.025097"}
{"text": "Stage P2 used MMIE GI triphones to generate the transcriptions for unsupervised test - set MLLR adaptation [ 8 , 3 ] with a 4-gram LM .A global transform ( A ' ' global transform ' ' denotes one transform for speech and a separate transform for silence . ) for the means ( block - diagonal ) and variances ( diagonal ) was computed for each side .", "label": "", "metadata": {}, "score": "40.203796"}
{"text": "A speech recognition system analyzes a user 's speech to determine what the user said .Most speech recognition systems are frame - based .In a frame - based system , a processor divides a signal descriptive of the speech to be recognized into a series of digital frames , each of which corresponds to a small time increment of the speech .", "label": "", "metadata": {}, "score": "40.398895"}
{"text": "The pre - filtering procedure reseeds the tree ( step 730 of .FIG .7 ) to account for this possibility .For a given frame , multiple nodes may produce words .However , the tree only needs to be reseeded once .", "label": "", "metadata": {}, "score": "40.437016"}
{"text": "the step of generating speech hypotheses generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition method as claimed in claim 19 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "40.58625"}
{"text": "When given a new segment of text to translate , these systems search the database to extract all relevant bi - fragments , i.e. , items in the database whose source - language fragment matches some portion of the new input .A subset of these matching bi - fragments is then searched for , such that each word of the input text is covered by exactly one bi - fragment in the subset , and that the combination of the target - language fragments produces a coherent translation .", "label": "", "metadata": {}, "score": "40.689278"}
{"text": "Identification of semantic clusters ( e.g. , by method 1000 ) may be performed as a nonrecurring software engineering process .Identification of stop segments ( e.g. , by method 1300 ) is typically performed as a user interaction process .After identification of semantic clusters and one or more stop segments , a message recognizer may operate in accordance with a semantic model according to various aspects of the present invention .", "label": "", "metadata": {}, "score": "40.722565"}
{"text": "The method may also include extracting the word and the phrase from the recognition result .Determining if the word closely corresponds to a portion of the phrase may include a determining if the word matches a substring of the phrase .Producing the speech recognition result may include producing the word .", "label": "", "metadata": {}, "score": "40.751404"}
{"text": "A speech recognition method is claimed in claim 25 , characterized in that : . each word in the source text bas a spelling comprising one or more letters , each letter being upper case or being lower case ; . the method further comprises the step of identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "41.30399"}
{"text": "The twenty dimension adapted feature vector signal X'(t ) from the adaptive labeler 60 is preferably provided to an auditory model 62 .Auditory model 62 may , for example , provide a model of how the human auditory system perceives sound signals .", "label": "", "metadata": {}, "score": "41.51917"}
{"text": "A message recognizer may also have the ability to initiate a context definition mode automatically .A context definition mode may be automatically initiated when the message recognizer determines that one or more alternative text segments ( e.g. , text segments receiving a high score during hypothesis testing ) may be expected to be especially useful for indicating context as stop segments .", "label": "", "metadata": {}, "score": "41.70141"}
{"text": "The control / interface module receives the list of recognition candidates for each utterance from the recognizer .Recognition candidates may correspond to dictated text , speech recognition commands , or external commands .When the best - scoring recognition candidate corresponds to dictated text , the control / interface module provides the text to an active application , such as a word processor .", "label": "", "metadata": {}, "score": "41.71187"}
{"text": "The exemplary embodiment may differ in this respect from the method disclosed in Cancedda , et al . .A gap size scoring feature ( h ELAST ) , illustrated at 34 , corresponding to the probability of the gap sizes assigned to ( s , t ) in the translation , relative to the distribution dist associated with ( s , t ) in the training phase .", "label": "", "metadata": {}, "score": "42.0346"}
{"text": "This paper investigates error - corrective language modeling using the perceptron algorithm on word lattices .The resulting model is encoded as a weighted finite - state automaton , and is used by intersecting the model with word lattices , making it simple and inexpensive to apply during decoding .", "label": "", "metadata": {}, "score": "42.057724"}
{"text": "In many translation systems , the quality of the resulting translation is assessed by means of a statistical translation model , which estimates the probability of observing some target - language segment of the text as the translation of the given source - language input .", "label": "", "metadata": {}, "score": "42.09468"}
{"text": "In a conventional speech recognizer such as a computer executing the NATURALLY SPEAKING software , the displayed list of alternative text changes as different characters are entered by a user using character input .At step 1450 , the selected text segment is modified responsive to the character input .", "label": "", "metadata": {}, "score": "42.354992"}
{"text": "For example , the syntactic language model in a shared language model may be trained by user correction of any type of erroneous message recognition performed by the message recognizer .An exemplary correction process 1400 is described below with reference to .FIG .", "label": "", "metadata": {}, "score": "42.39559"}
{"text": "Hypotheses having excessive scores are eliminated from further consideration .As noted above , hypotheses that comply with no active constraint grammar also are eliminated .When the recognizer determines that a word of a hypothesis has ended , the recognizer requests from the pre - filtering procedure a list of words that may have been spoken just after the ending - time of the word .", "label": "", "metadata": {}, "score": "42.44761"}
{"text": "The confidence level identifies a likelihood that the word in the word hypothesis was correctly recognized .A time alignment is performed at 1108 that aligns the digitized speech and the hypotheses using an acoustic model trained at 1110 using native - quality speech .", "label": "", "metadata": {}, "score": "42.464592"}
{"text": "These human modalities include Speech , Gesture and Pointing , . ... onist recognizer , which is well suited for handling temporal sequences of patterns as provided by this kind of input representation .The MSTDNN , which was originally proposed for continuous speech recognition tasks [ 13][6 ] , combines shift inv ... .", "label": "", "metadata": {}, "score": "42.48376"}
{"text": "As shown in .FIG .13L also illustrates another feature of the interface .As an utterance is being recognized , the control / interface module 220 may display a partial recognition candidate 1365 for the utterance .This partial candidate represents the best scoring hypotheses for the utterance at a point in time before the recognizer completes processing of the utterance .", "label": "", "metadata": {}, "score": "42.568108"}
{"text": "Accordingly , it is appropriate to ignore it .The model presupposes that , for each bi - phrase , the empirical statistics of its observed gap sizes in the training corpus are available .In the exemplary embodiment , a static library of bi - chunks constructed according to the method of Cancedda , et al . is used as the information source for obtaining these statistics .", "label": "", "metadata": {}, "score": "42.589413"}
{"text": "We demonstrate the importance of making the training conditions as close as possible to testing conditions .The best approach yields a 1.3 percent improvement in first pass accuracy , which translates to 0.5 percent improvement after other rescoring passes . by Eric K. Ringger , James F. Allen - In Proceedings of the Fourth International Conference on Spoken Language Processing ( ICSLP-96 , 1996 . \" ...", "label": "", "metadata": {}, "score": "42.634243"}
{"text": "Each of the retrieved text fragments is linked in a bi - phrase in the library to a text fragment from the source text .At least one target hypothesis is generated .Each of the target hypotheses comprises text fragments selected from the second language .", "label": "", "metadata": {}, "score": "42.738922"}
{"text": "This new hypothesis is then pushed onto a stack of hypotheses in need of being extended .To reduce the search space , partial translation hypotheses are scored , and the stack is periodically pruned based on these scores , i.e. , only the most promising hypotheses are retained on the stack .", "label": "", "metadata": {}, "score": "42.784256"}
{"text": "In addition , rapid obsolescence of computer hardware and software often renders adapts language , acoustic , and/or handwriting models unusable after the user has invested a significant amount of time to adapt those models .Conventionally , user training is also tied to a specific type of message recognizer .", "label": "", "metadata": {}, "score": "42.807148"}
{"text": "The need also remains for a speech recognizer that may be automatically activated when voice input is desired .U.S. Pat .No .5,857,172 to Rozak , for example , identifies a difficulty encountered with conventional speech recognizers in that such speech recognizers are either always listening and processing input or not listening .", "label": "", "metadata": {}, "score": "42.847588"}
{"text": "If a message recognizer operating in accordance with the semantic model of the invention were used , none of the alternative text segments listed in TABLE V would be useful to designate as stop segments .A semantic cluster includes any semantic association of text in a language model .", "label": "", "metadata": {}, "score": "42.985138"}
{"text": "No .4,980,918 to Bahl et al entitled \" Speech Recognition System with Efficient Storage and Rapid Assembly of Phonological Graphs \" .Preferably , according to the present invention , for each frequency band i of the adapted feature vector signal X'(t ) at time t , the auditory model 62 calculates a new parameter E.sub.i ( t ) according to Equations 23 and 24 : . where . and where K.sub.1 , K.sub.2 , and K.sub.3 are fixed parameters of the auditory model .", "label": "", "metadata": {}, "score": "43.045464"}
{"text": "In variations , it may be desirable for both approaches to be used in a combined language model , or combined in one semantic model .The first exemplary semantic model , adapted with M stop segments , may be implemented mathematically , using the following equation : .", "label": "", "metadata": {}, "score": "43.06595"}
{"text": "If the noted conditions are met , the node - processing procedure adds the word or words to the list ( step 1125 ) .A word is stored in the list of words along with the score propagated out of the last state .", "label": "", "metadata": {}, "score": "43.083344"}
{"text": "The decoder may buffer the parts of elastic phrases in the target text which are not yet consumed , and may use a heuristic cost function that takes into account these buffered parts in assessing the value of a partial translation hypothesis .In another aspect , a statistical translation model is provided that deals with such phrases .", "label": "", "metadata": {}, "score": "43.166725"}
{"text": "5,502,774 , issued Mar. 26 , 1996 to Bellegarda discloses a \" message recognition system \" using both speech and handwriting as complementary sources of information .Speech recognizers and handwriting recognizers are both examples of message recognizers .A need remains for message recognition with high accuracy .", "label": "", "metadata": {}, "score": "43.18217"}
{"text": "The scores for words in a list of words are returned along with the list of words .The recognizer 215 uses these scores in making the detailed match .Spreading the word across multiple lists ensures that these inaccuracies will not impinge on the accuracy of the speech recognition system .", "label": "", "metadata": {}, "score": "43.335716"}
{"text": "The recognizer 215 then retrieves a frame of parameters ( step 1210 ) and determines whether there are hypotheses to be considered for the frame ( step 1215 ) .The first frame always corresponds to silence so that there are no hypotheses to be considered for the first frame .", "label": "", "metadata": {}, "score": "43.340942"}
{"text": "The scoring may include maximizing a log - linear scoring function in which weights are assigned to each of the feature functions .In another aspect , a machine translation system translates source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "43.345814"}
{"text": "A time alignment is performed at 1008 that aligns the digitized speech and the hypotheses using an acoustic model trained at 1010 using native - quality speech .The time alignment calculates statistics regarding individual words and phonemes in the word hypotheses .Those statistics are utilized at 1012 to calculate features for the digitized speech .", "label": "", "metadata": {}, "score": "43.347824"}
{"text": "If the recognizer determines that a word is ending ( step 1245 ) , the recognizer sets a flag that indicates that the next frame may correspond to the beginning of a word ( step 1250 ) .If there are additional hypotheses to be considered for the frame ( step 1255 ) , then the recognizer selects the next hypothesis ( step 1260 ) and repeats the comparison ( step 1225 ) and other steps .", "label": "", "metadata": {}, "score": "43.422546"}
{"text": "Next , the node - processing procedure determines whether a word is to be added to a list of words ( step 1120 ) .A word is added to the list of words when the node being processed corresponds to the last phoneme of a word , a score has been propagated out of the last state of the node , and the score is less than a list threshold .", "label": "", "metadata": {}, "score": "43.4497"}
{"text": "A message recognizer may include , for example , a speech recognizer or a handwriting recognizer , or both .A speech recognizer according to various aspects of the present invention includes any suitable hardware and software for providing text data responsive to voice input .", "label": "", "metadata": {}, "score": "43.524784"}
{"text": "FIG .9 , a unified message model may suitably include , inter alia , a language model , an acoustic model , and/or a handwriting model .A message recognizer may include two or more types of subordinate message recognizers , each of which may operate in accordance with a shared language model .", "label": "", "metadata": {}, "score": "43.53357"}
{"text": "In this talk , two types of computational models of prosody will be described that integrate acoustic cues with text features based on recognizer sentence hypotheses and associated parser outputs .Also , methods for using prosodic information in automatic speech understanding systems will be described .", "label": "", "metadata": {}, "score": "43.618584"}
{"text": "No .11/315,043 ( hereinafter Cancedda , et al . ) discloses a computer implemented system and method for translating source text from a first language to target text in a second language different from the first language and to a method of training such a translation system .", "label": "", "metadata": {}, "score": "43.69902"}
{"text": "A message model is said to be specific to a particular user when the message model may be adapted by training input from the user .Such adaptation provides improved recognition .Training input includes corrections of misrecognized message input by any suitable form of user input .", "label": "", "metadata": {}, "score": "43.88692"}
{"text": "At step 1050 , the probability of shared cluster occurrence is computed for the selected text segments .Pr .w .i . w .j .V . ) k .N .Min .Z .i . k .Z .", "label": "", "metadata": {}, "score": "43.96134"}
{"text": "Accordingly , in message recognizer according to various aspects of the present invention provides a user with the opportunity to adapt and modify his or her user message model over time , regardless of the particular system in which he or she employs it for message recognition .", "label": "", "metadata": {}, "score": "44.009323"}
{"text": "Semantic approaches to language modeling have been disclosed .Generally speaking , such techniques attempt to capture meaningful word associations within a more global language context .For example , U.S. Pat .No .5,828,999 , issued Oct. 27 , 1998 to Bellegarda et al . discloses a large - span semantic language model which maps words from a vocabulary into a vector space .", "label": "", "metadata": {}, "score": "44.02112"}
{"text": "In this way , rule - based morphological , syntactic and/or semantic information is combined with knowledge extracted from bilingual texts which is then re - used in the translation process .Such methods are often collectively referred to as \" phrase - based methods \" .", "label": "", "metadata": {}, "score": "44.05777"}
{"text": "A set of one or more speech hypotheses , each comprising one or more words from the target language , are produced .Each speech hypothesis is modeled with an acoustic model .An acoustic match score for each speech hypothesis comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance .", "label": "", "metadata": {}, "score": "44.10491"}
{"text": "In other respects , the decoding procedure may be analogous to the procedure outlined in Cancedda , et al .Specifically , the current stack is traversed from top to bottom , then the decoder moves to the next stack .Extending Hypotheses in the Beam Search Procedure .", "label": "", "metadata": {}, "score": "44.117256"}
{"text": "We implemente ... . \" ...This paper investigates error - corrective language modeling using the perceptron algorithm on word lattices .The resulting model is encoded as a weighted finite - state automaton , and is used by intersecting the model with word lattices , making it simple and inexpensive to apply during decoding .", "label": "", "metadata": {}, "score": "44.40696"}
{"text": "In the second exemplary semantic model , text segments that appear often are less likely to receive a negative bias , even though they may appear often in the same semantic clusters as stop words .The expectation is that commonly occurring text segments can be expected to occur in many different contexts , and that the present context is less likely to exclude such text segments .", "label": "", "metadata": {}, "score": "44.485226"}
{"text": "For example , the speech recognition system may insert the text into a dictation window .If the top recognition candidate includes something other than text ( step 1410 ) , the speech recognition system determines whether the candidate includes a word - in - phrase command that permits active disambiguation of homophones so as to avoid homophone mistakes ( step 1420 ) .", "label": "", "metadata": {}, "score": "44.502537"}
{"text": "The integrated paradigm provides an estimate of the likelihood that a word , chosen from an underlying vocabulary , will occur given a prevailing contextual history .A prevailing contextual history may be a useful guide for performing general semantic analysis based on a user 's general writing style .", "label": "", "metadata": {}, "score": "44.73398"}
{"text": "Our MS - TDNN achieves 98.5/92.0 % word accuracy on speaker dependent / independent tasks , outperforming previously reported results on the same databases .We propose training techniques aimed at improving sentence level performance , including free alignment across word boundaries , word duration modeling and error backpropagation on the sentence rather than the word level .", "label": "", "metadata": {}, "score": "44.76907"}
{"text": "A semantic model may be combined with a syntactic model using any conventional technique .Suitable combination techniques may include maximum entropy , backoff , linear interpolation , and linear combination .In a variation , a semantic model may be integrated with a syntactic model in a single , hybrid language model of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "45.039986"}
{"text": "In this paper we describe an input representation for cursive handwriting , which combines this dynamic writing information with static bitmaps used in optical character recognition .This input representation is used with a connectionist recognizer , which is well suited for handling temporal sequences of patterns as provided by this kind of input representation .", "label": "", "metadata": {}, "score": "45.078705"}
{"text": "In general , the speech recognition software analyzes the digital samples to produce a sequence of text , and provides this sequence to the interface software 140 .The interface software 140 then transfers the text and the associated time stamp , if any , to the parser 142 , which processes the text in conjunction with the time stamp to generate a parsed version of the action item .", "label": "", "metadata": {}, "score": "45.372665"}
{"text": "After clustering the vectors , a first probability that a first word will occur ( given a history of prior words ) is computed .The probability is computed by calculating a second probability that the vector representing the first word belongs to each of the clusters ; calculating a third probability of each cluster occurring in a history of prior words ; and weighting the second probability by the third probability .", "label": "", "metadata": {}, "score": "45.486233"}
{"text": "When so configured , domain controller 220 may include a local database in non - volatile memory ( not shown ) for storing such models .As discussed above , a message recognizer according to various aspects of the present invention may be responsive to message input from a plurality of users to provide respective portions of text data .", "label": "", "metadata": {}, "score": "45.529667"}
{"text": "Conventional language , acoustic , and handwriting models respond to user training ( e.g. , corrections of misrecognized words ) to adapt and improve recognition with continued use .In conventional message recognition systems , user training is tied to a specific computer system ( e.g. , hardware and software ) .", "label": "", "metadata": {}, "score": "45.564766"}
{"text": "In a further variation , step 1060 may include refinement of a tabulation such as that of data structure 1200 in an iterative design process .Such a design process may include manual and/or computer controlled editing of such a tabulation .A suitable error constraint may be the perplexity of message recognition , which may be ( using a suitable test corpus ) performed in accordance with each semantic model resulting from each iteration of the tabulation .", "label": "", "metadata": {}, "score": "45.579155"}
{"text": "Statistics regarding individual words and phonemes of the word hypotheses may be calculated using the processing system based on the alignment .A time alignment may be performed between the digitized speech and the word hypotheses utilizing a reference acoustic model trained with native quality speech to associate word hypotheses with corresponding sounds of the digitized speech .", "label": "", "metadata": {}, "score": "45.59695"}
{"text": "5,812,865 to Theimer et al . , incorporated by reference above ) .A domain controller may also be incorporated into one tablet of a plurality of networked tablets .A message recognizer according to various aspects of present invention includes any suitable hardware and/or software for recognizing a message of at least one type to provide text data .", "label": "", "metadata": {}, "score": "45.708828"}
{"text": "For example , a word hypothesis may be selected that meets the criteria : .Certain timing information 612 may also be output from the speech recognition 602 such as the length of a response , a number of words in a response , as well as other statistics .", "label": "", "metadata": {}, "score": "45.71692"}
{"text": "No .6,304,841 , log - linear features are used to score word hypotheses depending on their lexical context .The following references relate to phrase - based statistical machine translation methods .U.S. Pat .No . 6,182,026 by Tillmann , et al . , entitled \" METHOD AND DEVICE FOR TRANSLATING A SOURCE TEXT INTO A TARGET USING MODELING AND DYNAMIC PROGRAMMING , \" discloses a method and device for translating a source text into a target using modeling and dynamic programming .", "label": "", "metadata": {}, "score": "45.770348"}
{"text": "More sophisticated language models based on probabilistic decision trees , stochastic context - free grammars , and automatically discovered classes of words have also been used .While statistical language models which use no knowledge or information about the actual utterance to be recognized are useful in scoring speech hypotheses in a speech recognition system , the best scoring speech hypotheses do not always correctly identify the corresponding utterances to be recognized .", "label": "", "metadata": {}, "score": "45.80882"}
{"text": "Text segments occurring in each cluster are counted and tabulated into a two - dimensional matrix Z. This matrix is used at steps 1030 and 1050 to compute probabilities of global and shared cluster occurrence , respectively .An exemplary process 1100 for carrying out step 1020 may be better understood with reference to .", "label": "", "metadata": {}, "score": "45.87278"}
{"text": "The system - wide backup vocabulary contains all the words known to the system , including words that may currently be in an active vocabulary .Referring again to .FIG .2 , the recognizer 215 may operate in parallel with a pre - filtering procedure 240 .", "label": "", "metadata": {}, "score": "45.87432"}
{"text": "In one embodiment , a measure of reordering is used which relies on the \" source coverage vector \" V of the sequence of decisions d 1 . . .d K .The coverage vector can be computed using the procedure of Cancedda , et al .", "label": "", "metadata": {}, "score": "45.927563"}
{"text": "Such a technique is taught in U.S. Pat .No .5,828,999 to Bellegarda et al . , incorporated by reference above .The disclosure found from column 4 , lines 8 - 54 ; from column 5 , line 35 through column 7 , line 35 ; and column 8 , lines 3 - 24 of the aforementioned patent is considered particularly instructive for the identification of semantic clusters of this first type .", "label": "", "metadata": {}, "score": "45.942986"}
{"text": "Such a model applies a negative bias against those text segments having a high probability of shared cluster occurrence with one or more stop segments identified by a user .Negative bias may be applied by any suitable technique or combination of techniques .", "label": "", "metadata": {}, "score": "46.0782"}
{"text": "In one embodiment , this combined score is computed as a sum over all complete sentence T ' in T.cndot .: # # EQU1 # # .The probability P(T ' ) is obtained from the language match score generator , and the probability P(S.vertline .", "label": "", "metadata": {}, "score": "46.084137"}
{"text": "The text segment receiving the highest score ( i.e. , the maximum likelihood text segment ) is conventionally provided as the message recognizer 's \" best guess \" of the text data corresponding to the message input .Text segments receiving high scores ( e.g. , in the top 10 ) but not the highest are considered alternative text segments .", "label": "", "metadata": {}, "score": "46.195267"}
{"text": "Previously , using an implementation from JHU , the technique was investigated using various training set sizes and levels of model complexity [ 7 ] .It was found that while consistent improvements were obtained , the improvement in WER was reduced when features such as VTLN and MLLR adaptation were included in the system .", "label": "", "metadata": {}, "score": "46.266457"}
{"text": "Conventionally , a message model includes a language model and a model specific to one type of message recognition .For example , a conventional speech recognizer operates in accordance with a type - specific message model consisting of a language model and an acoustic model .", "label": "", "metadata": {}, "score": "46.410515"}
{"text": "For example , the digitized signal may be transported via a network 208 for storage on a server 210 .[ 0025 ] FIG .3 is a flow diagram depicting a process for assessing spontaneous speech pronunciation of a non - native language speaker .", "label": "", "metadata": {}, "score": "46.482758"}
{"text": "The distribution dist is used to model the conditional probability of getting specific sizes for each of the k source and l target gaps conditional on the actual use of the given gapped bi - phrase in a ( sentence - level ) translation .", "label": "", "metadata": {}, "score": "46.542953"}
{"text": "The pre - filtering procedure removes the words that are least likely to correspond to the user 's speech and removes enough words to reduce the number of words on the list to the predefined number .The procedure also deletes any lists of words for times prior to the requested start time .", "label": "", "metadata": {}, "score": "46.561737"}
{"text": "A speech recognition method as claimed in claim 25 , characterized in that the step of generating an acoustic model comprises : . storing a plurality of acoustic letter models ; and .generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "46.60855"}
{"text": "[ Work supported by NSF and ARPA . ]A machine translation method includes receiving source text in a first language and retrieving text fragments in a target language from a library of bi - fragments to generate a target hypothesis .A machine translation method includes receiving source text in a first language and retrieving text fragments in a target language from a library of bi - fragments to generate a target hypothesis .", "label": "", "metadata": {}, "score": "46.623386"}
{"text": "A search for the optimal combination may be performed by means of a specially adapted , multiple - stack beam search .Within this search procedure , the potential of each candidate translation is evaluated by means of a statistical model of translation .", "label": "", "metadata": {}, "score": "46.643272"}
{"text": "The source text may be provided to the display by , for example , a source text input device 12 such as a computer system .The speech recognition system further comprises an acoustic processor 14 for generating a sequence of coded representations of an utterance to be recognized .", "label": "", "metadata": {}, "score": "46.67324"}
{"text": "The system performs four recognition passes : ( 1 ) bigram recognition with phone - loop - adapted , within - word triphone acoustic models , ( 2 ) lattice generation with transcription - m ... \" .We describe SRI 's large vocabulary conversational speech recognition system as used in the March 2000 NIST Hub-5E evaluation .", "label": "", "metadata": {}, "score": "46.687016"}
{"text": "Table 2 shows the number of clustered speech states and the number of Gaussians per state for each of these systems as well as word error rates on eval97sub .An initial 3.5-fold increase in the amount of training data results in a 4.6 % absolute reduction in word error rate ( WER ) .", "label": "", "metadata": {}, "score": "46.701256"}
{"text": "In one embodiment , the decoder performs as follows .The current partial translation is represented as : ( 1 ) a partial coverage of the words of the source sentence , and ( 2 ) a partial target sentence representation .As an example , .", "label": "", "metadata": {}, "score": "46.727894"}
{"text": "The recognizer then adds the modified list score to the score for the hypothesis and compares the result to a threshold value .If the result is less than the threshold value , then the recognizer maintains the hypothesis .Otherwise , the recognizer determines that the hypothesis does not merit further consideration and abandons the hypothesis .", "label": "", "metadata": {}, "score": "46.760803"}
{"text": "When the acoustic models are Markov models , acoustic match scores may be obtained , for example , by the forward pass of the Forward - Backward Algorithm .( See , for example , L. R. Bahl , et al , March 1983 , cited above . )", "label": "", "metadata": {}, "score": "46.76674"}
{"text": "However , this straight - forward implementation only brought moderate improvements in WER .The dictionaries in the HTK system explicitly contain silence models as part of a pronunciation .Experiments with or without inclusion of silence into the probability estimates were conducted [ 7 ] .", "label": "", "metadata": {}, "score": "46.79783"}
{"text": "( See , for example , \" Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme - Based Markov Word Models \" by L. R. Bahl , et al , IBM Technical Disclosure Bulletin , Volume 32 , No . 7 , December 1989 , pages 320 and 321 . )", "label": "", "metadata": {}, "score": "46.978806"}
{"text": "a processor which executes the instructions .A machine translation system for translating source text from a first language to target text in a second language , comprising : .The machine translation system of . claim 17 , wherein translation scoring function further includes at least one of : . a second feature function which evaluates bi - fragment discontinuities ; and .", "label": "", "metadata": {}, "score": "47.103058"}
{"text": "First , the average sentence length was 11.3 words compared to 9.5 words on the LDC transcripts that we previously used .This has the effect that LMs trained on the MSU transcripts have a higher test - set perplexity which is mainly due to the reduced probability of the sentence - end symbol .", "label": "", "metadata": {}, "score": "47.15013"}
{"text": "At step S 100 , source text to be decoded is input to the system 10 .At step S 110 , the processing component 16 identifies a string of the source text to be translated , such as a sentence .This step may include a tokenizing procedure , in which the processing component identifies words in the text , as well as punctuation marks .", "label": "", "metadata": {}, "score": "47.22043"}
{"text": "A speech recognition system as claimed in claim 17 , characterized in that the means for measuring the value of at least one feature of an utterance comprises a microphone .A speech recognition method comprising : . displaying a source text comprising one or more words in a source language ; . generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . generating an acoustic model of each speech hypothesis ; . generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text ; . generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "47.268677"}
{"text": "The \" fast \" match examines at least a portion of every word in the target vocabulary to find a number of words which are good possibilities for extending the candidate word strings .The fast match estimates the closeness of a match between an acoustic fast match model of a word and a portion of the sequence of coded representations of the utterance .", "label": "", "metadata": {}, "score": "47.28637"}
{"text": "0038 ]FIG .7 is a block diagram illustrating an exemplary alignment system 702 .A speech recognition engine 704 receives digitized speech 706 along with word hypotheses 708 for words contained within the digitized speech .The speech recognition engine 704 may also receive timing information , such as the length of the digitized speech 706 as well as other factors .", "label": "", "metadata": {}, "score": "47.314743"}
{"text": "This can be viewed as the use of a speaker - dependent global semi - tied block - full covariance matrix and can be efficiently implemented by transforming both the means and the input data .In our implementation , the full variance transform was computed after standard mean and variance maximum likelihood linear regression ( MLLR ) .", "label": "", "metadata": {}, "score": "47.327408"}
{"text": "If more words may be added , then the pre - filtering procedure retrieves the next frame of parameters ( step 700 ) and repeats the steps discussed above .If words can not be added to the word list ( step 735 ) , then the pre - filtering procedure returns the word list ( step 740 ) to the recognizer 215 .", "label": "", "metadata": {}, "score": "47.335037"}
{"text": "word translation probabilities p.sub.5 ( s.vertline.t ) for source words s and target words t satisfying # # EQU7 # # . alignment probabilities P.sub.6 ( i.vertline.j , l ) satisfying # # EQU8 # # .Values for these parameters can be determined from a large quantity of aligned source - target sentence pairs ( S.sup.l , T.sup.l ) . . .", "label": "", "metadata": {}, "score": "47.352596"}
{"text": "Additional variables can be introduced in such a log - linear model , so as to account for selected characteristics , and the feature functions can be extended accordingly .For example , if the translation t 1 J of s 1 I is produced using bi - fragments as described here , the model can be modified to take into account the actual set of decisions that lead to t 1 J .", "label": "", "metadata": {}, "score": "47.35722"}
{"text": "In the event that timing of the digital samples is a concern , sample buffering ( e.g. , using a FIFO ) may be incorporated in receiver 160 to ensure that samples are passed along to speech recognizer 162 at substantially constant intervals .", "label": "", "metadata": {}, "score": "47.426556"}
{"text": "Using this convention , contiguous and discontinuous phrases can be treated under a single type , with has the advantage of avoiding the need to learn separately different combinations of contiguous / non - contiguous components with the associated sparseness issues .The model may include other variants .", "label": "", "metadata": {}, "score": "47.444263"}
{"text": "In this paper we show how recognition performance in automated speech perception can be significantly improved by additional Lipreading , so called \" speech - reading \" .We show this on an extension of an existing state - of - theart speech recognition system , a modular MS - TDNN .", "label": "", "metadata": {}, "score": "47.467964"}
{"text": "In this paper we show how recognition performance in automated speech perception can be significantly improved by additional Lipreading , so called \" speech - reading \" .We show this on an extension of an existing state - of - theart speech recognition system , a modular MS - TDNN .", "label": "", "metadata": {}, "score": "47.467964"}
{"text": "Alternatively , a speech recognition system may be a \" continuous \" system that can recognize spoken words or phrases regardless of whether the user pauses between them .Continuous speech recognition systems typically have a higher incidence of recognition errors in comparison to discrete recognition systems due to complexities of recognizing continuous speech .", "label": "", "metadata": {}, "score": "47.602554"}
{"text": "[ 0035 ] Each entry in a dictionary 618 containing the words that are expected to be recognized by the system may be expressed as a sequence of Hidden Markov Models representing the phones assumed by that entry .The speech recognition engine 604 may also utilize a language model 620 trained using non - native and/or native training text 622 .", "label": "", "metadata": {}, "score": "47.61918"}
{"text": "For example , both the word .Referring to .FIG .14 , the speech recognition system may process recognition results according to a procedure 1400 .Initially , the speech recognition system generates or receives recognition results for an utterance ( step 1405 ) .", "label": "", "metadata": {}, "score": "47.668648"}
{"text": "Returning to FIG .1 , the speech recognition system further comprises an acoustic model generator 18 for generating an acoustic model for each speech hypothesis generated by the speech hypothesis generator 16 .The acoustic model generator 18 forms an acoustic model of a speech hypothesis by substituting , for each word in the speech hypothesis , an acoustic model of the word from a set of stored acoustic models .", "label": "", "metadata": {}, "score": "47.76282"}
{"text": "Advantageously , the model may cause such biasing by simply applying a negative bias against those text segments having a high probability of shared cluster occurrence with stop segments .A semantic model is said to bias against a tested segment when a message recognizer scores the tested segment less favorably in a hypothesis test due to the operation of the semantic model .", "label": "", "metadata": {}, "score": "47.80198"}
{"text": "d pruned to construct a corrected , more accurate n - best list .The novelty of our approach lies in : ( 1 ) the use of user click data in a deployed multi - modal s .. \" ...We demonstrate that transformation - based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9 % .", "label": "", "metadata": {}, "score": "47.84619"}
{"text": "The set of candidate words consists solely of words in the target language which are partial or full translations of words in the source text .One or more speech hypotheses are generated solely from words in the set of candidate words .The translation match score for a speech hypothesis may comprise , for example , an estimate of the probability of occurrence of the source text given the occurrence of the speech hypothesis , combined with an estimate of the probability of occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "47.86258"}
{"text": "Some of the bi - fragments are modeled as elastic bi - fragments where a gap between words is able to assume a variable size corresponding to a number of other words to occupy the gap .A machine translation method for translating source text from a first language to target text in a second language , comprising : . receiving the source text in the first language ; . retrieving text fragments from the second language from the library corresponding to text fragments in the source text ; . generating , by a processor , at least one target hypothesis , each of said target hypotheses comprising text fragments selected from the second language ; and .", "label": "", "metadata": {}, "score": "47.962463"}
{"text": "The bi - fragments may be learned from a large aligned bilingual corpus comprising source text and its translation into target text .The statistics of bi - phrases can be established through observation of a corpus .It is to be appreciated that direct observation of a bi - phrase in a given sentence pair is not necessary , but rather the presence of such a bi - phrase can be indirectly induced through phrase - level alignment mechanisms .", "label": "", "metadata": {}, "score": "47.96644"}
{"text": "Statistics are calculated regarding individual words and phonemes in the word hypotheses based on the alignment .A plurality of features for use in assessing pronunciation of the speech are calculated based on the statistics , an assessment score is calculated based on one or more of the calculated features , and the assessment score is stored in a computer - readable memory .", "label": "", "metadata": {}, "score": "47.981216"}
{"text": "The source text comprises one or more words in a source language .An acoustic processor generates a sequence of coded representations of an utterance to be recognized .The utterance comprising a series of one or more words in a target language different from the source language .", "label": "", "metadata": {}, "score": "48.007843"}
{"text": "Pr .w .i . ) j .M .[ .B .j .Pr .w .i . s .j .V . )Pr .w .i .V . ) . ]Consequently , the probability ( here , the score ) assigned to the tested text segment during hypothesis testing will be relatively low , effecting a negative bias based on the high probability of a single stop segment occurring in the same semantic cluster as the tested text segment .", "label": "", "metadata": {}, "score": "48.033142"}
{"text": "For ease of discussion , this candidate is referred to as the top recognition candidate .As the first step in processing the top recognition candidate , the speech recognition system determines whether the top recognition candidate includes only text ( step 1410 ) .", "label": "", "metadata": {}, "score": "48.07254"}
{"text": "The top scoring recognition results ( a list of sentence hypotheses ) are passed to the natural language system , which typically processes these taking the highest ranking recognition hypothesis that is interpretable and ignoring any acoustic information other than rank .The system can be improved by choosing the utterance interpretation based on a combination of the acoustic likelihood of a hypothesized word string and an associated language score ( e.g. , parse probability or semantic preference score ) .", "label": "", "metadata": {}, "score": "48.10282"}
{"text": "In contrast to MLE , discriminative training schemes , such as Maximum Mutual Information Estimation ( MMIE ) take account of possible competing word hypotheses and try to reduce the probability of incorrect hypotheses .The objective function to maximise in MMIE is the posterior probability of the true word transcriptions given the training data .", "label": "", "metadata": {}, "score": "48.112915"}
{"text": "On the 1998 evaluation set a relative reduction in word error rate of 11 % was obtained .The system presented here gave the lowest word error rate in the March 2000 Hub5E evaluation .While the overall system is complex , a much simpler setup based on the first few passes of the full system also gives competitive performance .", "label": "", "metadata": {}, "score": "48.16674"}
{"text": "In another variation , each group member may remove pooled user stop segments ( at least from operation of message recognition responsive to his or her message input ) that the member considers to be inside his or her particular user context .Advantageously , a user context may exclude words not consistent with the writing style of a particular user ( or collaborative group ) .", "label": "", "metadata": {}, "score": "48.169655"}
{"text": "Such models use probability functions to identify a target language string with maximum probability , i.e. , a string which optimizes one or more functions which model statistically probable translations .Using elastic bi - fragments in phrase - based translation methods raises a number of issues for the statistical translation model , which the present method addresses by incorporation of \" feature functions \" into a log - linear statistical translation model .", "label": "", "metadata": {}, "score": "48.176323"}
{"text": "The combined probability may be computed in any suitable fashion .In the equation above , the individual probabilities of each stop segment s j occurring in a given semantic cluster C k are simply summed together .Although such a summation could result in a combined probability greater than 1.00 , the combined probability permits multiple stop segments to contribute to an overall negative bias .", "label": "", "metadata": {}, "score": "48.219334"}
{"text": "The ratio computed in the first exemplary semantic model will thus be relatively low , effecting a negative bias based on the high probability of a single stop segment occurring in a given semantic cluster with the tested text segment .The negative bias incurred by identified stop segments may be tempered in the second exemplary semantic model by the inclusion of suitable mathematical terms .", "label": "", "metadata": {}, "score": "48.232048"}
{"text": "As may be better understood with reference to TABLE III below , examples of types of messages to be recognized include voice input ( for speech recognition ) , freehand input ( for handwriting recognition ) , and visual input ( for optical character recognition ) .", "label": "", "metadata": {}, "score": "48.251633"}
{"text": "As discussed in U.S. Pat .No .5,839,106 , issued Nov. 17 , 1998 to Bellegarda , conventional language models rely upon the classic N - gram paradigm to define the probability of occurrence , within a spoken vocabulary , of all possible sequences of N words .", "label": "", "metadata": {}, "score": "48.296425"}
{"text": "The non - contiguous bi - fragments are expressed in terms of elastic bi - phrases .Each elastic bi - phrase is considered to be a pair of gapped phrases , one in the source language and one in the target language .", "label": "", "metadata": {}, "score": "48.310307"}
{"text": "The front end component can combine four 25-bit values to form a single 100-bit word and then convert the 100-bit word into five 20-bit words .Additionally , in some embodiments a scrambler receives data from the forward error correcting component and randomizes the data .", "label": "", "metadata": {}, "score": "48.315746"}
{"text": "the speech hypothesis generator generates one or more speech hypotheses solely from words in the set of candidate words .A speech recognition system as claimed in claim 1 , characterized in that the acoustic match score comprises an estimate of the probability of occurrence of the sequence of coded representations of the utterance given the occurrence of the speech hypothesis .", "label": "", "metadata": {}, "score": "48.32896"}
{"text": "The final error rate from the system ( 25.4 % ) was lowest in the evaluation by a statistically significant margin .A further run on eval98 was performed to investigate the effect of using a combined MMIE / MLE system .For the results in Table 7 , MLE models were used to create the lattices and provide the adaptation supervision ( Pure MLE ) rather than using MMIE based models for P2/P3 and MMIE generated adaptation supervision for P4 .", "label": "", "metadata": {}, "score": "48.33255"}
{"text": "In general , the silence node is processed by comparing a frame to a model for silence and adding the resulting score to the minimum of the current score for the silence node and the score for the root node 505 .Next , the pre - filtering procedure reseeds the lexical tree ( step 730 ) .", "label": "", "metadata": {}, "score": "48.36492"}
{"text": "It specifically addresses the issue of robust interpretation of speech in the presence of recognition errors .Robustness is achieved by a combination of ... \" .This paper describes a system that leads us to believe in the feasibility of constructing natural spoken dialogue systems in task - oriented domains .", "label": "", "metadata": {}, "score": "48.489708"}
{"text": "736,278 , Filed on Jul. 25 , 1991 .Lucassen , J. M. et al . \"An Information Theoretic Approach To The Automatic Determination Of Phonemic Baseforms . \" Proceedings of the 1984 IEEE Inter - Conference on Acoustics , Speech , and Signal Processing , vol .", "label": "", "metadata": {}, "score": "48.506737"}
{"text": "Furthermore , it is important to enhance the discrimination of the acoustic models without overly relying on the language model to resolve difficulties .Therefore as suggested in [ 15 ] a unigram language model was used during MMIE training which also improves generalisation performance [ 19 ] .", "label": "", "metadata": {}, "score": "48.52025"}
{"text": "A shared language model according to various aspects of the present invention is any language model that may be shared between two types of subordinate message recognizers included within a message recognizer .A message recognizer of a first type may provide text data responsive to a first type of message input ( e.g. , voice input ) in accordance with the shared language model and a first type - specific model ( e.g. , an acoustic model ) .", "label": "", "metadata": {}, "score": "48.52302"}
{"text": "The pronunciation dictionary used in this task contains on average 1.1 to 1.2 pronunciations per word .Unigram pronunciation probabilities , that is the probability of a certain pronunciation variant for a particular word , were estimated based on an alignment of the training data .", "label": "", "metadata": {}, "score": "48.58171"}
{"text": "Two systems for online handwriting recognition are described .Experiments demonstrate the advantage of global training , and the flexibility of graph transformer networks .A graph transformer network for reading a bank check is also described .It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks .", "label": "", "metadata": {}, "score": "48.590157"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that the means for generating an acoustic model comprises : . means for storing a plurality of acoustic letter models ; and .means for generating an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model corresponding to the letter .", "label": "", "metadata": {}, "score": "48.5969"}
{"text": "S ) , while in another embodiment the translation match score is an estimate of a joint probability P(S , T.cndot . )In the latter embodiment , the translation match score generator includes three components : . a language match score generator which computes an estimate P(T ) of the prior probability of a target word sequence T ; . a conditional translation match score generator which computes an estimate P(S.vertline .", "label": "", "metadata": {}, "score": "48.649857"}
{"text": "The models may be context - independent or context - dependent .The models may be built up from submodels of phonemes .Context - independent acoustic Markov models may be produced , for example , by the method described in U.S. Pat .", "label": "", "metadata": {}, "score": "48.67388"}
{"text": "\" In this case , the speech recognition system determines which words in the transcript should be selected for replacement when \" rewrite all \" is recognized before \" as in .\" Once again , the words to be replaced may match or sound similar to .", "label": "", "metadata": {}, "score": "48.703045"}
{"text": "The first type of semantic cluster is a mathematically derived group of vectors representing text segments ( e.g. , words ) .The vectors may lie in a word - document vector space defined by a word - document matrix , which tabulates the number of times each word of a suitable training corpus occurs in each of a plurality of documents ( or paragraphs , sections , chapters , etc . ) in the corpus .", "label": "", "metadata": {}, "score": "48.728157"}
{"text": "A Post - Processing System to Yield Reduced Word Error Rates : Recogniser Output Voting Error Reduction ( ROVER ) .Proc .IEEE Workshop on Automatic Speech Recognition and Understanding , pp .347 - 354 , Santa Barbara .P.S. Gopalakrishnan , D. Kanevsky , A. Nadas & D. Nahamoo ( 1991 ) .", "label": "", "metadata": {}, "score": "48.75477"}
{"text": "The language model score corresponds to the difference between the language model score for the word and the incremental language model score that is already included in the score .In general , the list threshold has a lower value than the pruning threshold .", "label": "", "metadata": {}, "score": "48.813442"}
{"text": "In particular it can be used to model the empirical distribution of gaps for a given bi - phrase , without any smoothing / generalization , by just recording a histogram of the observed gap sizes in the training corpus .It also allows the modeling of conditional dependencies between gaps .", "label": "", "metadata": {}, "score": "48.94038"}
{"text": "Where the mean has already been exceeded , the h ELAST feature favors consuming the next buffered word immediately .In this description , a conceptually simple model may be used which assumes that there are gaps between any two words .In an alternative embodiment , a minor adaptation of the decoder allows the second word to be consumed as soon as the first word has been consumed , which is more efficient .", "label": "", "metadata": {}, "score": "48.952312"}
{"text": "A \" reordering \" feature function h reord ( s 1 I , t 1 J , d 1 K ) , illustrated at 46 , measures the difference in order between source and target fragments .Such feature functions are a standard component in most existing phrase - based machine translation systems .", "label": "", "metadata": {}, "score": "48.954636"}
{"text": "No semantic model may be used at step 1320 in certain situations , for example at the beginning of a message recognition session when the semantic model may not be considered ready for use .At decision step 1330 , a determination is made as to whether context definition is to be activated .", "label": "", "metadata": {}, "score": "49.045677"}
{"text": "No .468,546 , filed Jan. 23 , 1990 , entitled \" Apparatus and Method For Grouping Utterances of a Phoneme Into Context - Dependent Categories Based on Sound - Similarity For Automatic Speech Recognition . \"An acoustic match score generator 24 generates an acoustic match score for each speech hypothesis .", "label": "", "metadata": {}, "score": "49.057655"}
{"text": "A further approximately 3-fold increase in the amount of training data only brings a further 1.6 % absolute reduction in WER .The model parameters in HMM based speech recognition systems are normally estimated using Maximum Likelihood Estimation ( MLE ) .", "label": "", "metadata": {}, "score": "49.118717"}
{"text": "FIGS . 2 - 5 . )As exemplified in system 200 , a speech recognizer and/or handwriting recognizer may be responsive to identifying signal S ID to select a particular user message model to use when generating text responsive to voice input conveyed via stylus 110 .", "label": "", "metadata": {}, "score": "49.182278"}
{"text": "Each speech hypothesis comprises one or more words from the target language .An acoustic model generator produces an acoustic model of each speech hypothesis .An acoustic match score generator produces an acoustic match score for each speech hypothesis .Each acoustic match score comprises an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance produced by the acoustic processor .", "label": "", "metadata": {}, "score": "49.22657"}
{"text": "A speech recognition system as claimed in claim 7 , characterized in that : . each word in the source text has a spelling comprising one or more letters , each letter being upper case or being lower case ; .the system further comprises means for identifying each word in the source text which has an upper case first letter ; and .", "label": "", "metadata": {}, "score": "49.291763"}
{"text": "In one aspect , it has the ability to make use of elastic fragments of source and/or target language text .In another aspect , it includes components within a statistical translation model to model variable gap sizes .In yet another aspect , it includes procedures for efficiently computing the potential of partial translations with discontinuities with regard to the statistical model .", "label": "", "metadata": {}, "score": "49.371624"}
{"text": "A speech recognition result is produced if the word closely corresponds to a portion of the phrase .Implementations may include one or more of the following features .For example , the recognition result may include \" . \" in the Chinese language .", "label": "", "metadata": {}, "score": "49.414993"}
{"text": "Protect your idea and hire a patent lawyer .A speech recognition system displays a source text of one or more words in a source language .The system has an acoustic processor for generating a sequence of coded representations of an utterance to be recognized .", "label": "", "metadata": {}, "score": "49.42923"}
{"text": "6 is a block diagram illustrating an exemplary speech recognition system 602 .A speech recognition engine 604 receives digitized speech 606 from which to generate word hypotheses 608 , confidence scores 610 , and timing information 612 .The speech recognition engine 604 utilizes an acoustic model 614 trained with non - native training speech 616 , a dictionary 618 containing associations between words and their possible pronunciations , and a language model 620 trained based on transcribed non - native training speech and/or other training text 622 .", "label": "", "metadata": {}, "score": "49.464863"}
{"text": "However as discussed in [ 16 ] this can be approximated by using a word lattice which is generated once to constrain the number of word sequences considered .This lattice - based framework can be used to generate the necessary statistics to apply the Extended - Baum Welch ( EBW ) algorithm [ 5 , 13 , 16 ] to iteratively update the model parameters .", "label": "", "metadata": {}, "score": "49.481262"}
{"text": "With such a large number of hypotheses , it is not feasible to generate all possible hypotheses .Therefore , preferably , the hypothesis generator does not generate all possible hypotheses for the utterance to be recognized .No .4,748,670 by Lalit R. Bahl et al entitled \" Apparatus And Method For Determining A Likely Word Sequence From Labels Generated By An Acoustic Processor . \"", "label": "", "metadata": {}, "score": "49.547844"}
{"text": "Initial passes were used for test - data warp factor selection , gender determination and finding an initial word string for unsupervised mean and variance maximum likelihood linear regression ( MLLR ) adaptation [ 8 , 3 ] .Word - level lattices were then created using adapted triphone HMMs and a bigram model which were expanded to included the full 4-gram and class model probabilities .", "label": "", "metadata": {}, "score": "49.680992"}
{"text": "A recent development in statistical machine translation has entailed the step from word - based models to phrase - based models .While in traditional word - based statistical models , the atomic unit that translation operates on is the word , phrase - based methods acknowledge the significant role played in language by multi - word expressions , thus incorporating , in a statistical framework , the insight behind Example - Based Machine Translation .", "label": "", "metadata": {}, "score": "49.792282"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a simulated distribution of gaps in a target phrase of a bi - phrase for a selected number of gaps in the corresponding source phrase .FIG .2 is a block diagram of a system for machine translation of text which utilizes non - contiguous bi - fragments according to one aspect of the exemplary embodiment ; .", "label": "", "metadata": {}, "score": "49.911415"}
{"text": "730,714 Filed on Jul. 16 , 1991 .Bahl , L. R. , et al . \"A Maximum Likelihood Approach to Continuous Speech Recognition . \"IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .PAMI-5 , No . 2 , pp .", "label": "", "metadata": {}, "score": "49.932697"}
{"text": "( 1 ) e 1 and e 2 are already placed in the target sequence , in which case the value of the gap is known , and its contribution to h ELAST can be directly evaluated .This computation is performed when e 2 has just been moved from a suffix in the buffer to the sequence , using the sequence index of e 1 , which it is assumed has been maintained on the suffix .", "label": "", "metadata": {}, "score": "49.93608"}
{"text": "Such a decision produces a new state , which is placed on its proper stack , at a position determined by its heuristic cost , which can be an optimistic estimate of the best complete sequence of decisions passing through this state .In the decoder of Cancedda , et al .", "label": "", "metadata": {}, "score": "49.946087"}
{"text": "A unified message model according to various aspects of the present invention provides particular advantages over a conventional message model .A unified message model is a message model that includes a shared language model and models specific to at least two types of message recognition .", "label": "", "metadata": {}, "score": "50.017868"}
{"text": "Statistics regarding individual words and phonemes of the word hypotheses may be calculated using the processing system based on the alignment .BRIEF DESCRIPTION OF THE DRAWINGS .[0009 ] FIG .1 depicts a computer - implemented environment wherein users can interact with a non - native speech assessment engine hosted on one or more servers through a network .", "label": "", "metadata": {}, "score": "50.10245"}
{"text": "When the best - scoring recognition candidate is a command , the control / interface module 220 implements the command .For example , the control / interface module may control operation of the speech recognition software in response to speech recognition commands ( for example , \" wake up \" , \" make that \" ) , and may forward external commands to the appropriate software .", "label": "", "metadata": {}, "score": "50.198914"}
{"text": "For example , it may be distinguished by a special notation for convenience .In order to determine \u03bc for a gap size on the source side , consider s , and observe all sentence pairs in the training corpus which use a bi - phrase of the form ( s , t ) .", "label": "", "metadata": {}, "score": "50.2062"}
{"text": "Multilayer neural networks trained with the back - propagation algorithm constitute the best example of a successful gradientbased learning technique .Given an appropriate network architecture , gradient - based learning algorithms can be used to synthesize a complex decision surface that can classify high - dimensional patterns , such as handwritten characters , with minimal preprocessing .", "label": "", "metadata": {}, "score": "50.31845"}
{"text": "If the score exceeds the threshold value , then the recognizer 215 determines that the hypothesis is too unlikely to merit further consideration and deletes the hypothesis ( step 1240 ) .If the recognizer determines that the word or words corresponding to the hypothesis were likely to have been spoken by the user , then the recognizer determines whether the last word of the hypothesis is ending ( step 1245 ) .", "label": "", "metadata": {}, "score": "50.378494"}
{"text": "For each bi - fragment identified from the corpus , a probability distribution ( or a mean of the probability distribution ) representing the statistical probabilities of number of gaps in the target fragment for each observed number of gaps in the source may be stored and used to develop distributions .", "label": "", "metadata": {}, "score": "50.428577"}
{"text": "J .d .K . )i .J . log .Pr .t .j .t .j .N .j .\" Word - count \" and \" bi - fragment \" count feature functions h wc and h bc , illustrated at 40 and 42 , respectively .", "label": "", "metadata": {}, "score": "50.451584"}
{"text": "Automatic Determination of Pronunciation of Words From Their Spellings . \"IBM Technical Disclosure Bulletin , vol .32 , No .10B Mar. 1990 , pp .19 . gtoreq.23 .Bahl , L. R. et al .\" Fast Algorithm for Deriving Acoustic Prototypes for Automatic Speech Recognition . \" U.S. patent application Ser .", "label": "", "metadata": {}, "score": "50.53483"}
{"text": "U.S. Pat .No .5,502,774 to Bellegarda discloses a multi - source message recognizer that includes both a speech recognizer and a handwriting recognizer .The ' 774 patent briefly states , at column 4 , lines 54 - 58 , that the training of respective source parameters for speech and handwriting recognizers may be done globally using weighted sum formulae of likelihood scores with weighted coefficients .", "label": "", "metadata": {}, "score": "50.542137"}
{"text": "As a consequence , the model is able to consider an extended and more representative class of potential translations and to evaluate them more accurately .The exemplary embodiment introduces the notion of elastic bi - phrase , and provides a decoding mechanism for producing translations using the resulting model .", "label": "", "metadata": {}, "score": "50.543358"}
{"text": "The computer readable medium of .claim 35 wherein the software instructions comprise a sixth code segment to insert the produced speech recognition result into the text at a predetermined location if text has not been selected .Description .TECHNICAL FIELD .This invention relates to computer - implemented speech recognition , and more particularly , to speech recognition using a word - in - phrase command .", "label": "", "metadata": {}, "score": "50.58297"}
{"text": "All of these features made a significant contribution to the word error rate improvements of the complete system .In addition , several minor changes have been made and these include the use of additional training data and revised transcriptions ; acoustic data weighting ; and an increased vocabulary size .", "label": "", "metadata": {}, "score": "50.598217"}
{"text": "The 180 hour training set used for training the 1998 HTK system used various sources of Swbd1 transcriptions and turn - level segmentations .For the March 2000 system we took advantage of the January 2000 release from Mississippi State University ( MSU ) of Swbd1 transcriptions which should provide greater accuracy and consistency .", "label": "", "metadata": {}, "score": "50.608093"}
{"text": "The output information is applied to one or more output devices .Each program is preferably implemented in a high level procedural or object oriented programming language to communicate with a computer system .However , the programs can be implemented in assembly or machine language , if desired .", "label": "", "metadata": {}, "score": "50.807983"}
{"text": "The \" detailed \" acoustic match score estimates the closeness of a match between an acoustic detailed match model of a word and the sequence of coded representations of the utterance .Still referring to FIG .1 , the speech recognition system further comprises a translation match score generator 26 for generating a translation match score for each speech hypothesis .", "label": "", "metadata": {}, "score": "50.812027"}
{"text": "[ 0047 ] FIG .11 is a flow diagram depicting an example computer - implemented generation of a speech score using speech recognition filtering .Digitized speech is received at 1102 , and a speech recognition is performed at 1104 to generate a set of word hypotheses using an acoustic model trained using non - native speech at 1106 .", "label": "", "metadata": {}, "score": "50.823414"}
{"text": "The component scores , such as the pronunciation assessment 908 , may also be reported to identify areas that should be concentrated upon for further improvement .[ 0046 ]FIG .10 is a flow diagram depicting an example computer - implemented generation of a speech score .", "label": "", "metadata": {}, "score": "50.849606"}
{"text": "This approach combines a tree representation of the dictionary with e cent pruning techniques to reduce the search space without loosing much recognition performance compared to a at exhaustive search through all words in the dictionary .The tree search with pruning is about 15 times faster than a at search and allows us to run the NPen + + on - line handwriting recognition system in real - time with dictionary sizes up to 100,000 words .", "label": "", "metadata": {}, "score": "50.990448"}
{"text": "We demonstrate that transformation - based learning can be used to correct noisy speech recognition transcripts in the lecture domain with an average word error rate reduction of 12.9 % .Our method is distinguished from earlier related work by its robustness to small amounts of training data , and its resulting efficiency , in spite of its use of true word error rate computations as a rule scoring function . \" ...", "label": "", "metadata": {}, "score": "51.026817"}
{"text": "The value of the constant term A may be selected in accordance with design or user preference .The second exemplary semantic model , according to the present invention , expresses probability of a tested text segment as the product of negative biasing terms .", "label": "", "metadata": {}, "score": "51.05211"}
{"text": "The acoustic model generator 18 generates an acoustic model of at least one word in the source text which is not in the source vocabulary .In one embodiment of the invention , this comparator may operate according to a set of rules that describe the manner in which letters in the source language should be rewritten when translated into the target language .", "label": "", "metadata": {}, "score": "51.077576"}
{"text": "From the source text and from the translations , candidate word generator 20 generates a set of candidate words consisting solely of words in the target language which are partial or full translations of words in the source text .The set of candidate words is provided to speech hypothesis generator 16 .", "label": "", "metadata": {}, "score": "51.081024"}
{"text": "We demonstrate some experiments of spoken dialogue tasks and empirical results which show an improvement of the accuracy for both speech recognition and spoken language understanding .If we assume that the output word sequences produced by ASR are independent of ...Sign up to receive free email alerts when patent applications with chosen keywords are published SIGN UP .", "label": "", "metadata": {}, "score": "51.146744"}
{"text": "However , text segments that occur in very many contexts ( i.e. , very high probability of global cluster occurrence ) are of limited usefulness as stop segments because they could be expected to cause negative bias to many alternative text segments , in some cases a majority .", "label": "", "metadata": {}, "score": "51.17283"}
{"text": "With proportional biasing , the adapted model applies a more negative bias against text segments that can be expected to appear frequently in semantic clusters together with a stop segment .In a variation , the negative biasing may also be inversely proportional to the probability of global cluster occurrence of each tested text segment .", "label": "", "metadata": {}, "score": "51.183205"}
{"text": "13J , the user types the letters \" amicu \" , and the control / interface module 220 responds by producing an updated list 1351 of recognition candidates that start with the words \" those who pronounce \" and include a word starting with the letters \" amicu \" .", "label": "", "metadata": {}, "score": "51.19419"}
{"text": "This is because , as with the method of Cancedda , et al ., the stacks are generally pruned to remove the lowest ( highest cost ) states , once they reach a preselected number of states .In the elastic decoder , source coverage may not be the best way to place a hypothesis in the stack .", "label": "", "metadata": {}, "score": "51.20762"}
{"text": "If no clusters remain to be analyzed , process 1100 is complete and method 1000 continues at 1030 ( .FIG .10 ) .At step 1030 , the probability of global cluster occurrence is computed for most ( or all ) text segments in the corpus . \" V \" indicates \" given the entire vocabulary ( i.e. , corpus ) .", "label": "", "metadata": {}, "score": "51.226067"}
{"text": "Those statistics are utilized at 1112 to calculate features for the digitized speech .The assessment generation may only calculate features for words having a confidence level greater than a threshold level .Such a filtering may improve assessment score calculation quality by avoiding improperly penalizing exam takers based on mistakes in word recognition .", "label": "", "metadata": {}, "score": "51.2714"}
{"text": "The word - in - phrase command may be implemented in any language to correct or avoid recognition errors such as those due to homophone errors .The word - in - phrase command also may be implemented to correct other types of misrecognition results .", "label": "", "metadata": {}, "score": "51.311604"}
{"text": "Producing the speech recognition result may include producing the substring of the phrase that sounds similar to the word .The method may also include producing no speech recognition result if the word does not correspond to a portion of the phrase .The method may also include determining if previously recognized text has been selected .", "label": "", "metadata": {}, "score": "51.41416"}
{"text": "When the relative amplitude indicates that the voice input is impinging on the stylus from within a predetermined angular range , the comparator makes a positive determination that causes activation of speech recognition .When such an activation circuit is used , the function of a microphone may be easily incorporated into one of the sound sensors .", "label": "", "metadata": {}, "score": "51.476566"}
{"text": "In a tabulation performed at step 1060 of method 1000 , using a training corpus described above , it is expected that the secondary text segments shown in .In a variation , step 1060 may include manual editing of a tabulation such as that of data structure 1200 .", "label": "", "metadata": {}, "score": "51.522903"}
{"text": "Frequent misrecognitions make generation of text data tedious and slow .To minimize the frequency of misrecognitions , it is desirable to provide voice input of relatively high acoustic quality to the speech recognizer .A signal with high acoustic quality has relatively consistent reverberation and amplitude , and a high signal - to - noise ratio .", "label": "", "metadata": {}, "score": "51.58316"}
{"text": "As for the method of Cancedda , et al . , the beam search may proceed as for a conventional beam search except as otherwise noted .Hypotheses deemed equivalent may be merged together .With elastic bi - fragments , one conditions is verified before adding an edge from a hypothesis H : .", "label": "", "metadata": {}, "score": "51.611134"}
{"text": "For example , user correction of misrecognition by speech recognizer 312 adapts syntactic model 352 of shared language model 350 .Both speech recognizer 312 and handwriting recognizer 314 then operate in accordance with the adapted syntactic model .A local message model according to various aspects of the present invention is any message model that is local to a message recognizer operating in accordance with the message model .", "label": "", "metadata": {}, "score": "51.671562"}
{"text": "Convolutional neural networks , which are specifically designed to deal with the variability of two dimensional ( 2-D ) shapes , are shown to outperform all other techniques .Real - life document recognition systems are composed of multiple modules including field extraction , segmentation , recognition , and language modeling .", "label": "", "metadata": {}, "score": "51.69129"}
{"text": "The pre - filtering procedure does not reinitialize the lexical tree between requests for list of words .Accordingly , the state of the lexical tree when a list of words is requested corresponds to the state of the lexical tree after a previous list of words was returned .", "label": "", "metadata": {}, "score": "51.71141"}
{"text": "This invention was made with Government support under Contract Number N00014 - 91-C-0135 awarded by the office of Naval Research .The Government has certain rights in this invention .Claims .We claim : .A speech recognition system comprising : . means for displaying a source text comprising one or more words in a source language ; . an acoustic processor : for generating a sequence of coded representations of an utterance to be recognized , said utterance comprising one or more words in a target language different from the source language ; . means for generating a set of one or more speech hypotheses , each speech hypothesis comprising one or more words from the target language ; . means for generating an acoustic model of each speech hypothesis ; . means for generating an acoustic match score for each speech hypothesis , each acoustic match score comprising an estimate of the closeness of a match between the acoustic model of the speech hypothesis and the sequence of coded representations of the utterance ; . means for generating a translation match score for each speech hypothesis , each translation match score comprising an estimate of the probability of occurrence of the speech hypothesis given the occurrence , of the source text ; . means for generating a hypothesis score for each hypothesis , each hypothesis score comprising a combination of the acoustic match score and the translation match score for the hypothesis ; . means for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores ; and . means for outputting at least one word of one or more of the speech hypotheses in the subset of speech hypotheses having the best hypothesis scores .", "label": "", "metadata": {}, "score": "51.791267"}
{"text": "Computer - implemented systems and methods are provided for assessing non - native spontaneous speech pronunciation .Speech recognition on digitized speech is performed using a non - native acoustic model trained with non - native speech to generate word hypotheses for the digitized speech .", "label": "", "metadata": {}, "score": "51.79439"}
{"text": "In addition to the two features described above , the log linear model may include analogous features to those described in Cancedda , et al , along with their associated heuristics .These may include one or more of : .A \" compositional bi - fragment \" feature function h comp illustrated at 36 .", "label": "", "metadata": {}, "score": "51.799576"}
{"text": "In practice , it is to be expected that contiguous bi - fragments ( the gap size is 0 ) are somewhat more reliable than non - contiguous ones .Therefore , one aspect that the translation scoring function may take into account is the amount of \" non - contiguity \" that goes into any given translation .", "label": "", "metadata": {}, "score": "51.823883"}
{"text": "11 is a flow diagram of a process for analyzing semantic clusters of a training corpus according to various aspects of the present invention ; .FIG .12 illustrates a partial example of a data structure tabulating probabilities computed in the method of .", "label": "", "metadata": {}, "score": "51.850365"}
{"text": "Briefly , this procedure works as follows .The probability of the aligned sentence pairs is a computable function of the parameter values .The goal of the procedure it to find parameter values which locally maximize this function .This is accomplished iteratively .", "label": "", "metadata": {}, "score": "51.9073"}
{"text": "5,839,106 to Bellegarda , incorporated by reference above .In a further variation , the second exemplary semantic model may be combined with a conventional syntactic model by treating the semantic model as a negative biasing factor to the syntactic model .In the combination , the semantic model may be appropriately limited such that even a text segment that is a stop word may be selected as the message recognizer 's \" best guess \" if the segment is a very strong fit in the syntactic model .", "label": "", "metadata": {}, "score": "51.91996"}
{"text": "The estimates of the word posterior probabilities encoded in the confusion networks can be used directly as confidence scores ( which are essentially word - level posteriors ) , but they tend to be over - estimates of the true posteriors .This effect is due to the assumption that the word lattices represent the relevant part of the search space .", "label": "", "metadata": {}, "score": "52.03504"}
{"text": "Previous work demonstrated that a simple word - for - word channel model was sufficient to yield substantial increases in word accuracy .This paper demonstrates that some improvements in word accuracy result from augmenting the channel model with an account of word fertility in the channel .", "label": "", "metadata": {}, "score": "52.046783"}
{"text": "Bahl , L. R. , et al .\" Speaker - Independent Label Coding Apparatus \" .U.S. patent application Ser .No .673,189 , filed Mar. 22 , 1991 .Bahl , L. R. , et al . \"Vector Quantization Procedure For Speech Recognition Systems Using Discrete Parameter Phoneme - Based Markov Word Models . \"", "label": "", "metadata": {}, "score": "52.079197"}
{"text": "Currently , most speech recognition systems are based on hidden Markov models ( HMMs ) , a statistical framework that supports both acoustic and temporal modelin ... \" .This thesis examines how artificial neural networks can benefit a large vocabulary , speaker independent , continuous speech recognition system .", "label": "", "metadata": {}, "score": "52.08172"}
{"text": "In aspects of the exemplary embodiment , gaps in source and target phrases are modeled utilizing one or more of the following simplifying assumptions : .For a given bi - phrase ( s , t ) , the gap sizes ( source and/or target ) are mutually independent .", "label": "", "metadata": {}, "score": "52.122593"}
{"text": "In another aspect , a transmit digital processing system for wireless transmission of HDMI and/or DVI data is disclosed .The system converts the data into two data streams and includes a front end component multiplexing video data with control data .In non - limiting implementations of the transmit digital processing system , a forward error correcting component such as , e.g. , a Reed - Solomon encoder receives an output of the front end component , which outputs a substantially continuous stream of data to the Reed - Solomon Encoder .", "label": "", "metadata": {}, "score": "52.13916"}
{"text": "Moreover , the method may include inserting the produced speech recognition result into the text at a predetermined location if text has not been selected .The word - in - phrase command provides a natural way for Chinese users to disambiguate a homophone word by putting it in a larger phrase that is less ambiguous .", "label": "", "metadata": {}, "score": "52.161613"}
{"text": "^ .arg .max .t .J . m .M . m .h .m .s .I . t .J . )Exemplary Features Suitable for Treatment of Elastic Bi - Phrases .For example , the present model can include the following feature functions as illustrated in .", "label": "", "metadata": {}, "score": "52.167255"}
{"text": "While this architecture results in a complex overall system , this section also reports the results of each of the stages .This allows the performance of many system variants at different levels of complexity to be assessed .The VTLN acoustic models used in the system were either triphones ( 6165 speech states/16 Gaussians per state ) or quinphones ( 9640 states/16 Gaussians per state ) trained on h5train00 .", "label": "", "metadata": {}, "score": "52.16814"}
{"text": "Thus , the use of non - native training text 622 may enable stronger confidence in identification when matched with non - native input speech .In one example carried out by the inventors , a large corpus of about 100 hours of transcribed non - native speech was used in training the language model .", "label": "", "metadata": {}, "score": "52.228184"}
{"text": "An important feature of the MSU transcripts is the full - word transcription of false starts and mispronunciations .In order to make use of the extended transcripts a dictionary of false starts and mispronunciations was created for use during training .Three different training sets were used during the course of development : the 18 hour Minitrain set defined by BBN which gives a fast turnaround ; the full 265 hour training set ( h5train00 ) for the the March 2000 system and a subset of h5train00 denoted h5train00sub .", "label": "", "metadata": {}, "score": "52.33743"}
{"text": "Based on that data set , correlations may be calculated for each feature in the data set and the human grader 's scores .Features having high correlations with the human grader 's scores can be used to score pronunciation proficiency better .Thus , in one example , the scoring model 902 may be trained using a multiple regression technique based on the L3 , L6 , L7 , and S n features and the associated human grader 's score for each speech response of a prior speech test .", "label": "", "metadata": {}, "score": "52.362232"}
{"text": "The acoustic model 614 associates probabilities with speech units called phones , which represent a given phoneme .The recognition process divides the digitized speech 606 into intervals ( e.g. , 10 millisecond intervals or intervals of other durations ) , and spectral features are extracted for each interval .", "label": "", "metadata": {}, "score": "52.398678"}
{"text": "Where a decision results in consumption of a buffered word , the sequence remains in the same stack .Provided that this new hypothesis is not pruned , it is reconsidered in that stack .Eventually , one or more of the stacks may reach or exceed its maximum beam size and is culled by eliminating hypotheses which are statistically less favored to bring the number of hypotheses to the maximum permitted by the beam .", "label": "", "metadata": {}, "score": "52.425106"}
{"text": "a combined score generator which uses the language match score and the conditional translation match score to produce an estimate of a joint probability P(S , T.cndot . )The combined match score generator will now be described .In the prior art , language match scores and conditional translation match scores are combined only when the words of S are generated from the words T and no other words .", "label": "", "metadata": {}, "score": "52.452217"}
{"text": "Next , the module determines log(X(f ) ) 2 ( step 310 ) .The module may then perform frequency warping ( step 315 ) and a filter bank analysis ( step 320 ) to achieve speaker normalization .See S. Wegmann et al . , \" Speaker Normalization on Conversational Speech , \" Proc .", "label": "", "metadata": {}, "score": "52.50579"}
{"text": "For example , the user may highlight text from the dictation window .The user also may select text using an input device such as a mouse or keyboard .If text has been selected ( step 1445 ) , the speech recognition system replaces the text with the produced result ( step 1450 ) .", "label": "", "metadata": {}, "score": "52.522537"}
{"text": "Table 3 shows word error rates using triphone HMMs trained on h5train00 .These experiments required the generation of numerator and denominator lattices for each of the 267,611 training segments .It was found that two iterations of MMIE re - estimation gave the best test - set performance [ 19 ] .", "label": "", "metadata": {}, "score": "52.577248"}
{"text": "9 is a block diagram depicting the generation of an assessment score based on calculated features .A scoring model 902 can be trained 904 based on features of speech responses from prior language tests and associated scores 906 .For example , a data set may be received that identifies , for a number of prior speech evaluations , features such as those in FIG .", "label": "", "metadata": {}, "score": "52.637215"}
{"text": "The search terminates when all remaining hypotheses on the stack are complete , i.e. , they cover all the source words .Existing decoding procedures assume bi - fragments to be contiguous .The following method adapts a beam search procedure to handle elastic bi - fragments .", "label": "", "metadata": {}, "score": "52.64238"}
{"text": "The first and second text fragments may be a word or a phrase .The first and second fragments are associated in a word - aligned corpora , such as a library stored in memory , whereby a target fragment corresponding to an input source fragment may be retrieved .", "label": "", "metadata": {}, "score": "52.642525"}
{"text": "In a conventional speech recognizer such as a computer executing the NATURALLY SPEAKING software , the modifications to the selected text segment are entered when the user selects an \" OK \" control of a displayed user interface dialog .At step 1460 , the syntactic model is conventionally adapted in accordance with the user 's modification to the selected text segment .", "label": "", "metadata": {}, "score": "52.665573"}
{"text": "If no text segments remain to be counted in the present cluster , process 1100 continues at decision step 1140 .At decision step 1140 , a determination is made as to whether or semantic clusters of the corpus are still to be analyzed .", "label": "", "metadata": {}, "score": "52.70738"}
{"text": "For example , a list of text segments that are known to be especially useful as stop segments may be presented to a user .With a conventional dialog user interface , the user may check boxes appropriately labeled ( e.g. , \" User \" and \" Present \" ) next to listed text segments to identify them as stop segments being outside of the user or present context , respectively .", "label": "", "metadata": {}, "score": "52.797165"}
{"text": "The pooled estimates can than replace standard estimates , or they can be used as a back - off when too few occurrences of a bi - phrase are observed , and the standard estimates are likely to be unreliable .Gaps on the target side can be modeled analogously .", "label": "", "metadata": {}, "score": "52.878662"}
{"text": "Also p.sub.9 ( k.vertline.T ) is the probability of the set of all complete sentences which begin with T and contain k additional words .In one embodiment this probability is estimated as # # EQU4 # # where q is an estimate of the unigram probability of the end - of - sentence marker .", "label": "", "metadata": {}, "score": "52.891575"}
{"text": "Speech recognizer 162 , tablet interface 164 , and editor 168 are described in greater detail below .According to various aspects of the present invention , a communications link may employ any suitable medium , modulation format , and/or communications protocol to convey information from transmitter in a stylus to a receiver in , for example , a table .", "label": "", "metadata": {}, "score": "53.00583"}
{"text": "However as a side effect , we found that there were reduced benefits from multiple iterations of MLLR when used with a full variance transform .Confusion networks allow estimates of word posterior probabilities to be obtained .For each link in a particular word lattice ( from standard decoding ) a posterior probability is estimated using the forward - backward algorithm .", "label": "", "metadata": {}, "score": "53.02211"}
{"text": "The task of the conditional translation match score generator is to compute a conditional translation score P(S.vertline .T ) of a sequence S of source given a sequence T of target words .In one embodiment of a conditional translation match score generator , the probability of S given T is computed as # # EQU5 # # .", "label": "", "metadata": {}, "score": "53.050285"}
{"text": "FIG .15 , the speech recognition system processes the word - in - phrase command according to a procedure 1435 .After extraction , the system determines if is a substring of ( step 1505 ) .For example , the system may calculate a statistical difference between acoustic models for different words , and may designate words as being similar when this difference is less than a threshold amount .", "label": "", "metadata": {}, "score": "53.150234"}
{"text": "Stop segments may be identified by a user in a context definition mode .For example , a context definition mode may present the user with ( 1 ) a maximum likelihood text segment and ( 2 ) alternative text segments that the user could identify as being outside the present context or user context .", "label": "", "metadata": {}, "score": "53.175995"}
{"text": "The method of .claim 1 , wherein the translation scoring function utilizes a log - linear scoring model of the form : .Pr .t .J .d .K .s .I . )Z .f .I . exp .", "label": "", "metadata": {}, "score": "53.178673"}
{"text": "Robustness is achieved by a combination of statistical error post - correction , syntactically- and semantically - driven robust parsing , and extensive use of the dialogue context .We present an evaluation of the system using time - to - completion and the quality of the final solution that suggests that most native speakers of English can use the system successfully with virtually no training . by A. Stolcke , H. Bratt , J. Butzberger , H. Franco , V. R. Rao Gadde , M. Plauch\u00e9 , C. Richey , E. Shriberg , K. S\u00f6nmez , F. Weng , J. Zheng - In Proceedings of the NIST Speech Transcription Workshop , 2000 . \" ...", "label": "", "metadata": {}, "score": "53.194473"}
{"text": "The hypothesis score may then comprise the product of the acoustic match score multiplied by the translation match score .An acoustic model generator produces an acoustic model of each word in the source text which is not in the source vocabulary .Each word in the source text has a spelling comprising one or more letters .", "label": "", "metadata": {}, "score": "53.222115"}
{"text": "For example , a message model implemented in the same memory space , virtual machine , or CPU as a message recognizer is said to be local to the message recognizer .A user message model according to various aspects of the present invention is any message model that is specific to a particular user .", "label": "", "metadata": {}, "score": "53.299004"}
{"text": "I . t .J .d .K . ) k .K . log .Pr . target .d . k . ) source .d . k . )( 2 ) a gap size scoring feature ( h ELAST ) , of the general form : . h .", "label": "", "metadata": {}, "score": "53.32227"}
{"text": "Each frame includes 24 parameters and represents a short portion ( for example , 10 milliseconds ) of the utterance .As shown in .FIG .3 , the front end processing module 200 produces a frame from digital samples according to a procedure 300 .", "label": "", "metadata": {}, "score": "53.353912"}
{"text": "1 .Like reference symbols in the various drawings indicate like elements .DETAILED DESCRIPTION .A speech recognition system uses a word - in - phrase command in a recognition candidate to disambiguate homophone errors .As background information for the discussion of disambiguation of homophones , a speech recognition system that does not use a word - in - phrase command is discussed with reference to .", "label": "", "metadata": {}, "score": "53.36508"}
{"text": "w .i .V . ) k .N .Z .i . k .j .L . k .M .Z .j . k .At step 1040 , a subset of text segments is selected for further analysis .", "label": "", "metadata": {}, "score": "53.36654"}
{"text": "The data signals can carry any or all of the data disclosed herein that is provided to or from a device .Additionally , the methods and systems described herein may be implemented on many different types of processing devices by computer program code comprising program instructions that are executable by a processing system .", "label": "", "metadata": {}, "score": "53.36985"}
{"text": "In the decoder of Cancedda , et al . , the states ( sometimes called \" merged hypotheses \" ) are organized in stacks , where two states are put in the same stack if they have the same number of source words covered .", "label": "", "metadata": {}, "score": "53.39982"}
{"text": "When a score exceeds the pruning threshold , the likelihood that the word represented by the score was spoken is deemed to be too small to merit further consideration .For this reason , the procedure prunes the lexical tree by deactivating any state having a score that exceeds the pruning threshold ( step 1115 ) .", "label": "", "metadata": {}, "score": "53.46431"}
{"text": "Or the text may be input from a database or word document .The text may include one or more sentences , such as a paragraph or an entire document comprising multiple paragraphs .The translation system 10 includes a processing component 16 and a memory component 18 .", "label": "", "metadata": {}, "score": "53.488525"}
{"text": "[ 0006 ] In accordance with the teachings herein , computer - implemented systems and methods are provided for assessing spontaneous speech pronunciation , e.g. of non - native language speakers .Word hypotheses may be generated by performing speech recognition on digitized speech using a non - native acoustic model trained with non - native speech .", "label": "", "metadata": {}, "score": "53.501945"}
{"text": "The pre - filtering procedure reseeds the tree by replacing the score for the root node 505 with the minimum of the score for the silence node 512 and the scores for any words produced by leaf nodes of the lexical tree for the current frame .", "label": "", "metadata": {}, "score": "53.55775"}
{"text": "Other rules in such a system specify the dropping of accents from letters , or the modification of verbal endings .The comparator 34 may also identify words in the source text that begin with an uppercase letter , but do not appear in the source language vocabulary , and place them into the target language vocabulary .", "label": "", "metadata": {}, "score": "53.581543"}
{"text": "1 , the speech recognition system comprises a hypothesis score generator 28 for generating a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .The speech recognition system further comprises a storage device 30 for storing a subset of one or more speech hypotheses , from the set of speech hypotheses , having the best hypothesis scores .", "label": "", "metadata": {}, "score": "53.58693"}
{"text": "When the recognizer 215 is unsuccessful , the enrollment program prompts the user to repeat certain passages of the text .The recognizer uses acoustic information from the user 's utterances to train or adapt acoustic models 235 corresponding to the matched portions of the enrollment text .", "label": "", "metadata": {}, "score": "53.622604"}
{"text": "FIG .2 , a block diagram of an automated natural language translation system 10 for translating text comprising elastic bi - fragments is illustrated .Text is input to the system by an input device 12 .Input text may be directly input into the natural language translation system 10 ( for example , as with a person typing sentences into a computer using a keyboard ) .", "label": "", "metadata": {}, "score": "53.635433"}
{"text": "Activation signal S A of system 100 is conveyed with a single data bit that indicates either an \" activate \" command or an \" inactivate \" command .Alternately , a standard number of data bits may be used , with unused bits left unspecified or in a default state .", "label": "", "metadata": {}, "score": "53.636818"}
{"text": "While MMIE is very effective at reducing training set error a key issue is generalisation to test data .It is very important that the confusable data generated during training ( as found from the posterior distribution of state occupancy for the recognition lattice ) is representative to ensure good generalisation .", "label": "", "metadata": {}, "score": "53.659344"}
{"text": "in the logarithmic domain , where F ' .The normalized twenty dimension feature vector X(t ) may be further processed by an adaptive labeler 60 to adapt to variations in pronunciation of speech sounds .An adapted twenty dimension feature vector X'(t ) is generated by subtracting a twenty dimension adaptation vector A(t ) from the twenty dimension feature vector X(t ) provided to the input of the adaptive labeler 60 .", "label": "", "metadata": {}, "score": "53.661346"}
{"text": "The translation match score generator 26 will now be described .The role of the translation match score generator is to compute a translation match score Score(S , T.cndot . ) that a finite sequence S of source words is the translation of a sequence of target words beginning with the finite sequence T. Here and in the following , T.cndot . will denote the set of all complete target sentences that begin with the sequence of target words T. A complete sentence is a sequence that ends in a special end - of - sentence marker .", "label": "", "metadata": {}, "score": "53.685913"}
{"text": "A limit may be placed on the number of uncovered source language words to the left of the last covered word .Note that if the constraint completely disallows uncovered source language words to the left of the last covered word , the decoder works as a monotone decoder , meaning that reorderings are not allowed between the source and the target .", "label": "", "metadata": {}, "score": "53.737732"}
{"text": "The method includes the step of performing speech recognition on an utterance to produce a recognition result for the utterance .The command includes a word and a phrase ....A method is described that corrects incorrect text associated with recognition errors in computer - implemented speech recognition .", "label": "", "metadata": {}, "score": "53.744"}
{"text": "Applying a noisy - channelmodel , SPEECHPP uses a Viterbi beam - search that employs language and channel models .Previous work demonstrated that a simple word - for - word channel m ... \" .We have implemented a post - processor called SPEECHPP to correct word - level errors committed by an arbitrary speech recognizer .", "label": "", "metadata": {}, "score": "53.797585"}
{"text": "Working on a string of contiguous words in the source language , such as a sentence of the input text , the translation module 22 accesses the library 20 to retrieve fragments in the target language corresponding to input fragments in the source language text string .", "label": "", "metadata": {}, "score": "53.805054"}
{"text": "In this section a short overview of its features is given ( see [ 6 ] for details ) .The system uses perceptual linear prediction cepstral coefficients derived from a mel - scale filterbank ( MF - PLP ) [ 18 ] covering the frequency range from 125Hz to 3.8kHz .", "label": "", "metadata": {}, "score": "53.867043"}
{"text": "The language model components of the leaving penalties for all states in a particular node together represent a language model score for the phoneme associated with that node .The language model score represents the likelihood that a word including the phoneme will occur in speech .", "label": "", "metadata": {}, "score": "53.871994"}
{"text": "We present results for the writer independent recognition of isolated words .Tested on different dictionary sizes from 1,000 up to 100,000 words , recognition rates range from 98.0 % for the 1,000 word dictionary to 91.4 % on a 20,000 word dictionary and 82.9 % for the 100,000 word dictionary .", "label": "", "metadata": {}, "score": "53.954388"}
{"text": "Writer independent , large vocabulary on - line handwriting recognition systems require robust input representations , which make optimal use of the dynamic writing information , i.e. the temporal ordering of the sampled data points .In this paper we describe an input representation for cursive handwriti ... \" .", "label": "", "metadata": {}, "score": "53.97129"}
{"text": "2 is a block diagram of a portion of one example of a speech recognition system according to the invention .In this embodiment , the speech recognition system further comprises a candidate word generator 20 for generating a set of candidate words consisting solely of words in the target language which are partial or full translations of words in the source text .", "label": "", "metadata": {}, "score": "53.98454"}
{"text": "For a given model set a single Gaussian per state version was created .For each speech state in the single Gaussian system , the nearest two other states were found using a log - overlap distance metric [ 14 ] , which calculates the distance between two Gaussians as the area of overlap of the two probability density functions .", "label": "", "metadata": {}, "score": "54.015377"}
{"text": "No . 08/825,536 , titled \" ENROLLMENT IN SPEECH RECOGNITION \" and filed Mar. 28 , 1997 , now U.S. Pat .No .6,212,498 , which is incorporated by reference .When the system makes a recognition error , the user may invoke an appropriate correction command to remedy the error .", "label": "", "metadata": {}, "score": "54.032814"}
{"text": "11/315,043 , filed Dec. 22 , 2005 , entitled MACHINE TRANSLATION USING NON - CONTIGUOUS FRAGMENTS OF TEXT , by Nicola Cancedda , et al . , the disclosure of which is incorporated herein by reference in its entirety .BACKGROUND .The present exemplary embodiment is directed to the field of machine translation .", "label": "", "metadata": {}, "score": "54.094757"}
{"text": "Decoding , as this procedure is called , is done by expanding a search graph on the fly .Each node H in the graph is uniquely defined by the sequence of decisions d 1 k that connect it to an initial \" null hypothesis \" node H 0 ( a starting hypothesis in which there are no words translated ) .", "label": "", "metadata": {}, "score": "54.103416"}
{"text": "Automatic Determination of Pronunciation of Words From Their Spellings . \"IBM Technical Disclosure Bulletin , Volume 32 , No .10B , March 1990 , pages 19 - 23 ; and J. M. Lucassen , et al . \"An Information Theoretic Approach To The Automatic Determination of Phonemic Baseforms . \" Proceedings of the 1984 IEEE International Conference on Acoustic and Signal Processing , Vol . 3 , pages 42.5.1 - 42.5.4 , March 1984 . )", "label": "", "metadata": {}, "score": "54.182266"}
{"text": "This feature may be computed as in equation ( 7 ) : .Pr . t . s .s .t .t .t . s .s .Pr . t .s . )A \" target language \" feature function h tl illustrated at 38 .", "label": "", "metadata": {}, "score": "54.353134"}
{"text": "It would be desirable however , for a speech recognizer to be automatically activated without the need for a specific selection action by the user .Speech recognition may be viewed as one possible subset of message recognition .As defined in Merriam - Webster 's WWWebster Dictionary ( Internet edition ) , a message is \" a communication in writing , in speech , or by signals . \" U.S. Pat .", "label": "", "metadata": {}, "score": "54.357353"}
{"text": "This graph consists of a sequence of so called confusion sets , which contain competing single word hypotheses with associated posterior probabilities .A path through the graph is found by choosing one of the alternatives from each confusion set .By picking the word with the highest posterior from each set the sentence hypothesis with the lowest overall expected word error rate can be found .", "label": "", "metadata": {}, "score": "54.38687"}
{"text": "The system of claim 12 , wherein the speech is spontaneous , non - native speech of a non - native language speaker .The non - transitory computer - readable memory of claim 23 , wherein the instructions cause the processor to perform steps comprising : excluding words not reliably recognized in generating the word hypotheses from contributing to the assessment score .", "label": "", "metadata": {}, "score": "54.474648"}
{"text": "4,759,068 entitled \" Constructing Markov Models of Words From Multiple Utterances , \" or by any other known method of generating acoustic word models .For context - dependent acoustic Markov word models , the context can be , for example , manually or automatically selected .", "label": "", "metadata": {}, "score": "54.509598"}
{"text": "First an overview of the 1998 HTK system is given .This is followed by a description of the data sets used in the experiments and then by sections that discuss each of the major new features of the system .Finally the complete March 2000 evaluation system is described and the results of each stage of processing presented .", "label": "", "metadata": {}, "score": "54.63805"}
{"text": "bregler @ irauka.de , manke @ irauka.de In this paper we show how recognition performance in automated speech perception can be significantly improved by additional Lipreading , so called \" Speech - reading \" .We show this on an extension of an existing state - of - the - art speech recognition system , a modular MS - TDNN .", "label": "", "metadata": {}, "score": "54.742958"}
{"text": "The TDT-2 corpus consists of about 60,000 stories collected over a six - month period from both newswire and audio sources .When the TDT-2 corpus is employed in process 1000 , each story is preferably partitioned into a discrete semantic cluster at step 1010 .", "label": "", "metadata": {}, "score": "54.761242"}
{"text": "The computer 110 may be used for traditional speech recognition .Under control of the operating system 120 and the speech recognition software 124 , the processor 112 identifies utterances in the user 's continuous speech .Utterances are separated from one another by a pause having a sufficiently large , predetermined duration ( for example , 160 - 250 milliseconds ) .", "label": "", "metadata": {}, "score": "54.802086"}
{"text": "Otherwise , if no instance of a target gap of a given size is observed , translations using this configuration will not be generated automatically , even though instances with smaller and larger gaps exist in the training corpus .In the present exemplary embodiment , the gaps can assume different sizes with certain probabilities .", "label": "", "metadata": {}, "score": "54.835674"}
{"text": "The enrollment program collects acoustic information from a user and trains or adapts a user 's models based on that information .The vocabulary customizer optimizes the language model of a specific topic by scanning user supplied text .The vocabulary manager is a developer tool that is used to browse and manipulate vocabularies , grammars and macros .", "label": "", "metadata": {}, "score": "54.86458"}
{"text": "Many techniques have been incorporated into various components of spoken dialogue systems to try to provide robustness in the presence of speech recognition errors .This paper describes a study that explores how humans go about signaling communication problems caused by speech recognition errors .", "label": "", "metadata": {}, "score": "54.88092"}
{"text": "In other embodiments , various approximations are made to simplify the computation of this sum .One such approximation is # # EQU2 # # Here T.cndot .sup.k denotes the set of target sequences that begin with T and contain k additional words , n is a parameter specifying the maximum allowed number of additional words , and . alpha . is a special generic target word .", "label": "", "metadata": {}, "score": "54.903275"}
{"text": "The translation system includes a library of bi - fragments .Each of the bi - fragments includes a text fragment from the first language and a text fragment from the second language .At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "54.939552"}
{"text": "In .FIG .2 and also in .FIGS .3A and 3B ( discussed below ) , interconnection between functional blocks is depicted by single lines .Such lines represent transfer of data by any suitable hardware or software interconnection , including , for example , the transfer of memory contents between software objects instantiated in a single processing environment .", "label": "", "metadata": {}, "score": "54.971085"}
{"text": "On the March 2000 Hub5 evaluation set the CU - HTK system gave an overall word error rate of 25.4 % , which was the best performance by a statistically significant margin .This paper describes the new system features and gives the results of each processing stage for both the 1998 and 2000 evaluation sets .", "label": "", "metadata": {}, "score": "54.97488"}
{"text": "The acoustic model generator produces an acoustic model of each word in the source text which is not in the source vocabulary , and which has an upper case first letter .The acoustic model generator may comprise , for example , a memory for storing a plurality of acoustic letter models .", "label": "", "metadata": {}, "score": "54.97987"}
{"text": "For a gap size on the target side , an analogous procedure may be used .One further simplification may be made , namely to assume that all the components in a gapped phrase are of length one , that is , correspond exactly to one single word .", "label": "", "metadata": {}, "score": "54.99288"}
{"text": "As another example , if the top recognition candidate includes the command \" make that , \" then the speech recognition system performs the make that correction command .If the candidate includes a word - in - phrase command ( step 1420 ) , then the speech recognition system processes the word - in - phrase command ( step 1435 ) and determines whether a word - in - phrase result has been produced ( step 1440 ) .", "label": "", "metadata": {}, "score": "55.02354"}
{"text": "[0002 ] The technology described herein relates generally to spoken language proficiency testing and more specifically to spoken language pronunciation proficiency testing .BACKGROUND .[ 0003 ] Automated systems for evaluating highly predictable speech have emerged in the past decade due to the growing maturity of speech recognition and processing technologies .", "label": "", "metadata": {}, "score": "55.024208"}
{"text": "FIGS .4A and 4B , this notation indicates that \" select \" may be followed by any ordered sequence of previously - recognized words .This grammar does not permit optional or alternate words .Constraint grammars are discussed further in U.S. Pat .", "label": "", "metadata": {}, "score": "55.085266"}
{"text": "Other implementations may also be used , however , such as firmware or even appropriately designed hardware configured to carry out the methods and systems described herein .It is noted that data structures describe formats for use in organizing and storing data in databases , programs , memory , or other computer - readable media for use by a computer program .", "label": "", "metadata": {}, "score": "55.09826"}
{"text": "A handwriting model is any model that may be used to determine basic units of handwriting conveyed by freehand input ( e.g. , strokes of printed characters or cursive words ) responsive to suitably transformed freehand input , for example as disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "55.150314"}
{"text": "An activation circuit that senses angle of impingement of sound waves as a condition for activation may include a plurality ( e.g. , two ) of sound sensors that are responsive to sound impinging on the stylus from a plurality of respective angular ranges .", "label": "", "metadata": {}, "score": "55.1872"}
{"text": "The second type of semantic cluster is a discrete grouping of text that may be expected to have a particular context , e.g. , discussing a particular topic .Examples of semantic clusters of this type include documents within a document collection ( e.g. , an article within the standard Wall Street Journal training corpus ) , paragraphs within a document , and sections of text separated by headings in a document .", "label": "", "metadata": {}, "score": "55.23024"}
{"text": "11 is a flow diagram depicting an example computer - implemented generation of a speech score using speech recognition filtering .[ 0020 ] FIGS .12A , 12B , and 12C depict example systems for use in implementing a non - native speech assessment engine .", "label": "", "metadata": {}, "score": "55.34828"}
{"text": "ASA 127th Meeting M.I.T. 1994 June 6 - 10 .4aSP1 .Linking speech and language processing through prosody . D. Macannuco .Dept . of Elec .Comput . and Systems Eng . , Boston Univ . , Boston , MA 02215 .", "label": "", "metadata": {}, "score": "55.44381"}
{"text": "Table 6 lists the evaluation system performance on the March 2000 evaluation set .The performance on eval00 gives a similar per stage improvement to that obtained for eval98 .However the absolute WER levels are reduced by about 10 % absolute .( All participating sites found that the eval00 data was easier to recognise than past Hub5 evaluation data sets . )", "label": "", "metadata": {}, "score": "55.457542"}
{"text": "A language model is any model that may be used to determine units of language responsive to input sequences of text segments .Such input sequences may be derived , for example , from voice input and/or freehand input in accordance with a respective acoustic model or handwriting model .", "label": "", "metadata": {}, "score": "55.4619"}
{"text": "The unigram ' ' pronunciation ' ' probability is found separately for each of these entries and the distributions are smoothed with the overall silence distributions .Finally all dictionary probabilities are renormalised so that the pronunciation for each word which has the highest probability is set to one .", "label": "", "metadata": {}, "score": "55.492912"}
{"text": "Preferably , the techniques are implemented in computer programs executing on programmable computers that each include a processor , a storage medium readable by the processor ( including volatile and non - volatile memory and/or storage elements ) , at least one input device , and at least one output device .", "label": "", "metadata": {}, "score": "55.528667"}
{"text": "The system of . claim 13 wherein the memory further comprises a software instruction for determining if previously - recognized text has been selected .The system of .claim 22 wherein the memory further comprises a software instruction for replacing selected text with the produced speech recognition result if text has been selected .", "label": "", "metadata": {}, "score": "55.57222"}
{"text": "The adapted models may be manually transferred to a separate computer system , for example , by copying files to a magnetic storage medium .The inconvenience of such a manual operation limits the portability of models in a conventional speech recognition system .", "label": "", "metadata": {}, "score": "55.580788"}
{"text": "In the illustrated embodiment , at extension step S 201 , the consumed portion of a target phrase do is highlighted .In the buffer , the non - consumed portion want of the phrase is stored along with the index I 2 , indicating that the last word of the consumed portion of the phrase is in the second position of the sequence .", "label": "", "metadata": {}, "score": "55.672844"}
{"text": "These words are stored in vocabulary files for the user and for the dictation topic , and are available as part of the backup dictionary for the dictation topic regardless of user , and to the user regardless of which dictation topic is being used .", "label": "", "metadata": {}, "score": "55.76151"}
{"text": "A user may identify stop segments by any suitable system or method .Stop segments may be identified in a context definition mode , which may be initiated whenever the identification of stop segments may be useful to determine context .The context definition mode may be initiated during a message recognition session , or prior to it .", "label": "", "metadata": {}, "score": "55.8499"}
{"text": "( A negative probability in this example is considered the probability of a stop segment not occurring in a given semantic cluster . )In this variation , a high probability of any one stop segment occurring in a given semantic cluster will cause one low negative probability to be included in the product .", "label": "", "metadata": {}, "score": "55.882713"}
{"text": "A system is disclosed for generating text responsive to both voice and handwriting input , including a microphone , stylus , and a tablet having an flat - panel display integrated into its surface .The system performs speech and handwriting recognition using a shared language model , which in one embodiment is trainable responsive to user correction of errors in either speech or handwriting recognition .", "label": "", "metadata": {}, "score": "55.913864"}
{"text": "In variations where the benefits of a domain controller are not required , networked tablets may each include a message recognizer and a receiver for establishing a communication link with one or more styluses .As discussed in greater detail with reference to .", "label": "", "metadata": {}, "score": "55.92956"}
{"text": "IEEE Int'l Conf .Acoustics , Speech , and Signal Processing , 1993 . \" ... bregler @ irauka.de , manke @ irauka.de In this paper we show how recognition performance in automated speech perception can be significantly improved by additional Lipreading , so called \" Speech - reading \" .", "label": "", "metadata": {}, "score": "55.930683"}
{"text": "The content of the buffer may vary during the extension of a hypothesis as new suffixes are added and consumed suffixes are removed .Where a suffix includes more than one word , the words may be removed from the buffer one at a time .", "label": "", "metadata": {}, "score": "55.967804"}
{"text": "Cepstral mean subtraction and variance normalisation are performed for each conversation side .Vocal tract length normalisation ( VTLN ) was applied in both training and test .The acoustic modelling used cross - word triphone and quinphone hidden Markov models ( HMMs ) trained using conventional maximum likelihood estimation .", "label": "", "metadata": {}, "score": "55.974415"}
{"text": "The software components and/or functionality may be located on a single computer or distributed across multiple computers in communication with one another depending upon the situation at hand .[ 0060 ] It should be understood that as used in the description herein and throughout the claims that follow , the meaning of \" a , \" \" an , \" and \" the \" includes plural reference unless the context clearly dictates otherwise .", "label": "", "metadata": {}, "score": "55.977715"}
{"text": "5,799,279 , titled \" CONTINUOUS RECOGNITION OF SPEECH AND COMMANDS \" and issued Aug. 25 , 1998 , which is incorporated by reference .Another constraint grammar 225 that may be used by the speech recognition software 124 is a large vocabulary dictation grammar .", "label": "", "metadata": {}, "score": "55.983593"}
{"text": "The implementation we have used is rather different to the one in [ 16 ] and does a full forward - backward pass constrained by ( a margin around ) the phone boundary times that make up each lattice arc .Furthermore the smoothing constant in the EBW equations is computed on a per - Gaussian basis for fast convergence and a novel weight update formulation used .", "label": "", "metadata": {}, "score": "56.111515"}
{"text": "FIG .14 , if a result was not produced in processing the word - in - phrase command ( step 1440 ) , the speech recognition system takes no action and awaits further recognition results ( step 1405 ) .If a result was produced ( step 1440 ) , the speech recognition system determines if text has been selected ( step 1445 ) .", "label": "", "metadata": {}, "score": "56.118908"}
{"text": "0010 ]FIG .2 is a block diagram depicting elements used for capturing speech from a person being evaluated .[ 0011 ] FIG .3 is a flow diagram depicting a process for assessing non - native spontaneous speech pronunciation .[ 0012 ]", "label": "", "metadata": {}, "score": "56.15603"}
{"text": "For example , a constraint grammar may be associated with a particular application program 122 and may be activated when the user opens the application program and deactivated when the user closes the application program .The recognizer 215 discards any hypothesis that does not comply with an active constraint grammar .", "label": "", "metadata": {}, "score": "56.1892"}
{"text": "No .07/968,097 , which matured into U.S. Pat .No .5,425,129 to Garman .At column 1 , lines 54 - 56 , the ' 129 patent states the object of providing a speech recognition system with word spotting capability , which allows the detection of indicated words or phrases in the presence of unrelated phonetic data .", "label": "", "metadata": {}, "score": "56.22645"}
{"text": "As discussed below with reference to .FIGS .2 , 3 , and 5 , an alternative system 200 includes a multi - input message recognizer 310 .Message recognizer 310 performs both types of message recognition ( speech and handwriting ) in accordance with a unified message model .", "label": "", "metadata": {}, "score": "56.26883"}
{"text": "In system 100 , signal 840 is modulated into an electromagnetic signal suitable for transmission by transmitter 122 .Receiver 160 demodulates the signal to provide signals corresponding to signals S V , S ID , and S A to speech recognizer 162 .", "label": "", "metadata": {}, "score": "56.29022"}
{"text": "To compensate for this a decision tree was trained to map the estimates to confidence scores .The confusion networks with their associated word posterior estimates were also used in an improved system combination scheme .Previously the ROVER technique introduced in [ 2 ] had been used to combine the 1-best output of multiple systems .", "label": "", "metadata": {}, "score": "56.313705"}
{"text": "FIG .1 .In one embodiment , outliers are ignored in determining the distribution .As will be appreciated , this occurs because the alignments are constructed , and not directly observed .In such cases , it may be appropriate to discard the few suspect instances and force the distribution to have mean 0 .", "label": "", "metadata": {}, "score": "56.319298"}
{"text": "Thus , for example , prototype vector signals P5 , P1 , P2 , P4 , and P3 could have been assigned rank scores of \" 1 \" , \" 2 \" , \" 3 \" , \" 3 \" and \" 3 \" , respectively .", "label": "", "metadata": {}, "score": "56.43646"}
{"text": "A context definition mode may be initiated automatically when alternative text segments are located ( e.g. , during hypothesis testing ) that may be expected to be especially useful for indicating context as stop segments .Alternatively or in addition , a context definition mode may be initiated manually ( i.e. , by the user ) .", "label": "", "metadata": {}, "score": "56.437286"}
{"text": "No .5,903,668 , issued May 11 , 1999 to Beernink , incorporated by reference above .A message recognizer may include a visual handwriting recognizer for generating text data from scanned handwriting , which may be of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "56.47697"}
{"text": "The closer such a negative biasing term is to zero , the more it will reduce the product of biasing terms , and the more negative a bias it will cause in the second exemplary semantic model .In the second exemplary semantic model , the product may include one negative biasing term for each identified stop segment .", "label": "", "metadata": {}, "score": "56.479397"}
{"text": "For example , receiver 160 of tablet 150 includes a conventional receiver / demodulator for converting an analog signal transmitted by transmitter 122 and picked up by a suitable sensing device ( e.g. , an antenna , not shown ) .In a variation , a communications link may be of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "56.48811"}
{"text": "Referring to .FIGS .5 and 6 , the pre - filtering procedure 240 uses a lexical tree 500 that is initialized before processing begins .The lexical tree represents the active vocabulary 230 based on the phonetic relationships between words in the vocabulary .", "label": "", "metadata": {}, "score": "56.546646"}
{"text": "The one or more excluded words would not be included in calculations such as in the duration of the entire response , T s , the number of words in a response , n , or the calculation of other features 518 .[", "label": "", "metadata": {}, "score": "56.560986"}
{"text": "This is shown on a connected word recognition problem , the notoriously difficult letter spelling task .With speech - reading we could reduce the error rate up to half of the error rate of the pure acoustic recognition .I. INTRODUCTION Recent development in the design of human computer interfaces ( HCI ) requests a new field of research : MultiModal Recognition .", "label": "", "metadata": {}, "score": "56.63035"}
{"text": "ELAST .s .I . t .J .d .K . ) k .K .EL . target .d . k . ) source .d . k . ) where EL may be a function of the probabilities of gaps in the target and source fragments for a bi - fragment .", "label": "", "metadata": {}, "score": "56.637566"}
{"text": "More specifically , an interleaver can be used to distribute burst errors over multiple RS blocks and thereby increase chances that all errors are corrected .FIG .6 shows the data stream produced by the transmitter processor 22 .The non - limiting data format shown in .", "label": "", "metadata": {}, "score": "56.663376"}
{"text": "FIG .3 ): .Buffer consumption : one word from a buffered suffix ( step S 112 E ) and appending it to the end of the sequence ( step S 112 G ) .The process ends when both the coverage of the source is complete and the buffer is empty .", "label": "", "metadata": {}, "score": "56.699448"}
{"text": "An utterance includes a variable number of frames and corresponds , for example , to a period of speech followed by a pause of at least a predetermined duration .The processor determines what the user said by finding acoustic models that best match the digital frames of an utterance , and by identifying text that corresponds to those acoustic models .", "label": "", "metadata": {}, "score": "56.71353"}
{"text": "Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones .The EBM approach provides a common theoretical framework for many learning models , including traditional discriminative and generative approaches , as well as graph - transformer networks , conditional random fields , maximum margin Markov networks , and several manifold learning methods .", "label": "", "metadata": {}, "score": "56.77768"}
{"text": "These lattices were expanded expanded to contain language model probabilities generated by the interpolation of the word 4-gram and the class trigram .Subsequent passes rescored these lattices and operated in two branches : a branch using GI MMIE trained models ( branch ' ' a ' ' ) and a branch using GD , soft - tied , MLE models ( branch ' ' b ' ' ) .", "label": "", "metadata": {}, "score": "56.831837"}
{"text": "ICASSP'98 , pp .177 - 180 , Seattle .S.J. Young , J.J. Odell & P.C. Woodland ( 1994 ) .Tree - Based State Tying for High Accuracy Acoustic Modelling .[ ps ] [ pdf ] Proc .1994 ARPA Human Language Technology Workshop , pp .", "label": "", "metadata": {}, "score": "56.85821"}
{"text": "Portions of the echoes return to transducer 730 where they are sensed over an interval to form a time - varying signal that is coupled to transmitter / sensor 740 .Transmitter / sensor 740 converts the time - varying signal to a suitable form ( e.g. , Nyquist filtered discrete - time samples ) and conveys the converted signal to signal processor 745 .", "label": "", "metadata": {}, "score": "56.907017"}
{"text": "This value for the starting time indicates that the score at state 805 represents a word that started at a time corresponding to the first frame .The starting time moves with the score as the score propagates through the lexical tree .As noted above , each of the scores corresponds to a negative logarithmic probability .", "label": "", "metadata": {}, "score": "56.92065"}
{"text": "For example , comparator 640 performs subtraction and division operations to compute a residual portion SR of signal S 1 that is uniquely attributable to sensor 620 .Comparator 640 also performs comparisons of residual portion SR and signal S 2 to two predetermined thresholds T 1 and T 2 , respectively .", "label": "", "metadata": {}, "score": "56.933422"}
{"text": "Commands may include words , phrases , or sentences .The recognizer 215 processes the frames 210 of an utterance in view of one or more constraint grammars 225 .A constraint grammar , also referred to as a template or restriction rule , may be a limitation on the words that may correspond to an utterance , a limitation on the order or grammatical form of the words , or both .", "label": "", "metadata": {}, "score": "56.9382"}
{"text": "If context definition is to be initiated , a context definition mode is entered for user identification of one or more stop segments .In method 1300 , a context definition mode is activated at step 1340 .At step 1340 , alternative text segments are listed for possible identification as stop segments by the user .", "label": "", "metadata": {}, "score": "56.977554"}
{"text": "As each RS data block is decoded , the number of errors encountered may be monitored by a peak error detector if desired .Every 100 mS , the worst error count may be displayed on an LED bar graph and the peak error detector is reset to provide feedback to the user in adjusting the antenna for optimal operation .", "label": "", "metadata": {}, "score": "56.997074"}
{"text": "32 , No . 7 , Dec. 1989 , pp .320 - 321 .Brown , P. F. et al . \"Method and Apparatus For Translating A Series of Words From One Language to Another \" .U.S. patent application Ser .", "label": "", "metadata": {}, "score": "57.022728"}
{"text": "s .I . t .J .d .K . ) k .K .EL . target .d . k . ) source .d . k . ) where EL represents a logarithm of a probability of observing a certain configuration of gap sizes in the target and source fragments of a bi - fragment .", "label": "", "metadata": {}, "score": "57.127457"}
{"text": "[0008 ] As a further example , a computer - readable memory comprising computer - readable instructions that when executed cause a processing system to perform steps that include generating word by performing speech recognition on digitized speech using a non - native acoustic model trained with non - native speech .", "label": "", "metadata": {}, "score": "57.213684"}
{"text": "A user may specify text and initiate a correction mode by communicating any appropriate editing command ( e.g. , with freehand input , voice input , etc . ) .As alternatively specified in TABLE II , a user may also specify a selection of text by drawing a closed curve around a block of displayed text ( e.g. , a phrase of several words ) , then immediately tapping the stylus somewhere within the curve .", "label": "", "metadata": {}, "score": "57.233765"}
{"text": "Each count is tabulated in a respective element Z i , k of matrix Z , starting with element Z 0,0 .After each occurrence of step 1120 , a determination is made at decision step 1130 as to whether or text segments of the corpus are still to be counted in the present cluster .", "label": "", "metadata": {}, "score": "57.29922"}
{"text": "In another computing model , text data may be considered to be generated by an editor that is responsive to external sources of text input .For example , text data may be stored in a local memory segment that is controlled by an editor .", "label": "", "metadata": {}, "score": "57.350803"}
{"text": "claim 1 , wherein bi - fragments comprising contiguous text fragments are modeled as elastic bi - phrases .The method of .claim 1 , wherein the generation of a target hypothesis includes generating a partial hypothesis comprising a sequence of words without gaps from the text fragments selected from the second language and storing any unconsumed words of the text fragments used in generating the sequence in a buffer .", "label": "", "metadata": {}, "score": "57.41943"}
{"text": "The output of the respective branches served as the adaptation supervision to stage P5a / P5b .These were as P4a / P4b but were based on quinphone acoustic models .Finally for the MMIE branch only , a pass with two MLLR transforms was run ( P6a ) .", "label": "", "metadata": {}, "score": "57.49714"}
{"text": "We also obtained an unusually large improvement from modeling crossword pronunciation variants in \" multiword \" vocabulary items .The language model ( LM ) was enhanced with an \" anti - LM \" representing acoustically confusable word sequences .Finally , we applied a generalized ROVER algorithm to combine the N - best hypotheses from several systems based on different acoustic models .", "label": "", "metadata": {}, "score": "57.502552"}
{"text": "[0004 ]A construct is a set of knowledge , skills , and abilities measured by a test .The construct of a speaking test may be embodied in the rubrics that human raters use to score the test .For example , the construct of communicative competence may consist of three categories : delivery , language use , and topic development .", "label": "", "metadata": {}, "score": "57.503777"}
{"text": "reord .s .I . t .J .d .K . ) k .K .i .i .d . k . )I .V .i . k . )Beam Search Procedure .Beam - search procedures are described , for example , in Cancedda , et al . , incorporated herein by reference .", "label": "", "metadata": {}, "score": "57.514454"}
{"text": "The resulting count \" n \" is transmitted to the receiver as part of the header 's variable data \" n \" that is used in the receiver to regenerate the video clock in accordance with the above - incorporated applications divulging PLL - related inventions .", "label": "", "metadata": {}, "score": "57.578568"}
{"text": "3 , exemplary domain controller 220 implements functional blocks including : message recognizer 310 ; stylus interface 320 ; and domain interface 330 , which is coupled to message recognizer 310 as well as to a transfer buffer 340 .As discussed above , interconnection between functional blocks may include hardware connections ( e.g. , analog connections , digital signals , etc . )", "label": "", "metadata": {}, "score": "57.618874"}
{"text": "The one or more data stores 1208 may contain computer program code for implementing a non - native trained speech recognizer 1210 as well as computer readable program code for implementing a native trained acoustic model 1212 .[ 0050 ]FIG .12B depicts a system 1220 that includes a client server architecture .", "label": "", "metadata": {}, "score": "57.746525"}
{"text": "The large vocabulary dictation grammar also indicates the frequency with which words occur .A language model associated with the large vocabulary dictation grammar may be a unigram model that indicates the frequency with which a word occurs independently of context , or a bigram model that indicates the frequency with which a word occurs in the context of a preceding word .", "label": "", "metadata": {}, "score": "57.76938"}
{"text": "The recognizer may eliminate any hypothesis that is associated with a constraint grammar ( for example , a command hypothesis ) , but does not comply with the constraint grammar .Referring to .FIG .12 , the recognizer 215 operates according to a procedure 1200 .", "label": "", "metadata": {}, "score": "57.790306"}
{"text": "The noise cancellation processor 52 adapts to changing noise levels by periodically updating the noise vector N(t ) whenever the prior feature vector F(t-1 ) is identified as noise or silence .The noise vector N(t ) is updated according to the formula .", "label": "", "metadata": {}, "score": "57.80224"}
{"text": "6 .The confidence scores may be influenced by a variety of factors such as the quality of the speech , the accent of a speaker , noise levels , as well as others .[ 0030 ] The digitized speech 502 and the word hypotheses and other metrics 506 identified by the speech recognition 504 are provided for time alignment 508 .", "label": "", "metadata": {}, "score": "57.844025"}
{"text": "5 is a block diagram of an exemplary receive processor back end ; and .FIG .6 is a schematic diagram of a data stream .DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT .Referring initially to .FIG .1 , a system is shown , generally designated 10 , which includes a source 12 of baseband multimedia data , and in particular high definition ( HD ) digital video with audio .", "label": "", "metadata": {}, "score": "57.847183"}
{"text": "Heuristics .In adopting the h ELAST feature 34 discussed above , only a few adaptations to the admissible heuristics described in Cancedda , et al .need be made .One adaptation may include lower - bounding the value of h ELAST based on a consideration of the buffer .", "label": "", "metadata": {}, "score": "57.967743"}
{"text": "In another embodiment , two h ELAST features 34 A , 34 B may be utilized , one for the target side , and the other for the source side .Considering only a target side h ELAST feature may not be beneficial as it would not penalize \" spurious \" versions of source side phrases with unlikely gap sizes .", "label": "", "metadata": {}, "score": "57.997612"}
{"text": "1 .FIG .3 is a flow chart of a signal processing procedure performed by the software of .FIG .2 .FIGS .4A and 4B are state diagrams of a constraint grammar .FIG .5 is a graph of a lexical tree .", "label": "", "metadata": {}, "score": "58.011337"}
{"text": "[0037 ]Given the phone likelihoods from the acoustic model 614 , the speech recognition engine 604 is able to utilize the dictionary 618 to identify a most likely word candidate for a portion of digitized input speech 606 .An initial confidence score may be applied based on the magnitude of the phone likelihoods and the quality of the match between the identified phones and a dictionary engine .", "label": "", "metadata": {}, "score": "58.015305"}
{"text": "Each translation match score comprises an estimate of the probability of occurrence of the speech hypothesis given the occurrence of the source text .A hypothesis score generator produces a hypothesis score for each hypothesis .Each hypothesis score comprises a combination of the acoustic match score and the translation match score for the hypothesis .", "label": "", "metadata": {}, "score": "58.045517"}
{"text": "Each of the recognition candidates includes the correctly - recognized words that preceded \" Brown \" ( \" those who \" ) and the words that followed \" Brown \" ( \" to meet this \" ) .As shown in .FIG .", "label": "", "metadata": {}, "score": "58.060593"}
{"text": "U.S. Published Application No . 2004/0024581 by Koehn , et al . , entitled \" STATISTICAL MACHINE TRANSLATION , \" discloses a phrase - based statistical machine translation method using syntactic markers for contiguous phrases .U.S. Published Application No .2004/0030551 by Marcu , et al . , entitled \" PHRASE TO PHRASE JOINT PROBABILITY MODEL FOR STATISTICAL MACHINE TRANSLATION \" discloses a phrase to phrase joint probability model for statistical machine translation .", "label": "", "metadata": {}, "score": "58.076374"}
{"text": "Dedicated circuitry for identification circuit 128 is not necessary in such a configuration .A microcontroller - based identification circuit may be programmed after device fabrication as desired , possibly many times .Conventional interface circuitry ( e.g. , magnetic sensor and cooperating receiver , miniature 3-pin connector and serial port ) may be provided to facilitate such programming .", "label": "", "metadata": {}, "score": "58.086933"}
{"text": "For example , when the speech recognition software is being used in conjunction with a particular application ( for example , Microsoft Word ) , the control / interface module updates the active vocabulary to include command words associated with that application and activates constraint grammars associated with the application .", "label": "", "metadata": {}, "score": "58.088005"}
{"text": "As will be appreciated , at step S 112 A , the library 20 may identify an elastic bi - fragment which is retrieved by the processing component and which may ultimately used in translation of the text string .The library may also include contiguous bi - fragments ( fragments with a gap size of 0 ) , one or more of which may be utilized in combination with one or more elastic bi - fragments in translation of the source language text string .", "label": "", "metadata": {}, "score": "58.088753"}
{"text": "The proposed error corrective language model adaptation approach exploits domain - specific language variations and reco ... \" .We present a new language model adaptation framework integrated with error handling method to improve accuracy of speech recognition and performance of spoken language applications .", "label": "", "metadata": {}, "score": "58.091946"}
{"text": "Operation of the pre - filtering procedure 240 is illustrated in .FIG .7 .The pre - filtering procedure begins by retrieving the next frame of parameters for an utterance ( step 700 ) .Immediately after initialization , the next frame will be the first frame for the utterance .", "label": "", "metadata": {}, "score": "58.11751"}
{"text": "A distance profile is a characterization of the respective distances between an ultrasonic transducer and a scattering object that reflects echoes back to the transducer .For example , a distance profile may be a mathematical curve characterizing a sequential series of discrete - time samples .", "label": "", "metadata": {}, "score": "58.180958"}
{"text": "7A and 7B ) .In variations where the benefit of styluses having microphones is not required , conventional styluses without microphones may be used .A networked text processing system according to various aspects of the present invention may include a node to a wide area network ( e.g. , the Internet ) .", "label": "", "metadata": {}, "score": "58.22741"}
{"text": "For example , a semantic cluster of this type may include all words found within a given number ( e.g. , 50 ) of a particular reference word in a training corpus .Such a semantic cluster may be given indefinite boundaries by applying words further from the reference word may given a lower semantic association than text close to the reference point .", "label": "", "metadata": {}, "score": "58.238914"}
{"text": "The method of .The method of .claim 1 , wherein the generating of a target hypothesis includes performing a multiple stack beam - search .The method of .claim 1 , wherein the generating of at least one target hypothesis includes generation of a plurality of partial translation hypotheses , scoring the partial hypotheses , and optionally pruning partial translation hypotheses according to their scores .", "label": "", "metadata": {}, "score": "58.24183"}
{"text": "Bahl , L. R. et al . \" Apparatus and Method For Grouping Utterances of a Phoneme Into Context - Dependent Categories Based on Sound - Similarity For Automatic Speech Recognition . \" U.S. patent application Ser .No .468,546 , filed Jan. 23 , 1990 .", "label": "", "metadata": {}, "score": "58.309277"}
{"text": "The score represents the likelihood that a series of frames has placed the lexical tree in the state ( that is , the probability that the series of frames corresponds to the word or portion of a word to which the state corresponds ) .", "label": "", "metadata": {}, "score": "58.37337"}
{"text": "Other features , objects , and advantages of the invention will be apparent from the description and drawings , and from the claims .DESCRIPTION OF DRAWINGS .FIG .1 is a block diagram of a speech recognition system .FIG .2 is a block diagram of speech recognition software of the system of .", "label": "", "metadata": {}, "score": "58.45199"}
{"text": "3 is a flow diagram of a method for machine translation of text which utilizes elastic bi - fragments according to one aspect of the exemplary embodiment ; .FIG .4 is an example of the combination of bi - fragments to produce a translation ; .", "label": "", "metadata": {}, "score": "58.50809"}
{"text": "5 illustrates the development of hypothesis stacks for a partial translation of the sentence illustrated in .FIG .4 . DETAILED DESCRIPTION .Aspects of the exemplary embodiment relate to a method for translating source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "58.564995"}
{"text": "The system of claim 12 , wherein the steps further comprise excluding words not reliably recognized in generating the word hypotheses from contributing to the assessment score .The system of claim 13 , wherein a confidence level is associated with each word in the word hypotheses identifying a likelihood that a word in a word hypothesis was correctly recognized in generating the word hypotheses .", "label": "", "metadata": {}, "score": "58.579075"}
{"text": "The method of .claim 1 , wherein the evaluation of the at least one hypothesis includes selecting a hypothesis from a plurality of hypotheses which maximizes a log - linear scoring function in which weights are assigned to each of the feature functions .", "label": "", "metadata": {}, "score": "58.594868"}
{"text": "The system of . claim 13 wherein the memory further comprises software instructions for extracting the word and the phrase from the recognition result .The system of . claim 13 wherein the software instruction for determining if the word is similar to a portion of the phrase comprises a software instruction for determining if the word matches a substring of the phrase .", "label": "", "metadata": {}, "score": "58.621338"}
{"text": "This paper has discussed the substantial improvements in system performance that have been made to our Hub5 transcription system since the 1998 evaluation .The largest improvement stems from MMIE HMM training , however the MLE model set in their current configuration were shown to still work well .", "label": "", "metadata": {}, "score": "58.6258"}
{"text": "Despite their state - of - the - art performance , HMMs make a number of suboptimal modeling assumptions that limit their potential effectiveness .Neural networks avoid many of these assumptions , while they can also learn complex functions , generalize effectively , tolerate noise , and support parallelism .", "label": "", "metadata": {}, "score": "58.667435"}
{"text": "The system may also be considered to be implemented as a computer - readable storage medium , configured with a computer program , where the storage medium so configured causes a computer to operate in a specific and predefined manner .The user may be required to pause before and after the word - in - phrase command , but not necessarily within the word - in - phrase command .", "label": "", "metadata": {}, "score": "58.69542"}
{"text": "The method of .claim 1 wherein determining if the word is similar to a portion of the phrase comprises determining if the word matches a substring of the phrase .The method of . claim 5 wherein producing the speech recognition result comprises producing the word .", "label": "", "metadata": {}, "score": "58.723915"}
{"text": "For example , text data may be transferred between tablets .A document may be displayed in multiple views on displays of multiple tablets .A stylus including a microphone and identification circuit according to various aspects of the present invention may be used to generate text data in accordance with a message model associated with a particular owner of the stylus .", "label": "", "metadata": {}, "score": "58.75207"}
{"text": "A speech recognition method as claimed in claim 35 , characterized in that the means for measuring the value of at least one feature of in utterance comprises a microphone .Description .BACKGROUND OF THE INVENTION .The invention relates to automatic speech recognition .", "label": "", "metadata": {}, "score": "58.776535"}
{"text": "This limitation to scripted texts is largely based on the difficulty in accurately recognizing spontaneous , non - native speech using a computer .[ 0022 ] The non - native speech assessment engine 104 enables the assessment of delivery aspects and other aspects of a non - native speaker 's spontaneous speech .", "label": "", "metadata": {}, "score": "58.783264"}
{"text": "For example , a stop segment that has been identified as being outside a present context may be \" downgraded with age \" with the passage of time following the identification .As another example , the severity of negative bias may be downgraded ( i.e. , less negative bias ) with the generation of additional text following the identification .", "label": "", "metadata": {}, "score": "58.873833"}
{"text": "A hypothesis score for each hypothesis comprises a combination of the acoustic match score and the translation match score .At least one word of one or more speech hypot This invention was made with Government support under Contract Number N00014 - 91-C-0135 awarded by the office of Naval Research .", "label": "", "metadata": {}, "score": "58.91111"}
{"text": "Continuous speech recognition technology has recently matured to the point where it has become feasible to develop spoken dialogue interfaces to computer systems that help people perform tasks in simple domains .Word recognition accuracy for spontaneous dialogue , however , is still a long way from perfect , and any system that uses spoken natural language input hastohave somemechanism for dealing with speech recognition errors .", "label": "", "metadata": {}, "score": "58.919632"}
{"text": "469 - 477 of book chapter entitled \" Touch , Gesture , and Marking , \" in Human - Computer Interaction : Toward the Year 2000 , 2nd Edition , 1995 , Morgan Kaufmann Publishers , Inc. , San Francisco , CA .Tools . by George Ferguson , James F. Allen - In Proceedings of the Fifteenth National Conference on Artificial Intelligence ( AAAI-98 , 1998 . \" ...", "label": "", "metadata": {}, "score": "58.926994"}
{"text": "For example , the invention may be used to recognize an utterance in English of a translation of a sentence in French .In one study , it was found that the efficiency of a human translator who dictates a translation in one language corresponding to source text in another language , is greater than the efficiency of a human translator who writes or types a translation .", "label": "", "metadata": {}, "score": "58.93242"}
{"text": "In a variation , an ultrasonic proximity sensor may be suitably adapted from the disclosure of U.S. Pat .No .5,848,802 issued Dec. 15 , 1998 to Breed et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .", "label": "", "metadata": {}, "score": "59.04226"}
{"text": "In determining whether to add a feature , consideration should be given to balancing the modeling precision gained by adding new features with the increase in the size of the evaluation set on which the weights of the features are trained .In current log - linear ( maxent )", "label": "", "metadata": {}, "score": "59.0848"}
{"text": "As discussed above , such undesired audio input may be expected to be especially problematic in collaborative environments where pen - based computers are often used .The ' 172 patent discloses a manual activation system having a designated hot region on a video display for activating the speech recognizer .", "label": "", "metadata": {}, "score": "59.08615"}
{"text": "When a wireless connection is utilized , a transmitter may include appropriate conversion , modulation , and/or transmission circuitry .For example , transmitter 122 in stylus 110 includes a conventional analog - to - digital converter for converting voice input into digital samples , and a conventional modulator / transmitter for converting the digital samples into an analog signal having a desired modulation format ( e.g. , FSK , QPSK ) .", "label": "", "metadata": {}, "score": "59.08634"}
{"text": "Text segments that have been generated previously in a document ( or by a user ) may be excluded from consideration as stop segments , since it may be presumed that the user has previously considered such text segments to be in the present ( or user ) context .", "label": "", "metadata": {}, "score": "59.08876"}
{"text": "This is a complex task , because the number of possible translations typically grows exponentially with the size of the input , and so not all solutions can be examined in practice .INCORPORATION BY REFERENCE .The following references , the disclosures of which are incorporated herein in their entireties by reference , are mentioned : .", "label": "", "metadata": {}, "score": "59.127182"}
{"text": "Language use refers to the range , complexity , and precision of vocabulary and grammar use .Topic development refers to the coherence and fullness of the response .[ 0005 ] The delivery aspect may be measured on four dimensions : fluency , intonation , rhythm , and pronunciation .", "label": "", "metadata": {}, "score": "59.142284"}
{"text": "A message is any communication in writing , in speech , or by signals .A message model may include a language model as well as an acoustic and/or handwriting model .A stylus having an identification circuit may be associated with a user by a suitable registration system .", "label": "", "metadata": {}, "score": "59.15486"}
{"text": "156 - 163 , Philadelphia , 2002 .S.Kumar , W.Byrne , A Weighted Finite State Transducer Implementation of the Alignment Template Model for Statistical Machine Translation , In HLT - NAACL 2003 , Main Papers , pp .63 - 70 , Edmonton , Canada , May - Jun .", "label": "", "metadata": {}, "score": "59.169174"}
{"text": "The speech recognition system also may require to be a single word ( that may be one or more characters long ) .The length , which corresponds to the number of characters in the word , may be constrained to be shorter than the length of .", "label": "", "metadata": {}, "score": "59.22599"}
{"text": "( Acoustic model 356 is part of unified message model 316 . )Speech recognizer 312 then derives units of language here , strings of text data ) from the units of speech in accordance with language model 350 .Speech recognizer 312 provides the derived units of language as text data to domain interface 330 .", "label": "", "metadata": {}, "score": "59.24155"}
{"text": "First P1 ( GI non - VTLN MLE triphones , trigram LM , 27k dictionary ) , generated an initial transcription .This P1 pass is identical to the 1998 P1 setup [ 6 ] .The P1 output was used solely for VTLN warp - factor generation and assignment of a gender label for each test conversation side .", "label": "", "metadata": {}, "score": "59.33194"}
{"text": "0017 ]FIG .9 is a block diagram depicting the generation of an assessment score based on calculated features .[ 0018 ] FIG .10 is a flow diagram depicting an example computer - implemented generation of a speech score .[ 0019 ]", "label": "", "metadata": {}, "score": "59.35439"}
{"text": "A user may initiate a context definition mode manually .The context definition mode may be initiated when the user initiates a correction mode to correct a misrecognition by the message recognizer .Alternatively , a user may communicate an appropriate editing command to indicate , without initiating a correction process , that a text segment provided by a message recognizer is out of either the present context or the user context .", "label": "", "metadata": {}, "score": "59.36686"}
{"text": "The HMMs were trained on 180 hours of Hub5 training data .The system used a 27k vocabulary that covered all words in the acoustic training data .The core of the pronunciation dictionary was based on the 1993 LIMSI WSJ lexicon , but used a large number of additions along with various changes .", "label": "", "metadata": {}, "score": "59.422916"}
{"text": "This coding scheme enables the receiver to correct up to eight errors in each RS block of 216 words .As understood herein , forward error correction such as Reed - Solomon is advantageous to correct occasional transmission errors that can be present in wireless transmission systems , which if left uncorrected could temporarily disrupt the displayed image or produce video artifacts .", "label": "", "metadata": {}, "score": "59.424347"}
{"text": "Such a curve may be characterized by a width of the most prominent peak in the curve and a \" center of mass \" of the curve T cm , which may be calculated as follows : .T .c . m .", "label": "", "metadata": {}, "score": "59.494545"}
{"text": "User training may include adaptation of an acoustic model in the message model .Advantageously , the acoustic model may be adapted over time using a single stylus - mounted microphone .Use of the same stylus for user training provides acoustic consistency , even when the stylus is used with many different message recognition systems .", "label": "", "metadata": {}, "score": "59.513435"}
{"text": "Descriptions of the approach to robust language understanding can be found in ( Allen et al .1996 ) .The approach to planning , which is interesting for its emphasis on plan modification and i .. by James F. Allen , Bradford W. Miller , Eric K. Ringger , Teresa Sikorski - In 34th Meeting of the Association for Computational Linguistics , 1995 . \" ...", "label": "", "metadata": {}, "score": "59.5239"}
{"text": "( 2 ) e 1 and e 2 are both still in the buffer .In this case , the mode ( maximum ) of the distribution for the corresponding gap is known a priori .The mode can be used as the basis for the optimistic evaluation of the contribution of the gap to the h ELAST feature . log .", "label": "", "metadata": {}, "score": "59.59996"}
{"text": "10 ; .FIG .13 is a flow diagram of a method for adapting a semantic model by user identification of context according to various aspects of the present invention ; .FIG .14 is a flow diagram of a method for adapting a syntactic model by user correction of misrecognition according to various aspects of the present invention ; and .", "label": "", "metadata": {}, "score": "59.614147"}
{"text": "In a typical example , a topic may include approximately 30,000 words that are considered for normal recognition .A complete dictation vocabulary consists of the active vocabulary 230 plus a backup vocabulary 245 .The backup vocabulary may include files that contain user - specific backup vocabulary words and system - wide backup vocabulary words .", "label": "", "metadata": {}, "score": "59.692616"}
{"text": "Absent express definitions herein , claim terms are to be given all ordinary and accustomed meanings that are not irreconcilable with the present specification and file history .Abstract : .This paper describes the Cambridge University HTK ( CU - HTK ) system developed for the NIST March 2000 evaluation of English conversational telephone speech transcription ( Hub5E ) .", "label": "", "metadata": {}, "score": "59.69509"}
{"text": "Initialization of a model entails creation of a new model or modification of a model from a previous configuration to a new configuration .A local model may be initialized by any suitable hardware and/or software technique .For example , a local model may be initialized by programming of blank memory space with digital indicia of a user model that the local model is to conform with .", "label": "", "metadata": {}, "score": "59.718334"}
{"text": "\u00c9.Gaussier , Flow Network Models for Word Alignment and Terminology Extraction from Bilingual Corpora , In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th Intl Conference on Computational Linguistics , pp .444 - 450 , San Francisco , CA , 1998 .", "label": "", "metadata": {}, "score": "59.755333"}
{"text": "The method of . claim 1 further comprising determining if previously - recognized text has been selected .The method of . claim 10 further comprising replacing selected text with the produced speech recognition result if text has been selected .The method of .", "label": "", "metadata": {}, "score": "59.75811"}
{"text": "M . m .h .m .s .I . t .J .d .K . )I is a normalization constant and where at least one of the feature functions h m is selected from : .( 1 ) a \" bi - fragment \" feature h bf , of the general form : . h . bf .", "label": "", "metadata": {}, "score": "59.760242"}
{"text": "A first view of the document may be provided on a first display ( e.g. , of a first tablet ) , while a second view of the document may be provided on a second display ( e.g. , of a second tablet ) .", "label": "", "metadata": {}, "score": "59.830956"}
{"text": "gaps .s .I . t .J .d .K . )i .I .gapCount .b .d .i . ) where gapCount(b ) counts the number of gaps in the source and target fragments of b. .", "label": "", "metadata": {}, "score": "59.86196"}
{"text": "This is probably because while MLE training gives equal weight to all training utterances , MMIE training effectively gives greater weight to those training set utterances with low sentence posterior probabilities for the correct utterance .MMIE was also used to train quinphone HMMs .", "label": "", "metadata": {}, "score": "59.880783"}
{"text": "The speech recognition engine 704 utilizes an acoustic model 716 trained with native - quality training speech 718 as well as a dictionary 720 , which the speech recognition engine 704 uses in generating the pronunciation measurements 712 .For example , the speech recognition engine 704 can calculate likelihood estimations of all hypothesized words and vowel durations .", "label": "", "metadata": {}, "score": "59.894814"}
{"text": "For example , FIG .12A depicts an exemplary system 1200 that includes a stand alone computer architecture where a processing system 1202 ( e.g. , one or more computer processors ) includes a non - native speech assessment engine 1204 being executed on it .", "label": "", "metadata": {}, "score": "59.930405"}
{"text": "An exemplary correction process 1400 may be better understood with reference to .FIG .14 .Process 1400 begins at step 1410 with a user specifying , typically by freehand input , a selection of text that has been generated ( erroneously ) by a message recognizer .", "label": "", "metadata": {}, "score": "60.00238"}
{"text": "FIG .6 corresponds to the words \" heal \" and \" heel \" while also corresponding to the first three phonemes of the words \" heals \" , \" heels \" and \" healing \" .Node 620 also illustrates that , since multiple words may have the same phonetic spelling , a leaf node may correspond to more than one word .", "label": "", "metadata": {}, "score": "60.034405"}
{"text": "Members of a collaborative group may share a common pool of stop segments .A stop segment designated by any member of the group as being outside his or her user context is then considered to be outside the user context of each group member .", "label": "", "metadata": {}, "score": "60.154808"}
{"text": "For this work , two methods were investigated : the use of acoustic scaling and a weakened language model .Normally the language model probability and the acoustic model likelihoods are combined by scaling the language model log probabilities .This situation leads to a very large dynamic range in the combined likelihoods and a very sharp posterior distribution in the denominator of ( 1 ) .", "label": "", "metadata": {}, "score": "60.1713"}
{"text": "No .5,477,451 by Brown , et al .entitled \" METHOD AND SYSTEM FOR NATURAL LANGUAGE TRANSLATION , \" and U.S. Pat .No .6,304,841 by Berger , et al . , entitled \" AUTOMATIC CONSTRUCTION OF CONDITIONAL EXPONENTIAL MODELS FROM ELEMENTARY FEATURES \" describe word - based statistical machine translation methods and systems for natural language translation .", "label": "", "metadata": {}, "score": "60.2788"}
{"text": "It should be emphasised that the MMIE models were all gender independent while the MLE VTLN models were all gender dependent using soft - tying .All the acoustic models used Call Home weighting .9.2 Word List & Language Models .The word list was taken from two sources : the 1998 27k word list [ 6 ] and the most frequent 50,000 words occurring in the 204 million words of broadcast news ( BN ) LM training data .", "label": "", "metadata": {}, "score": "60.287865"}
{"text": "FIG .4A illustrates an example of a constraint grammar for a \" select \" command used to select previously recognized text .As shown , a constraint grammar may be illustrated as a state diagram 400 .The \" select \" command includes the word \" select \" followed by one or more previously - recognized words , with the words being in the order in which they were previously recognized .", "label": "", "metadata": {}, "score": "60.317974"}
{"text": "0036 ] The language model 620 may be trained to identify likelihoods of words being spoken based on corpuses of non - native training texts and other documents , such as books , newspapers , magazines , as well as others .For example , probabilities may be identified of observing a given word within a phrase of \" n \" words ( n - grams ) .", "label": "", "metadata": {}, "score": "60.359917"}
{"text": "As such , it ignores the source language sentence and the decomposition of the target into bi - phrases , to focus on the actual sequence of target - language words produced by the combination of bi - phrases : . h .u . s .", "label": "", "metadata": {}, "score": "60.362564"}
{"text": "The non - transitory computer - readable memory of claim 23 , wherein one or more of the features are utilized with related stress , intonation , vocabulary , or grammar to generate the assessment score indicating communicative competence or other construct of speaking proficiency that includes pronunciation proficiency .", "label": "", "metadata": {}, "score": "60.39243"}
{"text": "generate text data based at least in part on the voice input , the adjusted language model , and the acoustic model .The system of .claim 1 , wherein the system is further configured to : . identify information relating to a particular user ; . wherein the particular user message model is selected based at least in part on the information relating to the particular user ; and , . wherein the plurality of user message models includes two or more user message models corresponding to users that are different from the particular user .", "label": "", "metadata": {}, "score": "60.51001"}
{"text": "The one or more servers 1224 may access a computer readable memory 1230 as well as one or more data stores 1232 .The one or more data stores 1232 may contain a non - native trained speech recognizer 1234 as well as a native trained acoustic model 1236 .", "label": "", "metadata": {}, "score": "60.54406"}
{"text": "National Academy of the Sciences , 1966 . )In one approach to speech recognition , speech hypotheses are scored using two probability models .One model is a language model which estimates the probability that the speech hypothesis would be uttered , but which uses no knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "60.545883"}
{"text": "BRIEF DESCRIPTION .Aspects of the exemplary embodiment relate to a machine translation method and system for machine translation .In one aspect , a machine translation method is provided for translating source text from a first language to target text in a second language .", "label": "", "metadata": {}, "score": "60.55468"}
{"text": "Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy .Learning consists in ... \" .Energy - Based Models ( EBMs ) capture dependencies between variables by associating a scalar energy to each configuration of the variables .", "label": "", "metadata": {}, "score": "60.567146"}
{"text": "The Header Detector 74 also removes the special \" n \" value from the variable portion of the header and sends it to a video clock generator 77 for clock recovery in accordance with the above - incorporated applications directed to PLL inventions .", "label": "", "metadata": {}, "score": "60.584766"}
{"text": "The task of the language match score generator is to compute a score P(T ) for a finite sequence T of target words .The probability P(T ) of occurrence of the target word sequence may be approximated by the product of n - gram probabilities for all n - grams in each string .", "label": "", "metadata": {}, "score": "60.719837"}
{"text": "I . t .J .d .K . ) k .K . log .Pr . target .d . k . ) source .d . k . )The h bf feature estimates the log of the probability of observing the bi - phrase configuration ( s , t ) given the source phrase configuration s in the training corpus through the ratio of respective counts of these configurations .", "label": "", "metadata": {}, "score": "60.810226"}
{"text": "In addition to considering the pronunciation related features 518 , the scoring model may also consider other factors related to the received speech .For example , the scoring model may also consider metrics related to word stress , intonation , vocabulary , grammar , and other facets in computing an assessment score 516 .", "label": "", "metadata": {}, "score": "60.857117"}
{"text": "12C shows a block diagram of exemplary hardware for a stand alone computer architecture 1250 , such as the architecture depicted in FIG .12A , that may be used to contain and/or implement the program instructions of exemplary embodiments .A bus 1252 may serve as the information highway interconnecting the other illustrated components of the hardware .", "label": "", "metadata": {}, "score": "60.86782"}
{"text": "In addition to the standard computer - type components , the hardware may also include data input devices , such as a keyboard 1272 , or other input device 1274 , such as a microphone , remote control , pointer , mouse and/or joystick .", "label": "", "metadata": {}, "score": "60.885277"}
{"text": "1 is a block diagram showing the present system ; .FIG .2 is a block diagram of an exemplary transmit processor ; .FIG .3 is a block diagram of an exemplary transmit processor front end ; .FIG .4 is a block diagram of an exemplary receive processor ; .", "label": "", "metadata": {}, "score": "60.895264"}
{"text": "However , the Bulletin states that such a system is not intended for applications where speech and pen input are required simultaneously or in rapid alteration .See , for example , column 18 , lines 35 - 40 of EP 622724 .Speech data output from the microphone and received at the disclosed pen - based computer system can be converted into operational data and control information by a suitable voice recognition program in the system .", "label": "", "metadata": {}, "score": "60.90932"}
{"text": "A contrast ( not shown in the table ) showed that on P2 the use of MMIE models had given a 2.1 % absolute reduction in WER over the corresponding MLE models .The combination P4a+P6a denotes a system where only MMIE trained models have been used for decoding which yields a result 0.9 % absolute better than the corresponding MLE combination ( P5b+P4b ) .", "label": "", "metadata": {}, "score": "60.966682"}
{"text": "If the node is not the highest node , then the pre - filtering procedure goes to the next node having no unprocessed active subnodes ( step 720 ) and processes that node ( step 710 ) .When searching for the next node to process , the pre - filtering procedure considers inactive nodes having active subnodes or active siblings .", "label": "", "metadata": {}, "score": "60.966835"}
{"text": "A buffer 46 , illustrated herein as located in volatile memory 30 , stores unconsumed portion(s ) of a target phrase in a partial translation .FIG .3 illustrates steps in an exemplary method for translation of text comprising non - contiguous bi - fragments .", "label": "", "metadata": {}, "score": "61.07563"}
{"text": "The sequence 52 is a contiguous string of target words , and the buffer 46 comprises a set of target phrase suffixes 54 .Each target phrase suffix 54 constitutes the non - consumed portion of a target phrase for which the ongoing hypothesis has established that a gap exists between a first portion and a second portion of the target phrase .", "label": "", "metadata": {}, "score": "61.079163"}
{"text": "When the last state of the node was active prior to updating the scores for the node , the node - processing procedure uses the score for the last state to generate scores for any inactive subnodes of the node .If the generated score for a subnode does not exceed a pruning threshold , then the node - processing procedure activates that subnode and provides the subnode with the generated score .", "label": "", "metadata": {}, "score": "61.100464"}
{"text": "The encoder 54 evaluates each word pair as ten 2-bit entities , starting with the most significant bits .Each 2-bit value is compared to the previous 2-bit value .The difference may be represented using Gray code and output to I and Q stream Serializers 56 .", "label": "", "metadata": {}, "score": "61.102333"}
{"text": "If , for example , the parallel I data leads or lags the parallel Q data by one or more clocks , data is skewed and processing can not continue .To prevent this , the deserializer 72 performs the bonding operation by looking for a specific sequence of , e.g. , four words occurring in both the I and Q headers .", "label": "", "metadata": {}, "score": "61.190166"}
{"text": "The HDMI transmitter 18 employs HDMI protocols to process the multimedia data by , among other things , encrypting the data using High - Bandwidth Digital Content Protection ( HDCP ) and supporting TV resolutions such as 16\u00d79 display ratios to the multimedia data .", "label": "", "metadata": {}, "score": "61.24527"}
{"text": "This may be used to determine a cost function to a given hypothesis in a scoring model .In general , the cost decreases as the hypothesis moves toward a number of gaps which corresponds to the statistically determined most likely number of gaps .", "label": "", "metadata": {}, "score": "61.374245"}
{"text": "No .5,502,774 , issued Mar. 26 , 1996 to Bellegarda .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .This aforementioned patent cites a number of technical papers that may be reviewed for further guidance on the structure , training , and operation of conventional speech recognizers and handwriting recognizers .", "label": "", "metadata": {}, "score": "61.40464"}
{"text": "FIG .3 is a block diagram of a portion of an example of a speech recognition system according to the invention .In this embodiment of the invention , the system comprises a source vocabulary store 33 for storing the source language vocabulary .", "label": "", "metadata": {}, "score": "61.432285"}
{"text": "17 ] was then applied using quinphone models and confidence scores estimated using an N - best homogeneity measure for both the triphone and quinphone output .The final stage combined these two transcriptions using the ROVER program [ 2 ] .The system gave a 39.5 % word error rate on the September 1998 evaluation data .", "label": "", "metadata": {}, "score": "61.486843"}
{"text": "The normalization constants . lambda . sub.5 and . lambda . sub.6 are chosen so that the updated quantities are conditional probabilities .( See , U.S. patent application Ser .No .736,278 , filed on Jul. 25 , 1991 , by Peter F. Brown et al entitled \" Method and System For Natural Language Translation . \" )", "label": "", "metadata": {}, "score": "61.503086"}
{"text": "Digitized speech 502 is received and undergoes speech recognition 504 .In addition to generating word hypotheses 506 as to words identified in the digitized speech , the speech recognition 504 may provide other metrics with regard to the digitized speech .For example , the speech recognition 504 may , in addition to the word hypotheses , output timing metrics and confidence scores 506 related to the generated word hypotheses .", "label": "", "metadata": {}, "score": "61.53056"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "61.554718"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "61.554718"}
{"text": "At least some of the bi - fragments are modeled as elastic bi - fragments in which words of a fragment are spaced by a gap which is able to assume a variable size corresponding to a number of other words which are to occupy the gap .", "label": "", "metadata": {}, "score": "61.554718"}
{"text": "FIG .10 .Method 1000 begins with a suitable training corpus , which is partitioned into discrete semantic clusters at step 1010 .Discrete semantic clusters are semantic clusters of the second type discussed above .A commonly used corpus is the standard Wall Street Journal corpus .", "label": "", "metadata": {}, "score": "61.626053"}
{"text": "The comparison also may cause the score to be passed to a subsequent state through a path 825 .A score is passed to a subsequent state when the score , after being adjusted based on a comparison with a model for the subsequent state , is better than the score in the subsequent state , or when no score is associated with the subsequent state .", "label": "", "metadata": {}, "score": "61.629204"}
{"text": "5,794,189 , titled \" CONTINUOUS SPEECH RECOGNITION \" and issued Aug. 11 , 1998 , which is incorporated by reference .The constraint grammar also may be expressed in Backus - Naur Form ( BNF ) or Extended BNF ( EBNF ) .In EBNF , the grammar for the \" Select \" command is : .", "label": "", "metadata": {}, "score": "61.63387"}
{"text": "The 0 stack , by convention contains a null hypothesis .Stack 1 ( identified as 62 ) contains hypotheses in which the source projection is a string of one word , such as Je and danser .The beam search starts with an hypothesis in the first stack and adds a bi - phrase or consumes a buffered word using the principles outlined above .", "label": "", "metadata": {}, "score": "61.74308"}
{"text": "For illustrative purposes , several examples of the Chinese word - in - phrase command will now be discussed .Other embodiments are within the scope of the following claims .For example , the techniques described here are not limited to any particular hardware or software configuration ; they may find applicability in any computing or processing environment that may be used for speech recognition .", "label": "", "metadata": {}, "score": "61.795204"}
{"text": "09/616,157 filed on Jul. 14 , 2000 , now abandoned , which claims benefit of U.S. provisional application Ser .No .60/144,481 filed on Jul. 17 , 1999 .BACKGROUND OF THE INVENTION .Conventional computer workstations use a \" desktop \" interface to organize information on a single display screen .", "label": "", "metadata": {}, "score": "61.796356"}
{"text": "Prototype stores 54 and 56 may be electronic computer memory of the types discussed above .The prototype vectors in prototype store 38 may be obtained , for example , by clustering feature vector signals from a training set into a plurality of clusters , and then calculating the mean and standard deviation for each cluster to form the parameter values of the prototype vector .", "label": "", "metadata": {}, "score": "61.798466"}
{"text": "FIG .5 ) represents a sequence of states for a particular phoneme .For example , .FIG .8A illustrates a node 800 that includes a first state 805 , a second state 810 , and a third state 815 .A comparison with a frame of parameters may cause the score in a particular state to remain in the state ( through a path 820 ) .", "label": "", "metadata": {}, "score": "61.892223"}
{"text": "As shown in .The module 220 displays text 1305 ( \" When a justice needs a friend \" ) corresponding to a text portion of the utterance and implements the formatting command ( \" New - Paragraph \" ) included in the utterance .", "label": "", "metadata": {}, "score": "61.907665"}
{"text": "The method of claim 1 further comprising excluding words not reliably recognized in generating the word hypotheses from contributing to the assessment score .The method of claim 2 , wherein a confidence level is associated with each word in the word hypotheses identifying a likelihood that a word in a word hypothesis was correctly recognized in generating the word hypotheses .", "label": "", "metadata": {}, "score": "61.94082"}
{"text": "Depending on the type of identification circuit employed , a predetermined identification signal may be assigned during or after device fabrication .For example , identification circuit 128 is programmed to provide a 48-bit serial identification signal before being soldered to a printed circuit board along with other components of stylus 110 .", "label": "", "metadata": {}, "score": "61.95079"}
{"text": "Taking integrated systems to be ones that in - tegrate a variety of components in order to perform some task from start to finish , we believe that such systems ( a ) allow u ... \" .We discuss what constitutes an integrated system in AI , and why AI researchers should be interested in building and studying them .", "label": "", "metadata": {}, "score": "61.955235"}
{"text": "An identical PRN generator is used in the receiver to unscramble the data , and both PRN generators may be initialized every 20 uS. Data from the scrambler 50 is sent to a Header Generator 52 which periodically ( e.g. , every twenty microseconds ) outputs a header of , e.g. , forty words .", "label": "", "metadata": {}, "score": "61.96821"}
{"text": "claim 14 , wherein the translation scoring function outputs a translation for which the log - linear scoring model is maximized .A machine translation system for translating source text from a first language to target text in a second language , comprising : . memory which stores instructions for performing the method of .", "label": "", "metadata": {}, "score": "61.984867"}
{"text": "A network of tablets may be of the type disclosed in U.S. Pat .No .5,812,865 , issued Sep. 22 , 1998 to Theimer et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .", "label": "", "metadata": {}, "score": "62.0032"}
{"text": "The starting time identifies the hypothesized time at which the user began to speak the word or words represented by the state .In particular , the starting time identifies the time at which the score associated with the state entered the lexical tree ( that is , the time at which the score was passed from the state 840 along the path 850 ) .", "label": "", "metadata": {}, "score": "62.0157"}
{"text": "For example , text data may be numerical codes used in a computer to represent individual units ( e. & , characters , words , kanji ) of text .Voice input may be used primarily to generate text data , while freehand input may be used primarily to edit text data generated by voice input .", "label": "", "metadata": {}, "score": "62.037895"}
{"text": "The command includes a word and a phrase .The method includes determining if a word closely corresponds to a portion of the phrase .A speech recognition result is produced if the word closely corresponds to a portion of the phrase .The method of .", "label": "", "metadata": {}, "score": "62.07857"}
{"text": "As shown in [ 19 ] the gains from MLLR adaptation are as great for MMIE models as for MLE trained models .Hence the primary acoustic models used in the March 2000 CU - HTK evaluation system used gender - independent MMIE trained HMMs .", "label": "", "metadata": {}, "score": "62.20022"}
{"text": "An acoustic model also may represent a sound , or phoneme , that corresponds to a portion of a word .Collectively , the constituent phonemes for a word represent the phonetic spelling of the word .Acoustic models also may represent silence and various types of environmental noise .", "label": "", "metadata": {}, "score": "62.208363"}
{"text": "Receiver 160 may be configured to convey voice signal S V to speech recognizer 162 only when the activation bit is set to a true state .In a variation , speech recognizer 162 may be configured to generate text responsive to voice input only when the activation bit is in a true state .", "label": "", "metadata": {}, "score": "62.22712"}
{"text": "The method according to . claim 13 , further comprising switching from receiving audio input to receiving freehand input in response to detecting movement of the stylus towards a tablet .The method according to . claim 13 , further comprising accessing the selected user message model via a network .", "label": "", "metadata": {}, "score": "62.234756"}
{"text": "When deserializing data , the deserializer 72 determines where one word ends and the next begins within the serial data stream .This process is referred to as alignment .The deserializer 72 uses the first character of the header to perform this alignment operation in both the I and Q channels .", "label": "", "metadata": {}, "score": "62.297165"}
{"text": "Text may be \" stamped , \" for example , by displaying the text data in a particular color or with a hidden identifier that may be displayed upon user selection .A user message model is a message model that is specific to a particular user .", "label": "", "metadata": {}, "score": "62.37873"}
{"text": "Translating with Elastic Bi - Fragments .The translation of a source sentence f is produced by combining together bi - fragments so as to entirely cover the source sentence , and produce a well - formed target - language sentence , i.e. , a sequence without gaps .", "label": "", "metadata": {}, "score": "62.440083"}
{"text": "While the present invention has been described in terms of preferred embodiments and generally associated methods , it is contemplated that alterations and permutations thereof will become apparent to those skilled in the art upon a reading of the specification and study of the drawings .", "label": "", "metadata": {}, "score": "62.496017"}
{"text": "FIG .13B , the correction dialog box 1325 includes a \" Train \" button 1328 .When the user selects this button , the control / interface module responds by prompting the user through a training session to obtain one or more samples from the user of the word or words to be trained .", "label": "", "metadata": {}, "score": "62.522087"}
{"text": "The method of claim 1 , wherein the speech is spontaneous , non - native speech of a non - native language speaker .The method of claim 1 , wherein one or more of the features are utilized with related stress , intonation , vocabulary , or grammar to generate the assessment score indicating communicative competence or other construct of speaking proficiency that includes pronunciation proficiency .", "label": "", "metadata": {}, "score": "62.559605"}
{"text": "Many proper names are missing from even large vocabularies and yet are often translated directly from one language to another with no change in spelling .In one embodiment of the invention , the acoustic model generator 18 generates an acoustic model of a word by replacing each letter in the spelling of the word with an acoustic letter model , from an acoustic letter model store 35 , corresponding to the letter .", "label": "", "metadata": {}, "score": "62.599113"}
{"text": "During this initialization , digital indicia of user message model 262 is transferred to the local processing environment of message recognizer 310 via network connection 237 and digital signal S N .Message recognizer 310 may retrieve a selected user message model from a remote database via , for example , an Internet connection .", "label": "", "metadata": {}, "score": "62.765007"}
{"text": "We have found that the use of acoustic data weighting reduces the beneficial effect of soft - tying .There is a reduction in WER of 0.3 % absolute for triphones and 0.5 % for quinphones and a further 0.6 % absolute from using GD models .", "label": "", "metadata": {}, "score": "62.76629"}
{"text": "The system of claim 12 , wherein one or more of the features are utilized with related stress , intonation , vocabulary , or grammar to generate the assessment score indicating communicative competence or other construct of speaking proficiency that includes pronunciation proficiency .", "label": "", "metadata": {}, "score": "62.788506"}
{"text": "The translations may be produced by means of a beam - search decoder or other suitable optimizing function which takes into account the probabilities of various hypothetical translations being found in practice .The exemplary translation method translates natural language text , such as French to English , using as primary resource a collection of bi - fragments , i.e. , matching pairs of source - target fragments of text .", "label": "", "metadata": {}, "score": "62.827736"}
{"text": "Tools . by Yann Lecun , L\u00e9on Bottou , Yoshua Bengio , Patrick Haffner - Proceedings of the IEEE , 1998 . \" ...Multilayer neural networks trained with the back - propagation algorithm constitute the best example of a successful gradientbased learning technique .", "label": "", "metadata": {}, "score": "62.874954"}
{"text": "The FPGA converts the data into two data streams and includes a front end component multiplexing video data with control data .A complementary receive FPGA is also disclosed .... Method and system for processing wireless digital multimedia US 7953442 B2 .Abstract .", "label": "", "metadata": {}, "score": "62.893295"}
{"text": "Each header is associated with a unit of multimedia data from the scrambler .Furthermore , if desired a differential encoder can be used to represent absolute data from the header generator as phase shifted quadrature data .In preferred but non - limiting embodiments the transmit processing system is implemented by an FPGA configured for preparing the HDMI and/or DVI data for wireless transmission in the 60 GHz band .", "label": "", "metadata": {}, "score": "62.89598"}
{"text": "As the ' 106 patent points out , however , the N - gram paradigm does not contemplate word meaning , and limits on available processing and memory resources preclude the use of models in which N is made large enough to incorporate global language constraints .", "label": "", "metadata": {}, "score": "62.918488"}
{"text": "Then a programming mode may be entered by suitable user control ( e.g. , depressing a button on the body of stylus 710 ) at which point the distance profile is recorded or statistically characterized .The recorded or characterized distance profile then becomes the expected profile , which may be employed for subsequent operation of stylus 710 .", "label": "", "metadata": {}, "score": "62.992508"}
{"text": "A system for generating text responsive to voice and handwriting input , comprising : . a tablet surface , wherein the system is configured to receive freehand input based at least in part on manipulation of a stylus relative to the tablet surface ; and .", "label": "", "metadata": {}, "score": "62.99341"}
{"text": "33 , No . 2 , pp . 111 - 124 , 2005 .K.Papineni , et al . , Bleu : A Method for Automatic Evaluation of Machine Translation , Proc . of the 40th Annual Meeting of the Assn . for Computational Linguistics ( ACL ) , Philadelphia , pp . 311 - 318 , Jul. 2002 .", "label": "", "metadata": {}, "score": "63.00408"}
{"text": "In a variation , \" reversible paper \" may be overlaid on a tablet surface such that freehand input may be conveyed to the tablet surface through the \" paper . \"In such a variation , the stylus may include an erasable writing marking medium to provide visible indicia of such freehand input on the \" paper . \"", "label": "", "metadata": {}, "score": "63.004395"}
{"text": "At the desired proximity , infrared radiation from the head of user 240 occupies only that part of angular range AR 1 also occupied by range AR 2 .The residual portion of signal S 1 is thus relatively small , which causes the ratio between signal S 2 and the residual portion to reach the second predetermined threshold ( condition B ) .", "label": "", "metadata": {}, "score": "63.049126"}
{"text": "A scattering object having a distinct surface shape ( e.g. , a human face ) can be expected to provide a distinct distance profile curve .Comparator 750 compares the distance profile from signal processor 745 to an expected profile .The expected profile corresponds to acoustic field radiation expected to impinge on transducer 730 of stylus 710 when microphone 720 is at a predetermined proximity to the head of user 780 .", "label": "", "metadata": {}, "score": "63.06237"}
{"text": "An acoustic feature value measure 36 is provided for measuring the value of at least one feature of an utterance over each of a series of successive time intervals to produce a series of feature vector signals representing the feature values .Table 1 illustrates a hypothetical series of one - dimension feature vector signals corresponding to time intervals t1 , t2 , t3 , t4 , and t5 , respectively .", "label": "", "metadata": {}, "score": "63.07332"}
{"text": "9 is a diagram of relationships among various models of a message model according to various aspects of the present invention ; .FIG .10 is a flow diagram of a method for the identification of semantic clusters according to various aspects of the present invention ; .", "label": "", "metadata": {}, "score": "63.07418"}
{"text": "3 is a block diagram of a portion of another example of a speech recognition system according to the invention .FIG .4 is a block diagram of an example of an acoustic processor for a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "63.129868"}
{"text": "The use of quinphone models instead of triphone models gives a further gain of 0.9 % for both branches .Whereas the second adaptation stage with two speech transforms for the quinphone MMIE models brings 0.5 % , after obtaining CN output the difference is only 0.2 % .", "label": "", "metadata": {}, "score": "63.13994"}
{"text": "Editing may include user modification of text that has been correctly generated by processes 530 and 540 , but which the user desires to modify .Editing may further include user training , conveyed by freehand input , voice input , and/or character input .", "label": "", "metadata": {}, "score": "63.15171"}
{"text": "s .t . )i . k . log .Poisson .[ . i . ] gs .i . ) j .l . log .Poisson .[ .j . ] gt .j . )This feature estimates the log of the probability of observing certain values of the gap sizes as the sum of the logs of the individual Poisson distributions associated with each gap .", "label": "", "metadata": {}, "score": "63.158722"}
{"text": "Table 5 gives results for each processing stage for the 1998 evaluation set .The large difference ( 6.8 % absolute in WER ) between the P1 and P2 results is due to the combined effects of VTLN , MMIE models on the new training set , the larger vocabulary and a 4-gram LM .", "label": "", "metadata": {}, "score": "63.16477"}
{"text": "No . 673,810 , filed on Mar. 22 , 1991 entitled \" Speaker - Independent Label Coding Apparatus \" .", "label": "", "metadata": {}, "score": "63.183178"}
{"text": "The system of . claim 13 wherein the software instruction for determining if the word is similar to a portion of the phrase comprises a software instruction for determining if the word sounds similar to a substring of the phrase .The system of . claim 19 wherein the software instruction for producing the speech recognition result comprises a software instruction for producing the substring of the phrase that sounds similar to the word .", "label": "", "metadata": {}, "score": "63.184757"}
{"text": "6B is a simplified pictorial view illustrating operation of the activation circuit in the stylus of .FIG .6A ; .FIG .7A is a functional and simplified schematic block diagram of a stylus that includes an activation circuit having an acoustic radiation sensor and ultrasonic transmitter according to various aspects of the present invention ; .", "label": "", "metadata": {}, "score": "63.207977"}
{"text": "For example , ancillary data can include commands to increase / decrease display brightness .The Multiplexer 64 thus outputs only video pixel data and control data .Null fill data is generated in a 100-to-20 bit Converter 66 .As understood herein , eventually , the 25-bit output of the Multiplexer 64 must be converted to 20-bit values .", "label": "", "metadata": {}, "score": "63.211727"}
{"text": "FIG .3 , the Front End 46 of the Transmit FPGA 22 is responsible for multiplexing video data into a twenty - bit data stream .The primary issues associated with this task are as follows : .Both video and control data ( HS , VS , etc . ) must be multiplexed together with some means of separation at the receiver .", "label": "", "metadata": {}, "score": "63.242546"}
{"text": "The system of claim 12 , wherein speech samples are scored by a human , and a statistical model is built using the features and human scores ; wherein the assessment score is based on the scoring model and one or more of the calculated features .", "label": "", "metadata": {}, "score": "63.24515"}
{"text": "i .V . ) . ]An example of the operation of system 200 may be better understood with reference to the data flow diagrams of .FIGS .4 and 5 .Exemplary processes , described below with reference to these data flow diagrams , may be variously implemented with and without user intervention , in hardware and/or software .", "label": "", "metadata": {}, "score": "63.28924"}
{"text": "Such a distribution favors small gaps over large gaps .Another aspect of the modeling of the gap distribution may include the handling of outliers .Outliers are gap sizes for which an unusually large or unusually small number of instances are observed in the training corpus .", "label": "", "metadata": {}, "score": "63.355804"}
{"text": "FIG .8 .Signal S A may be asserted , with appropriate coding , whenever a change in the activation of speech recognition is desired .Signal S A may be asserted with an \" activate \" code when stylus 110 initially becomes situated to receive voice input .", "label": "", "metadata": {}, "score": "63.35884"}
{"text": "For example , durations of certain vowel sounds , word durations , likelihoods of individual words or phonemes being spoken , or other statistics may be extracted once those phonemes are aligned with the digitized speech .[ 0027 ] Those measurements for words and phonemes form a set of alignment results 310 that can be used in feature computation 312 .", "label": "", "metadata": {}, "score": "63.42512"}
{"text": "Further , the detailed description portion ( including referenced drawing figures ) of any U.S. patent or U.S. patent application incorporated by reference into either of these two aforementioned patents is also specifically incorporated herein by reference .A system and method according to various aspects of the invention provides the user with an opportunity to identify text segments that are not expected to occur in the present context of the text currently being generated , or in the user context .", "label": "", "metadata": {}, "score": "63.526222"}
{"text": "[ 0014 ]FIG .6 is a block diagram depicting considerations of an example speech recognition .[ 0015 ] FIG .7 is a block diagram depicting considerations of an example time alignment .[ 0016 ] FIG .8 is a block diagram depicting considerations of an example feature computation .", "label": "", "metadata": {}, "score": "63.613953"}
{"text": "For example , a non - native speech assessment engine 104 may be used in providing an assessment of a non - native student speaking spontaneously .It may be desirable to measure the quality of the non - native student 's speaking ability automatically using a computer , such as for an \" English as a second language \" exam .", "label": "", "metadata": {}, "score": "63.67519"}
{"text": "In each five - word group , the first word is examined , and if it is a null word , it is discarded along with the next four by the stripper 84 .In contrast , if the first word is not a null word , the five word group is assembled into a 100-bit word by the stripper 84 and written to a Back End FIFO 86 .", "label": "", "metadata": {}, "score": "63.73832"}
{"text": "In general , the user interface may be controlled using input devices such as a mouse or keyboard , or using voice commands processed by the speech recognition software 124 .After transferring data from the recorder , the interface software 140 provides the digital samples for an action item to the speech recognition software 124 .", "label": "", "metadata": {}, "score": "63.768436"}
{"text": "Finally , the module performs an IMELDA linear combination transformation to select the twenty - four most useful parameters from the twelve cepstral parameters , the twelve cepstral differences , and the twelve cepstral second differences ( step 345 ) .Referring again to .", "label": "", "metadata": {}, "score": "63.770073"}
{"text": "FIG .6 , leaf nodes may appear at different levels within the lexical tree .Leaf nodes also may correspond to commands .For example , a leaf node may correspond to the word \" select \" and to the command \" SELECT \" .", "label": "", "metadata": {}, "score": "63.78623"}
{"text": "In Chinese , the command may be implemented as , for example , \" . \" using a constraint grammar ( step 1420 ) .For Chinese , the constraint grammar requires \" .If the top recognition candidate includes a command other than the word - in - phrase command ( step 1420 ) , the speech recognition system processes the command ( step 1425 ) .", "label": "", "metadata": {}, "score": "63.831825"}
{"text": "Additional constraints can be imposed to reduce the search - space .Here are some examples : .A limit may be placed on the number of gaps in any partial translation t(H ) .Note that a constraint which imposes that partial translation t(H ) may not contain gaps at all amounts to translating exclusively with contiguous bi - fragments .", "label": "", "metadata": {}, "score": "63.90738"}
{"text": "7B is a simplified pictorial view illustrating operation of the activation circuit in the stylus of .FIG .7A ; .FIG .8 , including .FIGS .8A through 8D , is a timing diagram of signals provided by a stylus according to various aspects of the present invention ; .", "label": "", "metadata": {}, "score": "63.910557"}
{"text": "A system for wirelessly transmitting HDMI data from a source to a display includes a DVI receiver receiving HDMI data , and a transmit digital processing system receiving an output of the DVI receiver .A wireless transmitter receives an output of the transmit digital processing system and wirelessly sends it to a receiver , where a receive digital processing system receives an output of the receiver and sends it to a DVI transmitter .", "label": "", "metadata": {}, "score": "63.91102"}
{"text": "Speech recognition 304 generates word hypotheses 306 of the words in the digitized speech , which may be recorded in any suitable computer - readable storage medium or device .The word hypotheses are identifications of words that likely correspond to those of the digitized speech .", "label": "", "metadata": {}, "score": "64.0968"}
{"text": "claim 25 wherein the software instructions comprise a fourth code segment to produce no speech recognition result if the word is not similar to a portion of the phrase .The computer readable medium of .claim 25 wherein the software instructions comprise a fourth code segment to determine if previously - recognized text has been selected .", "label": "", "metadata": {}, "score": "64.12494"}
{"text": "The alignment results 510 are provided for feature computation 512 .Feature computation 512 may also be influenced by recognition results filtering 514 provided based on the confidence scores 506 assigned by the speech recognition 504 .The recognition results filtering 514 may identify words or phonemes that should not be considered in generating an assessment score 516 .", "label": "", "metadata": {}, "score": "64.137924"}
{"text": "Separate acoustic models 235 are provided for each user of the system .Initially , speaker - independent acoustic models of male or female speech are adapted to a particular user 's speech using an enrollment program .The acoustic models may be further adapted as the system is used .", "label": "", "metadata": {}, "score": "64.16005"}
{"text": "Where the decoder allows relatively large stacks , it may be reasonable to ignore the problem .However , there are various ways for accounting for the buffer consumption .One alternative is to allow the stack to increase in size , beyond its predetermined maximum number of hypotheses , where a consumption decision would otherwise result in the hypothesis being expelled from the stack .", "label": "", "metadata": {}, "score": "64.179504"}
{"text": "6 is a graph of a portion of the lexical tree of .FIG .5 .FIG .7 is a flow chart of a pre - filtering procedure performed by the software of .FIG .2 . FIGS .8A , 8 B and 8 C are state graphs representing nodes of the lexical tree of .", "label": "", "metadata": {}, "score": "64.20945"}
{"text": "IEEE Trans .Information Theory , Vol .37 , pp .107 - 113 .T.R. Niesler , E.W.D. Whittaker & P.C. Woodland ( 1998 ) .Comparison of Part - Of - Speech and Automatically Derived Category - Based Language Models for Speech Recognition .", "label": "", "metadata": {}, "score": "64.22272"}
{"text": "The FPGA converts the data into two data streams and includes a front end component multiplexing video data with control data .A complementary receive FPGA is also disclosed .The system of .The system of .claim 1 , wherein the front end component combines four 25-bit values to form a single 100-bit word and then converts the 100-bit word into five 20-bit words .", "label": "", "metadata": {}, "score": "64.318405"}
{"text": "The parameters of the model are : . conditional frequency distributions f.sub.3 ( t.sub.3 .vertline.t.sub.1 t.sub.2 ) , f.sub.2 ( t.sub.3 . vertline.t.sub.2 ) , f.sub.1 ( t.sub.3 ) , for target words t.sub.1,t.sub.2,t.sub.3 ; . a bucketing scheme c which assigns word pairs t.sub.1 t.sub.2 small number of classes ; . non - negative interpolation functions . lambda .", "label": "", "metadata": {}, "score": "64.340904"}
{"text": "2 : .A \" bi - fragment \" feature h bf , illustrated at 32 .It represents the probability of producing t 1 J using some set of bi - fragments , under the assumption that each source fragment produces a target fragment independently of the others : . h . bf .", "label": "", "metadata": {}, "score": "64.39904"}
{"text": "5,903,668 to Beernink ( incorporated by reference above ) .Speech recognizer 312 generates text data in accordance with acoustic model 356 and shared language model 350 .Handwriting recognizer 314 generates text data in accordance with handwriting model 358 and shared language model 350 .", "label": "", "metadata": {}, "score": "64.44251"}
{"text": "Statistical Model .Many existing machine translation systems are based on probability models , that assign to any target language sequence t 1 J a probability of being a translation of a source language sequence s 1 I .The general approach in statistical machine translation is to find the most probable translation for s 1 I : . t .", "label": "", "metadata": {}, "score": "64.57338"}
{"text": "Since EBMs have no requirement for proper normalization , this problem is naturally circumvented .EBMs can be viewed as a form of non - probabilistic factor graphs , and they provide considerably more flexibility in the design of architectures and training criteria than probabilistic approaches . \" ...", "label": "", "metadata": {}, "score": "64.57654"}
{"text": "5 is a block diagram of an example of an acoustic feature value measure for an acoustic processor for a speech recognition system according to the invention .DESCRIPTION OF THE PREFERRED EMBODIMENTS .Referring to FIG .1 , the speech recognition system comprises a display 10 for displaying a source text .", "label": "", "metadata": {}, "score": "64.65579"}
{"text": "6 allows video data rates up to exactly 80 MHz when used at the symbol rate of 110 MHz .In a 20 uS data frame , 2200 20-bit symbols are sent in a series of blocks 90 , with each block 90 containing its own header 92 and up to two hundred words of video / control data and if needed FEC data .", "label": "", "metadata": {}, "score": "64.71626"}
{"text": "claim 1 wherein determining if the word is similar to a portion of the phrase comprises determining if the word sounds similar to a substring of the phrase .The method of .claim 7 wherein producing the speech recognition result comprises producing the substring of the phrase that sounds similar to the word .", "label": "", "metadata": {}, "score": "64.79291"}
{"text": "When first and second tablets are networked together , modification made to the document by user input to the first tablet is communicated to the second tablet to modify the second view as needed to reflect the modification .Similarly , modification made to the document by user input to the second tablet is communicated to the first tablet to modify the first view as needed to reflect the modification .", "label": "", "metadata": {}, "score": "64.79949"}
{"text": "For example , a language model may be of the type disclosed in U.S. Pat .No .5,839,106 , issued Nov. 17 , 1998 to Bellegarda and U.S. Pat .No .5,828,999 , issued Oct. 27 , 1998 to Bellegarda et al .", "label": "", "metadata": {}, "score": "64.856"}
{"text": "The method of claim 1 , wherein speech samples are scored by a human , and a statistical model is built using the features and human scores ; wherein the assessment score is based on the scoring model and one or more of the calculated features .", "label": "", "metadata": {}, "score": "64.909256"}
{"text": "Any conventional tablet surface may be used , including , for example , a position sensitive membrane or an array of ultrasonic receivers for triangulating the position of a stylus pointer having a cooperating ultrasonic transmitter .A tablet interface and stylus may include cooperating circuitry of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "64.927605"}
{"text": "0044 ] After the scoring model 902 has been trained , assessment scores 908 may be generated based on received features 910 .For example , one or more features 910 may be received at the scoring model 902 .The trained scoring model 902 may have a set of coefficients for respective received features 910 .", "label": "", "metadata": {}, "score": "65.05885"}
{"text": "This is followed by twenty words of variable data , which can include control information that may be used by the receiver .Following the forty header words , the Header Generator 52 can pass ten scrambled RS blocks of data ( 2160 words ) on to a Differential Encoder 54 , and then repeat the process .", "label": "", "metadata": {}, "score": "65.191986"}
{"text": "6,064,959 , issued May 16 , 2000 , which are incorporated by reference .Word - In - Phrase Correction .Homophones are words that sound the same but differ in spelling or representation , origin , and meaning .A word is a sound or a combination of sounds that symbolizes and communicates a meaning and origin .", "label": "", "metadata": {}, "score": "65.2041"}
{"text": "By being mounted on a headset , a microphone may be mounted at a close , fixed proximity to the mouth of a person dictating voice input and thus receive voice input of relatively high acoustic quality .Head mounted headsets have long been viewed with disfavor for use with speech recognition systems , however .", "label": "", "metadata": {}, "score": "65.33132"}
{"text": "A receiver according to various aspects of present invention includes any suitable hardware and/or software for receiving information from a transmitter via a communications link .When a simple wired connection is utilized for a communications link , a receiver may consist of a circuit for amplifying and/or filtering the signal from a microphone .", "label": "", "metadata": {}, "score": "65.37813"}
{"text": "Table 5 : % WER and normalised cross entropy ( NCE ) values on eval98 for all stages of the evaluation system .The final system output is a combination of P4a , P4b , P6a and P5b . ''no FV ' ' denotes system output without full variance transform . ''", "label": "", "metadata": {}, "score": "65.43428"}
{"text": "Semantic clusters associated with user - identified stop segments may be expected , in accordance with the inventive semantic model , to be outside the present context .The semantic model may be part of a shared language model of the invention .In message recognizer 310 , for example , semantic model 354 is part of shared language model 350 .", "label": "", "metadata": {}, "score": "65.55441"}
{"text": "Optionally , program instructions may be stored on a computer readable storage medium such as a magnetic disk , optical disk , recordable memory device , flash memory , or other physical storage medium .Computer instructions may also be communicated via a communications signal , or a modulated carrier wave .", "label": "", "metadata": {}, "score": "65.61406"}
{"text": "Such a circuit may provide an identification signal that identifies an owner of the stylus by any suitable descriptive or uniquely characteristic content ( e.g. , a digital code ) .A few examples of suitable identification circuits and identification signals that may be provided by such circuits are provided in TABLE I below .", "label": "", "metadata": {}, "score": "65.656235"}
{"text": "FIG .5 shows details of the Back End 80 , which is complementary to the transmitter Front End 46 and which is responsible for taking the 20-bit data stream and extracting the original video and control data .This video and control data is then output to the DVI transmitter 38 shown in .", "label": "", "metadata": {}, "score": "65.687645"}
{"text": "Pronunciation is a factor that impacts the intelligibility and perceived comprehensibility of speech .Because pronunciation plays an important role in speech perception , features for assessing pronunciation are worth exploring , especially in the area of measuring spontaneous speech , which remains largely neglected .", "label": "", "metadata": {}, "score": "65.69092"}
{"text": "FIG . 1 ( including .FIGS .1A and 1B ) suitably includes a stylus 110 responsive to voice input and a tablet 150 responsive to freehand input , coupled together via a communications link 115 .A tablet according to various aspects of the present invention includes any suitable hardware and/or software for performing functions of , inter alia , a speech recognizer and an editor .", "label": "", "metadata": {}, "score": "65.710754"}
{"text": "The long felt need for a system incorporating speech recognition capability with pen - based computers has remained largely unfulfilled .Such a system presents conflicting requirements for high acoustic quality and freedom of movement .Pen - based computers are often used in collaborative environments having high ambient noise and extraneous speech .", "label": "", "metadata": {}, "score": "65.73688"}
{"text": "For example , a stylus may be shaped and dimensioned to have a look and feel similar to that of a high quality pen .If desired , such a stylus and pen may be packaged and carried about by a user as a complementary set .", "label": "", "metadata": {}, "score": "65.76146"}
{"text": "For example , the alignment may result in a list of words or phonemes with their respective start and end times .The alignment process may map the words of the hypotheses to their phoneme sequences based on entries in a dictionary .For example , the word \" cat \" may be mapped to the sequence : \" k ae t , \" where k , ae , and t are the three phonemes that are part of the standard pronunciation of the word \" cat \" .", "label": "", "metadata": {}, "score": "65.78391"}
{"text": "These were constructed by training separate models for transcriptions of the Hub5 acoustic training data and for Broadcast News data and then merging the resultant language models to effectively interpolate the component N - grams .The word - level 4-grams used were smoothed with a class - based trigram model using automatically derived classes [ 12 ] .", "label": "", "metadata": {}, "score": "65.784"}
{"text": "Other features are optionally included that calibrate certain dependencies between gap sizes ignored by the simple model of equation ( 6 ) .For example , a feature computing the ratio of the sum of gap sizes on the target side to the sum of gap sizes on the source side .", "label": "", "metadata": {}, "score": "65.80795"}
{"text": "Transmission of voice signal S V , identification signal S ID , and activation signal S A from stylus 110 to tablet 150 may be better understood with reference to .FIG . 8 ( including .FIGS .8A-8D ) .In system 100 , these signals are transmitted via communications link 115 as bursts of serial data .", "label": "", "metadata": {}, "score": "65.82803"}
{"text": "n errors remains as a long - standing goal in speech recognition research .Moreover , our survey of related studies reveals that the system - initiated , data - driven paradigm is widely adopted .Confidence Measures Confidence measures are referred to as a method for detecting ... . \" ...", "label": "", "metadata": {}, "score": "65.8938"}
{"text": "A speech recognition system as claimed in claim 1 , characterized in that the output means comprises a speech synthesizer .A speech recognition system as claimed in claim 1 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "65.93556"}
{"text": "The active vocabulary 230 uses a pronunciation model in which each word is represented by a series of phonemes that make up the phonetic spelling of the word .Each phoneme may be represented as a triphone that includes three nodes .A triphone is a context - dependent phoneme .", "label": "", "metadata": {}, "score": "65.942055"}
{"text": "The vocabulary files contain all of the words , pronunciations , and language model information for the user .Dictation and command grammars may be split between vocabulary files to optimize language model information and memory use , and to keep each single vocabulary file under a size limit , for example , 64,000 words .", "label": "", "metadata": {}, "score": "65.94588"}
{"text": "Handwriting recognizer 314 then derives units of language ( again , strings of text data ) from the units of handwriting in accordance with language model 350 .Handwriting recognizer 314 provides the derived units of language as text data to domain interface 330 .", "label": "", "metadata": {}, "score": "66.01289"}
{"text": "Each of the k gaps allows one word from another phrase to be inserted between two of the words of the elastic phrase .Similarly each target phrase includes l gaps and l+1 components .Both k and l can independently assume a positive natural number , including 0 , such as 0 , 1 , 2 , 3 , 4 , 5 etc .", "label": "", "metadata": {}, "score": "66.02839"}
{"text": "For example , S represents average vowel duration shifts in a portion of digitized speech .S is calculated as .[ 0028 ] With reference back to FIG .3 , the calculated features 314 may then be provided to a scoring model 316 for providing an assessment score 318 .", "label": "", "metadata": {}, "score": "66.10559"}
{"text": "In the \" words on \" example , the phrase \" straight horizontal line through \" was again dictated ( i.e. , communicated by voice input ) to a conventional speech recognizer , which was again a computer executing the NATURALLY SPEAKING software .", "label": "", "metadata": {}, "score": "66.11175"}
{"text": "Each respective local message model may be initialized to be in conformance with a user message model of a particular user .For example , message recognizer 310 may operate at times in accordance with a second local message model ( not shown ) .", "label": "", "metadata": {}, "score": "66.11853"}
{"text": "Following bonding , a Header Detector 74 searches for the twenty word header that was inserted at the transmitter as disclosed above .When the header is found , the Header Detector 74 signals a receiver controller 76 to synchronize itself with the data stream .", "label": "", "metadata": {}, "score": "66.19307"}
{"text": "FIG .1 , e.g. , a QPSK modulator .The Serializers 56 may include two special purpose FPGA cells that , in one non - limiting implementation , may be Xilinx \" RocketIO \" cells that are ten - bit serializers which accept the differentially encoded data in parallel and shift it out a bit at a time to the I / Q outputs .", "label": "", "metadata": {}, "score": "66.1945"}
{"text": "This interconnection is depicted in .FIGS .3A and 3B by signal lines for clarity .Stylus interface 320 receives electromagnetic signals from one or more styluses ( e.g. , styluses 210 and 212 ) and , responsive to the electromagnetic signal(s ) , couples one or more sets of signals to message recognizer 310 .", "label": "", "metadata": {}, "score": "66.22579"}
{"text": "After any editing by the user , and with user approval , the interface software then transfers the action item to the appropriate back - end software 144 .An example of back - end software with which the system works is personal information management software , such as Microsoft Outlook , which is available from Microsoft Corporation of Redmond , Wash. Other suitable back - end software includes contact management software , time management software , expense reporting applications , electronic mail programs , and fax programs .", "label": "", "metadata": {}, "score": "66.24001"}
{"text": "FIG .5 illustrates , by way of example , how the stacks utilized in the beam search procedure are developed .The string 60 to be translated is matched to the bi - phrase library 20 .It will be appreciated that only some of the possible library matches are illustrated .", "label": "", "metadata": {}, "score": "66.28796"}
{"text": "Steps S 112 A , 112 B , 112 C , 112 D , 112 E may be repeated multiple times in the process of generating a target language text string .The target language text string corresponding to the highest scoring complete hypothesis is then output at Step S 114 .", "label": "", "metadata": {}, "score": "66.32024"}
{"text": "A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a speech synthesizer .A speech recognition method as claimed in claim 19 , characterized in that the means for storing speech hypotheses comprises readable computer memory .", "label": "", "metadata": {}, "score": "66.339165"}
{"text": "The native speech data was force aligned to identify durations of all phonemes .This data was utilized to calculate the average duration of each vowel , which was output as a pronunciation measurement 712 .[ 0042 ] FIG .8 is a block diagram illustrating an exemplary feature computation system 802 .", "label": "", "metadata": {}, "score": "66.36167"}
{"text": "In this paper , we present an overview of research in our laboratories on Multimodal Human Computer Interfaces .The goal for such interfaces is to free human computer interaction from the limitations and acceptance barriers due to rigid operating commands and keyboards as only / main I / O - device .", "label": "", "metadata": {}, "score": "66.37631"}
{"text": "In this paper , we present an overview of research in our laboratories on Multimodal Human Computer Interfaces .The goal for such interfaces is to free human computer interaction from the limitations and acceptance barriers due to rigid operating commands and keyboards as only / main I / O - device .", "label": "", "metadata": {}, "score": "66.37631"}
{"text": "11 .The node - processing procedure determines whether the node should spawn additional active nodes and whether the node should be rendered inactive .If the node is a leaf node , the node - processing procedure also determines whether the word corresponding to the node should be added to a word list for a time associated with the node .", "label": "", "metadata": {}, "score": "66.45345"}
{"text": "It is an object of the invention the provide a speech recognition system which has an improved language model for increasing the accuracy of speech recognition .It is another object of the invention the provide a speech recognition system which estimates the probability of occurrence of each speech hypothesis using additional knowledge or information about the actual utterance to be recognized .", "label": "", "metadata": {}, "score": "66.45371"}
{"text": "As shown in .FIG .13N , the control / interface module 220 responds by placing the text 1372 in the dialog box 1310 .The various correction procedures are described in U.S. Pat .No .5,794,189 , issued Aug. 11 , 1998 and U.S. Pat .", "label": "", "metadata": {}, "score": "66.46264"}
{"text": "TABLE -US-00001 WORD PHONEME START END ( in seconds ) Cat 1.20 1.80 k 1.20 1.30 ae 1.30 1.60 t 1.60 1.80 .Such timing data is useful in making measurements related to the digitized speech .For example , the average length of the speaker 's \" ae \" pronunciations may be calculated and examined and compared to the standard length of an \" ae \" pronunciation based on native speech .", "label": "", "metadata": {}, "score": "66.48961"}
{"text": "by Teresa Zollo - Proceedings of AAAI Fall Symposium on Psychological Models of Communication in Collaborative Systems , 1999 . \" ...Continuous speech recognition technology has recently matured to the point where it has become feasible to develop spoken dialogue interfaces to computer systems that help people perform tasks in simple domains .", "label": "", "metadata": {}, "score": "66.528305"}
{"text": "At lower pixel clock rates , this can happen frequently to keep the data pipe full .Accordingly , at the output of the non - limiting Front End 46 , data is always packed in groups of five 20-bit words , to allow the receiver to reliably extract the video and control data without the need for any additional flags or identifiers embedded in the data stream .", "label": "", "metadata": {}, "score": "66.64721"}
{"text": "S .j .S . j .A .j .m .j .A . m .with the boundary condition that S 0,0 equals zero .An even more general form , in which the scores are expressed as functions of the various parameters , is illustrated in .", "label": "", "metadata": {}, "score": "66.68547"}
{"text": "12/628,611 filed Dec. 1 , 2009 , and entitled \" Systems and Methods for Assessment of Non - Native Spontaneous Speech , \" the entirety of which is herein incorporated by reference .This application claims priority to U.S. Provisional Application No .61/118,952 filed on Dec. 1 , 2008 , entitled \" Improved Pronunciation Features for Construct - Driven Assessment of Non - Native Spontaneous Speech , \" the entirety of which is herein incorporated by reference .", "label": "", "metadata": {}, "score": "66.73689"}
{"text": "A speech hypothesis generator 16 generates a set of one or more speech hypotheses .Each speech hypothesis comprises one or more words from the target language .For a sentence of , for example , 10 words out of a target language vocabulary of 20,000 words , there are 20,000 . times.10 .", "label": "", "metadata": {}, "score": "66.77226"}
{"text": "If a delta - sigma A / D converter is used , only a single - pole RC anti - aliasing filter is needed .In system 100 , signal S V is a serial bitstream having bursts 820 , 821 , 822 , 823 , 824 , and 825 of 16-bit samples , which are provided at a 8 kHz sample rate .", "label": "", "metadata": {}, "score": "66.77684"}
{"text": "t .f .t . ) t .T .f .t . ) t .Such a curve has a pronounced peak if the scattering object is relatively flat ( e.g. a wall ) , because all echoes are received at about the same time .", "label": "", "metadata": {}, "score": "66.80147"}
{"text": "The primary focus for research and development of such systems for US English has been the Switchboard / Call Home English corpora along with the regular NIST ' ' Hub5 ' ' evaluations .The Cambridge University HTK ( CU - HTK ) Hub5 system has been developed over several years .", "label": "", "metadata": {}, "score": "66.85303"}
{"text": "Preferably , the processor 1254 may access each component as required .[0054 ]A display interface 1268 may permit information from the bus 1256 to be displayed on a display 1270 in audio , graphic , or alphanumeric format .Communication with external devices may optionally occur using various communication ports 1272 .", "label": "", "metadata": {}, "score": "66.89844"}
{"text": "FIG .8 , identification signal S ID is asserted one time in burst 810 .Signal S ID may be asserted at a relatively low rate of repetition ( e.g. , once every second ) , just often enough to be able to signal a newly used tablet of the identity of stylus 110 without perceptible delay by the user .", "label": "", "metadata": {}, "score": "66.947105"}
{"text": "A user context is the context of all documents generated by a particular user .A user context may also be viewed as a context of all documents generated by a group of users , such as members of a particular collaborative group .", "label": "", "metadata": {}, "score": "66.97136"}
{"text": "Stylus position signal S P communicates freehand input ( conveyed to a tablet interface by a stylus ) to handwriting recognizer 314 via domain interface 330 .A stylus position signal conveys indicia of the position of the stylus on a tablet surface in any suitable format ( e.g. , multiplexed binary words representing X , Y coordinates ) .", "label": "", "metadata": {}, "score": "66.97295"}
{"text": "Some mechanism must allow the video data to move from the video clock domain to the 110 MHz clock domain .The Front End must provide a continuous stream of data out whenever FE_ENB is asserted .If valid video / control data is not available , null data must be generated and inserted .", "label": "", "metadata": {}, "score": "67.03111"}
{"text": "For example , text data provided responsive to voice ( or handwriting ) input from users 242 and 244 may be displayed separately ( e.g. , as alphanumeric characters , kanji , etc . ) on respective tablets 252 and 256 .Alternatively , a hierarchy of styluses may be established such that message input conveyed by a low - ranking stylus may be preempted by message input conveyed by a high - ranking stylus .", "label": "", "metadata": {}, "score": "67.12685"}
{"text": "Some links such as IEEE 802.11(a ) do have a bandwidth high enough to carry compressed HD video but not uncompressed SD or HD video .Also , in the case of 802.11(a ) copyright protection may be implicated because the link is sufficiently long range ( extending beyond the room in which it originates ) that it can be detected beyond the immediate location of the transmitting laptop .", "label": "", "metadata": {}, "score": "67.1757"}
{"text": "A Controller 60 is provided to synchronize all components of the non - limiting Transmit FPGA 22 shown in .FIG .2 .It tells the Header Generator 52 when to generate the forty - word header and initializes the PRN generator in the Scrambler 50 .", "label": "", "metadata": {}, "score": "67.34191"}
{"text": "[0048 ]In addition to assessing pronunciation proficiency , a non - native speech assessment engine may be used in a larger context of an assessment system for a broader construct such as communicative competence or more generally speaking proficiency .[ 0049 ] FIGS .", "label": "", "metadata": {}, "score": "67.346016"}
{"text": "The prototype vector signal P1 has the second best prototype match score with the feature vector signal at time t1 , and therefore is associated with the second - rank score of \" 2 \" .Similarly , for the feature vector signal at time t1 , prototype vector signals P2 , P4 , and P3 are ranked \" 3 \" , \" 4 \" and \" 5 \" respectively .", "label": "", "metadata": {}, "score": "67.39054"}
{"text": "Such a circuit may determine that the stylus is situated to receive voice input by sensing one or more conditions .Such conditions include , for example , the amplitude or angle of impingement of sound waves from the voice input and the proximity ( i.e. , displacement and/or orientation ) of the stylus with respect to the head of a user providing the voice input .", "label": "", "metadata": {}, "score": "67.42851"}
{"text": "Voice signal S V is asserted at regular intervals , for example in periodic bursts 820 - 825 of .FIG . 8B .Signal S V is comprised of periodic samples , which may be provided by a conventional A / D converter ( not shown ) at any desired point in the signal path from microphone 120 through transmitter 122 .", "label": "", "metadata": {}, "score": "67.44121"}
{"text": "This feature vector is augmented by a twenty - first dimension having a value equal to the square root of the sum of the squares of the values of the other twenty dimensions .For each centisecond time interval , a concatenator 64 preferably concatenates nine twenty - one dimension feature vectors representing the one current centisecond time interval , the four preceding centisecond time intervals , and the four following centisecond time intervals to form a single spliced vector of 189 dimensions .", "label": "", "metadata": {}, "score": "67.53174"}
{"text": "No .5,855,000 , issued Dec. 29 , 1998 to Waibel et al . , incorporated above by reference .In domain controller 220 , message recognizer 310 includes both a speech recognizer 312 and handwriting recognizer 314 for recognizing voice and handwriting types of message input , respectively .", "label": "", "metadata": {}, "score": "67.626305"}
{"text": "FIG .13E , the user causes the control / interface module 220 to generate the correction dialog box 1325 by saying the \" Correct That \" command 1331 .The correction dialog box 1325 includes a list 1332 of recognition candidates for the entire utterance 1329 .", "label": "", "metadata": {}, "score": "67.679306"}
{"text": "claim 23 wherein the memory further comprises a software instruction for inserting the produced speech recognition result into the text at a predetermined location if text has not been selected .A computer - readable medium , comprising software instructions for speech recognition , the software instructions comprising : . a first code segment to perform speech recognition on an utterance to produce a recognition result for the utterance , the recognition result including a command , a word , and a phrase ; . a second code segment to determine if the word is similar to a portion of the phrase ; and .", "label": "", "metadata": {}, "score": "67.69327"}
{"text": "However , it will be readily apparent to those skilled in the art that it is possible to embody the invention in specific forms other than those of the exemplary embodiments described above .The embodiments are merely illustrative and should not be considered restrictive .", "label": "", "metadata": {}, "score": "67.70097"}
{"text": "12 is a flow chart of a speech recognition procedure .FIGS .13A - N are screen displays of a user interface of the speech recognition system of .FIG .1 .FIGS .14 and 15 are flow charts of procedures implemented by the speech recognition system of .", "label": "", "metadata": {}, "score": "67.7926"}
{"text": "Normalization processor 58 normalizes the twenty - one dimension feature vector F'(t ) to produce a twenty dimension normalized feature vector X(t ) .The twenty - first dimension of the feature vector F'(t ) , representing the total amplitude or total power , is discarded .", "label": "", "metadata": {}, "score": "67.82196"}
{"text": "In consequence , once the current stack has been traversed , it is never seen again and the decoder can move to the next stack .However this addition , for an admissible heuristics , is positioned in the stack below the current state , because the heuristic cost of the new state will be larger than that of other states in the current stack .", "label": "", "metadata": {}, "score": "67.828865"}
{"text": "The system of .claim 1 , further comprising an activation circuit configured to sense movement of the stylus toward the tablet surface , wherein the system is further configured to receive the freehand input in response to the activation circuit sensing the movement .", "label": "", "metadata": {}, "score": "67.83086"}
{"text": "Such an integrated configuration is advantageous because , inter alia , visible \" ink \" may be displayed to show freehand input as if the stylus pointer were the actual tip of a pen .A display according to various aspects of the present invention may employ any conventional technology such as LCD .", "label": "", "metadata": {}, "score": "67.856384"}
{"text": "The details of the present invention , both as to its structure and operation , can best be understood in reference to the accompanying drawings , in which like reference numerals refer to like parts , and in which : .BRIEF DESCRIPTION OF THE DRAWINGS .", "label": "", "metadata": {}, "score": "67.90431"}
{"text": "Mixture Gaussian distributions for each tied state were then trained using sentence - level Baum - Welch estimation and iterative mixture splitting [ 20 ] .After gender independent ( GI ) models had been trained , a final training iteration using gender - specific training data and updating only the means and mixture weights was performed to estimate gender dependent ( GD ) model sets .", "label": "", "metadata": {}, "score": "67.905624"}
{"text": "I. FIELD OF THE INVENTION .The present invention relates generally to wireless multimedia presentation systems .II .BACKGROUND OF THE INVENTION .Digital video can be transmitted from a source , such as a DVD player , video receiver , ATSC tuner , or other computer , to a display , such as a flat panel video monitor , using a protocol known as Digital Visual Interface ( DVI ) .", "label": "", "metadata": {}, "score": "67.92882"}
{"text": "As shown in .FIG .13 M , the user again causes the control / interface module 220 to generate the correction dialog box 1325 by saying the \" Correct That \" command 1331 .The correction dialog box 1325 includes a list 1370 of recognition candidates for the entire utterance 1331 .", "label": "", "metadata": {}, "score": "67.95761"}
{"text": "In addition , the characterization of a contextual history by a machine may itself be inaccurate .For effective semantic analysis , the need remains for accurate and early identification of context .BRIEF DESCRIPTION OF THE DRAWING .Embodiments of the present invention are described below with reference to the drawing , wherein like designations denote like elements , and : .", "label": "", "metadata": {}, "score": "67.97841"}
{"text": "The enrollment program may operate in an interactive mode that guides the user through the enrollment process , or in a non - interactive mode that permits the user to enroll independently of the computer .In the interactive mode , the enrollment program displays the enrollment text to the user and the user reads the displayed text .", "label": "", "metadata": {}, "score": "68.03217"}
{"text": "FIG .13 .Method 1300 begins at step 1310 with a segment of message input provided by a user .Such a segment may include , for example , a sentence dictated by the user with voice input or a handwritten phrase conveyed by the user with freehand input .", "label": "", "metadata": {}, "score": "68.09591"}
{"text": "Local message model 316 may then be initialized to conform to the remotely located user model .When security is a concern , the user message model may be transmitted in an encrypted format , to be decrypted in accordance with a key that may be provided , for example , in identifying signal S ID .", "label": "", "metadata": {}, "score": "68.11075"}
{"text": "where is the composite model corresponding to the word sequence and is the probability of this sequence as determined by the language model .The summation in the denominator of ( 1 ) is taken over all possible word sequences allowed in the task and it can be replaced by .", "label": "", "metadata": {}, "score": "68.13551"}
{"text": "[ 0034 ] A variety of statistical models may be utilized .For example , Hidden Markov Models may be utilized for representing each phone .By training with many iterations of each phone in the non - native training speech 616 , which may be spoken by one or several non - native speakers , it is possible to account for spectral and temporal variations of the same phone .", "label": "", "metadata": {}, "score": "68.1503"}
{"text": "As discussed above , text data is any indicia of text that may be stored in conveyed numerically , including numerical codes representing individual units of text .In one computing model , text data may be considered to be generated by a source other than an editor .", "label": "", "metadata": {}, "score": "68.17186"}
{"text": "First , four 25-bit values are combined to form a single 100-bit word by the Converter 66 .When four 25-bit words have been assembled into a 100-bit word , they are immediately written into a Front End FIFO 68 .", "label": "", "metadata": {}, "score": "68.20943"}
{"text": "Table 4 shows that the use of pronunciation probabilities gives a reduction in WER of 1.4 - 1.7 % absolute on eval98 .On other test sets improvements greater than 1 % absolute have also been obtained and size of the gains is found to be fairly independent of the complexity of the underlying system .", "label": "", "metadata": {}, "score": "68.222115"}
{"text": "Handwriting may be distinguished from editing commands by a subsystem of the type disclosed in U.S. Pat .No .5,862,256 , issued Jan. 19 , 1999 to Zetts et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .", "label": "", "metadata": {}, "score": "68.38718"}
{"text": "The machine translation system of .claim 18 , wherein the second feature function counts gaps in the target text of the hypothesis and corresponding source text .The machine translation system of . claim 17 , wherein the library associates each of a plurality of elastic bi - fragments with a corresponding parameter representative of a probability distribution of gaps in at least one of the source text fragment and the target text fragment .", "label": "", "metadata": {}, "score": "68.391075"}
{"text": "FIG .7 ) , the pre - filtering procedure activates the root node 505 and associates the minimum of the reseeding score ( S RS ) and the score for the silence node 512 with the root node .During processing of the next frame , the active root node 505 may be used to activate nodes in the group 510 or to activate the silence node 512 .", "label": "", "metadata": {}, "score": "68.39421"}
{"text": "An acoustic model trained with native - quality speech may be utilized in alignment 308 to provide a reference model reflecting proper native speech characteristics .As used herein , native - quality speech means speech spoken by a native - language speaker or speech spoken by an experienced non - native language speaker whose pronunciation skill in the language of interest is commensurate with that of native language speakers .", "label": "", "metadata": {}, "score": "68.47498"}
{"text": "We discuss how the integrated system provides key advantages for help - ing both our work in natural language dialogue processing and in interactive planning and problem solving , and consider the opportunities such an approach affords for the future .Content areas : AI systems , natural language understanding , planning and control , problem solving , user interfaces . ... ources or agents ) can be easily added to the suite of resources at TRIPS ' disposal .", "label": "", "metadata": {}, "score": "68.47736"}
{"text": "DVI also supports HDCP as an optional characteristic .As recognized herein , to save table space and to increase people 's mobility and viewing lines in the room , it may be desirable to view the multimedia on a display using a minimum of wiring .", "label": "", "metadata": {}, "score": "68.48087"}
{"text": "After the second frame is retrieved , the state 810 becomes an active state .Similarly , leave C and leave D are leaving penalties for , respectively , states 810 and 815 .The sum of language model components of leave B , leave C and leave D represents the language model score for the phoneme represented by the node 800 .", "label": "", "metadata": {}, "score": "68.50791"}
{"text": "FIG .4B illustrates the state diagram 450 of the constraint grammar for the select command when a previously - recognized utterance is \" four score and seven \" .This state diagram could be expanded to include words from additional utterances .The \" select \" command and techniques for generating its constraint grammar are described further in U.S. Pat .", "label": "", "metadata": {}, "score": "68.53"}
{"text": "The encrypted multimedia data from the VBI receiver 20 is sent to a processor 22 , such as an application specific integrated circuit ( ASIC ) or field programmable gate array ( FPGA ) or other microprocessor .The processor 22 processes the data for wireless transmission by a wireless transmitter 24 over a transmitting antenna 26 .", "label": "", "metadata": {}, "score": "68.53792"}
{"text": "No .07/968,097 U.S. application , the need remains to integrate high - quality voice input for dictation with pen - based computers .This need is especially unfulfilled for a system that includes voice dictation capability with a number of electronic pads .", "label": "", "metadata": {}, "score": "68.54884"}
{"text": "5,337,353 , issued Aug. 9 , 1994 to Bole et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .A radiation sensor according to various aspects of the present invention includes any device responsive to field radiation impinging on the stylus .", "label": "", "metadata": {}, "score": "68.55148"}
{"text": "In the hypothetical example , the feature vector signals and the prototype vector signal are shown as having one dimension only , with only one parameter value for that dimension .In practice , however , the feature vector signals and prototype vector signals may have , for example , fifty dimensions , where each dimension has two parameter values .", "label": "", "metadata": {}, "score": "68.61006"}
{"text": "claim 1 , wherein the system is implemented by an FPGA configured for preparing the HDMI and/or DVI data for wireless transmission in the 60 GHz band .A receive digital processing system for wireless reception of HDMI and/or DVI data , the system deserializing received data using a deserializer aligning data by using a first character of a received header to perform alignment in both I and Q channels .", "label": "", "metadata": {}, "score": "68.62793"}
{"text": "Referring to .A memory 118 stores data and programs such as an operating system 120 , an application program 122 ( for example , a word processing program ) , and speech recognition software 124 .A computer suitable for one implementation of the system includes a 700 MHz Pentium \u2122 processor , 128 MB memory , 12 GB of available hard drive space .", "label": "", "metadata": {}, "score": "68.720024"}
{"text": "This is shown on a connected word recognition problem , the notoriously difficult letter spell - ing task .With speechreading we could reduce the error rate up to half of the error rate of the pure acoustic recog - ni ti on . by Yann Lecun , Sumit Chopra , Raia Hadsell , Fu Jie Huang , G. Bakir , T. Hofman , B. Sch\u00f6lkopf , A. Smola , B. Taskar ( eds - Predicting Structured Data , 2006 . \" ...", "label": "", "metadata": {}, "score": "68.72231"}
{"text": "For example , as indicated by the square marker , leaf node 615 of .FIG .6 corresponds to the word \" healing \" .An internal node of the tree also may represent a word of the vocabulary .For example , the node 520 might represent a particular vocabulary word in addition to representing the first two phonemes of other vocabulary words .", "label": "", "metadata": {}, "score": "68.7603"}
{"text": "No .5,839,106 to Bellegarda ( incorporated by reference below ) , particularly from column 3 , line 46 through column 4 , line 43 .A handwriting recognizer according to various aspects of the present invention includes any suitable hardware and software for providing text data responsive to freehand input .", "label": "", "metadata": {}, "score": "68.78485"}
{"text": "Preferably , the rank score processor 42 associates a rank score with all prototype vector signals for each feature vector signal .Each rank score represents the estimated closeness of the associated prototype vector signal to the feature vector signal relative to the estimated closeness of all other prototype vector signals to the feature vector signal .", "label": "", "metadata": {}, "score": "69.012566"}
{"text": "FIG .4 are complementary to those in the transmitter FPGA shown in .FIG .2 .With more specificity , a descrambler 78 contains a PRN generator that is initialized by the controller 76 at the proper time such that the data following the header is restored to its pre - scrambled values .", "label": "", "metadata": {}, "score": "69.02194"}
{"text": "Although a headset - mounted microphone would maintain acoustic quality in such situations , the headset is inconsistent with the freedom of movement desired by users of pen - based computers .It has long been known to incorporate a microphone into a stylus for the acquisition of high - quality speech in a pen - based computer system .", "label": "", "metadata": {}, "score": "69.03616"}
{"text": "Successors of a node also may be referred to as subnodes of the node .When the lexical tree is initialized , the silence node 512 is the only active node .Next , the pre - filtering procedure processes the current node ( step 710 ) according to a node - processing procedure 1100 that is discussed below with reference to .", "label": "", "metadata": {}, "score": "69.05321"}
{"text": "No .5,330,226 issued Jul. 19 , 1994 to Gentry et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .A proximity sensor according various aspects of a present invention may include a radiation sensor that is responsive to acoustic field radiation .", "label": "", "metadata": {}, "score": "69.09018"}
{"text": "A comparison processor 40 compares the closeness of the feature value of each feature vector signal to the parameter values of the prototype vector signals to obtain prototype match scores for each feature vector signal and each prototype vector signal .Table 3 illustrates a hypothetical example of prototype match scores for the feature vector signals of Table 1 , and the prototype vector signals of Table 2 .", "label": "", "metadata": {}, "score": "69.097374"}
{"text": "An article headlined \" Pen and Voice Unite \" in the October 1993 issue of Byte Magazine , for example , describes a system for responding to alternating spoken and written input .In the system described , the use of pen and voice alternates as the user either speaks or writes to the system .", "label": "", "metadata": {}, "score": "69.098915"}
{"text": "The Scrambler 50 is not used for any encryption purpose , which is effected by the higher - level protocol HDCP mentioned above .Instead , the Scrambler 50 randomizes the data to ensure that frequent transitions occur in the data stream , which advantageously allow the receiver to better synchronize itself to the bit clock and recover the data .", "label": "", "metadata": {}, "score": "69.15809"}
{"text": "The HDMI receiver 40 decrypts the multimedia data in accordance with HDCP principles and demultiplexes the audio data from the video data .The multimedia content may then be displayed on a display 44 , such as a cathode ray tube ( CRT ) , liquid crystal display ( LCD ) , plasma display panel ( PDP ) , or TFT , or projector with screen , etc .", "label": "", "metadata": {}, "score": "69.16026"}
{"text": "For uniformity of the notation here , it is retained .When solving the machine translation problem ( Eqn . 1 ) with this type of model , the value of Z f i . 1 need not be computed explicitly , because it depends only on s 1 I , and so the problem reduces to : . t .", "label": "", "metadata": {}, "score": "69.18912"}
{"text": "FIG .4 .More specifically , incoming I and Q data stream are processed to recover the clock and data by a non - limiting FPGA RocketIO cell with clock / data recovery capability , denoted in the block diagram as a \" deserializer \" 72 .", "label": "", "metadata": {}, "score": "69.20158"}
{"text": "15A and 15B are planar views of respective tablets displaying respective views of a document according to various aspects of the present invention .DESCRIPTION OF PREFERRED EXEMPLARY EMBODIMENTS .A text processing system according to various aspects of the present invention may be responsive to both voice input and freehand input to generate and edit text data .", "label": "", "metadata": {}, "score": "69.2034"}
{"text": "Because it is desirable to shift positions throughout a long workday , many users also prefer to physically pick up a document and review it in a reclining position instead of remaining in a fixed , upright seated position to review the document on a fixed display screen .", "label": "", "metadata": {}, "score": "69.42049"}
{"text": "First , a Front End 46 multiplexes 24-bit video data with 5-bit control data ( HS , VS and Control[3:1 ] ) and optional ancillary , data .The Front End 46 outputs a near continuous stream of 20-bit data at , e.g. , 110 MHz to a Reed - Solomon ( RS )", "label": "", "metadata": {}, "score": "69.501236"}
{"text": "However , many users find it more desirable to organize multiple documents on a work surface than to focus attention to multiple windows on a fixed display screen .A physical work surface is usually much larger than the screen display , and multiple documents may be placed on the surface for quick access and review .", "label": "", "metadata": {}, "score": "69.5794"}
{"text": "Respective activation signals S A activate message recognizer 310 ( particularly speech recognizer 312 ) to provide text data responsive to voice input provided by a user of one particular stylus ( e.g. , user 242 of stylus 210 ) .In the event that activation signals from multiple styluses are received at once ( indicating simultaneous voice input from multiple users ) , message recognizer 310 may provide respective portions of text data responsive to each voice input , for example by multitasking .", "label": "", "metadata": {}, "score": "69.58073"}
{"text": "A window generator 48 obtains , for example , a twenty millisecond duration sample of the digital signal from analog to digital converter 46 every ten milliseconds ( one centisecond ) .Each twenty millisecond sample of the digital signal is analyzed by spectrum analyzer 50 in order to obtain the amplitude of the digital signal sample in each of , for example , twenty frequency bands .", "label": "", "metadata": {}, "score": "69.59026"}
{"text": "According to the article , misrecognized words could be simply crossed out .The article suggests , however , that a system in which the use of pen and voice alternates as the user either speaks or writes to the system is less interesting than possibilities that could arise from simultaneous speech and writing .", "label": "", "metadata": {}, "score": "69.59065"}
{"text": "The rotation matrix used in rotator 66 may be obtained , for example , by classifying into M classes a set of 189 dimension spliced vectors obtained during a training session .The inverse of the covariance matrix for all of the spliced vectors in the training set is multiplied by the within - sample covariance matrix for all of the spliced vectors in all M classes .", "label": "", "metadata": {}, "score": "69.61296"}
{"text": "The non - transitory computer - readable memory of claim 25 , wherein words having corresponding word hypotheses that do not meet a confidence threshold are not considered in calculating the assessment score .The non - transitory computer - readable memory of claim 23 , wherein speech samples are scored by a human , and a statistical model is built using the features and human scores ; wherein the assessment score is based on the scoring model and one or more of the calculated features .", "label": "", "metadata": {}, "score": "69.69003"}
{"text": "Color displays permits visible \" ink \" of different colors to be displayed .However , monochromatic displays may also be used , for example to save costs when incorporated into a number of tablets that are used as \" scrap computers .\" A tablet interface interprets editing commands responsive to freehand input and provides indicia of the editing commands to an editor .", "label": "", "metadata": {}, "score": "69.705864"}
{"text": "FIG .7A includes a microphone 720 and an ultrasonic activation circuit 725 , which suitably includes : an ultrasonic radiation transmitter / sensor 740 ; a signal processor 745 ; and a comparator 750 .Transmitter / sensor 740 emits an ultrasonic ( e.g. , having a center frequency of 50 kHz ) pulse of acoustic field radiation from piezoelectric transducer 730 .", "label": "", "metadata": {}, "score": "69.77939"}
{"text": "claim 3 , wherein the generation of a hypothesis includes expanding an existing partial hypothesis by at least one of : . adding a word of a new text fragment to the sequence of words ; and . retrieving an unconsumed word from the buffer to the sequence of words .", "label": "", "metadata": {}, "score": "69.81798"}
{"text": "FIG .5 is a data flow diagram of the operation of the domain controller of .FIG .3A in cooperation with the stylus of .FIG .4 ; .FIG .6A is a functional and simplified schematic block diagram of a stylus that includes an activation circuit having an infrared radiation sensor responsive to infrared field radiation according to various aspects of the present invention ; .", "label": "", "metadata": {}, "score": "69.97426"}
{"text": "Correction conveyed back to either process adapts local message model 316 .Process 550 provides text output for display on tablet 252 .The processes illustrated in the data flow diagrams of .FIGS .4 and 5 continue as user 242 continues providing voice and/or freehand input for generation and editing of text data .", "label": "", "metadata": {}, "score": "70.010376"}
{"text": "Transmitter 122 and receiver 160 , discussed above with reference to .FIG .1A , are examples of suitable transmitters and receivers , respectively .Node 227 establishes bi - directional communications between domain controller 220 and each tablet 252 , 254 , and 256 .", "label": "", "metadata": {}, "score": "70.147415"}
{"text": "As illustrated in data structure 1200 , for example , the word UNCLE may be expected to have a high probability of shared cluster occurrence with other words related to familial relationships .By way of example ( and not intended to be a report of actual results ) , words AUNT , NEPHEW , RELATIVE , and BROTHER are shown in data structure 1200 as secondary text segments under the primary text segment UNCLE .", "label": "", "metadata": {}, "score": "70.17038"}
{"text": "The conditional probabilities may be determined empirically by examining large bodies of text .For example , the conditional probability f(W.sub.z . vertline . W.sub.x W.sub.y ) of word W.sub.z given the occurrence of the string W.sub.x W.sub.y may be estimated from the equation # # EQU10 # # .", "label": "", "metadata": {}, "score": "70.24611"}
{"text": ".. is how to provide robustness in the presence of speech recognition errors .Current spoken dialogue systems inform the user when they can ... . by Lina Zhou , Yongmei Shi , Dongsong Zhang , Andrew Sears - Journal of Management Information Systems , 2006 . \" ... interests include natural language processing , speech recognition , information retrieval , and machine learning .", "label": "", "metadata": {}, "score": "70.37784"}
{"text": "The previous configuration of the local model is in conformance with a previous local model ( i.e. , of a previous user ) .The new configuration is in conformance with a new local model ( i.e. , of a new user ) .", "label": "", "metadata": {}, "score": "70.41223"}
{"text": "The activation circuit may include circuitry of the type disclosed in U.S. Pat .No .4,489,442 , issued Dec. 18 , 1984 to Anderson et al .The disclosure found from column 2 , line 42 through column 11 , line 29 of this aforementioned patent , including drawing figures referenced therein , is incorporated herein by reference .", "label": "", "metadata": {}, "score": "70.41942"}
{"text": "5 .FIGS . 9 and 10 are charts of scores corresponding to the states of the state graphs of .FIGS .8A , 8 B and 8 C. .FIG .11 is a flow chart of a procedure for processing nodes of a lexical tree .", "label": "", "metadata": {}, "score": "70.46832"}
{"text": "As recognized herein , when selecting an RS code , the transmission channel should be characterized first and the RS code then selected to achieve a desired Bit Error Rate ( BER ) .The characteristics of the transmission channel can be a function of the particular installation .", "label": "", "metadata": {}, "score": "70.50077"}
{"text": "A \" non - contiguous phrase \" refers to two or more words in either the source or target language wherein two of the words are separated by a gap of at least one word .Each non - contiguous phrase includes a first portion comprising at least one word , which is separated from a second portion , comprising at least one word , by a gap of at least one word .", "label": "", "metadata": {}, "score": "70.58001"}
{"text": "In either computing model , an editor has access to text data to both read and modify the text data .For example , editor 168 of tablet 150 ( .FIG .1A ) performs any desired modifications to text data generated by speech recognizer 162 in accordance with editing commands communicated by freehand input and interpreted by tablet interface 164 .", "label": "", "metadata": {}, "score": "70.58765"}
{"text": "These pads are intended to be \" scrap computers \" , analogous to scrap paper and having no individualized identity or importance .The pads can be picked up and used anywhere in the work environment to display an electronic document and receive freehand input with a stylus .", "label": "", "metadata": {}, "score": "70.76874"}
{"text": "2 also shows a Clock Generator 58 , which synthesizes a clock ( such as a 1.1 GHz clock ) used by the Serializers 56 and a , e.g. , 110 MHz clock for shifting the parallel data though the system . 1.1 GHz can be used because the RF modulator and demodulators may be tuned to operate at this specific bit rate .", "label": "", "metadata": {}, "score": "71.02667"}
{"text": "The encrypted multimedia data is wirelessly transmitted over a wireless link 30 to a receiver antenna 32 , which sends the data to a wireless receiver 34 .Multimedia may be transmitted in an uncompressed form on the link 30 such that so much data is transmitted each second that bootlegging the content is essentially untenable , although some data compression less preferably may be implemented .", "label": "", "metadata": {}, "score": "71.05783"}
{"text": "BRIEF DESCRIPTION OF THE DRAWING .FIG .1 is a block diagram of an example of a speech recognition system according to the invention .FIG .2 is a block diagram of a portion of another example of a speech recognition system according to the invention .", "label": "", "metadata": {}, "score": "71.11151"}
{"text": "To implement the speech recognition and processing functions of the system 100 , the computer 110 runs interface software 140 , the speech recognition software 124 , a parser 142 , and back - end software 144 .Dragon NaturallySpeaking Preferred Edition 3.1 , available from Dragon Systems , Inc. of Newton , Mass. , offers one example of suitable speech recognition software .", "label": "", "metadata": {}, "score": "71.14339"}
{"text": "The h5train00sub set was chosen to include all the speakers from Swb1 in h5train00 as well as a subset of the available CHE sides .Furthermore results are given for the March 2000 evaluation data set , eval00 , which has 40 sides of Swb1 and 40 CHE sides .", "label": "", "metadata": {}, "score": "71.151535"}
{"text": "In system 100 , for example , activation circuit 126 provides a binary activation signal S A to transmitter 122 while microphone 120 provides transmitter 122 an analog voice signal S V .Transmitter 122 continuously conveys both activation signal S A and voice signal S V to receiver 160 .", "label": "", "metadata": {}, "score": "71.17058"}
{"text": "Statistical language models exploit the fact that not all word sequences occur naturally with equal probability .One simple model is the trigram model of English , in which it is assumed that the probability that a word will be spoken depends only on the previous two words that have been spoken .", "label": "", "metadata": {}, "score": "71.20032"}
{"text": "FIG .3 .A \" desktop \" domain controller may be placed on a work surface as a semi - permanent fixture in a work area .Suitable variations of domain controllers include , for example , unobtrusive ( perhaps hidden for aesthetic reasons ) wall panels and electronic \" white boards \" ( e.g. \" Boards \" of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "71.300064"}
{"text": "Turning to .FIG .2 and a non - limiting FPGA implementation of the transmit processor 22 ( accordingly referred to in the non - limiting disclosure below as a \" Transmit FPGA \" ) , one exemplary non - limiting Transmit FPGA converts 24-bit video data into two 1.1 Gbps data streams .", "label": "", "metadata": {}, "score": "71.36775"}
{"text": "Alternatively , the user may be presented with a simple question that may be quickly answered .For example , if the user dictates the sentence \" Can you recognize speech ? \" , the message recognizer may determine that the word BEACH could be especially useful for indicating context as a stop segment .", "label": "", "metadata": {}, "score": "71.40703"}
{"text": "State - of - the - art on - line handwriting recognition systems should be able to handle dictionary sizes of at least 25,000 words or more to be useful for real - world applications .Using dictionaries of this size requires fast search techniques to achieve reasonable recognition times .", "label": "", "metadata": {}, "score": "71.43303"}
{"text": "State - of - the - art on - line handwriting recognition systems should be able to handle dictionary sizes of at least 25,000 words or more to be useful for real - world applications .Using dictionaries of this size requires fast search techniques to achieve reasonable recognition times .", "label": "", "metadata": {}, "score": "71.43303"}
{"text": "Translated strings of text and the entire text , once translated , may be stored in volatile or non - volatile memory 30 .Alternatively , the processing component 16 accesses a remote database which stores the bi - fragment library , for example , via the internet or via a network link .", "label": "", "metadata": {}, "score": "71.46621"}
{"text": "According to the present invention , the DVI receiver 20 uses DVI protocols to process the received data .As part of the processing the HDMI transmitter 18 multiplexes the video and multiplexes the audio within the video data stream .The DVI receiver 20 demultiplexes the video while passing through the audio multiplexed within the data stream .", "label": "", "metadata": {}, "score": "71.469055"}
{"text": "1 .The Back End 82 receives bursts of data in which null data must be identified and discarded , with the remaining data being demultiplexed into video and control words and with the incoming and output data using completely unrelated clocks .Accordingly , the non - limiting Back End 82 may include a Stripper 84 which receives data from the RS Decoder 80 .", "label": "", "metadata": {}, "score": "71.472855"}
{"text": "It will be appreciated that various of the above - disclosed and other features and functions , or alternatives thereof , may be desirably combined into many other different systems or applications .Also that various presently unforeseen or unanticipated alternatives , modifications , variations or improvements therein may be subsequently made by those skilled in the art which are also intended to be encompassed by the following claims .", "label": "", "metadata": {}, "score": "71.47814"}
{"text": "Thus , different gap sizes can result in different hypotheses , each one being separately analyzed .For example , the bi - phrase : .As will be appreciated , where there are gaps on both the source and target sides , a substantial number of potential hypotheses may be generated corresponding to different numbers of gaps on both the source and target sides .", "label": "", "metadata": {}, "score": "71.49088"}
{"text": "For example , radiation sensor 615 includes two infixed sensors 620 and 630 .Sensors 620 and 630 provide respective sensing signals S 1 and S 2 with amplitudes proportional to the respective amounts of infrared field radiation sensed .Sensor 620 is positioned at the end of a wide aperture cone 622 such that it is responsive to infrared radiation impinging on stylus 110 within a wide angular range AR 1 .", "label": "", "metadata": {}, "score": "71.515884"}
{"text": "It should be understood that the non - native speech assessment engine 104 could also be provided on a stand - alone computer for access by a user .The non - native speech assessment engine 104 generates an assessment score for non - native spontaneous speech pronunciation utilizing two different acoustic models .", "label": "", "metadata": {}, "score": "71.51921"}
{"text": "When tablets 252 , 254 , and 256 are located in separate rooms from each other and/or from domain controller 220 , suitable RF field radiation ( e.g. , 1 GHz carrier frequency ) may be employed .A domain controller according to various aspects of the present invention includes any suitable hardware and/or software for controlling the interconnection of networked tablets to each other and to any styluses being used with the tablets .", "label": "", "metadata": {}, "score": "71.55646"}
{"text": "FIG .6 thus contains up to 1600 video words ( pixels or controls ) which , at 80 MHz , represents exactly 20 uS of video data .It is not necessary for a device or method to address each and every problem sought to be solved by the present invention , for it to be encompassed by the present claims .", "label": "", "metadata": {}, "score": "71.69082"}
{"text": "( 3 ) e 1 is already in the sequence but e 2 is still in the buffer .In this case , the minimum size m of the gap between e 1 and e 2 is known ( namely , the number of words in the sequence which are to the right of e 1 .", "label": "", "metadata": {}, "score": "71.78193"}
{"text": "Bigram , trigram and 4-gram LMs were trained on each data set ( LDC Hub5 , MSU Hub5 , BN ) and merged to form an effective 3-way interpolation .Furthermore , as described in [ 6 ] a class - based trigram model using 400 automatically generated word classes [ 12 , 9 ] was built to smooth the merged 4-gram language model by a further interpolation step to form the language model used in lattice rescoring .", "label": "", "metadata": {}, "score": "71.86197"}
{"text": "I.339-I.341 , which is incorporated by reference .From the normalized results , the module performs cepstral analysis to produce twelve cepstral parameters ( step 325 ) .The module generates the cepstral parameters by performing an inverse cosine transformation on the logarithms of the frequency parameters .", "label": "", "metadata": {}, "score": "71.936554"}
{"text": "The leaving penalty is added to any score that exits the state , and includes a duration component and a language model component .The duration component is related directly to the length of the sound represented by the state and to the length of the phoneme represented by the node to which the state belongs .", "label": "", "metadata": {}, "score": "72.00664"}
{"text": "After the word \" select \" , the constraint grammar permits a transition along a path 410 to a second state 415 that requires the next word in the command to be a previously - recognized word .A path 420 , which returns to the second state 415 , indicates that the command may include additional previously - recognized words .", "label": "", "metadata": {}, "score": "72.04521"}
{"text": "10 .Referring to .FIG .11 , a node may be processed according to a node - processing procedure 1100 .Initially , the node - processing procedure updates the scores and time values for each state of the node ( step 1105 ) .", "label": "", "metadata": {}, "score": "72.07916"}
{"text": "The node - processing procedure may deactivate a node or state by deleting a record associated with the node or state , or by indicating in the record that the node or state is inactive .Similarly , the node - processing procedure may activate a node or state by creating a record and associating the record with the node or state , or by indicating in an existing record that the node or state is active .", "label": "", "metadata": {}, "score": "72.09372"}
{"text": "Proximity sensor 124 further includes a comparator 640 for comparing sensing signals S 1 and S 2 .A comparator according various aspects of the present invention includes any circuit for making a determination that a stylus is situated to receive voice input based on one or more suitable conditions of signals provided by sensors .", "label": "", "metadata": {}, "score": "72.095856"}
{"text": "From the root node 505 , the tree expands to a group 510 of nodes that correspond to phonemes with which words start .A silence node 512 that represents silence also may be reached from the root node 505 .Each node in the group 510 represents a phoneme that appears at the beginning of one or more words .", "label": "", "metadata": {}, "score": "72.173965"}
{"text": "Video / control data enters a Front End Multiplexer 64 at the rate of one video pixel or one control word for every video clock .For each video clock , the Multiplexer 64 outputs a 25-bit word with DE as the most significant bit .", "label": "", "metadata": {}, "score": "72.400215"}
{"text": "The spectrum analyzer 50 may be , for example , a fast Fourier transform processor .Alternatively , it may be a bank of twenty band pass filters .The twenty - one dimension vector signals produced by spectrum analyzer 50 may be adapted to remove background noise by an adaptive noise cancellation processor 52 .", "label": "", "metadata": {}, "score": "72.40022"}
{"text": "claim 9 , wherein the semantic model comprises at least one semantic cluster selected from the group consisting of : . a semantic cluster of a first type that is a mathematically derived group of vectors representing text segments ; . a semantic cluster of a second type that is a discrete grouping of text having a particular context ; and .", "label": "", "metadata": {}, "score": "72.444786"}
{"text": "However , as understood herein a reduction in the ability to handle burst errors can accrue with the use of shorter codes .The ( 216,200 ) code is capable of correcting a burst of eight word errors ( 80 bit errors ) , whereas a ( 54,50 ) code can only correct a burst of two word errors ( 20 bit errors ) .", "label": "", "metadata": {}, "score": "72.58003"}
{"text": "In a variation , touch - sensitive pads ( each responsive to contact with pointer 610 ) may be provided below tablet surface 152 in red , yellow , black , and/or white colors to serve as such \" inkwells . \"An editor ( e.g. , editor 168 ) according to various aspects of the present invention includes any suitable hardware and/or software for modifying text data as specified by editing commands .", "label": "", "metadata": {}, "score": "72.591385"}
{"text": "Rather , the present invention is defined variously by the issued claims .Each variation of the present invention is intended to be limited only by the recited limitations of its respective claim , and equivalents thereof , without limitation by terms not present therein .", "label": "", "metadata": {}, "score": "72.69989"}
{"text": "Transmitter 122 is coupled to receiver 160 via communications link 115 .Speech recognizer 162 generates text data responsive to voice input from user 240 and provides the text data to editor 168 .Tablet interface 164 interprets editing commands responsive to freehand input and provides indicia of the editing commands to editor 168 .", "label": "", "metadata": {}, "score": "72.83341"}
{"text": "The memory component stores a library of bi - fragments 20 , including non - contiguous bi - fragments .The processing component 16 executes instructions for performing the method of .FIG .3 , which may be stored in an article of manufacture comprising a program storage medium readable by a computer comprising instructions for executing the method , such as memory 18 .", "label": "", "metadata": {}, "score": "72.83405"}
{"text": "However , the residual portion of signal S 1 will then be relatively large because a significant portion of the infrared radiation falls within angular range AR 1 but outside range AR 2 .Consequently , the ratio between signal S 1 and the residual portion will fall short of the second predetermined threshold and condition B will not be met .", "label": "", "metadata": {}, "score": "72.87729"}
{"text": "FIG .13B , the user corrects the incorrect recognition by selecting the word \" cancers \" using the mouse 110 and saying \" Spell That k i b i \" .The control / interface module responds to recognition of the \" Spell That \" command by displaying a correction dialog box 1325 , such as is illustrated in .", "label": "", "metadata": {}, "score": "72.89938"}
{"text": "A language model may be of a conventional type , for example consisting of an N - gram syntactic model .An example of a unified message model may be better understood with reference to .FIG .3B .Local message model 316 includes a language model 350 , an acoustic model 356 , and a handwriting model 358 .", "label": "", "metadata": {}, "score": "72.90767"}
{"text": "The second alternative corresponds to a transition from state 805 back to state 805 along path 820 .When the first alternative is the more likely , the starting time corresponding to the first frame that was stored previously for the state 805 is replaced by a value corresponding to the second frame .", "label": "", "metadata": {}, "score": "72.92616"}
{"text": "P . k . ) k . k . which is fully determined by a single parameter of the distribution , its mean \u03bc , as illustrated in .FIG .1 .In this way , gap sizes for which no observed examples are found in the corpus may nevertheless be assigned a non - zero probability of being found in a translation .", "label": "", "metadata": {}, "score": "73.00212"}
{"text": "M . m .h .m .s .I . t .J .d .K . ) 1 I is a normalization constant .The h m are thus real - valued features representing different aspects of a translation hypothesis , and the \u03bb m are weights associated with these features .", "label": "", "metadata": {}, "score": "73.16817"}
{"text": "Using the above - described wide channel and a simpler modulation scheme such as but not limited to DQPSK , QPSK , BPSK or 8-PSK , a high data rate yet simple system can be achieved .For example , when DQPSK is used , a data rate of twice the symbol rate can be achieved .", "label": "", "metadata": {}, "score": "73.46796"}
{"text": "Bi - Fragment Library .To produce translations , the method relies on a collection of bi - fragments , which is referred to as a bi - fragment library 20 .An Adapted Translation Method .It will be appreciated that a source string may be translated as multiple different target strings where multiple alternative bi - fragments exist .", "label": "", "metadata": {}, "score": "73.479294"}
{"text": "The 54k wordlist reduced the out - of - vocabulary ( OOV ) rate on eval98 from 0.94 % to 0.38 % .After the March 2000 evaluation it was found that using the 54k dictionary gave an OOV rate of 0.30 % on eval00 compared to 0.69 % if the 27k dictionary had been used .", "label": "", "metadata": {}, "score": "73.62923"}
{"text": "The first view of document 1500 , displayed on tablet 252 , is updated according to various aspects of the invention to reflect modification made by user input to tablet 254 .Modifications to the document may be centrally registered in a single data store , for example in domain controller 220 .", "label": "", "metadata": {}, "score": "73.67268"}
{"text": "The header bits of any given bursts identify that burst as a portion of either signal S V , S ID , or S A .In system 100 , two header bits are used to distinguish the three signals .The data bits convey signals S V , S ID , or S A as serial data .", "label": "", "metadata": {}, "score": "73.73645"}
{"text": "Bursts 820 - 825 are merely numeric representations of signal samples that have been acquired and digitized at constant intervals .Bursts 820 - 825 need only to be transmitted at the same mean rate as the rate at which the signal samples were acquired .", "label": "", "metadata": {}, "score": "73.92716"}
{"text": "Activation signal S A takes high priority because it determines whether or not bursts from voice signal S V are responded to .Consequently , burst 830 from signal S A is passed on signal 840 immediately after burst 810 , even though two bursts 821 and 822 of voice signal S V are ready to be passed on as well .", "label": "", "metadata": {}, "score": "73.92944"}
{"text": "When DQPSK is used the data rate may be 2.2 Gbps , and the link may have a data rate of approximately 2.5 Gbps .The link may have a fixed bandwidth of two and half GigaHertz ( 2.5 GHz ) .With this in mind , it may now be appreciated that the wireless transmitter 24 preferably includes an encoder for encoding in accordance with principles known in the art .", "label": "", "metadata": {}, "score": "73.95179"}
{"text": "A stylus may also include ( alternatively or additionally ) an identification circuit for identifying the owner of the stylus , for example to permit selection of an appropriate user message model , as described below .Exemplary stylus 110 further includes : proximity sensor 124 ; activation circuit 126 ; and identification circuit 128 .", "label": "", "metadata": {}, "score": "73.96005"}
{"text": "FIG .9 provides a simplified example of how scores propagate through the lexical tree .Before the first frame is retrieved ( row 900 ) , state 840 ( which corresponds to silence ) has a score of 0 and no other nodes are active .", "label": "", "metadata": {}, "score": "74.002106"}
{"text": "A proximity sensor may include a radiation sensor responsive to field radiation impinging on the stylus from a human head .For example , proximity sensor 124 includes radiation sensor 615 .Alternatively , a proximity sensor may be a capacitive proximity sensor of the type disclosed in U.S. Pat .", "label": "", "metadata": {}, "score": "74.017426"}
{"text": "13B .The box 1325 displays a numbered list of words 1326 starting with the indicated letters ( \" kibi \" ) .Instead of using the mouse 110 to select the word \" cancer \" , the user could have verbally selected the word using a \" Select \" command by saying \" Select cancer \" .", "label": "", "metadata": {}, "score": "74.09448"}
{"text": "The dictated phrase and the speech recognizer 's alternative text segments are listed below in TABLE V. .The phrase \" straight horizontal line through \" was dictated the second time by the Applicant during preparation of the present patent application .The text data \" straight words on a line through , \" generated by the conventional speech recognizer ( a computer executing the NATURALLY SPEAKING software ) in response , was not clearly out of the context of such a document .", "label": "", "metadata": {}, "score": "74.13837"}
{"text": "A.Kempe , J.M.Champarnaud , J.Eisner , A Note on Join and Auto - Intersection of n - ary Rational Relations , In B. Watson and L. Cleophas , editors , Proceedings Eindhoven FASTAR Days , No . 04- 40 in TU / e CS TR , pp .", "label": "", "metadata": {}, "score": "74.223236"}
{"text": "The system also may include an analog recorder port 126 and/or a digital recorder port 128 .The analog recorder port 126 is connected to the sound card 116 and is used to transmit speech recorded using an analog or digital hand - held recorder to the sound card .", "label": "", "metadata": {}, "score": "74.28204"}
{"text": "[ 0026 ] The digitized speech 302 and the word hypotheses 306 are then provided for time alignment 308 , sometimes referred to as forced alignment .Alignment 308 creates a time - alignment between a string of words identified in the word hypotheses 306 , as well as their phonemes contained therein , with the digitized speech signal .", "label": "", "metadata": {}, "score": "74.28502"}
{"text": "Thus the complete soft - tied system has the same number of Gaussians as the original system and three times as many mixture weights per state .After this revised structure has been created all system parameters are re - estimated .This approach allows the construction of both soft - tied triphone and quinphone systems in a straightforward manner .", "label": "", "metadata": {}, "score": "74.28572"}
{"text": "claim 25 wherein the second code segment comprises a code segment to determine if the word matches a substring of the phrase .The computer readable medium of .claim 29 wherein the third code segment comprises a code segment to produce the word .", "label": "", "metadata": {}, "score": "74.2948"}
{"text": "4 illustrates this with the example sentence Je ne veux plus danser le tango .In .FIG .4 , words in bold correspond to word of the source sentence which are covered in the decision and words of the target added to the sequence or buffer .", "label": "", "metadata": {}, "score": "74.41227"}
{"text": "If the incoming video data rate is insufficient to satisfy the RS Encoder , null words are generated such that the RS Encoder is never starved for data .The RS Encoder 48 may include two 10-bit encoders that apply an RS code of ( 216 , 200 ) .", "label": "", "metadata": {}, "score": "74.41441"}
{"text": "User identification of context may be performed in addition to , or alternatively to , correction of misrecognized message input .Local message model 316 may be initialized to be in conformance with one or more user message models ( e.g. , user message model 262 or 264 ) .", "label": "", "metadata": {}, "score": "74.42254"}
{"text": "Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a continuation of U.S. application Ser .No .10/061,052 filed on Jan. 28 , 2002 , now U.S. Pat .No .6,904,405 which is a continuation of U.S. application Ser .", "label": "", "metadata": {}, "score": "74.54593"}
{"text": "Alternatively , the expected profile may simply be a collection of statistics ( e.g. , center of mass and peak width ) characterizing the expected distance profile when microphone 720 is at a predetermined proximity to the head of user 780 .The distance profile may be compared to the expected profile by any suitable signal processing technique ( e.g. , comparison of statistics or cross correlation of data points ) .", "label": "", "metadata": {}, "score": "74.613396"}
{"text": "No .5,602,516 , issued Sep. 1 , 1998 to Shwarts et al . , and U.S. Pat .No .5,903,668 , issued May 11 , 1999 to Beernink .The detailed description portion ( including referenced drawing figures ) of each of these two aforementioned patents is incorporated herein by reference .", "label": "", "metadata": {}, "score": "74.74807"}
{"text": "4A depicts a table of example features that may be calculated based on the measurements and other data contained within the alignment results , and FIG .4B depicts a table that lists variables used in the feature table and their meanings .[ 0013 ] FIG .", "label": "", "metadata": {}, "score": "74.78851"}
{"text": "generating converted speech text data based at least in part on the audio input , on the adjusted language model , and on the acoustic model .An apparatus , comprising : . means for receiving freehand input based at least in part on manipulation of a stylus ; . means for receiving audio input based at least in part on audio information ; . means for selecting a particular user message model from a plurality of user message models ; . means for adjusting a language model of a local message model based at least in part on the particular user message model , the local message model including the language model , a handwriting model , and an acoustic model ; . means for generating converted handwriting text data based at least in part on the freehand input , on the adjusted language model , and on the handwriting model ; and .", "label": "", "metadata": {}, "score": "74.832184"}
{"text": "The FIFO 68 notifies a 100-to-20 bit converter 70 when data is available with its DAV output .The FIFO is written synchronously with the video clock and read synchronously with the 110 MHz clock .When FE_ENB is asserted , the 100-to-20 bit Converter 70 removes words from the FIFO and outputs them in bursts of five 20-bit words .", "label": "", "metadata": {}, "score": "74.91891"}
{"text": "Nos .11/036,932 and 11/035,845 , incorporated herein by reference .According to the present invention , the DVI receiver 20 , processor 22 , and wireless transmitter 24 may be contained on a single chip , or on separate substrates .Indeed , the DVI receiver 20 , processor 22 , and wireless transmitter 24 may be integrated into the media receiver 16 .", "label": "", "metadata": {}, "score": "74.969894"}
{"text": "FIG .8C , the root node 505 is represented by a single state 860 .Comparison with a frame causes the score in the node to be passed to one or more subsequent nodes ( including the silence node 512 ) through a path 865 .", "label": "", "metadata": {}, "score": "75.011566"}
{"text": "( A longer code may be used when the identification signal is employed for decryption . )Process 430 responds to the identification signal and to the enabled voice signal from process 420 to provide an electromagnetic signal .The electromagnetic signal suitably combines the identification signal and the voice signal .", "label": "", "metadata": {}, "score": "75.08065"}
{"text": "In a variation where the benefits of user context identification are not required , semantic model 354 may be of the type disclosed in U.S. Pat .No .5,828,999 to Bellegarda et al .( incorporated by reference below ) .In a further variation where the benefits of semantic analysis are not required , language model 350 may omit semantic model 354 .", "label": "", "metadata": {}, "score": "75.19236"}
{"text": "[ 0021 ] FIG .1 depicts a computer - implemented system 100 wherein users 102 can interact with a non - native speech assessment engine 104 hosted on one or more servers 106 through a network 108 .The speech assessment engine 104 can be used for assessing the pronunciation of s non - native language speaker , e.g. , a speaker whose native language is Spanish but who is speaking in English so as to obtain an assessment of her English pronunciation .", "label": "", "metadata": {}, "score": "75.26975"}
{"text": "Comparing the performance of MLE models in P4b , they are 0.7 % poorer than in the eval setup ( MLE models with MMIE lattices and adaptation supervision ) without confusion networks but only 0.3 % poorer with confusion networks .An interesting result shows that although the pure MLE branch is poorer than the mixed MMIE / MLE system it is still able to contribute to the 4-way combination by the same amount .", "label": "", "metadata": {}, "score": "75.31951"}
{"text": "The system of .claim 7 , wherein the system is configured to access the particular user message model via a wide area network .The system of .claim 1 , wherein the system adjusting the language model includes adjusting a syntactic model and a semantic model included in the - language model .", "label": "", "metadata": {}, "score": "75.31987"}
{"text": "Respective identification signals S ID permit message recognizer 310 to associate message input conveyed by a particular stylus with a user message model of the user of a stylus .An identification signal according to various aspects of the present invention may include an encryption key to permit message recognizer 310 to interpret a user message model that has been encrypted for secure transmission across a wide area network .", "label": "", "metadata": {}, "score": "75.40996"}
{"text": "An article of manufacture comprising a program storage medium readable by a computer comprising instructions for executing the method of . claim 1 .Description .CROSS REFERENCE TO RELATED PATENTS AND APPLICATIONS .Cross - reference is made to U.S. application Ser .", "label": "", "metadata": {}, "score": "75.57303"}
{"text": "The count n.sub.xy is the number of occurrence ; of the bigram W.sub.x W.sub.y in the training text .The values of the coefficients . lambda .sub.1 , . lambda . sub.2 , . lambda . sub.3 , and . lambda . sub.4 in equations [ 10 ] and [ 15 ] may be estimated by the deleted interpolation method .", "label": "", "metadata": {}, "score": "75.587715"}
{"text": "FIG .4A depicts a table of example features that may be calculated based on the measurements and other data contained within the alignment results .FIG .4B depicts a second table listing variables used to calculate the features in the feature table of FIG .", "label": "", "metadata": {}, "score": "75.93959"}
{"text": "A prototype vector store 38 stores a plurality of prototype vector signals .Each prototype vector signal has at least one parameter value and has a unique identification value .Table 2 shows a hypothetical example of five prototype vectors signals having one parameter value each , and having identification values P1 , P2 , P3 , P4 , and P5 , respectively .", "label": "", "metadata": {}, "score": "76.03848"}
{"text": "One example of an acoustic feature value measure is shown in FIG .5 .The measuring means includes a microphone 44 for generating an analog electrical signal corresponding to the utterance .The analog electrical signal from microphone 44 is converted to a digital electrical signal by analog to digital converter 46 .", "label": "", "metadata": {}, "score": "76.04507"}
{"text": "For example , the scoring model may be generated using a multiple regression procedure where coefficients are determined and those coefficients are applied to the calculated features 314 .Those products can then be summed to generate an assessment score 318 .[ 0029 ] FIG .", "label": "", "metadata": {}, "score": "76.34163"}
{"text": "A preferred example of a text processing system including a network of tablets may be better understood with reference to .FIGS . 2 - 5 .Styluses 210 and 212 are linked to domain controller 220 via wireless communications links 235 .System 200 further includes suitable wireless nodes 227 and 230 for establishing communications links 225 and 235 , respectively .", "label": "", "metadata": {}, "score": "76.40207"}
{"text": "Unfortunately , an HDMI transmitter will never send HDMI data to a DVI receiver once the transmitter discovers that the receiver is not HDMI , so it can be difficult to mix the two systems .Nonetheless , the present invention understands that it is possible to selectively use less expensive DVI components in an HDMI system .", "label": "", "metadata": {}, "score": "76.48469"}
{"text": "4 illustrates the partial coverage of the source sentence Je ne veux plus danser le tango and the corresponding partial coverage of the target sentence in a series of hypothesis extension steps S 201 , S 202 , S 203 , S 204 , S 206 .", "label": "", "metadata": {}, "score": "76.51964"}
{"text": "Since the first entry 1353 is the correct entry , the user clicks on an \" OK \" button 1354 at the bottom of the correction dialog box 1325 .As shown in .FIG .13 K , the control / interface module 220 responds by placing the correct version 1355 of the utterance in the dictation window 1310 .", "label": "", "metadata": {}, "score": "76.57927"}
{"text": "By facilitating identification , stylus 110 may be used in particularly advantageous applications .In system 200 , for example , stylus 110 may be owned by a particular user and used with a tablet that is shared with a number of users .", "label": "", "metadata": {}, "score": "76.60373"}
{"text": "Preferably , the display is configured to preserve clarity at a wide range of viewing angles and under varied lighting conditions .When tablet surface 152 is integrated with a color display , additional user controls may include a displayed group of software - based mode controls ( \" inkwells \" ) at the bottom of tablet surface 152 for changing the behavior of freehand input .", "label": "", "metadata": {}, "score": "76.70856"}
{"text": "FIG .6 , a node 610 corresponds to all words in the vocabulary that start with the phoneme \" H \" .Together , the nodes in the group 510 include representations of the starting phoneme of every word in the vocabulary .", "label": "", "metadata": {}, "score": "76.86206"}
{"text": "claim 1 , wherein the particular user message model includes user - specific acoustic model information and user - specific handwriting model information .A method , comprising : . receiving , at a device , freehand input based at least in part on manipulation of a stylus ; . receiving , at the device , audio input based at least in part on information received at a microphone ; . selecting , via the device , a selected user message model from a plurality of user message models ; . generating , by the device , converted handwriting text data based at least in part on the freehand input , the adjusted language model , and the handwriting model ; and .", "label": "", "metadata": {}, "score": "76.93758"}
{"text": "A \" non - contiguous bi - fragment , \" as used herein , refers to a bi - fragment in which at least one of the text fragments comprises a non - contiguous phrase .It should be noted that in many cases discussed herein , both fragments of a bi - fragment are phrases ( i.e. , a bi - phrase ) although they need not be so .", "label": "", "metadata": {}, "score": "76.98337"}
{"text": "These rare triphones may be represented by having closely - related triphones share the same set of PDFs .A large vocabulary dictation grammar may include multiple dictation topics ( for example , \" medical \" or \" legal \" ) , each having its own vocabulary file and its own language model .", "label": "", "metadata": {}, "score": "77.05344"}
{"text": "However , benefits of certain aspects may be obtained even when various other aspects are omitted .Thus , the example below should not be considered as limiting the scope of the invention in any way .The operation of stylus 210 , in the example , may be better understood with reference to the data flow diagram of .", "label": "", "metadata": {}, "score": "77.07342"}
{"text": "Editing commands include any commands for modifying text data that may be communicated by freehand input .A set of particularly advantageous editing commands is provided in TABLE II below .In a variation of tablet 150 , tablet interface 164 includes a conventional handwriting recognizer that responds to freehand input to generate text data in addition to , or as an alternative to , text data generated by voice input .", "label": "", "metadata": {}, "score": "77.09552"}
{"text": "Alternatively , a registration system may assign an identification signal to the user , and an associate the user with that predetermined identification signal .When stylus 110 is assigned ( e.g. , sold , loaned by an employer , etc . ) to a user , for example , the user may register personal information for association with the predetermined identification signal .", "label": "", "metadata": {}, "score": "77.18656"}
{"text": "A stylus according to various aspects of the present invention includes any device having a pointer for conveying freehand input to a tablet interface .A pointer may be a portion of one end of a stylus that is suitably narrow for a user to convey freehand input ( e.g. , gestures , editing commands , and handwriting ) .", "label": "", "metadata": {}, "score": "77.22308"}
{"text": "42.5.1 - 42.5.4 , Mar. 1984 .\" Language and Machines - Computers in Translation and Linguistics \" .National Academy of the Sciences , Washington , D.C. , Publication 1416 , 1966 .Primary Examiner : Fleming ; Michael R. Assistant Examiner : Hofiz ; Tarif R. Attorney , Agent or Firm : .", "label": "", "metadata": {}, "score": "77.35307"}
{"text": "Consequently , stylus 110 may use signal S ID to validate secured access to tablet 150 .Signal S ID may also be used as an encryption key , for example to facilitate secured transmission of a user message model on a wide area network .", "label": "", "metadata": {}, "score": "77.50015"}
{"text": "7,483,717 , which is a continuation of U.S. patent application Ser .No .11/215,463 , filed Aug. 30 , 2005 , now U.S. Pat .No .7,228,154 , which claims priority to U.S. provisional patent application Ser .No .60/624,940 , filed Nov. 3 , 2004 .", "label": "", "metadata": {}, "score": "77.50019"}
{"text": "11 .At step 1110 , process 1100 begins by setting pointers i and k both to zero for counting of the first text segment w 0 in the first cluster C 0 .At step 1120 , a count is made of the number of occurrences of one text segment w i of the corpus in one cluster C k of the corpus .", "label": "", "metadata": {}, "score": "77.62917"}
{"text": "[0045 ] The pronunciation assessment score 908 may be a final output score , or the pronunciation assessment score 908 may be a part of a composite score for a non - native speech exam .For example , the pronunciation assessment 908 may be combined with other scores such as a related word stress score , an intonation score , a vocabulary score , and/or a grammar score to generate an overall score .", "label": "", "metadata": {}, "score": "77.718506"}
{"text": "FIG .1A ) , identification circuit 128 is a four - pin serial PROM that identifies stylus 110 by providing , upon request by suitable controlling circuitry ( not shown ) , identification signal S ID as a 48-bit serial word to transmitter 122 .", "label": "", "metadata": {}, "score": "77.81446"}
{"text": "The user could achieve the same result by saying \" Choose 3 \" .As shown in .FIG .131 , the user then uses the mouse to select the words \" to meet this \" 1350 .Then , as shown in .", "label": "", "metadata": {}, "score": "77.82736"}
{"text": "4,969,180 issued Nov. 6 , 1990 to Watterson et al . , The disclosure found from column 5 , line 19 through column 14 , line 55 of the aforementioned patent , including drawing figures referenced therein , is incorporated herein by reference .", "label": "", "metadata": {}, "score": "78.00709"}
{"text": "The speech recognizer provided the text data \" straight Coors Uncle line through \" in response to the voice input .The dictated phrase and the speech recognizer 's alternative text segments are listed below in TABLE IV .The phrase \" straight horizontal line through \" was dictated by the Applicant during preparation of the present patent application .", "label": "", "metadata": {}, "score": "78.122894"}
{"text": "As shown in the example in .FIG .5 , not all of the phrases in the library which include two or more words need to be modeled as elastic bi - phrases , such as the phrases danser le tango and to tango .", "label": "", "metadata": {}, "score": "78.17062"}
{"text": "For example , for the phrase , \" Cat are my favorite animal , \" \" cat \" is the first word in the utterance , as identified by the received word hypotheses 708 .Upon locating the target phoneme in the digitized speech 706 , timing information is identified and output .", "label": "", "metadata": {}, "score": "78.32663"}
{"text": "It may further be appreciated that the wireless receiver 34 includes circuitry that is complementary to the wireless transmitter 24 , namely , a downconverter , a demodulator , and a decoder .In any case , the data from the wireless receiver 34 is sent to a processor 36 for error correction and re - multiplexing as appropriate for use by a DVI transmitter 38 .", "label": "", "metadata": {}, "score": "78.355385"}
{"text": "Easy To Use Patents Search & Patent Lawyer Directory .At Patents you can conduct a Patent Search , File a Patent Application , find a Patent Attorney , or search available technology through our Patent Exchange .Patents are available using simple keyword or date criteria .", "label": "", "metadata": {}, "score": "78.436646"}
{"text": "For example , the phrase \" take the train . . .from . . .to \" is considered as take . . .the . . .train . . .from . . .to , where it is assumed that gaps exist between all the words .", "label": "", "metadata": {}, "score": "78.63092"}
{"text": "A speech recognition system comprising : . a processor having communications links for transmitting information to and from the output and input devices ; and .The system of . claim 13 wherein the recognition result comprises a command in the Chinese language .", "label": "", "metadata": {}, "score": "78.69701"}
{"text": "Or , it can be a satellite , broadcast , or cable receiver , or it can be a DVD player or other multimedia source .The source 12 sends multiplexed multimedia data over lines 14 to a media receiver 16 , so that the source 12 and media receiver 16 together may be thought of as a \" source \" of data and specifically of HDMI data .", "label": "", "metadata": {}, "score": "78.76718"}
{"text": "No .5,828,999 to Bellegarda , incorporated by reference above .The approach taken in the ' 999 patent is to determine probability of a tested text segment as a positive function of its probability of occurrence in a history of prior words .", "label": "", "metadata": {}, "score": "78.99574"}
{"text": "A speech recognition method as claimed in claim 28 , characterized in that the display comprises a cathode ray tube .A speech recognition method as claimed in claim 28 , characterized in that the display comprises a liquid crystal display , .A speech recognition method as claimed in claim 19 , characterized in that the output means comprises a printer .", "label": "", "metadata": {}, "score": "79.12009"}
{"text": "An editor according to various aspects of the present invention may be implemented as part of a domain interface .For example , domain interface 330 may perform the same functions for networked tablets 252 , 254 , and 256 that editor 168 performs for tablet 150 .", "label": "", "metadata": {}, "score": "79.22526"}
{"text": "claim 25 wherein the second code segment comprises a code segment to determine if the word sounds similar to a substring of the phrase .The computer readable medium of .claim 31 wherein the third code segment comprises a code segment to produce the substring of the phrase that sounds similar to the word .", "label": "", "metadata": {}, "score": "79.23062"}
{"text": "Instead of constantly opening and closing applications in the on - screen windows of a single desktop computer , a user may stack the tablets around an office and have several of them in use for various tasks at the same time .A networked text processing system according to various aspects of the present invention includes any plurality of tablets networked together to permit exchange of data between them .", "label": "", "metadata": {}, "score": "79.4255"}
{"text": "The computer readable medium of .claim 25 wherein the recognition result comprises a command in the Chinese language .The computer readable medium of .The computer readable medium of .claim 25 wherein the software instructions comprise a fourth code segment to extract the word and the phrase from the recognition result .", "label": "", "metadata": {}, "score": "79.457054"}
{"text": "3A is a functional block diagram of a domain controller in the system of .FIG .2 ; .FIG .3B is a functional block diagram of a message model in the domain controller of .FIG .3A ; .FIG .", "label": "", "metadata": {}, "score": "79.50696"}
{"text": "By permitting alternating speech and pen input using stylus 110 , text processing system 100 provides speech recognition with improved accuracy and editing convenience .To generate text by dictation , user 240 moves stylus 110 toward his or her mouth and begins speaking into microphone 120 .", "label": "", "metadata": {}, "score": "79.58116"}
{"text": "If the most significant bit is a one , the remaining 24 bits are output as video data ( i.e. a pixel ) , but if the most significant bit is a zero , the remaining 24 bits are output as control data and ancillary data .", "label": "", "metadata": {}, "score": "79.63034"}
{"text": "Thus , the score for the state 840 ( S A2 ) equals the likelihood that both of the first and second frames correspond to silence .This process is repeated for subsequent frames ( for example , lines 915 and 920 ) so that the score for the state 840 at a frame \" n \" ( S An ) equals : .", "label": "", "metadata": {}, "score": "79.7692"}
{"text": "As shown in .FIG .13D , the recognizer 215 next misrecognizes a third utterance 1329 ( \" those who pronounce amicus \" ) and the control / interface module 220 responds by inserting the incorrect text 1330 ( \" those who Brown to meet this \" ) in the dictation window 1310 .", "label": "", "metadata": {}, "score": "79.81517"}
{"text": "The first of these two examples illustrates how certain text segments can be expected to be especially useful for indicating context as stop segments .The second example illustrates how certain text segments are less useful for indicating context .The first example of semantic model operation is referred to herein as the \" Coors Uncle \" example .", "label": "", "metadata": {}, "score": "79.910965"}
{"text": "A speech recognition system as claimed in claim 10 , characterized in that the display comprises a cathode ray tube .A speech recognition system as claimed in claim 10 , characterized in that the display comprises a liquid crystal display .A speech recognition system as claimed in claim 1 characterized in that the output means comprises a printer .", "label": "", "metadata": {}, "score": "80.14621"}
{"text": "No .5,855,000 to Waibel et al . .When so constructed , a tablet such as tablets 150 , 252 , 254 , and/or 256 may be used as a \" scrap computer , \" analogous to scrap paper and having limited individualized identity and importance .", "label": "", "metadata": {}, "score": "80.42705"}
{"text": "Tablet 150 may further include suitable conventional hardware ( e.g. , \" glue logic , \" display driving circuitry , battery power supply , etc . )and/or software ( e.g. , operating system , handwriting recognition engine , etc . ) .Tablet 150 may be an \" electronic book \" type of portable computer having a tablet surface and a display integrated into the tablet surface .", "label": "", "metadata": {}, "score": "80.50938"}
{"text": "Accordingly , to extend communication protocols to digital multimedia that includes audio for the purpose of , e.g. , playing digital movies and the like , a protocol referred to as High Definition Multimedia Interface ( HDMI ) has been developed .HDMI is similar to DVI except it envisions the use of audio as well as video data and it adds television - related resolutions .", "label": "", "metadata": {}, "score": "80.526566"}
{"text": "The output device 32 may be , for example , a display , such as a cathode ray tube or liquid crystal display , a printer , a loudspeaker , or a speech synthesizer .FIG .4 is a block diagram of an example of an acoustic processor 14 ( FIG .", "label": "", "metadata": {}, "score": "80.6024"}
{"text": "The person may respond by stating , \" Cat are my favorite animal .\" [ 0023 ] With reference to FIG .1 , the engine 104 contains software operations or routines for assessing non - native spontaneous speech pronunciation .User computers 102 can interact with the engine 104 through a number of ways , such as over one or more networks 108 .", "label": "", "metadata": {}, "score": "80.60781"}
{"text": "The system of . claim 5 , comprising a descrambler receiving data from the deserializer .The system of . claim 5 , wherein the system is implemented by an FPGA configured for processing the HDMI and/or DVI data received from wireless transmission in the 60 GHz band .", "label": "", "metadata": {}, "score": "80.65348"}
{"text": "The system of .claim 1 , wherein the system is further configured to train the particular user message model by updating the acoustic model .The system of .claim 1 , further comprising a proximity sensor configured to sense a proximity of a user to the microphone .", "label": "", "metadata": {}, "score": "80.84479"}
{"text": "5,007,085 , issued Apr. 9 , 1991 to Greanias et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .Further , the detailed description portion ( including referenced drawing figures ) of any U.S. patent or U.S. patent application incorporated by reference into this aforementioned patent is also specifically incorporated herein by reference .", "label": "", "metadata": {}, "score": "81.45416"}
{"text": "4 shows a non - limiting implementation of the receive processor 36 , referred to herein as a \" Receive FPGA \" .The receive FPGA accepts the I and Q data streams , processes the data and outputs 24-bit video .", "label": "", "metadata": {}, "score": "81.47327"}
{"text": "Gunnar Evermann has studentships from the EPSRC and the Cambridge European Trust , and Dan Povey holds a studentship from the Schiff Foundation .The authors are grateful to Thomas Niesler and Ed Whittaker for their help in building the class - based language models .", "label": "", "metadata": {}, "score": "81.494705"}
{"text": "Advantageously , a user message model may be initialized to be in conformance with an adapted local message model .For example , message recognizer 310 may transfer digital indicia of local message model 316 ( after user training ) from its local processing environment to a remote database where user message model 262 is stored .", "label": "", "metadata": {}, "score": "81.70164"}
{"text": "At this frequency the signal has very short range and can be directional such that the video may be transmitted in an uncompressed form such that so much data is transmitted each second that bootlegging the content is essentially untenable .Regardless of the particular application , the present invention makes the following critical observation about 60 GHz wireless links .", "label": "", "metadata": {}, "score": "81.95093"}
{"text": "An .S .An .A .An . m .n .A .Am . . .This expression assumes that the silence node 512 is not reseeded from the root node 505 .If reseeding occurs at a frame n , then the value of S An-1 would be replaced by the score in the root node 505 for the frame n-1 .", "label": "", "metadata": {}, "score": "81.95877"}
{"text": "[ 0024 ] FIG .2 is a block diagram depicting elements used for capturing speech from a person being evaluated .A user 202 may be prompted to speak spontaneously into a microphone , telephone , or other sound receiver 204 .For example , the user 202 may be prompted to discuss their favorite animal .", "label": "", "metadata": {}, "score": "82.11803"}
{"text": "FIGS .1A and 6A , activation circuit 126 is a preferred example of an activation circuit that senses proximity of stylus 110 with respect to the head of a user 240 providing voice input .Activation circuit 126 includes a proximity sensor 124 .", "label": "", "metadata": {}, "score": "82.14702"}
{"text": "The method according to . claim 13 , further comprising training the local message model by adjusting the acoustic model of the local message model and leaving the language model of the local message model unchanged .The method according to . claim 13 , further comprising training the local message model by adjusting the handwriting model of the local message model and leaving the language model of the local message model unchanged .", "label": "", "metadata": {}, "score": "82.40232"}
{"text": "claim 1 , wherein the gap size scoring feature models gap sizes observed in a training corpus as a distribution which is expressed in terms of a parameter of the distribution .The method of . claim 5 , wherein the distribution comprises a Poisson distribution and the parameter of the distribution comprises a mean value of the Poisson distribution .", "label": "", "metadata": {}, "score": "82.451935"}
{"text": "In the following description , we adopt the pinyin system of Romanization when transcribing Chinese characters and include this transcription in parenthesis following the Chinese characters .Additionally , each word in Chinese has an associated tone , which is represented by a number 1 , 2 , 3 , 4 , or 5 attached to the end of the word in the pinyin system .", "label": "", "metadata": {}, "score": "82.472046"}
{"text": "In the Chinese language , words are typically represented by one or more characters from a Chinese syllabary , where each character in the syllabary is a single syllable .Generally , the one or more characters that form a word provide information relating to the meaning and phonetic representation or sound of that word .", "label": "", "metadata": {}, "score": "82.50709"}
{"text": "However , the word BAR also has many synonymous meanings ( e.g. , \" sand bar , \" \" metal bar , \" \" patent bar , \" etc . ) and would have relatively high probability of global cluster occurrence in most corpora , thus limiting the negative bias .", "label": "", "metadata": {}, "score": "82.84142"}
{"text": "1A and 1B are , respectively , a functional block diagram and simplified pictorial view of a single - tablet system responsive to freehand input and voice input , according to various aspects of the present invention ; .FIG .2 is a combination block diagram and simplified pictorial view of a multiple - tablet system responsive to freehand input and voice input , according to various aspects of the present invention ; .", "label": "", "metadata": {}, "score": "83.01531"}
{"text": "m .M . k .N .Z .m . k .An exemplary data structure 1200 for the tabulation of these probabilities may be better understood with reference to .FIG .12 .Data structure 1200 includes tabulations of probabilities relating to three example words UNCLE , COORS , and BEACH of an exemplary subset .", "label": "", "metadata": {}, "score": "83.1113"}
{"text": "Description .This application is a divisional of U.S. patent application Ser .No .12/142,051 , filed Jun. 19 , 2008 , now U.S. Pat .No .7,684,826 which is a continuation of U.S. patent application Ser .No .11/649,086 , filed Jan. 3 , 2007 , now U.S. Pat .", "label": "", "metadata": {}, "score": "83.33168"}
{"text": "The hand - held recorder is connected to the port using a cable connected between the line in port and a line out or speaker port of the recorder .The analog recorder port 126 may be implemented as a microphone positioned so as to be next to the speaker of the hand - held recorder when the recorder is inserted into the port 126 , and also may be implemented using the microphone 102 .", "label": "", "metadata": {}, "score": "83.52694"}
{"text": "A domain controller may include a transfer buffer ( e.g. , a portion of memory ) to facilitate transmission of data from one networked tablet to another .For example , user 242 may cause text data to be copied from tablet 252 into transfer buffer 340 through domain interface 330 .", "label": "", "metadata": {}, "score": "84.06796"}
{"text": "Signal S A may be again asserted ( this time with an \" inactivate \" code ) when stylus 110 is no longer situated to receive voice input .For example , activation circuit 126 may so assert signal S A when user 240 moves stylus 110 away from his or her mouth and toward tablet 150 to begin editing text by freehand input .", "label": "", "metadata": {}, "score": "84.1395"}
{"text": "The exemplary scoring module 24 includes instructions for computing a plurality of features 32 , 34 , 36 , 37 , 38 , 40 , 42 , 44 .It is to be appreciated that the scoring module may employ fewer , more , or different features than the features shown .", "label": "", "metadata": {}, "score": "84.19803"}
{"text": "Field radiation includes acoustic radiation and electromagnetic radiation in the radio , infrared , or visible light spectrum .For example , communications link 115 uses modulated radio transmission to convey information from transmitter 122 to receiver 160 .A transmitter according to various aspects of present invention includes any suitable hardware and/or software for transmitting voice input via a communications link 115 .", "label": "", "metadata": {}, "score": "84.27845"}
{"text": "The non - transitory computer - readable memory of claim 23 , wherein the digitized speech is spontaneous , non - native speech of a non - native language speaker .Description : .[ 0001 ] This application is a continuation of U.S. patent application Ser .", "label": "", "metadata": {}, "score": "84.5358"}
{"text": "The table also shows the effect of giving a factor of three weighting to the CHE training data ( The test set is balanced across Switchboard and Call Home data but the training set is n't and so data weighting attempts to partially correct this imbalance ) .", "label": "", "metadata": {}, "score": "84.73198"}
{"text": "The present assignee has provided a wireless system that functions in the spectrum between 57 GHz and 64 GHz ( hereinafter \" 60 GHz band \" ) .Characteristics of the 60 GHz spectrum include short range , high directivity ( and , hence , inherent security ) , and large data bandwidth .", "label": "", "metadata": {}, "score": "84.73518"}
{"text": "This is an 11 % reduction in WER relative to the CU - HTK evaluation result obtained on the same data set in 1998 ( 39.5 % ) .Note that confusion network output consistently improves performance by about 1 % absolute and that combination of the 4 outputs using confusion network combination ( CNC ) is 0.4 % absolute better than using the ROVER approach .", "label": "", "metadata": {}, "score": "84.945465"}
{"text": "^ .arg .max .t .J .Pr .t .J .s .I . )Pr .t .J .d .K .s .I . )Z .f .I . exp .", "label": "", "metadata": {}, "score": "85.15478"}
{"text": "As discussed below , a user may identify stop segments by any suitable system or method .In the \" Coors Uncle \" example , the user may identify the word COORS as being outside the user context by drawing a double line through it , for example in the first alternative text listing \" Coors Uncle . \"", "label": "", "metadata": {}, "score": "85.26368"}
{"text": "The designation of a profanity or slang term as a stop word outside the user context may cause a semantic model of the invention to bias against other profanities or slang terms .Alternatively , a fiction writer may appreciate the opportunity to designate technical terms as being outside the user context .", "label": "", "metadata": {}, "score": "85.640335"}
{"text": "The control / interface module 220 displays this incorrect result 1316 ( \" There are two kinds of legal cancers \" ) in the dictation window 1310 .The control / interface module also displays the results of recognizing the current utterance , which , in this case , is the second utterance , in a display field 1320 at the bottom of the window 1310 .", "label": "", "metadata": {}, "score": "85.96467"}
{"text": "For example , transmitter 122 couples an RF signal to a conventional dipole antenna having elements 660 , which are oriented parallel to a longitudinal axis of stylus 110 .As a further example , infrared transmitter 760 in stylus 710 couples digital modulation to a conventional solid state infrared emitter 770 .", "label": "", "metadata": {}, "score": "86.119446"}
{"text": "5,862,251 , issued Jan. 19 , 1999 to Al - Karmi et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .Further , the detailed description portion ( including referenced drawing figures ) of any U.S. patent or U.S. patent application incorporated by reference into this aforementioned patent is also specifically incorporated herein by reference .", "label": "", "metadata": {}, "score": "86.12697"}
{"text": "When multiple styluses are included , they need not be of the same type so long as they are all compatible with the system .For example , stylus 210 and stylus 212 may both be of the type of stylus 110 ( .", "label": "", "metadata": {}, "score": "86.17259"}
{"text": "The matrices c k are twenty four by twenty four matrices .Each triphone node may be represented as a mixture of up to , for example , sixteen different PDFs .A particular PDF may be used in the representation of multiple triphone nodes .", "label": "", "metadata": {}, "score": "86.18103"}
{"text": "A non - technical word that appears in a technical document such as a patent application may be designated as a stop word outside the present context , regardless of the author 's overall writing habits .For example , words relating to familial relationships ( including the word UNCLE ) are not typically used in patent applications .", "label": "", "metadata": {}, "score": "86.2352"}
{"text": "The acoustic models 235 represent phonemes .In the case of triphones , the acoustic models 235 represent each triphone node as a mixture of Gaussian probability density functions ( \" PDFs \" ) .For example , node \" i \" of a triphone \" abc \" may be represented as ab i c : . ab .", "label": "", "metadata": {}, "score": "86.27226"}
{"text": "The executive is further quoted as saying that the headset is one of the biggest inhibitors to the acceptance of voice input .Being tethered to a headset is especially inconvenient when the user needs to frequently walk away from the computer .Even a cordless headset can be inconvenient .", "label": "", "metadata": {}, "score": "86.34099"}
{"text": "TABLE 4 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ Prototype Vector Rank Scores _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ time t1 t2 t3 t4 t5 Prototype Vector Identification Value P1 2 1 4 3 4 P2 3 1 3 1 3 P3 5 5 1 4 2 P4 4 3 2 2 1 P5 1 4 5 5 5 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ .", "label": "", "metadata": {}, "score": "86.38427"}
{"text": "The operation of domain controller 220 , in the example , may be better understood with reference to the data flow diagram of .FIG .5 .Process 510 extracts signal components from the electromagnetic signal provided by process 430 .The identification signal identifies the source of the electromagnetic signal as stylus 210 and , consequently , the source of the voice signal as user 242 .", "label": "", "metadata": {}, "score": "86.43825"}
{"text": "As may be better understood with reference to .FIG .6A , for example , stylus 110 includes a tapered pointer 610 , which a user 240 may manually manipulate on or near surface 152 of tablet 150 to convey freehand input .", "label": "", "metadata": {}, "score": "86.61797"}
{"text": "Referring to .FIG . 8B , the node 512 that corresponds to silence is represented by a single state 840 .Each comparison with a frame of parameters may cause a score in the node to remain in the state 840 ( through the path 845 ) and also may cause the score to be passed to the root node 505 through a path 850 .", "label": "", "metadata": {}, "score": "86.635056"}
{"text": "Language model 350 includes a syntactic model 352 and a semantic model 354 .Syntactic model 352 provides a set of a priori probabilities based on a local word context , and may be a conventional N - gram model .Semantic model 354 provides probabilities based on semantic relationships without regard to the particular syntax used to express those semantic relationships .", "label": "", "metadata": {}, "score": "86.82894"}
{"text": "Node 230 establishes communications between domain controller 220 and styluses 210 and 212 .Typically , only a unidirectional communications link is needed to a stylus , because the stylus typically only sends information .Particular types of communications may be employed in communications links 225 and 235 in particular situations .", "label": "", "metadata": {}, "score": "86.97592"}
{"text": "No .5,855,000 , issued Dec. 29 , 1998 to Waibel et al .The detailed description portion ( including referenced drawing figures ) of this aforementioned patent is incorporated herein by reference .Further , the detailed description portion ( including referenced drawing figures ) of any U.S. patent or U.S. patent application incorporated by reference into this aforementioned patent is also specifically incorporated herein by reference .", "label": "", "metadata": {}, "score": "87.27773"}
{"text": "Use of a particular PDF to represent multiple triphone nodes permits the models to include a smaller number of PDFs than would be required if each triphone node included entirely separate PDFs .Since the English language may be roughly represented using 43 different phonemes , there may be up to 79,507 ( 433 ) different triphones , which would result in a huge number of PDFs if each triphone node were represented by a separate set of PDFs .", "label": "", "metadata": {}, "score": "87.49084"}
{"text": "For example , multi - path distortion will affect BER and is a strong function of the environment .There are other factors that impact the decision of which RS code it best , including , for example , the amount of FPGA fabric ( flip - flops ) required to implement the code and the requirement for real - time operation .", "label": "", "metadata": {}, "score": "87.57182"}
{"text": "The Controller 60 can use a 2200 state counter , with the 2200 states being defined by the ten 216-word RS blocks ( 2160 states ) and forty header words .The Controller 60 may output a clock to a Video Clock Analyzer 62 with each pass through the 2200 state counter ( i.e. every 20 uS ) .", "label": "", "metadata": {}, "score": "87.77423"}
{"text": "The digital recorder port 128 may be implemented to transfer a digital file generated using a hand - held digital recorder 130 .This file may be transferred directly into memory 118 , or to a storage device such as hard drive 132 .", "label": "", "metadata": {}, "score": "87.97481"}
{"text": "A character interface is any user interface ( e.g. , conventional keyboard or software user interface simulating a keyboard on a display ) that is responsive to character input .Character input is a selection of specific characters using a character interface , for example by using a stylus to depress keys on a miniature keyboard or to tap areas of a display that represent keys of a simulated keyboard .", "label": "", "metadata": {}, "score": "88.40045"}
{"text": "In the article , electronic pads are characterized as thin note pads with a flat screen on top that a user could scribble on .The pads are intended to be as easy to use as paper .Instead of constantly opening and closing applications in the on - screen windows of a single desktop machine , a user could stack the pads around an office like paper and have all of them in use for various tasks at the same time .", "label": "", "metadata": {}, "score": "88.43429"}
{"text": "Therefore , we explore a class of systems called NN - HMM hybrids , in which neural networks perform acoustic modeling , and HMMs perform temporal modeling .We argue that a NN - HMM hybrid has several theoretical advantages over a pure HMM system , including better acoustic ... . ... t should be a state - based network , as in an HMM .", "label": "", "metadata": {}, "score": "89.03656"}
{"text": "0052 ] A disk controller 1260 interfaces one or more optional disk drives to the system bus 1252 .These disk drives may be external or internal floppy disk drives such as 1262 , external or internal CD - ROM , CD - R , CD - RW or DVD drives such as 1264 , or external or internal hard drives 1266 .", "label": "", "metadata": {}, "score": "89.16493"}
{"text": "claim 1 , further comprising an identification circuit configured to identify an owner of the stylus , wherein the system is configured to select the particular user message model at least in part in response to the identification circuit .The system of .", "label": "", "metadata": {}, "score": "89.61959"}
{"text": "c . k . w . k .N . k .c . k . ) where each w k is a mixture weight , . k . w . k .\u03bc k is a mean vector for the probability density function ( \" PDF \" ) N k , and c k is the covariance matrix for the PDF N k .", "label": "", "metadata": {}, "score": "89.67844"}
{"text": "4A .Other features are based on standard ( typical ) values of native - quality speech data 810 .For example , the duration shift for a vowel S v i is based on the absolute value of the difference between the duration of vowel v i and its standard ( i.e. , typical ) value from native - quality speech data .", "label": "", "metadata": {}, "score": "90.15776"}
{"text": "Domain interface 330 couples message recognizer 310 to tablets 252 , 254 , and 256 through wireless node 227 using any suitable multiple access and/or control protocol .Domain interface 330 conveys text data provided by speech recognizer 312 or handwriting recognizer 314 to a desired one of tablets 252 , 254 , and 256 .", "label": "", "metadata": {}, "score": "90.32155"}
{"text": "Signals S V , S ID , and S A are combined into a single signal 840 .As may be better understood with reference to .FIG .8D , data bursts in signal 840 do not have identical timing to corresponding bursts in originating signals S V , S ID , and S A .", "label": "", "metadata": {}, "score": "90.61958"}
{"text": "1A , 1 B , 6 A , 6 B ) .Alternatively , stylus 210 and stylus 212 may be of different types .For example , stylus 210 may be of the type of stylus 110 and stylus 212 may be of the type of stylus 710 ( .", "label": "", "metadata": {}, "score": "90.8087"}
{"text": "User 242 may then caused text data to be pasted from transfer buffer 340 to tablet 252 .Again using an editing command from the set provided in TABLE II , the user may cause a block of displayed text data to be pasted by using stylus 210 to double - tap at the desired insertion point on the display of tablet 254 .", "label": "", "metadata": {}, "score": "91.598526"}
{"text": "Because it would be very unusual for the Applicant to write a document relating to the consumption of alcoholic beverages , words relating to alcohol consumption ( e.g. , beer , brewing , etc . ) are unlikely to appear in any document the Applicant generates .", "label": "", "metadata": {}, "score": "91.93683"}
{"text": "Thus , the score for the state 840 ( S A1 ) is set equal to the likelihood that the first frame corresponds to silence .Retrieval of the first frame also causes the state 805 to become an active state .Thus , the score for the state 805 ( S B1 ) is set equal to the likelihood that the first frame corresponds to the state 805 .", "label": "", "metadata": {}, "score": "92.13869"}
{"text": "In any case , the media receiver 16 and media player 42 and respective components preferably are co - located in the same space , owing to the preferred 60 GHz wireless transmission frequency , which can not penetrate walls .Also , because the multimedia is never decrypted in the wireless connection established between the DVI components 20 , 38 inclusive , little or no licensing concerns are implicated .", "label": "", "metadata": {}, "score": "92.2047"}
{"text": "For example , system 200 includes a network node 232 for establishing a network connection link 237 between a plurality ( e.g. , two ) of user message models 264 and 262 .When link 237 is established , local message model 316 may be initialized to be in conformance with message model 264 and/or message model 262 , as discussed below .", "label": "", "metadata": {}, "score": "92.25282"}
{"text": "\" If the word BEACH is outside the user context , the user could select or say \" Always .\" For optimal configuration of a message recognizer to automatically initiate a context definition mode , a number of factors should be considered and weighed .", "label": "", "metadata": {}, "score": "92.30321"}
{"text": "Additional entries may be accessed using a scroll bar 1333 .As shown in .FIG .13F , the user selects the word \" Brown \" 1335 using the mouse 110 .As noted above , the user could also select the word \" Brown \" by using the voice command \" Select Brown \" .", "label": "", "metadata": {}, "score": "92.87898"}
{"text": "FIG .15 ( including .FIGS .15A and 15B ) .Tablet 252 is illustrated in .FIG .15A as displaying a first view of a graphical document 1500 .The first view shows a circle 1510 and a rectangle 1520 .", "label": "", "metadata": {}, "score": "92.894356"}
{"text": "The first condition is that the activation signal from process 410 has been positively asserted to indicate that stylus 210 is situated to receive voice input from user 242 .The second condition is that user 242 has provided voice input by speaking into the microphone of stylus 210 .", "label": "", "metadata": {}, "score": "93.48397"}
{"text": "Is the word BEACH of context ? \" and three \" radio buttons \" labeled \" No , \" \" Yes , this time , \" and \" Always .\" If the word BEACH is not out of context , the user could select ( or say ) \" No .", "label": "", "metadata": {}, "score": "94.28842"}
{"text": "Condition .A : .S .T .Condition B : .S .S .S .T .When microphone 120 is at a desired proximity to the head of user 240 , infrared radiation from the head of user 240 occupies most of angular range AR 2 .", "label": "", "metadata": {}, "score": "94.41481"}
{"text": "la vaisselle \" in the sentence \" On peut faire beaucoup d'argent en important en france de la vaisselle chinoise \" ) .However , other features could be utilized to account for such unlikely gap sizes .As will be appreciated , an object of any translation model is to provide accurate translations and each added feature may contribute to this accuracy to greater or lesser extents .", "label": "", "metadata": {}, "score": "94.5547"}
{"text": "For example , given a digitized speech sample that actually says , \" Cat are my favorite animal , \" the speech recognition 504 may identify the word hypotheses \" Cat are why favorite animal .\" The speech recognition 504 may further output confidence levels of : 99 % , 95 % , 62 % , 87 % , and 99 % , respectively , for each of the five hypothesis words , for example .", "label": "", "metadata": {}, "score": "94.7823"}
{"text": "2 illustrates components of the speech recognition software 124 .For ease of discussion , the following description indicates that the components carry out operations to achieve specified results .However , it should be understood that each component actually causes the processor 112 to operate in the specified manner .", "label": "", "metadata": {}, "score": "95.472885"}
{"text": "Tablet 254 is illustrated in .FIG .15B as displaying a second view of document 1500 .The second view shows a different portion of document 1500 , namely the portion inside bounding box 1530 .Cutout 1512 takes up most of the display of tablet 254 .", "label": "", "metadata": {}, "score": "95.79699"}
{"text": "The staying penalty is added to any score that stays in the state .The staying penalty is related inversely to the length of the sound represented by the state and to the length of the phoneme represented by the node to which the state belongs .", "label": "", "metadata": {}, "score": "95.82158"}
{"text": "w .i . ) k .M .[ .Pr .w .i .C . k . )A .j .L .B .j .Pr . s .j .C . k . ) .", "label": "", "metadata": {}, "score": "96.09045"}
{"text": "FIG .13 G , the user then says \" p r o n \" 1340 to indicate that the word Brown should be replaced with a word starting with the letters \" pron \" .The user could achieve the same result by typing the letters \" pron \" .", "label": "", "metadata": {}, "score": "97.62483"}
{"text": "Therefore , the speech recognition systems described above may fail to correctly distinguish homophones in the Chinese language .Although the Chinese language does not have an alphabet , the sounds of the Chinese characters may be transcribed or transliterated into the Roman alphabet or another alphabet .", "label": "", "metadata": {}, "score": "98.933586"}
{"text": "Process 540 performs handwriting recognition , also in accordance with initialized message model 316 , to generate text data responsive to freehand input from user 242 .Process 540 further interprets editing commands from user 242 responsive to freehand input .User 242 conveys freehand input by manipulating stylus 210 on a tablet surface ( not specifically shown ) of tablet 252 .", "label": "", "metadata": {}, "score": "99.39042"}
{"text": "Preferably , microphone 120 is disposed toward the proximal ( i.e. , upward in operation ) end of stylus 110 and is at least somewhat directional to improve quality of voice input when pointed toward the mouth of user 240 .When directional , microphone 120 is responsive to sound impinging on stylus 110 from the direction of its proximal end with a greater sensitivity than from its distal end .", "label": "", "metadata": {}, "score": "99.75729"}
{"text": "The user selects the correct word 1327 ( \" kibitzers \" ) by saying \" Choose 4 \" , where \" kibitzers \" is the fourth word on the choice list .As shown in .FIG .13C , the control / interface module 220 responds by replacing the incorrect word ( \" cancers \" ) with the selected word 1327 in the dictation window 1310 .", "label": "", "metadata": {}, "score": "100.32317"}
{"text": "Process 520 may employ any appropriate network protocol to transfer digital indicia of user message model 262 . )Not shown in .FIG .5 is a process for initializing user message model 262 to be in conformance with local message model 316 after adaptation of local message model 316 .", "label": "", "metadata": {}, "score": "101.13359"}
{"text": "Pr .w .i . )Pr .w .i .L .M . syn . ) j .M .[ .B .j .Pr .w .i . s .j .V . )Pr .", "label": "", "metadata": {}, "score": "101.20689"}
{"text": "FIG .1B to support and control tablet 150 with his or her left hand while manipulating stylus 110 with the right hand .If user 240 is one - handed , he or she may prefer to rest tablet 150 on a supporting surface and use stylus 110 for both freehand input and for depressing buttons 154 and 156 .", "label": "", "metadata": {}, "score": "101.808266"}
{"text": "Standard robust statistics may be used for treating \" atypical \" instances as outliers .For example , the target sentence \" He did not catch the train to Paris this week because he had no desire to travel by train anymore \" may be observed as including the non - contiguous phrase not_anymore with a gap size of 16 .", "label": "", "metadata": {}, "score": "101.81102"}
{"text": "User 240 need not be tethered to a head - mounted microphone ; he or she already has microphone 120 in hand when grasping stylus 110 .When positioned near the mouth of user 240 , microphone 120 is typically able to receive high quality voice input comparable to that received by a head - mounted microphone .", "label": "", "metadata": {}, "score": "102.66051"}
{"text": "The following discussion assumes that there are no leaving or staying penalties associated with the state 840 or the state 860 .The same result could be achieved by setting the leaving and staying penalties for states 840 and 860 equal to zero .", "label": "", "metadata": {}, "score": "104.552826"}
{"text": "For example , while the phrase \" are my favorite \" may be relatively common in the non - native training text 622 , the phrase \" are why favorite \" is much less common .Thus , the language model 620 can determine that the confidence score 610 for the hypotheses \" why \" in the set of hypotheses \" Cat are why favorite animal \" should be relatively low .", "label": "", "metadata": {}, "score": "106.511925"}
{"text": "In addition , the word will be written into the user - specific backup vocabulary .Then , if the user says \" ganglion \" while using a legal topic , the word \" ganglion \" will be available during correction from the backup dictionary .", "label": "", "metadata": {}, "score": "106.8672"}
{"text": "User 240 may find it natural and intuitive to move stylus 110 away from tablet surface 152 , and toward his or her mouth , while reviewing displayed text and preparing to generate additional text by dictation .No significant change in the orientation of stylus 110 is necessary because microphone 120 typically remains aimed toward the mouth of user 240 .", "label": "", "metadata": {}, "score": "107.43291"}
{"text": "When tablet 150 is positioned for comfortable use , for example resting on the thigh portion of a crossed leg of user 240 , only one hand is needed for support and control of tablet 150 .Typically , user 240 may then use an opposite hand to manipulate stylus 110 on surface 152 of tablet 150 .", "label": "", "metadata": {}, "score": "107.57177"}
{"text": "4 .Process 410 determines activation status of stylus 210 .When user 242 moves stylus 210 toward his or her head to begin dictation , process 410 positively asserts an activation signal to process 420 .Process 420 provides a voice signal responsive to voice input from user 242 .", "label": "", "metadata": {}, "score": "107.94139"}
{"text": "The signal produced by the user 202 speaking into the microphone 204 is provided to a computer 205 containing an analog to digital ( A / D ) converter 206 that converts the analog signal received from the microphone 204 into a digital representation of that signal .", "label": "", "metadata": {}, "score": "108.00502"}
{"text": "A tablet according to various aspects of the present invention may include user controls for convenient navigation in an electronic document .For example , tablet 150 includes a \" Page Up \" button 154 and a \" Page Down \" button 156 conveniently positioned together near the edge of tablet 150 .", "label": "", "metadata": {}, "score": "108.1155"}
{"text": "After receiving digitized speech 302 of a non - native speaker stating , \" Cat are my favorite animal , \" speech recognition 304 may output that same phrase , for example .However , speech recognition is not perfect , and the speech recognition 304 may output a different phase , such as \" Cat are why favorite animal , \" as the word hypotheses 306 of the words contained within the digitized speech 302 .", "label": "", "metadata": {}, "score": "109.440765"}
{"text": "When tablet 150 is positioned for comfortable use , for example resting on the thigh portion of a crossed leg of user 240 , the distance between tablet surface 152 and the head of user 240 may be only about twice the length of stylus 110 .", "label": "", "metadata": {}, "score": "115.32033"}
