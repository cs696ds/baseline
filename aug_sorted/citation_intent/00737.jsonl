{"text": "WSJ data ( Petrov and Klein , 2007 ; Foster , 2010 ) .The parser uses the English signature list described in Attia et al ( 2010 ) to assign partof - speech tags to unknown words . \" ...Domain adaptation is an important task in order for NLP systems to work well in real applications .", "label": "", "metadata": {}, "score": "39.7759"}
{"text": "SCFGs can easily be used for the random generation of combinatorial objects according to the probability distribution induced by a sample set , where the only problem is that they do not allow the user to fix the length of generated structures .", "label": "", "metadata": {}, "score": "40.122047"}
{"text": "For example , our clus- tering algorithm grouped first names in one group and measurements in another .We then added the cluster membership as a lexical feature to the parser .None of the resulting features helped adaptation .3.2Diversity Training diversity may be an effective source for adaptation .", "label": "", "metadata": {}, "score": "40.157104"}
{"text": "We tried a number of criteria to weigh sentences without suc- cess , including sentence length and number of verbs .Next , we trained a discriminative model on the pro- vided unlabeled data to predict the domain of each sentence based on POS n - grams in the sentence .", "label": "", "metadata": {}, "score": "40.319176"}
{"text": "L .For details , see [ 20 ] and the Appendix .Furthermore , it should be mentioned that we decided to assign relative frequencies to the production rules of .sto . , since such probabilities can be computed efficiently for unambiguous SCFGs .", "label": "", "metadata": {}, "score": "41.046417"}
{"text": "The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .To answer the question , we extend the parsing prediction model in ( Ravi et al . , 2008 ) to provide prediction for word segmentation and POS tagging as well .", "label": "", "metadata": {}, "score": "42.226185"}
{"text": "L .G . ) conditioned on any fixed size n is kept within the new framework .A well - known approach which allows for 1 ) is connected to the concept of admissible constructions used to describe a decomposable combinatorial class ( see above ) .", "label": "", "metadata": {}, "score": "42.338528"}
{"text": "However , as \u03f5 -freeness and loop - freeness are required preliminarily , we have to consider another unambiguous SCFG generating the same language .L . , where we have to guarantee that the same substructures are distinguished as are distinguished in .", "label": "", "metadata": {}, "score": "42.361908"}
{"text": "However , our results were obtained without adap- tation .Given our position in the ranking , this sug- gests that no team was able to significantly improve performance on either test domain beyond that of a state - of - the - art parser .", "label": "", "metadata": {}, "score": "42.408127"}
{"text": "As already mentioned , stochastic context - free grammars ( SCFGs ) are a powerful tool for modeling combinatorial classes and the essence of the non - uniform random sampling approach that will be worked out in this article .Therefore , we will now give the needed background information .", "label": "", "metadata": {}, "score": "43.034782"}
{"text": "The interested reader is referred to the corresponding section in [ 21 ] .For a more fundamental introduction on stochastic context - free languages , see for example [ 39 ] .In fact , the only information needed in the sequel is that if structures are modeled by a consistent SCFG , then the probability distribution on the production rules of the SCFG implies a probability distribution on the words of the generated language and thus on the modeled structures .", "label": "", "metadata": {}, "score": "44.27307"}
{"text": "There , it is described how to get algorithms for the random generation of objects of a previously fixed size according to an arbitrary ( non - uniform ) distribution implied by a given SCFG .Essentially , in [ 20 ] , a new admissible construction called weighting has been introduced in order to make non - uniform random generation possible .", "label": "", "metadata": {}, "score": "44.707237"}
{"text": ", 2010a ) , a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures .These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .", "label": "", "metadata": {}, "score": "44.769848"}
{"text": "( 1993 ) .In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which requ ... \" .We introduce a discriminatively trained , globally normalized , log - linear variant of the lexical translation models proposed by Brown et al .", "label": "", "metadata": {}, "score": "45.149742"}
{"text": ", 2009 ; Biber & Gray , 2010 ) , but the most interesting usages apply the divergence to a machine learning system .Despite the fact that authors have shown that a divergence ( Van Asch & Daelemans , 2010 ; Plank , 2011 ) or a linear combination of divergences ( McClosky , 2010 ) can be successfully used to link the sim ... . \" ...", "label": "", "metadata": {}, "score": "45.268368"}
{"text": "This paper outlines our participation in the 2007 CoNLL Shared Task on Domain Adaptation ( Nivre et al . , 2007 ) .The goal was to adapt a parser trained on a single source domain to a new target domain us- ing only unlabeled data .", "label": "", "metadata": {}, "score": "45.790085"}
{"text": "In addition to a change in the annotation guide- lines for NPs , we observed an important difference in the distribution of POS tags .NN tags were almost twice as likely in the BIO domain ( 14 % in WSJ and 25 % in BIO ) .", "label": "", "metadata": {}, "score": "45.790436"}
{"text": "In fact , an SCFG is derived by equipping the productions of a corresponding CFG with probabilities such that the induced distribution on the generated language models as closely as possible the distribution of the sample data .The needed formalities are given as follows : .", "label": "", "metadata": {}, "score": "45.830357"}
{"text": "Let us consider the SCFG .G .d . , which contains the following rules : . w .S .B . w .B .B . ) w .B .C .w .C .w .", "label": "", "metadata": {}, "score": "45.869118"}
{"text": "Notably , a complete structure of size n is generated by recursively unranking the distinct structural components from the corresponding subclasses ( of substructures with sizes less than n ) .In our case , the weighted unranking algorithm requires a precomputation step in worst - case time .", "label": "", "metadata": {}, "score": "46.111004"}
{"text": "Tokenization : The given text is divided into tokens so that they can be used for further analysis .The tokens may be words , punctuation marks , and utterance boundaries .Ambiguity look - up : This is to use lexicon and a guessor for unknown words .", "label": "", "metadata": {}, "score": "46.44193"}
{"text": "For unambiguous SCFGs , the relative frequencies can actually be counted efficiently , as for every word , there is only one leftmost derivation to consider .Modeling RNA Secondary Structure via SCFGs .Besides the popular planar graph representation of unknotted secondary structures , many other ways of formalizing RNA folding have been described in literature .", "label": "", "metadata": {}, "score": "47.193657"}
{"text": "n . for computing all weighted class sizes up to input size n .The worst - case complexity for generating a secondary structure of size n at random is then given by .O .n . log .n . ) since we are ranking structures according to the boustrophedon order ( see e.g. [ 7 ] ) .", "label": "", "metadata": {}, "score": "47.239334"}
{"text": "Our suspicions are supported by the observation that no team was able to im- prove target domain performance substan- tially over a state of the art baseline . 1 Introduction Dependency parsing , an important NLP task , can be done with high levels of accuracy .", "label": "", "metadata": {}, "score": "47.352287"}
{"text": "Many studies have indeed shown that cross - domain learned corpora yield poor language models [ 35].The field of domain adaptation attempts to compensate for the poor quality of cross - domain data , by adding carefully picked text from other domains [ 36,37 ] or other statistical mitigation techniques .", "label": "", "metadata": {}, "score": "47.55971"}
{"text": "s .L .n . ) , no structures are rejected .Hence , in this case , the corresponding random sample corresponds to the usual ( unrestricted ) output of our algorithm .The Wilcoxon test results for our native sample together with any of a number of random sample sets generated in the previously described restricted manner , respectively , can be found in Table 4 .", "label": "", "metadata": {}, "score": "47.684715"}
{"text": "We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al ., 2010a ... \" .Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .", "label": "", "metadata": {}, "score": "47.80729"}
{"text": "Prior Results and Basic Definitions .Uniform Random Generation .In the past , the problem of uniform random generation of combinatorial structures , that is the problem of randomly generating objects ( of a preliminary fixed input size ) of a specified class that have the same or similar properties , has been extensively studied .", "label": "", "metadata": {}, "score": "47.841385"}
{"text": "Since most of these systems were developed and tested using data from the WSJ corpus , we compare their generalization abilities by testing on both WSJ and the multilingual Multext - East corpus .Finally , we introduce the idea of evaluating systems based on their ability to produce cluster prototypes that are useful as input to a prototype - driven learner .", "label": "", "metadata": {}, "score": "47.846584"}
{"text": "This means that any structural feature is modeled by one or more specific grammar rules with corresponding probabilities observed from real - life data .According to [ 20 ] , our sampling method involves a weighted unranking algorithm for obtaining the final structures .", "label": "", "metadata": {}, "score": "47.858658"}
{"text": "w .C .w .C .w .C . 2 . . .As described earlier , this specification ( with weighted classes ) derived from reweighted grammar .G .d . transforms immediately into a recursion for the function size of all needed combinatorial classes .", "label": "", "metadata": {}, "score": "47.92544"}
{"text": "When Brown et al .( 1993 ) wan ... . \" ...We consider a new subproblem of unsupervised parsing from raw text , unsupervised partial parsing - the unsupervised version of text chunking .We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current ... \" .", "label": "", "metadata": {}, "score": "47.93158"}
{"text": "Since these annotations are new with respect to the WSJ guidelines , it is impossi- ble to parse these without injecting knowledge of the annotation guidelines.3 common , comprising 33 % of BIO and 30 % of WSJ tokens , the most popular POS tag by far .", "label": "", "metadata": {}, "score": "48.400833"}
{"text": "In what follows , we provide an er- ror analysis that attributes domain loss for this task to a difference in annotation guidelines between do- mains .We then overview our attempts to improve adaptation .While we were able to show limited adaptation on reduced training data or with first- order features , no modifications improved parsing with all the training data and second - order features .", "label": "", "metadata": {}, "score": "48.84218"}
{"text": "However , most ... \" .It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .", "label": "", "metadata": {}, "score": "49.095833"}
{"text": "To address conflicting annota-1053 .Page 4 . tions , we added slack variables to the MIRA learn- ing algorithm ( Crammer et al . , 2006 ) used to train the parsers , without success .We measured diversity by comparing the parses of each model .", "label": "", "metadata": {}, "score": "49.105198"}
{"text": "In the Bioinformatics area , algorithms for generating random biological sequences have been investigated for a long time ( see e.g. [ 2 , 3 ] ) .As stated in [ 4 ] , random sequences are a topic of great interest in genome analysis , since according to a powerful paradigm , they represent the background noise from which the actual biological information must differentiate .", "label": "", "metadata": {}, "score": "49.295227"}
{"text": "We observe redundancy levels of about 30 % and non - standard distribution of both words and concepts .We measure the impact of redundancy on two standard text - mining applications : collocation identification and topic modeling .We compare the results of these methods on synthetic data with controlled levels of redundancy and observe significant performance variation .", "label": "", "metadata": {}, "score": "49.303513"}
{"text": "L .G . ) , such that we generate a word of arbitrary size .In order to fix the size , we can proceed along the following lines : .We translate the grammar .G . into a new framework which allows to consider fixed sizes for the random generation , such that .", "label": "", "metadata": {}, "score": "49.308445"}
{"text": "These methods require labeled examples of syntactic structures to learn statistical patterns governing these structures .Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermo ... \" .Current efforts in syntactic parsing are largely data - driven .", "label": "", "metadata": {}, "score": "49.59335"}
{"text": "Results .In this article , we present a new general framework for deriving algorithms for the non - uniform random generation of combinatorial objects according to the encoding and probability distribution implied by a stochastic context - free grammar .Briefly , the framework extends on the well - known recursive method for ( uniform ) random generation and uses the popular framework of admissible specifications of combinatorial classes , introducing weighted combinatorial classes to allow for the non - uniform generation by means of unranking .", "label": "", "metadata": {}, "score": "49.652542"}
{"text": "Afterwards , we must repeat the process , as there is still one non - terminal symbol left .Unfortunately , there is one major problem that comes with this approach for the ( non - uniform ) random generation of combinatorial objects : The underlying ( consistent ) SCFG .", "label": "", "metadata": {}, "score": "49.6707"}
{"text": ".. our corpus .The second question is how one can estimate NLP systems ' performance when gold standard on the test data does not exist .Our experiments show that the predicted scores are close to the real scores when tested on the CTB data .", "label": "", "metadata": {}, "score": "49.671112"}
{"text": "Moreover , there exists a one - to - one correspondence between both representations , as illustrated by the following example : .Example 0.1 .The secondary structure shown in Figure 1 has the following equivalent bar - bracket representation that can be decomposed into subwords corresponding to the basic structural motifs that are distinguished in state - of - the - art thermodynamic models : .", "label": "", "metadata": {}, "score": "49.703167"}
{"text": "First , we have to find a suitable SCFG that generates .and models the distribution of the sample data as closely as possible .To reach this goal , it is important to appropriately specify the set of production rules in order to guarantee that all substructures that have to be distinguished are generated by different rules .", "label": "", "metadata": {}, "score": "49.77192"}
{"text": "Text - mining methods in particular can help disease modeling by mapping named - entities mentions to terminologies and clustering semantically related terms .EHR corpora , however , exhibit specific statistical and linguistic characteristics when compared with corpora in the biomedical literature domain .", "label": "", "metadata": {}, "score": "50.00228"}
{"text": "Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as paraphrasing , text entailment , and machine translation .We first apply chunking to raw corpora and then extract reliable chunks to ensure that high - quality predicate - argument structures are obtained from the chunks .", "label": "", "metadata": {}, "score": "50.00978"}
{"text": "We added features indicating when an edge was predicted by another parser and if an edge crossed a predicted edge , as well as conjunctions with edge types .This failed to improve BIO accuracy since these features were less reliable at test time .", "label": "", "metadata": {}, "score": "50.0111"}
{"text": "No additional knowledge about the target domain is included .A more realistic approach assumes that only raw text from the target domain is available .This assumption lends itself well to semi - supervised learning methods since these utilize both labeled and unlabeled examples .", "label": "", "metadata": {}, "score": "50.03221"}
{"text": "First , note that in [ 21 ] , to obtain the stochastic model for RNA secondary structures derived from real - world RNA data , the following unambiguous SCFG which unambiguously generates exactly the language .L .given in Definition 0.9 has been used : .", "label": "", "metadata": {}, "score": "50.10717"}
{"text": "Derivation of the Algorithm .The elaborate SCFG .sto .Then , we can use the resulting weighted class sizes for the straightforward construction of the desired unranking algorithm .However , to improve the worst - case complexity of the resulting unranking procedure from .", "label": "", "metadata": {}, "score": "50.26058"}
{"text": "It is well known that parsing accuracy suffers when a model is applied to out - of - domain data .It is also known that the most beneficial data to parse a given domain is data that matches the domain ( Sekine , 1997 ; Gildea , 2001 ) .", "label": "", "metadata": {}, "score": "50.410454"}
{"text": "Note that secondary structure prediction methods based on random sampling represent a non - deterministic counterpart to the up - to - date most successful and popular physics - based prediction methods that make use of the energy minimization paradigm and are realized by dynamic programming algorithms ( see e.g. [ 9 - 12 ] ) .", "label": "", "metadata": {}, "score": "50.41427"}
{"text": "For example , the model might prefer noun analyses over verb analyses if the preceding word is a preposition or article .Disambiguation is the most difficult problem in tagging .Applications of POS tagger .The POS tagger can be used as a preprocessor .", "label": "", "metadata": {}, "score": "50.488335"}
{"text": "For this purpose , random sequences must obviously obey to a certain model that takes into account some relevant properties of actual real - life sequences , where such models are usually based on statistical parameters only .However , it is known that these classical models can be enriched by adding structural parameters ( see [ 4 ] ) .", "label": "", "metadata": {}, "score": "50.495518"}
{"text": "This underlines the suggestion made in [ 21 ] that , although both energy models have been proven to be realistic , due to the more realistic variation of free energies connected to varying loop length , the dynamic model should be used for possible applications .", "label": "", "metadata": {}, "score": "50.640995"}
{"text": "However , this is not at all obvious with respect to ambiguity of the grammar .Definition 0.10 .The unambiguous \u03f5 -free SCFG .G .^ .sto . generating exactly the language .L . is given by .", "label": "", "metadata": {}, "score": "50.65702"}
{"text": "( B ) C obviously is generated in a unique way ; this resembles .L .L .u .L .l .u . and .l .u .l . )u . of .L .", "label": "", "metadata": {}, "score": "50.699028"}
{"text": "The unambiguous SCFG .G . sto . generating exactly the language .L . is given by .G . sto .I .G . sto .G . sto .R .G . sto .S . ) , where .", "label": "", "metadata": {}, "score": "50.901913"}
{"text": "n . ) , where a preprocessing time of .O .n . is required for the computation of all ( weighted ) class sizes up to input length n .Availability of Software .Furthermore , our Mathematica source code used to implement the webservice can be downloaded from our website and used under GNU public licence .", "label": "", "metadata": {}, "score": "51.237354"}
{"text": "We show that addressing this task directly , using probabilistic finite - state methods , produces better results than relying on the local predictions of a current best unsupervised parser , Seginer 's ( 2007 ) CCL .These finite - state models are combined in a cascade to produce more general ( full - sentence ) constituent structures ; doing so outperforms CCL by a wide margin in unlabeled PARSEVAL scores for English , German and Chinese .", "label": "", "metadata": {}, "score": "51.35008"}
{"text": "To confirm that nouns were problem- atic , we modified a first - order parser ( no second or- der features ) by adding a feature indicating correct noun - noun edges , forcing the parser to predict these edges correctly .", "label": "", "metadata": {}, "score": "51.63644"}
{"text": "G .A small example that shows how to proceed from SCFG to reweighted normal form and the corresponding weighted combinatorial classes which allow for non - uniform generation by means of unranking is discussed in the Appendix .Generating Random RNA Secondary Structures .", "label": "", "metadata": {}, "score": "51.642418"}
{"text": "In our model , arbitrary , nonindependent features may be freely incorporated , thereby overcoming the inherent limitation of generative models , which require that features be sensitive to the conditional independencies of the generative process .However , unlike previous work on discriminative modeling of word alignment ( which also permits the use of arbitrary features ) , the parameters in our models are learned from unannotated parallel sentences , rather than from supervised word alignments .", "label": "", "metadata": {}, "score": "51.655945"}
{"text": "Tagset .Tagset is the set of tags from which the tagger is supposed to choose to attach to the relevant word .Every tagger will be given a standard tagset .Most of the taggers use only fine grained tagset .", "label": "", "metadata": {}, "score": "51.67319"}
{"text": "In fact , it is known for a long time that SCFGs can be used to model RNA secondary structures ( see e.g. [ 40 ] ) .Additionally , SCFGs have already been used successfully for the prediction of RNA secondary structure [ 14 , 15 ] .", "label": "", "metadata": {}, "score": "51.759407"}
{"text": "Listed alphabetically . ' ' 'Painless Unsupervised Learning with Features .NAACL 2010 . ]Two Decades of Unsupervised POS induction : How far have we come ?In Proceedings of EMNLP 2010 . ] Learning Syntactic Categories Using Paradigmatic Representations of Word Context .", "label": "", "metadata": {}, "score": "51.90101"}
{"text": "Thus , within a longitudinal patient record , one expects to observe heavy redundancy .In this paper , we ask three research questions : ( i ) How can redundancy be quantified in large - scale text corpora ?( ii )", "label": "", "metadata": {}, "score": "51.94463"}
{"text": "Further experiments showed that any decrease in training data hurt parser perfor- mance .It would seem that the parser has no dif- ficulty learning important training sentences in the presence of unimportant training examples .A related idea focused on words , weighing highly tokens that appeared frequently in the target domain .", "label": "", "metadata": {}, "score": "52.040638"}
{"text": "D09 - 1025 [ bib ] : Marco Pennacchiotti ; Patrick Pantel Entity Extraction via Ensemble Semantics .D09 - 1026 [ bib ] : Daniel Ramage ; David Hall ; Ramesh Nallapati ; Christopher D. Manning Labeled LDA : A supervised topic model for credit attribution in multi - labeled corpora .", "label": "", "metadata": {}, "score": "52.047615"}
{"text": "Our er- ror analysis suggests that the primary cause of loss from adaptation is from differences in the annotation guidelines themselves .Therefore , significant im- provements can not be made without specific knowl- edgeofthetargetdomain'sannotationstandards .No amount of source training data can help if no rele- vant structure exists in the data .", "label": "", "metadata": {}, "score": "52.092834"}
{"text": "We began with the first ap- proach and removed a large number of features that we believed transfered poorly , such as most features for noun - noun edges .We obtained a small improve- ment in BIO performance on limited data only .", "label": "", "metadata": {}, "score": "52.195183"}
{"text": "Consequently , secondary structures without pseudoknots can be encoded as words of a context - free language and the class of all feasible structures can thus effectively be modeled via a corresponding CFG .Basically , that CFG can be constructed to describe a number of classical constraints ( e.g. the presence of particular motifs in structures ) and it can also express long - range interactions ( e.g. base pairings ) .", "label": "", "metadata": {}, "score": "52.216812"}
{"text": "These fine grained tags provide more information than coarse tags ; experiments that removed fine grained tags 1052 .Page 3 . hurt WSJ performance but did not affect BIO .Finally , we examined the effect of unknown words .Not surprisingly , the most significant dif- ferences in error rates concerned dependencies be- tween words of which one or both were unknown to the parser .", "label": "", "metadata": {}, "score": "52.29309"}
{"text": "B . )C .In fact , in the case of significant differences of the new probabilities ( w 5.1.1 , ... , w 5.1.4 and w 5.2 ) , we can expect a huge improvement in the model 's accuracy .", "label": "", "metadata": {}, "score": "52.321457"}
{"text": "Labeled data typically requires expert annotators which makes it both time consuming and costly to produce .Furthermore , once training data has been created for one textual domain , portability to similar domains is limited .This domain - dependence has inspired a large body of work since syntactic parsing aims to capture syntactic patterns across an entire language rather than just a specific domain .", "label": "", "metadata": {}, "score": "52.61647"}
{"text": "We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .Our method does not assume any knowledge about the target language ( in particular no tagging dictionary is assumed ) , making it applicable to a wide array of resource - poor languages .", "label": "", "metadata": {}, "score": "52.630035"}
{"text": "proposed an ensemble method ( Reichart and Rappoport , 2007 ) .They regarded parses as being of high quality if 20 different parsers agreed .They used an SVM regression approach on the basis of text - based and parse - based features .", "label": "", "metadata": {}, "score": "52.774925"}
{"text": "B . )C .A . w .B .C .w .B .C .A . w .C .w .C .C . . .This grammar unambiguously generates .L .for the following reasons : .", "label": "", "metadata": {}, "score": "52.9935"}
{"text": "The prime advantage of standard specifications is that they translate directly into procedures for computing the sizes of all combinatorial subclasses of the considered class .C . of combinatorial objects .This means they can be used to count the number of structures of a given size that are generated from a given non - terminal symbol .", "label": "", "metadata": {}, "score": "53.011684"}
{"text": "A weighted context - free grammar ( WCFG ) is a 5-tuple .G .I .T .R .S .W . , where I ( resp .T ) is an alphabet ( finite set ) of intermediate ( resp .", "label": "", "metadata": {}, "score": "53.04672"}
{"text": "Their symbolic component is amenable to inspection by humans , while their probabilistic component helps resolve ambiguity .They also permit the use of well - understood , generalpurpose learn ... \" .Probabilistic grammars offer great flexibility in modeling discrete sequential data like natural language text .", "label": "", "metadata": {}, "score": "53.053276"}
{"text": "D09 - 1161 [ bib ] : Hui Zhang ; Min Zhang ; Chew Lim Tan ; Haizhou Li K - Best Combination of Syntactic Parsers \" ...We show how web mark - up can be used to improve unsupervised dependency parsing .", "label": "", "metadata": {}, "score": "53.118187"}
{"text": "n . . .Note that in this context of unranking particular elements from a considered structure class , the corresponding algorithms make heavy use of their decomposability , as the distinct structural components are unranked from the corresponding subclasses .In fact , the class sizes can be derived according to the following recursion : . size ( .", "label": "", "metadata": {}, "score": "53.18377"}
{"text": "Indeed , unless you find an existing system that fulfills your needs , some effort will be required !Unsupervised approaches may help you bootstrapping a system , so as to see if you need to find / annotate a dedicated corpus .", "label": "", "metadata": {}, "score": "53.216408"}
{"text": "( 2006 ) , which achieved top results in the 2006 We were given around CoNLL - X shared task .Preliminary experiments in- dicated that the edge labeler was fairly robust to do- main adaptation , lowering accuracy by 3 % in the de- velopment domain as opposed to 2 % in the source , so we focused on unlabeled dependency parsing .", "label": "", "metadata": {}, "score": "53.279877"}
{"text": "n .A .n .B .Considering .C .n . , the actual unranking algorithms are quite straightforward .Therefore , they will not be presented here and we refer to [ 20 , 44 ] for details .", "label": "", "metadata": {}, "score": "53.305115"}
{"text": "Our approach is based on a model that locally mixes between supervised models from the helper languages .Parallel dat ... \" .We describe a method for prediction of linguistic structure in a language for which only unlabeled data is available , using annotated data from a set of one or more helper languages .", "label": "", "metadata": {}, "score": "53.327484"}
{"text": "Thus , although these newer methods have introduced potentially useful machine learning techniques , they should not be assumed to provide the best performance for unsupervised POS induction .In addi ... . \" ...Dependency parsing is a central NLP task .", "label": "", "metadata": {}, "score": "53.43151"}
{"text": "The first submission , the highest ranked constituency parsing system , uses a combination of PCFG - LA product grammar parsing and self - training .In the second submission , also a constituency parsing ... \" .The DCU - Paris13 team submitted three systems to the SANCL 2012 shared task on parsing English web text .", "label": "", "metadata": {}, "score": "53.46028"}
{"text": "In an empirical evaluation we show that our model consistently out - performs the current state - of - the - art across 10 languages . ...MM ) and also with the character LM ( 1HMM - LM ) .Starred entries denote results reported in CGS10 .", "label": "", "metadata": {}, "score": "53.51355"}
{"text": "In fact , this work and especially the considered elaborate SCFG could mark some sort of stepping stone towards new stochastic RNA secondary structure prediction methods realized by statistical random sampling .Appendix .How to Construct a Weighted Unranking Algorithm from a Given SCFG .", "label": "", "metadata": {}, "score": "53.554134"}
{"text": "A ( rather simple ) unambiguous SCFG .G . s .generating the language .L . is given by : . w .S . s .C .A . w .A .B . )C . w .", "label": "", "metadata": {}, "score": "53.613113"}
{"text": "C . . .The transformation of .G .d .Second , we have to replace each of these chains by a specific new rule .Consequently , our new rule set is now given by .w .^ .", "label": "", "metadata": {}, "score": "53.661293"}
{"text": "G . is a WCFG , then .G . is a stochastic context - free grammar ( SCFG ) iff the following additional restrictions hold : . f .R .Q .f . )A . w .f .", "label": "", "metadata": {}, "score": "53.77132"}
{"text": "Ideally a typical tagger should be robust , efficient , accurate , tunable and reusable .In reality taggers either definitely identify the tag for the given word or make the best guess based on the available information .As the natural language is complex it is sometimes difficult for the taggers to make accurate decisions about tags .", "label": "", "metadata": {}, "score": "53.96388"}
{"text": "Conversion procedures fall out of our ... \" .We show how web mark - up can be used to improve unsupervised dependency parsing .Starting from raw bracketings of four common HTML tags ( anchors , bold , italics and underlines ) , we refine approximate partial phrase boundaries to yield accurate parsing constraints .", "label": "", "metadata": {}, "score": "53.97375"}
{"text": "We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks .Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline .", "label": "", "metadata": {}, "score": "54.08464"}
{"text": "Web - scale experiments show that the DMV , perhaps because it is unlexicalized , does not benefit from orders of magnitude more annotated but noisier data .Our model , trained on a single blog , generalizes to 53.3 % accuracy out - of - domain , against the Brown corpus - nearly 10 % higher than the previous published best .", "label": "", "metadata": {}, "score": "54.463333"}
{"text": "L .R .p .Tools . \" ...We describe a novel approach for inducing unsupervised part - of - speech taggers for languages that have no labeled training data , but have translated text in a resource - rich language .", "label": "", "metadata": {}, "score": "54.49257"}
{"text": "In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .We develop a novel hidden Markov model incorporating sophisticated smoothing using a hierarchical Pitman - Yor processes prior , providing an elegant and principled means of incorporating lexical characteristics .", "label": "", "metadata": {}, "score": "54.602676"}
{"text": "We present a method for acquiring reliable predicate - argument structures from raw corpora for automatic compilation of case frames .Such lexicon compilation requires highly reliable predicate - argument structures to practically contribute to Natural Language Processing ( NLP ) applications , such as par ... \" .", "label": "", "metadata": {}, "score": "54.605934"}
{"text": "[ Show abstract ] [ Hide abstract ] ABSTRACT : Dependency parsing is a central NLP task .In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations .We show that for three leading unsupervised parsers ( Klein and Manning , 2004 ; Cohen and Smith , 2009 ; Spitkovsky et al .", "label": "", "metadata": {}, "score": "54.60944"}
{"text": "In practice , you 'll have to choose , either : .Find another NER systems that tags those types .Address this tagging task using knowledge - based / unsupervised approaches .Search for extra resources ( corpora ) that contain types you want recognize , and re - train a supervised NER system ( CoreNLP or other ) .", "label": "", "metadata": {}, "score": "54.629917"}
{"text": "But how does the observed EHR redundancy affect text mining ?Does such redundancy introduce a bias that distorts learned models ?Or does the redundancy introduce benefits by highlighting stable and important subsets of the corpus ?( iii )How can one mitigate the impact of redundancy on text mining ?", "label": "", "metadata": {}, "score": "54.668667"}
{"text": "At its ... \" .Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .It has also been found to benefit retrieval of spoken or written docu - ments .", "label": "", "metadata": {}, "score": "54.83813"}
{"text": "These works aim to predict the parser performance on a given target sentence .Ravi et al .( 2008 ) frame this as a regression problem .Kawahara and Uchimoto ( 2008 ) treat ... . \" ...Genre classification has been found to improve performance in many applications of statistical NLP , including language modeling for spoken language , domain adaptation of statistical parsers , and machine translation .", "label": "", "metadata": {}, "score": "54.94809"}
{"text": "A Comparative Study of English - to - Chinese Transliteration Models .D09 - 1070 [ bib ] : Taesun Moon ; Katrin Erk ; Jason Baldridge Unsupervised morphological segmentation and clustering with document boundaries .D09 - 1071 [ bib ] : Jurgen Van Gael ; Andreas Vlachos ; Zoubin Ghahramani The infinite HMM for unsupervised PoS tagging .", "label": "", "metadata": {}, "score": "54.94953"}
{"text": "Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .", "label": "", "metadata": {}, "score": "55.054153"}
{"text": "Current statistical parsers tend to perform well only on their training domain and nearby genres .While strong performance on a few related domains is sufficient for many situations , it is advantageous for parsers to be able to generalize to a wide variety of domains .", "label": "", "metadata": {}, "score": "55.054153"}
{"text": "They also permit the use of well - understood , generalpurpose learning algorithms .There has been an increased interest in using probabilistic grammars in the Bayesian setting .To date , most of the literature has focused on using a Dirichlet prior .", "label": "", "metadata": {}, "score": "55.13453"}
{"text": "f .p .f . k . , where .p .f .p .f . k .p .f .This way , we ensure that more common substructures are generated with higher probabilities than less common ones .", "label": "", "metadata": {}, "score": "55.28965"}
{"text": "or . and .B . )n .j .when considering the lexicographic order ( 1 , 2 , 3 , ... , n ) , which is induced by the specification .C .n .A .B .", "label": "", "metadata": {}, "score": "55.303642"}
{"text": "We take two popular dependency parsers - one graph - based and one transition - based - and compare results for both .Results show that using semisupervised learning in the form of self - training and co - training yields only very modest improvements in parsing accuracy .", "label": "", "metadata": {}, "score": "55.34955"}
{"text": "In fact , if the RNA data set used for determining the distribution induced by the grammar is not rich enough , then the corresponding stochastic RNA model is underestimated and its quality decreases .This is especially a problem when considering comprehensive CFGs that distinguish between many different structural motifs in order to get a realistic picture of the molecules ' behaviour ; such a grammar should however be preferred over simple lightweight grammars as basis for a non - uniform random generation method .", "label": "", "metadata": {}, "score": "55.38542"}
{"text": "Deriving such a \" realistic \" distribution on a given class of objects can easily be done by modeling the class by an appropriate stochastic context - free grammar ( SCFG ) .Details will follow in the next section .However , modeling these structures by an appropriate SCFG yields a more realistic RNA model , where the probability distribution on all structures is determined from a database of real world RNA data ( see e.g. [ 35 , 36 ] ) .", "label": "", "metadata": {}, "score": "55.42112"}
{"text": "However , most previous work on domain adaptation relied on the implicit assumption that domains are somehow given .As more and more data becomes available , automatic ways to select data that is beneficial for a new ( unknown ) target domain are becoming attractive .", "label": "", "metadata": {}, "score": "55.44709"}
{"text": "Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor systems .Here we evaluate seven different POS induction systems spanning nearly 20 years of work , using a variety of measures .", "label": "", "metadata": {}, "score": "55.462215"}
{"text": "A number of experimental results shows that our random generation method produces realistic output , at least with respect to the appearance of the different structural motifs .A link to download an implementation of our method ( in Wolfram Mathematica ) can be found there , too .", "label": "", "metadata": {}, "score": "55.484238"}
{"text": "This can be reached by scaling all productions by the same factor ( common denominator of all probabilities ) , while ensuring that derivations are of equal length for words of the same size ( ensured by using grammars in CNF ) .", "label": "", "metadata": {}, "score": "55.51526"}
{"text": "Since the guidelines differ , we observe no corresponding structure in the WSJ .It is telling that the parser labels this BIO example by attaching ev- ery token to the final proper noun \" P1 - 1 \" , exactly as the WSJ guidelines indicate .", "label": "", "metadata": {}, "score": "55.52848"}
{"text": "L . sto .given in our biological database .The resulting probabilities are given in Table 1 , their floating point approximations , rounded to the third decimal place in Table 2 .Table 1 .The probabilities ( relative frequencies ) for the production rules of the SCFG .", "label": "", "metadata": {}, "score": "55.541943"}
{"text": "In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' performance .We investigate the effect of genre variation on the performance of three NLP tools , namely , word segmenter , POS tagger , and parser .", "label": "", "metadata": {}, "score": "55.54738"}
{"text": "D09 - 1086 [ bib ] : David A. Smith ; Jason Eisner Parser Adaptation and Projection with Quasi - Synchronous Grammar Features .D09 - 1087 [ bib ] : Zhongqiang Huang ; Mary Harper Self - Training PCFG Grammars with Latent Annotations Across Languages .", "label": "", "metadata": {}, "score": "55.6651"}
{"text": "A . a .B .Different production rules with the same left - hand side give rise to the union of the corresponding cartesian products .Nevertheless , it should be noted that [ 25 ] also shows how to reduce specifications to standard form , where the corresponding standard specifications constitute the basis of the recursive method for uniform random generation and extends the usual Chomsky normal form ( CNF ) for CFGs .", "label": "", "metadata": {}, "score": "55.95096"}
{"text": "In order to overcome these pitfalls , one could take the competing point of view and consider only typical structural information observed in a set of sample data as the basis for a new random generation method .If that information draws a realistic picture for all the different motifs of a molecule 's folding , the corresponding sampling method is likely to produce realistic results .", "label": "", "metadata": {}, "score": "56.002045"}
{"text": "We found certain scaling techniques obtained tiny improvements on the target domain that , while significant compared to competition results , are not statistically significant .We also attempted a sim- ilar approach on the feature level .A very predic- tive source domain feature is not useful if it does not appear in the target domain .", "label": "", "metadata": {}, "score": "56.06797"}
{"text": "A .n . size .A .n . and the complexity results from [ 37 ] also hold for weighted classes .Hence , the corresponding class size computations up to n need .O .n . time .", "label": "", "metadata": {}, "score": "56.19574"}
{"text": "G . and the corresponding language ( combinatorial class ) .L .G . ) , a random word .w .L .G . ) can be generated in the following way : .Start with the sentential form S ( where S denotes the axiom of the grammar .", "label": "", "metadata": {}, "score": "56.232876"}
{"text": "The topic of random generation algorithms ( also called samplers ) has been widely studied by computer scientists .As considers software engineering , the so - called random testing approach is commonly used to test implementations of particular algorithms , as it is usually not feasible to consider all possible inputs and unknown which of these inputs are among the most interesting ones .", "label": "", "metadata": {}, "score": "56.345078"}
{"text": "Automatic assignment of descriptors to the given tokens is called Tagging .The descriptor is called tag .The tag may indicate one of the parts - of - speech , semantic information , and so on .So tagging a kind of classification .", "label": "", "metadata": {}, "score": "56.533817"}
{"text": "D09 - 1155 [ bib ] : Simone Teufel ; Advaith Siddharthan ; Colin Batchelor Towards Domain - Independent Argumentative Zoning : Evidence from Chemistry and Computational Linguistics .D09 - 1156 [ bib ] : Richard C. Wang ; William W. Cohen Character - level Analysis of Semi - Structured Documents for Set Expansion .", "label": "", "metadata": {}, "score": "56.586327"}
{"text": "On the Use of Virtual Evidence in Conditional Random Fields .D09 - 1135 [ bib ] : Xiaojun Lin ; Yang Fan ; Meng Zhang ; Xihong Wu ; Huisheng Chi Refining Grammars for Parsing with Hierarchical Semantic Knowledge .D09 - 1136 [ bib ] : Ding Liu ; Daniel Gildea Bayesian Learning of Phrasal Tree - to - String Templates .", "label": "", "metadata": {}, "score": "56.60341"}
{"text": "i . size .L .n . ) and then unranking the i th structure of size n .The worst - case runtime complexity of this procedure is equal to that of unranking and is thus given by .O .", "label": "", "metadata": {}, "score": "56.656864"}
{"text": "The main contribution of this manuscript is the derivation of a new and efficient algorithm for the random generation of RNA secondary structures according to an elaborate and thus very realistic model .For this purpose we use and generalize the approach from [ 20 ] .", "label": "", "metadata": {}, "score": "56.689068"}
{"text": "The number of outermost pairs of brackets in the entire string uniquely determines the corresponding sentential form to be used .B itself has to generate at least one additional pair of brackets .w .B .C .B . )", "label": "", "metadata": {}, "score": "56.6903"}
{"text": "References Shai Ben - David , John Blitzer , Koby Crammer , and Fer- nando Pereira .Analysis of representations for domain adaptation .In NIPS .Leo Breiman .Learning , 24(2):123 - 140 .Bagging predictors .Machine R. Brown .", "label": "", "metadata": {}, "score": "56.71791"}
{"text": "C . . .To apply the approach presented in [ 20 ] to transform a given SCFG to RNF , the grammar needs to be \u03f5 -free and loop - free .Thus , we first have to transform grammar .", "label": "", "metadata": {}, "score": "56.98088"}
{"text": "In the second submission , also a constituency parsing system , the n - best lists of various parsing models are combined using an approximate sentence - level product model .The third system , the highest ranked system in the dependency parsing track , uses voting over dependency arcs to combine the output of three constituency parsing systems which have been converted to dependency trees .", "label": "", "metadata": {}, "score": "57.05931"}
{"text": "Unquantified Results .Similar to [ 21 ] , we denote the free energy of a given secondary structure .s .L . according to the static and dynamic model by g stat ( s ) and g dyn ( s ) , respectively .", "label": "", "metadata": {}, "score": "57.07672"}
{"text": "Furthermore , a standard construction to make the grammar \u03f5 -free has been applied .That way , we can be sure that . sto . generates .L .( formally this fact easily follows by obvious bi - simulation proofs for each substitution and by the proven correctness of the used construction to ensure \u03f5 -freeness ) .", "label": "", "metadata": {}, "score": "57.27984"}
{"text": "In order to translate a CFG into the framework of admissible constructions , it is sufficient to make each terminal symbol an atom and to assume each non - terminal A to represent a class .A .( the set of all words which can be derived from non - terminal A ) .", "label": "", "metadata": {}, "score": "57.387833"}
{"text": "sto . offers for w .Eliminating all but the variable associated with the axiom and simplifying ( for this step we made use of Mathematica ) yields the single equation .from Example 0.3 which proves that for all n both grammars have the same number of derivation trees for words of size n .", "label": "", "metadata": {}, "score": "57.447533"}
{"text": "C .A .B .We will assume all elements of .A . to be of smaller order than those of .B .( this way we use the construction of the class to imply an ordering ) .", "label": "", "metadata": {}, "score": "57.482014"}
{"text": "r .E .P . g .s .t . a .t . )Quantified Results .The previously considered energy comparisons have been presented only by unquantified plots .Therefore , there is a need to consider some sort of quantification and additionally present corresponding quantified comparison results .", "label": "", "metadata": {}, "score": "57.520348"}
{"text": "n . , the corresponding unranking algorithm for a given input number .i . card .S .n . computes the single structure .s .S .n . having number i in the ranking scheme defined for class .", "label": "", "metadata": {}, "score": "57.640755"}
{"text": "G . d .Then , the reweighting of the production rules of ( the RNF of ) .G . d .After that , we obtain the following reweighted grammar .G .d .w .S .B . w .", "label": "", "metadata": {}, "score": "57.704266"}
{"text": "This means we have to find out if the energies related to a random sample ( generated by our unranking method ) and those related to a native sample ( given by the structures in our biological database ) come from a common distribution .", "label": "", "metadata": {}, "score": "57.822556"}
{"text": "Our grammar inducer is trained on the Wall Street Journal ( WSJ ) and achieves 59.5 % accuracy out - of - domain ( Brown sentences with 100 or fewer words ) , more than 6 % higher than the previous best results .", "label": "", "metadata": {}, "score": "57.84014"}
{"text": "For the most common 1While only 8 teams participated in the closed track with us , our score beat all of the teams in the open track .Page 2 .The parser was trained on the provided WSJ data .Digits are less than 4 % of the tokens in BIO .", "label": "", "metadata": {}, "score": "57.847126"}
{"text": "D09 - 1055 [ bib ] : Huihsin Tseng ; Longbin Chen ; Fan Li ; Ziming Zhuang ; Lei Duan ; Belle Tseng Mining Search Engine Clickthrough Log for Matching N - gram Features .D09 - 1056 [ bib ] : Javier Artiles ; Enrique Amig\u00f3 ; Julio Gonzalo The role of named entities in Web People Search .", "label": "", "metadata": {}, "score": "57.84878"}
{"text": "Actually , since we consider a huge CFG where all possible structural motifs are created by distinct productions , we generally obtain realistic probability distributions and RNA models ( see [ 21 ] ) .Finally note that of course we could make use of random sampling strategies originally designed to sample structures connected to a given sequence in order to generate a random secondary structure only .", "label": "", "metadata": {}, "score": "57.866"}
{"text": "The purpose of this section is to analyze the quality of randomly generated structures by considering some experimental results .Parameters for Structural Motifs .The determined results are presented in Table 3 .Comparing the specific values of all different parameters , we can guess that our algorithm produces random RNA secondary structures that are , related to the different structural motifs and thus related to the expected shape of such structures , in most cases realistic .", "label": "", "metadata": {}, "score": "57.962223"}
{"text": "We study this problem as a new task - multiple source parser adaptation .Our system trains on corpora from many different domains .It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy .", "label": "", "metadata": {}, "score": "58.015594"}
{"text": "This paper revisits an assump - tion that genre variation is continuous along multiple dimensions , and an early use of principal component analysis to find these dimensions .Results on a very heterogeneous corpus of post-1990s American English reveal four major dimensions , three of which echo those found in prior work and the fourth depending on features not used in the earlier study .", "label": "", "metadata": {}, "score": "58.070465"}
{"text": "Yet , various grammar parameters are expected to be correlated because the elements in language they represent share linguistic properties .In this paper , we suggest an alternative to the Dirichlet prior , a family of logistic normal distributions .We derive an inference algorithm for this family of distributions and experiment with the task of dependency grammar induction , demonstrating performance improvements with our priors on a set of six treebanks in different natural languages .", "label": "", "metadata": {}, "score": "58.098793"}
{"text": "w .L .G . ) w . has been randomly generated .Note that the choice of the production made in 3 ) according to the previously drawn random number is appropriate , since it is conform to the probability distribution on the grammar rules .", "label": "", "metadata": {}, "score": "58.112076"}
{"text": "i . card .A . )In this case , we recursively call the unranking procedure for .A .Otherwise ( i.e. if .i . card .A . ) , we consider .B . , searching for its ( .", "label": "", "metadata": {}, "score": "58.115067"}
{"text": "D09 - 1092 [ bib ] : David Mimno ; Hanna M. Wallach ; Jason Naradowsky ; David A. Smith ; Andrew McCallum Polylingual Topic Models .D09 - 1093 [ bib ] : Casey Whitelaw ; Ben Hutchinson ; Grace Y Chung ; Ged Ellis Using the Web for Language Independent Spellchecking and Autocorrection .", "label": "", "metadata": {}, "score": "58.143814"}
{"text": "Using the usual way of transforming a non-\u03f5 -free grammar into an \u03f5 -free one , the following definition can immediately be obtained from the previous one : .Definition App-.12 .The unambigous and \u03f5 -free SCFG .G . sto . generating exactly the language .", "label": "", "metadata": {}, "score": "58.236343"}
{"text": "3 Adaptation Approaches We survey the main approaches we explored for this task .While some of these approaches provided a modest performance boost to a simple parser ( lim- ited data and first - order features ) , no method added any performance to our best parser ( all data and second - order features ) .", "label": "", "metadata": {}, "score": "58.30713"}
{"text": "n . ) when using the boustrophedonic order .By repeating this procedure m times , a set of m ( not necessarily distinct ) random RNA secondary structures of size n can be generated in time .O . m .", "label": "", "metadata": {}, "score": "58.470825"}
{"text": "( Admissible ) Constructions and Specifications .According to [ 25 ] , a decomposable structure is a structure that admits an equivalent combinatorial specification : .If .A .A . k . are combinatorial classes and \u03f5 1 , ... , \u03f5 k are neutral objects , the combinatorial sum or disjoint union is defined as .", "label": "", "metadata": {}, "score": "58.520447"}
{"text": "Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multilingual model substantially outperforms independent learning , with larger gains both from more articulated phylogenies and as well as from increasing numbers of languages .Across eight languages , the multilingual approach gives error reductions over the standard monolingual DMV averaging 21.1 % and reaching as high as 39 % . ... inally given as simple multinomial distributions with one parameter per outcome . \" ...", "label": "", "metadata": {}, "score": "58.783733"}
{"text": "n . ) while other algorithms typically have a runtime in .O . m .n .Secondly , our approach works with integer arithmetic only which is faster and saves us from all the discomforting details of using floating point arithmetic with logarithmized probabilities .", "label": "", "metadata": {}, "score": "58.81022"}
{"text": "The CoNLL In Proc .Lawrence Saul and Fernando Pereira . gate and mixed - order markov models for statistical language modeling .In EMNLP . Aggre-1055 .Data provided are for informational purposes only .Although carefully collected , accuracy can not be guaranteed .", "label": "", "metadata": {}, "score": "58.828438"}
{"text": "This decision effectively removes NNP from the BIO domain and renders all features that depend on the NNP tag ineffective .In our above BIO NP example , all nouns are labeled NN , whereas the WSJ example contains NNP tags .", "label": "", "metadata": {}, "score": "58.84202"}
{"text": "For this reason , we assume our unranking algorithm a valuable contribution , even though it requires a more cumbersome framework .Unranking of Combinatorial Objects .The problem of unranking can easily be solved along the composition of the objects at hand , i.e. the operations used for its construction , once we know the number of possible choices for each substructure .", "label": "", "metadata": {}, "score": "58.93693"}
{"text": "n .n .In either case , given a fix number of considered combinatorial ( sub)classes ( or corresponding non - terminal symbols ) , the precomputation of all class size tables up to size n requires .O .n . operations on coefficients .", "label": "", "metadata": {}, "score": "58.9394"}
{"text": "The corresponding procedures ( for class size calculations and structure generations ) are actually required for ( uniform ) random generation of words of a given CFG by means of unranking .Simply speaking , the unranking of decomposable structures ( like for instance RNA secondary structures which can be uniquely decomposed into distinct structural components ) works as follows : Each structure s in the combinatorial class .", "label": "", "metadata": {}, "score": "59.0217"}
{"text": "These observations indicate that our weighted unranking algorithm produces random RNA secondary structures that are - related to the free energy of such structures ( in expectation and variation ) - in expectation realistic .Table 4 .Significance results for statistical hypothesis testing , computed by the Wilcoxon rank - sum method .", "label": "", "metadata": {}, "score": "59.102524"}
{"text": "B . )n .j .when considering the boustrophedon order .n .n .n . , induced by the specification .C .n .A .B .n .A .n .B .A .", "label": "", "metadata": {}, "score": "59.185883"}
{"text": "This causes no problems for the original application of such algorithms since the sequence - dependent preprocessing which is part of their overall procedure is at least quadratic in time and thus the dominating part .Here our approach is of advantage ( replacing a factor n by log ( n ) ) and since our preprocessing only depends on the size of the structure to be generated it is performed once and stored to disk for later reuse .", "label": "", "metadata": {}, "score": "59.242577"}
{"text": "As for this paper , the corresponding probability distribution will be induced by a set of sample ( SSU and LSU r)RNA secondary structures from the databases [ 45 , 46 ] , which will be referred to as biological database in the sequel .", "label": "", "metadata": {}, "score": "59.290623"}
{"text": "Integrated annotation for biomedical information ex- traction .In Proc . of the Human Language Technol- ogy Conference and the Annual Meeting of the North American Chapter of the Association for Computa- tional Linguistics ( HLT / NAACL ) .B. MacWhinney .", "label": "", "metadata": {}, "score": "59.35735"}
{"text": "The development data was 200 sentences of labeled biomedical oncology text ( BIO , the ONCO portion of the Penn Biomedical Treebank ) , as well as 200 K unlabeled sentences ( Kulick et al ., 2004 ) .The two test domains were a collection of medline chem- istry abstracts ( pchem , the CYP portion of the Penn Biomedical Treebank ) and the Child Language Data Exchange System corpus ( CHILDES ) ( MacWhin- ney , 2000 ; Brown , 1973 ) .", "label": "", "metadata": {}, "score": "59.366158"}
{"text": "p .^ .i .i . , by their relative frequencies , the resulting grammar .sto .has the consistency property , which means .sto . provides a probability distribution on the language .L . sto . )", "label": "", "metadata": {}, "score": "59.39249"}
{"text": "D09 - 1132 [ bib ] : Gerasimos Lampouras ; Ion Androutsopoulos Finding Short Definitions of Terms on Web Pages .D09 - 1133 [ bib ] : Junhui Li ; Guodong Zhou ; Hai Zhao ; Qiaoming Zhu ; Peide Qian Improving Nominal SRL in Chinese Language with Verbal SRL Information and Automatic Predicate Recognition .", "label": "", "metadata": {}, "score": "59.417595"}
{"text": "Instead , we scaled each feature 's value by a factor proportional to its frequency in the target do- main and trained the parser on these scaled feature values .We obtained small improvements on small amounts of training data .Target Focused Learning 4 Future Directions Given our pessimistic analysis and the long list of failed methods , one may wonder if parser adapta- tion is possible at all .", "label": "", "metadata": {}, "score": "59.483406"}
{"text": "We address the random generation of these structures according to a realistic distribution obtained from real - life data by using a very detailed context - free grammar ( that models the class of RNA secondary structures by distinguishing between all known motifs in RNA structure ) .", "label": "", "metadata": {}, "score": "59.492523"}
{"text": "n . of all feasible structures having size n is given a number ( rank ) .i . card .S .n . , defined by a particular ranking method .Based on this ordering of the considered structure class .", "label": "", "metadata": {}, "score": "59.588436"}
{"text": "Altogether , we can finally conclude that the non - uniform random generation method proposed in this article produces appropriate output and may thus be used ( for research issues as well as for practical applications ) to generate random RNA secondary structures .", "label": "", "metadata": {}, "score": "59.602364"}
{"text": "D09 - 1037 [ bib ] : Trevor Cohn ; Phil Blunsom A Bayesian Model of Syntax - Directed Tree to String Grammar Induction .D09 - 1038 [ bib ] : Tong Xiao ; Mu Li ; Dongdong Zhang ; Jingbo Zhu ; Ming Zhou", "label": "", "metadata": {}, "score": "59.60575"}
{"text": "Recognizing Textual Relatedness with Predicate - Argument Structures .D09 - 1083 [ bib ] : Wen - tau Yih Learning Term - weighting Functions for Similarity Measures .D09 - 1084 [ bib ] : Danushka Bollegala ; Yutaka Matsuo ; Mitsuru Ishizuka A Relational Model of Semantic Similarity between Words using Automatically Extracted Lexical Pattern Clusters from the Web .", "label": "", "metadata": {}, "score": "59.675476"}
{"text": "In an attempt to disprove that assumption , we decided to perform a series of Wilcoxon tests by considering a number of different random samples .s .L .n . is added to the random sample iff .[ . g .", "label": "", "metadata": {}, "score": "59.67597"}
{"text": "Note that when computing the sums for cartesian products , we can either consider the values for j in the sequential ( also called lexicographic ) order ( 1 , 2 , 3 , ... , n ) or in the so - called boustrophedon order .", "label": "", "metadata": {}, "score": "59.680424"}
{"text": "In particular , it is well - known that relative frequencies in our context yield a maximum likelihood ( ML ) estimator for the rule probabilities and thus a consistent estimator for the parameter set .We have trained the probabilities ( relative frequencies ) of .", "label": "", "metadata": {}, "score": "59.76236"}
{"text": "Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated .However , there are cases in which ... . by Shay B. Cohen , Noah A. Smith , Alex Clark , Dorota Glowacka , Colin De La Higuera , Mark Johnson , John Shawe - taylor . \" ...", "label": "", "metadata": {}, "score": "60.128925"}
{"text": "An easy way is to use the gazette feature .You need to read the docs carefully , several times . -Neil McGuigan Jan 27 ' 14 at 19:49 .1 Answer 1 .CoreNLP out - of - the - box will be restricted to types they mention : PERSON , LOCATION , ORGANIZATION , MISC , DATE , TIME , MONEY , NUMBER .", "label": "", "metadata": {}, "score": "60.16791"}
{"text": "G .d . , the recursion for the function size has the following form : . size .I .n . ) size .B .n .I .B . size .C .n .I .", "label": "", "metadata": {}, "score": "60.175434"}
{"text": "A . ) th element .Formally , we first need to specify an order on all objects of the considered combinatorial class that have the same size .This can be done in a recursive way according to the admissible specification of the class : .", "label": "", "metadata": {}, "score": "60.195244"}
{"text": "Both words were unknown only 5 % of the time in BIO , while one of the words being un- known was more common , reflecting 27 % of deci- sions .Upon further investigation , the majority of unknown words were nouns , which indicates that unknown word errors were caused by the problems discussed above .", "label": "", "metadata": {}, "score": "60.21989"}
{"text": "aFor text mining , preprocessing the EHR corpus with fingerprinting yields significantly better results .Conclusions Before applying text - mining techniques , one must pay careful attention to the structure of the analyzed corpora .While the importance of data cleaning has been known for low - level text characteristics ( e.g. , encoding and spelling ) , high - level and difficult - to - quantify corpus characteristics , such as naturally occurring redundancy , can also hurt text mining .", "label": "", "metadata": {}, "score": "60.29559"}
{"text": "First , we will consider statistical indicators of many important parameters related to particular structural motifs and compare the ones observed in the used sample set of real world RNA data to those observed in a corresponding set of random structures .", "label": "", "metadata": {}, "score": "60.33716"}
{"text": "Table 3 .Expectation and variance of important parameters related to particular structural motifs of RNA secondary structure .Values are derived from a native sample ( our biological database ) and from a random sample , respectively .num x denotes the number of occurrences of motif x in one secondary structure and unp x ( bps x ) denotes the number of accessible unpaired bases ( base pairs ) in one substructure of type x .", "label": "", "metadata": {}, "score": "60.474655"}
{"text": "Coming back to the random testing problem from software engineering , we observe that generating objects of a given class of input data according to a uniform distribution is sufficient for testing the correctness of particular algorithms .However , if one intends to gather information about the \" real - life behaviour \" of the algorithm ( e.g. with respect to runtime or space requirements ) , we need to perform simulations with input data that are as closely as possible related to corresponding application .", "label": "", "metadata": {}, "score": "60.49701"}
{"text": "Adaptation techniques focus on the former since it is impossible to determine the lat- ter without knowledge of the labeling function .In parsing adaptation , the former corresponds to a dif- ference between the features seen in each domain , such as new words in the target domain .", "label": "", "metadata": {}, "score": "60.516483"}
{"text": "D09 - 1143 [ bib ] : Truc - Vien T. Nguyen ; Alessandro Moschitti ; Giuseppe Riccardi Convolution Kernels on Constituent , Dependency and Sequential Structures for Relation Extraction .D09 - 1144 [ bib ] : Ekaterina Ovchinnikova ; Theodore Alexandrov ; Tonio Wandmacher Automatic Acquisition of the Argument - Predicate Relations from a Frame - Annotated Corpus .", "label": "", "metadata": {}, "score": "60.70253"}
{"text": "n . to .O .n . log .n . ) by using the boustrophedonic order instead of the sequential order , a simple change in Algorithm 4 ( Unranking of cartesian products ) is neccessary ( see e.g. [ 7 ] ) .", "label": "", "metadata": {}, "score": "60.76941"}
{"text": "While we believe this is not enough diversity , it was not feasible to repeat our experiment with a large number of parsers . 3.3Another approach to adaptation is to favor training examples that are similar to the target .We first mod- ified the weight given by the parser to each training sentence based on the similarity of the sentence to target domain sentences .", "label": "", "metadata": {}, "score": "60.861786"}
{"text": "S . of secondary structures , we can calculate the corresponding energy points .E .P .S .e .n .e . r . g .y . ) size .s . )e .n .", "label": "", "metadata": {}, "score": "60.97596"}
{"text": "We divided the available WSJ data into a train and test set , trained a parser on the train set and compared errors on the test set and BIO .Accuracy dropped from 90 % on WSJ to 84 % on BIO .", "label": "", "metadata": {}, "score": "61.076942"}
{"text": "Self - training creates semi - supervised learners from existing supervised learners with minimal effort .We first show results on self - training for constituency parsing within a single domain .While self - training has failed here in the past , we present a simple modification which allows it to succeed , producing state - of - the - art results for English constituency parsing .", "label": "", "metadata": {}, "score": "61.22869"}
{"text": "Actually , our webservice can be used for generating random secondary structures of any specified type of RNA .It just requires a database of known structures for the respective RNA type as input .Note that in this work , we abstract from sequence and consider only the structure size as input for our algorithm .", "label": "", "metadata": {}, "score": "61.46015"}
{"text": "D09 - 1045 [ bib ] : Jeff Mitchell ; Mirella Lapata Language Models Based on Semantic Composition .D09 - 1046 [ bib ] : Katrin Erk ; Diana McCarthy Graded Word Sense Assignment .D09 - 1047 [ bib ] : Daniel Dahlmeier ; Hwee Tou Ng ; Tanja Schultz Joint Learning of Preposition Senses and Semantic Roles of Prepositional Phrases .", "label": "", "metadata": {}, "score": "61.463844"}
{"text": "Neutral and atomic classes contain only one element , such that there is only one possible ordering .Furthermore , let . denote the ordering within the combinatorial class .C .n . , then .If .C .A .", "label": "", "metadata": {}, "score": "61.501396"}
{"text": "Nouns are far more The annota- 2We measured these drops on several other dependency parsers and found similar results .ery token is headed by \" Department \" .In contrast , a similar BIO phrase has a very different structure , pursuant to the BIO guidelines .", "label": "", "metadata": {}, "score": "61.760544"}
{"text": "Our method does not require any translated texts or token - level alignments .Instead , the phylogenetic prior couples languages at a parameter level .Joint induction in the multiling ... \" .We present an approach to multilingual grammar induction that exploits a phylogeny - structured model of parameter drift .", "label": "", "metadata": {}, "score": "61.78997"}
{"text": "NN , NNP - NNP - NNP , NN - IN - NN , and IN - NN - NN .However , when we examine the coarse POS tags , which do not distinguish between nouns , these dif- ferences disappear .", "label": "", "metadata": {}, "score": "61.891037"}
{"text": "The process of assigning one of the parts of speech to the given word is called Parts Of Speech tagging .It is commonly referred to as POS tagging .Parts of speech include nouns , verbs , adverbs , adjectives , pronouns , conjunction and their sub - categories .", "label": "", "metadata": {}, "score": "61.914093"}
{"text": "This indicates that most of the loss comes from missing these edges .The primary problem for nouns is the difference between structures in each domain . tion guidelines for the Penn Treebank flattened noun phrases to simplify annotation ( Marcus et al . , 1993 ) , so there is no complex structure to NPs . K\u00a8 ubler ( 2006 ) showed that it is difficult to compare the Penn Treebank to other treebanks with more com- plexnounstructures , suchasBIO.ConsidertheWSJ phrase \" the New York State Insurance Department \" .", "label": "", "metadata": {}, "score": "62.03297"}
{"text": "Formally : .Definition 0.4 .If .A . is a combinatorial class and \u03bb is an integer , the weighting of .A . by \u03bb is defined as .We will call two objects from a combinatorial class copies of the same object iff they only differ in the tags added by weighting operations .", "label": "", "metadata": {}, "score": "62.067207"}
{"text": "McClosky et al .( 2010 ) coined the term multiple source domain adaptation .Similar to us , McClosky et al .( 2010 ) regard a target domain as mixture of source domains , b .. by Joseph Le Roux , Jennifer Foster , Joachim Wagner , Rasul Samad , Zadeh Kaljahi , Anton Bryl . \" ...", "label": "", "metadata": {}, "score": "62.072807"}
{"text": "s . ) s .L .As these analytical energy results from [ 21 ] and our unranking algorithm have been derived from the same database of real - life RNA data and by modeling the same class .L . of structures via very similar SCFGs , it seems adequate to use them for comparisons with the energies of our randomly generated structures .", "label": "", "metadata": {}, "score": "62.422142"}
{"text": "First , there may be room for adaptation with our domains if a common annotation scheme is used .Second , we have stressed that typical adaptation , modifying a model trained on the source domain , will fail but there may be unsupervised parsing techniques that improve performance after adaptation , such as a rule based NP parser for BIO based on knowledge of the annotations .", "label": "", "metadata": {}, "score": "62.48761"}
{"text": "C .C .B .C .C .B .C .C .where each .W .i .i . , is integral .The ( now weighted ) grammar can easily be translated into a corresponding admissible specification , which includes the weighting of all involved combinatorial ( sub)classes , as described earlier .", "label": "", "metadata": {}, "score": "62.65054"}
{"text": "Most of the 3D structure is determined by the intramolecular base - pairing interactions in the plane , which together form the secondary structure of the molecule .For this reason , pseudoknots ( induced by crossing base pairs ) are considered as tertiary interactions and are usually not permitted in the definition of secondary structure .", "label": "", "metadata": {}, "score": "62.984566"}
{"text": "G .d . , this specification is given by the following equations : .S .B .S .B .C .S .B .C .B .Z .B .Z . )B .", "label": "", "metadata": {}, "score": "62.993305"}
{"text": "This process was iterated 40 times , resulting in a sample of 40 parameter sets .Finally , for each parameter we determined its variance along this sample of size 40 .The corresponding values lay between 0 ( resulting for intermediate symbols without alternatives ; for whose productions a probability of 1 is predetermined ) and 2.87652 \u00d7 10 -6 ( resp .", "label": "", "metadata": {}, "score": "63.0655"}
{"text": "Furthermore , note that an SCFG mirror of the famous Turner energy model has been used in [ 21 ] to perform the first analytical analysis of the free energy of RNA secondary structures ; this SCFG marks a cornerstone between stochastic and pyhsics - based approaches towards RNA structure prediction .", "label": "", "metadata": {}, "score": "63.13965"}
{"text": "O .n .for the computation of all weighted class sizes needed , with our approach a set of m random secondary structures of a given structure size n can be computed in worst - case time complexity .O . m .", "label": "", "metadata": {}, "score": "63.390404"}
{"text": "Some of this improvement is from training , but more than half is from parsing with induced constraints , in inference .Punctuation - aware decoding works with existing ( even already - trained ) parsing models and always increased accuracy in our experiments . ... mplementation , extension , understanding and debugging . \" ...", "label": "", "metadata": {}, "score": "63.444042"}
{"text": "Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were words ) do not perform well with Klein and Manning 's Dependency Model with Valence ( DMV ) .", "label": "", "metadata": {}, "score": "63.644493"}
{"text": "The cause for this is clear when the annotation guide- lines are considered .The proper nouns in WSJ are names of companies , people and places , while in BIO they are names of genes , proteins and chemi- cals .", "label": "", "metadata": {}, "score": "63.900295"}
{"text": "If those combinatorial classes are to correspond to a considered SCFG , we have to face the problem that the maximum likelihood ( ML ) training introduces rational weights for the production rules while weighting as an admissible construction needs integer arguments .", "label": "", "metadata": {}, "score": "63.917984"}
{"text": "A . k .A . k .A . k .Note that the constructions of disjoint union , cartesian product , sequence , set and cycle are all admissible : .The framework of ( admissible ) specifications obviously resembles that of context - free grammars ( CFGs ) known from formal language theory ( note that we assume the reader has basic knowledge of the notions concerning context - free languages and grammars .", "label": "", "metadata": {}, "score": "64.2062"}
{"text": "An example is the above BIO NP , in which the phrase \" glutathione transferase P1 - 1 \" is an appositive indicating which \" enzyme \" is meant .However , since there are no commas , the parser thinks \" P1 - 1 \" is the head .", "label": "", "metadata": {}, "score": "64.2096"}
{"text": "Lawrence Erlbaum .M. Marcus , B. Santorini , and M. Marcinkiewicz .Building a large annotated corpus of English : the Penn Treebank .Computational Linguistics , 19(2):313 - 330 .Ryan McDonald , Kevin Lerman , and Fernando Pereira .", "label": "", "metadata": {}, "score": "64.21667"}
{"text": "We present a number of semi - supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger , yet still relatively small , set of unlabelled sentences .We take two popular dependency parsers - one graph - based and one transition - based - and ... \" .", "label": "", "metadata": {}, "score": "64.518135"}
{"text": "Map every induced label to a gold standard tag greedily ( 45 labels to 45 tags of the Penn tag set ) .Use the mapping to compute tag accuracy on the Wall Street Journal portion of the Penn TreeBank .Two Decades of Unsupervised POS induction : How far have we come ?", "label": "", "metadata": {}, "score": "64.553314"}
{"text": "D09 - 1108 [ bib ] : Hui Zhang ; Min Zhang ; Haizhou Li ; Chew Lim Tan Fast Translation Rule Matching for Syntax - based Statistical Machine Translation .D09 - 1109 [ bib ] : Enrique Alfonseca ; Massimiliano Ciaramita ; Keith Hall Gazpacho and summer rash : lexical relationships from temporal patterns of web search queries .", "label": "", "metadata": {}, "score": "64.712944"}
{"text": "Formally : .( such that all production weights are integral ) , yielding reweighted WCFG .G .( with integral weights ) into the corresponding admissible specification .This specification ( with weighted classes ) can be translated directly- into a recursion for the function size of all involved combinatorial ( sub)classes ( where class sizes are weighted ) and . into generating algorithms for the specified ( weighted ) classes , . yielding the desired weighted unranking algorithm for generating random elements of .", "label": "", "metadata": {}, "score": "65.06545"}
{"text": "p .^ .D .B .p .^ .B .p .^ .B .B . strands . in . bulge .loop .p .^ .F .p .^ .F .", "label": "", "metadata": {}, "score": "65.2187"}
{"text": "The corresponding plots for the dynamic energy model are shown in Figure 5 .Looking at both figures , we immediately see that the energies for our set of randomly generated RNA secondary structures seem to fit to the ones for the considered RNA database and also to the corresponding analytically obtained energy results from [ 21 ] .", "label": "", "metadata": {}, "score": "65.2508"}
{"text": "I . is created from the corresponding intermediate symbol I .For our application it is crucial that . sto . - as claimed its definition - is unambiguous .To prove this , we first note that . sto .has been constructed starting from a simple grammar which generates .", "label": "", "metadata": {}, "score": "65.285645"}
{"text": "Across eight European languages , our approach results in an average absolute improvement of 10.4 % over a state - of - the - art baseline , and 16.7 % over vanilla hidden Markov models induced with the Expectation Maximization algorithm . \" ...", "label": "", "metadata": {}, "score": "65.316414"}
{"text": "Random generation stochastic context - free grammars RNA secondary structures .Electronic supplementary material .The online version of this article ( doi : 10 .1186/\u200b1748 - 7188 - 6 - 24 ) contains supplementary material , which is available to authorized users .", "label": "", "metadata": {}, "score": "65.3506"}
{"text": "Second and more recently , the so - called Boltzmann method [ 1 , 26 ] , where random objects ( under the corresponding Boltzmann model ) have a fluctuating size , but objects with the same size invariably occur with the same probability .", "label": "", "metadata": {}, "score": "65.40338"}
{"text": "e . r . g .y .s . )S .n . , respectively .To compare the energies of our randomly generated structures to the corresponding confidence interval(s ) , we decided to consider any . k . , meaning the probability that the free energy of a random RNA secondary structure of size n lies within the corresponding interval is greater than 0.5 , 0.75 , 0.9 , and 0.95 , respectively .", "label": "", "metadata": {}, "score": "65.62066"}
{"text": "B . w .^ .B .B . ) w .^ .C .C .B .C .C .B .C .C .A .A .A .This way , we obtain the following production set : . w .", "label": "", "metadata": {}, "score": "65.73611"}
{"text": "^ .C .C .B .C .C .B .C .C .However , since in our case , there is obviously nothing left to do , the transformation of .G .d . into RNF is finished .", "label": "", "metadata": {}, "score": "65.95747"}
{"text": "In Conference on Natural Language Learning ( CoNLL ) .J. Nivre , J. Hall , S. K\u00a8 ubler , R. McDonald , J. Nils- son , S. Riedel , and D. Yuret .2007 shared task on dependency parsing . of the CoNLL 2007 Shared Task .", "label": "", "metadata": {}, "score": "65.987564"}
{"text": "D09 - 1123 [ bib ] : Hany Hassan ; Khalil Sima'an ; Andy Way A Syntactified Direct Translation Model with Linear - time Decoding .D09 - 1124 [ bib ] : Samer Hassan ; Rada Mihalcea Cross - lingual Semantic Relatedness Using Encyclopedic Knowledge .", "label": "", "metadata": {}, "score": "66.1826"}
{"text": "D09 - 1073 [ bib ] : Min Zhang ; Haizhou Li Tree Kernel - based SVM with Structured Syntactic Knowledge for BTG - based Phrase Reordering .D09 - 1074 [ bib ] : Spyros Matsoukas ; Antti - Veikko I. Rosti ; Bing Zhang Discriminative Corpus Weight Estimation for Machine Translation .", "label": "", "metadata": {}, "score": "66.344406"}
{"text": "U .L . )N .p .N .U .p .U .U .p .U .\u03f5 . . .In this grammar , different intermediate symbols have been used to distinguish between different substructures .", "label": "", "metadata": {}, "score": "66.39027"}
{"text": "It is also known as the Wilcoxon rank - sum test [ 49 ] which however can only be applied for equal sample sizes .More specifically , the result of such a test , the so - called p - value , is a probability answering the following question : If the two samples really have the same distribution , what is the probability that the observed difference is due to chance alone ?", "label": "", "metadata": {}, "score": "66.438965"}
{"text": "A .B .Thus , .A .B . a . a .b .b .and within this class , a has relative frequency ., while b has relative frequency .Hence , this way it becomes possible to regard non - uniformly distributed classes .", "label": "", "metadata": {}, "score": "66.457535"}
{"text": "In my case , they are domain - specific and I call them : Entity , Action , Incident .I want to use these as a seed for extracting more named - entities .While I like the idea of using Support Vector Machines for doing named - entity recognition , I am stuck on how to encode the feature vector .", "label": "", "metadata": {}, "score": "66.487946"}
{"text": "D09 - 1126 [ bib ] : Matthew Honnibal ; James R. Curran Fully Lexicalising CCGbank with Hat Categories .D09 - 1127 [ bib ] : Liang Huang ; Wenbin Jiang ; Qun Liu Bilingually - Constrained ( Monolingual )Shift - Reduce Parsing .", "label": "", "metadata": {}, "score": "66.628525"}
{"text": "This is generally called a one hot encoding .So your feature vector is about size 100k with a little extra for POS and char tags , and is almost entirely 0s , except for 15 1s in positions picked according to your feature to index mappings .", "label": "", "metadata": {}, "score": "66.7185"}
{"text": "While there are non - terminal symbols ( in the currently considered sentential form ) , do the following:1 )Let A denote the leftmost non - terminal symbol .Draw a random number r from the interval ( 0,1].", "label": "", "metadata": {}, "score": "66.849"}
{"text": "D09 - 1052 [ bib ] : Koby Crammer ; Mark Dredze ; Alex Kulesza Multi - Class Confidence Weighted Algorithms .D09 - 1053 [ bib ] : Jianfeng Gao ; Qiang Wu ; Chris Burges ; Krysta Svore ; Yi Su ; Nazan Khan ; Shalin Shah ; Hongyan Zhou Model Adaptation via Model Interpolation and Boosting for Web Search Ranking .", "label": "", "metadata": {}, "score": "66.87973"}
{"text": "D09 - 1031 [ bib ] : Jason Baldridge ; Alexis Palmer How well does active learning actually work ?Time - based evaluation of cost - reduction strategies for language documentation .D09 - 1032 [ bib ] : Annie Louis ; Ani Nenkova Automatically Evaluating Content Selection in Summarization without Human Models .", "label": "", "metadata": {}, "score": "67.50148"}
{"text": "Parts Of Speech tagger or POS tagger is a program that does this job .Taggers use several kinds of information : dictionaries , lexicons , rules , and so on .Dictionaries have category or categories of a particular word .", "label": "", "metadata": {}, "score": "67.592735"}
{"text": "The last word is labeled as PERSON - END .Other words in the name are PERSON - MIDDLE .If a person 's name is expressed by a single word , it is labeled as PERSON - SINGLE .If a word does not belong to any named entities , it is labeled as OTHER .", "label": "", "metadata": {}, "score": "67.6245"}
{"text": "^ .J .p .^ .J .J .p .^ .K .p .^ .K .K . strands . in . interior .loop .p .^ .M .X .", "label": "", "metadata": {}, "score": "67.84333"}
{"text": "For further investigation on the accuracy of our random generator , we take on a completely different point of view and consider thermodynamics .Since we do not know the corresponding RNA sequences for the randomly generated structures , we can not use one of the common sequence - dependent thermodynamic models for RNAs .", "label": "", "metadata": {}, "score": "68.167595"}
{"text": "This rather descriptive and commonly used planar graph model for RNA secondary structures was first formalized in [ 5 ] .An example is shown in Figure 1 .An RNA secondary structure .Unpaired and paired bases are represented by white and gray points , respectively .", "label": "", "metadata": {}, "score": "68.193756"}
{"text": "D09 - 1114 [ bib ] : Nan Duan ; Mu Li ; Tong Xiao ; Ming ZhouThe Feature Subspace Method for SMT System Combination .D09 - 1115 [ bib ] : Yang Feng ; Yang Liu ; Haitao Mi ; Qun Liu ; Yajuan L\u00fc Lattice - based System Combination for Statistical Machine Translation .", "label": "", "metadata": {}, "score": "68.6281"}
{"text": "UPPARSE , the software used for the experiments in this paper , is available under an open - sourc ... . \" ...In this work we address the problem of unsupervised part - of - speech induction by bringing together several strands of research into a single model .", "label": "", "metadata": {}, "score": "68.70455"}
{"text": "y .s . ) s .Obviously , we can also compute the corresponding \" average energy points \" .A .v .E .P .S .e .n .e . r . g .y . )", "label": "", "metadata": {}, "score": "68.82975"}
{"text": "Prob .A . i .x . size .x . )n .Prob .A .x . size .x . )n . which is the posterior probability that we used production rule f i under the condition that a word of size n is generated .", "label": "", "metadata": {}, "score": "68.87726"}
{"text": "In Proc . of the 16th Nordic Conference on Computational Linguistics ( NODALIDA ) .Extended Sandra K\u00a8 ubler . schemes influence parsing results ? or how not to com- pare apples and oranges .In RANLP .How do treebank annotation 1054 .", "label": "", "metadata": {}, "score": "68.94003"}
{"text": "D09 - 1067 [ bib ] : Lin Sun ; Anna Korhonen Improving Verb Clustering with Automatically Acquired Selectional Preferences .D09 - 1068 [ bib ] : Yumao Lu ; Fuchun Peng ; Gilad Mishne ; Xing Wei ; Benoit Dumoulin Improving Web Search Relevance with Semantic Features .", "label": "", "metadata": {}, "score": "69.00821"}
{"text": "n . card .S .n . ) s .S .n .e .n .e . r . g .y .s . )S .n .and the corresponding \" energy variance points \" .", "label": "", "metadata": {}, "score": "69.00925"}
{"text": "Derivation of the Algorithm .In this section , we give a complete and detailled description of the derivation of our weighted unranking algorithm for RNA secondary structures .The different steps are made according to the approach described in [ 20 ] to get an unranking algorithm that generates random RNA secondary structures of a given size n according to the distribution on all these structures .", "label": "", "metadata": {}, "score": "69.02555"}
{"text": ".. \" ...We show how punctuation can be used to improve unsupervised dependency parsing .Our linguistic analysis confirms the strong connection between English punctuation and phrase boundaries in the Penn Treebank .However , approaches that naively include punctuation marks in the grammar ( as if they were wo ... \" .", "label": "", "metadata": {}, "score": "69.078224"}
{"text": "A . i . )n .C .A .B . and .C .n . , then .C .n . iff . size . size . or .j .size . size . and .", "label": "", "metadata": {}, "score": "69.090454"}
{"text": "H .P .Q .R .V .W .O .J .K .M .X .Y .Z .N .G .^ .sto . and .R . sto .contains exactly the following rules : . p .", "label": "", "metadata": {}, "score": "69.23253"}
{"text": "n . k . ) and .g .d .y .n .I . g .d .y .n .n . k . )( variant \" both \" ) . ] otherwise it is rejected .", "label": "", "metadata": {}, "score": "69.25937"}
{"text": "Harvard University Press .Koby Crammer , Ofer Dekel , Joseph Keshet , Shai Shalev-Shwartz , and Yoram Singer .Online passive- aggressive algorithms .Journal of Machine Learning Research , 7:551 - 585 , Mar. R. Johansson and P. Nugues .", "label": "", "metadata": {}, "score": "69.405106"}
{"text": "G . are WCFGs , then .G . and .G . are said to be word - equivalent iff .L .G . )L .G . and for each word .w .L .G . )", "label": "", "metadata": {}, "score": "69.42052"}
{"text": "D09 - 1138 [ bib ] : Yusuke Miyao ; Jun'ichi Tsujii Supervised Learning of a Probabilistic Lexicon of Verb Semantic Classes .D09 - 1139 [ bib ] : Christof M\u00fcller ; Iryna Gurevych A Study on the Semantic Relatedness of Query and Document Terms in Information Retrieval .", "label": "", "metadata": {}, "score": "69.51761"}
{"text": "d .y .n .n . k . )( variant \" dynamic \" ) . ] or .[ . g .s .t . a .t .I . g .s .t . a .", "label": "", "metadata": {}, "score": "69.76689"}
{"text": "D09 - 1149 [ bib ] : Longhua Qian ; Guodong Zhou ; Fang Kong ; Qiaoming Zhu Semi - Supervised Learning for Semantic Relation Classification using Stratified Sampling Strategy .D09 - 1150 [ bib ] : Changqin Quan ; Fuji Ren Construction of a Blog Emotion Corpus for Chinese Emotional Expression Analysis .", "label": "", "metadata": {}, "score": "69.919785"}
{"text": "^ .R .W .p .^ .V .J .O .p .^ .W .J .A .p .^ .O .A .K . other . interior .loops .", "label": "", "metadata": {}, "score": "70.02523"}
{"text": "Thank you very much for the clarification .On a related note , am I right in understanding that this approach is used for recognizing new named - entities as well ?Any suggestions from your side would be great . -", "label": "", "metadata": {}, "score": "70.312836"}
{"text": "Parallel data is not used , allowing the technique to be applied even in domains where human - translated texts are unavailable .We obtain state - of - theart performance for two tasks of structure prediction : unsupervised part - of - speech tagging and unsupervised dependency parsing . ... parsing .", "label": "", "metadata": {}, "score": "70.33002"}
{"text": "We selected with replacement 2000 training examples from the training data and trained three parsers .Each parser then tagged the remain- ing 13 K sentences , yielding 39 K parsed sentences .We then shuffled these sentences and trained a final parser .", "label": "", "metadata": {}, "score": "70.515976"}
{"text": "Background .Random biological sequences are a topic of great interest in genome analysis since , according to a powerful paradigm , they represent the background noise from which the actual biological information must differentiate .Accordingly , the generation of random sequences has been investigated for a long time .", "label": "", "metadata": {}, "score": "70.51663"}
{"text": "There , we compare the previously introduced \" average energy points \" and \" energy variance points \" to the analytically determined expected free energy and corresponding variance from [ 21 ] , respectively .( purple ) of a random RNA secondary structure of size n , together with the \" energy variance points \" .", "label": "", "metadata": {}, "score": "70.51895"}
{"text": "Free Word - Order Language .D09 - 1089 [ bib ] : Dmitry Davidov ; Ari Rappoport Enhancement of Lexical Concepts Using Cross - lingual Web Mining .D09 - 1090 [ bib ] : Istv\u00e1n Varga ; Shoichi Yokoyama Bilingual dictionary generation for low - resourced language pairs .", "label": "", "metadata": {}, "score": "70.787766"}
{"text": "These averaged values actually represent the free energy contributions that have to be added for the respective whole substructures .These models are based on the well - known Turner energy model [ 22 , 23 ] and model parameters have been derived from the same biological database ( of SSU and LSU rRNAs ) that we consider in this article .", "label": "", "metadata": {}, "score": "70.78801"}
{"text": "Part - of - speech ( POS ) induction is one of the most popular tasks in research on unsupervised NLP .Many different methods have been proposed , yet comparisons are difficult to make since there is little consensus on evaluation framework , and many papers evaluate against only one or two competitor syste ... \" .", "label": "", "metadata": {}, "score": "70.80854"}
{"text": "For example , run is both noun and verb .Taggers use probabilistic information to solve this ambiguity .There are mainly two type of taggers : rule - based and stochastic .Rule - based taggers use hand - written rules to distinguish the tag ambiguity .", "label": "", "metadata": {}, "score": "70.90857"}
{"text": "G . sto .contains exactly the following rules : . p .S .T .A .C .p .T .T .A .C .p .T .C .p .C .C .", "label": "", "metadata": {}, "score": "71.06395"}
{"text": "Basically you have build a map from ( non - rare ) words in the training set to indicies .Let 's say you have 20k unique words in your training set .You 'll have mapping from every word in the training set to [ 0 , 20000].", "label": "", "metadata": {}, "score": "71.227646"}
{"text": "Many - to-1 : ' ' 'Map every induced label to a gold standard tag greedily ( 45 labels to 45 tags of the Penn tag set ) .Use the mapping to compute tag accuracy on the Wall Street Journal portion of the Penn TreeBank .", "label": "", "metadata": {}, "score": "71.251434"}
{"text": "D09 - 1146 [ bib ] : Michael Paul ; Roxana Girju Cross - Cultural Analysis of Blogs and Forums with Mixed - Collection Topic Models .D09 - 1147 [ bib ] : Adam Pauls ; John Denero ; Dan Klein Consensus Training for Consensus Decoding in Machine Translation .", "label": "", "metadata": {}, "score": "71.25749"}
{"text": "D09 - 1042 [ bib ] : Wei Lu ; Hwee Tou Ng ; Wee Sun Lee Natural Language Generation with Tree Conditional Random Fields .D09 - 1043 [ bib ] : Michael White ; Rajakrishnan Rajkumar Perceptron Reranking for CCG Realization .", "label": "", "metadata": {}, "score": "71.769"}
{"text": "O .n . arithmetic operations when using the sequential method and .O .n . log .n . ) operations when using the boustrophedon method ( for details we refer to [ 25 ] ) .Obviously , using uniform unranking procedures to construct the i th structure of size n for a randomly drawn number i , any structure of size n is equiprobably generated .", "label": "", "metadata": {}, "score": "71.79444"}
{"text": "Contents .Many - to-1 : Map every induced label to a gold standard tag greedily ( 45 labels to 45 tags of the Penn tag set ) .Use the mapping to compute tag accuracy on the Wall Street Journal portion of the Penn TreeBank .", "label": "", "metadata": {}, "score": "71.87381"}
{"text": "D09 - 1158 [ bib ] : Dan Wu ; Wee Sun Lee ; Nan Ye ; Hai Leong Chieu Domain adaptive bootstrapping for named entity recognition .D09 - 1159 [ bib ] : Yuanbin Wu ; Qi Zhang ; Xuangjing Huang ; Lide Wu Phrase Dependency Parsing for Opinion Mining .", "label": "", "metadata": {}, "score": "71.91066"}
{"text": "However , this rule leads to productions with a conclusion of length 1 not being reweighted , hence we have to assure that all those productions already have integral weights .Furthermore , \u03f5 -productions need a special treatment .We do n't want to discuss full details here and conclude by noticing that the reweighting normal form ( RNF ) keeps track of all possible issues : .", "label": "", "metadata": {}, "score": "72.13396"}
{"text": "G . s .t .o .contains exactly the following rules : . p .S .S . p .S .A . p .S .A .C .p .S .T .A . p .", "label": "", "metadata": {}, "score": "72.19384"}
{"text": "Tested across six domains , our system outperforms all non - oracle baselines including the best domain - independent parsing model .Thus , we are able to demonstrate the value of customizing parsing models to specific domains . ... train models in many different domains but sidestep the problem of domain detection .", "label": "", "metadata": {}, "score": "72.44028"}
{"text": "D09 - 1098 [ bib ] : Patrick Pantel ; Eric Crestan ; Arkady Borkovsky ; Ana - Maria Popescu ; Vishnu Vyas Web - Scale Distributional Similarity and Entity Set Expansion .D09 - 1099 [ bib ] : Eduard Hovy ; Zornitsa Kozareva ; Ellen Riloff Toward Completeness in Concept Extraction and Classification .", "label": "", "metadata": {}, "score": "72.53648"}
{"text": "D09 - 1101 [ bib ] : Altaf Rahman ; Vincent Ng Supervised Models for Coreference Resolution .D09 - 1102 [ bib ] : GuoDong Zhou ; Fang Kong Global Learning of Noun Phrase Anaphoricity in Coreference Resolution via Label Propagation .", "label": "", "metadata": {}, "score": "72.6544"}
{"text": "Z .S . w .S . w .S . w .S .B . w .B . w .B . w .B .C .w .C .w .C . which can be simplified in the following way : .", "label": "", "metadata": {}, "score": "72.66577"}
{"text": "T .E .p .^ .T .C . shape . of .exterior . loop .p .^ .C .p .^ .C .C . strands . in .exterior . loop .", "label": "", "metadata": {}, "score": "73.00038"}
{"text": "p .^ .N .Z .p .^ .N .U . multiple . loop .p .^ .U .p .^ .U .U . strands . in .multiple . loop .", "label": "", "metadata": {}, "score": "73.02864"}
{"text": "Compiler or interpreter , lexicon and guessor make what is known as lexical analyzer .Ambiguity Resolution : This is also called disambiguation .Disambiguation is based on information about word such as the probability of the word .For example , power is more likely used as noun than as verb .", "label": "", "metadata": {}, "score": "73.13705"}
{"text": "Safety Collet Clamping System .The ER Zeta safety collet clamping system from Zollmann GmbH offers a unique profile that allows for various mounting adapters or spanner wrench to be screwed on by means of a bayonet - style locking feature .", "label": "", "metadata": {}, "score": "73.17918"}
{"text": "Publisher conditions are provided by RoMEO .Differing provisions from the publisher 's actual policy or licence agreement may be applicable .\" One of the main challenges in natural language processing ( NLP ) is to correct for biases in the manually annotated data available to system engineers .", "label": "", "metadata": {}, "score": "73.40064"}
{"text": "D09 - 1061 [ bib ] : Sajib Dasgupta ; Vincent Ng Topic - wise , Sentiment - wise , or Otherwise ?Identifying the Hidden Dimension for Unsupervised Text Classification .D09 - 1062 [ bib ] : Yejin Choi ; Claire Cardie Adapting a Polarity Lexicon using Integer Linear Programming for Domain - Specific Sentiment Classification .", "label": "", "metadata": {}, "score": "73.62089"}
{"text": "In this paper , we address two issues that are related to domain adaptation .The first question is how much genre variation will affect NLP systems ' per ... \" .Domain adaptation is an important task in order for NLP systems to work well in real applications .", "label": "", "metadata": {}, "score": "73.73251"}
{"text": "Considered Combinatorial Class .According to the common definition of RNA secondary structure , we decided to consider the combinatorial class of all RNA secondary structures without pseudoknots that meet the stereochemical constraint of hairpin loops consisting of at least 3 unpaired nucleotides , formally : .", "label": "", "metadata": {}, "score": "73.82751"}
{"text": "P .A .p .^ .P .A . small . interior .loops .p .^ .Q .O .p .^ .Q .V .p .^ .R .O .", "label": "", "metadata": {}, "score": "74.00309"}
{"text": "D09 - 1152 [ bib ] : Prakash Srinivasan ; Alexander Yates Quantifier Scope Disambiguation Using Extracted Pragmatic Knowledge : Preliminary Results .D09 - 1153 [ bib ] : Weiwei Sun ; Zhifang Sui ; Meng Wang ; Xin Wang Chinese Semantic Role Labeling with Shallow Parsing .", "label": "", "metadata": {}, "score": "74.0569"}
{"text": "C .n . , then .C .n . iff .A . i . )n . and .A .j . )n . and .i .j .or .A . i . )", "label": "", "metadata": {}, "score": "74.216354"}
{"text": "The collet nuts offer a high clamping torque that reduces overall runout .The ER Zeta safety collet clamping system from Zollmann GmbH offers a unique profile that allows for various mounting adapters or spanner wrench to be screwed on by means of a bayonet - style locking feature .", "label": "", "metadata": {}, "score": "74.22914"}
{"text": "D09 - 1064 [ bib ] : Nilesh Dalvi ; Ravi Kumar ; Bo Pang ; Andrew Tomkins Matching Reviews to Objects using a Language Model .D09 - 1065 [ bib ] : Brian Murphy ; Marco Baroni ; Massimo Poesio EEG responds to conceptual stimuli and corpus semantics .", "label": "", "metadata": {}, "score": "74.32237"}
{"text": "n .e . r . g .y .n .E .e .n .e . r . g .y .s . ) size .s . )n . and .e .n .", "label": "", "metadata": {}, "score": "74.32785"}
{"text": "The collet nuts offer a high clamping torque that reduces overall runout .The nuts are available in various sizes for both DIN6499 and DIN6388 collet styles .They are also available for Regofix sealing systems , Zollmann sealing systems and Mini nut .", "label": "", "metadata": {}, "score": "74.74986"}
{"text": "A . i .n .C .A .A . k .j .n .s .i .z .e .A .j .size .B .n .j .C .A .", "label": "", "metadata": {}, "score": "74.75282"}
{"text": "Speech processing uses POS tags to decide the pronunciation .POS tagger is used for making tagged corpora .I was checking out Stanford CoreNLP in order to understand NER and POS tagging . is CoreNLP useful in this case ?Yes , CoreNLP can use custom \" tags \" .", "label": "", "metadata": {}, "score": "74.77832"}
{"text": "d . into the following one : . w .^ .S .B . w .^ .B .B . ) w .^ .B .C .w .^ .C .w .^ .", "label": "", "metadata": {}, "score": "74.82022"}
{"text": "For ex- ample , trained on in - domain data , nouns that occur more often tend to be heads .However , none of these features transfered between domains .A final type of feature we added was based on the behavior of nouns , adjectives and verbs in each domain .", "label": "", "metadata": {}, "score": "74.93745"}
{"text": "^ .F .H .p .^ .H .p .^ .H .H . hairpin . loop .p .^ .P .A .p .^ .P .A .p .", "label": "", "metadata": {}, "score": "75.06112"}
{"text": "D09 - 1049 [ bib ] : Ram Boukobza ; Ari Rappoport Multi - Word Expression Identification Using Sentence Surface Features .D09 - 1050 [ bib ] : Ming - Hong Bai ; Jia - Ming You ; Keh - Jiann Chen ; Jason S. Chang Acquiring Translation Equivalences of Multiword Expressions by Normalized Correlation Frequencies .", "label": "", "metadata": {}, "score": "75.129974"}
{"text": "D09 - 1120 [ bib ] : Aria Haghighi ; Dan Klein Simple Coreference Resolution with Rich Syntactic and Semantic Features .D09 - 1121 [ bib ] : Tadayoshi Hara ; Yusuke Miyao ; Jun'ichi Tsujii Descriptive and Empirical Approaches to Capturing Underlying Dependencies among Parsing Errors .", "label": "", "metadata": {}, "score": "75.19761"}
{"text": "D09 - 1095 [ bib ] : Vivi Nastase ; Michael Strube Combining Collocations , Lexical and Encyclopedic Knowledge for Metonymy Resolution .D09 - 1096 [ bib ] : Andrew Lampert ; Robert Dale ; C\u00e9cile Paris Segmenting Email Message Text into Zones .", "label": "", "metadata": {}, "score": "75.560104"}
{"text": "S .E .p .^ .E .S . p .^ .E .S .C .p .^ .S .A .p .^ .S .T .A .p .", "label": "", "metadata": {}, "score": "75.994896"}
{"text": "n . ) w . size .B .n . )I .B .w . size .C .n . ) w . size .C .n . )I .C . else . . .", "label": "", "metadata": {}, "score": "76.04211"}
{"text": "Each sample is represented by a long binary vector , i.e. , a sequence of 0 ( false ) and 1 ( true ) .Only 15 elements are 1 .I do n't really understand how the binary vector here is being constructed .", "label": "", "metadata": {}, "score": "76.10276"}
{"text": "I .B . and .n . size .C .n .I .C .I .C . and .n .w . size .B .n . ) w . size .C .n . ) w .", "label": "", "metadata": {}, "score": "76.15939"}
{"text": "The results show that an unsupervised technique based on topic models is effective - it outperforms random data selection on both languages examined , English and Dutch .Moreover , the technique works better than manually assigned labels gathered from meta - data that is available for English . ...", "label": "", "metadata": {}, "score": "76.26165"}
{"text": "t . a .t .I . g .s .t . a .t .n . k . )( variant \" static \" ) . ] or .[ . g .d .y .n .", "label": "", "metadata": {}, "score": "76.310425"}
{"text": "B .C .Z . )B .Z .B .C .Z . )C .Z .C .C .Z .C .B .C .C .B .C .Z .", "label": "", "metadata": {}, "score": "76.39722"}
{"text": "p .^ .G .A .p .^ .G .A .D .p .^ .G .A .p .^ .G .D .A . shape . of . bulge .", "label": "", "metadata": {}, "score": "76.68835"}
{"text": "D09 - 1117 [ bib ] : Andrew Finch ; Eiichiro Sumita Bidirectional Phrase - based Statistical Machine Translation .D09 - 1118 [ bib ] : Matthew Frampton ; Jia Huang ; Trung Bui ; Stanley Peters Real - time decision detection in multi - party dialogue .", "label": "", "metadata": {}, "score": "76.74706"}
{"text": "y .n .V .e .n .e . r . g .y .s . ) size .s . )n . k . percent of the energies in .e .n .e . r . g .", "label": "", "metadata": {}, "score": "76.95215"}
{"text": "D09 - 1080 [ bib ] : Erin Fitzgerald ; Frederick Jelinek ; Keith Hall Integrating sentence- and word - level error identification for disfluency correction .D09 - 1081 [ bib ] : Yuval Marton ; Saif Mohammad ; Philip Resnik Estimating Semantic Distance Using Soft Semantic Constraints in Knowledge - Source - Corpus Hybrid Models .", "label": "", "metadata": {}, "score": "77.42441"}
{"text": "I . sto .sto .R . sto .S . , where .I .G .^ .sto .S .E .S .T .C .A .L .G .D .B .", "label": "", "metadata": {}, "score": "77.539276"}
{"text": "These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation .Therefore , the standard evaluation does not provide a true indication of algorithm quality .We present a new measure , Neutral Edge Direction ( NED ) , and show that it greatly reduces this undesired phenomenon .", "label": "", "metadata": {}, "score": "77.5657"}
{"text": "F .p .L .G .p .G .L . )p .G .L . )B .p .G .L . )p .G .B .L . )p .", "label": "", "metadata": {}, "score": "78.05238"}
{"text": "p .i . must hold .i . k .p .i .r .i . k .p .i .r .i . k .p .i .i . k .p .i .", "label": "", "metadata": {}, "score": "78.07622"}
{"text": "D09 - 1076 [ bib ] : Steve DeNeefe ; Kevin Knight Synchronous Tree Adjoining Machine Translation .D09 - 1077 [ bib ] : Tim Miller Word Buffering Models for Improved Speech Repair Parsing .D09 - 1078 [ bib ] : Robert C. Moore ; Chris Quirk Less is More : Significance - Based N - gram Selection for Smaller , Better Language Models .", "label": "", "metadata": {}, "score": "78.55017"}
{"text": "A .B .n .A .B .n .A .n .B . 0 . . .If .C .A .B . and .C .n . , then .C .n . iff .", "label": "", "metadata": {}, "score": "78.60987"}
{"text": "5 Acknowledgments We thank Joel Wallenberg and Nikhil Dinesh for their informative and helpful linguistic expertise , Kevin Lerman for his edge labeler code , and Koby Crammer for helpful conversations .Dredze is sup- ported by a NDSEG fellowship ; Ganchev and Taluk- dar by NSF ITR EIA-0205448 ; and Blitzer by DARPA under Contract No . NBCHD03001 .", "label": "", "metadata": {}, "score": "79.01907"}
{"text": "S . and .n .w . size .B .n . ) w . size .C .n . ) w .I .S . and .n .w . size .B .n . ) w . size .", "label": "", "metadata": {}, "score": "79.07488"}
{"text": "Z . )Z .C .Z . )Z .Z .Z . )C .Z .C .C .Z .Z .S . w .w .C .w .Z .w .", "label": "", "metadata": {}, "score": "79.621254"}
{"text": "^ .A .L . ) initiate . helix .p .^ .L .A .p .^ .L .M . initiate . stacked . pair . or .multiple . loop .p .^ .", "label": "", "metadata": {}, "score": "79.683716"}
{"text": "r .E .P .S .e .n .e . r . g .y . )n .n . card .S .n . ) s .S .n .n .e .", "label": "", "metadata": {}, "score": "79.928894"}
{"text": "D09 - 1028 [ bib ] : Dmitry Davidov ; Ari Rappoport Geo - mining : Discovery of Road and Transport Networks Using Directional Patterns .D09 - 1029 [ bib ] : Sara Tonelli ; Claudio Giuliano Wikipedia as Frame Information Repository .", "label": "", "metadata": {}, "score": "80.31045"}
{"text": "P .p .^ .L .Q .p .^ .L .R . initiate . interior .loop .p .^ .L .F .p .^ .L .G . initiate . hairpin . loop . or . bulge .", "label": "", "metadata": {}, "score": "80.53628"}
{"text": "D09 - 1129 [ bib ] : Aminul Islam ; Diana Inkpen Real - Word Spelling Correction using Google Web 1 T 3-grams .D09 - 1130 [ bib ] : Minwoo Jeong ; Chin - Yew Lin ; Gary Geunbae Lee Semi - supervised Speech Act Recognition in Emails and Forums .", "label": "", "metadata": {}, "score": "80.58606"}
{"text": "S .B . w .^ .w .^ .S .B .C .w .^ .w .^ .w .^ .S .B .C .w .^ .B .", "label": "", "metadata": {}, "score": "81.58268"}
{"text": "D09 - 1111 [ bib ] : Colin Cherry ; Hisami Suzuki Discriminative Substring Decoding for Transliteration .D09 - 1112 [ bib ] : Marco Dinarelli ; Alessandro Moschitti ; Giuseppe Riccardi Re - Ranking Models Based - on Small Training Data for Spoken Language Understanding .", "label": "", "metadata": {}, "score": "81.732025"}
{"text": "C .p .A .L . )p .L .L . )p .L .M .p .L .P .p .L .Q .p .L .R .p .", "label": "", "metadata": {}, "score": "81.75303"}
{"text": "RNA is a single - stranded nucleotide polymer and a major component of cellular processes ( like DNA and proteins ) .An RNA strand is formed by linking together certain nucleotide units .The specific sequence of nucleotides along this chain is called the primary structure of the molecule .", "label": "", "metadata": {}, "score": "82.11065"}
{"text": "D09 - 1034 [ bib ] : Brian Roark ; Asaf Bachrach ; Carlos Cardenas ; Christophe Pallier Deriving lexical and syntactic expectation - based measures for psycholinguistic modeling via incremental top - down parsing .D09 - 1035 [ bib ] : Rajesh Ranganath ; Dan Jurafsky ; Dan McFarland It 's Not You , it 's Me : Detecting Flirting and its Misperception in Speed - Dates .", "label": "", "metadata": {}, "score": "82.93884"}
{"text": "G . s .t .o .S .T .C .A .L .G .B .F .H .P .Q .R .J .K .M .N .G . sto . and .", "label": "", "metadata": {}, "score": "83.00812"}
{"text": "w .B .C .B . )C .A . w .B .B . ) w .B .C .B . ) w .B .B . )C .w .B .", "label": "", "metadata": {}, "score": "83.02801"}
{"text": "D09 - 1058 [ bib ] : Jun Suzuki ; Hideki Isozaki ; Xavier Carreras ; Michael Collins An Empirical Study of Semi - supervised Structured Conditional Models for Dependency Parsing .D09 - 1059 [ bib ] : Richard Johansson Statistical Bistratal Dependency Parsing .", "label": "", "metadata": {}, "score": "83.050514"}
{"text": "P .L . )p .P .L . )p .Q .L . )K .p .Q .J .L . )K .p .R .L . )K .p .", "label": "", "metadata": {}, "score": "83.44192"}
{"text": "B .C . w .S .B .C .w .B .B . ) w .B .B .C .w .B .B .C . ) w .C .C .", "label": "", "metadata": {}, "score": "83.47618"}
{"text": "D09 - 1039 [ bib ] : Daniel Galron ; Sergio Penkale ; Andy Way ; I. Dan Melamed Accuracy - Based Scoring for DOT : Towards Direct Error Minimization for Data - Oriented Translation .D09 - 1040 [ bib ] : Yuval Marton ; Chris Callison - Burch ; Philip Resnik Improved Statistical Machine Translation Using Monolingually - Derived Paraphrases .", "label": "", "metadata": {}, "score": "83.82139"}
{"text": "min . size . size . or .min . size . size .min . size . size .B . and . size . size . or .j .size . size . and .A . ) j .", "label": "", "metadata": {}, "score": "84.21505"}
{"text": "J .L . )p .J .J .p .J .p .K .K .p .K .p .M .U .L . )U .L . )N .p .", "label": "", "metadata": {}, "score": "84.55791"}
{"text": "p .C .p .C .C .p .A .L . )p .L .L . )p .L .M .p .L .P .p .L .Q .", "label": "", "metadata": {}, "score": "84.93018"}
{"text": "^ .w .^ .B .B .C .w .^ .w .^ .w .^ .B .B .C . ) w .^ .C .C .w .^ .", "label": "", "metadata": {}, "score": "85.25636"}
{"text": "B .p .B .p .F .p .F .p .F .H .p .H .H .p .H .p .P .L . )p .P .L . )", "label": "", "metadata": {}, "score": "85.40084"}
{"text": "Word : Paper , Tag : Noun Word : Go , Tag : Verb Word : Famous , Tag : Adjective .Note that some words can have more than one tag associated with .For example , chair can be noun or verb depending on the context .", "label": "", "metadata": {}, "score": "85.42467"}
{"text": "t .o .S .S .T .C .A .L .G .B .F .H .P .Q .R .J .K .M .N .G . sto . and .", "label": "", "metadata": {}, "score": "85.49992"}
{"text": "G . s .t .o .I .G . s .t .o .G . s .t .o .R .G . s .t .o .S . , where .I .", "label": "", "metadata": {}, "score": "85.51042"}
{"text": "p .^ .X .A .p .^ .X .U .A .p .^ .Y .Z .p .^ .Z .X .p .^ .Z .X .", "label": "", "metadata": {}, "score": "87.017334"}
{"text": "n . )C . is . neutral . and .n .C . is . neutral . and .n .C . is . atomic . and .n .C . is . atomic . and .n .", "label": "", "metadata": {}, "score": "90.76122"}
{"text": "D09 - 1104 [ bib ] : Octavian Popescu Person Cross Document Coreference with Name Perplexity Estimates .D09 - 1105 [ bib ] : Roy Tromble ; Jason Eisner Learning Linear Ordering Problems for Better Translation .D09 - 1106 [ bib ] : Yang Liu ; Tian Xia ; Xinyan Xiao ; Qun Liu Weighted Alignment Matrices for Statistical Machine Translation .", "label": "", "metadata": {}, "score": "90.96091"}
{"text": "Another problem concerns appositives .For ex- ample , the phrase \" Howard Mosher , president and chief executive officer , \" has \" Mosher \" as the head of \" Howard \" and of the appositive NP delimited by commas .", "label": "", "metadata": {}, "score": "91.202194"}
{"text": "D09 - 1141 [ bib ] : Preslav Nakov ; Hwee Tou Ng Improved Statistical Machine Translation for Resource - Poor Languages Using Related Resource - Rich Languages .D09 - 1142 [ bib ] : Vivi Nastase ; Marius Popescu What 's in a name ?", "label": "", "metadata": {}, "score": "91.36302"}
{"text": "T .A .C .p .T .A .p .T .A .C .p .T .T .A .p .T .T .A .C .p .T .", "label": "", "metadata": {}, "score": "98.44516"}
{"text": "Proceedings of the CoNLL Shared Task Session of EMNLP - CoNLL 2007 , pp .1051 - 1055 , Prague , June 2007 .c ?upenn.edu 2L2F - INESC - ID Lisboa / IST , Rua Alves Redol 9 , 1000 - 029 , Lisboa , Portugal javg@l2f.inesc-id.pt Abstract We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation .", "label": "", "metadata": {}, "score": "105.7641"}
