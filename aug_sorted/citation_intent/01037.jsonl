{"text": "It is common to work with discrete or Gaussian Distributions since that simplifies calculations .Sometimes only constraints on a distribution are known ; one can then use the principle of maximum entropy to determine a single distribution , the one with the greatest entropy given the constraints .", "label": "", "metadata": {}, "score": "27.503609"}
{"text": "This solution , however , assumes that the experimental noise can be interpreted as \" hard \" limits and does not represent the fact that the experimental measurement is just an estimate of the underlying true value .As an alternative solution to this problem , Cavalli et al . propose a combination of the maximum entropy principle and Bayesian inference with a prior distribution that reflects the uncertainty of the measured quantity .", "label": "", "metadata": {}, "score": "28.42815"}
{"text": "This solution , however , assumes that the experimental noise can be interpreted as \" hard \" limits and does not represent the fact that the experimental measurement is just an estimate of the underlying true value .As an alternative solution to this problem , Cavalli et al . propose a combination of the maximum entropy principle and Bayesian inference with a prior distribution that reflects the uncertainty of the measured quantity .", "label": "", "metadata": {}, "score": "28.42815"}
{"text": "With this brief introduction , we hope that we have conveyed the general applicability of the principle of maximum entropy .In particular , in combination with Bayesian inference , it is a powerful tool for consistent reasoning in the light of new data .", "label": "", "metadata": {}, "score": "29.198116"}
{"text": "With this brief introduction , we hope that we have conveyed the general applicability of the principle of maximum entropy .In particular , in combination with Bayesian inference , it is a powerful tool for consistent reasoning in the light of new data .", "label": "", "metadata": {}, "score": "29.198116"}
{"text": "Next , through examples , it shows that maximizing entropy sometimes can stand in direct opposition to Bayesian updating based on reasonable prior beliefs .The paper concludes that if we take the Bayesian approach that probability is about reasonable belief based on all available information , then we can resolve the conflict between the maximum entropy approach and the Bayesian approach that is demonstrated in the examples .", "label": "", "metadata": {}, "score": "32.41858"}
{"text": "However , it was Shannon 's effort to quantify information that led him to entropy [ 1 ] , which in turn inspired Jayne 's maximum entropy principle .Information and entropy ( uncertainty ) have a fairly simple and intuitive relationship : The more information , the narrower the probability distribution , and the less the entropy .", "label": "", "metadata": {}, "score": "32.631172"}
{"text": "Maximum entropy models that constrain only the firing rates of all the neurons ( i.e. ) are called \" independent models \" ; we denote their distribution functions by .As a second constraint we take the correlations between neurons , two by two .", "label": "", "metadata": {}, "score": "33.381332"}
{"text": "A guide for authors and other relevant information for submission of manuscripts is available on the Instructions for Authors page .Entropy is an international peer - reviewed Open Access monthly journal published by MDPI .Abstract : The Principle of Maximum Entropy is often used to update probabilities due to evidence instead of performing Bayesian updating using Bayes ' Theorem , and its use often has efficacious results .", "label": "", "metadata": {}, "score": "33.384834"}
{"text": "In a sense , the maximum entropy approach is the opposite of what we usually do in making models or theories .The conventional approach is to hypothesize some dynamics for the network we are studying , and then calculate the consequences of these assumptions ; inevitably , the assumptions we make will be wrong in detail .", "label": "", "metadata": {}, "score": "34.52842"}
{"text": "Furthermore , some numerical experiments indicate that the robust model in this paper can result in betterportfolio performance than a classical mean - variance model .Abstract : The maximum entropy method was originally proposed as a variational technique to determine probability densities from the knowledge of a few expected values .", "label": "", "metadata": {}, "score": "34.76849"}
{"text": "This procedure is sometimes referred to as the principle of minimum discrimination information or minimum cross entropy , but can be seen as a natural extension of the maximum entropy approach .There is a substantial literature on the foundations of maximum entropy and why it is an appropriate framework for inference [ 1 ] , [ 14 ] - [ 16 ] .", "label": "", "metadata": {}, "score": "35.134197"}
{"text": "This procedure is sometimes referred to as the principle of minimum discrimination information or minimum cross entropy , but can be seen as a natural extension of the maximum entropy approach .There is a substantial literature on the foundations of maximum entropy and why it is an appropriate framework for inference [ 1 ] , [ 14 ] - [ 16 ] .", "label": "", "metadata": {}, "score": "35.134197"}
{"text": "In addition , their choice of words throughout the text implies a frequentist definition ( see above ) .However , they also make extensive use of conditional probabilities , as well as the maximum entropy principle advocated by Jaynes ( for example , see footnote on page 118 [ 33 ] ) .", "label": "", "metadata": {}, "score": "35.457146"}
{"text": "The motivation is that the Shannon entropy of a probability distribution measures the amount of information contained the distribution .The larger the entropy , the less information is provided by the distribution .Thus , by maximizing the entropy over a suitable set of probability distributions on , one finds that distribution that is least informative in the sense that it contains the least amount of information consistent with the constraints that define the set .", "label": "", "metadata": {}, "score": "35.66456"}
{"text": "Finally , the fact that a maximum entropy model which matches a particular set of experimental observations is successful does not mean that this choice of observables ( e.g. , pairwise correlations ) is unique or minimal .For all these reasons , we do not think about our models in terms of their parameters , but rather as a description of the probability distribution itself , which encodes the collective behavior of the system .", "label": "", "metadata": {}, "score": "36.264275"}
{"text": "Note that there is an important difference between adding a constraint on the second moment ( the variance ) and incorporating knowledge about the error of the first moment ( error of the mean ) .The former can easily be dealt with using the maximum entropy principle , while the latter is more problematic .", "label": "", "metadata": {}, "score": "36.5535"}
{"text": "Note that there is an important difference between adding a constraint on the second moment ( the variance ) and incorporating knowledge about the error of the first moment ( error of the mean ) .The former can easily be dealt with using the maximum entropy principle , while the latter is more problematic .", "label": "", "metadata": {}, "score": "36.5535"}
{"text": "The central idea of the maximum entropy method is that , for each experimental observation that we want to reproduce , we add only the minimum amount of structure required .An important feature of the maximum entropy approach is that the mathematical form of a maximum entropy model is exactly equivalent to a problem in statistical mechanics .", "label": "", "metadata": {}, "score": "36.82892"}
{"text": "Maximum entropy approach .Given the possibility of both overfitting experimental data and underrestraining by unfavourable data - to - parameter ratios , it would be preferable to have a theoretically well - founded method for combining experiment and simulation .Incorporating experimental data into a simulation is essentially a matter of updating a probability distribution ( the original Boltzmann distribution defined by the force field ) in the light of new data .", "label": "", "metadata": {}, "score": "36.884357"}
{"text": "Maximum entropy approach .Given the possibility of both overfitting experimental data and underrestraining by unfavourable data - to - parameter ratios , it would be preferable to have a theoretically well - founded method for combining experiment and simulation .Incorporating experimental data into a simulation is essentially a matter of updating a probability distribution ( the original Boltzmann distribution defined by the force field ) in the light of new data .", "label": "", "metadata": {}, "score": "36.884357"}
{"text": "The maximum entropy principle of Jaynes is a formal expression of logic , and it is the most fundamental principle within probability theory , since it defines probabilities and enables their derivation .All probabilities should be maximum entropy distributions ( MEDs ) .", "label": "", "metadata": {}, "score": "38.524902"}
{"text": "This conditionality is the defining feature of Jaynesian probabilities .The rules relating knowledge to probability are essentially just \" common sense \" or logic , as embodied within the principle of maximum entropy .The maximum entropy principle requires that we fully acknowledge our ignorance by considering all possibilities equally probable unless we have evidence to the contrary .", "label": "", "metadata": {}, "score": "38.55767"}
{"text": "Higher values of will draw the mean value closer to the restraint , while increasing values of will increase the variance .Convergence can be assessed by simultaneously probing the violation of the expectation values relative to the restraints and the entropy of the distribution ( or the entropy corresponding to the restrained subspace ) .", "label": "", "metadata": {}, "score": "38.646706"}
{"text": "Higher values of will draw the mean value closer to the restraint , while increasing values of will increase the variance .Convergence can be assessed by simultaneously probing the violation of the expectation values relative to the restraints and the entropy of the distribution ( or the entropy corresponding to the restrained subspace ) .", "label": "", "metadata": {}, "score": "38.646706"}
{"text": "The maximum entropy distribution ( MED ) is the one that most fully acknowledges ignorance , making no assumptions beyond those required by the available evidence .According to Jaynes , all probability distributions are ( or should be ) MEDs .", "label": "", "metadata": {}, "score": "38.808628"}
{"text": "In particular , the maximum entropy approach made clear that genuinely collective behavior in the network can be consistent with relatively weak correlations among pairs of neurons , so long as these correlations are widespread , shared among most pairs of cells in the system .", "label": "", "metadata": {}, "score": "39.094833"}
{"text": "Since the general goal of probability theory is to derive probabilities distributions ( MEDs ) , and since in principle this can be done directly through entropy maximization , BT is not essential , in principle .As a practical matter , it is extremely useful because it allows us to generate new MEDs from existing MEDs in a manner that is much easier mathematically than directly maximizing entropy .", "label": "", "metadata": {}, "score": "39.4272"}
{"text": "Maximum entropy models have a simple form [ Eq ( 1 ) ] that connects precisely with statistical physics .But to complete the construction of a maximum entropy model , we need to impose the condition that averages in the maximum entropy distribution match the experimental measurements , as in Eq ( 4 ) .", "label": "", "metadata": {}, "score": "39.50143"}
{"text": "Often these conditional distributions include parameters which are unknown and must be estimated from data , sometimes using the maximum likelihood approach .Direct maximization of the likelihood ( or of the posterior probability ) is often complex when there are unobserved variables .", "label": "", "metadata": {}, "score": "39.775887"}
{"text": "The central idea is that among all the infinite number of distributions that are compatible with the data , one should select the distribution which maintains the largest degree of uncertainty about the variables of interest , thus ensuring that the data has been used as conservatively as possible .", "label": "", "metadata": {}, "score": "39.85506"}
{"text": "The central idea is that among all the infinite number of distributions that are compatible with the data , one should select the distribution which maintains the largest degree of uncertainty about the variables of interest , thus ensuring that the data has been used as conservatively as possible .", "label": "", "metadata": {}, "score": "39.85506"}
{"text": "Abstract : The maximum entropy method is a theoretically sound approach to construct an analytical form for the probability density function ( pdf ) given a sample of random events .In practice , numerical methods employed to determine the appropriate Lagrange multipliers associated with a set of moments are generally unstable in the presence of noise due to limited sampling .", "label": "", "metadata": {}, "score": "39.97889"}
{"text": "In particular , they showed that the maximum entropy solution appears as a limit of the replica method when the harmonic potential enforcing the replica - averaged restraint becomes infinitely narrow .More precisely , the distribution of each replica in a replica - averaged ensemble simulation ( Eq . 1 ) will approach the maximum entropy distribution if both and .", "label": "", "metadata": {}, "score": "40.354248"}
{"text": "In particular , they showed that the maximum entropy solution appears as a limit of the replica method when the harmonic potential enforcing the replica - averaged restraint becomes infinitely narrow .More precisely , the distribution of each replica in a replica - averaged ensemble simulation ( Eq . 1 ) will approach the maximum entropy distribution if both and .", "label": "", "metadata": {}, "score": "40.354248"}
{"text": "The \" frequentist \" view is that probabilities are ( or can be ) essentially equivalent to frequencies , and that they are therefore properties of a physical system , independent of any observer of the system .E.T. Jaynes developed the alternate \" Bayesian \" definition , in which probabilities are always conditional on a state of knowledge through the rules of logic , as expressed in the maximum entropy principle .", "label": "", "metadata": {}, "score": "40.38219"}
{"text": "And in the continuous case , the maximum entropy prior given that the density is normalized with mean zero and variance unity is the standard normal distribution .A related idea , reference priors , was introduced by Jose M. Bernardo .", "label": "", "metadata": {}, "score": "40.821514"}
{"text": "The model of maximum entropy distributions is characterized to be totally geodesic with respect to the linear connection associated with the divergence .A natural extension for the classical theory for the maximum likelihood method under the maximum entropy model in terms of the Boltzmann - Gibbs - Shannon entropy is given .", "label": "", "metadata": {}, "score": "41.07672"}
{"text": "The relationship between replica - based simulations and the maximum entropy formalism was clarified and mathematically proven in papers by Roux and Weare [ 10 ] and Cavalli et al .[ 11 ] , both of which demonstrated that a replica - based approach is equivalent to the maximum entropy solution .", "label": "", "metadata": {}, "score": "41.111855"}
{"text": "The relationship between replica - based simulations and the maximum entropy formalism was clarified and mathematically proven in papers by Roux and Weare [ 10 ] and Cavalli et al .[ 11 ] , both of which demonstrated that a replica - based approach is equivalent to the maximum entropy solution .", "label": "", "metadata": {}, "score": "41.111855"}
{"text": "Maximize entropy .The function that correctly describes the information is the one that maximizes the entropy of Equation 1 ( and sums to one , as all probability distributions must ) .All \" correct \" probability distributions are MEDs .", "label": "", "metadata": {}, "score": "41.1716"}
{"text": "Furthermore , if X corresponds to information that is minimal ( very high entropy ) , then any additional information ( XA , XB , etc . ) will reduce entropy and thus have a positive value .This fits with common intuition and helps to elucidate the rationale for Jaynes 's maximum entropy principle : any distribution that has less than the maximum entropy assumes information that is not actually available ( and any distribution that has higher entropy ignores the available information ) .", "label": "", "metadata": {}, "score": "41.545013"}
{"text": "Maximum entropy models assign an effective energy to every possible combination of spiking and silence in the network , from Eq ( 20 ) .Learning the model means specifying all the parameters in this expression , so that the mapping from states to energies is completely determined .", "label": "", "metadata": {}, "score": "42.038734"}
{"text": "The posterior gives a universal sufficient statistic for detection applications , when one wants to choose values for the variable subset which minimize some expected loss function , for instance the probability of decision error .A Bayesian network can thus be considered a mechanism for automatically applying Bayes ' theorem to complex problems .", "label": "", "metadata": {}, "score": "42.1848"}
{"text": "This maximizes the expected posterior information about when the prior density is .The reference prior is defined in the asymptotic limit , i.e. , one considers the limit of the priors so obtained as the number of data points goes to infinity .", "label": "", "metadata": {}, "score": "42.70079"}
{"text": "The results also show that that one can reach apparent convergence at lower values of the force constant , but that the resulting distribution in this case will not be the maximum entropy solution .For these and related problems , we also need better methods to check for convergence , both to study the effect of varying these restraint - parameters and to monitor and ensure sufficient sampling of the ensembles .", "label": "", "metadata": {}, "score": "42.72512"}
{"text": "The results also show that that one can reach apparent convergence at lower values of the force constant , but that the resulting distribution in this case will not be the maximum entropy solution .For these and related problems , we also need better methods to check for convergence , both to study the effect of varying these restraint - parameters and to monitor and ensure sufficient sampling of the ensembles .", "label": "", "metadata": {}, "score": "42.72512"}
{"text": "There is therefore a crucial need of models allowing the prediction of this distribution .However , atomization processes are partially known and so far a universal model is not available .For almost thirty years , models based on the Maximum Entropy Formalism have been proposed to fulfill this task .", "label": "", "metadata": {}, "score": "43.047302"}
{"text": "Since by definition the sum of the probabilities must equal one ( which is merely a trivial but useful convention ) , the probability of each outcome is 0.25 .In contrast to this contrived example , we often have information that does not constrain the number of possible states , but does constrain the location and scale .", "label": "", "metadata": {}, "score": "43.684"}
{"text": "Another question is related to the steepness of the potential used to implement the restraint : how narrow should the potential be to mimic the appropriate -function that will ensure the maximum entropy correspondence ?Previous work has either found optimal values of the number of replicas by cross - validation , or simply chosen a sufficiently large to obtain convergence .", "label": "", "metadata": {}, "score": "43.85224"}
{"text": "Another question is related to the steepness of the potential used to implement the restraint : how narrow should the potential be to mimic the appropriate -function that will ensure the maximum entropy correspondence ?Previous work has either found optimal values of the number of replicas by cross - validation , or simply chosen a sufficiently large to obtain convergence .", "label": "", "metadata": {}, "score": "43.85224"}
{"text": "The number of maximum entropy applications in our field has grown steadily in recent years , in areas as diverse as sequence analysis , structural modelling , and neurobiology .In this Perspectives article , we give a broad introduction to the method , in an attempt to encourage its further adoption .", "label": "", "metadata": {}, "score": "44.28395"}
{"text": "The number of maximum entropy applications in our field has grown steadily in recent years , in areas as diverse as sequence analysis , structural modelling , and neurobiology .In this Perspectives article , we give a broad introduction to the method , in an attempt to encourage its further adoption .", "label": "", "metadata": {}, "score": "44.28395"}
{"text": "Under mild regularity conditions this process converges on maximum likelihood ( or maximum posterior ) values for parameters .A more fully Bayesian approach to parameters is to treat parameters as additional unobserved variables and to compute a full posterior distribution over all nodes conditional upon observed data , then to integrate out the parameters .", "label": "", "metadata": {}, "score": "44.712013"}
{"text": "In fact , one of Jaynes ' great achievements was to demonstrate that many results in statistical mechanics could be derived by the simple application of this principle .Box 1 .A Primer to the Principle of Maximum Entropy ( Adapted from Ref .", "label": "", "metadata": {}, "score": "44.72324"}
{"text": "In fact , one of Jaynes ' great achievements was to demonstrate that many results in statistical mechanics could be derived by the simple application of this principle .Box 1 .A Primer to the Principle of Maximum Entropy ( Adapted from Ref .", "label": "", "metadata": {}, "score": "44.72324"}
{"text": "This degree of trust is currently not encoded in the force fields commonly employed in simulations .In principle , this information could be specified by providing distributions ( or at least variances ) for all estimated parameters in the force field , in the spirit of Bayesian inference , allowing the inference machinery to deduce or integrate out the relevant weights .", "label": "", "metadata": {}, "score": "44.7493"}
{"text": "This degree of trust is currently not encoded in the force fields commonly employed in simulations .In principle , this information could be specified by providing distributions ( or at least variances ) for all estimated parameters in the force field , in the spirit of Bayesian inference , allowing the inference machinery to deduce or integrate out the relevant weights .", "label": "", "metadata": {}, "score": "44.7493"}
{"text": "The problem of maximum entropy in the context of noisy data has been addressed numerous times in other fields , leading to various forms of generalized maximum entropy procedures [ 46 ] , [ 47 ] and regularization approaches [ 48 ] , [ 49 ] .", "label": "", "metadata": {}, "score": "44.829636"}
{"text": "The problem of maximum entropy in the context of noisy data has been addressed numerous times in other fields , leading to various forms of generalized maximum entropy procedures [ 46 ] , [ 47 ] and regularization approaches [ 48 ] , [ 49 ] .", "label": "", "metadata": {}, "score": "44.829636"}
{"text": "A note on the prior distributions of Weibull parameters .SCIMA 19 : 5 - 13 MATH .Zellner A ( 1991 )Bayesian methods and entropy in economics and econometrics .In : Grandy WT Jr , Schick LH(eds ) Maximum entropy and Bayesian methods .", "label": "", "metadata": {}, "score": "45.40849"}
{"text": "There are at least three important criticisms of this view .First , these models presume that the brain needs to \" know the probabilities \" and that a probability is the answer to an inference problem and should thus correspond to a neuronal output .", "label": "", "metadata": {}, "score": "45.43393"}
{"text": "The probability distribution that correctly describes a state of knowledge is the one with maximum entropy , and thus mathematical methods to find a probability distribution should seek to maximize the entropy ( H ) .The entropy is conditional on information XY ( see below for distinction between X and Y ) .", "label": "", "metadata": {}, "score": "45.862774"}
{"text": "Unfortunately , even computing the entropy of our model distribution is not simple .But this is terribly inefficient ( see Methods : Computing the entropy and the partition function ) .An alternative is to make more thorough use of the mathematical equivalence between maximum entropy models and statistical mechanics .", "label": "", "metadata": {}, "score": "46.058594"}
{"text": "Among all possible models compatible with the new data , this will be the one that is the least biased .Or alternatively phrased , this will be the model that is as close as possible to the original distribution , while taking the new data into account .", "label": "", "metadata": {}, "score": "46.075943"}
{"text": "Among all possible models compatible with the new data , this will be the one that is the least biased .Or alternatively phrased , this will be the model that is as close as possible to the original distribution , while taking the new data into account .", "label": "", "metadata": {}, "score": "46.075943"}
{"text": "But all probabilities are conditional and descriptive and are never presumed to exist in an ontological , physical sense . )Additional evidence of a partially frequentist perspective comes from the use of Bayes 's Theorem ( BT ) .Within books [ 3 , 33 , 48 , 49 ] as well as many papers , BT is written and described as if the prior distribution is not ( or at least , may not be ) conditional on any information .", "label": "", "metadata": {}, "score": "46.38111"}
{"text": "For instance , it is traditionally assumed that all our prior knowledge about the measured quantities can be expressed in terms of probability distributions .Often , however , we only obtain information about the average value of these quantities from experiments .", "label": "", "metadata": {}, "score": "46.467308"}
{"text": "For instance , it is traditionally assumed that all our prior knowledge about the measured quantities can be expressed in terms of probability distributions .Often , however , we only obtain information about the average value of these quantities from experiments .", "label": "", "metadata": {}, "score": "46.467308"}
{"text": "See my previous post for more .Non - Bayesian estimators come in many different flavors : .Best upper bound ( BUB ) estimator is a bias correction method which bounds the maximum error in entropy estimation .James - Stein ( JS ) estimator regularizes entropy by estimating a mixture of uniform distribution and the empirical histogram with the James - Stein shrinkage .", "label": "", "metadata": {}, "score": "46.518734"}
{"text": "In computational biology , maximum entropy approaches are also becoming increasingly common .Examples include the formulation of models of collective neural stimuli [ 2 ] , reconstruction of protein signaling networks [ 3 ] , optimization of force fields for molecular simulation [ 4 ] , and modelling covariation among sites in protein sequences [ 5 ] , [ 6 ] .", "label": "", "metadata": {}, "score": "46.553295"}
{"text": "In computational biology , maximum entropy approaches are also becoming increasingly common .Examples include the formulation of models of collective neural stimuli [ 2 ] , reconstruction of protein signaling networks [ 3 ] , optimization of force fields for molecular simulation [ 4 ] , and modelling covariation among sites in protein sequences [ 5 ] , [ 6 ] .", "label": "", "metadata": {}, "score": "46.553295"}
{"text": "When implemented in this fashion , it is important to note that the simulations are not forced to agree perfectly with the experimental data .Instead , the level of agreement is now governed by the weight of the biasing energy term .", "label": "", "metadata": {}, "score": "46.661995"}
{"text": "When implemented in this fashion , it is important to note that the simulations are not forced to agree perfectly with the experimental data .Instead , the level of agreement is now governed by the weight of the biasing energy term .", "label": "", "metadata": {}, "score": "46.661995"}
{"text": "However , the principle of indifference was of only limited applicability , and it appeared to some scientists to be arbitrary .In an effort to give probabilities an \" objective \" status , others proposed that the formal use of probabilities should be restricted to cases in which there was a clear relationship to measured frequencies of events .", "label": "", "metadata": {}, "score": "46.69426"}
{"text": "In particular , we discuss the complications that may arise when using the technique in practice , including the fact that all experiments contain various , sometimes unknown , sources of noise .Jaynes ' Principle of Maximum Entropy .Jaynes originally proposed the maximum entropy principle to establish a link between Shannon 's information theory [ 12 ] and statistical mechanics [ 1 ] .", "label": "", "metadata": {}, "score": "46.70692"}
{"text": "In particular , we discuss the complications that may arise when using the technique in practice , including the fact that all experiments contain various , sometimes unknown , sources of noise .Jaynes ' Principle of Maximum Entropy .Jaynes originally proposed the maximum entropy principle to establish a link between Shannon 's information theory [ 12 ] and statistical mechanics [ 1 ] .", "label": "", "metadata": {}, "score": "46.70692"}
{"text": "This paper discusses some of these cases , and discusses how to identify some of the situations in which this principle should not be used .The paper starts by reviewing three approaches to probability , namely the classical approach , the limiting frequency approach , and the Bayesian approach .", "label": "", "metadata": {}, "score": "46.798927"}
{"text": "One approach that we foresee could be potentially useful in molecular simulation was proposed by Gull and Daniell in the context of image reconstruction [ 50 ] .The idea is to replace the many individual constraints with a single constraint on the statistic over all data , only matching them up to their experimental uncertainty : ( 7 )", "label": "", "metadata": {}, "score": "46.83928"}
{"text": "One approach that we foresee could be potentially useful in molecular simulation was proposed by Gull and Daniell in the context of image reconstruction [ 50 ] .The idea is to replace the many individual constraints with a single constraint on the statistic over all data , only matching them up to their experimental uncertainty : ( 7 )", "label": "", "metadata": {}, "score": "46.83928"}
{"text": "However , one can often infer a frequentist view .For example , it is routine to find language related to \" estimating \" or \" measuring \" or \" approximating \" a \" true \" probability distribution .This implies that probabilities are an inherent property of an object ( such as an \" urn \" ) , and they exist independent of the knowledge of any observer .", "label": "", "metadata": {}, "score": "47.084297"}
{"text": "However , except for the simplest states of information , deriving MEDs directly is challenging if not impossible as a practical matter .Fortunately there is a rule known as Bayes 's Theorem ( BT ) that allows us to manipulate probabilities without the trouble of directly maximizing entropy ( in a formal , mathematical sense ) .", "label": "", "metadata": {}, "score": "47.198875"}
{"text": "Such a tool allows us to refine the celebrated Lieb bound for Wehrl entropies and to discover thermodynamic - like relations that involve the degree of delocalization .Fisher - related thermal uncertainty relations are developed and the degree of purity of canonical distributions , regarded as mixed states , is connected to this Fisher measure as well .", "label": "", "metadata": {}, "score": "47.200703"}
{"text": "Even when the goal of \" taking the neuron 's point of view \" has been explicitly stated by the authors , the actual probability distributions have been conditional on the information of the scientists about a neuron 's inputs and outputs .", "label": "", "metadata": {}, "score": "47.228138"}
{"text": "Among his greatest achievements in this regard was his simultaneous introduction in 1957 of the maximum entropy principle and its application to the field of statistical mechanics ( SM ) [ 34 ] .The central challenge in SM is to estimate the properties of microscopic molecules ( their positions and velocities , for example ) given knowledge of macroscopic factors that we can readily measure , such as pressure and temperature .", "label": "", "metadata": {}, "score": "47.40551"}
{"text": "We can make these considerations a bit more precise by exploring the dependence of coincidence probabilities on N .We expect that the negative logarithm of the coincidence probability , like the entropy itself , will grow linearly with N ; equivalently we should see an exponential decay of coincidence probability as we increase the size of the system .", "label": "", "metadata": {}, "score": "47.459633"}
{"text": "For reasons disclosed below ( Section 3.9 ) , I will use the term \" Jaynesian \" rather than \" Bayesian \" in referring to probability theory according to Jaynes .The Jaynesian Definition of Probability : The Maximum Entropy Principle .", "label": "", "metadata": {}, "score": "47.496357"}
{"text": "First of all , if you have continuous ( analogue ) observation , read the title of this post .CDM , PYM , DPM , NSB are Bayesian estimators , meaning that they have explicit probabilistic assumptions .Those estimators provide posterior distributions or credible intervals as well as point estimates of entropy .", "label": "", "metadata": {}, "score": "47.844124"}
{"text": "Math .Finance 2013 ) to find the maximum entropy density of an asset price to the relative entropy case .This is applied to study the impact of the choice of prior density in two market scenarios .In the first scenario , call option prices are prescribed at only a small number of strikes , and we see that the choice of prior , or indeed its omission , yields notably different densities .", "label": "", "metadata": {}, "score": "47.9011"}
{"text": "The most general point to be made here is that those maintaining a frequentist view are in reality basing their probabilities on their own information .Frequentist probabilities are not actually properties of an observed system , as frequentists claim .Some specific faults with the frequentist definition of probability are summarized below .", "label": "", "metadata": {}, "score": "48.059647"}
{"text": "But even with the use of probabilities restricted to cases involving measured frequencies , steps were required to transform a frequency into a probability .Without any general and unified principle to guide this transformation , statisticians developed an ad hoc collection of rules that were applicable in special cases .", "label": "", "metadata": {}, "score": "48.08206"}
{"text": "Some authorities prefer the term objective prior .In parameter estimation problems , the use of an uninformative prior typically yields results which are not too different from conventional statistical analysis , as the likelihood function often yields more information than the uninformative prior .", "label": "", "metadata": {}, "score": "48.105476"}
{"text": "For our example , if we consider all possible outcomes of throws of a die , only a subset of these would be compatible with a given model .Maximizing the entropy ensures that this subset is as large as possible given the observed average value , not ruling out any more realizations than strictly necessary .", "label": "", "metadata": {}, "score": "48.1707"}
{"text": "For our example , if we consider all possible outcomes of throws of a die , only a subset of these would be compatible with a given model .Maximizing the entropy ensures that this subset is as large as possible given the observed average value , not ruling out any more realizations than strictly necessary .", "label": "", "metadata": {}, "score": "48.1707"}
{"text": "A similar analysis for Sydney is also described .Contents .It is meant to attribute uncertainty rather than randomness to the uncertain quantity .One applies Bayes ' theorem , multiplying the prior by the likelihood function and then normalizing , to get the posterior probability distribution , which is the conditional distribution of the uncertain quantity given the data .", "label": "", "metadata": {}, "score": "48.38704"}
{"text": "As one makes repeated observations of the outcome , one can begin to \" estimate \" the probabilities .To be certain of the probabilities , one would have to make an infinite number of observations .According to a strict frequentist definition , probabilities are equivalent to frequencies ; they are a physical property of a system , and they are independent of an observer 's knowledge of the system .", "label": "", "metadata": {}, "score": "48.63978"}
{"text": "Like all of probability theory , BT is simply an expression of logic acting on information .It is an equality derived from the decomposition of a joint probability into a product of its components .However , the observer does know the general nature of the relationship between x and Y , and thus imagines that x has the specific value x ' ; so as to find its consequences for the probability of the evidence Y .", "label": "", "metadata": {}, "score": "48.793335"}
{"text": "The most common approximate inference algorithms are stochastic MCMC simulation , mini - bucket elimination which generalizes loopy belief propagation , and variational methods .Parameter learning .In order to fully specify the Bayesian network and thus fully represent the joint probability distribution , it is necessary to specify for each node X the probability distribution for X conditional upon X ' s parents .", "label": "", "metadata": {}, "score": "48.88223"}
{"text": "Because a Bayesian network is a complete model for the variables and their relationships , it can be used to answer probabilistic queries about them .For example , the network can be used to find out updated knowledge of the state of a subset of variables when other variables ( the evidence variables ) are observed .", "label": "", "metadata": {}, "score": "48.906574"}
{"text": "It is important that the mapping between maximum entropy models and a Boltzmann distribution with some effective energy function is not an analogy , but rather a mathematical equivalence .We are also not assuming that the temporal dynamics of the network is described by Newton 's laws or Brownian motion on the energy landscape .", "label": "", "metadata": {}, "score": "49.2603"}
{"text": "One can nevertheless compute , for each of these distributions at temperature T , the heat capacity , and then thermodynamics teaches us that ; we can thus invert this relation to compute the entropy : ( 29 ) .The heat capacity might seem irrelevant since there is no \" heat \" in our problem , but this quantity is directly related to the variance of energy , , with as in Figure 8 .", "label": "", "metadata": {}, "score": "49.366707"}
{"text": "Unseen estimator uses a Poissonization of fingerprint and linear programming to find the likely underlying fingerprint , and use the entropy as an estimate .These methods are faster than the others , if you need speed .There are many software packages available out there .", "label": "", "metadata": {}, "score": "49.46338"}
{"text": "One of the challenges associated with these hybrid energies is choosing such weights and other parameters for the biasing potential .Often these parameters are tuned manually .An alternative , Bayesian approach , called inferential structure determination , however , provides an elegant solution to this problem , by treating such unknown quantities as \" nuisance parameters \" and integrating them out [ 23 ] , [ 24 ] .", "label": "", "metadata": {}, "score": "49.56128"}
{"text": "One of the challenges associated with these hybrid energies is choosing such weights and other parameters for the biasing potential .Often these parameters are tuned manually .An alternative , Bayesian approach , called inferential structure determination , however , provides an elegant solution to this problem , by treating such unknown quantities as \" nuisance parameters \" and integrating them out [ 23 ] , [ 24 ] .", "label": "", "metadata": {}, "score": "49.56128"}
{"text": "It is an underdetermined problem , in the sense that there may be an infinite number of possible prior distributions that are compatible with this piece of data .A simple , but general solution to this type of problem was provided by Jaynes in 1957 , who proposed that among all the models fulfilling the constraints from the data , one should select the model containing the least amount of information [ 1 ] .", "label": "", "metadata": {}, "score": "49.56346"}
{"text": "It is an underdetermined problem , in the sense that there may be an infinite number of possible prior distributions that are compatible with this piece of data .A simple , but general solution to this type of problem was provided by Jaynes in 1957 , who proposed that among all the models fulfilling the constraints from the data , one should select the model containing the least amount of information [ 1 ] .", "label": "", "metadata": {}, "score": "49.56346"}
{"text": "Entropy estimation is a challenging problem .As explained in the text , the usual approach of counting samples and identifying frequencies with probabilities will fail catastrophically in all the cases of interest here , even if we are free to draw samples from our model rather than from real data .", "label": "", "metadata": {}, "score": "49.6215"}
{"text": "This result has immediate practical applications .Rather than determining Lagrange multipliers for all experimental observations , it is sufficient to conduct an ensemble - averaged simulation with -function constraints with a large number of replicas .In practice , -functions are difficult to work with and are often replaced with a steep potential , for instance a harmonic term .", "label": "", "metadata": {}, "score": "49.637672"}
{"text": "This result has immediate practical applications .Rather than determining Lagrange multipliers for all experimental observations , it is sufficient to conduct an ensemble - averaged simulation with -function constraints with a large number of replicas .In practice , -functions are difficult to work with and are often replaced with a steep potential , for instance a harmonic term .", "label": "", "metadata": {}, "score": "49.637672"}
{"text": "The three new papers highlighted in this Perspectives article have provided substantial new insights to the field of molecular simulation under experimental restraints .Of particular interest is the result that the current common practice of replica - averaged simulations is tightly linked to the solution prescribed by the maximum entropy formalism .", "label": "", "metadata": {}, "score": "49.81125"}
{"text": "The three new papers highlighted in this Perspectives article have provided substantial new insights to the field of molecular simulation under experimental restraints .Of particular interest is the result that the current common practice of replica - averaged simulations is tightly linked to the solution prescribed by the maximum entropy formalism .", "label": "", "metadata": {}, "score": "49.81125"}
{"text": "An interpretation is given in the objective framework of channel coding .In a subjective framework , the performance of the method is shown in a reliability context when flat but proper priors are elicited for the Weibull lifetime distributions .Such priors appear as practical tools for sensitivity studies .", "label": "", "metadata": {}, "score": "49.8688"}
{"text": "It alone defines probability and provides an objective function to derive probabilities from information .By contrast , BT ( Equation 3 ) is irrelevant to the definition , as it does not specify precisely what probabilities are or how they are related to information .", "label": "", "metadata": {}, "score": "49.86938"}
{"text": "However , it should be noted that this use of the Boltzmann distribution has not yet been rigorously justified .A single sensor was proposed to specify a sigmoidal probability distribution [ 5 , 61 ] , but a probability distribution over an inherently infinite state space ( such as voltage ) can not have a sigmoid shape ( since the probabilities must sum to one ) .", "label": "", "metadata": {}, "score": "49.87829"}
{"text": "However , estimating entropy from observations is surprisingly difficult , and is still an active area of research .There are many estimators that aim to overcome these difficulties to some degree .Deciding which estimator to use can be overwhelming , so here 's my recommendation in the form of a flow chart : .", "label": "", "metadata": {}, "score": "49.89544"}
{"text": "The posterior probabilities will still sum ( or integrate ) to 1 even if the prior values do not , and so the priors only need be specified in the correct proportion .Taking this idea further , in many cases the sum or integral of the prior values may not even need to be finite to get sensible answers for the posterior probabilities .", "label": "", "metadata": {}, "score": "50.403828"}
{"text": "Technically , we can not capture the impact within low orders of perturbation theory ( Methods : Are the networks in the perturbative regime ? ) , but qualitatively this means that the behavior of the network as a whole is not in any sense \" close \" to the behavior of non - interacting neurons .", "label": "", "metadata": {}, "score": "50.42839"}
{"text": "Statisticians assume \" stationarity \" , or a \" Gaussian process \" , or \" statistical independence \" .These assumptions actually constitute relevant information ( or lack thereof ) that the statistician possesses and uses .But conventional ( frequentist ) statistics does not have overarching principles for how to use this information , only a collection of ad hoc rules .", "label": "", "metadata": {}, "score": "50.51317"}
{"text": "We apply the methodology to a problem consisting of the determination of a probability density from a few values of its numerically - determined Laplace transform .This problem can be mapped onto a problem consisting of the determination of a probability density on [ 0 , 1 ] from the knowledge of a few of its fractional moments up to some measurement errors stemming from insufficient data .", "label": "", "metadata": {}, "score": "50.563557"}
{"text": "Their status is epistemic and not ontological .Second , if the brain does in fact need to calculate probabilities , then the first calculations must identify maximum entropy distributions that are conditional on the brain 's information .BT is useless until probabilities are available .", "label": "", "metadata": {}, "score": "50.59474"}
{"text": "We could quantify information on an absolute scale using Equation 2 if we can identify a probability distribution corresponding to a state X of complete ignorance ( so that no distribution could have a higher entropy ) .The next best thing would be to identify a primitive state of minimal information that is universal ( shared by all observers ) .", "label": "", "metadata": {}, "score": "50.770885"}
{"text": "Shannon 's entropy is a more general expression of Laplace 's \" indifference , \" and common sense tells us that in making inferences , we should not assume any information that we do not actually have .In 1957 , E.T. Jaynes demonstrated that the maximization of entropy provided an objective basis for deriving probabilities from information [ 34 ] .", "label": "", "metadata": {}, "score": "50.867905"}
{"text": "The starting point of the maximum entropy method for neural networks is that the network could , if we do n't know anything about its function , wander at random among all possible states .We then take measured , average properties of the network activity as constraints , and each constraint defines some minimal level of structure .", "label": "", "metadata": {}, "score": "50.87913"}
{"text": "Our new algorithm can use as constraints arbitrary functions , not only single and pairwise marginals as before .Parameters of the Hamiltonian are learned sequentially in an order which greedily optimizes a bound on the log likelihood , and we use a variant of histogram Monte Carlo to estimate the values of constrained statistics during learning steps [ 107 ] .", "label": "", "metadata": {}, "score": "50.890907"}
{"text": "The resulting expression , however , relies on the averages , which are not know a priori .A possible strategy would be to estimate and iteratively , by repeatedly estimating from a simulation , adjusting to match the calculated and expected value for , and then rerunning the simulation ( or reweighting the statistics from the previous one [ 8 ] ) .", "label": "", "metadata": {}, "score": "50.983925"}
{"text": "The resulting expression , however , relies on the averages , which are not know a priori .A possible strategy would be to estimate and iteratively , by repeatedly estimating from a simulation , adjusting to match the calculated and expected value for , and then rerunning the simulation ( or reweighting the statistics from the previous one [ 8 ] ) .", "label": "", "metadata": {}, "score": "50.983925"}
{"text": "( A )The distributions of pairwise couplings , , in pairwise models of Eq ( 19 ) , for different network sizes ( N ) .The distribution is pooled over 30 networks at each N .Precision of entropy estimates .", "label": "", "metadata": {}, "score": "50.994534"}
{"text": "There are remarkably successful applications of probability theory that do not even use BT [ 34 ] .It is not even known whether Bayes himself subscribed exclusively to the \" Bayesian definition \" of probability .I therefore propose that the term \" Bayesian \" be replaced with \" Jaynesian \" in making general reference to probability theory and inference , and in particular in referring to the definition of probabilities based on maximum entropy .", "label": "", "metadata": {}, "score": "51.02664"}
{"text": "Bayesian inference Expert opinion Kullback - Leibler distance Shannon 's entropy Noninformative priors Channel coding Sensitivity study Weibull .Share .References .Andrieu C , Doucet A , Fitzgerald WJ , P\u00e9rez JM ( 2001 )Bayesian computational approaches to model selection .", "label": "", "metadata": {}, "score": "51.043533"}
{"text": "Where do we stand on maximum entropy ?In : Levine R , Tribus M , editors .The Maximum Entropy Formalism .Cambridge , MA : MIT Press . pp . 1 - 104 .Lindorff - Larsen K , Maragakis P , Piana S , Eastwood MP , Dror RO , et al .", "label": "", "metadata": {}, "score": "51.050034"}
{"text": "Where do we stand on maximum entropy ?In : Levine R , Tribus M , editors .The Maximum Entropy Formalism .Cambridge , MA : MIT Press . pp . 1 - 104 .Lindorff - Larsen K , Maragakis P , Piana S , Eastwood MP , Dror RO , et al .", "label": "", "metadata": {}, "score": "51.050034"}
{"text": "Nevertheless , we explicitly checked that there is no overfitting by comparing the log likelihood of the data under the learned maximum entropy model , for each of the 360 subgroups , on the training and testing set , as shown in Figure 3 .", "label": "", "metadata": {}, "score": "51.125"}
{"text": "We then use the shadow values and nominal input prices to estimate crop - specific production functions using generalized maximum entropy ( GME ) to capture individual heterogeneity of the production environment while replicating observed inputs and outputs to production .The two - stage GME approach can be implemented with small data sets .", "label": "", "metadata": {}, "score": "51.125504"}
{"text": "Cavalli et al . provide a possible path in this direction , and in this paper , we have sketched out a few potential alternatives .From a theoretical viewpoint , it seems desirable to combine Bayesian inference , which provides a robust toolbox for dealing with noisy data , with the maximum entropy principle for deriving probability distributions in underdetermined systems .", "label": "", "metadata": {}, "score": "51.342392"}
{"text": "Cavalli et al . provide a possible path in this direction , and in this paper , we have sketched out a few potential alternatives .From a theoretical viewpoint , it seems desirable to combine Bayesian inference , which provides a robust toolbox for dealing with noisy data , with the maximum entropy principle for deriving probability distributions in underdetermined systems .", "label": "", "metadata": {}, "score": "51.342392"}
{"text": "Three recent papers [ 9 ] - [ 11 ] , have explored the assumptions underlying existing methods in the light of the maximum entropy principle , leading to suggestions for new avenues to optimally utilize the complementary information available from experiments and molecular simulations .", "label": "", "metadata": {}, "score": "51.346004"}
{"text": "Three recent papers [ 9 ] - [ 11 ] , have explored the assumptions underlying existing methods in the light of the maximum entropy principle , leading to suggestions for new avenues to optimally utilize the complementary information available from experiments and molecular simulations .", "label": "", "metadata": {}, "score": "51.346004"}
{"text": "The time requirement of an exhaustive search returning back a structure that maximizes the score is superexponential in the number of variables .A local search strategy makes incremental changes aimed at improving the score of the structure .A global search algorithm like Markov chain Monte Carlo can avoid getting trapped in local minima .", "label": "", "metadata": {}, "score": "51.35489"}
{"text": "Nonetheless , it has been suggested that maximum entropy methods are successful only because correlations are weak , and hence that we ca n't really capture non - trivial collective behaviors with this approach [ 34 ] .To make the question of whether correlations are weak or strong precise , we ask whether we can approximate the maximum entropy distribution with the leading orders of perturbation theory .", "label": "", "metadata": {}, "score": "51.382446"}
{"text": "They already showed that the number of clusters inferred from DP mixture model is inconsistent ( at ICERM workshop 2012 , and last year 's NIPS workshop ) .In this paper they show theoretical examples , one of which says : If the true distribution is a normal distribution , then the probability that # of components inferred by DPM ( with ) is 1 goes to zero , as a function of # of samples .", "label": "", "metadata": {}, "score": "51.485153"}
{"text": "In addition to introducing maximum entropy inference , Jaynes provided an amazingly short and simple derivation of SM , and he demonstrated that the probability distributions of SM are based on an extremely simple state of information .Specifically , he demonstrated that given only knowledge of the temperature ( average or expected energy ) of a physical system , the maximum entropy probability distribution over energies within the observed system is exponential ( in the case of just one degree of freedom ) .", "label": "", "metadata": {}, "score": "51.555126"}
{"text": "( B )As the network of N neurons gets larger , the discrepancy in the prediction of the probability of silence , P ( 0 ) , grows in a qualitatively similar way as under naturalistic stimulation .( D )", "label": "", "metadata": {}, "score": "51.615166"}
{"text": "Assuming that the data are generated from a Bayesian network and that all the variables are visible in every iteration , optimization based search method can be used to find the structure of the network .It requires a scoring function and a search strategy .", "label": "", "metadata": {}, "score": "51.680347"}
{"text": "The apparent randomness of neural activity has provided the uncertainty in purportedly \" Bayesian \" models of how neurons can perform inference ( see Section 4.3 ) .Consider the output of each of four systems in the case of a known and invariant input ( Figure 3 a ) .", "label": "", "metadata": {}, "score": "51.797707"}
{"text": "Finally , this paper proposes a simple linear iterative method that generalizes the biproportional method to the data balancing problem with arbitrary data structure , uncertainty estimates and multiple data quality levels .Abstract : We show that a special entropic quantifier , called the statistical complexity , becomes maximal at the transition between super - Poisson and sub - Poisson regimes .", "label": "", "metadata": {}, "score": "51.859676"}
{"text": "When the force constant is determined from the experimental noise , this is no longer possible , suggesting that rather than the limit , an intermediate value of might be more appropriate in order to provide a balance between matching the mean and the variance .", "label": "", "metadata": {}, "score": "51.993214"}
{"text": "When the force constant is determined from the experimental noise , this is no longer possible , suggesting that rather than the limit , an intermediate value of might be more appropriate in order to provide a balance between matching the mean and the variance .", "label": "", "metadata": {}, "score": "51.993214"}
{"text": "At right , the parameters of a pairwise maximum entropy model [ from Eq ( 19 ) ] that reproduces these data : ( D ) coupling constants , ( E ) fields , and ( F ) the distribution of couplings in this group of neurons .", "label": "", "metadata": {}, "score": "52.026344"}
{"text": "In other words , information about \" X \" is not the same as information about the probability of \" X \" .The former concerns the probability of \" X \" , whereas the latter concerns the probability of a probability of \" X \" .", "label": "", "metadata": {}, "score": "52.11378"}
{"text": "Quantifying the Amount of Information .A probability distribution is conditional on information , and it fully and quantitatively characterizes the information upon which it is conditional ( with respect to a particular state space ) .Having derived a probability distribution , we can also measure the amount of information .", "label": "", "metadata": {}, "score": "52.140087"}
{"text": "Accordingly , when we say that the brain \" performs inference \" or \" minimizes uncertainty \" , we can have a very clear understanding of what this means , even if as a practical matter we can not actually quantify it using probabilities .", "label": "", "metadata": {}, "score": "52.19256"}
{"text": "Remaining challenges .The replica - averaged approach described in the previous section is a remarkably elegant , easily implementable technique that provides the least - biased distribution consistent with any observed expectation values over the data .From our perspective , it represents a significant step forward in our understanding of how experimental data should be used in molecular simulations .", "label": "", "metadata": {}, "score": "52.272976"}
{"text": "Remaining challenges .The replica - averaged approach described in the previous section is a remarkably elegant , easily implementable technique that provides the least - biased distribution consistent with any observed expectation values over the data .From our perspective , it represents a significant step forward in our understanding of how experimental data should be used in molecular simulations .", "label": "", "metadata": {}, "score": "52.272976"}
{"text": "Leaning further on the mapping to statistical physics , we realize that the heat capacity is a summary statistic for the density of states .There are Monte Carlo sampling methods , due to Wang and Landau [ 73 ] ( WL ) , that aim specifically at estimating this density , and those allow us to compute the entropy from a single simulation run .", "label": "", "metadata": {}, "score": "52.539833"}
{"text": "The hope is that while there are states of the system as a whole , there is a much smaller number of measurements , , with and , which will be sufficient to capture the essential structure of the collective behavior in the system .", "label": "", "metadata": {}, "score": "52.56509"}
{"text": "Importantly , the transition states are at energies quite high relative to the metastable states ( Figure 11D ) , so the peaks of the probability distribution are well resolved from one another .In many cases it takes a large number of steps to find the transition state , so that the metastable states are substantially separated in Hamming distance .", "label": "", "metadata": {}, "score": "52.571865"}
{"text": "In contrast , according to a Jaynesian definition , probabilities are fully and precisely specified by a state of knowledge .With respect to the brain , we could either say that the brain unconsciously \" knows \" the probabilities precisely , or that the brain knows nothing about probabilities , but that probabilities provide a means by which we can quantitively describe the information in a brain .", "label": "", "metadata": {}, "score": "52.592476"}
{"text": "In this article , we provide an overview of the current strategies and point out where and how the method of maximum entropy has been introduced in this area .Its applicability in various disciplines has been abundantly demonstrated .We give several examples of applications of maximum entropy in different stages of drug discovery .", "label": "", "metadata": {}, "score": "52.65593"}
{"text": "In the view of Jaynes , this is the most important step .Agreement and understanding requires that all observers are utilizing the same information ( a fact which is universally apparent to anyone above a certain mental age , and which is not at all unique to probability theory ) .", "label": "", "metadata": {}, "score": "52.727203"}
{"text": "The probabilities merely describe the information in the brain , whether the brain functions well or is pathological [ 45 ] .It is critical to note in this regard that although Jaynesian logic may be \" perfect \" , it does not necessarily imply intelligence .", "label": "", "metadata": {}, "score": "52.87111"}
{"text": "However , neuroscience literature virtually never specifies any definition of probability , nor does it acknowledge any dispute concerning the definition .Although there has recently been tremendous interest in Bayesian approaches to the brain , even in the Bayesian literature it is common to find probabilities that are purported to come directly and unconditionally from frequencies .", "label": "", "metadata": {}, "score": "52.887665"}
{"text": "Basically , the lesson is , never take conditional probabilities on sets of measure zero ( not to be confused with conditional densities ) .Furthermore , he told us about a formula to produce infinitely many paradoxes from Jaynes ' book ( ch 15 ) based on ill - defined series convergences .", "label": "", "metadata": {}, "score": "52.927197"}
{"text": "The variance for a binary variable given its mean , , is ; with R independent repeats , the error on the estimate in the average should decrease as .We note that our largest models have constrained statistics that are estimated from at least 15\u00d7 as many statistically independent samples .", "label": "", "metadata": {}, "score": "52.946926"}
{"text": "This model is formulated by a Gibbs distribution , under the MaxEnt framework , that can be sampled to generate plausible scenes .Once an And - Or graph is sampled from the ensemble , the hierarchical constraints are employed to sample the Or - nodes ( style variations ) and the contextual constraints are subsequently used to enforce the corresponding relations that must be satisfied by the And - nodes .", "label": "", "metadata": {}, "score": "52.954742"}
{"text": "Three very recent papers have explored this problem using the maximum entropy approach , providing both new theoretical and practical insights to the problem .We highlight each of these contributions in turn and conclude with a discussion on remaining challenges .", "label": "", "metadata": {}, "score": "52.99862"}
{"text": "Three very recent papers have explored this problem using the maximum entropy approach , providing both new theoretical and practical insights to the problem .We highlight each of these contributions in turn and conclude with a discussion on remaining challenges .", "label": "", "metadata": {}, "score": "52.99862"}
{"text": "Prior information is now considered to be given by calibrated Heston , Sch\u00f6bel - Zhu or Variance Gamma models .We find that the resulting digital option prices are essentially the same as those given by the ( non - relative ) Buchen - Kelly density itself .", "label": "", "metadata": {}, "score": "53.02581"}
{"text": "If one were to quantify information , it would simply be the difference between the entropy ( H ) that would correspond to the absence of the information , and the entropy that corresponds to the presence of the information .In Equation 2 , \" Y \" corresponds to the information that we wish to quantify , and \" X \" ; corresponds to \" background \" or \" prior \" information that is also available to the observer .", "label": "", "metadata": {}, "score": "53.047443"}
{"text": "First , because it allows to include convex constraints in a natural way , and second , because it allows to incorporate and to estimate ( additive ) measurement errors from the data .Here we shall see both methods in action in a specific example .", "label": "", "metadata": {}, "score": "53.074722"}
{"text": "In principle , this could be tested experimentally .Characterizing the Information that One Physical System Has about Another .In virtually all applications of probability theory , the probabilities and information have no known physical basis .Jaynesian probabilities are conditional on information , and we can presume that the information must be within some physical substrate , but we can not say precisely what the substrate is or how the probabilities are conditional on that physical reality .", "label": "", "metadata": {}, "score": "53.087856"}
{"text": "In this view , a pathological brain could be just as Jaynesian as a healthy brain ( though likely possessing less information ) [ 45 ] .Faults with the Frequentist Definition of Probability .Like any other intellectual endeavor , the frequentist approach to probabilities relies on the application of reason to information .", "label": "", "metadata": {}, "score": "53.103294"}
{"text": "From Boltzmann machines ( maximum entropy models ) to constraint satisfaction problems ( e.g. Sudoku ) , noisy SNN 's can be designed to sample from the posterior , and converges exponentially fast from any initial state .This is done by irreversible MCMC sampling of the neurons , and it can be generalized to continuous time and state space .", "label": "", "metadata": {}, "score": "53.10498"}
{"text": "The second term acts to enforce that the simulation is in agreement with the experiments , but penalizing the entire ensemble only when the ensemble averaged quantities , deviate from experiment .For linearly averaged quantities , .In this way , the calculated quantities in individual conformation ( ) may differ from experiment as long as their ensemble average , , matches the experiment within a scale that is implicitly determined by the force constant , .", "label": "", "metadata": {}, "score": "53.152412"}
{"text": "The second term acts to enforce that the simulation is in agreement with the experiments , but penalizing the entire ensemble only when the ensemble averaged quantities , deviate from experiment .For linearly averaged quantities , .In this way , the calculated quantities in individual conformation ( ) may differ from experiment as long as their ensemble average , , matches the experiment within a scale that is implicitly determined by the force constant , .", "label": "", "metadata": {}, "score": "53.152412"}
{"text": "In addition , assuming incoherence ( mixed or complex tuning ) , neural measurements can be seen as random projections of the high - dimensional space ; along with low - dimensional dynamics , the data recovers the correct true dimension .", "label": "", "metadata": {}, "score": "53.2007"}
{"text": "Abstract : We propose a continuous maximum entropy method to investigate the robustoptimal portfolio selection problem for the market with transaction costs and dividends .This robust model aims to maximize the worst - case portfolio return in the case that allof asset returns lie within some prescribed intervals .", "label": "", "metadata": {}, "score": "53.27662"}
{"text": "We can evaluate the predictions of spike probability vs. time by computing the correlation coefficient between our predicted PSTH and the experimental PSTH , as has been done in many other contexts [ 78 ] , [ 83 ] , [ 84 ] .", "label": "", "metadata": {}, "score": "53.28148"}
{"text": "In practice , we can draw sample states from a Monte Carlo simulation , compute the energy of each such state , and estimate the variance over a long simulation .Importantly , it is well known that such estimates stabilize long before we have collected enough samples to visit every state of the system [ 72 ] .", "label": "", "metadata": {}, "score": "53.32351"}
{"text": "Mathematically , the derivation of probabilities is simpler for simpler sets of information .Furthermore , when we consider physical inference , it may be that we typically need to consider only small and relatively simple states of information .When we think of a brain , or a computer , we imagine a large and complex set of information of the sort that in practice would overwhelm a team of the best probability theorists .", "label": "", "metadata": {}, "score": "53.336113"}
{"text": "[ citation needed ] talk about using mutual information between variables and finding a structure that maximizes this .They do this by restricting the parent candidate set to k nodes and exhaustively searching therein .Pearl , Judea .Bayesian Networks : A Model of Self - Activated Memory for Evidential Reasoning .", "label": "", "metadata": {}, "score": "53.375717"}
{"text": "Similarly , distinctions have been made between the neural representation of \" known and unknown probabilities \" ( e.g. , [ 50 ] ) , and between \" expected and unexpected uncertainty \" [ 51 ] .As in the quotations described above , this language makes sense only if some probabilities are essentially the same as frequencies .", "label": "", "metadata": {}, "score": "53.690186"}
{"text": "Abstract : We discuss a special class of generalized divergence measures by the use of generator functions .Any divergence measure in the class is separated into the difference between cross and diagonal entropy .The diagonal entropy measure in the class associates with a model of maximum entropy distributions ; the divergence measure leads to statistical estimation via minimization , for arbitrarily giving a statistical model .", "label": "", "metadata": {}, "score": "53.70783"}
{"text": "Perturbative vs exact solution for the pairwise maximum entropy models .( A )The comparison of couplings for a group of neurons , computed using the exact maximum entropy reconstruction algorithm , with the lowest order perturbation theory result , , where and [ 34 ] , [ 113 ] .", "label": "", "metadata": {}, "score": "53.708763"}
{"text": "In the same spirit that many groups have studied the statistical structures of natural scenes [ 55 ] - [ 60 ] , we would like to understand the statistical structure of the codewords that represent these scenes .The maximum entropy method is not a model for network activity .", "label": "", "metadata": {}, "score": "53.74539"}
{"text": "( A - C ) for subnetworks of size ; error bars are s.d .across random halves of the duration of the experiment .( D )The probability of silence in the network , as a function of population size ; error bars are s.d .", "label": "", "metadata": {}, "score": "53.747253"}
{"text": "In those cases in which frequency distributions are available , it is unclear over what finite range or period they ought to be measured in order to derive probabilities .It is common to assume that the world is \" stationary \" and then to extrapolate an observed frequency to its limit as one imagines an infinite set of observations .", "label": "", "metadata": {}, "score": "53.765648"}
{"text": "We will highlight the most important ones here .Ensuring convergence .There are some challenges involved in determining optimal values for the number of replicas and the force constant .Ideally , should be chosen as large as possible , but as illustrated by Figure 2 , this will impose a requirement for a larger number of replicas as well .", "label": "", "metadata": {}, "score": "53.831223"}
{"text": "We will highlight the most important ones here .Ensuring convergence .There are some challenges involved in determining optimal values for the number of replicas and the force constant .Ideally , should be chosen as large as possible , but as illustrated by Figure 2 , this will impose a requirement for a larger number of replicas as well .", "label": "", "metadata": {}, "score": "53.831223"}
{"text": "For compactness , boldface is used to denote a sets of replicas or restraints ( e.g. , ) .Assuming independence between the restraints , we have : ( 4 ) Assuming flat priors , we have , and assuming independent Gaussian distributions on the latter , ( 5 ) where again is the calculated ensemble averaged quantity of data .", "label": "", "metadata": {}, "score": "53.85534"}
{"text": "For compactness , boldface is used to denote a sets of replicas or restraints ( e.g. , ) .Assuming independence between the restraints , we have : ( 4 ) Assuming flat priors , we have , and assuming independent Gaussian distributions on the latter , ( 5 ) where again is the calculated ensemble averaged quantity of data .", "label": "", "metadata": {}, "score": "53.85534"}
{"text": "The algorithm \" UNSEEN \" has a free parameter that controls the error tolerance .Their theorem states that the total variations is bounded by with only samples where n denotes the support size .The resulting estimate of the fingerprint can be used to estimate entropy , unseen probability mass , support , and total variations .", "label": "", "metadata": {}, "score": "53.954872"}
{"text": "Sample - based entropy estimation .( A )The bias in entropy estimates computed directly from samples drawn from K - pairwise models .The NSB entropy estimate [ 71 ] in bits per neuron computed using samples from the model ( same size as the experimental data set ) on y - axis ; the true entropy ( using heat capacity integration ) method on x - axis .", "label": "", "metadata": {}, "score": "53.97327"}
{"text": "As emphasized repeatedly here , we should not confuse our knowledge of a physical system with the physical system itself ( which we believe to have an independent existence ) .\" Random \" Neural Activity in Models of Bayesian Inference .", "label": "", "metadata": {}, "score": "54.014694"}
{"text": "Scientists routinely use the Boltzmann distribution to describe the probability that a voltage - sensitive channel will be in a particular conformation ( and therefore have a particular conductance across the membrane ) conditional on knowledge of the membrane voltage [ 53 ] .", "label": "", "metadata": {}, "score": "54.08056"}
{"text": "For example , a Bayesian network can be used to calculate the probability of a patient having a specific disease , given the absence or presence of certain symptoms , if the probabilistic independencies between symptoms and disease as encoded by the graph hold .", "label": "", "metadata": {}, "score": "54.091415"}
{"text": "Also , suppose that the rain has a direct effect on the use of the sprinkler ( namely that when it rains , the sprinkler is usually not turned on . )Then the situation can be modelled with the adjacent Bayesian network .", "label": "", "metadata": {}, "score": "54.165337"}
{"text": "The maximum entropy models that we construct from the data do not appear to simplify along any conventional axes .The matrix of correlations among spikes in different cells ( Figure 1A ) is of full rank , so that principal component analysis does not yield significant dimensionality reduction .", "label": "", "metadata": {}, "score": "54.181084"}
{"text": "The frequentist definition contradicts intuition , and is of limited use , because it completely fails to account for information that is obviously relevant to probabilities .The simple example given above is the statement that \" there are ' N ' mutually exclusive states \" ( Table 1 ) .", "label": "", "metadata": {}, "score": "54.260468"}
{"text": "One critique of the method relates to the ratio of the number of free parameters ( atomic coordinates ) to the number of experimental data points .In \" normal \" ( non - ensemble ) structure determination , there are typically fewer experimental data than atomic positions to be determined ; the problem is underdetermined and additional ( prior ) information , e.g. , from a force field , is needed to determine structures .", "label": "", "metadata": {}, "score": "54.333015"}
{"text": "One critique of the method relates to the ratio of the number of free parameters ( atomic coordinates ) to the number of experimental data points .In \" normal \" ( non - ensemble ) structure determination , there are typically fewer experimental data than atomic positions to be determined ; the problem is underdetermined and additional ( prior ) information , e.g. , from a force field , is needed to determine structures .", "label": "", "metadata": {}, "score": "54.333015"}
{"text": "This expression now only includes , the experimentally determined quantity that is an estimate of the true , underlying value .The equation above , derived by Cavalli et al . is quite striking , in the sense that it corresponds exactly to the form used in classic ensemble simulations ( Eq . 1 ) , except that the force constant that can normally be tuned freely is now determined uniquely by the uncertainty in the observed experimental values .", "label": "", "metadata": {}, "score": "54.48159"}
{"text": "This expression now only includes , the experimentally determined quantity that is an estimate of the true , underlying value .The equation above , derived by Cavalli et al . is quite striking , in the sense that it corresponds exactly to the form used in classic ensemble simulations ( Eq . 1 ) , except that the force constant that can normally be tuned freely is now determined uniquely by the uncertainty in the observed experimental values .", "label": "", "metadata": {}, "score": "54.48159"}
{"text": "Similarly , if asked to estimate an unknown proportion between 0 and 1 , we might say that all proportions are equally likely and use a uniform prior .Alternatively , we might say that all orders of magnitude for the proportion are equally likely , which gives a prior proportional to the logarithm .", "label": "", "metadata": {}, "score": "54.518368"}
{"text": "Unfortunately , admissibility is often difficult to check , although some results are known ( e.g. , Berger and Strawderman 1996 ) .The issue is particularly acute with hierarchical Bayes models ; the usual priors ( e.g. , Jeffreys ' prior ) may give badly inadmissible decision rules if employed at the higher levels of the hierarchy .", "label": "", "metadata": {}, "score": "54.528015"}
{"text": "Two fundamentally distinct definitions of probability have been proposed , \" Jaynesian \" ( better known as \" Bayesian \" ) and \" frequentist \" ( summarized in Table 1 ) .The key difference is that the Jaynesian definition asserts that probabilities are a property of the information possessed by an observer about an object , whereas the frequentist definition asserts that the probabilities are a property of the object itself , independent of any observer ( Figure 1 ) .", "label": "", "metadata": {}, "score": "54.66691"}
{"text": "We envisage that new theoretical developments , such as the link between ensemble simulations and maximum entropy solutions , can be directly applicable in other fields .Similarly , new methods for deriving modified models in the context of noisy data should have broad applicability .", "label": "", "metadata": {}, "score": "54.670414"}
{"text": "We envisage that new theoretical developments , such as the link between ensemble simulations and maximum entropy solutions , can be directly applicable in other fields .Similarly , new methods for deriving modified models in the context of noisy data should have broad applicability .", "label": "", "metadata": {}, "score": "54.670414"}
{"text": "The ratio of the log - likelihoods on test vs training data , shown as a function of the network size N .Error bars are the standard deviation across 30 subgroups at each value of N .Do the models work ?", "label": "", "metadata": {}, "score": "54.76615"}
{"text": "The most likely state of the network , in all the cases we have explored , is complete silence .Further , in the K - pairwise models , this probability is reproduced exactly , since it is just .But then we see that , in this model , estimating the probability of silence ( which we can do directly from the data ) is the same as estimating the partition function Z , which usually is very difficult since it involves summing over all possible states .", "label": "", "metadata": {}, "score": "54.831596"}
{"text": "Furthermore , if the patterns of spiking and silence really are codewords for the stimulus , then the mutual information between the stimulus and response , , can be at most the entropy of the codewords , .Thus , the entropy of the system 's output bounds the information transmission .", "label": "", "metadata": {}, "score": "55.011143"}
{"text": "Intuitively , we would like to quantify the amount of information in such a way that entropy corresponds directly to an absence of information , similar to the way in which a vacuum corresponds to an absence of matter .We would like to use a single and universal scale , allowing us to measure it in an absolute sense , in isolation and without consideration of any other information .", "label": "", "metadata": {}, "score": "55.035942"}
{"text": "They had developed SM through statistical inferences , but because of frequentist notions , SM had not been recognized as purely a matter of inference .Given the lack of any objective basis for inference , the derivation of probability distributions was conceptually complex and full of assumptions ( such as \" ergodicity \" ) that seemed presumptuous and rather arbitrary .", "label": "", "metadata": {}, "score": "55.06265"}
{"text": "That he utilizes both Jaynesian and frequentist definitions , side by side and without acknowledgement , is not at all exceptional within the field of Bayesian brain theory ( see below ) .The prevalence of the frequentist view in neuroscience is not immediately obvious because it is routinely used without any acknowledgment that more than one definition of probability exists .", "label": "", "metadata": {}, "score": "55.09895"}
{"text": "They are all relative to the information X of the observer , and thus for two observers to agree on their values , the two observers must work from the same frame of reference X .Shannon did not state this explicitly , at least in his original work [ 1 ] .", "label": "", "metadata": {}, "score": "55.150734"}
{"text": "For sufficiently large values of the harmonic term mimics a -function and when is increased for such values of the distribution converges towards the maximum entropy solution without explicitly determining any Lagrange multipliers .In this example , we only have experimental data regarding the y - dimension of the distribution ( target value indicated by dotted line ) .", "label": "", "metadata": {}, "score": "55.258568"}
{"text": "For sufficiently large values of the harmonic term mimics a -function and when is increased for such values of the distribution converges towards the maximum entropy solution without explicitly determining any Lagrange multipliers .In this example , we only have experimental data regarding the y - dimension of the distribution ( target value indicated by dotted line ) .", "label": "", "metadata": {}, "score": "55.258568"}
{"text": "Having convinced ourselves that we can build models which give an accurate description of the probability distribution over the states of spiking and silence in the network , we can ask what these models teach us about function .As emphasized in Ref [ 4 ] , one corollary of collective behavior is the possibility of error correction or pattern completion - we can predict the spiking or silence of one neuron by knowing the activity of all the other neurons .", "label": "", "metadata": {}, "score": "55.344906"}
{"text": "Another issue of importance is that if an uninformative prior is to be used routinely , i.e. , with many different data sets , it should have good frequentist properties .Normally a Bayesian would not be concerned with such issues , but it can be important in this situation .", "label": "", "metadata": {}, "score": "55.380035"}
{"text": "In fact , even if they are in the wrong class , these estimators are consistent , and often give reasonable answers even in the undersampled regime .Nemenman - Shafee - Bialek ( NSB ) uses a mixture of Dirichlet prior to have an approximately uninformative implied prior on entropy .", "label": "", "metadata": {}, "score": "55.396286"}
{"text": "We note that is a special case of a more general linear function , , where are arbitrary weights and \u03b8 is a threshold ( and for K ) .In any case , the K - pairwise model should not be regarded as \" the ultimate model \" of the retinal code : it is a model that emerged from pairwise constructions in a data - driven attempt to account for systematic deficiencies of Ising - like models on large populations .", "label": "", "metadata": {}, "score": "55.466106"}
{"text": "While it is intriguing to think about the neural code as being organized around the \" collective metastable states , \" some of which we have identified using the maximum entropy model , further work is necessary to explore this idea in detail .", "label": "", "metadata": {}, "score": "55.627773"}
{"text": "Then we can compute the average energy from a single MC sampling run , and find the entropy for each network .As shown in Figures S2B and C , the results agree both with the heat capacity integration and with the Wang - Landau method , to an accuracy of better than 1 % .", "label": "", "metadata": {}, "score": "55.651024"}
{"text": "If input does not fully determine the output of a physical system , is the system partially \" random \" ?The answer , with respect to each of the four cases above , is \" no \" according to the view of contemporary physics .", "label": "", "metadata": {}, "score": "55.679237"}
{"text": "In those cases in which ( non - flat ) priors have been used , they have been determined by the experimenter 's knowledge of a frequency distribution and they have had no known biophysical basis .The failure of the authors to adopt a neurocentric perspective might be attributed to lingering frequentist notions ( see Section 4.1 ) .", "label": "", "metadata": {}, "score": "55.690056"}
{"text": "In many cases , natural data have a vast number of possible symbols , as in the case of species samples or language , and have power - law ( or scale - free ) distributions .Power - law tails can hide a lot of entropy in their tails , in which case PYM is recommended .", "label": "", "metadata": {}, "score": "55.702774"}
{"text": "The results , in Figure S2A , are in excellent agreement with the heat capacity integration .K - pairwise models have the attractive feature that , by construction , they match exactly the probability of the all - silent pattern , , seen in the data .", "label": "", "metadata": {}, "score": "55.839333"}
{"text": "doi : 10.1529/biophysj.107.108241 .Cavalli A , Camilloni C , Vendruscolo M ( 2013 ) Molecular dynamics simulations with replica averaged structural restraints generate structural ensembles according to the maximum entropy principle .J Chem Phys 138 : 094112 . doi : 10.1063/1.4793625 .", "label": "", "metadata": {}, "score": "55.894375"}
{"text": "doi : 10.1529/biophysj.107.108241 .Cavalli A , Camilloni C , Vendruscolo M ( 2013 ) Molecular dynamics simulations with replica averaged structural restraints generate structural ensembles according to the maximum entropy principle .J Chem Phys 138 : 094112 . doi : 10.1063/1.4793625 .", "label": "", "metadata": {}, "score": "55.894375"}
{"text": "We shall see that OME is actually a particular instance of MEM , when the reference measure is a Poisson Measure .Abstract : A generalized maximum entropy estimator is developed for the linear simultaneous equations model .Monte Carlo sampling experiments are used to evaluate the estimator 's performance in small and medium sized samples , suggesting contexts in which the current generalized maximum entropy estimator is superior in mean square error to two and three stage least squares .", "label": "", "metadata": {}, "score": "55.924866"}
{"text": "Bayesian inference is commonly put forward as an answer to this question .It provides a simple recipe for how to produce a new model ( posterior ) by modifying an existing model ( prior ) after observing a new set of data .", "label": "", "metadata": {}, "score": "55.97675"}
{"text": "Bayesian inference is commonly put forward as an answer to this question .It provides a simple recipe for how to produce a new model ( posterior ) by modifying an existing model ( prior ) after observing a new set of data .", "label": "", "metadata": {}, "score": "55.97675"}
{"text": "These could include either new higher - order interactions , global constraints , or couplings across time bins , as suggested by Refs [ 12 ] , [ 90 ] .Perhaps the most useful global test of our models is to ask about the distribution of state probabilities : how often should we see combinations of spiking and silence that occur with probability P ?", "label": "", "metadata": {}, "score": "55.9861"}
{"text": "As an example , Box 1 contains a primer of the basic maximum entropy procedure on the simple problem of inferring the probability of the different outcomes of a ( possibly biased ) die , given only information about the average observed after a large number of throws .", "label": "", "metadata": {}, "score": "55.987587"}
{"text": "As an example , Box 1 contains a primer of the basic maximum entropy procedure on the simple problem of inferring the probability of the different outcomes of a ( possibly biased ) die , given only information about the average observed after a large number of throws .", "label": "", "metadata": {}, "score": "55.987587"}
{"text": "Here we summarize the evidence that these multiple tools lead to consistent answers , so that we can be confident in our estimates .Our first try at entropy estimation is based on the heat capacity integration in Eq .To begin , with neurons , we can enumerate all states of the network and hence we can find the maximum entropy distributions exactly ( with no Monte Carlo sampling ) .", "label": "", "metadata": {}, "score": "56.015244"}
{"text": "By observing that sample , the probability distribution conditional on the information of the molecular sensor ( and thus the neuron ) is narrower than it would be were the sensor not there to observe the sample ( see Figure 1 in [ 61 ] ) .", "label": "", "metadata": {}, "score": "56.080555"}
{"text": "In some cases our knowledge derives almost entirely from observing the past frequency of an event , in which case the probability distribution that best describes our knowledge may closely resemble the observed frequency distribution .Thus knowledge derived from measurement of frequency distributions is treated just like any other knowledge .", "label": "", "metadata": {}, "score": "56.094143"}
{"text": "( B )The responses of a set of 120 neurons to a single stimulus repeat , black dots designate spikes .Maximum entropy .The idea of maximizing entropy has its origin in thermodynamics and statistical mechanics .Here we provide a description of this approach which we hope makes the ideas accessible to a broad audience .", "label": "", "metadata": {}, "score": "56.136215"}
{"text": "The random component typically corresponds to a component of the model that is not sufficiently understood , or not sufficiently important , for the scientist to specify in detail .The scientist has complete knowledge of the model , which he or she created .", "label": "", "metadata": {}, "score": "56.230396"}
{"text": "There is no meaningful sense in which a moving object \" performs \" the methods of calculus .The methods of calculus simply help us to accurately describe the motion .Likewise , the use of probabilities is purely descriptive with respect to a neuron 's information and uncertainty .", "label": "", "metadata": {}, "score": "56.255295"}
{"text": "In general , the solution to this type of optimization problem takes the form : ( 9 ) where runs over the number of constraints , and Z is the partition function , which ensures proper normalization .Notice how the value produces the uniform distribution as we expect , while higher or lower values produce gradually more skewed distributions .", "label": "", "metadata": {}, "score": "56.263763"}
{"text": "In general , the solution to this type of optimization problem takes the form : ( 9 ) where runs over the number of constraints , and Z is the partition function , which ensures proper normalization .Notice how the value produces the uniform distribution as we expect , while higher or lower values produce gradually more skewed distributions .", "label": "", "metadata": {}, "score": "56.263763"}
{"text": "The top - left plot is the unperturbed potential , while the top - right plot shows the maximum entropy solution with a numerically optimized Lagrange multiplier .The plots in the matrix show the behavior of different combinations of the force constant of the harmonic potential and the number of replicas used in the simulation .", "label": "", "metadata": {}, "score": "56.297447"}
{"text": "The top - left plot is the unperturbed potential , while the top - right plot shows the maximum entropy solution with a numerically optimized Lagrange multiplier .The plots in the matrix show the behavior of different combinations of the force constant of the harmonic potential and the number of replicas used in the simulation .", "label": "", "metadata": {}, "score": "56.297447"}
{"text": "When MD or MC methods are used to sample protein conformations they typically give rise to an ensemble of conformations that are distributed according the celebrated Boltzmann distribution , , that relates the probability of observing a given conformation to the energy of that conformation .", "label": "", "metadata": {}, "score": "56.392925"}
{"text": "When MD or MC methods are used to sample protein conformations they typically give rise to an ensemble of conformations that are distributed according the celebrated Boltzmann distribution , , that relates the probability of observing a given conformation to the energy of that conformation .", "label": "", "metadata": {}, "score": "56.392925"}
{"text": "The joint probability function is : .P .G .S .R . )P .G .S .R . )P .S .R . )P .R . )P .R .T .", "label": "", "metadata": {}, "score": "56.444046"}
{"text": "Kass RE , Wasserman L ( 1996 )The selection of prior distributions by formal rules .J Am Stat Assoc 91 : 1343 - 1370 MATH CrossRef .Lannoy A , Procaccia H ( 2003 ) L'utilisation du jugement d'expert en s\u00fbret\u00e9 de fonctionnement .", "label": "", "metadata": {}, "score": "56.459553"}
{"text": "We can then repeat this for every neuron , in turn .If the model is correct , spiking probability should depend on the effective field according to Eq ( 26 ) .We emphasize that there are no new parameters to be fit , but rather a parameter - free relationship to be tested .", "label": "", "metadata": {}, "score": "56.478413"}
{"text": "This problem can be substantially avoided by adherence to the principles developed by E.T. Jaynes [ 34 , 35 ] .Although Jaynes expressed these principles in the rigorous mathematical form of probability theory , they apply to all knowledge and reason , and thus they are relevant far beyond the formal application of probabilities .", "label": "", "metadata": {}, "score": "56.514015"}
{"text": "We use the least squares cross validation to derive the optimal weights .MonteCarlo simulations demonstrate that the proposedW - GME estimator is comparable to and often outperforms the conventional GME estimator , which places equal weights on the entropies of coefficient and disturbance distributions .", "label": "", "metadata": {}, "score": "56.577194"}
{"text": "This model already has a lot of structure , including the extreme inhomogeneity that we have emphasized here .The idea that biological networks might organize themselves to critical points has a long history , and several different notions of criticality have been suggested [ 31 ] .", "label": "", "metadata": {}, "score": "56.592964"}
{"text": "To imagine otherwise is wishful thinking .Without any real , unique , and well - defined frequency distribution , the concept of a \" true \" probability , which we try to estimate , is just fantasy .Because measurement of a frequency distribution over a finite period is never sufficient to fully specify a probability distribution , derivation of a probability distribution requires incorporation of additional information that is not present in the observed frequency distribution .", "label": "", "metadata": {}, "score": "56.614586"}
{"text": "JS ] .I. Nemenman .Coincidences and estimation of entropies of random variables with large cardinalities .Entropy , 13(12):2013 - 2023 , 2011 .[Asymptotic NSB ] .I. Nemenman , F. Shafee , and W. Bialek .Entropy and inference , revisited .", "label": "", "metadata": {}, "score": "56.733322"}
{"text": "The paper shows that conventional data balancing methods , such as generalized least squares , weighted least squares and biproportional methods are particular cases of the general method described here .As a consequence , it is possible to determine the underlying assumptions and range of application of each traditional method .", "label": "", "metadata": {}, "score": "56.766945"}
{"text": "Friston attributes entropy to a fish , stating \" a fish that frequently forsook water would have high entropy \" .He goes on to define a mathematical measure of \" surprise \" ( as well as entropy ) as a function of probabilities , yet he does not specify that these probabilities are conditional on any information .", "label": "", "metadata": {}, "score": "56.767677"}
{"text": "Results are shown in Figure 8A ( analogous results for the pairwise model are shown in Figure S5 ) .We see that the distributions of energies in the data and the model are very similar .There is an excellent match in the \" low energy \" ( high probability ) region , and then as we look at the high energy tail ( ) we see that theory and experiment match out to probabilities of better than .", "label": "", "metadata": {}, "score": "56.878044"}
{"text": "Are these cases fundamentally distinct from one another ?What is a \" random process \" , and how would one recognize it ?Figure 3 .Illustration of neural systems that are sometimes said to be \" random , stochastic , noisy , or probabilistic . \"", "label": "", "metadata": {}, "score": "56.96176"}
{"text": "We used a modified version of our previously published learning procedure to compute the maximum entropy models given measured constraints [ 37 ] ; the proof of convergence for the core of this L1-regularized maximum entropy algorithm is given in Ref .", "label": "", "metadata": {}, "score": "57.003212"}
{"text": "A classic example of work in this spirit is the Hopfield model of associative or content - addressable memory [ 1 ] , which is able to recover the correct memory from any of its subparts of sufficient size .On the other hand , precisely because of these abstractions , it has not always been clear how to bring the predictions of the models into contact with experiment .", "label": "", "metadata": {}, "score": "57.15814"}
{"text": "Conventional Approaches to Probability and Information in Neuroscience .The Frequentist View in Neuroscience .The frequentist view of probability may be very slowly falling out of favor , but it still exerts a dominant influence within neuroscience .In 2004 , a prominent neuroscientist wrote in a letter to the author : \" How can probabilities of external events be conditional on the internal information an animal has , unless we assume telekinesis \" ?", "label": "", "metadata": {}, "score": "57.16347"}
{"text": "The effect can also be understood in the detailed analysis by Roux and Weare of a 1D harmonic potential with a harmonic restraint .In particular , their calculations show that when the number of replicas is increased for a fixed force constant , the mean of the restrained ensemble converges to that of the prior reference distribution while the variance increases to its correct value .", "label": "", "metadata": {}, "score": "57.255398"}
{"text": "The effect can also be understood in the detailed analysis by Roux and Weare of a 1D harmonic potential with a harmonic restraint .In particular , their calculations show that when the number of replicas is increased for a fixed force constant , the mean of the restrained ensemble converges to that of the prior reference distribution while the variance increases to its correct value .", "label": "", "metadata": {}, "score": "57.255398"}
{"text": "( B ) Extrapolating the N dependence of to large N .K - pairwise models reproduce the coincidence probability very well , with the fractional error in at of 0.3 % .If one constructs a model that reproduces exactly the silent state probability , while in the non - silent patterns the neurons are assumed to spike independently , all with the identical firing rate equal to the population mean , the error in prediction is 7.5 % .", "label": "", "metadata": {}, "score": "57.334167"}
{"text": "Finally , there are methods that allow us to estimate entropy by counting samples even in cases where the number of samples is much smaller than the number of states [ 71 ] ( NSB ) .As N increases , these direct estimates of entropy become significantly dependent on the sample size , and start to disagree with the heat capacity integration .", "label": "", "metadata": {}, "score": "57.5968"}
{"text": "A Bayesian network is a carrier of the conditional independencies of a set of variables , not of their causal connections .However , causal relations can be modelled by the closely related causal Bayesian network .Using this semantics , one can predict the impact of external interventions from data obtained prior to intervention .", "label": "", "metadata": {}, "score": "57.64428"}
{"text": "\" There are significant challenges associated with estimating and sampling from such models , but recent work provides hope for the eventual feasibility of such an approach .First , advances in techniques for force field optimization [ 8 ] , [ 52 ] allow for a Bayesian approach to integrate experimental data and , e.g. , quantum - level data , bringing us closer to the ability to probe the uncertainties associated with individual parameters .", "label": "", "metadata": {}, "score": "57.65114"}
{"text": "\" There are significant challenges associated with estimating and sampling from such models , but recent work provides hope for the eventual feasibility of such an approach .First , advances in techniques for force field optimization [ 8 ] , [ 52 ] allow for a Bayesian approach to integrate experimental data and , e.g. , quantum - level data , bringing us closer to the ability to probe the uncertainties associated with individual parameters .", "label": "", "metadata": {}, "score": "57.65114"}
{"text": "In the case of brain research , each of these might be difficult or even impossible as a practical matter , but there is no reason to think that it can not be done as a matter of principle .Furthermore , there is reason to think this may not be so challenging with respect to the neural basis of inference ( see Section 5.3 ) .", "label": "", "metadata": {}, "score": "57.66564"}
{"text": "If the local distributions of no variable depends on more than 3 parent variables , the Bayesian network representation only needs to store at most . values .One advantage of Bayesian networks is that it is intuitively easier for a human to understand ( a sparse set of ) direct dependencies and local distributions than complete joint distribution .", "label": "", "metadata": {}, "score": "57.669296"}
{"text": "The Jaynesian answer is that , in the absence of any additional information , logic requires that all outcomes be considered equally likely , and thus the probability of each is 1/4 or 0.25 .The frequentist response is that the question is inappropriate .", "label": "", "metadata": {}, "score": "57.776398"}
{"text": "Jaynes derived the probability distributions of SM , most notably the Boltzmann distribution , conditional only on a single \" observation \" ( or \" sample \" ) of temperature .This idea formed the basis for my previous proposal that the Boltzmann distribution can describe the information held within an ion channel ( and less directly , the information held within membrane voltage , which is determined by ion channels ) [ 5 , 61 ] .", "label": "", "metadata": {}, "score": "57.84577"}
{"text": "Thus , the correlations among neurons make the recurrence of combinatorial patterns thousands of times more likely than would be expected from independent neurons , and this effect is even larger than simply the reduction in entropy .This suggests that the true distribution over states is extremely inhomogeneous , not just because total silence is anomalously probable but because the dynamic range of probabilities for the different active states also is very large .", "label": "", "metadata": {}, "score": "57.94833"}
{"text": "Error bars on data computed by bootstrapping ; error bars on MC estimates obtained by repeated MC runs generating a number of samples that is equal to the original data size .( C )The distribution of the difference between true and model values for covariance matrix elements , normalized by the estimated error bar in the data ; red overlay is a Gaussian with zero mean and unit variance .", "label": "", "metadata": {}, "score": "58.065815"}
{"text": "Jaynesian probability theory incorporates uncertainty by describing confidence ( or strength of belief ) in a proposition on a continuous scale of 0 to 1 .Conventional logic and deductive reasoning can therefore be understood as a special case of Jaynesian rationality .", "label": "", "metadata": {}, "score": "58.086975"}
{"text": "The term \" Jaynesian \" may draw attention to the more fundamental importance of the definition of probabilities as exclusively conditional .The most critical problem which has been created by frequentist notions , and which still persists in work purported to be \" Bayesian \" , is the misattribution of knowledge , as documented below .", "label": "", "metadata": {}, "score": "58.0915"}
{"text": "We verified the correctness of the algorithm explicitly for groups of 10 and 20 neurons where exact numerical solutions are feasible .Supplementary Figure S1 provides a summary of the models we have learned for populations of different sizes .In small networks there is a systematic bias to the distribution of parameters , but as we look to larger networks this vanishes and the distribution of becomes symmetric .", "label": "", "metadata": {}, "score": "58.132637"}
{"text": "What had once appeared to some to be \" random \" could then be reclassified as \" deterministic \" , but of course all that would have changed is the knowledge of scientists .The case of action potential generation is better understood and is generally thought to be more directly relevant to neural inference .", "label": "", "metadata": {}, "score": "58.238625"}
{"text": "It remains to be seen whether this poses a significant problem for the application of this method in practice .Estimating Lagrange multipliers .Despite the convenience of the replica - averaged method , it remains unclear whether this method is always preferable to an approach that estimates the Lagrange multipliers explicitly .", "label": "", "metadata": {}, "score": "58.40325"}
{"text": "It remains to be seen whether this poses a significant problem for the application of this method in practice .Estimating Lagrange multipliers .Despite the convenience of the replica - averaged method , it remains unclear whether this method is always preferable to an approach that estimates the Lagrange multipliers explicitly .", "label": "", "metadata": {}, "score": "58.40325"}
{"text": "Quantify the amount of information .For inference and most applications , this is not necessary .Response to Criticisms of Jaynesian Probabilities .Several criticisms have been made of Jaynesian probabilities .Extensive counterarguments have been given elsewhere ( e.g. , [ 35 , 38 ] ) , but I will briefly summarize several general points and relate these to models of brain function .", "label": "", "metadata": {}, "score": "58.562134"}
{"text": "Finally , we study variance swaps and derive a simple formula relating the fair variance swap rate to entropy .Then we show , again , that the prior loses its influence on the fair variance swap rate as the number of strikes increases .", "label": "", "metadata": {}, "score": "58.58726"}
{"text": "Reading independencies and d -separation .The graph encodes independencies between variables .Conditional independence is represented by the graphical property of d -separation : If two sets of nodes X and Y are d -separated in the graph by a third set Z , then the corresponding variable sets X and Y are independent given the variables in Z .", "label": "", "metadata": {}, "score": "58.598885"}
{"text": "Machine Learning .Using \" Poissonization \" of the fingerprint ( a.k.a .Zipf plot , count histogram , pattern , hist - hist , collision statistics , etc . ) , they find a simplest distribution such that the expected fingerprint is close to the observed fingerprint .", "label": "", "metadata": {}, "score": "58.661057"}
{"text": "While this sounds like a trivial principle , it is actually violated by many existing methods .Conceptually , the maximum entropy procedure is simple , and we proceed exactly as we did for the die example .Technical issues , however , seem to have hindered a practically useful implementation of the method .", "label": "", "metadata": {}, "score": "58.83393"}
{"text": "While this sounds like a trivial principle , it is actually violated by many existing methods .Conceptually , the maximum entropy procedure is simple , and we proceed exactly as we did for the die example .Technical issues , however , seem to have hindered a practically useful implementation of the method .", "label": "", "metadata": {}, "score": "58.83393"}
{"text": "Striking ( and model - independent ) evidence for nontrivial collective behavior in these networks is obtained by asking for the probability that K out of the N neurons generate a spike in the same small window of time , as shown in Figure 5 .", "label": "", "metadata": {}, "score": "58.85314"}
{"text": "Here I argue in favor of the view of Jaynes ( and Laplace , shown at left ) that probabilities are always conditional on the information of an observer about an object .I presume that the observer 's information must be in a physical form inside the observer .", "label": "", "metadata": {}, "score": "59.01323"}
{"text": "However , the latter is paradoxical , since it appears that a consistent system declares its own inconsistency , and the natural number system that we are familiar with is not a model for the system .But , it could be resolved by creating a non - standard model of arithmetic .", "label": "", "metadata": {}, "score": "59.01429"}
{"text": "( B ) Fractional difference between the heat capacity method and the entropy determined from the all - silent pattern .( C ) Fractional difference between the Wang - Landau sampling method and the entropy determined from the all - silent pattern .", "label": "", "metadata": {}, "score": "59.216507"}
{"text": "Abstract .Maximum entropy models are the least structured probability distributions that exactly reproduce a chosen set of statistics measured in an interacting network .Here we use this principle to construct probabilistic models which describe the correlated spiking activity of populations of up to 120 neurons in the salamander retina as it responds to natural movies .", "label": "", "metadata": {}, "score": "59.43927"}
{"text": "Clarke B , Barron AR ( 1994 ) Jeffreys ' prior is asymptotically least favorable under entropy risk .J Stat Plan Inference 41 : 37 - 60 MATH CrossRef MathSciNet .Clarke B ( 2007 )Information optimality and Bayesian modelling .", "label": "", "metadata": {}, "score": "59.451756"}
{"text": "Fairhall , A.L. ; Lewen , G.D. ; Bialek , W. ; de Ruyter van Steveninck , R.R. Efficiency and ambiguity in an adaptive neural code .Nature 2001 , 412 , 787 - 792 .[ Google Scholar ] .Simoncelli , E.P. ; Olshausen , B.A. Natural image statistics and neural representation .", "label": "", "metadata": {}, "score": "59.468166"}
{"text": "Instead of maximizing mutual information between the features and target variable for dimensionality reduction , they propose to minimize the dependence between the non - feature space and the joint of target variable and feature space .As a dependence measure , they use HSIC ( Hilbert - Schmidt independence criterion : squared distance between joint and the product of marginals embedded in the Hilbert space ) .", "label": "", "metadata": {}, "score": "59.48826"}
{"text": "Roux and Weare point out that even when successfully finding all Lagrange multipliers , one still has to run an entire simulation .Similar problems seem to assert themselves for the replica - case , where production runs can only be conducted once convergence in entropy has been ensured .", "label": "", "metadata": {}, "score": "59.580738"}
{"text": "Roux and Weare point out that even when successfully finding all Lagrange multipliers , one still has to run an entire simulation .Similar problems seem to assert themselves for the replica - case , where production runs can only be conducted once convergence in entropy has been ensured .", "label": "", "metadata": {}, "score": "59.580738"}
{"text": "Trappenberg , T.P. Fundamentals of Computational Neuroscience , 2nd ed ; Oxford University Press : Oxford , UK , 2010 .[ Google Scholar ] .Schultz , W. ; Preuschoff , K. ; Camerer , C. ; Hsu , M. ; Fiorillo , C.D. ; Tobler , P.N. ; Bossaerts , P. Explicit neural signals reflecting reward uncertainty .", "label": "", "metadata": {}, "score": "59.638702"}
{"text": "And if maximization of entropy can somehow be achieved readily , even for complex sets of information , then BT is unnecessary .Third , it has been proposed that neurons perform BT , taking a prior and one or more likelihood functions as inputs , and generating an output that represents the posterior [ 6 , 27 , 28 , 29 , 30 , 31 ] .", "label": "", "metadata": {}, "score": "59.666985"}
{"text": "P .R .d .o .G .T . )P .R . )For example , a naive way of storing the conditional probabilities of 10 two - valued variables as a table requires storage space for .", "label": "", "metadata": {}, "score": "59.668804"}
{"text": "As such , the system - specific force field correction introduced by the restraints , when applied appropriately , may be viewed as a natural extension of the Boltzmann ensemble when one is provided with additional information beyond the energy .In addition to the theoretical developments highlighted in this article , an important area for future studies is how best to implement them in practice .", "label": "", "metadata": {}, "score": "59.725086"}
{"text": "As such , the system - specific force field correction introduced by the restraints , when applied appropriately , may be viewed as a natural extension of the Boltzmann ensemble when one is provided with additional information beyond the energy .In addition to the theoretical developments highlighted in this article , an important area for future studies is how best to implement them in practice .", "label": "", "metadata": {}, "score": "59.725086"}
{"text": "The matrix shows various combinations of force constant ( ) and number of replicas ( ) when enforcing the restraint through a harmonic potential .In these calculations , corresponds to the standard method for structure calculation , and corresponds to ensemble refinement .", "label": "", "metadata": {}, "score": "59.817814"}
{"text": "The matrix shows various combinations of force constant ( ) and number of replicas ( ) when enforcing the restraint through a harmonic potential .In these calculations , corresponds to the standard method for structure calculation , and corresponds to ensemble refinement .", "label": "", "metadata": {}, "score": "59.817814"}
{"text": "Andrew Tan : Holographic entanglement entropy .Andrew wanted to connect how space - time structure can be derived from holographic entanglement entropy , and furthermore to link it to graphical models such as the restricted Boltzmann machine .He gave overviews of quantum mechanics ( deterministic linear dynamics of the quantum states ) , density matrix , von Neumann entropy , and entanglement entropy ( entropy of a reduced density matrix , where we assume partial observation and marginalization over the rest ) .", "label": "", "metadata": {}, "score": "60.002632"}
{"text": "We emphasize that building a model is , in this view , the first step rather than the last step .But building a model is challenging , because the space of states is very large and data are limited .Thus , long , stable recordings are even more crucial than usual .", "label": "", "metadata": {}, "score": "60.147636"}
{"text": "Linnemann JT ( 2000 ) Upper limits and priors .FNAL CL Worshop ( with notes added in January 2003 ) .Luttrell SP ( 1985 )The use of transinformation in the design of data sampling schemes for inverse problems .", "label": "", "metadata": {}, "score": "60.14932"}
{"text": "A note on noninformative priors for Weibull distributions .J Stat Plan Inference 61 : 319 - 338 MATH CrossRef .van Noortwijk JM , Dekker R , Cooke RM , Mazzuchi TA ( 1992 ) Expert judgment in maintenance optimization .", "label": "", "metadata": {}, "score": "60.226288"}
{"text": "The most common technique is to combine a physical force field , with an experimentally derived \" biasing potential , \" : .The function acts to bias simulations to provide structures that are compatible with experiments and typically takes the form of a harmonic potential that penalizes protein structures that are not in agreement with experiments : .", "label": "", "metadata": {}, "score": "60.34099"}
{"text": "The most common technique is to combine a physical force field , with an experimentally derived \" biasing potential , \" : .The function acts to bias simulations to provide structures that are compatible with experiments and typically takes the form of a harmonic potential that penalizes protein structures that are not in agreement with experiments : .", "label": "", "metadata": {}, "score": "60.34099"}
{"text": "How should we choose the functions ?In this work we consider three classes of possibilities : .We expect that networks have very different behaviors depending on the overall probability that neurons generate spikes as opposed to remaining silent .Thus , our first choice of functions to constrain in our models is the set of mean spike probabilities or firing rates , which is equivalent to constraining , for each neuron i. These constraints contribute a term to the energy function ( 5 )", "label": "", "metadata": {}, "score": "60.490433"}
{"text": "But the real networks are far from this prediction , as we can see in Figure 14A .( A )The probability that the combination of spikes and silences is exactly the same at two randomly chosen moments of time , as a function of the size of the population .", "label": "", "metadata": {}, "score": "60.540787"}
{"text": "Figure 1 .Are the information and probabilities used in neuroscience properties of the environment ( an observed object ) , the neural system under investigation , or the scientist ?The frequentist view is that probabilities are a property of a physical system ( or object ) that generates frequency distributions , and they exist independent of any observer of that system .", "label": "", "metadata": {}, "score": "60.550198"}
{"text": "Guida M , Calabria R , Pulcini G ( 1989 ) Bayes inference for a non - homogeneous Poisson process with power intensity law reliability .IEEE Trans Inform Theory 5 : 603 - 609 .Hill SD , Spall JC ( 1994 ) Sensitivity of a Bayesian analysis to the prior distribution .", "label": "", "metadata": {}, "score": "60.611782"}
{"text": "Although this superficially sounds reasonable - indeed the idea of the biasing potential is to bring the conformations to be in agreement with experiments - it brings with it some additional consequences .The basic problem arises because the experimental data , , are typically averaged over a very large number of molecules as well as averaged over timescales that are long compared to those typical of macromolecular fluctuations .", "label": "", "metadata": {}, "score": "60.75051"}
{"text": "Although this superficially sounds reasonable - indeed the idea of the biasing potential is to bring the conformations to be in agreement with experiments - it brings with it some additional consequences .The basic problem arises because the experimental data , , are typically averaged over a very large number of molecules as well as averaged over timescales that are long compared to those typical of macromolecular fluctuations .", "label": "", "metadata": {}, "score": "60.75051"}
{"text": "Because there are only N neurons , there are only N +1 possible values of K , and hence only N unique moments .Constraining all of these moments contributes a term to the energy function ( 17 ) where V is an effective potential [ 39 ] , [ 40 ] .", "label": "", "metadata": {}, "score": "60.81913"}
{"text": "The entire scientific enterprise is based on our belief that Nature is full of information that we can acquire through careful observation .Were we to abandon this belief in favor of the view that a particular system is \" random \" , then the implication is that the system has no further information to yield and thus we should not invest our time in searching for a deeper understanding of it .", "label": "", "metadata": {}, "score": "60.887836"}
{"text": "An interesting feature of the method is its potential to incorporate errors in the data .Here , we examine two possible ways of doing that .The two approaches have different intuitive interpretations , and one of them allows for error estimation .", "label": "", "metadata": {}, "score": "60.93772"}
{"text": "It uses an objective function that is the sum of the entropies for coefficient distributions and disturbance distributions .This method can be generalized to the weighted GME ( W - GME ) , where different weights are assigned to the two entropies in the objective function .", "label": "", "metadata": {}, "score": "61.120087"}
{"text": "A very different idea is provided by the Hopfield model , in which collective behavior is expressed in the stabilization of many discrete patterns of activity , combinations of spiking and silence across the entire network [ 1 ] , [ 2 ] .", "label": "", "metadata": {}, "score": "61.34607"}
{"text": "Frequentist probabilities tend to be very similar ( numerically ) to third - person Jaynesian probabilities .However , we can expect that the information of a brain or neuron is likely to be very different from that of a scientist about that same aspect of the world , in which case the probabilities will be very different .", "label": "", "metadata": {}, "score": "61.363304"}
{"text": "The Jeffreys prior for an unknown proportion is , which differs from Jaynes ' recommendation .Practical problems associated with uninformative priors include the requirement that the posterior distribution be proper .The usual uninformative priors on continuous , unbounded variables are improper .", "label": "", "metadata": {}, "score": "61.388317"}
{"text": "At the risk of oversimplification , the approach of Jaynes can be summarized by the statement : .Logic is Objective , Information is Subjective .Logic is universal and indisputable , whereas information is localized in space and time to an observer , and it typically differs over time between observers .", "label": "", "metadata": {}, "score": "61.47251"}
{"text": "Thus probabilities attributed to a brain under investigation have in fact been entirely a property of the scientist 's brain , an error of gross proportions .Despite these serious flaws with frequentism , I have found one small and not so serious piece of evidence in its favor .", "label": "", "metadata": {}, "score": "61.611553"}
{"text": "It could also help us to overcome the potentially confusing fact that \" information gain \" can actually be a negative number corresponding to information loss .For example , common language agrees without intuition that an observation Y corresponds to information and should have a positive value .", "label": "", "metadata": {}, "score": "61.663155"}
{"text": "The use of these words to describe physical systems is at best misleading , and at worst , pathological and harmful to science .However , it is perfectly clear and appropriate to state that a model of a physical system has a \" random \" or \" stochastic \" component .", "label": "", "metadata": {}, "score": "61.671383"}
{"text": "The third point of the talk was the difference between model - based vs model - free reinforcement learning .Model - based learning can use how the world ( state ) is organized and plan accordingly , while model - free learns values associated with each state .", "label": "", "metadata": {}, "score": "61.88411"}
{"text": "Nevertheless , for any particular model assignment of activity patterns to collective states , one could ask how well those collective modes capture the information about the stimuli , and use that as a direct measure of model performance .We believe this to be a promising avenue for future research .", "label": "", "metadata": {}, "score": "61.957134"}
{"text": "The probability of coincidences , analogous to Figure 14 , computed for the DG model ( red ) and compared to data ( black ) ; gray line is the independent model .Maximum entropy models for the checkerboard stimulation .We stimulated a separate retina with a checkerboard stimulus .", "label": "", "metadata": {}, "score": "62.01509"}
{"text": "Conclusions .A major source of that confusion has been the longstanding frequentist notion that the probabilities and information that scientists have characterized are properties of the physical systems that scientists observe , when in fact these probabilities describe the information of the scientists themselves about those systems .", "label": "", "metadata": {}, "score": "62.019527"}
{"text": "Throughout , red shows the data , grey the independent model , and black the pairwise model .Maximum entropy models that match the mean spike rate and pairwise correlations in a network make an unambiguous , quantitative prediction for , with no adjustable parameters .", "label": "", "metadata": {}, "score": "62.034508"}
{"text": "Building a precise model of activity patterns required us to match the statistics of global activity ( the probability that K out of N neurons spike in the same small window of time ) .Several recent works suggested alternative means of capturing the higher - order correlations [ 12 ] , [ 98 ] - [ 103 ] .", "label": "", "metadata": {}, "score": "62.073853"}
{"text": "By this way , we can represent HOS as a polynomial function of second - order statistics to improve the anti - noise performance and accuracy .In addition , the proposed method can work well for short time series .Abstract : The existence of noise has great influence on the real features of observed time series , thus noise reduction in time series data is a necessary and significant task in many practical applications .", "label": "", "metadata": {}, "score": "62.08031"}
{"text": "In a uniform distribution , if we pick two states at random then the probability that these states are the same is given by .On the hypothesis of uniformity , this probability is sufficiently small that large groups of neurons should never visit the same state twice during the course of a one hour experiment .", "label": "", "metadata": {}, "score": "62.10767"}
{"text": "We see that , throughout the range of fields that are well sampled in the experiment , there is good agreement between the data and Eq ( 26 ) .As we go into the tails of the distribution , we see some deviations , but error bars also are ( much ) larger .", "label": "", "metadata": {}, "score": "62.115955"}
{"text": "To solve any particular problem , the brain must have information about some relevant aspect of the world ( with a corresponding state space ) .If the brain is then asked to verbally specify a probability , the relevant aspect of the world ( and its state space ) now corresponds to the abstract notion of \" probabilities \" themselves , and the problem facing the brain has radically changed .", "label": "", "metadata": {}, "score": "62.12113"}
{"text": "Analysis results of both several different synthetic series and typical observed time series data have verified the performance of the new method .A comprehensive discussion of the results indicates that compared with traditional wavelet de - noising methods , the new proposed method is more effective and universal .", "label": "", "metadata": {}, "score": "62.126137"}
{"text": "Why Is a Strict Jaynesian View Critical to Neuroscience ?The advantages of Jaynesian probability theory have been well documented ( Section 3 ) .But in many applications , these advantages may be seen as rather philosophical and technical matters that have only modest influence on the actual probabilities .", "label": "", "metadata": {}, "score": "62.135597"}
{"text": "Lawless JF ( 2003 ) Statistical models and methods for lifetime data , 2nd edn .Wiley , London MATH .Le Besnerais G , Bercher J - F , Demoment G ( 1999 )A new look at entropy for solving linear inverse problems .", "label": "", "metadata": {}, "score": "62.215508"}
{"text": "Random Structures & Algorithms , 19(3 - 4):163 - 193 , 2001 .Bayesian estimation of discrete entropy with mixtures of stick - breaking priors .In P. Bartlett , F. Pereira , C. Burges , L. Bottou , and K. Weinberger , editors , Advances in Neural Information Processing Systems 25 , pages 2024 - 2032 .", "label": "", "metadata": {}, "score": "62.30457"}
{"text": "Within the field of \" Bayesian brain theory \" , \" Bayesian \" has indeed been used to denote any use of BT .In fact , BT has often been used with frequentist rather than Jaynesian probabilities ( see Section 4 ) .", "label": "", "metadata": {}, "score": "62.306587"}
{"text": "( A ) Independent entropy per neuron , , in black , and the entropy of the K - pairwise models per neuron , , in red , as a function of N .Dashed lines are fits from ( B ) .", "label": "", "metadata": {}, "score": "62.328648"}
{"text": "CDMentropy ] . A. Chao and T. Shen .Nonparametric estimation of Shannon 's index of diversity when there are unseen species in sample .Environmental and Ecological Statistics , 10(4):429 - 443 , 2003 .[ CAE ] .P. Grassberger .", "label": "", "metadata": {}, "score": "62.35274"}
{"text": "Monte Carlo experiments are also used to provide evidence on the power and size of test statistics .An empirical application is included to demonstrate the practical implementation of the estimator .Abstract : We use the principle of maximum entropy to propose a parsimonious model for the generation of simulated rainfall during the wettest three - month season at a typical location on the east coast of Australia .", "label": "", "metadata": {}, "score": "62.684387"}
{"text": "Replica - averaged simulations have a substantial track record and have in many cases been shown to improve the quality of structural ensembles - e.g . , measured through cross - validation with unrelated experimental data - and to provide new biological insights .", "label": "", "metadata": {}, "score": "62.68564"}
{"text": "Replica - averaged simulations have a substantial track record and have in many cases been shown to improve the quality of structural ensembles - e.g . , measured through cross - validation with unrelated experimental data - and to provide new biological insights .", "label": "", "metadata": {}, "score": "62.68564"}
{"text": "As in Figure 7 , three - point correlations are binned ; shown are the means for the predictions in a given bin , error - bars are omitted for clarity .DG underperforms the K - pairwise model specifically for negative correlations .", "label": "", "metadata": {}, "score": "62.702873"}
{"text": "NSB ] .L. Paninski .Estimation of entropy and mutual information .Neural Computation , 15:1191 - 1253 , 2003 .[BUB ] .S. Panzeri and A. Treves .Analytical estimates of limited sampling biases in different information measures .", "label": "", "metadata": {}, "score": "62.721584"}
{"text": "Taken together , Figures 2 , 3 , and 4 suggest strongly that our data and algorithms are sufficient to construct maximum entropy models , reliably , for networks of more than one hundred neurons .Here the repeats have been reordered so that the training repeats precede testing repeats ; in fact , the choice of test repeats is random .", "label": "", "metadata": {}, "score": "62.73494"}
{"text": "Tka\u010dik G , Marre O , Mora T , Amodei D , Berry MJ II ( 2013 ) Bialek W ( 2013 )The simplest maximum entropy model for collective behavior in a neural network .J Stat Mech P03011 . doi : 10.1088/1742 - 5468/2013/03/p03011 .", "label": "", "metadata": {}, "score": "63.07819"}
{"text": "( It is noteworthy that Jaynes accomplished all of this without any use of BT [ 34 ] . )Jaynes demonstrated that a small and simple state of information is sufficient to derive probability distributions , and in some cases it can be extremely powerful in making accurate predictions about the world .", "label": "", "metadata": {}, "score": "63.080418"}
{"text": "The distinction between \" observer \" and \" object \" is merely one of perspective , and does not imply any fundamental distinction between the qualities of the two .Figure 1 .Are the information and probabilities used in neuroscience properties of the environment ( an observed object ) , the neural system under investigation , or the scientist ?", "label": "", "metadata": {}, "score": "63.092003"}
{"text": "One of the main practical issues is that one needs to numerically determine the optimal values for the Lagrange multiplier corresponding to each constraint .Since experimental data will easily provide hundreds of these constraints , this optimization is a formidable task .", "label": "", "metadata": {}, "score": "63.24357"}
{"text": "One of the main practical issues is that one needs to numerically determine the optimal values for the Lagrange multiplier corresponding to each constraint .Since experimental data will easily provide hundreds of these constraints , this optimization is a formidable task .", "label": "", "metadata": {}, "score": "63.24357"}
{"text": "Given the limited accuracy of force fields , macromolecular simulations sometimes produce results that are at not in complete and quantitative accordance with experiments .A common solution to this problem is to explicitly ensure agreement between the two by perturbing the potential energy function towards the experimental data .", "label": "", "metadata": {}, "score": "63.276028"}
{"text": "Given the limited accuracy of force fields , macromolecular simulations sometimes produce results that are at not in complete and quantitative accordance with experiments .A common solution to this problem is to explicitly ensure agreement between the two by perturbing the potential energy function towards the experimental data .", "label": "", "metadata": {}, "score": "63.276028"}
{"text": "Empirical evidence has shown that given the same information , there are many cases in which frequentist methods make less accurate predictions than Jaynesian methods [ 35 ] .Thus frequentist methods do not consistently make optimal ( logical ) use of information .", "label": "", "metadata": {}, "score": "63.298332"}
{"text": "See Figure S5 for an analogous plot for the pairwise model .Error bars are s.d .over 30 subnetworks at a given size N .An interesting effect is shown in Figure 7B , where we look at the average absolute deviation between predicted and measured , as a function of the group size N .", "label": "", "metadata": {}, "score": "63.341026"}
{"text": "doi : 10.1007/s10858 - 007 - 9150 - 1 .Olsson S , Boomsma W , Frellsen J , Bottaro S , Harder T , et al .( 2011 ) Generative probabilistic models extend the scope of inferential structure determination .", "label": "", "metadata": {}, "score": "63.35903"}
{"text": "doi : 10.1007/s10858 - 007 - 9150 - 1 .Olsson S , Boomsma W , Frellsen J , Bottaro S , Harder T , et al .( 2011 ) Generative probabilistic models extend the scope of inferential structure determination .", "label": "", "metadata": {}, "score": "63.35903"}
{"text": "Even someone who sees merit in both definitions could question the validity of such a hybrid .What is a \" Random Process \" ?The frequentist perspective and its terminology have a powerful influence on how we view natural processes , including those in the brain .", "label": "", "metadata": {}, "score": "63.380375"}
{"text": "Frequentist methods are narrow in their objective and do not address inference in general .Frequentist methods do not incorporate logic in a formal sense , and thus can not help in understanding its neural basis .The most severe fault is the misattribution of knowledge .", "label": "", "metadata": {}, "score": "63.39398"}
{"text": "Structure learning .In the simplest case , a Bayesian network is specified by an expert and is then used to perform inference .In other applications the task of defining the network is too complex for humans .In this case the network structure and the parameters of the local distributions must be learned from data .", "label": "", "metadata": {}, "score": "63.406105"}
{"text": "These hidden states act \" behind the scenes \" to determine the observed output of the system for any given input .The apparent randomness of the system is merely the product of the ignorance of the scientist .According to present scientific dogma , it is only at the very small scale of quantum physics that physical systems are \" truly random \" .", "label": "", "metadata": {}, "score": "63.483215"}
{"text": "Fiorillo , C.D. A neurocentric approach to bayesian inference .Nat .Rev. Neurosci .[ Google Scholar ] .Friston , K. Is the free - energy principle neurocentric ?Nat .Rev. Neurosci .[ Google Scholar ] .MacKay , D.J.C. Information Theory , Inference , and Learning Algorithms ; Cambridge University Press : Cambridge , UK , 2003 .", "label": "", "metadata": {}, "score": "63.532494"}
{"text": "This is , in general , a hard problem .We need not only to solve this problem , but also to convince ourselves that our solution is meaningful , and that it does not reflect overfitting to the limited set of data at our disposal .", "label": "", "metadata": {}, "score": "63.558575"}
{"text": "The physical system could correspond to a neural system or anything else .Here I argue in favor of the view of Jaynes ( and Laplace , shown at left ) that probabilities are always conditional on the information of an observer about an object .", "label": "", "metadata": {}, "score": "63.639515"}
{"text": "Cooke R ( 1991 )Experts in uncertainty : opinion and subjective probability in science .Oxford University Press , New York .Cover TM , Thomas JA ( 1991 ) Elements of information theory .Wiley , New York MATH CrossRef .", "label": "", "metadata": {}, "score": "63.977753"}
{"text": "Although efficient algorithms exist for improving molecular force fields based on experimental data [ 8 ] , a common approach is to introduce a system - specific modification to the energy function , and thereby modify the structural ensemble to become in agreement with the experimental data .", "label": "", "metadata": {}, "score": "64.062256"}
{"text": "Although efficient algorithms exist for improving molecular force fields based on experimental data [ 8 ] , a common approach is to introduce a system - specific modification to the energy function , and thereby modify the structural ensemble to become in agreement with the experimental data .", "label": "", "metadata": {}, "score": "64.062256"}
{"text": "Firing rates and pairwise correlations focus on the properties of particular neurons .As an alternative , we can consider quantities that refer to the network as a whole , independent of the identity of the individual neurons .A simple example is the \" distribution of synchrony \" ( also called \" population firing rate \" ) , that is , the probability that K out of the N neurons spike in the same small slice of time .", "label": "", "metadata": {}, "score": "64.15589"}
{"text": "P. Valiant and G. Valiant .Estimating the Unseen : Improved Estimators for Entropy and other Properties .In Advances in Neural Information Processing Systems 26 , pp .2157 - 2165 , 2013 .[ UNSEEN ] .V. Q. Vu , B. Yu , and R. E. Kass .", "label": "", "metadata": {}, "score": "64.16681"}
{"text": "Constructing partial prior specifications for models of complex physical systems .The Statistician 47 : 37 - 53 .Forrester Y ( 2005 )The quality of expert judgement : an interdisciplinary investigation .Ph.D. thesis , University of Maryland .Friedlander MP , Gupta MR ( 2006 )", "label": "", "metadata": {}, "score": "64.243286"}
{"text": "Since our goal is to address policy questions , we emphasize the model 's ability to reproduce characteristics of the existing production system and predict outcomes of policy changes at a disaggregate level .Measurement of distributional impacts of policy changes requires use of farm - level models estimated across a wide spectrum of sizes and types , which is often difficult with traditional econometric methods due to data limitations .", "label": "", "metadata": {}, "score": "64.499054"}
{"text": "Predictions seem of more variable quality for cells with lower average spike rate , but this is a small effect .Our ability to predict the state of individual neurons by reference to the network , but not the visual input , means that the representation of the sensory input in this population is substantially redundant .", "label": "", "metadata": {}, "score": "64.58924"}
{"text": "The relevancy , hierarchical and contextual constraints are extracted from a set of training scenes and utilized to generate plausible synthetic scenes that in turn satisfy these constraints .After applying the proposed framework , scenes that are plausible representations of the training examples are automatically generated .", "label": "", "metadata": {}, "score": "64.62076"}
{"text": "This is evident in the work of Rieke and colleagues [ 33 ] , which represents perhaps the most comprehensive effort to date to relate information theory to neural function in a quantitative sense .In using BT to characterize the information in a neuron 's spike output , purportedly from \" the neuron 's point of view , \" they assume that the prior distribution is equivalent to the frequency distribution of the neuron 's input .", "label": "", "metadata": {}, "score": "64.64386"}
{"text": "These advancements would strengthen the utility of Maxent for wildlife research and management .Abstract : The high pay packages of U.S. CEOs have raised serious concerns about what would constitute a fair pay .Since the present economic models do not adequately address this fundamental question , we propose a new theory based on statistical mechanics and information theory .", "label": "", "metadata": {}, "score": "64.67368"}
{"text": "This is an extension of two papers , [ 10 ] and [ 4 ] , which proposed the estimation of parameters where only spatial constraints were taken into account .The extension we propose allows one to properly handle memory effects in spike statistics , for large - sized neural networks .", "label": "", "metadata": {}, "score": "64.69136"}
{"text": "The models predict the probability of occurrence for all possible combinations of spiking and silence in the network , and it seems natural to use this huge predictive power to test the models .In small networks , this is a useful approach .", "label": "", "metadata": {}, "score": "64.71504"}
{"text": "Coincidences and surprises .Usually we expect that , as the number of elements N in a system becomes large , the entropy becomes proportional to N and the distribution becomes nearly uniform over states .This is the concept of \" typicality \" in information theory [ 74 ] and the \" equivalence of ensembles \" in statistical physics [ 75 ] , [ 76 ] .", "label": "", "metadata": {}, "score": "64.72797"}
{"text": "Abstract : There are two entropy - based methods to deal with linear inverse problems , which we shall call the ordinary method of maximum entropy ( OME ) and the method of maximum entropy in the mean ( MEM ) .", "label": "", "metadata": {}, "score": "64.85234"}
{"text": "Harold Gutch told us about the Borel - Kolmogorov paradox .What is the conditional distribution on a great circle when points are uniformly distributed on the surface of a sphere ?One argument says it should be uniform by symmetry .", "label": "", "metadata": {}, "score": "64.859375"}
{"text": "If we imagine monitoring a Hopfield network over a long time , the distribution of states that it visits will be dominated by the local minima of the energy function .Thus , even if we ca n't take the details of the dynamical model seriously , it still should be true that the energy landscape determines the probability distribution over states in a Boltzmann - like fashion , with multiple energy minima translating into multiple peaks of the distribution .", "label": "", "metadata": {}, "score": "64.92562"}
{"text": "The claim that a network of neurons exhibits collective behavior is really the claim that the distribution of states taken on by the network has some nontrivial structure that can not be factorized into contributions from individual cells or perhaps even smaller subnetworks .", "label": "", "metadata": {}, "score": "64.92781"}
{"text": "Philosophical problems associated with uninformative priors are associated with the choice of an appropriate metric , or measurement scale .Suppose we want a prior for the running speed of a runner who is unknown to us .We could specify , say , a normal distribution as the prior for his speed , but alternatively we could specify a normal prior for the time he takes to complete 100 metres , which is proportional to the reciprocal of the first prior .", "label": "", "metadata": {}, "score": "65.00547"}
{"text": "Bethge M & Berens P ( 2008 ) Near - maximum entropy models for binary neural representations of natural images .In : Platt J et al . eds .Adv Neural Info Proc Sys 20 : : 97 - 104 .", "label": "", "metadata": {}, "score": "65.17766"}
{"text": "It thus is possible that these real networks exceed the \" capacity \" of model networks [ 2 ] , [ 3 ] .Every recorded pattern is assigned to its basin of attraction by descending on the energy landscape .The number of distinct basins is shown as a function of the network size , N , for K - pairwise models ( black line ) .", "label": "", "metadata": {}, "score": "65.23518"}
{"text": "P .X .X .n . )i .n .P .X .i . parents .X .i . )If node X i has no parents , its local probability distribution is said to be unconditional , otherwise it is conditional .", "label": "", "metadata": {}, "score": "65.25238"}
{"text": "There were several noise correlation ( joint variability in the population activity ) related talks : .The \" sign rule \" says that if the signal correlation is opposite of the noise correlation , linear Fisher information ( and OLE performance ) is improved ( see Fig 1 , Averbeck , Latham , Pouget 2006 ) .", "label": "", "metadata": {}, "score": "65.300125"}
{"text": "In particular , the predicted PSTH has near zero spike probability over most of the time , the short epochs of spiking are at the correct moments , and these epochs have the sharp onsets observed experimentally .These are features of the data which are very difficult to reproduce in models that , for example , start by linearly filtering the visual stimulus through a receptive field [ 78 ] - [ 82 ] .", "label": "", "metadata": {}, "score": "65.36453"}
{"text": "Information Theory , IEEE Transactions on , 35(3):669 - 675 , 1989 .J. Hausser and K. Strimmer .Entropy inference and the James - Stein estimator , with application to nonlinear gene association networks .The Journal of Machine Learning Research , 10:1469 - 1484 , 2009 .", "label": "", "metadata": {}, "score": "65.41118"}
{"text": "Error bars are s.d .over 30 subnetworks at every N .Note the logarithmic scale for the number of MS states .Figure 11A provides a more detailed view of the most prominent metastable states , and the \" energy valleys \" that surround them .", "label": "", "metadata": {}, "score": "65.43535"}
{"text": "One approach to explore this problem is first to use synthetic data that have themselves been generated from simulations and compare the restrained ensemble with the ensemble used to generate the data [ 37 ] , [ 38 ] .With real - world experimental data , a suitable value of can be determined by cross - validating with independent data not used in the structure determination [ 39 ] .", "label": "", "metadata": {}, "score": "65.541336"}
{"text": "One approach to explore this problem is first to use synthetic data that have themselves been generated from simulations and compare the restrained ensemble with the ensemble used to generate the data [ 37 ] , [ 38 ] .With real - world experimental data , a suitable value of can be determined by cross - validating with independent data not used in the structure determination [ 39 ] .", "label": "", "metadata": {}, "score": "65.541336"}
{"text": "The Computational Goal of the Nervous System .The concepts of probability and information are of fundamental importance to the computational goal of the nervous system .Since at least as far back as von Helmholz ( 1896 ) [ 4 ] , it has been recognized that inferring the state of the world is critical to brain function ( or equivalently , \" estimating \" or \" predicting \" ) .", "label": "", "metadata": {}, "score": "65.5533"}
{"text": "The terms \" prior \" and \" posterior \" are generally relative to a specific datum or observation .An uninformative prior expresses vague or general information about a variable .The term \" uninformative prior \" is a misnomer ; such a prior might be called a not very informative prior .", "label": "", "metadata": {}, "score": "65.69839"}
{"text": "Thus , we envisage that in future applications it might be possible to integrate out not only experimental noise and \" nuisance parameters , \" but potentially also the uncertainty associated with the parameterization of a force field .We note that distributed computing platforms may be particularly well suited to sample from such models as one might need to perform multiple , independent simulations that differ only slightly in the force field used .", "label": "", "metadata": {}, "score": "65.72637"}
{"text": "Thus , we envisage that in future applications it might be possible to integrate out not only experimental noise and \" nuisance parameters , \" but potentially also the uncertainty associated with the parameterization of a force field .We note that distributed computing platforms may be particularly well suited to sample from such models as one might need to perform multiple , independent simulations that differ only slightly in the force field used .", "label": "", "metadata": {}, "score": "65.72637"}
{"text": "The General Method of Jaynes .The general method of Jaynesian probability theory can be summarized as a sequence of steps that apply to the formal derivation and manipulation of probabilities ( Figure 2 ) .Here it is described in four steps , only the first two of which are universal and essential to any application .", "label": "", "metadata": {}, "score": "65.73035"}
{"text": "Scientists ideally try to work from a common , shared body of knowledge , and to thereby describe nature from the common perspective of a unified Science .To the extent that two rational scientists share the same information , they will naturally agree on the probabilities .", "label": "", "metadata": {}, "score": "65.82166"}
{"text": "[ Google Scholar ] .London , M. ; Roth , A. ; Beergen , L. ; Hausser , M. ; Latham , P.E. Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex .Nature 2010 , 466 , 123 - 127 .", "label": "", "metadata": {}, "score": "65.8816"}
{"text": "On training data , we computed the constrained statistics ( mean firing rates , covariances , and the K - spike distribution ) , and used bootstrapping to estimate the error bars on each of these quantities ; the constraints were the only input to the learning algorithm .", "label": "", "metadata": {}, "score": "65.91121"}
{"text": "As the system responds to its inputs , it visits each of these states with some probability .Even before we ask what the different states mean , for example as codewords in a representation of the sensory world , specifying this distribution requires us to determine the probability of each of possible states .", "label": "", "metadata": {}, "score": "66.09854"}
{"text": "If a model based on pairwise correlations does n't quite account for the data , it is tempting to try and include correlations among triplets of neurons .An alternative is to use itself as a constraint on our models , as explained above in relation to Eq ( 17 ) .", "label": "", "metadata": {}, "score": "66.19401"}
{"text": "To understand how a single molecule could \" perform inference \" , it is not essential to understand proteins or biophysics or even Boltzmann 's distribution .The simple point is that at each moment in time , a molecular sensor \" observes \" a single \" sample \" of some external quantity ( external to the neuron ) , such as neurotransmitter concentration .", "label": "", "metadata": {}, "score": "66.21012"}
{"text": "S .R .d .o .G .T . )P .S .R . )P .R . ) obtained by removing the factor .P .G .S .R . ) from the pre - intervention distribution .", "label": "", "metadata": {}, "score": "66.27623"}
{"text": "Abstract : In this paper we present a simple model to describe a rather general system in a stationary non - equilibrium state , which is an open system traversed by a stationary flux .The probabilistic description is provided by a non - homogeneous Markov chain , which is not assumed on the basis of a model of the microscopic interactions but rather derived from the knowledge of the macroscopic fluxes traversing the system through a maximum entropy rate principle .", "label": "", "metadata": {}, "score": "66.32593"}
{"text": "The corresponding values for the predictions are grouped together , yielding the mean and the s.d .of the prediction ( y - axis ) .Inset shows a zoom - in of the central region , for the K - pairwise model .", "label": "", "metadata": {}, "score": "66.36682"}
{"text": "The simple characterization of Jaynesian probabilities as \" subjective \" can be misleading .As expressions of logic , Jaynesian probabilities are objective properties of information without any ambiguity .Two observers with the same information will apply the same probabilities , and two observers with different information will apply different probabilities .", "label": "", "metadata": {}, "score": "66.40236"}
{"text": "This gave me ideas to enhance Eleksius !This paper connects the Bayesian least squares ( MMSE ) estimation and MAP estimation under Gaussian likelihood .Their theorem shows that MMSE estimate with some prior is also a MAP estimate under some other prior ( or equivalently , a regularized least squares ) .", "label": "", "metadata": {}, "score": "66.44082"}
{"text": "For example , Edwin T. Jaynes has published an argument ( Jaynes 1968 ) based on Lie groups that suggests that the prior for the proportion of voters voting for a candidate , given no other information , should be .Priors can be constructed which are proportional to the Haar measure if the parameter space carries a natural group structure .", "label": "", "metadata": {}, "score": "66.442215"}
{"text": "Cambridge University Press , London MATH .Kadane JB , Wolfson J ( 1998 )A experiences in elicitation .The Statistician 47 : 3 - 19 .Kaminskiy MP , Krivtsov VV ( 2005 )A simple procedure for Bayesian estimation of the Weibull distribution .", "label": "", "metadata": {}, "score": "66.67249"}
{"text": "It is the ability of Jaynesian probabilities to objectively describe subjective information that makes them so useful in understanding brain function .A second criticism of Jaynesian probabilities is that it is not always clear how they should be calculated .Although this is undoubtedly true , it is not a valid criticism of Jaynesian methods .", "label": "", "metadata": {}, "score": "66.76906"}
{"text": "Minimally structured models are attractive , both because of the connection to statistical mechanics and because they represent the absence of modeling assumptions about data beyond the choice of experimental constraints .Of course , these features do not guarantee that such models will provide an accurate description of a real system .", "label": "", "metadata": {}, "score": "66.83082"}
{"text": "J Comput Graph Stat 7 : 267 - 277 CrossRef .Oakley JE , O'Hagan A ( 2007 ) Uncertainty in prior elicitations : a nonparametric approach .Biometrika 94 : 427 - 441 MATH CrossRef MathSciNet .O'Hagan A ( 1998 )", "label": "", "metadata": {}, "score": "66.859406"}
{"text": "Proceedings MEA Meeting .BIOPRO Baden - W\u00fcrttemberg , 2008 .pp .197 .Dudik M , Phillips SJ & Schapire RE ( 2004 )Performance guarantees for regularized maximum entropy density estimation .Proceedings 17th Annual conference on learning theory .", "label": "", "metadata": {}, "score": "66.86667"}
{"text": "As noted above , this is the opposite of what we usually do in building models or theories - rather than trying to impose some hypothesized structure on the world , we are trying to remove all structures that are not explicitly contained within the chosen set of experimental constraints .", "label": "", "metadata": {}, "score": "66.885376"}
{"text": "Regardless of the interpretations of quantum physics , there should not be any controversy within neuroscience , where according to modern physics , no system of interest is \" truly random \" .The inability of neuroscientists to predict the state of a neural system should be entirely attributed to the ignorance of neuroscientists .", "label": "", "metadata": {}, "score": "67.039825"}
{"text": "Miller DJ , Yan L ( 2000 )Approximate maximum entropy joint feature inference consistent with arbitrary lower order probability constraints : application to statistical classification .Neural Comput 12 : 2175 - 2208 CrossRef .Natarajan R , McCulloch CE ( 1998 )", "label": "", "metadata": {}, "score": "67.18381"}
{"text": "[PYMentropy ] .Bayesian Entropy Estimation for Countable Discrete Distributions .arXiv:1302.0328 , 2013 .[PYMentropy ] . E. Archer , I. M. Park , and J. Pillow .Bayesian entropy estimation for binary spike train data using parametric prior knowledge .", "label": "", "metadata": {}, "score": "67.32463"}
{"text": "But although the brain needs to know about aspects of the real world , it is not at all clear that it needs to \" know about the probabilities \" of aspects of the real world .Information about \" X \" is not the same as information about the probability of \" X \" , the latter relating to the probability of the probability of \" X \" ( Section 3.6 ) .", "label": "", "metadata": {}, "score": "67.33124"}
{"text": "According to contemporary physics , none of these systems are \" random \" , \" stochastic \" , etc .These terms purport to describe a physical system , but they only indicate our ignorance of the system ; ( b ) an example of a typical model of an ion channel .", "label": "", "metadata": {}, "score": "67.36064"}
{"text": "Indeed , we can not even adequately understand the function of a single ion channel without consideration of its many hidden internal states ( Figure 3 b ) .Since the rejection of Skinner 's xenocentric approach some decades ago , psychologists have adopted a neurocentric approach ( \" cognitivism \" ) , treating a person or even an animal as an observer .", "label": "", "metadata": {}, "score": "67.40882"}
{"text": "Introduction .Physicists have long hoped that the functional behavior of large , highly interconnected neural networks could be described by statistical mechanics [ 1 ] - [ 3 ] .The goal of this effort has been not to simulate the details of particular networks , but to understand how interesting functions can emerge , collectively , from large populations of neurons .", "label": "", "metadata": {}, "score": "67.43193"}
{"text": "Thus the general function of the nervous system can be described in quantitative terms as the minimization of uncertainty ( or more specifically , the minimization of uncertainty about biologically relevant aspects of the world ) , which is essentially the same as maximization of information .", "label": "", "metadata": {}, "score": "67.519066"}
{"text": "Exploring the energy landscape .To find the metastable ( MS ) states , we start with a pattern that appears in the data , and attempt to flip spins from their current state into , in order of increasing i. A flip is retained if the energy of the new configuration is smaller than before the flip .", "label": "", "metadata": {}, "score": "67.57226"}
{"text": "Whether this approximation in practice proves more efficient than finding local - optima in the full restraint Lagrange problem remains to be seen .One direction that is worth pursuing further in this respect is to develop a replica analogy to the approach , alleviating the need for the numerical determination of the Lagrange multiplier .", "label": "", "metadata": {}, "score": "67.67728"}
{"text": "Whether this approximation in practice proves more efficient than finding local - optima in the full restraint Lagrange problem remains to be seen .One direction that is worth pursuing further in this respect is to develop a replica analogy to the approach , alleviating the need for the numerical determination of the Lagrange multiplier .", "label": "", "metadata": {}, "score": "67.67728"}
{"text": "IEEE Trans Inform Theory 52 : 238 - 245 CrossRef MathSciNet .Dai Y - S , Xie M , Long Q , Ng S - H ( 2007 ) Uncertainty analysis in software reliability modeling by bayesian analysis with maximum - entropy principle .", "label": "", "metadata": {}, "score": "67.7695"}
{"text": "Nat .Neurosci .[ Google Scholar ] [ CrossRef ] .Beck , J.M. ; Ma , W.J. ; Kiani , R. ; Hanks , T. ; Churchland , A.K. ; Roitman , J. ; Shadlen , M.N. ; Latham , P.E. ; Pouget , A. Probabilistic population codes for Bayesian decision making .", "label": "", "metadata": {}, "score": "67.792725"}
{"text": "Federico told me that there were no sharp peaks in the cross - correlation .He further extrapolated the choice probability to the network level based on multivariate Gaussian approximation , and a simplification to categorize neurons into two classes ( transient or sustained response ) .", "label": "", "metadata": {}, "score": "67.81643"}
{"text": "Venegaz - Martinez F ( 2004 )On information measures and prior distributions : a synthesis .Morfismos 8 : 27 - 50 .Zellner A ( 1977 ) Maximal data information prior distributions .In : Aykae A , Brumat C ( eds ) New developments in the applications of Bayesian methods , Amsterdam .", "label": "", "metadata": {}, "score": "68.04553"}
{"text": "Recent studies indicate Maxent is relatively insensitive to spatial errors associated with location data , requires few locations to construct useful models , and performs better than other presence - only modeling approaches .Further advances are needed to better define model thresholds , to test model significance , and to address model selection .", "label": "", "metadata": {}, "score": "68.127625"}
{"text": "For R , some of these estimators are implemented in a package called entropy ( in CRAN ; written by the authors of JS estimator ) .There 's also a python package called pyentropy .Targeting a more neuroscience specific audience , Spike Train Analysis Toolkit contains a few of estimators implemented in MATLAB / C. References . A. Antos and I. Kontoyiannis .", "label": "", "metadata": {}, "score": "68.183945"}
{"text": "Combining Experiments and Simulations Using the Maximum Entropy Principle .PLoS Comput Biol 10(2 ) : e1003406 .doi:10.1371/journal.pcbi.1003406 .Editor : Michael Levitt , Stanford University , United States of America .Published : February 20 , 2014 .", "label": "", "metadata": {}, "score": "68.2248"}
{"text": "Combining Experiments and Simulations Using the Maximum Entropy Principle .PLoS Comput Biol 10(2 ) : e1003406 .doi:10.1371/journal.pcbi.1003406 .Editor : Michael Levitt , Stanford University , United States of America .Published : February 20 , 2014 .", "label": "", "metadata": {}, "score": "68.2248"}
{"text": "Decades of research have gone into the development and fine - tuning of these force fields , and they have proven useful in a multitude of applications [ 7 ] .Figure 1 Despite their success , it is , however , still a common scenario that the results obtained through simulations do not quantitatively match those obtained from experiments .", "label": "", "metadata": {}, "score": "68.24628"}
{"text": "Decades of research have gone into the development and fine - tuning of these force fields , and they have proven useful in a multitude of applications [ 7 ] .Figure 1 Despite their success , it is , however , still a common scenario that the results obtained through simulations do not quantitatively match those obtained from experiments .", "label": "", "metadata": {}, "score": "68.24628"}
{"text": "[ Google Scholar ] [ CrossRef ] .Corlett , P. ; Gancsos , M.E. ; Fiorillo , C.D. The Bayesian Self and Its Disruption in Psychosis .In Phenomenological Neuropsychiatry : How Patient Experience Bridges Clinic with Clinical Neuroscience ; Mishara , A. , Corlett , P. , Fletcher , P. , Schwartz , M. , Eds . ; Springer - Verlag : Berlin / Heidelberg , Germany , 2012 ; in press .", "label": "", "metadata": {}, "score": "68.293884"}
{"text": "Phillips , W.A. Self - organized complexity and Coherent Infomax from the viewpoint of Jaynes 's probability theory .Information 2012 , 3 , 1 - 15 .[ Google Scholar ] [ CrossRef ] .Kay , J. ; Phillips , W.A. Coherent Infomax as a computational goal for neural systems .", "label": "", "metadata": {}, "score": "68.32817"}
{"text": "The problem is not merely that frequency distributions may not be available as a practical matter , but rather that in many cases no relevant frequency distribution could ever exist .A more glaring illustration of this fault , with respect to neuroscience , is that a frequentist view has no means to address the plain fact that different brains have different information and therefore place different probabilities on the same event .", "label": "", "metadata": {}, "score": "68.41819"}
{"text": "These errors are largely corrected in the K - pairwise model , despite the fact that adding a constraint on does n't add any information about the identity of the neurons in the different triplets .Fixing the distribution of global activity thus seems to capture something about the network that individual spike probabilities and pairwise correlations have missed .", "label": "", "metadata": {}, "score": "68.45238"}
{"text": "Common input explains higher - order correlations and entropy in a simple model of neural population activity .Phys Rev Lett 106 : 1 - 4 . doi : 10.1103/physrevlett.106.208102 .Santos GS , Gireesh ED , Plenz D ( 2010 ) Nakahara H ( 2010 )", "label": "", "metadata": {}, "score": "68.46338"}
{"text": "Proc Natl Acad Sci ( USA ) 109 : 4786 - 4791 .doi : 10.1073/pnas.1118633109 .Roudi Y , Nirenberg S ( 2009 ) Latham PE ( 2009 )Pairwise maximum entropy models for studying large biological systems : when they can work and when they ca n't .", "label": "", "metadata": {}, "score": "68.4773"}
{"text": "How can we interpret the meaning of the K - spike constraint and its biological relevance ?One possibility would be to view it as a global modulatory effect of , e.g. , inhibitory interneurons with dense connectivity .Alternatively , might be an important feature of the neural code for downstream neurons .", "label": "", "metadata": {}, "score": "68.502"}
{"text": "Wiley , London MATH .Skilling J ( 1989 ) Maximum entropy and Bayesian methods .Kluwer , Dordrecht MATH .Smith , CR , Erickson , G , Neudorfer , PO ( eds ) ( 1992 ) Maximum entropy and Bayesian methods ( fundamental theories of physics ) .", "label": "", "metadata": {}, "score": "68.55562"}
{"text": "Figure 3 .Illustration of neural systems that are sometimes said to be \" random , stochastic , noisy , or probabilistic . \"( a ) Fixed and known inputs to these systems result in variable outputs .According to contemporary physics , none of these systems are \" random \" , \" stochastic \" , etc .", "label": "", "metadata": {}, "score": "68.57289"}
{"text": "Thus the input does not fully determine the output .Cases 1 and 2 , on a more microscopic scale , are routinely referred to as \" stochastic \" or \" random \" processes , case 3 at the \" cellular level \" is sometimes referred to in this manner .", "label": "", "metadata": {}, "score": "68.60309"}
{"text": "The Relation of bayesian and maximum entropy methods ( 510Kb ) .In : Erickson GJ , Smith CR(eds ) Maximum - entropy and Bayesian methods in science and engineering , vol 1 .Kluwer , Dordrecht , pp 25 - 29 .", "label": "", "metadata": {}, "score": "68.613495"}
{"text": "This is a tremendous virtue , since different brains have different information .Two Jaynesian Approaches : First - person ( Neurocentric ) versus Third - person ( Xenocentric ) .In studying a physical system , a scientist is an observer , and the physical system is the object of inference .", "label": "", "metadata": {}, "score": "68.848206"}
{"text": "Konrad started off the workshop by posting some philosophical questions about how big data might change the way we do science .He argued that neuroscience is rife with theories ( for instance , how uncertainty is ... .Shannon 's entropy is the fundamental building block of information theory - a theory of communication , compression , and randomness .", "label": "", "metadata": {}, "score": "68.85007"}
{"text": "[ Google Scholar ] .Fiorillo , C.D. Towards a general theory of neural computation based on prediction by single neurons .PLoS One 2008 , 3 , e3298 .[ Google Scholar ] [ CrossRef ] .Friston , K. The free energy principle : A unified brain theory ?", "label": "", "metadata": {}, "score": "68.8654"}
{"text": "As we have here hinted , the problem of uncertainties in the data appears to be related to the problem of determining the relative weight between force field and restraint - potential .A relevant question in this context is whether such a weight can be meaningfully defined and assigned without considering the inherent accuracy of the force field itself .", "label": "", "metadata": {}, "score": "68.87388"}
{"text": "As we have here hinted , the problem of uncertainties in the data appears to be related to the problem of determining the relative weight between force field and restraint - potential .A relevant question in this context is whether such a weight can be meaningfully defined and assigned without considering the inherent accuracy of the force field itself .", "label": "", "metadata": {}, "score": "68.87388"}
{"text": "To our knowledge , this method has not yet been applied to molecular simulation , and the practical applicability of the approach therefore remains to be established .Finally , we note that an alternative approach has very recently been suggested to derive structural ensembles from noisy , ensemble - averaged experimental data [ 51 ] .", "label": "", "metadata": {}, "score": "68.98378"}
{"text": "To our knowledge , this method has not yet been applied to molecular simulation , and the practical applicability of the approach therefore remains to be established .Finally , we note that an alternative approach has very recently been suggested to derive structural ensembles from noisy , ensemble - averaged experimental data [ 51 ] .", "label": "", "metadata": {}, "score": "68.98378"}
{"text": "The primary purpose in doing so is not to characterize information and inference in exquisite , quantitative detail , but to be as clear and precise as possible about what it means to perform inference and how the biophysics of the brain could achieve this goal .", "label": "", "metadata": {}, "score": "69.09025"}
{"text": "doi : 10.1103/physrevlett.110.058104 .Hopfield JJ ( 2008 )Searching for memories , sudoku , implicit check bits , and the iterative use of not - always - correct rapid neural computation .Neural Comp 20 : 1119 - 1164 .", "label": "", "metadata": {}, "score": "69.11469"}
{"text": ": It has been proposed that the general function of the brain is inference , which corresponds quantitatively to the minimization of uncertainty ( or the maximization of information ) .However , there has been a lack of clarity about exactly what this means .", "label": "", "metadata": {}, "score": "69.142166"}
{"text": "There is a further stipulation that is particularly critical for understanding the brain and the physical basis of information and inference .In addition to precisely stating the information , we must also specify where we believe that information to be , and if possible , its putative physical basis . \" Where \" must at least specify whether it is possessed by a scientist , a neural system under investigation , or whether it is in the environment external to the neural system ( Figure 1 ) .", "label": "", "metadata": {}, "score": "69.19533"}
{"text": "We should not confuse ignorance with lack of reason .We know for a fact that the brain does not rationally integrate all of its information at all times .However , logical integration of smaller amounts of information , perhaps at the level of single neurons , is certainly conceivable .", "label": "", "metadata": {}, "score": "69.239044"}
{"text": "Unfortunately , Lancaster interaction measure is incorrect for 4 + variables , and the correct version becomes very complicated very quickly .This paper presents a meta - active - learning problem where active learning is used to find the best policy to teach a system ( e.g. , human ) .", "label": "", "metadata": {}, "score": "69.23956"}
{"text": "The adhesive layer between the skin and core is modeled using linear springs , the rigidities of which are reduced in debonded sectors .The algorithm is validated using experimental data of an aluminum honeycomb panel under different damage scenarios .Abstract : We study the problem of finding probability densities that match given European call option prices .", "label": "", "metadata": {}, "score": "69.29778"}
{"text": "I believe that one virtue of Jaynesian theory in contemporary neuroscience is to counter what I see as an excessive emphasis on BT ( Equation 3 ) .Whereas the mainstream view seems to be that to \" be Bayesian \" is synonymous with the \" performance \" of BT , BT is not an absolutely essential aspect of probability theory ( Section 3 ) .", "label": "", "metadata": {}, "score": "69.38001"}
{"text": "Howson , C. ; Urbach , P. Bayesian reasoning in science .Nature 1991 , 350 , 371 - 374 .[ Google Scholar ] [ CrossRef ] .Barlow , H.B. Possible Principles Underlying the Transformation of Sensory Messages .In Sensory Communication ; Rosenblith , W.A. , Ed . ; MIT Press : Cambridge , MA , USA , 1961 ; pp .", "label": "", "metadata": {}, "score": "69.45685"}
{"text": "Suppose a seismic wavelet can be modeled by a formula with three free parameters ( scale , frequency and phase ) .We can transform the estimation of the wavelet into determining these three parameters .The phase of the wavelet is estimated by constant - phase rotation to the seismic signal , while the other two parameters are obtained by the Higher - order Statistics ( HOS ) ( fourth - order cumulant ) matching method .", "label": "", "metadata": {}, "score": "69.51476"}
{"text": "As noted above , we can think of this potential as providing a global regulation of the network activity , such as might be implemented by inhibitory interneurons with ( near ) global connectivity .All of the tests given in the previous section can be redone in this case , and again we find that we can learn the K - pairwise models from the available data with no signs of overfitting .", "label": "", "metadata": {}, "score": "69.53737"}
{"text": "Sanger , T.D. Probability density estimation for the interpretation of neural population codes .J. Neurophysiol .[ Google Scholar ] .Knill , D.C. ; Pouget , A. The Bayesian brain : The role of uncertainty in neural coding and computation .", "label": "", "metadata": {}, "score": "69.53812"}
{"text": "J Am Stat Assoc 90 : 598 - 604 MATH CrossRef MathSciNet .Goldstein M ( 2006 ) Subjective Bayesian analysis : principles and practice .Bayesian Anal 1 : 403 - 420 CrossRef MathSciNet .Goossens LHJ , Cooke RM ( 2006 ) Expert judgement - calibration and combination .", "label": "", "metadata": {}, "score": "69.56622"}
{"text": "The \" basin size \" of a given MS state is the number of patterns in the recorded data from which the given MS state is reached by descending on the energy landscape .The results presented in Figure 11 are typical of the transitions we observe across multiple subnetworks of 120 neurons .", "label": "", "metadata": {}, "score": "69.743996"}
{"text": "The pairwise correlations between neurons in this system are quite weak .Thus , if we make a model for the activity of just two neurons , treating them as independent is a very good approximation .It might seem that this statement is invariant to the number of neurons that we consider - either correlations are weak , or they are strong .", "label": "", "metadata": {}, "score": "69.85695"}
{"text": "I have proposed that the only fundamental problem in making decisions is uncertainty ( lack of information ) about the world [ 5 ] .Introspection tells each of us that if we knew everything about the world , and thus we had no uncertainty , then we would always know exactly what to do and we could choose the best possible motor outputs .", "label": "", "metadata": {}, "score": "69.99333"}
{"text": "Simulating replicas .One intuitive strategy to overcome this problem is to simultaneously simulate several replicas of the system and apply restraints on the average of the back - calculated experimental values , rather than on the individual structures [ 25 ] .", "label": "", "metadata": {}, "score": "70.141594"}
{"text": "Simulating replicas .One intuitive strategy to overcome this problem is to simultaneously simulate several replicas of the system and apply restraints on the average of the back - calculated experimental values , rather than on the individual structures [ 25 ] .", "label": "", "metadata": {}, "score": "70.141594"}
{"text": "The estimates show considerable distributional differences resulting from policies that change water subsidies in the region or shift price supports to direct payments .Abstract : This paper addresses the problem of balancing statistical economic data , when data structure is arbitrary and both uncertainty estimates and a ranking of data quality are available .", "label": "", "metadata": {}, "score": "70.19551"}
{"text": "Wiley , Chichester MATH CrossRef .Press SJ ( 2003 ) Subjective and objective Bayesian statistics , 2nd edn .Wiley , New York MATH .Robert CP ( 2001 )The Bayesian choice .a decision - theoretic motivation , 2nd edn .", "label": "", "metadata": {}, "score": "70.259384"}
{"text": "These are all adjectives that are properly used to describe an observed system , not the observer ( \" noisy \" is sometimes used in this manner as well ) .Thus the strong implication of this language is that these are intrinsic properties of these physical systems , rather than merely a description of our own ignorance and consequent inability to make accurate predictions .", "label": "", "metadata": {}, "score": "70.26538"}
{"text": "Some will choose a conjugate prior when they can , to make calculation of the posterior distribution easier .An informative prior expresses specific , definite information about a variable .An example is a prior distribution for the temperature at noon tomorrow .", "label": "", "metadata": {}, "score": "70.27844"}
{"text": "As the stimulus movie proceeds , all of the cells in the network are spiking , dynamically , so that the state of the system varies .Through the conditional distribution , this varying state predicts a varying spike probability for the one cell in the network on which we are focusing , and we can plot this predicted probability vs. time in the same way that we would plot a conventional PSTH .", "label": "", "metadata": {}, "score": "70.290115"}
{"text": "Further , the energy function is built out of terms that are related to the experimental observables that we are trying to reproduce .Thus , for example , if we try to reproduce the correlations among spiking in pairs of neurons , the energy function will have terms describing effective interactions among pairs of neurons .", "label": "", "metadata": {}, "score": "70.40389"}
{"text": "Experiment results prove that Hierarchical Geometry Verification based on Maximum Entropy Saliency can not only improve retrieval accuracy , but also reduce the time consumption of the full retrieval .Abstract : Drug discovery applies multidisciplinary approaches either experimentally , computationally or both ways to identify lead compounds to treat various diseases .", "label": "", "metadata": {}, "score": "70.433655"}
{"text": "Figure 8 shows that these cumulative distributions computed from the model agree with experiment far into the tail of low probability states .These states are so rare that , individually , they almost never occur , but there are so many of these rare states that , in aggregate , they make a measurable contribution to the distribution of energies .", "label": "", "metadata": {}, "score": "70.581406"}
{"text": "Shannon CE ( 1948 )A mathematical theory of communication .Bell Syst Technol J 27:379 - 423 , 623 - 656 .Shulman N , Feder M ( 2004 )The uniform distribution as a universal prior .IEEE Trans Inform Theory 50 : 1356 - 1362 CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "70.60037"}
{"text": "There has been a rapidly growing view that Bayesian principles of reason and inference may provide us with a general computational description of brain function ( e.g. , [ 5 , 6 ] ) .There have also been efforts to relate Bayesian principles to neuronal mechanisms , although these have generally been quite speculative ( e.g. , [ 5 , 27 , 28 , 29 , 30 , 31 , 32 ] ) .", "label": "", "metadata": {}, "score": "70.63683"}
{"text": "Every cell in the body is full of such molecular sensors , and this certainly does not in itself provide clear insight into the structure of the nervous system .In addition , the goal is not to minimize uncertainty in general , but specifically to minimize uncertainty about biologically relevant aspects of the world .", "label": "", "metadata": {}, "score": "70.749146"}
{"text": "If correlations are weak , there is a simple relationship between the correlations and the corresponding interactions [ 34 ] , [ 113 ] .We conclude that treating correlations as a small perturbation is inconsistent with the data .Supporting Information .", "label": "", "metadata": {}, "score": "70.759964"}
{"text": "This induces the group structure of the translation group on , and the resulting prior is a constant improper prior .Similarly , some measurements are naturally invariant to the choice of an arbitrary scale ( i.e. , it does n't matter if we use centimeters or inches , we should get results that are physically the same ) .", "label": "", "metadata": {}, "score": "70.85887"}
{"text": "In particular , the typical coupling does not decline as , as would be expected in conventional spin glass models [ 70 ] .This implies , as emphasized previously [ 9 ] , that the \" thermodynamic limit \" ( very large N ) for these systems will be different from what we might expect based on traditional physics examples .", "label": "", "metadata": {}, "score": "70.98377"}
{"text": "BT does not specify any transformation of information , it only expresses the same information in alternate forms .The alternate view favored here is that to function well the nervous system needs to have the right information in the right place at the right time , but that it has no need to perform any of the mathematics of probability theory [ 5 , 61 ] .", "label": "", "metadata": {}, "score": "70.99837"}
{"text": "Information theory is widely applied to neuroscience and sometimes to machine learning .Jonathan sympathized with Shannon 's note ( 1956 ) called \" the bandwagon \" , criticized the possible abuse / overselling of information theory .First , Jonathan focused on the derivation of a \" universal \" rate - distortion theory based on the \" information bottleneck principle \" .", "label": "", "metadata": {}, "score": "71.02692"}
{"text": "Author Summary .Sensory neurons encode information about the world into sequences of spiking and silence .Multi - electrode array recordings have enabled us to move from single units to measuring the responses of many neurons simultaneously , and thus to ask questions about how populations of neurons as a whole represent their input signals .", "label": "", "metadata": {}, "score": "71.126785"}
{"text": "Abstract : In this paper we present a stochastic route choice model for transit networks that explicitly addresses route correlation due to overlapping alternatives .The model is based on a multi - objective mathematical programming problem , the optimality conditions of which generate an extension to the Multinomial Logit models .", "label": "", "metadata": {}, "score": "71.30252"}
{"text": "An unconventional adaptive simulated annealing technique , called funnel diffusion , determines expansion coefficients for Chebyshev polynomials in the exponential function .Abstract : We review here the difference between quantum statistical treatments and semiclassical ones , using as the main concomitant tool a semiclassical , shift - invariant Fisher information measure built up with Husimi distributions .", "label": "", "metadata": {}, "score": "71.3799"}
{"text": "The often subjective nature of the input information .The reliance on Bayes 's conditioning as the basis for updating information .The distinction between causal and evidential modes of reasoning , which underscores Thomas Bayes 's paper of 1763 .Formally , Bayesian networks are directed acyclic graphs whose nodes represent variables , and whose arcs encode conditional independencies between the variables .", "label": "", "metadata": {}, "score": "71.476974"}
{"text": "The parametrization of the K - pairwise Hamiltonian of Eq ( 20 ) is degenerate , that is , there are multiple sets of coupling constants that specify mathematically identical models .This is because adjusting all local fields by a constant offset adds a term linear in K to ; similarly , adjusting all pairwise couplings by a constant offset adds a quadratic term to .", "label": "", "metadata": {}, "score": "71.48153"}
{"text": "Noninformative priors do not exist : a discussion .J Stat Plan Inference 65 : 159 - 189 CrossRef MathSciNet .Billy F , Bousquet N , Celeux G ( 2006 )Modelling and eliciting expert knowledge with fictitious data .In : Proceedings of the workshop on the use of expert judgement for decision - making , CEA Cadarache .", "label": "", "metadata": {}, "score": "71.4951"}
{"text": "To the best of my knowledge , this \" physical inference \" has never been described in a precise quantitative manner for any physical system , either in physics or neuroscience .Although Jaynes was a physicist , he did little to directly address the physical basis of information or logic .", "label": "", "metadata": {}, "score": "71.51039"}
{"text": "This prediction is in agreement with observed data for the bottom 90%-95 % of the working population .The theory estimates that the top 35 U.S. CEOs were overpaid by about 129 times their ideal salaries in 2008 .We also provide an insight of entropy as a measure of fairness , which is maximized at equilibrium , in an economic system .", "label": "", "metadata": {}, "score": "71.751526"}
{"text": "The talks offered both philosophical perspectives and methodological aspects , reflecting diverse viewpoints and approaches to high - dimensional neural data .Many of the discussions continued the next day in our sister workshop .Here we summarize each talk : .", "label": "", "metadata": {}, "score": "71.865326"}
{"text": "Redundancy and predictability .In the retina we usually think of neurons as responding to the visual stimulus , and so it is natural to summarize their response as spike rate vs. time in a ( repeated ) movie , the post - stimulus time histogram ( PSTH ) .", "label": "", "metadata": {}, "score": "72.00682"}
{"text": "Attribution .A Very Brief History of Probability Theory .Thomas Bayes was the first to explicitly and formally introduce the concept that probabilities can be conditional on information in what is now known as Bayes 's Theorem ( BT ) ( he died in 1761 , and his theorem was published posthumously ) .", "label": "", "metadata": {}, "score": "72.079834"}
{"text": "Measuring spike - field coherence and spike train synchrony .He emphasized on using nonparametric statistics for testing circular variable of interest : the phase of LFP oscillation conditioned on spike timings .In the second part , he talked about spike - distance ( see Kreuz 2012 ) which is a smooth , time scale invariant measure of instantaneous synchrony among spike trains .", "label": "", "metadata": {}, "score": "72.08858"}
{"text": "The probability of the same state repeating is many orders of magnitude larger than expected for independent neurons , and this really is quite startling ( Figure 14 ) .Thus , some combination of spiking and silence across this huge population should repeat exactly every few seconds .", "label": "", "metadata": {}, "score": "72.1839"}
{"text": "For each activity pattern in recorded data we computed the effective field , and binned these values ( shown on x - axis ) .For every bin we estimated from data the probability that the N -th neuron spiked ( black circles ; error bars are s.d .", "label": "", "metadata": {}, "score": "72.18782"}
{"text": "Despite its molecular complexity , most systems level neural models describe it as a scalar valued strength .Biophysical evidence suggests discrete states within the synapse and discrete levels of synaptic strength , which is troublesome because memory will be quickly overwritten for discrete / binary - valued synapses .", "label": "", "metadata": {}, "score": "72.24448"}
{"text": "The Jaynesian definition of probability has been advanced by many authors over the years ( e.g. , [ 38 ] ) , but the account given here is based upon the posthumous textbook of Jaynes from 2003 [ 35 ] .According to Jaynes , probability theory is a natural extension of earlier work on logic .", "label": "", "metadata": {}, "score": "72.25932"}
{"text": "One set of ideas is that the activity of the network as a whole may be confined to some very low dimensional trajectory , such as a global , coherent oscillation .Such oscillatory activity is observable in the summed electrical activity of large numbers of neurons - the EEG - and should be reflected as oscillations in the ( auto-)correlation functions of spike trains from individual neurons .", "label": "", "metadata": {}, "score": "72.261894"}
{"text": "[ Google Scholar ] .Oaksford , M. ; Chater , N. Precis of bayesian rationality : The probabilistic approach to human reasoning .Behav .Brain Sci .[ Google Scholar ] [ CrossRef ] .Daunizeau , J. ; Den Ouden , H.E.M. ; Pessiglione , M. ; Stephan , K.E. ; Friston , K.J. Observing the observer ( I ) : meta - Bayesian models of learning and decision making .", "label": "", "metadata": {}, "score": "72.30766"}
{"text": "The necessity of physical integration tremendously simplifies the problem , because a physical observer does not possess a great diversity of information at any one time ( though of course a nervous system consists of many such observers ) .We can presume that the knowledge of an observer about the external world corresponds to \" knowledge of self \" .", "label": "", "metadata": {}, "score": "72.321724"}
{"text": "They are not restricted to representing random variables , which represents another \" Bayesian \" aspect of a Bayesian network .Efficient algorithms exist that perform inference and learning in Bayesian networks .Bayesian networks that model sequences of variables ( such as for example speech signals or protein sequences ) are called dynamic Bayesian networks .", "label": "", "metadata": {}, "score": "72.33354"}
{"text": "A paradox is a puzzling contradiction ; using some sort of reasoning one derives two contradicting conclusions .Consistency is an essential quality of a reasoning system , that is , it should not be able to produce contradictions by itself .", "label": "", "metadata": {}, "score": "72.34163"}
{"text": "Similarly , this variability has been declared to be \" noise \" which could only increase a brain 's subjective uncertainty about the \" signal \" ( e.g. , [ 54 , 55 ] ) .But if this variability is not inherently random , and is instead determined by the information of ion channels ( as described by Hodgkin - Huxley models ) about the external stimulus , then it is far from clear why it should have any correspondence to subjective uncertainty .", "label": "", "metadata": {}, "score": "72.45972"}
{"text": "The same metastable states and corresponding valleys are identifiable from different subsets of the full population , providing a measure of redundancy that we explore more fully below .( B )The probability ( across stimulus repeats ) that the population is in a particular basin of attraction at any given time .", "label": "", "metadata": {}, "score": "72.55825"}
{"text": "Centered Dirichlet mixture ( CDM ) is a Bayesian estimator with a special prior designed for binary observations .It comes in two flavors depending if your observation is close to independent ( DBer ) or the total number of 1 's is a good summary statistic ( DSyn ) .", "label": "", "metadata": {}, "score": "72.60446"}
{"text": "Although Claude Shannon gave this definition in 1948 [ 1 ] , his work is still not widely known and understood in biology and medicine .By contrast , his \" information theory \" is nearly universally known and accepted by engineers , physicists and mathematicians .", "label": "", "metadata": {}, "score": "72.66136"}
{"text": "It sometimes matters whether we use the left - invariant or right - invariant Haar measure .For example , the left and right invariant Haar measures on the affine group are not equal .Berger ( 1985 , p. 413 ) argues that the right - invariant Haar measure is the correct choice .", "label": "", "metadata": {}, "score": "72.843185"}
{"text": "329 - 334 , August 15 - 17 , 1985 .Pearl , Judea .Fusion , propagation , and structuring in belief networks .Artificial Intelligence 29 ( 3):241 - 288 , 1986 .David Heckerman , A Tutorial on Learning with Bayesian Networks .", "label": "", "metadata": {}, "score": "72.88016"}
{"text": "Trans .R. Soc .B 2008 , 363 , 3801 - 3811 .[ Google Scholar ] [ CrossRef ] .Yu , A.J. ; Dayan , P. Uncertainty , neuromodulation , and attention .Neuron 2006 , 46 , 681 - 692 .", "label": "", "metadata": {}, "score": "72.91041"}
{"text": "References .Shannon , C.E. A mathematical theory of communication .Bell Syst .Tech .J. 1948 , 27 , 379 - 423 .[ Google Scholar ] .Kandel , E.R. ; Schwartz , J.H. ; Jessel , T.M. Principles of Neural Science , 4th ed ; McGraw - Hill : New York , NY , USA , 2000 .", "label": "", "metadata": {}, "score": "72.94948"}
{"text": "The model is a GQM with quadratic term , if the spike - triggered and non - spike - triggered distributions are Gaussian with covariances and .When both distributions are allowed to be mixture - of - Gaussians , then it turns out the nonlinear function becomes a soft - max of quadratic terms making it an LNLN model .", "label": "", "metadata": {}, "score": "72.951065"}
{"text": "Jaynes , E.T. Probability in Quantum Theory .[ Google Scholar ] .Hille , B. Ionic Channels of Excitable Membranes,3rd ed . ; Sinauer Associates Inc. : Sunderland , MA , USA , 2001 .[ Google Scholar ] .Shadlen , M.N. ; Newsome , W.T. The variable discharge of cortical neurons : Implications for connectivity , computation , and information coding .", "label": "", "metadata": {}, "score": "72.97446"}
{"text": "These notes are primarily for my future reference .There are three main categories of energy cost in the brain : ( 1 ) maintenance , ( 2 ) spike generation , and ( 3 ) synapse .Assuming a finite energy budget for the brain , the optimal efficient coding strategy can vary from small number of neurons with high rate to large population with sparse coding [ see Fig 3 , Laughlin 2001 ] .", "label": "", "metadata": {}, "score": "72.97605"}
{"text": "Thus the inputs and output of a neuron that performs BT would have the same information content .This contradicts the common and well supported notion that by transforming inputs to output , a neuron transforms information about one object ( \" stimulus \" or \" cause \" , as determined by its receptive field ) into information about another object .", "label": "", "metadata": {}, "score": "73.088295"}
{"text": "MIT Press , Cambridge , MA , 2002 .[NSB ] .I. Nemenman , W. Bialek , and R. Van Steveninck .Entropy and information in neural spike trains : Progress on the sampling problem .Physical Review E , 69(5):056111 , 2004 .", "label": "", "metadata": {}, "score": "73.098885"}
{"text": "Word by word , determines how surprised the brain should be by each particular pattern of response , including the possibility that the response was corrupted by noise in the retinal circuit and thus should be corrected or ignored [ 54 ] .", "label": "", "metadata": {}, "score": "73.117615"}
{"text": "Jaynes ET ( 1957 )Information theory and statistical mechanics .Phys Rev 106 , 620 - 630 , and 108 , 171 - 190 .Jaynes ET ( 1982 )On the rationale of maximum entropy methods .Proc IEEE 70 : 939 - 952 CrossRef .", "label": "", "metadata": {}, "score": "73.17905"}
{"text": "This example illustrates common features of neural responses to naturalistic sensory inputs - long epochs of near zero spike probability , interrupted by brief transients containing a small number of spikes [ 77 ] .Can our models predict this behavior , despite the fact that they make no explicit reference to the visual input ?", "label": "", "metadata": {}, "score": "73.25668"}
{"text": "Using the first - passage time , they ordered states , and derived an upper bound .Area is bounded by where M and N denote number of internal states per synapse and synapses , respectively .Therefore , less synapses with more internal state is better for longer memory .", "label": "", "metadata": {}, "score": "73.29263"}
{"text": "Niemeier , M. ; Crawford , J.D. ; Tweed , D. Optimal transsaccadic integration explains distorted spatial perception .Nature 2003 , 422 , 76 - 80 .[ Google Scholar ] .Singh , K. ; Scott , S.H. A motor learning strategy reflects neural circuitry for limb control .", "label": "", "metadata": {}, "score": "73.323326"}
{"text": "If Jaynesian logic is viewed as prescriptive , then this would naturally call for experimental tests to verify how well , if at all , the brain follows Jaynesian principles .The results may be expected to vary on a case by case basis , with the brain being rational in some respects but not in others .", "label": "", "metadata": {}, "score": "73.32924"}
{"text": "This is compared with a parameter - free prediction ( red line ) from Eq ( 26 ) .For comparison , gray squares show the analogous analysis for the pairwise model ( error bars omitted for clarity , comparable to K - pairwise models ) .", "label": "", "metadata": {}, "score": "73.33534"}
{"text": "Predicted vs real distributions of energy , E , for the pairwise model .The cumulative distribution of energies , from Eq ( 22 ) , for the patterns generated by the pairwise models ( red ) and the data ( black ) , in a population of 120 neurons .", "label": "", "metadata": {}, "score": "73.44882"}
{"text": "Top , in red , experimental data .Lower traces , in black , predictions based on states of other neurons in an N -cell group , as described in the text .Solid lines are the mean prediction across all trials , and thin lines are the envelope \u00b1 one standard deviation .", "label": "", "metadata": {}, "score": "73.523026"}
{"text": "Treves A , Panzeri S , Rolls ET , Booth M ( 1999 ) Wakeman EA ( 1999 ) Firing rate distributions and efficiency of information transmission in inferior temporal cortex neurons to natural visual stimuli .Neural Comput 11 : 601 - 631 .", "label": "", "metadata": {}, "score": "73.88266"}
{"text": "Laplace , P.S. Essai Philosophique sur les Probabiliti\u00e9s ; Courier Imprimeur : Paris , France , 1819 .[ Google Scholar ] .Feller , W. An Introduction to Probability Theory and its Applications ; Wiley : New York , NY , USA , 1950 .", "label": "", "metadata": {}, "score": "73.88464"}
{"text": "We now have to ask what we can learn about neural function from this description .Basins of attraction .In the Hopfield model , dynamics of the neural network corresponds to motion on an energy surface .Simple learning rules can sculpt the energy surface to generate multiple local minima , or attractors , into which the system can settle .", "label": "", "metadata": {}, "score": "73.93653"}
{"text": "Green line shows the equivalent mean behavior computed for the green empty circles in ( B ) .The maximum entropy models that we have constructed predict the distribution of states taken on by the network as a whole , .From this we can construct the conditional distribution , , which tells us the probability of spiking in one cell given the current state of all the other cells , and hence we have a prediction for the spike probability in one neuron at each moment in time .", "label": "", "metadata": {}, "score": "73.99446"}
{"text": "Berger JO , Bernardo JM ( 1992 )On the development of reference priors ( with discussion ) .In : Bernardo JM , Berger JO , Dawid AP , Smith AFM(eds ) Bayesian statistics , vol 4 .Oxford University Press , USA , pp 35 - 60 .", "label": "", "metadata": {}, "score": "74.11596"}
{"text": "It is obvious that the same system can not be \" random \" when observed with one technique and \" deterministic \" when observed with another technique .It is also clear that neuroscience would have suffered a tremendous setback if early pioneers of extracellular recording had concluded that action potential generation was substantially random , and that we should therefore be satisfied merely to characterize Poisson spike statistics without seeking greater knowledge of underlying mechanisms .", "label": "", "metadata": {}, "score": "74.209656"}
{"text": "The detailed structure of the energy landscape .10 MS patterns from ( A ) are shown in the energy ( y - axis ) vs log basin size ( x - axis ) diagram ( silent state at lower right corner ) .", "label": "", "metadata": {}, "score": "74.306946"}
{"text": "Cambridge University Press , London .Bacha M , Celeux G , Id\u00e9e E , Lannoy A , Vasseur D ( 1998 ) Estimation de mod\u00e8les de dur\u00e9es de vie fortement censur\u00e9es .Eyrolles .Beirlant J , Dudewicz E , Gyorfi L , van der Meulen E ( 1997 )", "label": "", "metadata": {}, "score": "74.33227"}
{"text": "Ganmor E , Segev R ( 2011 )Schniedman E ( 2011 )Sparse low - order interaction network underlies a highly correlated and learnable neural population code .Proc Natl Acad Sci ( USA ) 108 : 9679 -9684 . doi : 10.1073/pnas.1019641108 .", "label": "", "metadata": {}, "score": "74.35034"}
{"text": "The arrows indicate the typical direction of information flow .The distinction between \" observer \" and \" object \" is merely one of perspective , and does not imply any fundamental distinction between the qualities of the two .Information and Probability .", "label": "", "metadata": {}, "score": "74.372055"}
{"text": "Population rate dynamics and multi neuron firing patterns in sensory cortex .J Neurosci 32 : 17108 - 17119 .doi : 10.1523/jneurosci.1831 - 12.2012 .Rieke F , Warland D , de Ruyter van Steveninck RR & Bialek W ( 1997 )", "label": "", "metadata": {}, "score": "74.38991"}
{"text": "The Statistician 47(1 ) : 21 - 35 MathSciNet .O'Hagan A ( 2006 )Research in elicitation .In : Upadhyay SK , Singh U , Dey DK(eds )Bayesian statistics and its applications .Anamaya , New Delhi , pp 375 - 382 . O'Hagan A , Buck CE , Daneshkhah A , Eiser JR , Garthwaite PH , Jenkinson DJ , Oakley JE , Rakow T ( 2006 )", "label": "", "metadata": {}, "score": "74.42915"}
{"text": "Results .Can we learn the model ?We have applied the maximum entropy framework to the analysis of one large experimental data set on the responses of ganglion cells in the salamander retina to a repeated , naturalistic movie .In the present data set , we have selected 160 neurons that pass standard tests for the stability of spike waveforms , the lack of refractory period violations , and the stability of firing across the duration of the experiment ( see Methods and Ref [ 36 ] ) .", "label": "", "metadata": {}, "score": "74.455986"}
{"text": "Why should we study the neural vocabulary , , at all ?The idea of decomposing correlations dates back to a time when it was hoped that correlations among spikes could be used to map the synaptic connections between neurons [ 52 ] .", "label": "", "metadata": {}, "score": "74.56424"}
{"text": "Science 1995 , 269 , 1880 - 1882 .[ Google Scholar ] .Knill , D.C. ; Richards , R.W. Perception as Bayesian Inference ; Cambridge University Press : Cambridge , UK , 1996 .[ Google Scholar ] .Seidenberg , M.S. Language acquisition and use : Learning and applying probabilistic constraints .", "label": "", "metadata": {}, "score": "74.58682"}
{"text": "With this Perspectives article , we will highlight the approach in some detail , hopefully communicating the elegance of the procedure and encouraging further work in this direction .As a concrete example , we will focus our attention on a recent application in the field of structural biology , namely , the problem of conducting molecular simulations under restraints from experimental data .", "label": "", "metadata": {}, "score": "74.66997"}
{"text": "With this Perspectives article , we will highlight the approach in some detail , hopefully communicating the elegance of the procedure and encouraging further work in this direction .As a concrete example , we will focus our attention on a recent application in the field of structural biology , namely , the problem of conducting molecular simulations under restraints from experimental data .", "label": "", "metadata": {}, "score": "74.66997"}
{"text": "doi : 10.1016/j.jneumeth.2013.02.020 .Marre O , Boustani SE , Fregnac Y ( 2009 ) Destexhe A ( 2009 ) Prediction of spatio - temporal patterns of neural activity from pairwise correlations .Phys Rev Lett 102 : 138101 .", "label": "", "metadata": {}, "score": "74.76424"}
{"text": "Multi - information of the K - pairwise models is shown in dark red .Dashed red line is a best quadratic fit for dependence of on ; this can be rewritten as , where \u03b3 ( N ) ( shown in inset ) is the effective scaling of multi - information with system size N .", "label": "", "metadata": {}, "score": "74.824585"}
{"text": "Bayesian analysis for the poly - Weibull distribution .J Am Stat Assoc 88 : 1412 - 1418 MATH CrossRef MathSciNet .Berger JO , Sun D ( 1994 )Bayesian sequential reliability for Weibull and related distributions .Ann Inst Stat Math 46 : 221 - 249 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "75.0724"}
{"text": "Roudi Y , Trycha J ( 2009 )Hertz J ( 2009 )The Ising model for neural data : model quality and approximate methods for extracting functional connectivity .Phys Rev E 79 : 051915 .doi : 10.1103/physreve.79.051915 .", "label": "", "metadata": {}, "score": "75.32071"}
{"text": "17 ] ) .Jaynes ' die problem .A die has been tossed many times , and we are provided with the information that the average outcome was some value , rather than the 3.5 that one would expect from a fair die .", "label": "", "metadata": {}, "score": "75.39648"}
{"text": "17 ] ) .Jaynes ' die problem .A die has been tossed many times , and we are provided with the information that the average outcome was some value , rather than the 3.5 that one would expect from a fair die .", "label": "", "metadata": {}, "score": "75.39648"}
{"text": "Extracting information in time patterns and correlations with wavelets .Using Haar wavelet time bins as the feature space , he proposed scale free linear analysis of spike trains .In addition , he proposed discovering relevant temporal structure through a feature selection using mutual information .", "label": "", "metadata": {}, "score": "75.51418"}
{"text": "We analyze data from more than a hundred salamander retinal ganglion cells and characterize their collective response using maximum entropy models of statistical physics .Citation : Tka\u010dik G , Marre O , Amodei D , Schneidman E , Bialek W , Berry MJ II ( 2014 )", "label": "", "metadata": {}, "score": "75.54088"}
{"text": "all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .Special Issue \" Maximum Entropy \" .", "label": "", "metadata": {}, "score": "75.54886"}
{"text": "Our models are not precisely Hopfield models , nor are they instances of the standard ( more random ) spin glass models .To search for local minima of the energy landscape , we take every combination of spiking and silence observed in the data and move \" downhill \" on the function from Eq ( 20 ) ( see Methods : Exploring the energy landscape ) .", "label": "", "metadata": {}, "score": "75.61091"}
{"text": "F .F .If , on the other hand , we wish to answer an interventional question : \" What is the likelihood that it would rain , given that we wet the grass ? \" the answer would be governed by the post - intervention joint distribution function .", "label": "", "metadata": {}, "score": "75.73865"}
{"text": "doi : 10.1016/j.jmr.2011.08.039 .Figures .Abstract .A key component of computational biology is to compare the results of computer modelling with experimental measurements .Despite substantial progress in the models and algorithms used in many areas of computational biology , such comparisons sometimes reveal that the computations are not in quantitative agreement with experimental data .", "label": "", "metadata": {}, "score": "75.847626"}
{"text": "\u00a9 2012 by the authors ; licensee MDPI , Basel , Switzerland .Abstract .Priors elicited according to maximal entropy rules have been used for years in objective and subjective Bayesian analysis .However , when the prior knowledge remains fuzzy or dubious , they often suffer from impropriety which can make them uncomfortable to use .", "label": "", "metadata": {}, "score": "75.92476"}
{"text": "Figure 13A shows the entropy per neuron of the K - pairwise model as a function of network size , N .For comparison , we also plot the independent entropy , i.e. the entropy of the non - interacting maximum entropy model that matches the mean firing rate of every neuron defined in Eq ( 5 ) .", "label": "", "metadata": {}, "score": "75.976204"}
{"text": "Roelfsema PR ( 2000 )The effects of pairwise and higher - order correlations in the firing rate of a postsynaptic neuron .Neural Comput 12 : 153 - 179 .doi : 10.1162/089976600300015934 .Macke JH , Opper M ( 2011 )", "label": "", "metadata": {}, "score": "76.00496"}
{"text": "Distributions of high - dimensional network states as knowledge base for networks of spiking neurons in the brain ( workshop ) Wolfgang Maass .In a series of papers ( B\u00fcsing et al .2011 , Pecevski et al .2011 , Habenschuss et al .", "label": "", "metadata": {}, "score": "76.06568"}
{"text": "[ Google Scholar ] [ CrossRef ] .Fiorillo , C.D. A New Approach to the Information in Neural Systems .In Integral Biomathics : Tracing the Road to Reality ; Simeonov , P.L. , Smith , L.S. , Ehresmann , A.C. , Eds . ; Springer - Verlag : Berlin / Heidelberg , Germany , 2012 .", "label": "", "metadata": {}, "score": "76.279366"}
{"text": "doi : 10.1073/pnas.0906705106 .Bayesian network .This article or section includes a list of references or a list of external links , but its sources remain unclear because it lacks in - text citations .You can improve this article by introducing more precise citations .", "label": "", "metadata": {}, "score": "76.53752"}
{"text": "Whether in science or our personal lives , we must always observe from a particular perspective .In psychology and cognitive neuroscience , this same issue was once actively debated .B.F. Skinner was the most prominent advocate of a xenocentric approach to behavior ( \" behaviorism \" ) , in which the input - output relationship of an animal or human is studied as an object like any other physical system .", "label": "", "metadata": {}, "score": "76.580215"}
{"text": "The argument ( \" thirder \" position ) says that you are twice more likely to be awakened for the tail toss , hence the probability should be 1/3 .If a certain reward was assigned to making a correct guess , the thirder position seems to be correct probability to use as the guess , but do we necessarily have matching belief ?", "label": "", "metadata": {}, "score": "76.595634"}
{"text": "Finally , the constraint on the overall distribution of activity generates a term which we can interpret as resulting from the interaction between all the spins / neurons in the system and one other , hidden degree of freedom , such as an inhibitory interneuron .", "label": "", "metadata": {}, "score": "76.63823"}
{"text": "Pouget , A. ; Dayan , P. ; Zemel , R. Information processing with population codes .Nat .Rev. Neurosci .[ Google Scholar ] [ CrossRef ] .Deneve , S. ; Latham , P.E. ; Pouget , A. Efficient computation and cue integration with noisy population codes .", "label": "", "metadata": {}, "score": "76.72885"}
{"text": "Although complete silence repeats more frequently , a wide range of other states also recur , so that many different combinations of spikes and silence occur often enough that we ( or the brain ) can simply count them to estimate their probability .", "label": "", "metadata": {}, "score": "76.86166"}
{"text": "The global potential , V ( K ) , where K is the number of synchronous spikes .See Methods : Parametrization of the K - pairwise model for details .Since we did n't make explicit use of the triplet correlations in constructing the K - pairwise model , we can test the model by predicting these correlations .", "label": "", "metadata": {}, "score": "76.88187"}
{"text": "Capturing the intangible concept of information .J Am Stat Assoc 89 : 1243 - 1254 MATH CrossRef MathSciNet .Soofi ES ( 2000 ) Principal information theoretic approaches .J Am Stat Assoc 95 : 1349 - 1353 MATH CrossRef MathSciNet .", "label": "", "metadata": {}, "score": "76.89218"}
{"text": "Table 1 to illustrate the two definitions ( \" There are 4 possibilities .... \" ) .I then require them to vote on whether they favor the frequentist or Jaynesian definition .Across multiple classes of students , I consistently find that at least two of three students favor the frequentist definition .", "label": "", "metadata": {}, "score": "77.09389"}
{"text": "Introduction .Picture this scenario : you have spent years developing an elaborate model for a particular scientific phenomenon .Now , new experimental data have been measured for the same phenomenon , and the data disagree with your model .How do you proceed ?", "label": "", "metadata": {}, "score": "77.10515"}
{"text": "Introduction .Picture this scenario : you have spent years developing an elaborate model for a particular scientific phenomenon .Now , new experimental data have been measured for the same phenomenon , and the data disagree with your model .How do you proceed ?", "label": "", "metadata": {}, "score": "77.10515"}
{"text": "Burling FT , Weis WI , Flaherty KM , Brunger AT ( 1996 ) Direct observation of protein salvation and discrete disorder with experimental crystallographic phases .Science 271 : 72 - 77 .doi : 10.1126/science.271.5245.72 .J Biomol NMR 15 : 315 - 330 .", "label": "", "metadata": {}, "score": "77.28374"}
{"text": "Burling FT , Weis WI , Flaherty KM , Brunger AT ( 1996 ) Direct observation of protein salvation and discrete disorder with experimental crystallographic phases .Science 271 : 72 - 77 .doi : 10.1126/science.271.5245.72 .J Biomol NMR 15 : 315 - 330 .", "label": "", "metadata": {}, "score": "77.28374"}
{"text": "Thus we try to \" take the brain 's point of view \" , a challenge that has been approached through a variety of different methods [ 5 , 21 , 24 , 33 ] .In this endeavor , we are presented with three entities , the scientist , the neural system under investigation , and the environment external to the neural system ( Figure 1 ) .", "label": "", "metadata": {}, "score": "77.301834"}
{"text": "( C ) Dependence of CC on the population size N .Thin blue lines follow single neurons as predictions are based on increasing population sizes ; red line is the cell illustrated in ( A ) , and the line with error bars shows mean \u00b1 s.d .", "label": "", "metadata": {}, "score": "77.40549"}
{"text": "He illustrated the balance through various laws of diminishing return plots .( question marks are from the original slide ) .She proposed that the observation of high trial - to - trial variability in spike trains from single neurons is due to degeneracy in the population encoding .", "label": "", "metadata": {}, "score": "77.44342"}
{"text": "Spatio - temporal correlations and visual signalling in a complete neuronal population .Nature 454 : 995 - 999 .doi : 10.1038/nature07140 .Fairhall AL , Burlingame CA , Narasimhan R , Harris RA , Puchalla JL ( 2006 )", "label": "", "metadata": {}, "score": "77.49353"}
{"text": "Definitions and concepts .If there is an arc from node A to another node B , A is called a parent of B , and B is a child of A .The set of parent nodes of a node X i is denoted by parents ( X i ) .", "label": "", "metadata": {}, "score": "77.51299"}
{"text": "This is a question about overfitting : is it possible that the parameters are being finely tuned to match even the statistical errors in our data ?To test for overfitting ( Figure 4 ) , we exploit the fact that the stimuli consist of a short movie repeated many times .", "label": "", "metadata": {}, "score": "77.541824"}
{"text": "For small networks ( ) the bias is negligible , but estimation from samples significantly underestimates the entropy for larger networks .Red line shows the mean \u00b1 s.d .over 30 subnetworks at each size .( C )The NSB estimation of entropy from samples drawn from the model ( x - axis ) vs the samples from real experiment ( y - axis ) ; each dot is a subnetwork of a given size ( color as in ( A ) ) .", "label": "", "metadata": {}, "score": "77.56211"}
{"text": "While one could argue that the observed redundancy among neurons is less than expected from the structure of natural images or movies , none of what we have described here would happen if the retina truly \" decorrelated \" its inputs .", "label": "", "metadata": {}, "score": "77.962204"}
{"text": "This seems ridiculous - what are such rare states doing in our analysis , much less as part of the claim that theory and experiment are in quantitative agreement ?The model thus is predicting things far beyond what can be inferred directly from the frequencies with which common patterns are observed to occur in realistic experiments .", "label": "", "metadata": {}, "score": "78.10014"}
{"text": "Abstract .A key component of computational biology is to compare the results of computer modelling with experimental measurements .Despite substantial progress in the models and algorithms used in many areas of computational biology , such comparisons sometimes reveal that the computations are not in quantitative agreement with experimental data .", "label": "", "metadata": {}, "score": "78.13913"}
{"text": "As such , they may in particular benefit from improved force fields , and we expect that as force fields continue to improve it should become possible to study more complex systems with less experimental information .Importantly , a consistent theoretical framework should allow us to transition smoothly between traditional , mostly data - driven methods for structure determination and molecular simulations in the absence of any experimental data .", "label": "", "metadata": {}, "score": "78.18965"}
{"text": "As such , they may in particular benefit from improved force fields , and we expect that as force fields continue to improve it should become possible to study more complex systems with less experimental information .Importantly , a consistent theoretical framework should allow us to transition smoothly between traditional , mostly data - driven methods for structure determination and molecular simulations in the absence of any experimental data .", "label": "", "metadata": {}, "score": "78.18965"}
{"text": "They built a model that can predict ( average ) activity of V4 and IT neurons in response to objects .Current computer vision methods do not perform well under high variability induced by transformation , rotation , and etc , while IT neuron response seems to be quite invariant to them .", "label": "", "metadata": {}, "score": "78.25287"}
{"text": "The neurons are again sorted in the order of decreasing firing rates .( A ) Pairwise interactions , , and the comparison with the interactions of the pairwise model , ( B ) .( C ) Single - neuron fields , , and the comparison with the fields of the pairwise model , ( D ) .", "label": "", "metadata": {}, "score": "78.36207"}
{"text": "A Bayesian analysis of industrial lifetime data with Weibull distributions , HAL - INRIA research report RR-6025 .Bousquet N ( 2006b )Analyse bay\u00e9sienne de la dur\u00e9e de vie de composants industriels ( Elements of Bayesian analysis for the prediction of the lifetime of industrial components ) .", "label": "", "metadata": {}, "score": "78.36705"}
{"text": "Bousquet N , Celeux G ( 2006 )Bayesian agreement between prior and data .In : Proceedings of the ISBA congress , Benidorm , Spain .Celeux G , Marin J - M , Robert CP ( 2006 ) Iterated importance sampling in missing data problems .", "label": "", "metadata": {}, "score": "78.44553"}
{"text": "Inset : the average absolute difference between the true and perturbative coupling , normalized by the average true coupling .The turquoise line ( squares ) shows the same comparison in which the pairwise model parameters , , were calculated perturbatively .", "label": "", "metadata": {}, "score": "78.90796"}
{"text": "Thus , the term that emerges from constraining the mean spike probabilities of every neuron is analogous to a magnetic field being applied to each spin , where spin \" up \" ( ) marks a spike and spin \" down \" ( ) denotes silence .", "label": "", "metadata": {}, "score": "78.925224"}
{"text": "There are several possible resolutions , such as , Pinocchio can not say that statement , Pinocchio 's world is inconsistent ( and hence can not have physical reality attached to it ) , Pinocchio can not know the truth value , and so on .", "label": "", "metadata": {}, "score": "78.9391"}
{"text": "MIT Press , Cambridge , MA , 1999 .Also appears as Technical Report MSR - TR-95 - 06 , Microsoft Research , March , 1995 .An earlier version appears as Bayesian Networks for Data Mining , Data Mining and Knowledge Discovery , 1:79 - 119 , 1997 .", "label": "", "metadata": {}, "score": "79.09818"}
{"text": "D - separation is defined as follows : A path p is said to be d - separated ( or blocked ) by a set of nodes Z if and only if .OR .A set Z is said to d - separate x from y in a directed acyclic graph G if all paths from x to y in G are d - separated by Z. The 'd ' in d - separation stands for ' directional ' , since the behavior of a three node link on a path depends on the direction of the arrows in the link .", "label": "", "metadata": {}, "score": "79.10071"}
{"text": "While DG does well when compared with pairwise models on our data , it is significantly less successful than the full K - pairwise models that we have explored here .In particular , the DG predictions of three - neuron correlations are much less accurate than in our model , and the probability of coincidences is underestimated by an amount that grows with increasing N ( Figure S6 ) .", "label": "", "metadata": {}, "score": "79.12906"}
{"text": "Soofi ES ( 1992 )Information theory and Bayesian statistics .In : Berry DA , Chaloner KM , Geweke JK(eds )Bayesian analysis in statistics and econometrics in honor of Arnold Zellner .Wiley , New York , pp 179 - 189 .", "label": "", "metadata": {}, "score": "79.398346"}
{"text": "The distinction made here between an object and an observer is only one of perspective , or frame of reference , with no connotation of any particular physical distinction ( other than displacement of the two in space ) .We can distinguish two observers , or perspectives , that are critical within neuroscience .", "label": "", "metadata": {}, "score": "79.54524"}
{"text": "As new experimental data have been collected , new models have been made with more \" hidden states \" in order to better account for the experimental data .Unfortunately we do not currently have a technique that can \" see \" the states that are hidden from patch electrodes , but it is conceivable that one day these states may no longer be hidden from us .", "label": "", "metadata": {}, "score": "79.5854"}
{"text": "Perhaps it could inspire novel search strategies for odor tracking robots .Another possibility is to build neuromorphic chips that emulate artificial neurons using the same principle to encode temporal patterns into instantaneously accessible information .This could be a part of low - power sensory processing unit in a robot .", "label": "", "metadata": {}, "score": "79.60681"}
{"text": "Stephens GJ , Mora T , Tka\u010dik G ( 2013 ) Bialek W ( 2013 ) Statistical thermodynamics of natural images .Phys Rev Lett 110 : 018701 .doi : 10.1103/physrevlett.110.018701 .Churchland MM , Cunningham JP , Kaufman MT , Ryu SI ( 2010 ) Shenoy KV ( 2010 )", "label": "", "metadata": {}, "score": "79.62702"}
{"text": "I propose here that these difficulties can be attributed in large part to confusion about the nature of information .Scientists have often mistaken their own knowledge about physical systems for inherent properties of the systems themselves .One consequence of this confusion has been that the concept of information has been divorced from the biophysical substrates that constitute the nervous system .", "label": "", "metadata": {}, "score": "79.896904"}
{"text": "J Neurophysiol 96 : 2724 - 2738 . doi : 10.1152/jn.00995.2005 .Tka\u010dik G , Granot - Atedgi E , Segev R ( 2013 ) Schneidman E ( 2013 ) Retinal metric : a stimulus distance measure derived from population neural responses .", "label": "", "metadata": {}, "score": "79.90719"}
{"text": "Jaynesian probabilities are not simply a number that a person verbally reports based upon introspection .A person typically struggles to state the probability that one of their beliefs is true .This may be in part for the same reason that scientists and experts on probability theory struggle to rationally calculate probabilities in cases in which a great diversity of information is relevant .", "label": "", "metadata": {}, "score": "80.06444"}
{"text": "The previous sections assumed that the experimentally observed values were obtained with perfect accuracy .In any real - world scenario there will , however , be some level of noise or uncertainty associated with such experimental data .As an example , consider the case of the die in Box 1 : the experiment from which the averages are observed will always consist of a finite number of tosses , and the average is therefore only determined within some uncertainty .", "label": "", "metadata": {}, "score": "80.07764"}
{"text": "The previous sections assumed that the experimentally observed values were obtained with perfect accuracy .In any real - world scenario there will , however , be some level of noise or uncertainty associated with such experimental data .As an example , consider the case of the die in Box 1 : the experiment from which the averages are observed will always consist of a finite number of tosses , and the average is therefore only determined within some uncertainty .", "label": "", "metadata": {}, "score": "80.07764"}
{"text": "But Jaynesian methods have no problem incorporating information derived from frequency distributions .In those cases in which frequentist methods succeed , they give the same probabilities that could be derived from Jaynesian methods .At their best , frequentist methods are a special case of the more general Jaynesian methods .", "label": "", "metadata": {}, "score": "80.09975"}
{"text": "A broad range of physical models can be studied within this approach .We use one - dimensional classical spin systems to illustrate the theoretical ideas .The examples studied in this paper are : the Ising model , the Potts model and the Blume - Emery - Griffiths model .", "label": "", "metadata": {}, "score": "80.16734"}
{"text": "J. Neurosci .[ Google Scholar ] [ CrossRef ] .Howe , C.Q. ; Lotto , R.B. ; Purves , D. Comparison of Bayesian and empirical ranking approaches to visual perception .J. Theor .Biol .[ Google Scholar ] [ CrossRef ] .", "label": "", "metadata": {}, "score": "80.22003"}
{"text": "Normative models and identification of nonlinear neural representations ( workshop ) Matthias Bethge .In the first half of his talk , Matthias talked about probabilistic models of natural images ( Theis et al .2012 ) which I did n't understand very well .", "label": "", "metadata": {}, "score": "80.241844"}
{"text": "The most frequent transitions are decays to the silent state .Other frequent transitions ( and their probabilities ) shown using vertical arrows between respective states .Typical transition statistics ( for MS 3 decaying into the silent state ) shown in the inset : the distribution of spin - flip attempts needed , P ( L ) , and the distribution of energy barriers , , over 1000 observed transitions .", "label": "", "metadata": {}, "score": "80.26734"}
{"text": "Second , despite the explosion of knowledge about the mechanics of the nervous system that has taken place over the last 60 years , there has been relatively little progress towards understanding the information processing function of the nervous system .For example , there has not been a corresponding explosion in the development of artificial intelligence .", "label": "", "metadata": {}, "score": "80.369934"}
{"text": "Cambridge : MIT Press .Bazaraa MS , Sherali HD & Shetty CM ( 2005 )Nonlinear programming : Theory and algorithms .Hoboken NJ , USA : Wiley & Sons .Pillow JW , Shlens J , Paninski L , Sher A , Litke AM , Chichilnisky EJ ( 2008 )", "label": "", "metadata": {}, "score": "80.51599"}
{"text": "In studying \" high - level \" , \" cognitive \" systems that are presumed closer in function to the conscious experience of humans ( e.g. , prefrontal cortex ) , a neurocentric perspective is adopted .But in considering parts of the nervous system that are viewed as sensory or motor , or in considering any brain region in a \" lower \" animal , biologists have seldom taken a neurocentric approach .", "label": "", "metadata": {}, "score": "80.5364"}
{"text": "Richter B , Gsponer J , V\u00e1rnai P , Salvatella X , Vendruscolo M ( 2007 )The MUMO ( minimal underrestraining minimal over - restraining ) method for the determination of native state ensembles of proteins .J Biomol NMR 37 : 117 - 135 .", "label": "", "metadata": {}, "score": "80.574554"}
{"text": "Richter B , Gsponer J , V\u00e1rnai P , Salvatella X , Vendruscolo M ( 2007 )The MUMO ( minimal underrestraining minimal over - restraining ) method for the determination of native state ensembles of proteins .J Biomol NMR 37 : 117 - 135 .", "label": "", "metadata": {}, "score": "80.574554"}
{"text": "Introduction .It is almost universally agreed that the nervous system is specialized for processing information .But for most people , that statement would seem too vague to be meaningful .While everyone has some intuitive notion of the meaning of \" information \" for most people , including neuroscientists , the concept is too poorly defined to provide any deep insight into the function of the nervous system .", "label": "", "metadata": {}, "score": "80.62674"}
{"text": "A serious point to this amusing example is that according to Jaynes , the students who reject Jaynes 's definition may be making a perfectly rational inference given their limited knowledge .Jaynesian and Not Merely Bayesian .Although Jaynes referred to his own work as \" Bayesian \" , as opposed to \" frequentist \" , there are reasons to favor the term \" Jaynesian \" for many aspects of contemporary probability theory .", "label": "", "metadata": {}, "score": "80.95276"}
{"text": "This figure is analogous to Figure 8 ; the same group of neurons is used here .( A )The distribution of synchronous spikes , P ( K ) , in the data ( black ) and in the DG model fit to data ( red ) .", "label": "", "metadata": {}, "score": "81.03658"}
{"text": "Methods in Protein Design , Academic Press , Volume 523 of Methods in Enzymology .pp .109 - 143 .Best RB , Hummer G ( 2009 ) Optimized molecular dynamics force fields applied to the helix - coil transition of polypeptides .", "label": "", "metadata": {}, "score": "81.23111"}
{"text": "Methods in Protein Design , Academic Press , Volume 523 of Methods in Enzymology .pp .109 - 143 .Best RB , Hummer G ( 2009 ) Optimized molecular dynamics force fields applied to the helix - coil transition of polypeptides .", "label": "", "metadata": {}, "score": "81.23111"}
{"text": "The gray shaded region shows the distribution of the values of over all 120 neurons and all patterns in the data .What do the models teach us ?Figures 7 through 9 indicate that these models give a fairly accurate description of the distribution of states - the myriad combinations of spiking and silence - taken on by the network as a whole .", "label": "", "metadata": {}, "score": "81.308624"}
{"text": "Third , we lack consensus even on the gross general function of the brain .The authoritative text of Dayan and Abbott , entitled Theoretical Neuroscience , does not even speculate about whether or not the bran has a general computational goal [ 3 ] .", "label": "", "metadata": {}, "score": "81.333374"}
{"text": "The presence of such competing interactions generates \" frustration , \" where ( for example ) triplets of neurons can not find a combination of spiking and silence that simultaneously minimizes all the terms in the energy function [ 4 ] .", "label": "", "metadata": {}, "score": "81.4618"}
{"text": "Systematic validation of protein force fields against experimental data .PLOS One 7 : e32131 doi:10.1371/journal.pone.0032131 .Leaver - Fay A , O'Meara MJ , Tyka M , Jacak R , Song Y , et al .. ( 2013 ) Chapter six - scientific benchmarks for guiding macromolecular energy function improvement .", "label": "", "metadata": {}, "score": "81.6615"}
{"text": "Systematic validation of protein force fields against experimental data .PLOS One 7 : e32131 doi:10.1371/journal.pone.0032131 .Leaver - Fay A , O'Meara MJ , Tyka M , Jacak R , Song Y , et al .. ( 2013 ) Chapter six - scientific benchmarks for guiding macromolecular energy function improvement .", "label": "", "metadata": {}, "score": "81.6615"}
{"text": "Neurosci .[ Google Scholar ] [ CrossRef ] .Kording , K.P. ; Wolpert , D.M. Bayesian integration in sensorimotor learning .Nature 2004 , 427 , 244 - 247 .[ Google Scholar ] .Vaziri , S. ; Diedrichsen , J. ; Shadmehr , R. Why does the brain predict sensory consequences of oculomotor commands ?", "label": "", "metadata": {}, "score": "81.89674"}
{"text": "Figure 10 shows how the number of metastable states that we identify in the data grows with the size N of the network .Indeed , we see no sign that the number of metastable states is saturating , and the growth is certainly faster than linear in the number of neurons .", "label": "", "metadata": {}, "score": "81.99272"}
{"text": "Inset shows the distribution of three - point correlations ( grey filled region ) and the distribution of differences between two halves of the experiment ( dashed line ) ; note the logarithmic scale .( A )The cumulative distribution of energies , from Eq ( 22 ) , for the K - pairwise models ( red ) and the data ( black ) , in a population of 120 neurons .", "label": "", "metadata": {}, "score": "82.1723"}
{"text": "Int J Math Stat Sci 6 : 17 - 39 MATH MathSciNet .Berger JO ( 1985 ) Statistical decision theory and Bayesian analysis .Springer , New York MATH .Berger J ( 2006 )The Case for objective bayesian analysis .", "label": "", "metadata": {}, "score": "82.748856"}
{"text": "Garthwaite PH , Kadane JB , O'Hagan A ( 2005 ) Statistical methods for eliciting probability distributions .J Am Stat Assoc 100 : 680 - 701 MATH CrossRef MathSciNet .Gelfand AE , Mallick BK , Dey DK ( 1995 )", "label": "", "metadata": {}, "score": "82.81002"}
{"text": "Neuron 1997 , 18 , 959 - 968 .[ Google Scholar ] [ CrossRef ] .Brenner , N. ; Bialek , W. ; de Ruyter van Steveninck , R.R. Adaptive rescaling maximizes information transmission .Neuron 2000 , 26 , 695 - 702 .", "label": "", "metadata": {}, "score": "82.84839"}
{"text": "Although a full discussion of this issue is beyond the scope of the present work , several important points should be noted .Humans are capable of reason , and there are numerous instances in which brain function is at least semi - rational .", "label": "", "metadata": {}, "score": "82.924484"}
{"text": "Inset shows the detailed structure of several transitions out of the all - silent state ; overlapping lines of the same color show that the same transition is identified robustly across different subnetwork choices of 120 neurons out of 160 .Entropy .", "label": "", "metadata": {}, "score": "83.049774"}
{"text": "Math .Biol .[ Google Scholar ] [ CrossRef ] .Seung , H.S. ; Sompolinsky , H. Simple models for reading neural population codes .Proc .Natl .Acad .Sci .USA 1993 , 90 , 10749 - 10753 .", "label": "", "metadata": {}, "score": "83.1494"}
{"text": "We notice that correlations are weak , but widespread , as in previous experiments on smaller groups of neurons [ 4 ] , [ 6 ] , [ 9 ] , [ 65 ] , [ 66 ] .A subgroup of 100 neurons from our set of 160 has been sorted by the firing rate .", "label": "", "metadata": {}, "score": "83.178986"}
{"text": "These constraints contribute a term to the energy function ( 7 )It is more conventional to think about correlations between two neurons in terms of their spike trains .If we define ( 8) where neuron i spikes at times , then the spike - spike correlation function is [ 43 ] ( 9 ) and we also have the average spike rates .", "label": "", "metadata": {}, "score": "83.39758"}
{"text": "As schematized in Figure 1 , these methods make it possible to record from ganglion cells in the relevant densely interconnected patch , while projecting natural movies onto the retina .Access to these large populations poses new problems for the inference of maximum entropy models , both in principle and in practice .", "label": "", "metadata": {}, "score": "83.65813"}
{"text": "The Journal of Neuroscience , 34(3):941 - 952 .[ pdf ] .[ This blog post is collaboratively written by Evan and Memming ] The Scalable Models workshop was a remarkable success !It attracted a huge crowd from the wee morning hours till the 7:30 pm close of the day .", "label": "", "metadata": {}, "score": "83.837234"}
{"text": "The channel can exist in any one of multiple states .With a patch electrode , we can only discriminate open ( \" O \" ) from closed ( \" C \" ) states .The various closed states are \" hidden \" from our observation , but we can infer their existence through careful analysis .", "label": "", "metadata": {}, "score": "83.87846"}
{"text": "He showed a multiple - choice exam example where maximizing mutual information can be worse , and a linear neural coding example for different cost functions .Searching for Collective Behavior in a Large Network of Sensory Neurons .Affiliations : Institut de la Vision , INSERM U968 , UPMC , CNRS U7210 , CHNO Quinze - Vingts , Paris , France , Department of Molecular Biology , Princeton Neuroscience Institute , Princeton University , Princeton , New Jersey , United States of America .", "label": "", "metadata": {}, "score": "83.903824"}
{"text": "Local statistics in natural scenes predict the saliency of synthetic textures .Proc Natl Acad Sci ( USA ) 107 : 18149 - 18154 .doi : 10.1073/pnas.0914916107 .Lezon TR , Banavar JR , Cieplak M , Maritan A ( 2006 ) Federoff NV ( 2006 )", "label": "", "metadata": {}, "score": "84.00627"}
{"text": "It is appealing to think that there must be some simplification , that we wo n't need a million parameters , but it is not obvious that any particular simplification strategy will work .Surprising or not , it certainly is important that , as the community contemplates monitoring the activity of ever larger number of neurons [ 96 ] , we can identify theoretical approaches that have the potential to tame the complexity of these large systems .", "label": "", "metadata": {}, "score": "84.2341"}
{"text": "He ( and Dimitri Yatsenko ) used latent - variable graphical lasso to enforce a sparse inverse covariance matrix , and found that the estimate is more accurate and very different from raw noise correlation estimates .Whole - brain functional imaging and motor learning in the larval zebrafish ( workshop ) Misha Ahrens .", "label": "", "metadata": {}, "score": "84.52933"}
{"text": "The set of MS states found can depend on the manner in which descent is performed , in particular when some of the states visited during descent are on the \" ridges \" between multiple basins of attraction .Note that whether a pattern is a MS state or not is independent of the descent method ; what depends on the method is which MS states are found by starting from the data patterns .", "label": "", "metadata": {}, "score": "84.867325"}
{"text": "Next year , NIPS will be at Montreal , Canada .Theoretical Neuroscience .Neural Reinforcement Learning ( Posner lecture )Peter Dayan .He described how theoretical quantities in reinforcement learning such as TD - error correlate with neuromodulators such as dopamine .", "label": "", "metadata": {}, "score": "84.91615"}
{"text": "To perform logic and inference , the information must be physically integrated in both space and time .In computers , this physical integration occurs in transistors , and in a nervous system , it occurs most notably in proteins ( such as ion channels ) and within the membrane voltage of neurons ( which integrate information from many ion channels ) .", "label": "", "metadata": {}, "score": "85.07278"}
{"text": "J Neurosci 28 : 505 - 518 .doi : 10.1523/jneurosci.3359 - 07.2008 .Tka\u010dik G , Schneidman E , Berry MJ II & Bialek W ( 2009 ) Spin - glass models for a network of real neurons .", "label": "", "metadata": {}, "score": "85.20901"}
{"text": "Neuron 68 : 387 - 400 . doi : 10.1016/j.neuron.2010.09.015 .Rutishauser U , Kotowicz A ( 2013 ) Laurent G ( 2013 )A method for closed - loop presentation of sensory stimuli conditional on the internal brain - state of awake animals .", "label": "", "metadata": {}, "score": "85.349686"}
{"text": "Dayan , P. ; Abbott , L.F. Theoretical Neuroscience ; MIT Press : Cambridge , MA , USA , 2001 .[ Google Scholar ] .von Helmholz , H. Concerning the Perceptions in General .In Treatise on Physiological Optics , 1896 ; Reprinted in Visual Perception ; Yantis , S. , Ed . ; Psychology Press : Philadelphia , PA , USA , 2001 ; pp .", "label": "", "metadata": {}, "score": "85.37754"}
{"text": "Nat .Neurosci .[ Google Scholar ] [ CrossRef ] .Ernst , M.O. ; Banks , M.S. Humans integrate visual and haptic information in a statistically optimal fashion .Nature 2002 , 415 , 429 - 433 .[ Google Scholar ] .", "label": "", "metadata": {}, "score": "85.502625"}
{"text": "[ Google Scholar ] .de Ruyter van Steveninck , R.R. ; Laughlin , S.B. The rate of information transfer at graded potential synapses .Nature 1996 , 379 , 642 - 645 .[ Google Scholar ] [ CrossRef ] .", "label": "", "metadata": {}, "score": "85.52352"}
{"text": "Personal dialogue with these neuroscientists and others , and published discourse with Friston [ 46 , 47 ] , indicated that they were not intentionally advocating the frequentist view over the Jaynesian view , but rather that they had been substantially unaware of the distinct definitions .", "label": "", "metadata": {}, "score": "85.596664"}
{"text": "Macromolecular Structure Determination .Molecular simulations typically utilize either molecular dynamics ( MD ) or Monte Carlo ( MC ) methods to sample conformations according to an energy function , .Here , represents the structure of a molecule and possibly also solvent molecules and other co - factors , and represents a mathematical function that relates the structure to the \" energy \" of the system .", "label": "", "metadata": {}, "score": "85.66855"}
{"text": "Macromolecular Structure Determination .Molecular simulations typically utilize either molecular dynamics ( MD ) or Monte Carlo ( MC ) methods to sample conformations according to an energy function , .Here , represents the structure of a molecule and possibly also solvent molecules and other co - factors , and represents a mathematical function that relates the structure to the \" energy \" of the system .", "label": "", "metadata": {}, "score": "85.66855"}
{"text": "The overlaps , , between all pairs of identified patterns belonging to basins 2, ... ,10 ( MS 1 left out due to its large size ) .Patterns within the same basin are much more similar between themselves than to patterns belonging to other basins .", "label": "", "metadata": {}, "score": "85.69827"}
{"text": "Simon Pierre Laplace ( shown in Figure 1 ) was arguably the most important person in the early development and application of probability theory .In the early 19th century , he referred to probability theory as \" nothing but common sense reduced to calculation \" [ 36 ] .", "label": "", "metadata": {}, "score": "85.71181"}
{"text": "An ion channel is either open or closed .A vesicle containing neurotransmitter is either released or not released from a presynaptic terminal .A neuron either generates an action potential or it does not .In a \" tail - flick assay \" , a rat either removes its tail from a hotplate or it does not .", "label": "", "metadata": {}, "score": "85.93376"}
{"text": "According to well known models of neuronal membrane biophysics ( such as Hodgkin - Huxley models ) , there is nothing random about the generation of action potentials [ 53 ] .When synaptic excitation occurs , whether or not the neuron emits a spike will be determined by a large variety of \" hidden \" variables that can not usually be observed even by intracellular electrodes , such as the conductance of potassium channels .", "label": "", "metadata": {}, "score": "86.23979"}
{"text": "It either grows or not grows .If it grows , he is telling the truth , so it should not grow .If it is false , then it should grow , but then it is true again .Our natural language allows self - referencing , but is it really logically possible ?", "label": "", "metadata": {}, "score": "86.5084"}
{"text": "The tags added to the feature descriptors are used to compute the saliency matching score , and the scores are regarded as the weight information in the geometry verification step .Second we define a spatial pattern as a triangle composed of three matched features and evaluate the similarity between every two spatial patterns .", "label": "", "metadata": {}, "score": "86.600075"}
{"text": "First , despite its considerable contributions with respect to engineering , information theory still has not found its way into biology and medical textbooks after 60 years .Even standard neuroscience textbooks , including the authoritative text of Kandel and colleagues [ 2 ] with 1414 pages , make no reference to information theory or Claude Shannon .", "label": "", "metadata": {}, "score": "86.68697"}
{"text": "Jaynes , E.T. Information theory and statistical mechanics .Phys .Rev. 1957 , 106 , 120 - 130 .[ Google Scholar ] .Jaynes , E.T. Probability Theory : The Logic of Science ; Cambridge University Press : Cambridge , UK , 2003 .", "label": "", "metadata": {}, "score": "86.80575"}
{"text": "The structure of the energy landscape explored with Monte Carlo .Starting in the all - silent state , single spin - flip steps are taken until the configuration crosses the energy barrier into another basin .Here , two such paths are depicted ( green , ultimately landing in the basin of MS 9 ; purple , landing in basin of MS 5 ) as projections into 3D space of scalar products ( overlaps ) with the MS 1 , 5 , and 9 .", "label": "", "metadata": {}, "score": "86.853035"}
{"text": "Shlens J , Field GD , Gaulthier JL , Greschner M , Sher A , Litke AM ( 2009 ) Chichilnisky EJ ( 2009 )The structure of large - scale synchronized firing in primate retina .J Neurosci 29 : 5022 - 5031 .", "label": "", "metadata": {}, "score": "86.96554"}
{"text": "If correct , these predictions would have a substantial impact on how we think about coding in the retina , and about neural network function more generally .Correspondingly , there is some controversy about all these issues [ 32 ] - [ 35 ] .", "label": "", "metadata": {}, "score": "87.377205"}
{"text": "The 10 most frequently occurring metastable ( MS ) states ( active neurons for each in red ) , and 50 randomly chosen activity patterns for each MS state ( black dots represent spikes ) .MS 1 is the all - silent basin .", "label": "", "metadata": {}, "score": "87.39729"}
{"text": "This approach has , for instance , been used to study the structural dynamics of folded proteins [ 31 ] - [ 33 ] , unfolded proteins [ 34 ] , membrane proteins [ 35 ] , and intrinsically disordered proteins [ 36 ] .", "label": "", "metadata": {}, "score": "87.63858"}
{"text": "This approach has , for instance , been used to study the structural dynamics of folded proteins [ 31 ] - [ 33 ] , unfolded proteins [ 34 ] , membrane proteins [ 35 ] , and intrinsically disordered proteins [ 36 ] .", "label": "", "metadata": {}, "score": "87.63858"}
{"text": "Here I argue that the adoption of a strictly Jaynesian approach will prevent such errors and will provide us with the philosophical and mathematical framework that is needed to understand the general function of the brain .Accordingly , our challenge becomes the identification of the biophysical basis of Jaynesian information and logic .", "label": "", "metadata": {}, "score": "87.77321"}
{"text": "With a patch electrode , we can only discriminate open ( \" O \" ) from closed ( \" C \" ) states .The various closed states are \" hidden \" from our observation , but we can infer their existence through careful analysis .", "label": "", "metadata": {}, "score": "87.842926"}
{"text": "This article presents a new supervised learning algorithm to identify debonded regions in aluminum honeycomb panels .The algorithm uses a linear approximation method handled by a statistical inference model based on the maximum - entropy principle .The merits of this new approach are twofold : training is avoided and data is processed in a period of time that is comparable to the one of neural networks .", "label": "", "metadata": {}, "score": "88.25185"}
{"text": "Ion channels are the most microscopic of the four systems listed above .Until the invention of the patch - clamp method of electrophysiological recording , it was not possible for us to observe the electrical behavior of single channels .Now we know that many single channels have just two conductance states , open or closed , and we can not predict with high accuracy the conductance state of a single channel at any moment .", "label": "", "metadata": {}, "score": "88.29904"}
{"text": "doi : 10.1126/science.1157092 .Roux B , Islam SM ( 2013 ) Restrained - ensemble molecular dynamics simulations based on distance histograms from double electron - electron resonance spectroscopy .J Phys Chem B 117 : 4733 - 4739 .", "label": "", "metadata": {}, "score": "88.353615"}
{"text": "doi : 10.1126/science.1157092 .Roux B , Islam SM ( 2013 ) Restrained - ensemble molecular dynamics simulations based on distance histograms from double electron - electron resonance spectroscopy .J Phys Chem B 117 : 4733 - 4739 .", "label": "", "metadata": {}, "score": "88.353615"}
{"text": "Berry MJ II ( 2012 )Mapping a complete neural population in the retina .J Neurosci 32 : 14859 - 14873 .doi : 10.1523/jneurosci.0723 - 12.2012 .Broderick T , Dudik M , Tka\u010dik , G Schapire RE & Bialek W ( 2007 ) Faster solutions of the inverse pairwise Ising problem .", "label": "", "metadata": {}, "score": "88.36165"}
{"text": "Discussion .It is widely agreed that neural activity in the brain is more than the sum of its parts - coherent percepts , thoughts , and actions require the coordinated activity of many neurons in a network , not the independent activity of many individual neurons .", "label": "", "metadata": {}, "score": "88.470665"}
{"text": "( 2011 ) Direct - coupling analysis of residue coevolution captures native contacts across many protein families .Proc Natl Acad Sci U S A 108 : E1293-E1301 .doi : 10.1073/pnas.1111471108 .Klepeis JL , Lindorff - Larsen K , Dror RO , Shaw DE ( 2009 )", "label": "", "metadata": {}, "score": "88.672066"}
{"text": "( 2011 ) Direct - coupling analysis of residue coevolution captures native contacts across many protein families .Proc Natl Acad Sci U S A 108 : E1293-E1301 .doi : 10.1073/pnas.1111471108 .Klepeis JL , Lindorff - Larsen K , Dror RO , Shaw DE ( 2009 )", "label": "", "metadata": {}, "score": "88.672066"}
{"text": "doi : 10.1021/jp901540 t .Tyka MD , Jung K , Baker D ( 2012 )Efficient sampling of protein conformational space using fast loop building and batch minimization on highly parallel computers .J Comput Chem 33 : 2483 - 2491 .", "label": "", "metadata": {}, "score": "88.779236"}
{"text": "doi : 10.1021/jp901540 t .Tyka MD , Jung K , Baker D ( 2012 )Efficient sampling of protein conformational space using fast loop building and batch minimization on highly parallel computers .J Comput Chem 33 : 2483 - 2491 .", "label": "", "metadata": {}, "score": "88.779236"}
{"text": "Traditional structure determination methods .Despite recent substantial developments in the accuracy of molecular energy functions [ 18 ] - [ 20 ] it is still not possible routinely and consistently to use molecular simulations to predict or refine the structure of proteins [ 21 ] , [ 22 ] .", "label": "", "metadata": {}, "score": "88.99475"}
{"text": "Traditional structure determination methods .Despite recent substantial developments in the accuracy of molecular energy functions [ 18 ] - [ 20 ] it is still not possible routinely and consistently to use molecular simulations to predict or refine the structure of proteins [ 21 ] , [ 22 ] .", "label": "", "metadata": {}, "score": "88.99475"}
{"text": "Structure determination by NMR : The modeling of NMR parameters as ensemble averages .In : Hoch JC , Poulsen FM , Redfield C , editors .Computational aspects of the Study of Biological Macromolecules by Nuclear Magnetic Resonance Spectroscopy .New York : Plenum Press . pp .", "label": "", "metadata": {}, "score": "89.01513"}
{"text": "Structure determination by NMR : The modeling of NMR parameters as ensemble averages .In : Hoch JC , Poulsen FM , Redfield C , editors .Computational aspects of the Study of Biological Macromolecules by Nuclear Magnetic Resonance Spectroscopy .New York : Plenum Press . pp .", "label": "", "metadata": {}, "score": "89.01513"}
{"text": "Ian Stevenson .Our ability to infer functional connectivity among neurons is limited by data .Using current - injection , he investigated exactly how much data is required for detecting synapses of various strength under the generalized linear model ( GLM ) .", "label": "", "metadata": {}, "score": "89.29672"}
{"text": "Neurosci .[ Google Scholar ] [ CrossRef ] .Ma , W.J. ; Beck , J.M. ; Latham , P.E. ; Pouget , A. Bayesian inference with probabilistic population codes .Nat .Neurosci .[ Google Scholar ] [ CrossRef ] .", "label": "", "metadata": {}, "score": "89.301025"}
{"text": "Dedmon MM , Lindorff - Larsen K , Christodoulou J , Vendruscolo M , Dobson CM ( 2005 ) Mapping long - range interactions in \u03b1 - synuclein using spin - label NMR and ensemble molecular dynamics simulations .J Am Chem Soc 127 : 476 - 477 .", "label": "", "metadata": {}, "score": "89.39605"}
{"text": "Dedmon MM , Lindorff - Larsen K , Christodoulou J , Vendruscolo M , Dobson CM ( 2005 ) Mapping long - range interactions in \u03b1 - synuclein using spin - label NMR and ensemble molecular dynamics simulations .J Am Chem Soc 127 : 476 - 477 .", "label": "", "metadata": {}, "score": "89.39605"}
{"text": "For planned papers , a title and short abstract ( about 100 words ) can be sent to the Editorial Office for announcement on this website .Submitted manuscripts should not have been published previously , nor be under consideration for publication elsewhere ( except conference proceedings papers ) .", "label": "", "metadata": {}, "score": "89.513115"}
{"text": "Jaynesian Approaches in Neuroscience .The value of Jaynesian probability theory is in providing the foundational framework for understanding information and logic .Jaynes did not address the physical or neurobiological basis of information and logic .In another article in this same issue of Information , Phillips also explores the work of Jaynes in relation to brain function and he comes to conclusions that are similar in several respects to my own [ 56 ] .", "label": "", "metadata": {}, "score": "89.56705"}
{"text": "over 30 subnetworks at each size N .These results are consistent with earlier suggestions in Ref [ 4 ] .There the multi - information increased proportionally to for small populations ( cells ) , which we also see .The previous paper also suggested from very general theoretical grounds that this scaling would break down at larger network sizes , as we now observe .", "label": "", "metadata": {}, "score": "89.5671"}
{"text": "Purves , D. ; Wojtach , W.T. ; Lotto , R.B. Understanding vision in wholly empirical terms .Proc .Natl .Acad .Sci .USA 2011 , 108 , 15588 - 15595 .[ Google Scholar ] .Purves , D. ; Lotto , R.B. Why We See What We Do Redux ; Sinauer Associates Inc. : Sunderland , MA , USA , 2011 .", "label": "", "metadata": {}, "score": "89.7328"}
{"text": "Therefore , you are not going to be in constant contact with the odor ( say the scent of freshly baked chocolate chip cookies ) while you search for the source ( cookies ! )You might not even smell it at all for a long periods of time , even if the target is nearby depending on the air flow .", "label": "", "metadata": {}, "score": "89.73952"}
{"text": "There is reason to expect that once this practice has been recognized as a fallacy , progress towards understanding the brain will be greatly accelerated .Acknowledgements .I would like to thank Bill Phillips for discussions and comments on the manuscript , Karl Friston and Peter Freed for helpful comments on earlier versions of the manuscript , and Chang Sub Kim for helping me to better understand statistical mechanics .", "label": "", "metadata": {}, "score": "89.795746"}
{"text": "Swendsen RH ( 1988 )New Monte Carlo technique for studying phase transitions .Phys Rev Lett 61 : 2635 - 2638 . doi : 10.1103/physrevlett.61.2635 .Cocco S , Leibler S ( 2009 ) Monasson R ( 2009 ) Neuronal couplings between retinal ganglion cells inferred by efficient inverse statistical physics methods .", "label": "", "metadata": {}, "score": "89.816536"}
{"text": "Experiments from the retina does not obey the sign rule ; noise correlation is positive even for cells tuned to the same direction , however , it is still near optimal according to their theory .During a vibration detection task , cross - correlations among neurons in the premotor cortex ( in a 250 ms window ) were shown to be dependent on behavior ( see Carnevale et al .", "label": "", "metadata": {}, "score": "89.904915"}
{"text": "Some statisticians use improper priors as uninformative priors .Since all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .", "label": "", "metadata": {}, "score": "89.950096"}
{"text": "Lindorff - Larsen K , Kristjansdottir S , Teilum K , Fieber W , Dobson CM , et al .( 2004 ) Determination of an ensemble of structures representing the denatured state of the bovine acyl - coenzyme A binding protein .", "label": "", "metadata": {}, "score": "90.22768"}
{"text": "Lindorff - Larsen K , Kristjansdottir S , Teilum K , Fieber W , Dobson CM , et al .( 2004 ) Determination of an ensemble of structures representing the denatured state of the bovine acyl - coenzyme A binding protein .", "label": "", "metadata": {}, "score": "90.22768"}
{"text": "In particular , we found that the pairwise model began to break down at a network size ( Figure 5 ) .Although the primary goal of this work was to examine the responses of the retina under naturalistic stimulation , we also checked that the K - pairwise models are able to capture the joint behavior of retinal ganglion cells under a very different , random checkerboard stimulation ( Figure S7 ) .", "label": "", "metadata": {}, "score": "90.52789"}
{"text": "Regardless of the source of these correlations , however , the question of whether they are driven by the stimulus or are intrinsic to the network is unlikely a question that the brain could answer .The brain has access only to the output of the retina : the patterns of activity which are drawn from the distribution , rather than activity conditional on the stimulus , so the neural mechanism by which the correlations could be split into signal and noise components is unclear .", "label": "", "metadata": {}, "score": "90.98049"}
{"text": "All these analyses suggest that , in the salamander retina , the roughly 200 interconnected neurons that represent a small patch of the visual world should exhibit dramatically collective behavior .In particular , the states of these large networks should cluster around local minima of the energy landscape , much as for the attractors in the Hopfield model of associative memory [ 1 ] .", "label": "", "metadata": {}, "score": "91.02353"}
{"text": "However , we realized that they can work like little clocks .When external odor molecules stimulate the neuron , it sends a signal in a time dependent manner .Each neuron is too noisy to be a precise clock , but there is a whole population of these neurons , such that together they can measure the temporal aspects critical for the olfactory scene analysis .", "label": "", "metadata": {}, "score": "91.04813"}
{"text": "In general , I am interested in understanding how neurons process and represent information in their output through which they communicate with other neurons and collectively compute .In this paper , we show how a subset of olfactory neurons can be used like a stop watch to measure temporal patterns of smell .", "label": "", "metadata": {}, "score": "91.22742"}
{"text": "This year , I found out that April 30th ( 1916 ) is Claud Shannon 's birthday so I decided the theme would be his information theory .I started by introducing probabilistic reasoning as an extension of logic in this uncertain world ( as Michael Buice told us in BBD7 ) .", "label": "", "metadata": {}, "score": "91.36138"}
{"text": "[ Google Scholar ] .Yang , Z. ; Purves , D. A statistical explanation of visual space .Nat .Neurosci .[ Google Scholar ] [ CrossRef ] .Purves , D. ; Lotto , R.B. Why We See What We Do : An Empirical Theory of Vision ; Sinauer Associates Inc. : Sunderland , MA , USA , 2003 .", "label": "", "metadata": {}, "score": "91.46388"}
{"text": "Surya Ganguli .Several recent studies showed low - dimensional state - space of trial - averaged population activities ( e.g. , Churchland et al .2012 , Mante et al 2013 ) .Surya asks what would happen to the PCA analysis of neural trajectories if we record from 1 billion neurons ?", "label": "", "metadata": {}, "score": "91.75136"}
{"text": "Statistics in medicine , 26 ( 21):4039 - 4060 , 2007 .[ CAE ] .This year , NIPS ( Neural Information Processing Systems ) had a record registration of 1900 + ( it has been growing over the years ) with 25 % acceptance rate .", "label": "", "metadata": {}, "score": "91.76731"}
{"text": "Noise correlations in the brain are small ( 0.01 range ; e.g. , Renart et al .2010 ) .Anesthetized animals have higher firing rate and higher noise correlation ( 0.06 range ) .He showed how latent variable model ( GPFA ) can be used to decompose the noise correlation into that of the latent and the rest .", "label": "", "metadata": {}, "score": "92.041916"}
{"text": "Stimulus display .The stimulus was presented using standard optics , at a rate of 30 frames per second , and gamma corrected for the display .Data preparation .We randomly selected 30 subgroups of cells for analysis from the total of 160 sorted cells .", "label": "", "metadata": {}, "score": "92.323326"}
{"text": "Kim Y , Prestegard J ( 1989 )A dynamic model for the structure of acyl carrier protein in solution .Biochemistry 28 : 8792 - 8797 .doi : 10.1021/bi00448a017 .Kuriyan J , \u00d6sapay K , Burley SK , Br\u00fcnger AT , Hendrickson WA , et al .", "label": "", "metadata": {}, "score": "92.58314"}
{"text": "Kim Y , Prestegard J ( 1989 )A dynamic model for the structure of acyl carrier protein in solution .Biochemistry 28 : 8792 - 8797 .doi : 10.1021/bi00448a017 .Kuriyan J , \u00d6sapay K , Burley SK , Br\u00fcnger AT , Hendrickson WA , et al .", "label": "", "metadata": {}, "score": "92.58314"}
{"text": "Raval A , Piana S , Eastwood MP , Dror RO , Shaw DE ( 2012 ) Refinement of protein structure homology models via long , all - atom molecular dynamics simulations .Proteins 80 : 2071 - 2079 .doi : 10.1002/prot.24098 .", "label": "", "metadata": {}, "score": "92.59236"}
{"text": "Raval A , Piana S , Eastwood MP , Dror RO , Shaw DE ( 2012 ) Refinement of protein structure homology models via long , all - atom molecular dynamics simulations .Proteins 80 : 2071 - 2079 .doi : 10.1002/prot.24098 .", "label": "", "metadata": {}, "score": "92.59236"}
{"text": "doi : 10.1073/pnas.0805923106 .Marks DS , Colwell LJ , Sheridan R , Hopf TA , Pagnani A , Zecchina R ( 2011 )Sander C ( 2011 )Protein 3D structure computed from evolutionary sequence variation .PLoS One 6 : e28766 . doi : 10.1371/journal.pone.0028766 .", "label": "", "metadata": {}, "score": "92.859985"}
{"text": "The recordings were sorted using custom spike sorting software developed specifically for the new dense array [ 36 ] .234 neurons passed the standard tests for the waveform stability and the lack of refractory period violations .Of those , 160 cells whose firing rates were most stable across stimulus repeats were selected for further analysis .", "label": "", "metadata": {}, "score": "93.16762"}
{"text": "Shlens J , Field GD , Gaulthier JL , Grivich MI , Petrusca D , Sher A , Litke AM ( 2006 ) Chichilnisky EJ ( 2006 )The structure of multi - neuron firing patterns in primate retina .J Neurosci 26 : 8254 - 8266 .", "label": "", "metadata": {}, "score": "93.21185"}
{"text": "Adri\u00e0 Tauste Campo .Estimation of directed information between simultaneous spike trains in decision making .Bayesian conditional information estimation through the use of context - tree weighting was used to infer directional information ( analogous to Granger causality , but with mutual information ) .", "label": "", "metadata": {}, "score": "93.38394"}
{"text": "Thus , it was very surprising when we ( Dr. Yuriy Bobkov ) found that those neurons were spontaneously generating signals - in the form of regular bursts of action potentials - even in the absence of odor stimuli [ Bobkov & Ache 2007].", "label": "", "metadata": {}, "score": "93.462105"}
{"text": "Alivisatos AP , Chun M , Church GM , Greenspan RJ , Roukes ML ( 2012 )Yuste R ( 2012 )The brain activity map project and the challenge of functional connectomics .Neuron 74 : 970 - 974 .doi : 10.1016/j.neuron.2012.06.006 .", "label": "", "metadata": {}, "score": "93.596535"}
{"text": "Curr Opin Struct Biol 19 : 120 - 127 .doi : 10.1016/j.sbi.2009.03.004 .Norgaard AB , Ferkinghoff - Borg J , Lindorff - Larsen K ( 2008 )Experimental parameterization of an energy function for the simulation of unfolded proteins .", "label": "", "metadata": {}, "score": "93.878586"}
{"text": "Curr Opin Struct Biol 19 : 120 - 127 .doi : 10.1016/j.sbi.2009.03.004 .Norgaard AB , Ferkinghoff - Borg J , Lindorff - Larsen K ( 2008 )Experimental parameterization of an energy function for the simulation of unfolded proteins .", "label": "", "metadata": {}, "score": "93.878586"}
{"text": "[ Google Scholar ] [ CrossRef ] .Deneve , S. Bayesian spiking neurons I : Inference .Neural Comput .[ Google Scholar ] [ CrossRef ] .Rieke , F. ; Warland , D. ; de Ruyter van Steveninck , R.R. ; Bialek , W. Spikes : Exploring the Neural Code ; MIT Press : Cambridge , MA , USA , 1997 .", "label": "", "metadata": {}, "score": "93.977066"}
{"text": "Cessac B ( 2012 )Gibbs distribution analysis of temporal correlations structure in retina ganglion cells .J Physiol Paris 3 - 4 : 120 - 127 .doi : 10.1016/j.jphysparis.2011.11.001 .Tka\u010dik G , Prentice JS , Victor JD ( 2010 )", "label": "", "metadata": {}, "score": "94.14267"}
{"text": "While there are more structures that we could use - notably , the correlations across time - we find it remarkable that so much is learnable from just an afternoon 's worth of data .As it becomes more routine to record the activity of such ( nearly ) complete sensory representations , it will be interesting to take the organism 's point of view [ 43 ] more fully , and try to extract meaning from the spike trains in an unsupervised fashion .", "label": "", "metadata": {}, "score": "94.216995"}
{"text": "Pouget 's talk was focused on \" differential correlation \" which is the noise in the direction of the manifold that tuning curves encode information ( noise that looks like signal ) .Peter talked about why there are so many neurons in the brain with linear Fisher information and additive noise ( but I forgot the details ! )", "label": "", "metadata": {}, "score": "94.321045"}
{"text": "Ethics statement .This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health .The protocol was approved by the Institutional Animal Care and Use Committee ( IACUC ) of Princeton University ( Protocol 1827 for guinea pigs and 1828 for salamanders ) .", "label": "", "metadata": {}, "score": "94.816635"}
{"text": "If extracellular electrodes are used , then it is observed that the neuron 's spike output varies even when its input ( \" stimulus \" ) is the same .The neuron 's output ( firing rate ) has thus been modeled as a Poisson \" random \" process .", "label": "", "metadata": {}, "score": "94.86474"}
{"text": "Proteins 10 : 340 - 358 .doi : 10.1002/prot.340100407 .Lange OF , Lakomek NA , Far\u00e8s C , Schr\u00f6der GF , Walter KF , et al .( 2008 )Recognition dynamics up to microseconds revealed from an rdc - derived ubiquitin ensemble in solution .", "label": "", "metadata": {}, "score": "95.04289"}
{"text": "Proteins 10 : 340 - 358 .doi : 10.1002/prot.340100407 .Lange OF , Lakomek NA , Far\u00e8s C , Schr\u00f6der GF , Walter KF , et al .( 2008 )Recognition dynamics up to microseconds revealed from an rdc - derived ubiquitin ensemble in solution .", "label": "", "metadata": {}, "score": "95.04289"}
{"text": "We study lobsters .Lobsters heavily rely on olfaction to track , avoid , and detect odor sources such as other lobsters , predators , and food , therefore , it is important for them to constantly analyze olfactory sensory information to put together an olfactory scene .", "label": "", "metadata": {}, "score": "95.13118"}
{"text": "Rev. Neurosci .[ Google Scholar ] [ CrossRef ] .Gregory , R.L. Perceptions as hypotheses .Philos .Trans .R. Soc .Lond .B 1980 , 290 , 181 - 197 .[ Google Scholar ] [ CrossRef ] .", "label": "", "metadata": {}, "score": "95.46619"}
{"text": "Proc Natl Acad Sci ( USA ) 103 : 19033 - 19038 .doi : 10.1073/pnas.0609152103 .Weigt M , White RA , Szurmant H , Hoch JA ( 2009 ) Hwa T ( 2009 ) Identification of direct residue contacts in protein - protein interaction by message passing .", "label": "", "metadata": {}, "score": "95.46684"}
{"text": "The state of the retina was represented by if the neuron i spiked at least once ( was silent ) in a given time bin t .Time discretization resulted in 953 time bins per stimulus repeat ; 297 presented repeats yielded a total of N -bit binary samples during the course of the experiment for each subgroup .", "label": "", "metadata": {}, "score": "95.84955"}
{"text": "( Hence its nickname : painter 's paradox ) .Karin Knudson introduced the Banach - Tarski paradox where one solid unit sphere in 3D can be decomposed into 5 pieces , and only by translation and rotation , they are reconstructed into two solid unit spheres .", "label": "", "metadata": {}, "score": "96.27139"}
{"text": "Dimensionality , dynamics and ( de)synchronisation in the auditory cortex ( workshop ) Maneesh Sahani .Maneesh compared the underlying latent dynamical systems fit from synchronized state ( drowsy / inattentive / urethane / ketamine / xylazine ) and desyncrhonized state ( awake / attentive / urethane+stimulus / fentany / medtomidine / midazolam ) .", "label": "", "metadata": {}, "score": "96.386894"}
{"text": "Furthermore , in Lister 2013 , they showed a special type of DNA methylation ( mCH ) in the brain grows over the lifespan , coincides with synaptogenesis , and regulates gene expressions .Computational Neuroscience .What can slice physiology tell us about inferring functional connectivity from spikes ?", "label": "", "metadata": {}, "score": "96.51259"}
{"text": "Each check was randomly selected to be either black or white on each frame displayed at a rate of 30 Hz .The entire stimulus consisted of 69 repeats of 30 seconds each , and subgroups of up to 120 neurons were analyzed .", "label": "", "metadata": {}, "score": "96.75712"}
{"text": "Ohiorhenuan IE , Mechler F , Purpura KP , Schmid AM , Hu Q ( 2010 )Victor JD ( 2010 ) Sparse coding and higher - order correlations in fine - scale cortical networks .Nature 466 : 617 - 621 .", "label": "", "metadata": {}, "score": "97.37091"}
{"text": "Furthermore , this temporal memory was previously believed to be formed in the brain , but our results suggest a simple yet effective mechanism in the very front end , the sensors themselves .Applications : Currently electronic nose technology is mostly focused on discriminating ' what ' the odor is .", "label": "", "metadata": {}, "score": "97.7151"}
{"text": "Similarly , one critical component for olfactory scene analysis is the temporal structure of the odor pattern .Therefore , we wanted to find out how neurons encode and process this information .The neurons we study are of a subtype of olfactory sensory neurons .", "label": "", "metadata": {}, "score": "97.87056"}
{"text": "I presented a poster on Bayesian entropy estimation in the main meeting , and gave a talk about nonparametric ( kernel ) methods for spike trains in the workshop .Last Sunday ( April 28th , 2013 ) was the 8th Black board day ( BBD ) , which is a small informal workshop I organize every year .", "label": "", "metadata": {}, "score": "97.97502"}
{"text": "This is an open - access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited .The funders had no role in the preparation of the manuscript .", "label": "", "metadata": {}, "score": "98.189514"}
{"text": "This is an open - access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited .The funders had no role in the preparation of the manuscript .", "label": "", "metadata": {}, "score": "98.189514"}
{"text": "Lapedes AS , Giraud BG , Liu L , Stormo GD ( 1999 ) Correlated mutations in models of protein sequences : phylogenetic and structural effects .Lect Notes Monogr Ser 33 : 236 - 256 .doi : 10.1214/lnms/1215455556 .", "label": "", "metadata": {}, "score": "98.407745"}
{"text": "Lapedes AS , Giraud BG , Liu L , Stormo GD ( 1999 ) Correlated mutations in models of protein sequences : phylogenetic and structural effects .Lect Notes Monogr Ser 33 : 236 - 256 .doi : 10.1214/lnms/1215455556 .", "label": "", "metadata": {}, "score": "98.407745"}
{"text": "This is an open - access article distributed under the terms of the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original author and source are credited .The funders had no role in study design , data collection and analysis , decision to publish , or preparation of the manuscript .", "label": "", "metadata": {}, "score": "98.8893"}
{"text": "Kyle Mandi presented the classical Zeno 's paradox where your intuition on infinite sum of finite things being infinite is challenged .He also showed Gabriel 's horn where a simple ( infinite ) object with finite volume , but infinite surface area is given .", "label": "", "metadata": {}, "score": "99.33303"}
{"text": "Bobkov , Y. V. and Ache , B. W. ( 2007 ) .Intrinsically bursting olfactory receptor neurons .J Neurophysiol , 97(2):1052 - 1057 .Park , I. M. , Bobkov , Y. V. , Ache , B. W. , and Pr\u00edncipe , J. C. ( 2014 ) .", "label": "", "metadata": {}, "score": "99.35336"}
{"text": "Ralph Andrzejak .Detecting directional couplings between spiking signals and time - continuous signals .Using distance based directional coupling analysis ( see Chicharro , Andrzejak 2009 ; Andrzejak , Kreuz 2011 ) , he showed that it is possible to find unidirectional coupling between continuous signals and spike trains via spike train distances .", "label": "", "metadata": {}, "score": "99.59563"}
{"text": "The third - person perspective of the scientist could be denoted as \" xenocentric \" , since the scientist is \" foreign \" and external with respect to the brain and its world , whereas the first - person perspective of the brain could be denoted as \" neurocentric \" .", "label": "", "metadata": {}, "score": "101.42953"}
{"text": "doi : 10.1016/j.jmr.2011.08.039 .all Addendum Article Book Review Case Report Comment Commentary Communication Concept Paper Conference Report Correction Creative Data Descriptor Discussion Editorial Erratum Essay Interesting Images Letter New Book Received Obituary Opinion Project Report Reply Retraction Review Short Note Technical Note .", "label": "", "metadata": {}, "score": "102.11066"}
{"text": "Every last Sunday of April , I have been organizing a small workshop called BBD .We discuss logic , math , and science on a blackboard ( this year , it was actually on a blackboard unlike the past 3 years ! )", "label": "", "metadata": {}, "score": "102.36238"}
{"text": "Special Issue Information .Submission .Once you are registered , click here to go to the submission form .Manuscripts can be submitted until the deadline .Papers will be published continuously ( as soon as accepted ) and will be listed together on the special issue website .", "label": "", "metadata": {}, "score": "102.456024"}
{"text": "Abstract : Honeycomb sandwich structures are used in a wide variety of applications .Nevertheless , due to manufacturing defects or impact loads , these structures can be subject to imperfect bonding or debonding between the skin and the honeycomb core .", "label": "", "metadata": {}, "score": "102.539604"}
{"text": "We analyzed the recordings from the tiger salamander ( Ambystoma tigrinum ) retinal ganglion cells responding to naturalistic movie clips , as in the experiments of Refs .[ 4 ] , [ 36 ] , [ 65 ] .In brief , animals were euthanized according to institutional animal care standards .", "label": "", "metadata": {}, "score": "103.72045"}
{"text": "T . )P .G .T .R .T . )P .G .T . )S .T .P .G .T .S .R .T . )S .R .", "label": "", "metadata": {}, "score": "104.116264"}
{"text": "Jonathan Pillow , continuing on the fairy tale theme , presented the sleeping beauty paradox .Toss a coin , sleeping beauty will be awakened once if it is head , twice if it is tail .Every time she is awakened , she is asked \" What is your belief that the coin was heads ? \" , and given a drug that erases the memory of this awakening , and goes back to sleep .", "label": "", "metadata": {}, "score": "104.95601"}
{"text": "PLoS Comput Biol 10(1 ) : e1003408 .doi:10.1371/journal.pcbi.1003408 .Editor : Olaf Sporns , Indiana University , United States of America .Received : June 14 , 2013 ; Accepted : November 5 , 2013 ; Published : January 2 , 2014 .", "label": "", "metadata": {}, "score": "105.26769"}
{"text": "J Neurosci 30 : 8720 - 8733 .doi : 10.1523/jneurosci.6141 - 09.2010 .Amodei D , Schwartz G & Berry MJ II ( 2008 ) Correlations and the structure of the population code in a dense patch of the retina .", "label": "", "metadata": {}, "score": "105.6845"}
{"text": "Thus , we avoid the need for a posteriori adjustment of simulated monthly totals in order to correctly simulate the observed seasonal statistics .Detailed results are presented for the modelling and simulation of seasonal rainfall in the town of Kempsey on the mid - north coast of New South Wales .", "label": "", "metadata": {}, "score": "106.344925"}
{"text": "[ Google Scholar ] [ CrossRef ] .Gregory , R.L. Knowledge in perception and illusion .Philos .Trans .R. Soc .Lond .B 1997 , 352 , 1121 - 1128 .[ Google Scholar ] [ CrossRef ] .", "label": "", "metadata": {}, "score": "106.43085"}
{"text": "I was one of the many who were live tweeting via # NIPS2013 throughout the main meeting and workshops .Compared to previous years , it seemed like there were less machine learning in the invited / keynote talks .Also I noticed more industrial engagements ( Zuckerberg from facebook was here ( also this ) , and so was the amazon drone ) as well as increasing interest in neuroscience .", "label": "", "metadata": {}, "score": "106.84659"}
{"text": "He showed that the dynamics fit from either state were actually very similar .Eftychios have been developing various methods to infer spike trains from calcium image movies .He showed a compressive sensing framework for spiking activity can be inferred .", "label": "", "metadata": {}, "score": "107.37561"}
{"text": "I had a wonderful time , and I really appreciate my friends for joining me in this event !Recently , there was a press release and a youtube video from University of Florida about one of my recent papers on neural code in the lobster olfactory system , and also by others [ e.g. 1 , 2 , 3 , 4 ] .", "label": "", "metadata": {}, "score": "108.128815"}
{"text": "P .G .T .S .R . )T .T .T . )T .F .T . )T .T .T .T .T .F .T .F .T .", "label": "", "metadata": {}, "score": "109.23686"}
{"text": "Finally , we note that our approach to building models for the activity of the retinal ganglion cell population is entirely unsupervised : we are making use only of structure in the spike trains themselves , with no reference to the visual stimulus .", "label": "", "metadata": {}, "score": "109.85491"}
{"text": "Affiliations : Joseph Henry Laboratories of Physics , Princeton University , Princeton , New Jersey , United States of America , Lewis - Sigler Institute for Integrative Genomics , Princeton University , Princeton , New Jersey , United States of America .", "label": "", "metadata": {}, "score": "110.04486"}
{"text": "We estimated the new model on the Santiago ( Chile ) Metro network and compared the results with other route choice models that can be found in the literature .The new model has better explanatory and predictive power that many other alternative models , correctly capturing the correlation factor .", "label": "", "metadata": {}, "score": "111.690125"}
{"text": "Tissue was flattened and attached to a dialysis membrane using polylysine .The retina was then lowered with the ganglion cell side against a multi - electrode array .Arrays were first fabricated in university cleanroom facilities [ 105 ] .Subsequently , production was contracted out to a commercial MEMS foundry for higher volume production ( Innovative Micro Technologies , Santa Barbara , CA ) .", "label": "", "metadata": {}, "score": "112.78407"}
{"text": "This year it was held in Paris , next year is Qu\u00e9bec City , Canada .There are more theoretical and simulation based studies , compared to experimental studies .Among the experimental studies , there were a lot of oscillation and synchrony related subjects .", "label": "", "metadata": {}, "score": "116.064255"}
{"text": "Christopher D. Fiorillo .Department of Bio and Brain Engineering , KAIST , Daejeon 305 - 701 , Korea ; Email : Tel . : +82 - 42 - 350 - 4326 .Received : 9 December 2011 ; in revised form : 3 March 2012 / Accepted : 9 April 2012 / Published : 20 April 2012 .", "label": "", "metadata": {}, "score": "119.53558"}
{"text": "Today ( April 27th , 2014 ) , we had several paradoxes presented : .Memming : I presented the Pinocchio paradox , which is a fun variant of the Liar paradox .Pinocchio 's nose grows if and only if Pinocchio tells a false statement .", "label": "", "metadata": {}, "score": "126.63771"}
