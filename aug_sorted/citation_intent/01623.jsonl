{"text": "In this case , the classifier combination involves merging the individual ( usually weaker and/or diverse ) classifiers to obtain a single ( stronger ) expert of superior performance .Examples of this approach include bagging predictors ( Breiman 1996 ) , boosting ( Schapire 1990 ) , AdaBoost ( Freund 2001 ) and their many variations .", "label": "", "metadata": {}, "score": "23.43656"}
{"text": "Boosting methods have been crowned as the most accurate available off - the - shelf classifiers on a wide variety of datasets [ 107 ] .But , it is observed that boosting methods are sensitive to noise and outliers , especially for small datasets [ 46 , 56 , 107 ] .", "label": "", "metadata": {}, "score": "24.494415"}
{"text": "Boosting works by sequentially applying a classification algorithm to reweighted versions of the training data and then taking a weighted majority vote of the sequence of classifiers thus produced .For many classification algorithms , this simple strategy results in dramatic improvements in performance .", "label": "", "metadata": {}, "score": "26.789833"}
{"text": "The algorithmic simplicity of boosting also provides financial advantages , which could make it attractive to companies .\" Someone who finished a first - year programming class and understands algorithms can implement it , \" he said .In contrast , many other predictive analysis techniques require sophisticated knowledge of programming and statistics .", "label": "", "metadata": {}, "score": "28.241962"}
{"text": "We suggest a minor modification to boosting that can reduce computation , often by factors of 10 to 50 .Finally , we apply these insights to produce an alternative formulation of boosting decision trees .This approach , based on best - first truncated tree induction , often leads to better performance , and can provide interpretable descriptions of the aggregate decision rule .", "label": "", "metadata": {}, "score": "29.85184"}
{"text": "Similar to bagging , boosting also creates an ensemble of classifiers by resampling the data , which are then combined by majority voting .However , in boosting , resampling is strategically geared to provide the most informative training data for each consecutive classifier .", "label": "", "metadata": {}, "score": "30.286043"}
{"text": "This ( also called stacked generalization ) is a way of combining multiple classifiers using the concept of a metalearner [ 79 ] .Unlike bagging and boosting , stacking may be utilized to combine classifiers of different types .Note that steps ( 1 ) to ( 3 ) are the same as cross - validation , but instead of using a winner - takes - all approach , the base learners are combined , possibly non linearly .", "label": "", "metadata": {}, "score": "30.52172"}
{"text": "Boosting was the predecessor of the AdaBoost family of algorithms - which arguably became one of the most popular machine learning algorithms in recent times .Since these seminal works , research in ensemble systems have expanded rapidly , appearing often in the literature under many creative names and ideas .", "label": "", "metadata": {}, "score": "31.254238"}
{"text": "It 's kind of counterintuitive . \"Boosting is just beginning to emerge from academia into commercial use , but researchers believe it may have a wide range of potential applications .Companies , for example , might use it to identify patterns in consumer behavior , a practice called datamining .", "label": "", "metadata": {}, "score": "31.67303"}
{"text": "Ensemble classification techniques such as bagging , ( Breiman , 1996a ) , boosting ( Freund & Schapire , 1997 ) and arcing algorithms ( Breiman , 1997 ) have received much attention in recent literature .Such techniques have been shown to lead to reduced classification error on unseen cases .", "label": "", "metadata": {}, "score": "32.374912"}
{"text": "Such a set of classifiers is said to be diverse .Classifier diversity can be achieved in several ways .Preferably , the classifier outputs should be class - conditionally independent , or better yet negatively correlated .The most popular method is to use different training datasets to train individual classifiers .", "label": "", "metadata": {}, "score": "32.95977"}
{"text": "Thus the differences among these methods only come from their loss functions .\\ ] .\\ ] .\\ ] .Those experiments also showed that removing the regularization term in the objective functions of these classifiers resulted in significant performance degradation .", "label": "", "metadata": {}, "score": "33.11136"}
{"text": "[ 38 ] .The mRMR method uses the mutual information between a feature and a class or a feature and another feature .The relevance of a feature set .As a result , the best feature set is obtained by optimizing expressions of ( 4 ) and ( 5 ) simultaneously according to ( 6 ) or ( 7 ) .", "label": "", "metadata": {}, "score": "33.485443"}
{"text": "The two ways of measuring performance are complementary to each other , and both are informative .Other evaluation metrics include utility functions defined over weighted TP , FP , TN and FN , and rank - based metrics like Mean Average Precision ( MAP ) which evaluates the ability of a system to rank categories for a given document .", "label": "", "metadata": {}, "score": "33.986458"}
{"text": "The decisions made by each classifier can then be combined by any of the combination rules described below .Confidence Estimation .The very structure of an ensemble based system naturally allows assigning a confidence to the decision made by such a system .", "label": "", "metadata": {}, "score": "34.056164"}
{"text": "In this way the base classifier focuses on the hardest instances .Then the boosting algorithm combines the base rules taking a weighted majority vote of the base classifiers which are based on the accuracy of the classifiers [ 46 ] .", "label": "", "metadata": {}, "score": "34.23932"}
{"text": "Nock , R. , & Sebban , M. ( 2001 ) .A Bayesian boosting theorem .Pattern Recognition Letters , 22 , 413 - 419 .Pardalos , P. M. , & Xue , G. ( 1999 ) .Algorithms for a class of isotonic regression problems .", "label": "", "metadata": {}, "score": "34.3771"}
{"text": "Other applications of ensemble learning include assigning a confidence to the decision made by the model , selecting optimal ( or near optimal ) features , data fusion , incremental learning , nonstationary learning and error - correcting .This article focuses on classification related applications of ensemble learning , however , all principle ideas described below can be easily generalized to function approximation or prediction type problems as well .", "label": "", "metadata": {}, "score": "34.561268"}
{"text": "Boosting has earned fans among computational learning researchers , in part because it provides a happy marriage of theory and experiment .\" It 's very easy to implement and understand .It 's very efficient .It yields good results .", "label": "", "metadata": {}, "score": "34.731262"}
{"text": "Boosting , like other aspects of computational learning , raises fundamental questions about the nature of artificial intelligence .Despite the accuracy of boosting , the algorithm does not really \" know \" anything : It does not search for certainty , but merely makes mathematically calibrated predictions based on precedent .", "label": "", "metadata": {}, "score": "34.857918"}
{"text": "In essence , boosting works by assigning different priorities to rules of thumb by analyzing historical data fed in by the programmer .The technique has five steps , which it repeats until its accuracy can not be improved upon .Given a rule of thumb , it tries it out , then analyzes the errors , and assigns a priority ranking .", "label": "", "metadata": {}, "score": "34.90355"}
{"text": "Finally , they assumed that the training set is without attacks , by filtering it with a signature - based IDS , in order to throw out at least known attacks .Similar approach was also proposed by Corona et al .", "label": "", "metadata": {}, "score": "34.925503"}
{"text": "It has been empirically observed that linear classifiers with proper regularization are often sufficient for solving practical text categorization problems , with performance comparable or better than non - linear classifiers .Furthermore , linear methods are generally computationally efficient , both at training as well as at classification .", "label": "", "metadata": {}, "score": "35.060287"}
{"text": "The outputs of these classifiers on their pseudo - training blocks , along with the actual correct labels for those blocks constitute the training dataset for the Tier 2 classifier ( see Figure 7 ) .Jordan and Jacobs ' mixture of experts ( Jacobs 1991 ) generates several experts ( classifiers ) whose outputs are combined through a ( generalized ) linear rule .", "label": "", "metadata": {}, "score": "35.192085"}
{"text": "The three classifiers are combined through a three - way majority vote .The pseudocode and implementation detail of boosting is shown in Figure 5 . \\ )Also , the ensemble error is a training error bound .Hence , a stronger classifier is generated from three weaker classifiers .", "label": "", "metadata": {}, "score": "35.199738"}
{"text": "However , it has been shown that a properly trained ensemble decision is usually correct if its confidence is high , and usually incorrect if its confidence is low .Using such an approach then , the ensemble decisions can be used to estimate the posterior probabilities of the classification decisions ( Muhlbaier 2005 ) .", "label": "", "metadata": {}, "score": "35.56769"}
{"text": "Nor an improvement on the ensemble 's average performance can be guaranteed except for certain special cases ( Fumera 2005 ) .Hence combining classifiers may not necessarily beat the performance of the best classifier in the ensemble , but it certainly reduces the overall risk of making a particularly poor selection .", "label": "", "metadata": {}, "score": "35.667473"}
{"text": "If there was a conflict then the decision given by the classifier with the highest weight is taken into account .They showed that the performance of the proposed method is superior to that of single usage of base classification methods .", "label": "", "metadata": {}, "score": "35.85643"}
{"text": "Improved boosting algorithms using confidence - rated predictions .Machine Learning , 37 :3 , 297 - 336 .CrossRef .Vapnik , V. ( 1998 ) .Statistical Learning Theory .New York : John Wiley & Sons , Inc. .", "label": "", "metadata": {}, "score": "35.929054"}
{"text": "Bishop [ 67 ] proposed five methods for combining the individual classifiers .The methods are Bayesian model averaging , committees , boosting , tree - based models , and conditional mixture models .Boosting is further divided into two types : ( 1 ) minimizing exponential error ; ( 2 ) error functions for boosting .", "label": "", "metadata": {}, "score": "35.964817"}
{"text": "A decision - theoretic generalization of on - line learning and an application to boosting .Journal or Computer and System Sciences , 55 :1 , 119 - 139 .MathSciNet .Friedman , J. , Hastie , T. , & Tibshirani , R. ( 2000 ) .", "label": "", "metadata": {}, "score": "36.018257"}
{"text": "For a detailed overview of these and other combination rules , see ( Kuncheva 2005 ) .Other applications of ensemble systems .Ensemble based systems can be used in problem domains other than improving the generalization performance of a classifier .", "label": "", "metadata": {}, "score": "36.125015"}
{"text": "The objective of this method is to offer an alternative classification when base classifiers disagree ( arbiter tree ) or to combine the predictions of base classifiers by learning their relationships with the correct class labels ( combiner trees ) [ 80 , 81 ] .", "label": "", "metadata": {}, "score": "36.289734"}
{"text": "MathSciNet .Bennett , K. P. , Demiriz , A. , & Shawe - Taylor , J. ( 2000 ) .A column generation algorithm for boosting .Conference Proceedings 17th ICML .Buja , A. , Hastie , T. , & Tibshirani , R. ( 1989 ) .", "label": "", "metadata": {}, "score": "36.58995"}
{"text": "( 4 ) Some unstable classifiers like neural networks show different results with different initializations due to the randomness inherent in the training procedure .Instead of selecting the best network and discarding the others , one can combine various networks .", "label": "", "metadata": {}, "score": "36.865276"}
{"text": "Most of popular ensemble methods proposed and implemented in the literature utilize data level .These methods are used to generate different training sets and a learning algorithm , which can be applied to the obtained subsets of data in order to produce multiple hypotheses .", "label": "", "metadata": {}, "score": "36.941025"}
{"text": "24 , no . 2 , pp .123 - 140 , 1996 .Y. Freund and R. E. Schapire , \" Decision - theoretic generalization of on - line learning and an application to boosting , \" Journal of Computer and System Sciences , vol .", "label": "", "metadata": {}, "score": "37.105522"}
{"text": "These heterogeneous features can not be used all together to train a single classifier ( and even if they could - by converting all features into a vector of scalar values - such a training is unlikely to be successful ) .", "label": "", "metadata": {}, "score": "37.14762"}
{"text": "108 - 117 , 2002 .M. S. Kamel and N. M. Wanas , \" Data dependence in combining classifiers , \" in Proceedings of 4th International Workshop on Multiple Classifier Systems ( MCS ' 03 ) , T. Windeattand and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "37.156"}
{"text": "This paper presents some interesting results from an empirical study performed on a set of representative datasets using the decision tree learner C4.5 ( Quinlan , 1993 ) .An exponential - like decay in the variance of the edge is observed as the number of boosting trials is increased .", "label": "", "metadata": {}, "score": "37.297474"}
{"text": "Even with the linear tree exploration , however , training the decision tree can be time consuming and computationally expensive .We propose a technique that replaces the SVM decision tree structure proposed in [ 6 ] with a classification frame work based on boosting .", "label": "", "metadata": {}, "score": "37.332916"}
{"text": "A weakness of this approach , on the other hand , is the very \" hands - on \" tuning and feature selection that is required to obtain good generalization .Simply put , this method obtains a more informed set of features and provides better results for that reason .", "label": "", "metadata": {}, "score": "37.404423"}
{"text": "Diversity among the base classifiers can be measured by implicit or explicit methods [ 47 , 49 ] .In order to evaluate the performance , different performance metrics can be computed based upon benchmarked datasets .Discussion .Over the past decade , ID based upon ensemble approaches has been a widely studied topic , being able to satisfy the growing demand of reliable and intelligent IDS .", "label": "", "metadata": {}, "score": "37.557182"}
{"text": "Many researchers suggested the use of feature selection / reduction techniques [ 16 , 122 ] .These techniques help remove irrelevant and redundant features and identify appropriate features for intrusion detection .The reduction in features reduces the amount of audit data for effective IDS .", "label": "", "metadata": {}, "score": "37.55836"}
{"text": "This can be done by using ( 1 ) different initialization parameters of base classifiers ; or ( 2 ) different subsets of feature space ( feature level ) ; or ( 3 ) different data subsets ( data level ) to train the base classifiers .", "label": "", "metadata": {}, "score": "37.616814"}
{"text": "To ensure that individual boundaries are adequately different , despite using substantially similar training data , weaker or more unstable classifiers are used as base models , since they can generate suffi - ciently different decision boundaries even for small perturbations in their training parameters .", "label": "", "metadata": {}, "score": "37.96"}
{"text": "The smaller subset of classifiers may be selected according to clustering [ 91 ] or by selecting the classifiers whose performance exceeds specific threshold values .However , in the general literature on classifier combination , it is observed that there is no evidence supporting the use of base classifiers of the same type or different types [ 33 , 46 ] .", "label": "", "metadata": {}, "score": "38.22945"}
{"text": "In order for this process to be effective , the individual experts must exhibit some level of diversity among themselves , as described later in this article in more detail .Within the classification context , then , the diversity in the classifiers - typically achieved by using different training parameters for each classifier - allows individual classifiers to generate different decision boundaries .", "label": "", "metadata": {}, "score": "38.28873"}
{"text": "Quinlan , J. ( 1996 ) .Boosting first order learning .In Proceedings of the Seventh International Workshop on Algorithmic Learning Theory ( S. Arikawa and A. Sharma , eds . )Lecture Notes in Artificial Intelligence 1160 143 - 155 .", "label": "", "metadata": {}, "score": "38.45668"}
{"text": "If this set of classifiers is fixed , the problem focuses on the ensemble integration phase .It is also possible to use a fixed combiner and optimize the set of input classifiers ; the problem focuses on the generation and selection phases .", "label": "", "metadata": {}, "score": "38.51136"}
{"text": "a new explanation for the effectiveness of voting methods .Annals of Statistics 1998 , 26 : 1651 - 1686 .View Article .Schapire RE , Singer Y : Improved Boosting Using Confidence - rated Predictions .Machine Learning 1999 , 37 ( 3 ) : 297 - 336 .", "label": "", "metadata": {}, "score": "38.58392"}
{"text": "A boosting - based system for text categorization .Machine Learning , 39(2/3):135 - 168 .Sebastiani F. ( 2002 ) Machine learning in automated text categorization .ACM Computing Surveys , 34(1):1 - 47 .Yang Y. , Liu X. ( 1999 )", "label": "", "metadata": {}, "score": "38.608543"}
{"text": "Some researchers proposed to use a distributed environment in which each node is assigned a part of dataset .An ensemble method was used to fuse or select predictions .Thirdly , an important feature of IDS is the capability to adapt to dynamic behavior of intrusive and normal traffic .", "label": "", "metadata": {}, "score": "38.739456"}
{"text": "But boosting shifts the emphasis to simply finding rules that do better than chance .A number of studies have shown that boosting is often as accurate or even more accurate than older predictive techniques .In addition , it can be applied to already complex methods , like neural networks and decision trees , to further \" boost \" their accuracy .", "label": "", "metadata": {}, "score": "38.749832"}
{"text": "View Article PubMed .Freund Y , Schapire R : A decision - theoretic generalization of on - line learning and an application to boosting .Journal of Computer and System Sciences 1997 , 55 : 119 - 139 .View Article .", "label": "", "metadata": {}, "score": "38.766953"}
{"text": "The methods are in general characterized by a two step approach : first , at learning of the classes as a set of independent classification problems ; second , at combination of the predictions by exploiting the associations between classes that describe the hierarchy .", "label": "", "metadata": {}, "score": "38.80993"}
{"text": "Further details can be studied in [ 4 , 5 ] .Since the first introduction , IDSs have been evaluated using a number of different ways based upon evaluation datasets [ 6 ] .Various features of IDS can be evaluated , which may range from performance and correctness to usability .", "label": "", "metadata": {}, "score": "38.8293"}
{"text": "However , the practice of AI - based classifiers reveals that each of them has advantages and disadvantages for intrusion detection .Ensemble has the power to combine the strengths of these classifiers in such a way that their disadvantages will be compensated , thus offering better solutions .", "label": "", "metadata": {}, "score": "38.846645"}
{"text": "The combination of the classifiers is then based on the given instance : the classifier trained with data closest to the vicinity of the instance , according to some distance metric , is given the highest credit .One or more local experts can be nominated to make the decision ( Jacobs 1991 , Woods 1997 , Alpaydin 1996 , Giacinto 2001 ) .", "label": "", "metadata": {}, "score": "38.916786"}
{"text": "Finally , diversity can also be achieved by using different features , or different subsets of existing features .In fact , generating different classifiers using random feature subsets is known as the random subspace method ( Ho 1998 ) , as described later in this article .", "label": "", "metadata": {}, "score": "39.00076"}
{"text": "They used fuzzy clustering technique to generate different homogeneous training subsets from heterogeneous training set , which are further used to ANN models as base models .Finally , a metalearner , fuzzy aggregation module , was employed to aggregate these results .", "label": "", "metadata": {}, "score": "39.100986"}
{"text": "Machine Learning 11 63 - 90 .proposed by Freund and Mason ( 1999 ) .They represent decision trees as sums of very simple functions and use boosting to simultaneously learn both the decision rules and the way to average them .", "label": "", "metadata": {}, "score": "39.10875"}
{"text": "However , such a system is useless .For this reason , recall , precision and F1 are more commonly used instead of accuracy and error in text categorization evaluations .In multi - label classification , the simplest method for computing an aggregate score across categories is to average the scores of all binary task .", "label": "", "metadata": {}, "score": "39.125153"}
{"text": "Other reasons for combining different classifiers include [ 26 ] the following .( 1 ) a designer may have access to a number of different classifiers , each developed in a different context and for an entirely different representation / description of the same problem .", "label": "", "metadata": {}, "score": "39.260662"}
{"text": "They proposed different concepts to describe the improved performance , reduced generalization error , and successful applications of ensembles to different fields over individual classifier .For example , Allwein et al .[59 ] interpreted the improved performance in the framework of large margin classifiers , Kleinberg in the reference of Stochastic Discrimination theory [ 60 ] , and Breiman in the terms of the bias variance analysis [ 61 ] .", "label": "", "metadata": {}, "score": "39.29232"}
{"text": "For the two - class problem , boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion .We develop more direct approximations and show that they exhibit nearly identical results to boosting .", "label": "", "metadata": {}, "score": "39.395653"}
{"text": "Stacked Generalization .In Wolpert 's stacked generalization ( or stacking ) , an ensemble of classifiers is first trained using bootstrapped samples of the training data , creating Tier 1 classifiers , whose outputs are then used to train a Tier 2 classifier ( meta - classifier ) ( Wolpert 1992 ) .", "label": "", "metadata": {}, "score": "39.503914"}
{"text": "Processing of high - dimensional data for ID is highly computationally expensive .This cause may lose real - time capability of IDS .The computation overhead may be reduced by applying feature reduction techniques , which can be further explored in [ 15 , 16 ] .", "label": "", "metadata": {}, "score": "39.541653"}
{"text": "\\ ) Empirical evaluations have shown the performance of those non - linear classifiers comparable to stronger linear classifiers ( Lewis , 1994 ; Yang , 1994 , 1999 ; Wiener et al . , 1995 ; Joachims , 1998 , Li & Yang , 2003 ) .", "label": "", "metadata": {}, "score": "39.57372"}
{"text": "But , still some research issues exist .The major issues include diversity among the base classifiers , ensemble size , computational overhead , input feature space , and combining strategy .Ensemble Classifiers .The ensembles involve the employment of multiple base classifiers and combine their predictions to obtain reliable and more accurate predictions .", "label": "", "metadata": {}, "score": "39.59623"}
{"text": "When the number of categories reaches the magnitude of tens of thousands or higher , the conventional approach of using all the documents to train a two - way classifier per category is no longer computationally feasible .Liu et al .", "label": "", "metadata": {}, "score": "39.602486"}
{"text": "In his 2000 review article , Dietterich lists three primary reasons for using an ensemble based system : i ) statistical ; ii ) computational ; and iii ) representational ( Dietterich 2000 ) .Note that these reasons are similar to those listed above .", "label": "", "metadata": {}, "score": "39.65364"}
{"text": "Known as an instance - based or lazy learning method , kNN uses the k nearest neighbors of each new document in the training set to estimate the local likelihood of each category for the new document .Just as in the case of linear classifiers , it is important to find the right trade - off between empirical risk and model complexity for non - linear classifiers as well .", "label": "", "metadata": {}, "score": "39.723095"}
{"text": "But , computations of multiple predictions in ensembles increase computational overhead .Many researchers and practitioners advocate ensemble classifiers by keeping following points in mind .( 1 )The availability of enormous computational power ( to cope up computational overhead of ensemble classifiers ) ; ( 2 ) lack of quality training data for realistic evaluation ; ( 3 ) improved performance ( over single classifier ) of ensembles .", "label": "", "metadata": {}, "score": "39.99886"}
{"text": "Kuncheva [ 46 ] proposed a basic taxonomy for generating a diverse pool of classifiers .She proposed that diverse classifiers can be generated by using various methods at four different levels , namely , ( 1 ) combination level ; ( 2 ) classifier level ; ( 3 ) feature level ; ( 4 ) data level .", "label": "", "metadata": {}, "score": "40.008892"}
{"text": "Mach .Learning 40 139 - 158 .Wheway , V. ( 1999 ) .Variance reduction trends on ' boosted ' classifiers .Available from virg@cse . unsw.edu.au .Friedman ( 1999 ) .It does not involve any \" reweighting \" .", "label": "", "metadata": {}, "score": "40.025433"}
{"text": "Many researchers proposed fuzzy set theory to combine base classifiers using fuzzy aggregation connectives to determine ensemble prediction [ 76 , 77 ] .Fuzzy combination methods are effective as they measure the strength of every subset of classifiers .Thus to determine the class of any unclassified instance is the decision of ensemble which is based upon competence of every subset of based classifiers [ 50 ] .", "label": "", "metadata": {}, "score": "40.028038"}
{"text": "Citation .Friedman , Jerome ; Hastie , Trevor ; Tibshirani , Robert .Additive logistic regression : a statistical view of boosting ( With discussion and a rejoinder by the authors ) .Ann .Statist .28 ( 2000 ) , no . 2 , 337 - -407 .", "label": "", "metadata": {}, "score": "40.052322"}
{"text": "This is perhaps the primary reason why ensemble based systems are used in practice : what is the most appropriate classifier for a given classification problem ?The most commonly used procedure - choosing the classifiers with the smallest error on training data - is unfortunately a flawed one .", "label": "", "metadata": {}, "score": "40.06198"}
{"text": "Managing Gigabytes ( 2 edn . )San Francisco : Morgan - Kaufmann Publishers , Inc. .Zhang , T. , & Oles , F. J. ( 2001 ) .Text categorization based on regularized linear classification methods .Information Retrieval , 4 :1 , 5 - 31 .", "label": "", "metadata": {}, "score": "40.120438"}
{"text": "They found that these multistrategy techniques , particularly the belief function , performed better than all three neural nets individually .The overall performance was also comparable to or better than a single neural net trained on the entire feature set ; however , the single neural net did a better job identifying previously unseen attacks .", "label": "", "metadata": {}, "score": "40.208042"}
{"text": "Some of these combination rules operate on class labels only , whereas others need continuous outputs that can be interpreted as support given by the classifier to each of the classes .Xu et al ( Xu 1992 ) .defines three types of base model outputs to be used for classifier combination .", "label": "", "metadata": {}, "score": "40.353424"}
{"text": "Kruegel et al .[109 ] proposed a multimodel approach that uses a number of different anomaly detection techniques ( Bayesian technique ) to detect attacks against web servers and web - based applications .The multimodels help to reduce the vulnerability of the detection process with respect to mimicry attacks .", "label": "", "metadata": {}, "score": "40.435127"}
{"text": "Combining multiple classifiers is an effective technique for improving accuracy .There are many general combining algorithms , such as Bagging , Boosting , or Error Correcting Output Coding , that significantly improve classifiers like decision trees , rule learners , or neural networks .", "label": "", "metadata": {}, "score": "40.46244"}
{"text": "Combining multiple classifiers is an effective technique for improving accuracy .There are many general combining algorithms , such as Bagging , Boosting , or Error Correcting Output Coding , that significantly improve classifiers like decision trees , rule learners , or neural networks .", "label": "", "metadata": {}, "score": "40.46244"}
{"text": "Using this training sample , a supervised learning algorithm aims to find the optimal classification rule , i.e. , a function mapping from the p - dimensional feature space to the one - dimensional class label .Optimal generally means that the classification rule can both accurately classify training documents and generalize well to new documents beyond the training set .", "label": "", "metadata": {}, "score": "40.57737"}
{"text": "These approaches combine complementary multiple classifiers .They use combined knowledge to meet the challenges of ID - like high false alarm rate , low detection accuracy , and better performance in lack of sufficient amount of quality training dataset .The results of ensemble approach are proved to be improved than single best classifier .", "label": "", "metadata": {}, "score": "40.639343"}
{"text": "Similar approach of multiclassifier systems is also advocated by Sabhnani and Serpen [ 23 ] by combining three different machine learning techniques , namely , an ANN , k - means clustering , and a Gaussian classifier .However , they do not provide further implementation details about training the classifiers , nor about determining output of the ensemble .", "label": "", "metadata": {}, "score": "40.731594"}
{"text": "In the hierarchical architecture , individual classifiers are combined into a structure , which is similar to that of a decision tree classifier .The tree nodes , however , may now be associated with complex classifiers demanding a large number of features .", "label": "", "metadata": {}, "score": "40.84297"}
{"text": "For example , a series of multilayer perceptron ( MLP ) neural networks can be trained by using different weight initializations , number of layers / nodes , error goals , etc .Adjusting such parameters allows one to control the instability of the individual classifiers , and hence contribute to their diversity .", "label": "", "metadata": {}, "score": "40.85725"}
{"text": "Especially , the ensemble learning algorithms such as bagging and adaboost are shown to be superior to a single classifier [ 26 , 27 ] .In this study , a combination of four different methods was proposed for feature extraction from CT images .", "label": "", "metadata": {}, "score": "40.958916"}
{"text": "Sometimes feature selection methods ( e.g. selecting the features with highes document frequency or mutual information ) are used to reduce dimensionality .It is useful to differentiate text classification problems by the number of classes a document can belong to .", "label": "", "metadata": {}, "score": "41.024506"}
{"text": "36 , no . 1 - 2 , pp .105 - 139 , 1999 .View at Google Scholar \u00b7 View at Scopus .R. Maclin and D. Opitz , \" Empirical evaluation of bagging and boosting , \" in Proceedings of the 14th National Conference on Artificial Intelligence ( AAAI ' 97 ) , pp .", "label": "", "metadata": {}, "score": "41.227165"}
{"text": "There are many general combining algorithms , such as Bagging or Error Correcting Output Coding , that significantly improve classifiers like decision trees , rule learners , or neural networks .Unfortunately , many combinin ... \" .Combining multiple classifiers is an effective technique for improving accuracy .", "label": "", "metadata": {}, "score": "41.253136"}
{"text": "Another way of averaging is to sum over TP , FP , TN , FN and N over all the categories first , and then compute each of the above metrics .The resulted scores are called micro - averaged .Macro - averaging gives an equal weight to each category , and is often dominated by the system 's performance on rare categories ( the majority ) in a power - law like distribution .", "label": "", "metadata": {}, "score": "41.255306"}
{"text": "They reported that the proposed method provides significant improvement of prediction accuracy in ID .Muda et al .[120 ] proposed a combined approach of clustering and classification .The clustering is performed by using K - means algorithm to form groups of similar data in earlier stage .", "label": "", "metadata": {}, "score": "41.292236"}
{"text": "Many researchers proposed use of population - based approaches to generate diverse set of classifiers .Although some promising results have been achieved by current AI - based ensembles to IDSs , there are still challenges that lie ahead for researchers in this area .", "label": "", "metadata": {}, "score": "41.35891"}
{"text": "418 - 435 , 1992 . E. Allwein , R. E. Schapire , and Y. Singer , \" Reducing Multiclass to Binary : A Unifying Approach for Margin Classifiers , \" Journal of Machine Learning Research , vol .1 , pp .", "label": "", "metadata": {}, "score": "41.366566"}
{"text": "Eventually , the results from all trials are combined and the program \" votes \" on the final answer .In racetrack prediction , boosting might work as follows : A programmer would feed the boosting algorithm historical data on the race , the horses and their records , in addition to a number of rules of thumb , to be executed in order .", "label": "", "metadata": {}, "score": "41.444176"}
{"text": "The predictions are combined by a majority vote .The performance of RF is comparable to AdaBoost , but is more robust to noise [ 57 , 95 ] .( iv ) Boosting .Boosting [ 96 ] is popular meta - algorithm for generating ensemble [ 33 ] .", "label": "", "metadata": {}, "score": "41.481342"}
{"text": "In Machine Learning : Proceedings of the Thirteenth International Conference 148 - 156 .Morgan Kaufman , San Francisco . in a paper by Amit and Geman ( 1997 ) .Using this approach and 100 iterations gives the following test - set errors as compared to the best corresponding values for LogitBoost .", "label": "", "metadata": {}, "score": "41.52921"}
{"text": "While decision trees , logical rules , and instance - based rules have been explored for text classification , the most commonly used type of classification rules are linear rules .Often an additional feature that always takes value 1 is added to simulate a constant offset .", "label": "", "metadata": {}, "score": "41.61579"}
{"text": "( 2 ) Some times more than a single training set is available , each collected at a different time or in a different environment .These training sets may even use different features .( 3 ) Different classifiers trained on the same data may not only differ in their global performances , but they also may show strong local differences .", "label": "", "metadata": {}, "score": "41.62535"}
{"text": "945 - 954 , 1995 .K. Woods , W. P. J. Kegelmeyer , and K. Bowyer , \" Combination of multiple classifiers using local accuracy estimates , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .19 , no .", "label": "", "metadata": {}, "score": "41.69191"}
{"text": "401 - 407 , 2001 .L. I. Kuncheva , \" Switching between selection and fusion in combining classifiers : An ex - periment , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B : Cybernetics , vol .", "label": "", "metadata": {}, "score": "41.70115"}
{"text": "Apte , C. , Damerau , F. , & Weiss , S. ( 1998 ) .Text mining with decision rules and decision trees .Conference Proceedings The Conference on Automated Learning and Discovery , CMU .Aslam , J. ( 2000 ) .", "label": "", "metadata": {}, "score": "41.78305"}
{"text": "Fairfax , VA . .Breiman , L. ( 1999 ) .Using adaptive bagging to debias regressions .Technical Report 547 , Dept .Statistics , Univ .California , Berkeley .Freund , Y. ( 1999 ) .An adaptive version of boost by majority algorithm .", "label": "", "metadata": {}, "score": "41.80214"}
{"text": "They claimed 94.71 % detection accuracy with 3.8 % of false alarm rate of old as well as new attacks .Chen et al .[ 41 ] suggested a hybrid flexible neural - tree - based IDS based on flexible neural tree , evolutionary algorithm , and particle swarm optimization ( PSO ) .", "label": "", "metadata": {}, "score": "41.831963"}
{"text": "But , availability of irrelevant and redundant feature affects detection performance of classifiers .Homogeneous ensembles focus on different features of training dataset and/or different training subsets and/or other ways to generate diverse base classifiers .Applications of the AI - based ensembles revealed that they have pros and cons .", "label": "", "metadata": {}, "score": "41.908497"}
{"text": "However , this purpose of dataset is also under criticism [ 121 ] .Therefore , using these datasets only is not sufficient to reveal the efficiency of a learning algorithm .So , new and good quality benchmark datasets need to be developed for realistic evaluation of IDS .", "label": "", "metadata": {}, "score": "41.97962"}
{"text": "Some IDSs have been developed based on single - classification technique while other IDSs ( called hybrid / ensemble IDS ) implement more - than - one - classfication technique .Ensemble - based IDSs have many advantages over the IDS implementing single technique ( refer to Section 2 ) .", "label": "", "metadata": {}, "score": "42.09195"}
{"text": "So that , a .-dimensional feature vector was obtained .In this way , at least 99 % value of the total variance for each pattern was taken into account .To select the best features that contribute to the performance of classification system in the training set , the mRMR method was utilized .", "label": "", "metadata": {}, "score": "42.218277"}
{"text": "Two recent tutorials written by the current curator of this article also provide a comprehensive overview of ensemble systems ( Polikar 2006 , Polikar 2007 ) .Diversity .The success of an ensemble system - that is , its ability to correct the errors of some of its members - rests squarely on the diversity of the classifiers that make up the ensemble .", "label": "", "metadata": {}, "score": "42.28617"}
{"text": "This update rule ensures that the weights of all correctly classified instances and the weights of all misclassified instances always add up to \\ ( 1/2\\ .\\ )More specifically , the requirement for the training error of the base classifier to be less than \\ ( 1/2\\ ) forces teh algorithm to correct at least one mistake made by the previous base model .", "label": "", "metadata": {}, "score": "42.313576"}
{"text": "In other words , it simulates , albeit in a more sophisticated and accurate manner , human intuition .\" Philosophically it 's an interesting result , \" said Robert Schapire , who has done seminal work on boosting with fellow AT&T researchers Yoav Freund and Yoram Singer .", "label": "", "metadata": {}, "score": "42.357372"}
{"text": "As a result of the use of primitives in the set of learners , one poor feature can not affect a good feature just because they are both being used to learn a weak classifier at the same time .The classification performance as more types of features are added can be seen in Figure 11 .", "label": "", "metadata": {}, "score": "42.436348"}
{"text": "Selection is guaranteed by design to give at least the same training accuracy as the best individual classifier .However , the model might overtrain , giving a deceptively low training error .To guard against overtraining we may use confidence intervals and nominate a classifier only when it is significantly better than the others [ 46 ] .", "label": "", "metadata": {}, "score": "42.515858"}
{"text": "The optimal value of \u03bb is typically determined via cross - validation .To analyze the differences and similarities among popular classifiers , let us look at three of the more successful linear classification methods in text categorization as examples : linear SVM , ridge regression and regularized logistic regression .", "label": "", "metadata": {}, "score": "42.59088"}
{"text": "A comparison of the performance of reported CAD systems was shown in Table 4 .As seen from the table , the proposed classification approach achieved a sensitivity of 89.6 % and an accuracy of 90.7 % in the range of 2 - 20 mm nodule size .", "label": "", "metadata": {}, "score": "42.621365"}
{"text": "Friedman , J. H. and Hall , P. ( 1999 ) .On bagging and nonlinear estimation .J. Comput .Graph .Statist .To appear .Grove , A. and Schuurmans , D. ( 1998 ) .Boosting in the limit : maximizing the margin of learned ensembles .", "label": "", "metadata": {}, "score": "42.639626"}
{"text": "The selection approach , specially its dynamic form , selects one ( or more ) classifier from the ensemble according to the prediction performance of these classifiers on similar data from the validation set .The ensemble integration phase involves many strategies to combine multiple predictions because these strategies have performance variability when tackling different problems .", "label": "", "metadata": {}, "score": "42.69577"}
{"text": "However , especially , in the hybrid approach ( method 4 ) combining the best features of the three methods , nonlinear multilayered ANN is shown to be superior to the other classifiers .Our approach uses ANN classifier with fewer features to avoid generalization problems , high complexity , and computational burden that can be caused by using an ANN with very large number of ( potentially irrelevant ) features .", "label": "", "metadata": {}, "score": "42.712032"}
{"text": "Selecting the best features of the above three methods with mRMR ( minimum Redundancy Maximum Relevance ) method , hybrid features are obtained by combining the best features .To perform a rigorous validation with the proposed system , completely independent training and testing datasets are utilized .", "label": "", "metadata": {}, "score": "42.74786"}
{"text": "Therefore , individual classifiers in an ensemble system need to make different errors on different instances .The intuition , then , is that if each classifier makes different errors , then a strategic combination of these classifiers can reduce the total error , a concept not too dissimilar to low pass filtering of the noise .", "label": "", "metadata": {}, "score": "42.804806"}
{"text": "This process is recursively repeated [ 85 ] .( iii ) Dynamic Classifier Selection Method .This method measure the competence of each base classifier to determine the prediction of ensemble classifier .The competence of base classifier can be determined dynamically either by using prior information about base classifiers or by posterior information produced by them in terms of their predictions [ 86 , 87 ] .", "label": "", "metadata": {}, "score": "42.818592"}
{"text": "Wadsworth , Belmont , CA .Dietterich , T. ( 1998 ) .An experimental comparison of three methods for constructing ensembles of decision trees : bagging , boosting , and randomization .Machine Learning ?Freund , Y. ( 1995 ) .", "label": "", "metadata": {}, "score": "43.08055"}
{"text": "119 - 139 , 1997 .J. Kittler , M. Hatef , R. P. W. Duin , and J. Mates , \" On combining classifiers , \" IEEE Trans . on Pattern Analysis and Machine Intelligence , vol .20 , no . 3 , pp .", "label": "", "metadata": {}, "score": "43.0899"}
{"text": "When the amount of training data is too large to make a single classifier training difficult , the data can be strategically partitioned into smaller subsets .Each partition can then be used to train a separate classifier which can then be combined using an appropriate combination rule ( see below for different combination rules ) .", "label": "", "metadata": {}, "score": "43.119877"}
{"text": "These metrics depend on the ordering of the cases , not the actual predicted values .As long as ordering is preserved , it makes no difference .These metrics measure how well the attack instances are ordered before normal instances and can be viewed as a summary of model performance across all possible thresholds .", "label": "", "metadata": {}, "score": "43.206924"}
{"text": "and then generates more features from the existing set of features by applying wavelet filters and principal component analysis on the original features ( which partly recovers transition probability information lost in the feature compression used in [ 6 ] ) .", "label": "", "metadata": {}, "score": "43.37204"}
{"text": "Another benefit of bagging methods is that they are parallel in nature in both the training and classification phases , whereas the boosting method is sequential in nature [ 33 ] .AI Based Ensembles for ID .Many researchers employed AI - based ensembles and hybrid approaches to improve performance of IDS .", "label": "", "metadata": {}, "score": "43.44417"}
{"text": "Here , each member is trained by different dataset .Ensemble output is determined by one classifier .In nutshell , fusion based combination methods combine all the outputs of the base classifiers , whereas selection based combination methods try to choose the best classifiers among the set of the available base classifiers .", "label": "", "metadata": {}, "score": "43.494354"}
{"text": "Text classification rules are typically evaluated using performance measures from information retrieval .Common metrics for text categorization evaluation include recall , precision , accuracy and error rate and F1 .Given a test set of N documents , a two - by - two contingency table with four cells can be constructed for each binary classification problem .", "label": "", "metadata": {}, "score": "43.51739"}
{"text": "e misclassi ed points in the training set and then combine the predictions of several classi ers .Adaptive resampling methods like boosting are also useful in selecting relevant examples even though their original goal was to improve the performance of weak learning algorithms [ 14].", "label": "", "metadata": {}, "score": "43.53297"}
{"text": "34 , no .4 , pp .369 - 374 , 1998 .View at Google Scholar .G. Giacinto and F. Roli , \" Dynamic classifier fusion , \" in Proceedings of the Multiple Classifier Systems .First International Workshop ( MCS ' 00 ) , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "43.558983"}
{"text": "Everything else being equal , one may be tempted to choose at random , but with that decision comes the risk of choosing a particularly poor model .Using an ensemble of such models - instead of choosing just one - and combining their outputs by - for example , simply averaging them - can reduce the risk of an unfortunate selection of a particularly poorly performing classifier .", "label": "", "metadata": {}, "score": "43.615208"}
{"text": "Certain problems are just too difficult for a given classifier to solve .In fact , the decision boundary that separates data from different classes may be too complex , or lie outside the space of functions that can be implemented by the chosen classifier model .", "label": "", "metadata": {}, "score": "43.768158"}
{"text": "Many researchers also used these methods to reduce the false alarms by correlating similar alarms [ 18 , 88 ] .These methods are employed to analyze root cause of false alarms [ 17 ] .( v ) Statistical Selection Method .", "label": "", "metadata": {}, "score": "43.859177"}
{"text": "In the latter case , classifier outputs are often normalized to the [ 0 , 1 ] interval , and these values are interpreted as the support given by the classifier to each class , or as class - conditional posterior probabilities .", "label": "", "metadata": {}, "score": "43.881786"}
{"text": "Xiang et al .[36 ] proposed a hierarchical hybrid system involving multiple - level hybrid classifier , which combines the supervised decision tree classifiers and unsupervised Bayesian clustering to detect intrusions .It was able to achieve a higher true positive rate than previously reported in the literature on the original training and test sets of the KDD Cup 99 dataset .", "label": "", "metadata": {}, "score": "43.91251"}
{"text": "The resulting synergy has been shown to be an effective way for building IDSs with improved performance in terms of detection accuracy and false - positive rate .It may be concluded that by considering appropriate base classifiers , training sample size & combination method , the performance of hybrid classifier / ensemble can be improved .", "label": "", "metadata": {}, "score": "43.985336"}
{"text": "[40 ] suggested a hybrid of SVM and clustering to cut down the training time .A hierarchical clustering algorithm is engaged to establish boundary points in the data that best separate the two classes .These boundary points are used to train the SVM .", "label": "", "metadata": {}, "score": "44.03495"}
{"text": "The approach is based on the motivation that human experts use different feature sets to detect different kinds of attacks .They generated different neural network - based classifiers by training them using different feature subsets of KDD cup 99 dataset , namely , intrinsic , content , and traffic features .", "label": "", "metadata": {}, "score": "44.12617"}
{"text": "Both the experts themselves and the gating network requires the input instances for training .Several mixture - of - experts models can also be further combined to obtain a hierarchical mixture of experts ( Jordan 1994 ) .Mixture of experts are particularly useful when different experts are trained on different parts of the feature space , or when heterogeneous sets of features are available to be used for a data fusion problem .", "label": "", "metadata": {}, "score": "44.18373"}
{"text": "Zainal et al .[35 ] proposed heterogeneous ensemble of linear genetic programming ( LGP ) , adaptive neural fuzzy inference system ( ANFIS ) , and random forest ( RF ) for ID .Base classifiers are generated by using class - specific features of KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "44.202225"}
{"text": "Selected subsets of feature are used to train accurate and diverse base classifiers .Final ensemble is constructed by using ensemble selection method .They reported improvement of ID in terms of detection rate and false - positive rate over other related approaches .", "label": "", "metadata": {}, "score": "44.26397"}
{"text": "Many researchers applied and evaluated AI - based techniques using different evaluation datasets for ID .They reported many challenges related to AI - based techniques and dataset for ID .Each technique may have its own region in the feature space where it performs the best [ 26 ] .", "label": "", "metadata": {}, "score": "44.323875"}
{"text": "The proposed system suffers from limitation of incremental learning .It requires continuous retraining for changing environment .Cretu et al .[113 ] proposed a micromodel - based ensemble of anomaly sensors to sanitize the training data .Here , different models are generated to produce provisional labels for each training input , and models are combined in a voting scheme to determine which parts of the training data may represent attacks .", "label": "", "metadata": {}, "score": "44.34945"}
{"text": "The training data subset for the second classifier \\(C_2\\ ) is chosen as the most informative subset , given \\(C_1\\ .\\ ) Specifically , \\(C_2\\ ) is trained on a training data only half of which is correctly classified by \\(C_1\\ , \\ ) and the other half is misclassified .", "label": "", "metadata": {}, "score": "44.37713"}
{"text": "AI - based techniques and their ensembles are presently attracting considerable attention from the research community for intrusion detection .Their features , such as flexibility , adaptability , new pattern recognition , fault tolerance , learning capabilities , high computational speed , and error resilience for noisy data , fit the prerequisite of building effective IDS .", "label": "", "metadata": {}, "score": "44.441433"}
{"text": "L. I. Kuncheva , J. C. Bezdek , and R. Duin , \" Decision templates for multiple classifier fu - sion : an experimental comparison , \" Pattern Recognition , vol .34 , no . 2 , pp .299 - 314 , 2001 .", "label": "", "metadata": {}, "score": "44.467064"}
{"text": "The system was tested on data gathered at Google , Inc. and two universities in USA and Europe , showing promising results .However , they used anomaly detection technique ( Bayesian technique ) to model attribute inputs without taking into account typical semantic differences between classes of characters ( alphabetic , numeric , and non - alphanumeric ) , which usually determine their meaning .", "label": "", "metadata": {}, "score": "44.514465"}
{"text": "This decision is reached by calculating a number of anomaly scores : one for the query itself and one for each attribute .A query is reported as anomalous if at least one of these anomaly scores is above the corresponding detection threshold .", "label": "", "metadata": {}, "score": "44.527103"}
{"text": "The possibility of boosting was first suggested by Leslie Valiant and Michael Kearns in a 1988 paper presented at Harvard University .The first algorithms were developed within a year or two , and experiments began in 1992 .Much work has come from AT&T 's laboratories .", "label": "", "metadata": {}, "score": "44.597324"}
{"text": "Troika combines base classifiers in three stages : specialists level , metaclassifiers , and super classifier .In order to conclude which ensemble performs best over multiple datasets , they followed the procedure proposed in [ 118 ] .Wang et al .", "label": "", "metadata": {}, "score": "44.614357"}
{"text": "10 , pp .993 - 1001 , 1990 .R. E. Schapire , \" The Strength of Weak Learnability , \" Machine Learning , vol .5 , no . 2 , pp .197 - 227 , 1990 .R. A. Jacobs , M. I. Jordan , S. J. Nowlan , and G. E. Hinton , \" Adaptive mixtures of local ex - perts , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "44.61855"}
{"text": "Learn + + primarily for incremental learning problems that do not introduce new classes ( Polikar2001 ) , and Learn + + .NC for those that introduce new classes with additional datasets ( Muhlbaier 2008 ) are two examples of ensemble based incremental learning algorithms .", "label": "", "metadata": {}, "score": "44.693108"}
{"text": "In order to solve these problems , many researchers utilized AI - based ensembles for ID successfully .They proved that AI - based ensembles can improve detection performance over a single technique / classifier [ 27 - 29 ] .The concept of ensemble is to employ multiple base classifiers and their individual predictions are combined in some way to obtain reliable and more accurate predictions [ 17 , 25 ] .", "label": "", "metadata": {}, "score": "44.7352"}
{"text": "The explosive growth in data warehousing and internet usage has made large amounts of data potentially available for developing classi cation models ... \" .Classi cation modeling ( a.k.a . supervised learning ) is an extremely useful analytical technique for developing predictive and forecasting applications .", "label": "", "metadata": {}, "score": "44.8208"}
{"text": "R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .4 , pp .59 - 72 , 2007 .", "label": "", "metadata": {}, "score": "44.973396"}
{"text": "Various classification techniques ( classifiers ) from different disciplines have been applied to detect the intrusions efficiently .Examples of these techniques include statistical techniques , artificial - intelligence- ( AI- ) based techniques , and its subfield techniques [ 5 , 19 , 20 ] .", "label": "", "metadata": {}, "score": "45.028793"}
{"text": "T.K. Ho , \" Complexity of classification problems and comparative advantages of combined classifiers , \" Int .Workshop on Multiple Classifier Systems , lecture Notes on Computer Science , Vol .1857 , pp .97 - 106 , 2000 , Springer- Verlag . F. Roli , G. Giacinto , \" Design of Multiple Classifier Systems , \" in H. Bunke and A. Kandel ( Eds . )", "label": "", "metadata": {}, "score": "45.04803"}
{"text": "Section 3 lists the reasons and benefits for combining multiple base classifiers .Various taxonomies proposed in the literature are presented in Section 4 .The section also describes various methods used at different levels to generate ensembles .Section 5 highlights various AI - based ensembles proposed for ID during the last decade .", "label": "", "metadata": {}, "score": "45.088768"}
{"text": "In supervised learning - based classification , ensembles have been successfully employed to different application domains .In the literature , many researchers have proposed different ensembles by considering different combination methods , training datasets , base classifiers , and many other factors .", "label": "", "metadata": {}, "score": "45.17222"}
{"text": "Peng , F. Long , and C. Ding , \" Feature selection based on mutual information : criteria of max - dependency , max - relevance , and min - redundancy , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "45.25678"}
{"text": "Different ensemble members were generated by training from reduced dataset .The final outputs were decided as follows : each classifier 's output is given a weight ( 0 - 1 scale ) depending on the generalization performance during the training process .", "label": "", "metadata": {}, "score": "45.27243"}
{"text": "Incremental learning refers to the ability of an algorithm to learn from new data that may become available after a classifier ( or a model ) has already been generated from a previously available dataset .Hence , an incremental learning algorithm must learn the new information , and retain previously acquired knowledge , without having access to previously seen data .", "label": "", "metadata": {}, "score": "45.29116"}
{"text": "The algorithms described above have their built in combination rules , such as simple majority voting for bagging , weighted majority voting for AdaBoost , a separate classifier for stacking , etc .However , an ensemble of classifiers can be trained simply on different subsets of the training data , different parameters of the classifiers , or even with different subsets of features as in random subspace models .", "label": "", "metadata": {}, "score": "45.31027"}
{"text": "Khreich et al .[ 2 ] proposed an iterative Boolean combination ( IBC ) method to efficiently fuse the predictions from multiple classifiers .The IBC efficiently exploits all Boolean functions applied to the ROC curves and requires no prior assumptions about conditional independence of classifiers or convexity of ROC curves .", "label": "", "metadata": {}, "score": "45.318172"}
{"text": "N. C. Oza and K. Tumer , \" Input Decimation Ensembles : Decorrelation through Dimensio - nality Reduction , \" 2nd Int .Workshop on Multiple Classifier Systems , in Lecture Notes in Computer Science , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "45.401375"}
{"text": "This level focuses on ensemble generation phase of ensemble learning process .Here , pool of classifiers is generated by using different feature subsets of dataset for the training of the base classifiers .Basic reason behind this level is to improve the computational efficiency of the ensemble and to increase the accuracy [ 46 ] .", "label": "", "metadata": {}, "score": "45.498917"}
{"text": "In the literature , various classification algorithms for CAD systems have been extensively studied .In order to reduce the complexity of the algorithm and the computational load , the use of fewer features is extremely important , while maintaining an acceptable detection performance .", "label": "", "metadata": {}, "score": "45.55676"}
{"text": "Final ensemble prediction is the weighted voting of base classifiers .They empirically proved that by assigning proper weights to classifiers in ensemble approach improves the detection accuracy of all classes of network traffic than individual classifier .Menahem et al .", "label": "", "metadata": {}, "score": "45.600266"}
{"text": "Here , each member is trained by the same dataset with all features .To determine final prediction of ensemble , the combiner applies some method to combine the predictions of ensemble members in certain way to get final ensemble prediction , for example , average or majority vote ( most popular ) method .", "label": "", "metadata": {}, "score": "45.67154"}
{"text": "Performance Comparison .To evaluate the performance of the classification approach , the results of this study were compared with previously reported CAD systems .It is highly difficult task to make comparison between previously published CAD systems due to different datasets , nodule size or type , and nodule or nonnodule patterns .", "label": "", "metadata": {}, "score": "45.690884"}
{"text": "On the other hand , the purpose of modular systems is to break down a complex problem into several subproblems so that each learning algorithm either solves a different task or trained by different training set .The taxonomy proposed by Sharkey [ 63 ] was further extended by Rokach [ 64 ] .", "label": "", "metadata": {}, "score": "45.72126"}
{"text": "The fuzzy inference module implements nonlinear mappings from the outputs of the neurofuzzy classifiers of the pervious layer to the final output space which specifies if the input data are normal or intrusive .The genetic algorithm is used to optimize the structure of neurofuzzy engine .", "label": "", "metadata": {}, "score": "45.76664"}
{"text": "4 , pp .497 - 508 , 2001 .R. Polikar , \" Bootstrap inspired techniques in computational intelligence : ensemble of classifiers , incremental learning , data fusion and missing features , IEEE Signal Processing Magazine , v. 24 , no .", "label": "", "metadata": {}, "score": "45.836586"}
{"text": "Khreich et al .[ 2 ] proposed an iterative Boolean combination ( IBC ) technique for efficient fusion of the responses from any crisp or soft detector trained on fixed - size datasets in the ROC space .The proposed technique applies all Boolean functions to combine the ROC curves corresponding to multiple classifiers .", "label": "", "metadata": {}, "score": "45.856483"}
{"text": "The best features for each method are determined using the mRMR feature selection only in the training dataset .Then , the classification accuracies of the methods are calculated using these features in the test dataset .In the study , four different methods were proposed .", "label": "", "metadata": {}, "score": "45.874016"}
{"text": "Web - page Taxonomy ( 2004 ) exhibits a power law .The scale of real - world text classification applications , both in terms of the number of classes as well as the ( often highly unbalanced ) number of training examples , poses interesting research challenges .", "label": "", "metadata": {}, "score": "45.96212"}
{"text": "They utilized multiple classifiers and tried to exploit their strengths .They used C4.5 decision tree [ 114 ] , Na\u00efve Bayes [ 115 ] , k - NN clustering [ 116 ] , VFI - voting feature intervals [ 34 ] , and OneR [ 117 ] classifiers as base classifiers over five malware datasets .", "label": "", "metadata": {}, "score": "45.991676"}
{"text": "Therefore , such systems are also known as multiple classifier systems , or just ensemble systems .There are several scenarios where using an ensemble based system makes statistical sense , which are discussed below in detail .For example , we typically ask the opinions of several doctors before agreeing to a medical procedure , we read user reviews before purchasing an item ( particularly big ticket items ) , we evaluate future employees by checking their references , etc .", "label": "", "metadata": {}, "score": "46.068573"}
{"text": "If both classifiers agree then the output is decided accordingly .If there is a conflict then the decision given by the classifier with the highest weight is taken into account .By using hybrid approach , the authors reported that Normal , Probe , and DOS could be detected with 100 % accuracy and U2R and R2L with 84 % and 99.47 % accuracies , respectively .", "label": "", "metadata": {}, "score": "46.16072"}
{"text": "Conference Proceedings Tenth Conference on Uncertainty in Artificial Intelligence , Seattle , WA .Maclin , R. ( 1998 ) .Boosting classifiers locally .Conference Proceedings Proceedings of AAAI .Mason , L. , Bartlett , P. L. , & Baxter , J. ( 2000 ) .", "label": "", "metadata": {}, "score": "46.16078"}
{"text": "Dietterich and Bakiri introduced ECOC to be used within the ensemble setting ( Dietterich 1995 ) .The idea is to use a different class encoding for each member of the ensemble .The encodings constitute a binary C by T code matrix , where C and T are the number of classes and ensemble size , respectively , combined by the minimum Hamming distance rule .", "label": "", "metadata": {}, "score": "46.16414"}
{"text": "Many techniques generate a pool of homogeneous base classifiers , for examples , genetic algorithms .Although in ensembles , the combined knowledge is important , it is very computationally expensive to combine a large number of classifiers from the whole population [ 33 ] .", "label": "", "metadata": {}, "score": "46.285385"}
{"text": "The underlying complex decision boundary can then be approximated by an appropriate combination of different classifiers .In many applications that call for automated decision making , it is not unusual to receive data obtained from different sources that may provide complementary information .", "label": "", "metadata": {}, "score": "46.339462"}
{"text": "Classifier level may consider different models and may design base learners for specific ensemble methods .At third level , different subsets of features can be used for the classifiers .Finally , different data subsets , so that each base classifier in the ensemble is trained on its own data , can be used to build up the committee of learning machines .", "label": "", "metadata": {}, "score": "46.385788"}
{"text": "Hastie , T. and Tibshirani , R. ( 1998 ) .Bayesian backfitting .Technical report , Stanford Univ . .Heikkinen , J. ( 1998 ) .Curve and surface estimation using dynamic step functions .In Practical Nonparametric and Semiparametric Bayesian Statistics ( D. Dey , P. M \u00a8uller and D. Sinha , eds . )", "label": "", "metadata": {}, "score": "46.438267"}
{"text": "113 - 141 , 2001 .View at Google Scholar \u00b7 View at Scopus . A. Sharkey , \" Types of multi - ney systems , \" in Multiple Classifier Systems , Third International Workshop ( MCS ' 02 ) , F. Roli and J. Kittler , Eds . , vol .", "label": "", "metadata": {}, "score": "46.46958"}
{"text": "Hu et al .[112 ] suggested an AdaBoost algorithm - based ensemble which in turn uses decision stump as base classifiers .They utilized continuous and categorical features separately without any forced conversion .The proposed system was evaluated using KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "46.471573"}
{"text": "Kuncheva and Whitaker [ 50 ] proposed different metrics to measure diversity .The diversity in ensembles can be obtained by using different ( 1 ) starting point in hypothesis space ; ( 2 ) set of accessible hypotheses ; ( 3 ) traversal of hypothesis space [ 47 ] .", "label": "", "metadata": {}, "score": "46.488953"}
{"text": "146 - 156 , 2002 .R. E. Schapire , Y. Freund , P. Bartlett , and W. S. Lee , \" Boosting the Margin : A New Explanation for the Effectiveness of Voting Methods , \" Annals of Statistics , vol .", "label": "", "metadata": {}, "score": "46.54131"}
{"text": "This section includes the database information , feature extraction , feature selection , and classifier algorithms .Overall performance of the proposed system as well as comparisons with six other previously presented CAD systems is presented in Section 3 .Conclusions are given in Section 4 .", "label": "", "metadata": {}, "score": "46.567593"}
{"text": "Ridgeway , G. ( 1999 ) .The state of boosting .In Proceedings of the Thirty - first Symposium on the Interface 172 - 181 .Abstract .Schapire and Singer 's improved version of AdaBoost for handling weak hypotheses with confidence rated predictions represents an important advance in the theory and practice of boosting .", "label": "", "metadata": {}, "score": "46.62482"}
{"text": "This handicap of naive Bayes can be attributed to the independence assumption in computing the joint probabilities of features .In the proposed approach , weak learners are obtained using individual features ( feature primitives ) and not a group of features .", "label": "", "metadata": {}, "score": "46.730927"}
{"text": "They proposed to partition the original dataset into two subsets .The first subset is reserved to form the metadataset and the second subset is used to build the base - level classifiers .This classifier ( Metaclassifier ) combines the different predictions into a final one .", "label": "", "metadata": {}, "score": "46.733963"}
{"text": "Now , assume that each classifier has a probability p of making a correct decision .Weighted majority voting .Other combination rules .There are several other combination rules , which are arguably more sophisticated than the ones listed above .", "label": "", "metadata": {}, "score": "46.832108"}
{"text": "If a vast majority of the classifiers agree with their decisions , such an outcome can be interpreted as the ensemble having high confidence in its decision .If , however , half the classifiers make one decision and the other half make a different decision , this can be interpreted as the ensemble having low confidence in its decision .", "label": "", "metadata": {}, "score": "46.844727"}
{"text": "The performances of the proposed approaches are evaluated by using different classifiers and performance metrics such as accuracy , sensitivity , specificity , AUROC , Kappa statistic , and RMSE .The proposed classification approach utilizes 30 features combined by the hybrid approach with sensitivity of 89.6 % , accuracy of 90.7 , and specificity of 87.5 % .", "label": "", "metadata": {}, "score": "46.880936"}
{"text": "Rules of thumb that are only partially accurate can be combined into precise prediction tools using an artificial intelligence technique called boosting .Boosting has two qualities that distinguish it from existing computer prediction methods .While highly accurate , it is simpler to execute than many other programs .", "label": "", "metadata": {}, "score": "46.89619"}
{"text": "The approach proposed by Winters - Hilt et al .provides excellent classification accuracy in classifying DNA hairpins that differ only in one base - pair .This approach requires a decision tree structure consisting of binary classifiers at each node .", "label": "", "metadata": {}, "score": "47.042946"}
{"text": "The basic principle is to evaluate several individual pattern classifiers , and integrate them in order to reach a classification that is better than the one obtained by each of them separately .Our overview focused on supervised AI - based ensemble for intrusion detection proposed in last decade , since historically these were the first to be studied and applied to several application domains .", "label": "", "metadata": {}, "score": "47.09684"}
{"text": "The basic idea is that one can combine rules of thumb to form an ensemble whose joint decision rule has good performance on the training set .Successive component classifiers are trained on a subset of the training data that is most informative .", "label": "", "metadata": {}, "score": "47.104557"}
{"text": "The classifiers are generated by using training on KDD99 dataset .They observed in the experiments that different models provided complementary information about the patterns to be classified .The final prediction of ensemble is computed based upon highest score of base classifiers .", "label": "", "metadata": {}, "score": "47.191093"}
{"text": "Combination Level .This level focuses on ensemble integration phase of ensemble learning process .Here , predictions of base classifiers are combined in some way to improve the performance of ensemble .The researchers proposed that there are three main ways in combining classifiers , namely , fusion , selection , and Mixture of expert systems [ 33 ] .", "label": "", "metadata": {}, "score": "47.205902"}
{"text": "These methods try to choose the best classifiers among the set of the available base classifiers .The final prediction of ensemble is the prediction of selected base classifier or fused prediction of subset of base classifiers as described in aforementioned text .", "label": "", "metadata": {}, "score": "47.215485"}
{"text": "The former approach involves the use of different classifiers to take a unique decision about the data pattern typically related to a single network packet whereas the later approach is mainly aimed at providing a high - level description of the detected pattern / attack by using the outputs of different classifiers / IDS .", "label": "", "metadata": {}, "score": "47.287952"}
{"text": "AI - based techniques and their ensembles can help to address this important issue , but still only a small number of researchers have focused it so far .Most of the methods discussed in this paper have their roots in the ensemble learning process .", "label": "", "metadata": {}, "score": "47.29829"}
{"text": "A linear classifier , one that is capable of learning linear boundaries , can not learn this complex non - linear boundary .However , appropriate combination of an ensemble of such linear classifiers can learn any non - linear boundary .", "label": "", "metadata": {}, "score": "47.3366"}
{"text": "Important methods proposed in the literature are described in the following section .( i )The Test and Select Method .This method describes a greedy method which adds a new classifier to ensemble if it reduces the squared error [ 71 ] .", "label": "", "metadata": {}, "score": "47.42012"}
{"text": "So , for a particular instance to be classified if all of them have different opinions , then their scores are considered .The classifier having the highest score is declared as winner and used to predict the final output of the ensemble .", "label": "", "metadata": {}, "score": "47.447105"}
{"text": "The findings of research work in each study are systematically summarized and compared , which allows us to clearly identify existing research challenges for intrusion detection and underline research directions .It is expected that this paper can serve as a practical channel through the maze of the literature .", "label": "", "metadata": {}, "score": "47.465748"}
{"text": "They reported the better performance of proposed hybrid approach over single Na\u00efve Bayes classifier over KDD 1999 dataset .But the proposed method suffers from limitation that it is unable to detect similar attacks like U2R and R2L. Here , architecture of system can be parallel , cascading , or hierarchical [ 35 ] , the classifiers can be combined by ensemble- or hybrid - combining approach .", "label": "", "metadata": {}, "score": "47.49958"}
{"text": "Lee , J. ( 1999 ) .A computer program that plays a hunch .New York Times August 17 .Quinlan , J. ( 1996 ) .Bagging , boosting , and C4.5 .In Proceedings Thirteenth American Association for Artificial Intelligence National Conference on Artificial Intelligence 725 - 730 .", "label": "", "metadata": {}, "score": "47.5055"}
{"text": "160 - 171 , Berlin , Germany , 1998 .T. G. Dietterich and G. Bakiri , \" Error - correcting output codes : a general method for improving multiclass inductive learning programs , \" in Proceedings of the 9th AAAI National Conference on Artificial Intelligence , pp .", "label": "", "metadata": {}, "score": "47.638775"}
{"text": "J. Friedman and P. Hall , \" On bagging and nonlinear estimation , \" Tech .Rep. , Statistics Department , University of Stanford , Palo Alto , Calif , USA , 2000 .View at Google Scholar .L. I. Kuncheva , F. Roli , G. L. Marcialis , and C. A. Shipp , \" Complexity of data subsets generated by the random subspace method : an experimental investigation , \" in Multiple Classi_er Systems .", "label": "", "metadata": {}, "score": "47.775173"}
{"text": "A particular limitation of boosting is that it applies only to binary classification problems .This limitation is removed with the AdaBoost algorithm .AdaBoost .Arguably the best known of all ensemble - based algorithms , AdaBoost ( Adaptive Boosting ) extends boosting to multi - class and regression problems ( Freund 2001 ) .", "label": "", "metadata": {}, "score": "47.812946"}
{"text": "Ensemble integration involves the combination of predictions of set of base classifiers selected in ensemble selection phase .It can use two different methods : ( 1 ) combination ( also called fusion ) ; or ( 2 ) selection [ 46 ] .", "label": "", "metadata": {}, "score": "47.938164"}
{"text": "26 ] .The basic reason for adopting this taxonomy is its simplicity , popularity , and it covers basic aspects for building diverse pool of base classifiers .The author highlighted that diverse pool of classifiers can be generated by using different methods at four levels .", "label": "", "metadata": {}, "score": "47.97828"}
{"text": "Despite many studies and conjectures , the reasons behind this improved performance and understanding of the underlying probabilistic structures remain open and challenging problems .More recently , diagnostics such as edge and margin ( Breiman , 1997 ; Freund & Schapire , 1997 ; Schapire et al .", "label": "", "metadata": {}, "score": "47.978294"}
{"text": "( 1 ) Decision optimization : it refers to methods to choose and optimize the combiner for a fixed ensemble of base classifiers .This method corresponds to level A ( combination level as described above ) , ( 2 ) Coverage optimization : it refers to methods for creating diverse base classifiers assuming a fixed combiner .", "label": "", "metadata": {}, "score": "48.022842"}
{"text": "( iv )Clustering - Based Selection Method .This method employs clustering technique to search subset of base classifiers which perform similar predictions about the unclassified instance .Then the method selects a model from each cluster to select subset of available base classifiers .", "label": "", "metadata": {}, "score": "48.063732"}
{"text": "If there are more than two classes ( e.g. positive / neutral / negative ) and each document falls into exactly one class , this is a \" multi - class \" problem .In many cases , however , a document may have more than one associated category in a classification scheme , e.g. , a journal article could belong to computational biology , machine learning and some sub - domains in both categories .", "label": "", "metadata": {}, "score": "48.121857"}
{"text": "Potential boosters ?Conference Proceedings Advances in Neural Information Processing Systems 11 .Duffy , N. , & Helmbold , D. ( 2000 ) .Leveraging for regression .Conference Proceedings 13th Annual Conference on Computational Learning Theory .San Francisco .", "label": "", "metadata": {}, "score": "48.15426"}
{"text": "( 1 ) Ensemble comprise of multiple weak classifiers instead of single classifier .The multiple classifiers complement weaknesses of each other and hence improve the performance .( 2 ) Ensembles use the combined knowledge to model the hypothesis of the problem upon different subset of dataset or feature subspace .", "label": "", "metadata": {}, "score": "48.190018"}
{"text": "Each classifier 's training set is generated by randomly drawing , with replacement , N examples - where N is the size of the original training dataset ; many of the original examples may be repeated in the resulting training set while others may be left out .", "label": "", "metadata": {}, "score": "48.27121"}
{"text": "Combination of classifiers involves development of ensemble at generation and selection phases of learning , whereas ensemble integration phase involve combination of different predictions of multiple classifiers .In the following paragraphs , we presented important AI - based ensembles studies proposed in the last decade and compared them by various evaluation metrics .", "label": "", "metadata": {}, "score": "48.328186"}
{"text": "But since 1995 , mounting experimental evidence has fueled an interest in boosting among researchers in machine learning and statistics .Boosting research is still in its early stages .\" By analogy with aircraft , Rob et al have demonstrated that it 's possible to fly , \" noted Dr. Ross Quinlan , an Australian researcher , referring to Dr. Schapire 's early work .", "label": "", "metadata": {}, "score": "48.373833"}
{"text": "79 - 87 , 1991 .M. J. Jordan and R. A. Jacobs , \" Hierarchical mixtures of experts and the EM algorithm , \" Neural Computation , vol .6 , no . 2 , pp .181 - 214 , 1994 . D. H. Wolpert , \" Stacked generalization , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "48.39228"}
{"text": "Ridgeway , G. ( 1999 ) .The state of boosting .In Proceedings of the Thirty - first Symposium on the Interface 172 - 181 .Ridgeway , G. ( 1999 ) .The state of boosting .In Computing Science and Statistics 31 ( K. Berk , M. Pourahmadi , eds . )", "label": "", "metadata": {}, "score": "48.565666"}
{"text": "L. I. Kuncheva , \" A theoretical study on six classifier fusion strategies , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .24 , no . 2 , pp .281 - 286 , 2002 .S. B. Cho and J. H. Kim , \" Multiple network fusion using fuzzy logic , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "48.58215"}
{"text": "Researchers also proposed that there are trainable and nontrainable ensembles .Trainable ensembles need additional training to create the ensemble ( either during the base classifier training or after all base classifiers is trained ) [ 33 ] .On the other hand , nontrainable ensembles do not need training after the base classifiers have been induced [ 55 , 65 ] .", "label": "", "metadata": {}, "score": "48.84329"}
{"text": "This proves that there is neither a perfect combination strategy , nor one generally outperforming each other .This statement can be used as theoretical guideline for a malicious user to disrupt or evade the system , once combination strategy implemented is known to them .", "label": "", "metadata": {}, "score": "48.89842"}
{"text": "This level focuses on ensemble selection phase of ensemble learning process .It determines which base classifiers are used to constitute the ensemble prediction .Many researchers investigated the combination of base classifiers at this level very advantageous particularly for ID [ 23 , 28 - 30 , 32 - 44 ] .", "label": "", "metadata": {}, "score": "48.98803"}
{"text": "Some initial theory is presented which indicates that a lack of correlation between the errors of individual classifiers is a key factor in this variance reduction .Affiliated with .Affiliated with .Abstract .Background .We present a novel strategy for classification of DNA molecules using measurements from an alpha - Hemolysin channel detector .", "label": "", "metadata": {}, "score": "49.004272"}
{"text": "418 - 435 , 1992 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. Domingos and M. Pazzani , \" On the optimality of the simple Bayesian classifier under zero - one loss , \" Machine Learning , vol .", "label": "", "metadata": {}, "score": "49.035664"}
{"text": "M. Y. Su , K. C. Chang , H. F. Wei , and C. Y. Lin , \" Feature weighting and selection for a real - time network intrusion detection system based on GA with KNN , \" Intelligence and Security Informatics , vol .", "label": "", "metadata": {}, "score": "49.04908"}
{"text": "G. Fumera and F. Roli , \" A Theoretical and Experimental Analysis of Linear Combiners for Multiple Classifier Systems , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , Vol .27 , no .6 pp .942 - 956 , 2005 .", "label": "", "metadata": {}, "score": "49.22964"}
{"text": "Second challenge to tackle in AI - based ensembles is the enormous amount of audit data that make it difficult to build effective IDS .The processing of enormous amount of data increases computational overhead and causes delay in detection of intrusions .", "label": "", "metadata": {}, "score": "49.258717"}
{"text": "( 3 ) Since ensemble use the multiple classifiers , so it helps to find the global solution that leads to reduce the false alarm rate and increase the detection accuracy .( 4 ) Unstable base classifiers help to generate the diverse set of base classifiers for efficient ensemble .", "label": "", "metadata": {}, "score": "49.276306"}
{"text": "Bagging predictors .Machine Learning 24 123 - 140 .Friedman , J. and Stuetzle , W. ( 1981 ) .Projection pursuit regression .J. Amer .Statist .Assoc .76 817 - 823 .Holte , R. ( 1993 ) .", "label": "", "metadata": {}, "score": "49.338715"}
{"text": "Number of the reduced dimension is based on summed contribution of the eigenvalues exceeding 99 % .It provides a dimensionality reduction with an unsupervised learning algorithm [ 29 ] .Consider the following .Morphological Image Processing .Morphology is a cornerstone of the mathematical set of tools underlying the development of techniques that extract the meaning features from an image [ 31 ] .", "label": "", "metadata": {}, "score": "49.353737"}
{"text": "Localized boosting .Conference Proceedings 13thCOLT .Palo Alto , California .Mitchell , T. M. ( 1997 ) .Machine learning .Boston : WCB / McGraw - Hill .Moerland , P. , & Mayoraz , E. ( 1999 ) .", "label": "", "metadata": {}, "score": "49.385757"}
{"text": "Different models are generated by using a wide range of different features of client queries .The system derives automatically the parameter profiles associated with web applications ( e.g. , length and structure of parameters ) and relationships between queries ( e.g. , access times and sequences ) from the analyzed data .", "label": "", "metadata": {}, "score": "49.410236"}
{"text": "Many definitions exist to evaluate similarity between an alarm and a meta - alarm .In fact , the distance between an alarm and a meta - alarm is defined in terms of correlation between them , which is in turn defined as an application of a distance function to features characterizing each of the raised alarms .", "label": "", "metadata": {}, "score": "49.43017"}
{"text": "[ 22 ] .Sabhnani and Serpen [ 23 ] proposed a multi - classifier approach to detect intrusions .They utilized different classifiers , namely , an ANN , k - means clustering , and a Gaussian classifier to classify different classes of intrusions by using KDD 1999 dataset .", "label": "", "metadata": {}, "score": "49.481068"}
{"text": "Iteratively , support vectors are calculated and the SVM is tested against a stopping criterion to determine if a desirable threshold of accuracy has been achieved .Otherwise the iterative process continues .The authors reported 91 % , 97 % , 23 % , and 43 % detection of Probe , DoS , U2R , and R2L attack classes .", "label": "", "metadata": {}, "score": "49.551277"}
{"text": "( vi ) Metalearning Method .The method employs a second level of combiner to fuse the predictions of base classifiers for determining final ensemble prediction , for example , stacking .In stacking , the predictions of base classifiers are fed to an intermediate combiner to perform trained combinations of predictions of base classifiers [ 79 ] .", "label": "", "metadata": {}, "score": "49.64277"}
{"text": "Joachims , T. ( 1998 ) .Text categorization with Support Vector Machines : Learning with many relevant features .In Machine Learning : ECML-98 , Tenth European Conference on Machine Learning , pp .137 - -142 .Lewis D.L. , Yang Y. , Rose T.G. , Li F. ( 2004 ) : RCV1 : A New Benchmark Collection for Text Categorization Research .", "label": "", "metadata": {}, "score": "49.77717"}
{"text": "( ii )Cascading Classifiers Method .In this method , different base classifiers are employed sequentially to unclassified instance and confidence level of first classifier is recorded .If its level is high enough then its prediction is the ensemble final prediction .", "label": "", "metadata": {}, "score": "49.896965"}
{"text": "( i )To be used as an effective filtering method to reduce the number of false positives in a CAD system .( ii )To increase the diagnostic accuracy of the detection system .The rest of this paper is organized as follows .", "label": "", "metadata": {}, "score": "50.018284"}
{"text": "The KDD 99 derived from DARPA 1998 & 1999 datasets are main benchmarks used to evaluate the performance of network intrusion detection systems .However , they are suffering from a fatal drawback : failing to realistically simulate a real - world network [ 102 , 121 ] .", "label": "", "metadata": {}, "score": "50.253075"}
{"text": "Diversity of classifiers in bagging is obtained by using bootstrapped replicas of the training data .That is , different training data subsets are randomly drawn - with replacement - from the entire training dataset .Each training data subset is used to train a different classifier of the same type .", "label": "", "metadata": {}, "score": "50.27716"}
{"text": "In each case , a final decision is made by combining the individual decisions of several experts .In doing so , the primary goal is to minimize the unfortunate selection of an unnecessary medical procedure , a poor product , an unqualified employee or even a poorly written and misguiding article .", "label": "", "metadata": {}, "score": "50.331703"}
{"text": "Using these three basic architectures , we can build even more complicated classifier combination systems .He listed eighteen different methods for classifier combination and divided them into different categories according to their trainability , adaptivity , and requirement on the output of individual classifiers .", "label": "", "metadata": {}, "score": "50.33312"}
{"text": "However , IBC does not allow to efficiently adapt a fusion function over time when new data becomes available , since it requires a fixed number of classifiers .The IBC technique was further improved as incremental Boolean combination ( incrBC ) by the authors [ 119 ] .", "label": "", "metadata": {}, "score": "50.33563"}
{"text": "In the following section we discuss a framework that eliminates the need for a decision tree structure for multiclass classification .DNA Molecule Classification Using Boosting Over Stumps .To obtain a single multiclass learner , the boosting approach proposed in the previous section was modified .", "label": "", "metadata": {}, "score": "50.483036"}
{"text": "Feature selection .As mentioned earlier in this article , one way to improve diversity in the ensemble is to train individual classifiers different subsets of the available features .Selecting the feature subsets at random is known as the random subspace method , a term coined by ( Ho 1998 ) , who used it on constructing decision tree ensembles .", "label": "", "metadata": {}, "score": "50.483337"}
{"text": "The IDSs are evaluated by comparing the true - positive rate ( i.e. , the percentage of attacks that were correctly recognized ) and the false - positive rate ( i.e. , the percentage of legitimate traffic flagged as an attack ) .", "label": "", "metadata": {}, "score": "50.585968"}
{"text": "An equivalent way of thinking about these methods consists in encoding each class as a bit string ( named codeword ) and in training a different two - class base classifier in order to separately learn each codeword bit .When the classifiers are applied to classify new points , a suitable measure of dissimilarity between the codeword computed by the ensemble and the codeword classes is used to predict the class ( e.g. , Hamming distance ) [ 100 ] .", "label": "", "metadata": {}, "score": "50.593674"}
{"text": "Many feature selection techniques for ensemble classifiers are proposed in the literature which can be further investigated in [ 15 , 16 , 103 ] .Data Level .This level focuses on ensemble generation phase of ensemble learning process .Here , different data subsets are used to train the pool of base classifiers .", "label": "", "metadata": {}, "score": "50.636192"}
{"text": "In order to validate the evaluation results of IDS on a simulated dataset , one has to develop a methodology to quantify the similarity of simulated and real network traces .KDD cup 1999 dataset and its original form possess some special features , such as huge volume , high dimension , and highly skewed data distribution .", "label": "", "metadata": {}, "score": "50.667053"}
{"text": "In the gated parallel variant , the outputs of individual classifiers are selected or weighted by a gating device before they are combined .In the cascading architecture , individual classifiers are invoked in a linear sequence .The number of possible classes for a given pattern is gradually reduced as more classifiers in the sequence have been invoked .", "label": "", "metadata": {}, "score": "50.714935"}
{"text": "Thresholds are fixed independently for each application - specific module .Perdisci et al .[ 88 ] proposed a clustering - based fusion module to combine multiple alarms that help to reduce the volume of alarms produced by IDSs .The produced meta - alarms provide the system administrator with a concise high - level description of the attack .", "label": "", "metadata": {}, "score": "50.825653"}
{"text": "The error of this hypothesis with respect to the current distribution is calculated as the sum of distribution weights of the instances misclassified by \\(h_t\\ ) ( Equation 4 ) .AdaBoost . M1 requires that this error be less than \\(1/2\\ .", "label": "", "metadata": {}, "score": "50.826096"}
{"text": "36 , no . 1 , pp .105 - 139 , 1999 .View at Google Scholar \u00b7 View at Scopus .R. E. Banfield , L. O. Hall , K. W. Bowyer , and W. P. Kegelmeyer , \" A comparison of decision tree ensemble creation techniques , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "50.877098"}
{"text": "After several rounds of boosting , no weak learner with an accuracy greater than 50 % was found .This can be attributed to the fact that some features in the HMM projections are noisy which are affecting the posterior probability and hence no weak learner is obtained .", "label": "", "metadata": {}, "score": "50.87715"}
{"text": "497 - 501 , 1995 .L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" Fuzzy Sets and Systems , L. I. Kuncheva , \" Using measures of similarity and inclusion for multiple classifier fusion by decision templates , \" vol .", "label": "", "metadata": {}, "score": "50.95758"}
{"text": "[5 ] .Recent advances in the field of AI led many researchers to apply AI - based techniques for ID successfully .The major difference between AI - based and traditional IDSs is that only AIs can learn new rules automatically , whereas in traditional systems the security administrator must add new rules for each new attack type or each new allowed program .", "label": "", "metadata": {}, "score": "51.076263"}
{"text": "Such a classifier can not learn the boundary shown in Figure 2 .Now consider a collection of circular decision boundaries generated by an ensemble of such classifiers as shown in Figure 3 , where each classifier labels the data as class O or class X , based on whether the instances fall within or outside of its boundary .", "label": "", "metadata": {}, "score": "51.118088"}
{"text": "241 - 259 , 1992 .T. K. Ho , J. J. Hull , and S. N. Srihari , \" Decision combination in multiple classifier systems , \" IEEE Trans . on Pattern Analy .Machine Intel . , vol .16 , no . 1 , pp .", "label": "", "metadata": {}, "score": "51.14991"}
{"text": "J. McCumber , \" Information system security : a comprehensive model , \" in Proceedings of the 14th National Computer Security Conference , Baltimore , Md , USA , 1991 .W. Khreich , E. Granger , A. Miri , and R. Sabourin , \" Iterative Boolean combination of classifiers in the ROC space : an application to anomaly detection with HMMs , \" Pattern Recognition , vol .", "label": "", "metadata": {}, "score": "51.300056"}
{"text": "The authors reported 84.1 % , 99.5 % , 14.1 % , and 31.5 % detection of Probe , DoS , U2R , and R2L attack classes .Yan and Hao [ 111 ] presented ensemble of neural network for ID based upon improved MOGA ( improvement of NSGA - II ) .", "label": "", "metadata": {}, "score": "51.304184"}
{"text": "[28 ] proposed a hybrid approach to detect intrusions .They utilized Bayesian networks ( BNs ) and classification and regression trees ( CARTs ) and their ensemble to generate hybrid system .They empirically proved that CART performed best for Normal , Probe , and U2R and the ensemble approach worked best for R2L and DoS. The heterogeneous ensemble was generated by training the individual classifiers from reduced KDD cup 99 dataset .", "label": "", "metadata": {}, "score": "51.312344"}
{"text": "Final prediction of ensemble is generated by fusing different predictions of individual base classifiers .Generally fusion of predictions is performed by using majority voting method .However , this is not always possible due to the size of the dataset .", "label": "", "metadata": {}, "score": "51.371246"}
{"text": "238 - 247 , 2001 .G. Brown , J. Wyatt , R. Harris , X. Yao , \" Diversity Creation Methods : A Survey and Categorisation , \" Journal of Information Fusion ( Special issue on Diversity in Multiple Classifier Systems ) .", "label": "", "metadata": {}, "score": "51.374443"}
{"text": "The related studies of AI - based ensembles are compared by set of evaluation metrics driven from ( 1 ) architecture & approach followed ; ( 2 ) different methods utilized in different phases of ensemble learning ; ( 3 ) other measures used to evaluate classification performance of the ensembles .", "label": "", "metadata": {}, "score": "51.38793"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , F. Roli , J. Kittler , and T. Windeatt , Eds . , vol .3077 , pp . 1 - 15 , 2004 .L. I. Kuncheva , Combining Pattern Classifiers , Methods and Algorithms .", "label": "", "metadata": {}, "score": "51.43351"}
{"text": "The metrics for binary - decisions are defined as : .Due to the often highly unbalance number of positive vs. negative examples , note that TN often dominates the accuracy and error of a system , leading to miss - interpretation of the results .", "label": "", "metadata": {}, "score": "51.585136"}
{"text": "G. Rogova , \" Combining the results of several neural network classifiers , \" Neural Networks , vol .7 , no .5 , pp .777 - 781 , 1994 .L. Lam and C. Y. Suen , \" Optimal combinations of pattern classifiers , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "51.614326"}
{"text": "Statist .Breiman , L. ( 1998b ) .Combining predictors .Technical report , Dept .Statistics , Univ .California , Berkeley .Breiman , L. , Friedman , J. , Olshen , R. and Stone , C. ( 1984 ) .", "label": "", "metadata": {}, "score": "51.648754"}
{"text": "A new feature vector was created by combining the best features of the above three methods , aiming at increasing the sensitivity of the proposed classification approach .A total of 30 features selected by the three methods were now applied to the test dataset .", "label": "", "metadata": {}, "score": "51.66438"}
{"text": "4544 - 4566 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. P. W. Duin , \" The combining classifier : to train or not to train ? \" in Proceedings of 16th International Conference on Pattern Recognition ( ICPR ' 02 ) , pp .", "label": "", "metadata": {}, "score": "51.697388"}
{"text": "Robertson , S. E. , & Walker , S. ( 1994 ) .Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval .Conference Proceedings 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval .", "label": "", "metadata": {}, "score": "51.72605"}
{"text": "The weighted majority voting then chooses the class \\(\\omega\\ ) receiving the highest total vote from all classifiers .This result may appear to go against the conventional wisdom ( see Occam 's razor ) indicating that adding too many classifiers - beyond a certain limit - would eventually lead to overfitting of the data .", "label": "", "metadata": {}, "score": "51.730133"}
{"text": "Whenever we use the term HMM projections in the remaining part of this report , it would refer to the features extracted using the method explained in this section .The process of feature extraction can be found in a greater detail in [ 6 ] .", "label": "", "metadata": {}, "score": "51.76309"}
{"text": "A single classifier could have a higher test error .The combination of classifiers can produce a lower test error than that of the single classifier because the diversity of classifiers usually compensates for errors of any single classifier [ 45 ] .", "label": "", "metadata": {}, "score": "51.794662"}
{"text": "In this paper , we present MFS , a combining algorithm designed to improve the accuracy of the nearest neighbor ( NN ) classifier .MFS combines multiple NN classifiers each using only a random subset of features .The experimental results are encouraging : On 25 datasets from the UCI Repository , MFS signi cantly outperformed several standard NN variants and was competitive with boosted decision trees .", "label": "", "metadata": {}, "score": "51.858665"}
{"text": "For any given instance , the class chosen by most number of classifiers is the ensemble decision .Since the training datasets may overlap substantially , additional measures can be used to increase diversity , such as using a subset of the training data for training each classifier , or using relatively weak classifiers ( such as decision stumps ) .", "label": "", "metadata": {}, "score": "51.871475"}
{"text": "TCA ( total classification accuracy ) represents the probability of correctly classified patterns .For RMSE ( root mean squared error ) , . depict actual value , predicted value , and number of data patterns , respectively .In order to measure the performance of the classification system , AUROC is often used as well as sensitivity and specificity [ 49 ] .", "label": "", "metadata": {}, "score": "51.98699"}
{"text": "The classifiers were compared , and overall performance results of the proposed classification approach were given in Table 2 .The performance measurements are given by .Table 2 : Overall performance results of the proposed classification approach .Sensitivity is the number of correctly predicted positives divided by the total number of positive cases .", "label": "", "metadata": {}, "score": "52.01129"}
{"text": "Kim , W. G. , & Wilbur , W. J. ( 2001 ) .Corpus - based statistical screening for content - bearing terms .Journal of the American Society for Information Science , 52 :3 , 247 - 259 .Langley , P. , & Sage , S. ( 1994 ) .", "label": "", "metadata": {}, "score": "52.016685"}
{"text": "Fusion - Based Combination Methods .These methods combine the predictions of the base classifiers to determine ensemble prediction .The major methods proposed in literature are described below .( i ) Majority Voting Method .In majority voting ensemble , each base classifier votes for specific class and the class that collects majority of vote is predicted as ensemble final prediction [ 70 - 72 ] .", "label": "", "metadata": {}, "score": "52.030373"}
{"text": "2212 , pp .85 - 103 , 2001 .View at Google Scholar .P. Garc\u00eda - Teodoro , J. D\u00edaz - Verdejo , G. Maci\u00e1 - Fern\u00e1ndez , and E. V\u00e1zquez , \" Anomaly - based network intrusion detection : techniques , systems and challenges , \" Computers & Security , vol .", "label": "", "metadata": {}, "score": "52.12594"}
{"text": "In the first stage it combines all base classifiers using specialist classifiers which have a dichotomous model .Second stage contains k metaclassifiers which are used to learn the prediction characteristics of the specialist classifiers .Each metaclassifier is in charge of one class only and will combine all the specialist classifiers which are able to classify their own particular class .", "label": "", "metadata": {}, "score": "52.163296"}
{"text": "3541 , pp .326 - 335 , Seaside .Monterey , CA , June 2005 .M. Muhlbaier , A. Topalis , R. Polikar , \" Learn++ .NC : Combining Ensemble of Classifiers with Dynamically Weighted Consult - and - Vote for Efficient Incremental Learning of New Classes , \" IEEE Transactions on Neural Networks , In press , 2008 .", "label": "", "metadata": {}, "score": "52.233776"}
{"text": "405 - 410 , 1997 .S. B. Cho and J. H. Kim , \" Combining multiple neural networks by fuzzy integral for robust classification , \" IEEE Transactions on Systems , Man and Cybernetics , vol .25 , no . 2 , pp .", "label": "", "metadata": {}, "score": "52.446438"}
{"text": "The value of threshold metrics lies in range from 0 to 1 .Ranking metrics include false - positive rate ( FPR ) , detection rate ( DR ) , precision ( PR ) , and area under ROC curve ( ROC ) .", "label": "", "metadata": {}, "score": "52.593876"}
{"text": "Ensemble learning process has three phases : ( 1 ) ensemble generation ; ( 2 ) ensemble selection ; ( 3 ) ensemble integration .Ensemble generation is homogenous if the same induction algorithm is used to generate all the classifiers of the ensemble , otherwise it is said to be heterogeneous .", "label": "", "metadata": {}, "score": "52.59405"}
{"text": "Given new training data , a new pool of HMMs is generated from newly acquired data using different HMM states and initializations .The responses from these newly trained HMMs are then combined with those of the previously trained HMMs in ROC space using the incremental Boolean combination ( incrBC ) technique .", "label": "", "metadata": {}, "score": "52.646408"}
{"text": "The task of a model is to assign a probability value to either a query as a whole or one of the query 's attributes .This probability value reflects the probability of the occurrence of the given feature value with regards to an established profile .", "label": "", "metadata": {}, "score": "52.653625"}
{"text": "( i ) Bagging .Bagging ( bootstrap aggregating ) is originally proposed by Breiman [ 61 ] .The method is dependent on the instability of the base classifiers .The instability of base classifiers refers to sensitivity to configuration of base classifier and/or training data .", "label": "", "metadata": {}, "score": "52.686737"}
{"text": "This is made possible by imposing appropriate constraints on ... \" .We describe SLIPPER , a new rule learner that generates rulesets by repeatedly boosting a simple , greedy , rule - builder .Like the rulesets built by other rule learners , the ensemble of rules created by SLIPPER is compact and comprehensible .", "label": "", "metadata": {}, "score": "52.88919"}
{"text": "The most appealing way to reduce false alarms is to develop better IDSs that generate fewer false alarms .The process of reducing the false alarms is very challenging because the false alarms are the result of many problems .These causes require IDS to be faster , flexible ( instead of strict thresholds ) , and adaptive ( instead of fixed rules ) , and dynamic learning of new patterns and aggregate logically corelated false alarms to identify root cause of alarms .", "label": "", "metadata": {}, "score": "52.896652"}
{"text": "The method selects those base classifiers which perform better performance than others .Then the method combines the selected base classifiers through majority voting method ( described in previous section ) [ 89 ] .Mixture of Expert Systems .This method is a general method similar to ensemble selection [ 90 ] .", "label": "", "metadata": {}, "score": "52.89972"}
{"text": "Kappa statistics is a chance - corrected measure of agreement between the classifications and the true classes .If Kappa is equal to 1 , it indicates perfect agreement .If Kappa is equal to 0 , it represents chance agreement .", "label": "", "metadata": {}, "score": "53.00445"}
{"text": "This means that if a statistical classifier requires approximately 100 positive training examples per category for learning sufficiently accurate models , then we can only solve the classification problem for only 1 % of the categories even if we use the entire set of 800,000 pages as training examples .", "label": "", "metadata": {}, "score": "53.013912"}
{"text": "Value of RMSE lies in range from 0 to 1 .The metric is minimized when the predicted value for each attack class coincides with the true conditional probability of that class being normal class .Generally , these metrics are computed from confusion matrix .", "label": "", "metadata": {}, "score": "53.0809"}
{"text": "where m is the total number of features .This is a very strong assumption but has been shown to work in practice .The label class label predicted by the naive Bayes classifiers is the one for which the p ( \u03c9 i ) is maximum .", "label": "", "metadata": {}, "score": "53.156136"}
{"text": "The different methods cited in the above section can be summarized in Table 1 .Fusion is the simples and popular method to combine different base classifiers .This method works on the assumption that all base classifiers are of same importance but practically it may not be true , whereas in selection method , generally , only one classifier is chosen to label an unclassified instance .", "label": "", "metadata": {}, "score": "53.162315"}
{"text": "In spite of its relative simplicity , SLIPPER is highly scalable , and an effiective learner . by Kai Ming Ting , Ian H. Witten - Journal of Artificial Intelligence Research , 1999 . \" ...Stacked generalization is a general method of using a high - level model to combine lower - level models to achieve greater predictive accuracy .", "label": "", "metadata": {}, "score": "53.169853"}
{"text": "The aim of a CAD system is to provide diagnosis information to improve clinical decision - making process ; therefore , its success is related directly to its disease detection accuracy [ 2 ] .Today , CAD systems are frequently utilized to detect and diagnose numerous abnormalities in routine clinical work .", "label": "", "metadata": {}, "score": "53.21857"}
{"text": "\\ )A commonly used loss functions is the 0/1-loss , which returns 0 if the prediction matches the true label , and 1 otherwise .In this case , the risk is equal to the probability that the classification rule will misclassify an example .", "label": "", "metadata": {}, "score": "53.305725"}
{"text": "347 - 352 , 1993 .L. Xu , A. Krzyzak , and C.Y. Suen , Methods for combining multiple classifiers and their applications to handwriting recognition , IEEE Trans . on Systems , Man , and Cyb . , Vol .", "label": "", "metadata": {}, "score": "53.4299"}
{"text": "33 , no . 7 , pp .2642 - 2653 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus Ensemble learning is the process by which multiple models , such as classifiers or experts , are strategically generated and combined to solve a particular computational intelligence problem .", "label": "", "metadata": {}, "score": "53.477367"}
{"text": "Data set IV : Data set III enhanced with approximation and detail coefficients obtained using a second and tenth order Symlets wavelet filter .In each case number of rounds of boosting were equal to the total number of features available in the data set .", "label": "", "metadata": {}, "score": "53.482025"}
{"text": "If a classifier can not meet this requirement , the algorithm aborts .The normalized error \\(\\beta_t\\ , \\ ) is then computed ( Equation 5 ) so that the actual error that is in the [ 0 0.5 ] interval is mapped to [ 0 1 ] interval .", "label": "", "metadata": {}, "score": "53.549553"}
{"text": "Experiments on . by William W. Cohen , Yoram Singer - IN PROCEEDINGS OF ANNUAL CONFERENCE OFAMERICAN ASSOCIATION FOR ARTI CIAL INTELLIGENCE , 1999 . \" ...We describe SLIPPER , a new rule learner that generates rulesets by repeatedly boosting a simple , greedy , rule - builder .", "label": "", "metadata": {}, "score": "53.554176"}
{"text": "use an SVM - based decision - tree to classify features vectors obtained from blockade current measurements from a nanopore detector .The DNA hairpins they choose differ only in one base pair .Their results show accuracies close to 99 % .", "label": "", "metadata": {}, "score": "53.57845"}
{"text": "depict the goal output signal and the activation function , respectively .Random Forest .Random forest was proposed by Breiman in 1999 [ 42 ] .It is a new development in tree based classifiers and fast proven to be one of the most important algorithms in the machine learning .", "label": "", "metadata": {}, "score": "53.582962"}
{"text": "Several other metrics are utilized by researchers to measure the performance of IDS .These metrics can be divided into three classes : threshold , ranking , and probability metrics [ 7 , 8 ] .Threshold metrics include classification rate ( CR ) , F - measure ( FM ) , and cost per example ( CPE ) .", "label": "", "metadata": {}, "score": "53.592117"}
{"text": "Machine Learning , 48 :1 , 253 - 285 .CrossRef .Duda , R. O. , Hart , P. E. , & Stork , D. G. ( 2000 ) .Pattern Classification ( 2 edn . )New York : John Wiley & Sons , Inc. .", "label": "", "metadata": {}, "score": "53.595917"}
{"text": "The AUROC of a classifier is equivalent to the probability that the classifier ranks a randomly chosen positive instance higher than a randomly chosen negative instance [ 50 ] .For our proposed methods , ROC curves are illustrated in Figure 8 .", "label": "", "metadata": {}, "score": "53.707638"}
{"text": "Multi - label and multi - class tasks are often handled by reducing them to \\(k\\ ) binary classification tasks , one for each category .For each such binary classification tasks , members of the respective category are designated as positive examples , while all others are designated as negative examples .", "label": "", "metadata": {}, "score": "53.72229"}
{"text": "G. Wang , H. Jinxing , M. Jian , and H. Lihua , \" A new approach to intrusion detection using Artificial Neural Networks and fuzzy clustering , \" Expert Systems with Applications , vol .37 , no . 9 , pp .", "label": "", "metadata": {}, "score": "53.75807"}
{"text": "However , there is no comprehensive review of ensembles in general and AI - based ensembles for ID to examine and understand their current research status to solve the ID problem .Here , an updated review of ensembles and their taxonomies has been presented in general .", "label": "", "metadata": {}, "score": "53.828655"}
{"text": "The motivatio ... \" .In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .The framework that is described here for people is easily applied to other objects as well .", "label": "", "metadata": {}, "score": "53.866325"}
{"text": "Conference Proceedings Proceedings ACL'99 .Univ .Maryland .Kim , W. , Aronson , A. R. , & Wilbur , W. J. ( 2001 ) .Automatic MeSH term assignment and quality assessment .Conference Proceedings Proc .AMIA Symp .", "label": "", "metadata": {}, "score": "53.91355"}
{"text": "From our own work [ B \u00a8uhlmann and Yu ( 2000 ) ] we know that stumps evaluated at x have high variances for x in a whole region of the covariate space .From an asymptotic point of view , this region is \" centered around \" the true optimal split point for a stump and has \" substantial \" size O n-1/3 .", "label": "", "metadata": {}, "score": "53.95154"}
{"text": "The negative sign converts the distance metric into a support value , whose largest value can be zero in case of a perfect match .Note that this output does not match any of the code words exactly , and this is where the error correcting ability of the ECOC lies .", "label": "", "metadata": {}, "score": "54.01391"}
{"text": "A ROC curve is usually used as a technique to visualize the performance of classifiers and is extremely useful to compare the performance of different classifiers in medical decision - making systems .The curve indicates the tradeoff between the true positive and false positive rates .", "label": "", "metadata": {}, "score": "54.10485"}
{"text": "Unfortunately , many combining methods do not improve the nearest neighbor classifier .In this paper , we present MFS , a combining algorithm designed to improve the accuracy of the nearest neighbor ( NN ) classifier .MFS combines multiple NN classifiers each using only a random subset of features .", "label": "", "metadata": {}, "score": "54.380608"}
{"text": "A hybrid strategy can loop back and forth multiple times between methods or can embed one method within another method .In this study , we adopted and presented the taxonomy proposed by Kuncheva [ 46 ] and additional aspects borrowed from Jain et al .", "label": "", "metadata": {}, "score": "54.396393"}
{"text": "59 - 72 , 2007 .R. Polikar , \" Ensemble based systems in decision making , \" IEEE Circuits and Systems Magazine , vol .6 , no.3 , pp .21 - 45 , 2006 .Muhlbaier M. , Topalis A. , Polikar R. , \" Ensemble confidence estimates posterior probability , \" 6th Int .", "label": "", "metadata": {}, "score": "54.48482"}
{"text": "Figure 1 graphically illustrates this concept , where each classifier - trained on a different subset of the available training data - makes different errors ( shown as instances with dark borders ) , but the combination of the ( three ) classifiers provides the best decision boundary .", "label": "", "metadata": {}, "score": "54.491264"}
{"text": "R. Anand , K. Mehrotra , C. K. Mohan , and S. Ranka , \" Efficient classification for multiclass problems using modular neural networks , \" IEEE Transactions on Neural Networks , vol .6 , no . 1 , pp .", "label": "", "metadata": {}, "score": "54.497192"}
{"text": "I. H. Witten and E. Frank , Data Mining : Practical Machine Learning Tools and Techniques , The Morgan Kaufmann Series in Data Management Systems , Morgan Kaufmann , San Francisco , Calif , USA , 2nd edition , 2005 .C. M. Bishop , Pattern Recognition and Machine Learning , Information Science and Statistics , Springer , New York , NY , USA , 2006 .", "label": "", "metadata": {}, "score": "54.501648"}
{"text": "The details can be further explored in [ 7 ] .KDD cup 1999 dataset is most popular publically available evaluation benchmarked dataset .But , the dataset is critically discussed in the literature for being nowadays outdated due to the type of attacks and background traffic used and for the methodology implemented for building it [ 14 ] .", "label": "", "metadata": {}, "score": "54.532883"}
{"text": "In method 2 , the statistical features , minimum ( min ) , maximum ( max ) , mean , standard deviation ( std ) , variance ( var ) , and 3rd moment values , are calculated in the training dataset .", "label": "", "metadata": {}, "score": "54.634388"}
{"text": "918 - 924 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .N. B. Anuar , H. Sallehudin , A. Gani , and O. Zakari , \" Identifying false alarm for network intrusion detection system using hybrid data mining and decision tree , \" Malaysian Journal of Computer Science , vol .", "label": "", "metadata": {}, "score": "54.872704"}
{"text": "B. V. Dasarathy and B. V. Sheela ( 1979 ) , \" Composite classifier system design : concepts and me - thodology , \" Proceedings of the IEEE , vol .67 , no .5 , pp .708 - 713 .", "label": "", "metadata": {}, "score": "54.909676"}
{"text": "While in bagging each instance is drawn with equal probability from the available training dataset , in wagging each instance is extracted according to a weight stochastically assigned .( iii ) Random Forest ( RF ) .This method is a version of bagging which comprised of decision trees ( DTs ) [ 95 ] .", "label": "", "metadata": {}, "score": "54.987354"}
{"text": "Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , Vol .1857 , pp . 1 - 15 , 2000 , Springer - Verlag .L. K. Hansen and P. Salamon , \" Neural network ensembles , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "55.148895"}
{"text": "G. Kumar and K. Kumar , \" A novel evaluation function for feature selection based upon information theory , \" in Proceedings of the IEEE International Conference on Electrical and Computer Engineering ( CCECE ' 11 ) , pp .000395- 000399 , Niagara Falls , Canada , May 2011 .", "label": "", "metadata": {}, "score": "55.22014"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Moreira and E. Mayoraz , \" Improved pairwise coupling classification with correcting classifiers , \" in Proceedings of the 10th European Conference on Machine Learning , C. Nedellec and C. Rouveirol , Eds . , vol .", "label": "", "metadata": {}, "score": "55.239243"}
{"text": "In order to improve the performance result of the final ensemble , adaboost algorithms consist of diverse weak classifiers .Especially , the boosting algorithm adaboost .M1-the first directly - extends the original adaboost algorithm to the multiclass case without reducing it to multiple two - class problems .", "label": "", "metadata": {}, "score": "55.391438"}
{"text": "In the next phase , feature selection is performed using AdaBoost .AdaBoost provides an ensemble of weak learners of various types learned from feature primitives .Results and Conclusion .We show that our technique , despite its inherent simplicity , provides a performance comparable to recent multi - class DNA molecule classification results .", "label": "", "metadata": {}, "score": "55.408966"}
{"text": "553 - 568 , 1997 .View at Google Scholar \u00b7 View at Scopus .L. Xu , A. Krzyzak , and C. Y. Suen , \" Methods of combining multiple classifiers and their applications to handwriting recognition , \" IEEE Transactions on Systems , Man and Cybernetics , vol .", "label": "", "metadata": {}, "score": "55.41549"}
{"text": "5 , pp .1407 - 1414 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Kong , L. Wang , E. K. Teoh , X. Li , J.-G. Wang , and R. Venkateswarlu , \" Generalized 2D principal component analysis for face image representation and recognition , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "55.421623"}
{"text": "It is extremely important to consider the small nodule size in the classification of a CAD system .This increases the probability of early detection of nodules .Considering these results , it can be seen that the proposed study represents a relatively high sensitivity .", "label": "", "metadata": {}, "score": "55.479874"}
{"text": "G. Kumar , K. Kumar , and M. Sachdeva , \" The use of artificial intelligence based techniques for intrusion detection - a review , \" Artificial Intelligence Review , vol .34 , no .4 , pp .369 - 387 , 2010 .", "label": "", "metadata": {}, "score": "55.53727"}
{"text": "Majority of research works described here were trained & tested on KDD cup 1999 dataset .Since these works are evaluated in different environments using different training and test datasets extracted from KDD cup 1999 dataset , these studies can not be critically analyzed based upon these reported results .", "label": "", "metadata": {}, "score": "55.57689"}
{"text": "For example , one might want to classify documents by sentiment ( Pang & Lee , 2002 ) : is a review positive or negative ?Or one might want to classify a message as deceptive or not .While representations and methods developed for topic - based classification are applicable to these classification problems as well , special considerations reflecting the nature of the classification task will probably lead to improved performance .", "label": "", "metadata": {}, "score": "55.59699"}
{"text": "C. Kruegel , F. Valeur , and G. Vigna , Intrusion Detection and Correlation , Challenges and Solution , Advances in Information Security , Springer , 2005 .View at Scopus .View at Publisher \u00b7 View at Google Scholar .J. W. Haines , R. P. Lippmann , D. J. Fried , E. Tran , S. Boswell , and M. A. Zissman , \" DARPA intrusion detection system evaluation : design and procedures , \" Tech .", "label": "", "metadata": {}, "score": "55.615128"}
{"text": "L. J. Hargrove , G. Li , K. B. Englehart , and B. S. Hudgins , \" Principal components analysis preprocessing for improved classification accuracies in pattern - recognition - based myoelectric control , \" IEEE Transactions on Biomedical Engineering , vol .", "label": "", "metadata": {}, "score": "55.64916"}
{"text": "We conclude with a discussion of the various de nitions that have been proposed in the past as well as a method for estimating these quantities on real data sets . ... tterich and Bakiri , 1995 ) , Bagging ( Breiman , 1996a ) and AdaBoost ( Freund and Schapire , 1996 ) .", "label": "", "metadata": {}, "score": "55.679436"}
{"text": "Bagging works well if classifier predictions of base classifiers were independent and classifiers had the same individual accuracy , and then the majority vote is guaranteed to improve on the individual performance [ 46 ] .( ii ) Wagging .Wagging method is a variant of bagging .", "label": "", "metadata": {}, "score": "55.847504"}
{"text": "The diversity in ensembles refers to different errors made by different base classifiers on data records .In order to produce diverse classifiers , researchers used two types of methods : ( 1 ) Implicit ; ( 2 ) Explicit [ 47 , 49 ] .", "label": "", "metadata": {}, "score": "55.861603"}
{"text": "The horizontal lines indicate the test er ... . \" ...In this paper we present a component based person detection system that is capable of detecting frontal , rear and near side views of people , and partially occluded persons in cluttered scenes .", "label": "", "metadata": {}, "score": "55.868565"}
{"text": "Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classifier h ... \" .Much recent attention , both experimental and theoretical , has been focussed on classification algorithms which produce voted combinations of classifiers .", "label": "", "metadata": {}, "score": "55.869553"}
{"text": "These probabilities , accompanied by the predictions of each of the classifiers , are passed on to the selector , which then determines the final output .These probabilities can be used to stochastically select the expert , or to choose the expert according to a winner - takes - all paradigm , or as weights to combine the outputs of the multiple base classifiers [ 33 , 46 ] .", "label": "", "metadata": {}, "score": "55.88358"}
{"text": "The performance variation of various classifiers for different intrusions may be described by two aspects .The first aspect is the different design principles of classifiers which work to optimize different parameters .For example , SVM is designed to minimize structural risk based upon statistical theory whereas ANN to minimize empirical risk in which classification function is derived by minimizing the mean square error over the training dataset .", "label": "", "metadata": {}, "score": "56.013107"}
{"text": "349 - 358 , Cambridge , UK , 2001 .M. Sewell , \" Ensemble Learning , \" Research Note RN/11/02 , UCL department of computer science , 2011 . E. Mayoraz and M. Moreira , \" On the decomposition of polychotomies into dichotomies , \" in Proceedings of the XIV International Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "56.18306"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. B. Langdon and B. F. Buxton , \" Genetic programming for improved receiver operating characteristics , \" in Proceedings of the 2nd International Conference on Multiple Classifier System , J. Kittler and F. Roli , Eds . , pp .", "label": "", "metadata": {}, "score": "56.268444"}
{"text": "Especially , Random forest can predict what features are important in the classification .It can process efficiently large data sets .Also it can be utilized as an effective method to estimate missing data .Bagging .Bagging is unstable learning algorithm for small data set if small changes in the training data will generate very diverse classifiers .", "label": "", "metadata": {}, "score": "56.332844"}
{"text": "Statist .Friedman , J. H. ( 1999a ) .Greedy function approximation : a gradient boosting machine .Ann .Statist .To appear .Friedman , J. H. ( 1999b ) .Stochastic gradient boosting .Technical report , Dept .", "label": "", "metadata": {}, "score": "56.54539"}
{"text": "Here , the authors addressed the problem related to the presence of noise ( i.e. , attacks ) in the training set .The proposed model composed of a set of ( independent ) application - specific modules .Each module , composed by multiple HMM ensembles , is trained using queries on a specific web application and , during the operational phase , outputs a probability value for each query on this web application .", "label": "", "metadata": {}, "score": "56.61841"}
{"text": "Then the computer might go on to the next rule of thumb to see how often a horse that won its previous race won again .In a third analysis , how often a horse with the largest winning streak continued winning might be examined , and so on .", "label": "", "metadata": {}, "score": "56.720207"}
{"text": "The reason may be either class imbalance in training dataset or 11 attack types in these two classes only appear in the test dataset , not the training set , and they constitute more than 50 % of the data .However , ensembles utilized the combined knowledge of multiple classifiers to improve the performance in these minority attacks classes .", "label": "", "metadata": {}, "score": "56.733803"}
{"text": "[ 27 ] studied the SVM , ANNs ( artificial neural networks ) , LGPs ( linear genetic programs ) , and MARSs ( Multivariate Adaptive Regression Splines ) for classification of KDD dataset into five classes .As these classifiers obtained better performance over the others to detect different classes of intrusion in terms of detection accuracy , attack severity , training & testing time ( scalability ) .", "label": "", "metadata": {}, "score": "56.803677"}
{"text": "6 , pp .447 - 458 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Retico , P. Delogu , M. E. Fantacci , I. Gori , and A. Preite Martinez , \" Lung nodule detection in low - dose and thin - slice computed tomography , \" Computers in Biology and Medicine , vol .", "label": "", "metadata": {}, "score": "56.873276"}
{"text": "338 - 345 , 1995 .M. V. Mahoney and P. K. Chan , \" An analysis of the 1999 DARPA / Lincoln laboratory evaluation data for network anomaly detection , \" Tech .Rep. CS-200302 , Computer Science Department , Florida Institute of Technology , 2003 .", "label": "", "metadata": {}, "score": "56.89788"}
{"text": "173 - 180 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. L. Allwein , R. E. Schapire , and Y. Singer , \" Reducing multiclass to binary : a unifying approach for margin classifiers , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "56.92199"}
{"text": "View at Google Scholar \u00b7 View at Scopus . Y. Guan , C. L. Myers , D. C. Hess , Z. Barutcuoglu , A. A. Caudy , and O. G. Troyanskaya , \" Predicting gene function in a hierarchical context with an ensemble of classifiers , \" Genome Biology , vol .", "label": "", "metadata": {}, "score": "56.923065"}
{"text": "Adaboost .Adaboost is one of the powerful methods for pattern recognition [ 46 ] .Adaboost classifier firstly introduced by Freund and Schapire [ 47 , 48 ] is an ensemble classifier composed of many weak classifiers for the two - class classification problem .", "label": "", "metadata": {}, "score": "56.940872"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" Experiments with a new boosting algorithm , \" in Proceedings of the 13th Conference on Machine Learning , pp .148 - 156 , 1996 .", "label": "", "metadata": {}, "score": "56.969055"}
{"text": "Li F. , Yang Y. ( 2003 )A Loss Function Analysis for Classification Methods in Text Categorization .International Conference on Machine Learning ( ICML ) : 472 - 479 .Liu T. , Yang Y. , Wan H. , Zeng H. , Chen Z. , Ma W. ( 2005 ) Support vector machines classification with a very large - scale taxonomy .", "label": "", "metadata": {}, "score": "57.03241"}
{"text": "Stacked generalization is a general method of using a high - level model to combine lower - level models to achieve greater predictive accuracy .We find that best results are obtained when the higher - level model combines the confidence ( and not just the predictions ) of the lower - level ones . ...", "label": "", "metadata": {}, "score": "57.079834"}
{"text": "4 , pp .217 - 225 , 2009 .View at Google Scholar .C. Xiang , P. C. Yong , and L. S. Meng , \" Design of multiple - level hybrid classifier for intrusion detection system using Bayesian clustering and decision trees , \" Pattern Recognition Letters , vol .", "label": "", "metadata": {}, "score": "57.10088"}
{"text": "Shape quantization and recognition with randomized trees .Neural Comput .Breiman , L. ( 1999a ) .Random forests .Breiman , L. ( 1999b ) .Prediction games and arcing algorithms .Neural Comput .Dietterich , T. ( 2000 ) .", "label": "", "metadata": {}, "score": "57.276955"}
{"text": "We noticed that 90 % of the information is contained in the first 50 principal components .We hence use only first 50 principal components as our new feature set .Naive Bayes classifiers are used once again as weak learners for AdaBoost in one against rest and all pairs settings .", "label": "", "metadata": {}, "score": "57.446"}
{"text": "\\ ] .Term \\(G(h_\\theta ) \\ ) is called the ' ' regularization term ' ' , measuring the complexity of any rule \\(h_\\theta\\ .\\ ) Exact definitions of the empirical risk term and the regularization term may differ in various classification methods .", "label": "", "metadata": {}, "score": "57.47076"}
{"text": "R. Polikar , L. Udpa , S. S. Udpa , and V. Honavar , \" Learn++ : An incremental learning algo - rithm for supervised neural networks , \" IEEE Transactions on Systems , Man and Cybernetics Part C : Applications and Reviews , vol .", "label": "", "metadata": {}, "score": "57.479862"}
{"text": "The classifiers obtained highest accuracies on different categories of intrusions are used to detect corresponding category of intrusions .They reported that classifier combination results the improvement of classification performance .They reported that probability of detection of 88.7 % , 97.3 % , 29.8 % , and 9.6 % with 0.4 % false alarm rate for Probe , DoS , U2R , and 0.1 % for R2L attack classes , respectively .", "label": "", "metadata": {}, "score": "57.602005"}
{"text": "The Annals of Statistics , 38 : 2 , 337 - 374 .MathSciNet .Hardle , W. ( 1991 ) .Smoothing Techniques : With Implementation in S. New York : Springer - Verlag .Johnson , M. , Geman , S. , Canon , S. , Chi , Z. , & Riezler , S. ( 1999 ) .", "label": "", "metadata": {}, "score": "57.625748"}
{"text": "Cross validation type selection is typically used for training the Tier 1 classifiers : the entire training dataset is divided into T blocks , and each Tier-1 classifier is first trained on ( a different set of ) T -1 blocks of the training data .", "label": "", "metadata": {}, "score": "57.65255"}
{"text": "The goal of this stage is to produce Troika 's final prediction .The inputs of the super classifier are the outputs produced by the metaclassifiers from the previous stage .In the training phase , the super classifier learns the conditions which enable one or more of the metaclassifiers to predict correctly or incorrectly .", "label": "", "metadata": {}, "score": "57.675076"}
{"text": "MFS was also robust to corruption by irrelevant features compared to the kNN classifier .Finally , we show that MFS is able to reduce both bias and variance components of error . by Vijay S. Iyengar , Chidanand Apte , Tong Zhang - In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2000 . \" ...", "label": "", "metadata": {}, "score": "57.755272"}
{"text": "44 - 51 , 2010 .View at Google Scholar .G. Kumar and K. Kumar , \" An information theoretic approach for feature selection , \" Security and Communication Networks , vol .5 , pp .178 - 185 , 2012 .", "label": "", "metadata": {}, "score": "57.75718"}
{"text": "Figure 8 shows a particular code matrix for a 5-class problem that uses 15 encodings .This encoding , suggested in ( Dietterich 1995 ) , is a ( pseudo ) exhaustive coding because it includes all possible non - trivial and non - repeating codes .", "label": "", "metadata": {}, "score": "57.814926"}
{"text": "1226 - 1238 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Scopus . H. I. Erdal , O. Karakurt , and E. Namli , \" High performance concrete compressive strength forecasting using ensemble models based on discrete wavelet transform , \" Engineering Applications of Artificial Intelligence , vol .", "label": "", "metadata": {}, "score": "57.842117"}
{"text": "However , these tasks require labeled data sets of su ciently high quality with adequate instances for training the predictive models .Much of the on - line data , particularly the unstructured variety ( e.g. , text ) , is unlabeled .", "label": "", "metadata": {}, "score": "57.849953"}
{"text": "Delivery Method : .References .Breiman , L. ( 1997 ) .Prediction games and arcing algorithms .Technical Report 504 , Dept .Statistics , Univ .California , Berkeley .Breiman , L. ( 1998a ) .Arcing classifiers ( with discussion ) .", "label": "", "metadata": {}, "score": "57.94537"}
{"text": "Machine Learning , 38 , 243 - 255 .CrossRef .McCallum , A. , & Nigam , K. ( 1998 ) .A comparison of event models for naive bayes text classification .Conference Proceedings AAAI-98 Workshop on Learning for Text Categorization .", "label": "", "metadata": {}, "score": "58.065094"}
{"text": "The result is a new algorithm which we term PAV - AdaBoost .We give several examples illustrating problems for which this new algorithm provides advantages in performance .Keywords . boosting isotonic regression convergence document classification k nearest neighbors .Share .", "label": "", "metadata": {}, "score": "58.162956"}
{"text": "For example , natural language text is widely available in many forms ( e.g. , electronic mail , news articles , reports , and web page contents ) .Categorization of data is a common activity which can be automated to a large extent using supervised learning methods .", "label": "", "metadata": {}, "score": "58.179672"}
{"text": "11503 - 11509 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. Kubota , A. K. Jerebko , M. Dewan , M. Salganicoff , and A. Krishnan , \" Segmentation of pulmonary nodules of various densities with morphological approaches and convexity models , \" Medical Image Analysis , vol .", "label": "", "metadata": {}, "score": "58.36017"}
{"text": "\\ ) Na\u00efve Bayes is \" na\u00efve \" in that it assumes conditional independence between all feature values in a feature vector .Which this assumption is clearly violated to text classification , Na\u00efve Bayes nevertheless produces fairly accurate classification rules in many cases .", "label": "", "metadata": {}, "score": "58.481697"}
{"text": "26 ] grouped different classifier combination schemes into three main categories according to their architecture : ( 1 ) parallel ; ( 2 ) cascading ( or serial combination ) ; ( 3 ) hierarchical ( tree like ) .In the parallel architecture , all the individual classifiers are invoked independently , and their results are then combined by a combiner .", "label": "", "metadata": {}, "score": "58.563656"}
{"text": "Finally , Section 6 concludes the paper and presents future research directions .Intrusion Detection .An intrusion detection system ( IDS ) defined as \" an effective security technology , which can detect , prevent and possibly react to the computer attacks \" is one of the standard components in security infrastructures .", "label": "", "metadata": {}, "score": "58.567657"}
{"text": "Adaboost makes a committee of member weak classifiers by adaptively adjusting the weights at each loop .While the weights of the training patterns classified correctly by a weak classifier are decreased , the weights of the training patterns misclassified by the weak classifier are increased .", "label": "", "metadata": {}, "score": "58.581528"}
{"text": "The paper clearly shows that some researchers have applied their knowledge to address different issues at these phases for intrusion detection .But still there is a need to focus more on issues of each phase of ensemble learning .It is expected that new discoveries and a deepened understanding of different techniques suitable for different phases in ensemble learning of ID problem will be the subject of future work .", "label": "", "metadata": {}, "score": "58.723305"}
{"text": "This architecture is known as Adaptive Combination of Classi ers ( ACC ) .The system performs very well and is capable of detecting people even when all components of a person are not found .The performance of the system is signi cantly better than a full body . ...", "label": "", "metadata": {}, "score": "58.78607"}
{"text": "73 , no . 7 - 9 , pp .1533 - 1537 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .P. K. Chan and S. J. Stolfo , \" On the accuracy of meta - learning for scalable data mining , \" Journal of Intelligent Information Systems , vol . 8 , no . 1 , pp .", "label": "", "metadata": {}, "score": "58.913967"}
{"text": "4 , pp .1246 - 1254 , 2013 .View at Publisher \u00b7 View at Google Scholar .I. Mukherjee and S. Routroy , \" Comparing the performance of neural networks developed by using Levenberg - Marquardt and Quasi - Newton with the gradient descent algorithm for modelling a multiple response grinding process , \" Expert Systems with Applications , vol .", "label": "", "metadata": {}, "score": "59.04221"}
{"text": "We present an abstract algorithm for finding linear combinations of functions that minimize arbitrary cost functionals ( i.e functionals that do not necessarily depend on the margin ) .Many existing voting methods can be shown to be special cases of this abstract algorithm .", "label": "", "metadata": {}, "score": "59.151035"}
{"text": "One problem that faces all predictive techniques based on historical data is that past is not always prologue .Also , because boosting focuses on items that produce repeated errors , it can be influenced by extreme examples or incorrectly entered data .", "label": "", "metadata": {}, "score": "59.19213"}
{"text": "The authors reported superior performance of Troika over other stacking methods .Discussion .Which is better bagging or boosting ?Many researchers compared the two methods including some large - scale experiments [ 56 , 57 , 107 , 108 ] .", "label": "", "metadata": {}, "score": "59.236443"}
{"text": "Most of attacks are likely to generate multiple related alarms .This flood of mostly false alarms makes it very difficult to identify the hidden true positives ( i.e. , those alarms that correctly flag attacks ) [17 ] .Current intrusion - detection systems do not make it easy for network administrator to logically group related alerts .", "label": "", "metadata": {}, "score": "59.265766"}
{"text": "For multi - class DNA classification problems , practitioners usually adopt approaches that use decision trees consisting of binary classifiers .Finding the best tree topology requires exploring all possible tree topologies and is computationally prohibitive .We propose a computational framework based on feature primitives that eliminates the need of a decision tree of binary classifiers .", "label": "", "metadata": {}, "score": "59.28929"}
{"text": "-dimensional feature vector was obtained .The best feature ranking that performed with the mRMR method is 3rd moment , min , mean , std , max , and var .The number of best features performed with the mRMR method was the first 5 features which are 3rd moment , min , mean , std , and max .", "label": "", "metadata": {}, "score": "59.345943"}
{"text": "Selection of base classifiers may be done from pool of classifiers , which are trained using different induction algorithms ( called heterogeneous ensembles ) or the same induction algorithm ( called homogeneous ensembles ) .Many researcher generated ensemble by selecting heterogeneous base classifiers [ 23 , 24 , 27 - 29 , 36 ] .", "label": "", "metadata": {}, "score": "59.361584"}
{"text": "In their technique , signal acquisition is performed using a time - domain , thresholding , Finite State Automaton .This is followed by adaptive pre - filtering using a wavelet - domain Finite State Automaton .Feature extraction on acquired channel blockades is done by Hidden Markov Model processing ; and classification is done by Support Vector Machine ( SVM ) .", "label": "", "metadata": {}, "score": "59.373856"}
{"text": "Researchers have proposed different taxonomies to categorize ensembles .Since research of ensemble is continuously evolving , there is no existing taxonomy that covers every aspect of ensembles .The literature review of important taxonomies is as follows .Jain et al .", "label": "", "metadata": {}, "score": "59.374428"}
{"text": "Active learning is an approach to solving this problem and works by identifying a subset of the data that needs to be labeled and uses this subset to generate classi cation models .We present an active learning method that uses adaptive resampling in a natural way to signi cantly reduce the size of the required labeled set and generates a classi cation model that achieves the high accuracies possible with current adaptive resampling methods . .", "label": "", "metadata": {}, "score": "59.38433"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Freund and R. E. Schapire , \" Experiments with a new boosting algorithm , \" in Proceedings of the 30th International Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "59.413956"}
{"text": "Supervisor classifier selects the most suitable member of the ensemble on the basis of the available input data .Two additional components are incorporated in mixture of expert 's model : ( 1 ) a gating network ; ( 2 ) selector .", "label": "", "metadata": {}, "score": "59.451336"}
{"text": "An ensemble is obtained that consists of a weighted vote of the weak learners chosen by AdaBoost .Nanopore Detectors : Experimental Setup .Each experiment is conducted using one alpha - Hemolysin channel inserted into a diphytanoyl - phosphatidylcholine / hexadecane bilayer as shown in Figure 2 , where the bilayer is formed across a 20-micron diameter horizontal Teflon aperture [ 5 ] .", "label": "", "metadata": {}, "score": "59.46137"}
{"text": "262 - 294 , 2000 .View at Google Scholar .G. Kumar , K. Kumar , and M. Sachdeva , \" An empirical comparative analysis of feature reduction methods for intrusion detection , \" International Journal of Information and Telecommunication , vol .", "label": "", "metadata": {}, "score": "59.49108"}
{"text": "However , his de nition of \\margin \" di ers from Vapnik 's ( Vapnik , 1995[23 ] ) .Bauer and Kohavi present a study of such structures including bagging and boosting , oriented towards determining the circ ... . \" ...", "label": "", "metadata": {}, "score": "59.616665"}
{"text": "10 , pp .2201 - 2212 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .L. Khan , M. Awad , and B. Thuraisingham , \" A new intrusion detection system using support vector machines and hierarchical clustering , \" The International Journal on Very Large Data Bases , vol .", "label": "", "metadata": {}, "score": "59.62664"}
{"text": "Threshold Plurality Vote Method .This method is further generalization of majority vote method proposed by Xu et al .[ 73 ] .( iii ) Na\u00efve Bayes Decision Method : This method assumes the conditional independence between classifiers .The method selects the class with the highest posterior probability computed through the estimated class conditional probabilities and Bayes ' theorem [ 74 , 75 ] .", "label": "", "metadata": {}, "score": "59.681046"}
{"text": "Output code method works by manipulating the coding of classes in multiclass classification problems .Here ensembles are designed to partially correct errors performed by the base classifiers by exploiting the redundancy in the bit - string representation of the classes [ 25 , 105 ] .", "label": "", "metadata": {}, "score": "59.868908"}
{"text": "Finally , the representational reason is to address to cases when the chosen model can not properly represent the sought decision boundary , which is discussed under divide and conquer section above .Perhaps one of the earliest work on ensemble systems is Dasarathy and Sheela 's 1979 paper ( Dasarathy 1979 ) , which first proposed using an ensemble system in a divide - and - conquer fashion , partitioning the feature space using two or more classifiers .", "label": "", "metadata": {}, "score": "59.901344"}
{"text": "Suzuki et al .used the dataset of 20 CT scans ( 1.25 mm slice thickness and 0.6 mm pixel interval ) containing 195 noncalcified nodule patterns ( .3 mm ) [ 35 ] .Tartar et al .utilized low - dose CT images scanned from 71 different patients with a total of 121 nodules ( 8 - 20 mm nodule size interval ) , totaling 101 CT scans ( 10 mm slice thickness and 0.586 - 0.684 pixel interval ) [ 36 ] .", "label": "", "metadata": {}, "score": "59.938713"}
{"text": "This set of weights associated with the training data at each round t is denoted by D t ( i ) .In general , sampling weights associated with each example are initially set equal , i.e. a uniform sampling distribution is assumed .", "label": "", "metadata": {}, "score": "59.971245"}
{"text": "In this method , the ensembles are populated one classifier at the time .Each classifier is trained on selective subset of data from the original dataset .For the first base classifier , the data is selected uniformly .For successive classifiers , the sampling distribution is continuously updated so that instances that are more difficult to classify are selected more often than those that are easy to classify .", "label": "", "metadata": {}, "score": "60.06397"}
{"text": "Freund , Y. and Mason , L. ( 1999 ) .The alternating decision tree learning algorithm .In Machine Learning : Proceedings of the Sixteenth International Conference 124 - 133 .Friedman , J. H. ( 1991 ) .Multivariate adaptive regression splines ( with discussion ) .", "label": "", "metadata": {}, "score": "60.143295"}
{"text": "It has been observed by several authors , including those of the current paper , that AdaBoost is not an optimal method in this case .The problem seems to be that AdaBoost overemphasizes the atypical examples which eventually results in inferior rules .", "label": "", "metadata": {}, "score": "60.205788"}
{"text": "Web page taxonomy .They found the divide - and - conquer strategy not only addressed the scaling issue but also yielded better classification performance because of local optimization of classifiers based on domains and sub - domains .Finally , there are interesting challenges in new applications of text classification .", "label": "", "metadata": {}, "score": "60.24151"}
{"text": "Typically AdaBoost is used to perform classification for binary classification problems .To perform classification of five classes of molecules the AdaBoost approach was modified .It should be noted that x is no longer a vector of features .Features obtained for an 8GC hairpin after applying a 10 th order Daubechies wavelet filter .", "label": "", "metadata": {}, "score": "60.327682"}
{"text": "The systems that find true positive findings from the medical images are especially important in that they can also help radiologists in the identification of early stage pulmonary nodules .To best interpret the information revealed in the images , experienced physicians are required ; however , such experts may reach different diagnosis results for the same set of medical imaging .", "label": "", "metadata": {}, "score": "60.332558"}
{"text": "In the first layer , there are five ANFIS modules which are trained to explore the intrusive activity from the input data .Each ANFIS module belongs to one of the classes in the dataset each providing an output which specifies the degree of relativity of the data to the specific class .", "label": "", "metadata": {}, "score": "60.34365"}
{"text": "4 , pp .507 - 521 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Google Scholar .Z. S. Pan , S. C. Chen , G. B. Hu , and D. Q. Zhang , \" Hybrid neural network and C4.5 for misuse detection , \" in Proceedings of the International Conference on Machine Learning and Cybernetics , pp .", "label": "", "metadata": {}, "score": "60.575657"}
{"text": "The classifiers are generated by using different feature subset of training dataset .They proved empirically that result of the proposed method is improved .They performed experiments by using 41 features and 12 features of KDD dataset .They reported 98.39 % , 98.75 % , 99.70 % , and 99.09 % detection of Probe , DoS , U2R , and R2L attack classes using 41 features of KDD dataset .", "label": "", "metadata": {}, "score": "60.709446"}
{"text": "S. Peddabachigari , A. Abraham , C. Grosan , and J. Thomas , \" Modeling intrusion detection system using hybrid intelligent systems , \" Journal of Network and Computer Applications , vol .30 , no . 1 , pp .114 - 132 , 2007 .", "label": "", "metadata": {}, "score": "60.84841"}
{"text": "Research , vol .2 , pp .263 - 286 , 1995 .T. K. Ho , \" Random subspace method for constructing decision forests , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .20 , no . 8 , pp .", "label": "", "metadata": {}, "score": "60.854393"}
{"text": "Authors ' Affiliations .Department of Electrical Engineering and Computer Science , Tulane University .References .Akeson M , Branton D , Kasianowicz J , Brandin E , Deamer DW : Microsecond time - scale discrimination among polycytidilic acid , polyadenylic acid and polyuridylic acid as homopolymers or as segments within single RNA molecules .", "label": "", "metadata": {}, "score": "60.94476"}
{"text": "September 5 - 7 , 2001 .Boosting trees for anti - spam email filtering .Conference Proceedings RANLP2001 , Tzigov Chark , Bulgaria .Collins , M. , Schapire , R. E. , & Singer , Y. ( 2002 ) .", "label": "", "metadata": {}, "score": "60.970245"}
{"text": "7 , no . 3 , pp .788 - 792 , 1996 .G. Giacinto and F. Roli , \" Approach to the automatic design of multiple classifier systems , \" Pattern Recognition Letters , vol .22 , no . 1 , pp .", "label": "", "metadata": {}, "score": "61.034363"}
{"text": "I. Corona , D. Ariu , and G. Giacinto , \" HMM - web : a framework for the detection of attacks against web applications , \" in Proceedings of the IEEE International Conference on Communications ( ICC ' 09 ) , June 2009 .", "label": "", "metadata": {}, "score": "61.070625"}
{"text": "M. Sabhnani and G. Serpen , \" Application of machine learning algorithms to KDD intrusion detection dataset within misuse detection context , \" in Proceedings of the International Conference on Machine Learning ; Models , Technologies and Applications ( MLMTA ' 03 ) , pp .", "label": "", "metadata": {}, "score": "61.18297"}
{"text": "MathSciNet .Ratsch , G. , Mika , S. , & Warmuth , M. K. ( 2001 ) .On the Convergence of Leveraging ( NeuroCOLT2 Technical Report 98 ) .London : Royal Holloway College .Ratsch , G. , Onoda , T. , & Muller , K.-R. Soft margins for AdaBoost .", "label": "", "metadata": {}, "score": "61.184723"}
{"text": "Measurements of compactness , roundness , circularity and ellipticity are computed by the definitions given in Table 1 [ 37 ] .Feature Selection .The mRMR Method .The mRMR ( minimum Redundancy Maximum Relevance ) method from the feature selection methods has been providing shorter calculation time and higher accuracy for the classifier .", "label": "", "metadata": {}, "score": "61.19703"}
{"text": "Inform . and Comput .Freund , Y. and Schapire , R. ( 1996a ) .Game theory , on - line prediction and boosting .In Proceedings of the Ninth Annual Conference on Computational Learning Theory 325 - 332 .Freund , Y. and Schapire , R. E. ( 1996b ) .", "label": "", "metadata": {}, "score": "61.213623"}
{"text": "View at Scopus .W. Leigh , R. Purvis , and J. M. Ragusa , \" Forecasting the NYSE composite index with technical analysis , pattern recognizer , neural network , and genetic algorithm : a case study in romantic decision support , \" Decision Support Systems , vol .", "label": "", "metadata": {}, "score": "61.24624"}
{"text": "View at Publisher \u00b7 View at Google Scholar .K. Suzuki , S. G. Armato III , F. Li , S. Sone , and K. Doi , \" Massive training artificial neural network ( MTANN ) for reduction of false positives in computerized detection of lung nodules in low - dose computed tomography , \" Medical Physics , vol .", "label": "", "metadata": {}, "score": "61.26935"}
{"text": "18 - 28 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. C. Ponce , \" Intrusion detection system with artificial intelligence , \" in Proceedings of the FIST Conference , Universidad Pontificia Comillas de Madrid , 2004 , edition : 1/28 .", "label": "", "metadata": {}, "score": "61.430405"}
{"text": "565 - 570 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . S. Iwano , T. Nakamura , Y. Kamioka , M. Ikeda , and T. Ishigaki , \" Computer - aided differentiation of malignant from benign solitary pulmonary nodules imaged by high - resolution CT , \" Computerized Medical Imaging and Graphics , vol .", "label": "", "metadata": {}, "score": "61.43267"}
{"text": "L. Lam and C. Y. Suen , \" Application of majority voting to pattern recognition : an analysis of its behavior and performance , \" IEEE Transactions on Systems , Man , and Cybernetics A , vol .27 , no .", "label": "", "metadata": {}, "score": "61.449604"}
{"text": "Abstract .When using squared error loss , bias and variance and their decomposition of prediction error are well understood and widely used concepts .However , there is no universally accepted de nition for other loss functions .Numerous attempts have been made to extend these concepts beyond squared ... \" .", "label": "", "metadata": {}, "score": "61.476845"}
{"text": "View at Publisher \u00b7 View at Google Scholar .V. Engen , Machine learning for network based intrusion detection [ Ph.D. thesis ] , Bournemouth University , June 2010 .G. D. Guvenir , \" Classification by voting feature intervals , \" in Proceedings of the European Conference on Machine Learning , pp .", "label": "", "metadata": {}, "score": "61.530006"}
{"text": "Nonavailability of signatures of novel attacks in databases leads to high false alarm rate and low detection accuracy .In fact , practitioners as well as researchers have observed that IDSs can easily trigger thousands of alarms per day , upto 99 % of which are false positives ( i.e. , alarms that were mistakenly triggered by benign events ) [", "label": "", "metadata": {}, "score": "61.57323"}
{"text": "10 , pp .921 - 933 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. Hanamiya , T. Aoki , Y. Yamashita , S. Kawanami , and Y. Korogi , \" Frequency and significance of pulmonary nodules on thin - section CT in patients with extrapulmonary malignant neoplasms , \" European Journal of Radiology , vol .", "label": "", "metadata": {}, "score": "61.58222"}
{"text": "5 , pp . 1651 - 1686 , 1998 .Y. S. Huang and C. Y. Suen , \" Behavior - knowledge space method for combination of mul - tiple classifiers , \" Proc . of IEEE Computer Vision and Pattern Recog .", "label": "", "metadata": {}, "score": "61.681965"}
{"text": "View at Scopus .T.-K. An and M.-H. Kim , \" A new diverse AdaBoost classifier , \" in Proceedings of the International Conference on Artificial Intelligence and Computational Intelligence ( AICI ' 10 ) , pp .359 - 363 , Sanya , China , October 2010 .", "label": "", "metadata": {}, "score": "61.73706"}
{"text": "4 , pp .525 - 534 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .R. C. Hardie , S. K. Rogers , T. Wilson , and A. Rogers , \" Performance analysis of a new computer aided detection system for identifying lung nodules on chest radiographs , \" Medical Image Analysis , vol .", "label": "", "metadata": {}, "score": "61.7382"}
{"text": "Tests are done on a PC with Intel Core i7 , 1.90 GHz CPU , and 4.00 GB RAM .For evaluating the classifiers , 5-fold cross - validation technique is used .Results .Various classification methods are utilized for feature extraction and selection in medical pattern recognition .", "label": "", "metadata": {}, "score": "61.74347"}
{"text": "Decision Template Method .The main concept of decision template is to compare a prototypical answer of ensemble for prediction of class of a given instance .The method may use different similarity measure to evaluate the matching between matrix of classifiers output and matrix of templates .", "label": "", "metadata": {}, "score": "61.74528"}
{"text": "Another widespread application of text categorization is spam filtering , where email messages are classified into the two categories of spam and non - spam , respectively .Contents .Instead of manually classifying documents or hand - crafting automatic classification rules , statistical text categorization uses machine learning methods to learn automatic classification rules based on human - labeled training documents .", "label": "", "metadata": {}, "score": "61.749348"}
{"text": "HIDSs and NIDSs have been designed to perform misuse detection and anomaly detection .Anomaly based ID allows detecting unknown attacks for which the signatures have not yet been extracted [ 2 ] .In practice , anomaly detectors generate false alarms due , in large part , to the limited data used for training and to the complexity of underlying data distributions that may change dynamically over time .", "label": "", "metadata": {}, "score": "62.020096"}
{"text": "4 , pp .361 - 377 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. C. Tan , D. Gilbert , and Y. Deville , \" Multi - class protein fold classification using a New Ensemble Machine Learning Approach , \" Genome Informatics , vol .", "label": "", "metadata": {}, "score": "62.031136"}
{"text": "When using squared error loss , bias and variance and their decomposition of prediction error are well understood and widely used concepts .However , there is no universally accepted de nition for other loss functions .Numerous attempts have been made to extend these concepts beyond squared error loss .", "label": "", "metadata": {}, "score": "62.055378"}
{"text": "Hastie , T. and Tibshirani , R. ( 1990 ) .Generalized Additive Models .Chapman and Hall , London .Seminar f \u00a8ur Statistik ETH - Zentrum , LEO D72 CH-8092 Zurich Switzerland E - mail buhlmann@stat.math.ethz.ch Department of Statistics University of California Berkeley , California 94720 - 3860 .", "label": "", "metadata": {}, "score": "62.102734"}
{"text": "103 - 130 , 1997 .View at Google Scholar \u00b7 View at Scopus .R. O. Duda , P. E. Hart , and D. G. Stork , Pattern Classification , John Wiley & Sons , New York , NY , USA , 2nd edition , 2001 .", "label": "", "metadata": {}, "score": "62.223106"}
{"text": "Algebraic combiners .Algebraic combiners are non - trainable combiners , where continuous valued outputs of classifiers are combined through an algebraic expression , such as minimum , maximum , sum , mean , product , median , etc .Specifically .", "label": "", "metadata": {}, "score": "62.498955"}
{"text": "55 , no . 1 , pp .119 - 139 , 1997 .View at Google Scholar \u00b7 View at Scopus .B. Li , K. Chen , L. Tian , Y. Yeboah , and S. Ou , \" Detection of pulmonary nodules in CT images based on fuzzy integrated active contour model and hybrid parametric mixture model , \" Computational and Mathematical Methods in Medicine , vol .", "label": "", "metadata": {}, "score": "62.529465"}
{"text": "The main objective of IDS is to classify intrusive and nonintrusive network activities in an efficient manner .The process of intrusion detection involves the tasks : ( 1 ) data acquisition/ collection ; ( 2 ) data Preprocessing & feature selection ; ( 3 ) model selection for data analysis ; ( 4 ) classification and result analysis [ 3 ] .", "label": "", "metadata": {}, "score": "62.57332"}
{"text": "Winters - Hilt S , Vercoutere W , DeGuzman VS , Deamer D , Akeson M , Haussler D : Highly Accurate Classification of Watson - Crick Base - pairs on Termini of Single DNA Molecules .Biophysical Journal 2003 , 84 : 967 - 976 .", "label": "", "metadata": {}, "score": "62.744274"}
{"text": "[42 ] proposed a 3-tier hybrid approach to detect intrusions .First tier of system is a signature - based approach to filter the known attacks using black list concept .Second tier of system is anomaly detector that uses the white list concept to distinguish the normal and attack traffic that has by passed first tier .", "label": "", "metadata": {}, "score": "62.89949"}
{"text": "View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .T. G. Dietterich , \" Ensemble methods in machine learning , \" in Proceedings of the Multiple Classifier Systems .First International Workshop ( MCS ' 00 ) , J. Kittler and F. Roli , Eds . , vol .", "label": "", "metadata": {}, "score": "63.050274"}
{"text": "The results are compared to similar techniques in the literature by using standard measures .The proposed approach with the hybrid features results in 90.7 % classification accuracy ( 89.6 % sensitivity and 87.5 % specificity ) .Introduction .Computer aided detection ( CAD ) system is an extremely important task for the detection of pulmonary nodules in medical images .", "label": "", "metadata": {}, "score": "63.1395"}
{"text": "First objective is to present an updated review of ensembles and their taxonomies in general for supervised classification .Third objective is to highlight research gaps and directions in developing efficient ensemble for ID .Paper Overview The rest of paper is organized as follows .", "label": "", "metadata": {}, "score": "63.14806"}
{"text": "AdaBoost is outlined in Algorithm 1 below .It is interesting to note that \u03b1 t measures the importance assigned to the hypothesis h t and it gets larger as the training error \u03b5 t gets smaller .The final classification decision H of a test point x is a weighted majority vote of the weak hypotheses .", "label": "", "metadata": {}, "score": "63.15854"}
{"text": "Two - dimensional principal component analysis ( 2D - PCA ) applied to dataset .Method 2 .Statistical features obtained from 2D - PCA values .Method 3 .Geometric features obtained by using the regional descriptors of the 2D patterns based on the basic morphological shape information .", "label": "", "metadata": {}, "score": "63.20344"}
{"text": "In addition , several approaches have been proposed to detect pulmonary nodules in thin - slice helical computed tomography images [ 17 , 18 ] .Similar techniques are introduced by using genetic algorithm with the random subspace method [ 19 , 20 ] , a single support vector machine [ 21 ] , and random forest classifiers [ 22 , 23 ] .", "label": "", "metadata": {}, "score": "63.237923"}
{"text": "Strong negatives are handed to the next node ( another binary classifier ) in the decision tree .Although it can be automated ( removing the expert from the problem application ) , the technique requires exploring all possible topologies of the SVM decision tree structure to be comprehensive .", "label": "", "metadata": {}, "score": "63.35134"}
{"text": "Vercoutere W , Winters - Hilt S , Olsen H , Deamer D , Haussler D , Akeson M : Rapid discrimination among individual DNA hairpin molecules at single - nucleotide resolution using an ion channel .Nature Biotechnology 2001 , 19 ( 3 ) : 248 - 252 .", "label": "", "metadata": {}, "score": "63.381912"}
{"text": "In AdaBoost . M1 , bootstrap training data samples are drawn from a distribution \\(D\\ ) that is iteratively updated such that subsequent classifiers focus on increasingly difficult instances .This is done by adjusting \\(D\\ ) such that previously misclassified instances are more likely to appear in the next bootstrap sample .", "label": "", "metadata": {}, "score": "63.38264"}
{"text": "Conference Proceedings 13thCOLT .Palo Alto , California .Ayer , M. , Brunk , H. D. , Ewing , G. M. , Reid , W. T. , & Silverman , E. ( 1954 ) .An empirical distribution function for sampling with incomplete information .", "label": "", "metadata": {}, "score": "63.384674"}
{"text": "2006 ) , with a total of approximately 800,000 documents and a vocabulary of over 4 million unique words .Figure 1 shows the power - law correspondence between the category size ( in terms of the number of documents ) on the X - axis and the number of same - sized categories on the Y - axis .", "label": "", "metadata": {}, "score": "63.630173"}
{"text": "Error - correcting Output Coding ( ECOC ) [ 100 ] , and data - driven ECOC [ 101 ] .( vii ) Troika .Troika is an improvement to stacking proposed by Menahem et al .[ 106 ] .", "label": "", "metadata": {}, "score": "63.73642"}
{"text": "The classification results obtained are summarized in Table 1 and Table 2 .Table 1 .Results of one against rest approach on principal components obtained from the HMM projections .In order to obtain a single classifier for classifying all five molecules a decision tree structure is used , where each of the nodes is a binary classifier which classifies the input into two groups .", "label": "", "metadata": {}, "score": "63.88889"}
{"text": "These di erences stem from disagreement as to the essential characteristics thatvariance and bias should display .This paper suggests an explicit list of rules that we feelany \\reasonable \" set of de nitions should satisfy .Using this framework , bias and variance de nitions are produced which generalize to any symmetric loss function .", "label": "", "metadata": {}, "score": "64.08703"}
{"text": "Minimum rule .\\(\\alpha\\rightarrow \\infty \\Rightarrow\\ ) maximum rule .Voting based methods .The ensemble then chooses class J that receives the largest total vote : .Majority ( plurality ) voting .Under the condition that the classifier outputs are independent , it can be shown the majority voting combination will always lead to a performance improvement .", "label": "", "metadata": {}, "score": "64.101"}
{"text": "Instead of simple binary voting a weak hypothesis is allowed to vote for or against a classification with a variable strength or confidence .The Pool Adjacent Violators ( PAV ) algorithm is a method for converting a score into a probability .", "label": "", "metadata": {}, "score": "64.35103"}
{"text": "6515 of Proceedings of the SPIE , San Diego , Calif , USA , 2007 .View at Publisher \u00b7 View at Google Scholar .G. D. Rubin , J. K. Lyo , D. S. Paik et al . , \" Pulmonary nodules on multi - detector row CT scans : performance comparison of radiologists and computer - aided detection , \" Radiology , vol .", "label": "", "metadata": {}, "score": "64.362976"}
{"text": "195 - 204 , 2008 .View at Publisher \u00b7 View at Google Scholar .J. Xiao and H. Song , \" A novel intrusion detection method based on adaptive resonance theory and principal component analysis , \" in Proceedings of the International Conference on Communications and Mobile Computing ( CMC ' 09 ) , pp .", "label": "", "metadata": {}, "score": "64.54905"}
{"text": "They generated HMMs models as base classifiers by training them using different number of HMM states and random initializations .They applied multiple HMM to dataset and final prediction is computed by exploiting all Boolean functions applied to the ROC curves .", "label": "", "metadata": {}, "score": "64.61401"}
{"text": "Thus , when any physician detects a pulmonary nodule on the CT slices , he / she chooses the largest 2D pattern which is labeled and used in the dataset .For comparative analysis , it is examined recently and reported that CAD systems have utilized the LIDC ( Lung Image Database Consortium ) database to evaluate detection systems [ 32 - 34 ] .", "label": "", "metadata": {}, "score": "64.69046"}
{"text": "Ratsch , G. ( 1998 ) .Ensemble learning methods for classification .Masters thesis , Dept .Computer Science , Univ .Potsdam .Ratsch , G. , Onoda , T. and Muller , K. R. ( 2000 ) .Soft margins for AdaBoost .", "label": "", "metadata": {}, "score": "65.14039"}
{"text": "A common ANN is the multilayer perceptron ( MLP ) algorithm which is made up from three layers as shown in Figure 7 .The ANN is trained by entering information from the input layer through the hidden and output layers of the network [ 40 ] .", "label": "", "metadata": {}, "score": "65.16778"}
{"text": "All CT images are in size of .Feature Extraction .Two - Dimensional Principal Component Analysis ( 2D - PCA ) .Principal component analysis ( PCA ) is defined as a classical dimension reduction method for feature extraction and data representation technique widely used in the areas of pattern recognition , computer vision , and signal processing [ 28 ] .", "label": "", "metadata": {}, "score": "65.2453"}
{"text": "The distribution \\(D\\ ) starts out as uniform ( Equation 3 in Figure 6 ) , so that all instances have equal probability to be drawn into the first data subset \\(S_1\\ .\\ )At each iteration t , a new training set is drawn , and a weak classifier is trained to produce a hypothesis \\(h_t\\ .", "label": "", "metadata": {}, "score": "65.37779"}
{"text": "To cover various aspects of ensembles , many researchers proposed different taxonomies for ensembles .Keeping the advantages of AI - based techniques over other techniques and ensembles , many researchers proposed AI - based ensembles for ID .However , there exists no comprehensive review of taxonomies of ensembles ( in general ) and AI - based ensembles for intrusion detection ( ID ) ( in specific ) .", "label": "", "metadata": {}, "score": "65.49801"}
{"text": "B Pang , L Lee , S Vaithyanathan ( 2002 ) .Thumbs up ?Sentiment Classification using Machine Learning Techniques , Proceeding of the ACM Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 79 - 86 .", "label": "", "metadata": {}, "score": "65.512375"}
{"text": "274 - 283 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Sahiner , L. M. Hadjiiski , H.-P. Chan et al . , \" Effect of CAD on radiologists ' detection of lung nodules on thoracic CT scans : observer performance study , \" in Medical Imaging 2007 : Image Perception , Observer Performance , and Technology Assessment , vol .", "label": "", "metadata": {}, "score": "65.95124"}
{"text": "( viii ) Boolean Combination ( BC ) Methods .Boolean functions especially the conjunction AND and disjunction OR operations have recently been investigated to combine predictions of different classifiers within the ROC space [ 2 ] .These methods were shown to improve performance .", "label": "", "metadata": {}, "score": "66.0195"}
{"text": "The set of activities that violates security objectives is called intrusion .Out of these phases , the perfect detection of an intrusion is the most important .As only after correct detection of intrusion , correct reaction and recovery phase of information security can be implemented .", "label": "", "metadata": {}, "score": "66.20905"}
{"text": "43 - 53 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W.-J. Choi and T.-S. Choi , \" Genetic programming - based feature transform and classification for the automatic detection of pulmonary nodules on computed tomography images , \" Information Sciences , vol .", "label": "", "metadata": {}, "score": "66.220566"}
{"text": "M. P. Perrone and L. N. Cooper , \" When networks disagree : ensemble methods for hybrid neural networks , \" in Artificial Neural Networks for Speech and Vision , R. J. Mammone , Ed . , pp .126 - 142 , Chapman & Hall , London , UK , 1993 .", "label": "", "metadata": {}, "score": "66.3094"}
{"text": "5 , pp .416 - 422 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Chen , J. Zhang , Y. Xu , B. Chen , and K. Zhang , \" Performance comparison of artificial neural network and logistic regression model for differentiating lung nodules on CT scans , \" Expert Systems with Applications , vol .", "label": "", "metadata": {}, "score": "66.384476"}
{"text": "Heterogeneous ensembles exploit the characteristics of different classifiers to improve the results over single classifier .It is supported by the fact that different base classifiers perform differently upon different categories of intrusions ( e.g. , DoS , Probe , U2R , R2L , etc . )", "label": "", "metadata": {}, "score": "66.40175"}
{"text": "The AdaBoost algorithm .Obtain the error rates \u03b5 t of h t over the distribution D t such that . end .DNA Molecule Classification Using Boosted Naive Bayes .Given n classes and an input x , naive Bayes assigns to x the class label \u03c9 i for class i for which the posterior probability given by the following expression is maximum : .", "label": "", "metadata": {}, "score": "66.83772"}
{"text": "It can be seen that the overall classification performance improves as more feature types are added to the dataset .In Figure 11 , when only HMM features were used , the classification accuracy for the 8GC , 9AT , 9CG , 9GC , and 9TA molecules was 93.3 % , 82.3 % , 64.1 % , 83.1 % , and 84.3 % respectively .", "label": "", "metadata": {}, "score": "66.9488"}
{"text": "101 - 115 , 2008 .View at Google Scholar \u00b7 View at Scopus .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. N. Toosi and M. Kahani , \" A new approach to intrusion detection based on an evolutionary soft computing model using neuro - fuzzy classifiers , \" Computer Communications , vol .", "label": "", "metadata": {}, "score": "67.052666"}
{"text": "Table 4 : Comparison of the classification performance of reported CAD systems .Conclusions .In this paper , a new classification approach of pulmonary nodules for a CAD system from CT imagery is presented .An important feature of a CAD system desired by radiologists is that it is able to detect and classify small nodule patterns .", "label": "", "metadata": {}, "score": "67.25353"}
{"text": "The modules are network to monitor , Data collection & storage unit , Data analysis & processing unit , and signal [ 4 , 5 ] as depicted in Figure 1 .Based upon these modules , IDSs can be categorized into different classes like host - based IDS ( HIDS ) versus network - based IDS ( NIDS ) , misuse- or signature - based IDS versus anomaly - based IDS , Passive IDS versus Active IDS , and so forth [ 5 ] .", "label": "", "metadata": {}, "score": "67.3239"}
{"text": "A classification task forms the backbone of a computer aided detection system .In this paper , we propose a new classification approach for pulmonary nodules using hybrid features to be used in such a CAD system .The objective of the proposed study is to analyze the effect of the hybrid features on classification of pulmonary nodules .", "label": "", "metadata": {}, "score": "67.76807"}
{"text": "Multidetector computed tomography system is a very sensitive imaging modality to detect small pulmonary nodules .In previous studies , classification systems were developed by using the features of nodule candidate patterns with image - processing techniques [ 6 - 8 ] , by classifying the shape of pulmonary nodule patterns [ 9 , 10 ] and by using morphological features [ 11 , 12 ] .", "label": "", "metadata": {}, "score": "67.9849"}
{"text": "Background .During the past decade , nanopore detectors have been shown to be helpful in DNA molecule classification [ 1 - 5 ] .The detectors relate ionic current blockade measurements from a nanometer - scale pore to single molecule translocation [ 1 - 3 ] .", "label": "", "metadata": {}, "score": "68.35231"}
{"text": "Z. Zhang and X. Xie , \" Research on AdaBoost . M1 with random forest , \" in Proceedings of the 2nd International Conference on Computer Engineering and Technology ( ICCET ' 10 ) , vol .1 , pp .647 - 652 , Chengdu , China , April 2010 .", "label": "", "metadata": {}, "score": "68.464035"}
{"text": "We applied several rounds of AdaBoost on data sets consisting of following feature sets .Data set I : HMM Projections .Data set II : Data set I enhanced with first 50 principal components obtained from HMM projections , approximation and detail coefficients obtained using a haar filter .", "label": "", "metadata": {}, "score": "68.84011"}
{"text": "2732 - 2752 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .S. Axelsson , \" Research in intrusion detection system - a survey , \" Tech .Rep. CMU / SEI , 1999 .", "label": "", "metadata": {}, "score": "68.86189"}
{"text": "62 - 69 , 2013 .View at Publisher \u00b7 View at Google Scholar . A. Bosch , A. Zisserman , and X. Mu\u00f1oz , \" Image classification using random forests and ferns , \" in Proceedings of the IEEE 11th International Conference on Computer Vision ( ICCV ' 07 ) , Rio de Janeiro , Brazil , October 2007 .", "label": "", "metadata": {}, "score": "68.97061"}
{"text": "Sahiner et al .used the dataset having a total of 73 nodules by combining 28 CT scans from the LIDC and 20 scans from another database [ 34 ] .Rubin et al .used a total of 84 CT scans with a total of 143 nodules in the range of 3 - 30 mm in nodule size [ 33 ] .", "label": "", "metadata": {}, "score": "69.01515"}
{"text": "57 - 78 , 2012 .View at Google Scholar .Q. Wang , W. Kang , C. Wu , and B. Wang , \" Computer - aided detection of lung nodules by SVM based on 3D matrix patterns , \" Clinical Imaging , vol .", "label": "", "metadata": {}, "score": "69.104614"}
{"text": "A third theory postulates that the ensembles work because of the reduction in \\variance \" , caused by the aggl ...August 17 , 1999 .A Computer Program That Plays a Hunch .By JENNIFER 8 .LEE .It was intuition , after all , not statistics , that rewarded those who picked Charismatic to win the Kentucky Derby in May at 32-to-1 odds .", "label": "", "metadata": {}, "score": "69.947426"}
{"text": "We applied Haar , Daubechies and Symlets wavelet filters of different orders on the HMM projections and used them to enhance the existing feature set .Figure 9 and Figure 10 show the features obtained as a result of applying Haar and Daubechies wavelet filters .", "label": "", "metadata": {}, "score": "70.082664"}
{"text": "240 - 258 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. J. Su\u00e1rez - Cuenca , P. G. Tahoces , M. Souto et al . , \" Application of the iris filter for automatic detection of pulmonary nodules on computed tomography images , \" Computers in Biology and Medicine , vol .", "label": "", "metadata": {}, "score": "70.10271"}
{"text": "SantaLucia J : A unified view of polymer , dumbbell , and oligonucleotide DNA nearest - neighbor thermodynamics .Proceedings of National Academy of Sciences 1998 , 95 ( 4 ) : 1460 - 1465 .View Article .Song L , Hobaugh M , Shustak C , Cheley S , Bayley H , Gouaux JE : Structure of staphylococcal alpha - Hemolysin , a heptameric transmembrane pore .", "label": "", "metadata": {}, "score": "70.9388"}
{"text": "Early detection of pulmonary nodules is extremely important for the diagnosis and treatment of lung cancer .In this study , a new classification approach for pulmonary nodules from CT imagery is presented by using hybrid features .Four different methods are introduced for the proposed system .", "label": "", "metadata": {}, "score": "70.94696"}
{"text": "133 - 154 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .D.-T. Lin , C.-R. Yan , and W.-T. Chen , \" Autonomous detection of pulmonary nodules on CT images with a neural network - based fuzzy system , \" Computerized Medical Imaging and Graphics , vol .", "label": "", "metadata": {}, "score": "71.315414"}
{"text": "Artificial Neural Network .An artificial neural network ( ANN ) is one of the tools of artificial intelligence intended to imitate the complex operation of organizing and processing information of the neurons in the human brain .ANN can recognize patterns correlating strongly with a set of data which correspond to a class by a learning process , in which interneuron connection weights are utilized to store knowledge about specific features identified within the data [ 39 ] .", "label": "", "metadata": {}, "score": "71.67995"}
{"text": "177 - 189 , Springer , Cagliari , Italy , 2000 .X. Yao and M. Md.Islam , \" Evolving artificial neural network ensembles , \" IEEE Computational Intelligence Magazine , vol .3 , pp .31 - 42 , 2008 .", "label": "", "metadata": {}, "score": "71.887436"}
{"text": "2397 - 2407 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .L. Breiman , \" Random forests , \" Tech .Rep. , Statistics Department , University of California , Berkeley , Calif , USA , 1999 .", "label": "", "metadata": {}, "score": "71.904495"}
{"text": "5 - 6 , pp .585 - 594 , 2005 .View at Publisher \u00b7 View at Google Scholar .R. Gonzales and R. Woods , Image Processing , Prentice Hall , New York , NY , USA , 2007 .", "label": "", "metadata": {}, "score": "72.08043"}
{"text": "152 - 157 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .M. C. Lee , L. Boroczky , K. Sungur - Stasik et al . , \" Computer - aided diagnosis of pulmonary nodules using a two - step approach for feature selection and classifier ensemble construction , \" Artificial Intelligence in Medicine , vol .", "label": "", "metadata": {}, "score": "72.22774"}
{"text": "View at Google Scholar .J. McHugh , \" Testing intrusion detection systems : a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln laboratory , \" ACM Transactions on Information and System Security , vol .", "label": "", "metadata": {}, "score": "72.43153"}
{"text": "The Annals of Statistics , 17 : 2 453 - 555 .MathSciNet .Burges , C. J. C. ( 1999 ) .A tutorial on support vector machines for pattern recognition ( Available electronically from the author ) : Bell Laboratories , Lucent Technologies .", "label": "", "metadata": {}, "score": "73.179016"}
{"text": "These are denoted by 9GC , 9CG , 9TA , and 9AT .The sequence of the 9CG hairpin is 5'- CTTCGAACG TTTT CGTTCGAAG -3 ' .The base - pairing region is underlined .An eight base - pair DNA hairpin with a 5'-GC-3 ' terminus was also tested .", "label": "", "metadata": {}, "score": "73.25476"}
{"text": "The geometric features include the area , perimeter , diameter , solidity , eccentricity , aspect ratio , compactness , roundness , circularity , and ellipticity of the patterns .The number of best features performed with the mRMR method was 5 features consisting of compactness , aspect ratio , area , solidity and ellipticity .", "label": "", "metadata": {}, "score": "73.57938"}
{"text": "View Article PubMed .Kasianowicz J , Brandin E , Deamer DW : Characterization of individual polynucleotide molecules using a membrane channel .Proceedings of National Academy of Sciences 1996 , 93 ( 24 ) : 13770 - 13773 .View Article .", "label": "", "metadata": {}, "score": "73.625015"}
{"text": "The geometric features consist of the area , perimeter , diameter , solidity , eccentricity , aspect ratio , compactness , roundness , circularity , ellipticity of the patterns in this study .These features are given by its definitions in Table 1 .", "label": "", "metadata": {}, "score": "73.69652"}
{"text": "11 , pp . 169 - 198 , 1999 .View at Google Scholar \u00b7 View at Scopus . H. Fan and H. Wang , \" Preditcing protein subcellular location by AdaBoost . M1 algorithm , \" in Proceedings of the 2nd International Conference on Artificial Intelligence , Management Science and Electronic Commerce ( AIMSEC ' 11 ) , pp .", "label": "", "metadata": {}, "score": "73.89264"}
{"text": "It is obviously shown that the performance results of a CAD system can differ significantly depending on those variables .A single 2D slice is selected for each 3D object as seen in Figure 1 .Pulmonary nodules are observed on a several slice range of the whole CT scan .", "label": "", "metadata": {}, "score": "74.53815"}
{"text": "From these features , Solidity denotes the proportion of the pixels in the convex hull that are also in the region .Eccentricity depicts the eccentricity of the ellipse that has the same second moments as the region .Also it is the ratio of the distance between the foci of the ellipse and its major axis length .", "label": "", "metadata": {}, "score": "74.73437"}
{"text": "206 - 217 , 2003 .View at Google Scholar .R. Moskovitch , Y. Elovici , and L. Rokach , \" Detection of unknown computer worms based on behavioral classification of the host , \" Computational Statistics and Data Analysis , vol .", "label": "", "metadata": {}, "score": "75.22917"}
{"text": "The DNA oligonucleotides were synthesized using an ABI 392 Synthesizer , purified by PAGE , and stored at -70C in TE buffer .The prediction that each hairpin would adopt one base - paired structure was tested and confirmed using the DNA mfold server [ 7 ] .", "label": "", "metadata": {}, "score": "75.32009"}
{"text": "Duda R , Hart P , Stork D : Pattern Classification .Second Edition 2001 .Copyright .\u00a9 Iqbal et al .2006 .This article is published under license to BioMed Central Ltd.More by Robert Tibshirani .Abstract .", "label": "", "metadata": {}, "score": "75.561646"}
{"text": "The structure of the 9 bp hairpin shown here is rendered to scale using WebLab ViewerPro .Once the blockade current measurements are obtained , features are obtained using time domain finite state automata and wavelet pre - filtering followed by HMM profiling with expectation maximization .", "label": "", "metadata": {}, "score": "76.04188"}
{"text": "35 , no .12 , pp .5799 - 5820 , 2008 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . 13 , no .5 , pp .757 - 770 , 2009 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . S. Iwano , T. Nakamura , Y. Kamioka , and T. Ishigaki , \" Computer - aided diagnosis : a shape classification of pulmonary nodules imaged by high - resolution CT , \" Computerized Medical Imaging and Graphics , vol .", "label": "", "metadata": {}, "score": "76.2682"}
{"text": "Proceedings of National Academy of Sciences 2000 , 97 ( 3 ) : 1079 - 1084 .View Article .Meller A , Nivon L , Branton D : Voltage - driven DNA translocations through a nanopore .Physical Review Letters 2001 , 86 ( 15 ) : 3435 - 3438 .", "label": "", "metadata": {}, "score": "76.6641"}
{"text": "used the dataset containing 67 pulmonary nodules and 67 nonnodules obtained from 46 patients in our previous study [ 51 ] .In this study , a dataset containing 95 pulmonary nodules and 75 nonnodules patterns obtained from two - dimensional CT images from 63 patients is used .", "label": "", "metadata": {}, "score": "76.878555"}
{"text": "Signature HMM Projections for the five DNA hairpins ( 8GC , 9AT , 9CG , 9GC , 9TA ) are shown in Figure 4 , Figure 5 , Figure 6 , Figure 7 and Figure 8 .AdaBoost : An Overview .", "label": "", "metadata": {}, "score": "76.932335"}
{"text": "The paper will help the better understanding of different directions in which research of ensembles has been done in general and specifically : field of intrusion detection systems ( IDSs ) .Introduction .The threat of Internet attacks is quite real and frequent so this has increased a need for securing information on any network on the Internet .", "label": "", "metadata": {}, "score": "77.08178"}
{"text": "Random forest has given robust and improved results of classifications on standard data sets .It is providing very good competition to neural networks and ensemble techniques on different classification problems .Random forest is related to be special type of ensembles using bagging and random splitting methods to grow multiple trees [ 42 , 43 ] .", "label": "", "metadata": {}, "score": "77.25103"}
{"text": "For DNA classification , the alpha - Hemolysin pore is optimal due to the fact that single - stranded DNA ( ssDNA ) translocates in alpha - Hemolysin pore whereas double - stranded DNA ( dsDNA ) does not .Instead it is held in a vestibule of the pore [ 5 ] .", "label": "", "metadata": {}, "score": "78.22751"}
{"text": "Pulmonary Nodule Database and Imaging Protocol .In the study , dataset containing 95 pulmonary nodules and 75 nonnodules patterns obtained from two - dimensional ( 2D ) CT images from 63 patients was utilized .The 2D pulmonary nodule patterns are manually marked on CT image by radiologists .", "label": "", "metadata": {}, "score": "78.80774"}
{"text": "18 , no .6 , pp .1369 - 1378 , 2007 .View at Google Scholar .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. R. Quinlan , C4.5 Programs for Machine Learning , Morgan Kaufmann , San Mateo , Calif , USA , 1997 .", "label": "", "metadata": {}, "score": "78.821075"}
{"text": "The mRMR method was applied for feature selection .The entire dataset is randomly partitioned into training and testing sets .The entire dataset is divided into approximately 50 % training dataset and 50 % test dataset .The training dataset consists of 47 pulmonary nodules and 37 nonnodule patterns ( total number of patterns is 84 ) .", "label": "", "metadata": {}, "score": "79.89891"}
{"text": "1602 - 1617 , 2003 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Tartar , N. K\u0131l\u0131\u00e7 , and A. Akan , \" Bagging support vector machine approaches for pulmonary nodule detection , \" in Proceedings of the International Conference on Control , Decision and Information Technologies , Tunisia , May 2013 .", "label": "", "metadata": {}, "score": "80.45776"}
{"text": "The age distribution of the patients is illustrated in Figure 5 .The dataset was obtained from chest CT images of patients scanned by using \" Sensation 16 \" CT scanner ( Siemens Medical Systems ) between 2010 and 2012 at Radiology Department , Cerrahpasa Medicine Faculty , Istanbul University .", "label": "", "metadata": {}, "score": "81.06056"}
{"text": "Only the first 100 ms of blockade signal is used to identify each current signature .In the diagrams , the stick figure in blue is a two - dimensional section of the alpha - Hemolysin pore derived from x - ray crystallographic data [ 8 ] .", "label": "", "metadata": {}, "score": "83.62707"}
{"text": "Once one channel is formed , further pores are prevented from forming by thoroughly perfusing the cis chamber with buffer .Molecular blockade signals are then observed by mixing analytes into the cis chamber .Examination of DNA duplex ends using a voltage - pulse routine .", "label": "", "metadata": {}, "score": "83.7482"}
{"text": "View at Publisher \u00b7 View at Google Scholar .T. Fawett , \" ROC graphs : notes and practical considerations for data mining researches , \" Tech .Rep. HPL-2003 - 4 , HP Labs , 2003 .View at Google Scholar .", "label": "", "metadata": {}, "score": "83.874016"}
{"text": "A ring of threonines that circumscribe the narrowest 2.3 nm diameter section of the pore mouth is highlighted in green .In our working model , the four dT hairpin loop ( yellow ) is perched on this narrow ring of threonines , suspending the duplex stem in the pore vestibule [ 5 ] .", "label": "", "metadata": {}, "score": "84.2796"}
{"text": "6 , no.3 , pp .21 - 45 , 2006 .", "label": "", "metadata": {}, "score": "85.02403"}
{"text": "One end of double - stranded DNA ( dsDNA ) can be captured by the alpha - Hemolysin pore and held for an extended period of time [ 5 ] .Extensive characterization of the ionic current blockade associated with such an event is thus made possible .", "label": "", "metadata": {}, "score": "87.79082"}
{"text": "Recently , lung cancer is still considered a major cause of deaths from cancer worldwide .In particular , it is one of the main public health issues in the developed industrial countries [ 4 , 5 ] .This makes the treatment of lung cancer a very important task in the war against cancer .", "label": "", "metadata": {}, "score": "88.40285"}
{"text": "In time , duplex DNA is pulled into the pore by the applied potential causing an abrupt current decrease ( image B , with arrows and solid bar delineating region of blockade signal ) .After the 250 ms forward bias , the potential is briefly reversed ( -40 mV , trans side ) then set at 0 mV for 50 ms which clears the pore ( image C , with arrow indicating the voltage reversal spike ) .", "label": "", "metadata": {}, "score": "88.50551"}
{"text": "Conflict of Interests .The authors have no conflict of interests with the trademarks included in the paper .Acknowledgment .This work was supported by Scientific Research Projects Coordination Unit of Istanbul University , Project Numbers : 24014 , 14381 , 31474 , and 35119 .", "label": "", "metadata": {}, "score": "88.50879"}
{"text": "ACM Special Interest Group of Information Retrieval ( SIGIR ) : 42 - 49 .Zhang T and Oles F. ( 2001 )Text Categorization Based on Regularized Linear Classification Methods .Information Retrieval 4(1):5 - 31 .Classification of Pulmonary Nodules by Using Hybrid Features . 1 Department of Engineering Sciences , Istanbul University , 34320 Avc\u0131lar , Istanbul , Turkey 2 Department of Electrical and Electronics Engineering , Istanbul University , 34320 Avc\u0131lar , Istanbul , Turkey .", "label": "", "metadata": {}, "score": "89.02847"}
{"text": "The nine base - pair hairpins differ in only their terminal base - pairs .The variants are chosen to include the two possible Watson - Crick base - pairs and the two possible orientations of those base - pairs at the duplex ends .", "label": "", "metadata": {}, "score": "89.81056"}
{"text": "Other patterns in the lung parenchyma similar to nodules but not marked as \" nodule \" by the radiologists are selected as the member patterns of nonnodule class . years].mm .The diameter distribution of the nodules used in the database is shown in Figure 3 .", "label": "", "metadata": {}, "score": "91.316376"}
{"text": "At the start of each voltage cycle the voltage across the pore is reset to 0mV. A potential difference of 120 mV ( trans side positive ) is then applied for 250 ms , initially resulting in an open channel current of 120 pA", "label": "", "metadata": {}, "score": "92.25863"}
{"text": "A completed bilayer between the chambers is indicated by the lack of ionic current flow when a voltage is applied across the bilayer ( using Ag - AgCl electrodes ) .Once the bilayer is in place , a dilute solution of alpha - Hemolysin ( monomer ) is added to the cis chamber .", "label": "", "metadata": {}, "score": "92.47421"}
{"text": "One experiment called for a computer to make deductions based on statistical analysis of newspaper text .\" I asked the computer , ' Where is the Taj Mahal ? ' \" Dr. Freund said .\" It answered , ' Atlantic City , ' because according to the text that was where the Taj Mahal was mentioned most . \" Text categorization ( a.k.a . text classification ) is the task of assigning predefined categories to free - text documents .", "label": "", "metadata": {}, "score": "95.68729"}
{"text": "Variance reduction trends on ' boosted ' classifiers .Copyright \u00a9 2004 Virginia Wheway .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "103.482666"}
{"text": "Received 4 April 2012 ; Accepted 11 July 2012 .Academic Editor : Farid Melgani .Copyright \u00a9 2012 Gulshan Kumar and Krishan Kumar .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "105.29443"}
{"text": "Copyright \u00a9 2013 Ahmet Tartar et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "112.687805"}
