{"text": "Our treebank is still too small to achieve state - of - the - art results in parsing but we can still induce valuable lexical information from its output by using a large corpus and simple hypothesis testing techniques to outweigh the noise of the occasional error [ Bamman and Crane 2008 ] .", "label": "", "metadata": {}, "score": "50.80545"}
{"text": "We use a publicly available structured output SVM to create a max - margin syntactic aligner with a soft cohesion constraint .The resulting aligner is the first , to our knowledge , to use a discriminative learning method to train an ITG bitext parser . ... rence for links to appear near one another ( Vogel et al .", "label": "", "metadata": {}, "score": "51.981087"}
{"text": "If you want to obtain the same results , you can either POS - tag your corpus before tagging it ( see # 12 ) or you can disable the POS tagger in CoreNLP by updating the list of annotators : .", "label": "", "metadata": {}, "score": "52.03693"}
{"text": "The corpus was POS - tagged with the TreeTagger using this tagset , and lemmatized using the Morph - it ! lexicon , more information available here .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .", "label": "", "metadata": {}, "score": "52.353577"}
{"text": "In step 3 , we then align these 1 - 1 sentences using GIZA++ [ Och and Ney 2003 ] .This word alignment is performed in both directions in order to discover multi - word expressions ( MWEs ) in the source language .", "label": "", "metadata": {}, "score": "52.524456"}
{"text": "When the model is finished training , or when you want to test one of the intermediate models , you can run it using the standard LexicalizedParser commands .In our experiments , we found that simpler PCFG models actually make better underlying PCFG models .", "label": "", "metadata": {}, "score": "52.73511"}
{"text": "The supplied file makeSerialized.csh shows exactly what options we used to train the parsers that are included in the distribution .If you want to train the parser on a new language and/or treebank format , you can ( and people have done so ) , but you need to spend a while learning about the code , especially if you wish to develop language - specific features .", "label": "", "metadata": {}, "score": "54.496136"}
{"text": "There is a call , setConstraints , which you can make before using the LexicalizedParserQuery to run the parser .If you add a ParserConstraint object spanning a set of words , the parser will only produce parse trees which include that span of words as a constituent .", "label": "", "metadata": {}, "score": "54.99865"}
{"text": "( If you do not have a Hadoop installation , you might consider setting one up .Hadoop can be installed in a \" pseudo - distributed \" mode that allows it to use just a few machines or a number of processors on a single machine .", "label": "", "metadata": {}, "score": "55.162357"}
{"text": "Both grammar formats are extracted with the Thrax software .By default , the Joshua pipeline extract a Hiero grammar , but this can be altered with the --type samt flag .Other high - level options .The following command - line arguments control run - time behavior of multiple steps : . --threads N ( 1 ) .", "label": "", "metadata": {}, "score": "56.06256"}
{"text": "This vector can be translated into the other language , since we already know the translations of the seed words are already known .The lexicon builder 125 can search for the best matching context vector in the target language , and decide upon the corresponding word to construct a word mapping .", "label": "", "metadata": {}, "score": "56.48848"}
{"text": "See the parser.lexparser package documentation , the LexicalizedParser.main method documentation , the TreePrint class , and the documentation of variables in the Train , Test , and Options classes , and appropriate language - particular TreebankLangParserParams .For the rest , you need to look at the source code .", "label": "", "metadata": {}, "score": "56.702923"}
{"text": "es.tok.lc.grammar .You will also need to create a small \" glue grammar \" , in a file called model / hiero .glue that contains these rules that allow hiero - style grammars to reach the goal state : .Step 6 : Run minimum error rate training .", "label": "", "metadata": {}, "score": "56.97619"}
{"text": "An example command line for this process , with some of the most useful flags , is java -mx4 g edu.stanford.nlp.parser.dvparser.CacheParseHypotheses -model /path / to / pcfg / pcfg.ser.gz -treebank /path / to / wsj 200 - 2199 -output cached.wsj.ser.gz -numThreads 6 .", "label": "", "metadata": {}, "score": "57.35695"}
{"text": "Programmatically , you can do the same things by creating a TokenizerFactory with the appropriate options , such as : .getWordsFromString(str ) ) ; .There is nevertheless a potential cost of making tokenization changes .This normalization was added in the first place because the parser is trained on American English , normalized according to Penn Treebank conventions .", "label": "", "metadata": {}, "score": "58.222137"}
{"text": "We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .The CRF is conditioned on both the source and target texts , and thus allows for the use of arbitrary and overlapping features over these data .", "label": "", "metadata": {}, "score": "58.33531"}
{"text": "If you call the parser programmatically and then convert the parse tree to a list of grammatical relations , you have to call setGenerateOriginalDependencies(true ) on your instance of TreebankLanguagePack as shown in the following snippet : .Alternatively , if you use SemanticGraphFactory.makeFromTree ( ) to build a SemanticGraph from a constitueny tree , then use the following method with originalDependencies set to true .", "label": "", "metadata": {}, "score": "58.418877"}
{"text": "This corpus - based approach has since been augmented in two dimensions .[ 4 ] At the same time , researchers are also subjecting their corpora to more complex automatic processes to extract more knowledge from them .While word frequency and collocation analysis is fundamentally a task of simple counting , projects such as Kilgarriff 's Sketch Engine [ Kilgarriff et al .", "label": "", "metadata": {}, "score": "58.98271"}
{"text": "If you have a tuned model file , you can test new corpora by passing in a test corpus with references ( --test ) .You 'll need to provide a run name ( --name ) to store the results of this run , which will be placed under test / NAME .", "label": "", "metadata": {}, "score": "59.01803"}
{"text": "In another embodiment , a system may align text segments in comparable , non - parallel corpora , matching strings in the corpora , and using the matched strings to build a parallel corpus .The system may build a Bilingual Suffix Tree ( BST ) and traverse edges of the BST to identify matched strings .", "label": "", "metadata": {}, "score": "59.159073"}
{"text": "This allows the use of a much wider range of parallel corpora for training , and can be combined with a standard phrase - table using conventional smoothing methods .Experimental results demonstrate BLEU improvements for triangulated models over a standard phrase - based system .", "label": "", "metadata": {}, "score": "59.385666"}
{"text": "People are often confused about how to get from that example to parsing paragraphs of text .You need to split the text into sentences first and then to pass each sentence to the parser .To do that , we use the included class DocumentPreprocessor .", "label": "", "metadata": {}, "score": "59.47043"}
{"text": "Building a translation lexicon from comparable , non - parallel corpora US 8234106 B2 .Abstract .A machine translation system may use non - parallel monolingual corpora to generate a translation lexicon .The system may identify identically spelled words in the two corpora , and use them as a seed lexicon .", "label": "", "metadata": {}, "score": "59.549294"}
{"text": "To facilitate these tasks , the pipeline script : - Runs the complete SMT pipeline , from corpus normalization and tokenization , through model building , tuning , test - set decoding , and evaluation .Caches the results of intermediate steps ( using robust SHA-1 checksums on dependencies ) , so the pipeline can be debugged or shared across similar runs with ( almost ) no time spent recomputing expensive steps .", "label": "", "metadata": {}, "score": "59.60385"}
{"text": "We can contrast this computer - assisted lexicography with a new variety - which we might more properly call \" computational lexicography \" - that has emerged with the COBUILD project [ Sinclair 1987 ] of the late 1980s .This corpus evidence allows lexicographers to include frequency information as part of a word 's entry ( helping learners concentrate on common words ) and also to include sentences from the corpus that demonstrate a word 's common collocations - the words and phrases that it frequently appears with .", "label": "", "metadata": {}, "score": "59.97438"}
{"text": "PukWaC : the same as ukWaC , but with a further layer of annotation added , i.e. a full dependency parse .The parsing was performed with the MaltParser .Some useful information about the dependency relations used in PukWaC can be found on pp .", "label": "", "metadata": {}, "score": "60.277775"}
{"text": "The conversion code generally expects Penn Treebank style trees which have been stripped of functional tags and empty elements .This generally corresponds to the output of the Stanford , Charniak or Collins / Bikel parsers .The exception is that it gets value from the -TMP annotation on bare temporal NPs in order to recognize them as having temporal function ( tmod ) .", "label": "", "metadata": {}, "score": "60.582474"}
{"text": "deWaC : a 1.7 billion word corpus constructed from the Web limiting the crawl to the .de domain and using medium - frequency words from the SudDeutsche Zeitung corpus and basic German vocabulary lists as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger using this tagset , more information available here . sdewac a 0.88 billion word corpus derived from deWaC , duplicate sentences and some noise have been removed .", "label": "", "metadata": {}, "score": "60.63748"}
{"text": "You may use the parse method that takes a String argument to have this done for you or you may be able to use of classes in the process package , such as DocumentPreprocessor and PTBTokenizer for tokenization , much as the main method of the parser does .", "label": "", "metadata": {}, "score": "60.736294"}
{"text": "SdeWaC comes in two versions , in POS - tagged / lemmatized version or as a one sentence per line format , each supplemented with metadata ( e.g. parse error rate ) .More information on sdewac .Italian .itWaC : a 2 billion word corpus constructed from the Web limiting the crawl to the .", "label": "", "metadata": {}, "score": "60.75663"}
{"text": "Other arguments are as follows .This determines the language model code that will be used when decoding .These implementations are described in their respective papers ( PDFs : KenLM , BerkeleyLM ) .--lmfile FILE .Specifies a pre - built language model to use when decoding .", "label": "", "metadata": {}, "score": "60.760452"}
{"text": "This HeadFinder will give consistent left - branching binarization .If you would like to also get out the true probabilities that a vanilla PCFG parser would produce , there are a couple more options that you need to set : . -smoothTagsThresh", "label": "", "metadata": {}, "score": "60.97512"}
{"text": "This survey presents a tutorial overview of state - of - the - art SMT at the beginning of 2007 .We begin with the context of the current research , and then move to a formal problem description and an overview of the four main subproblems : translational equivalence modeling , mathematical modeling , parameter estimation , and decoding .", "label": "", "metadata": {}, "score": "61.021908"}
{"text": "The complexity is exponential in the size of individual grammar rules due to arbitrary re - orderings between the two languages .We develop a theory of binarization for synchronous context - free grammars and present a linear - time algorithm for binarizing synchronous rules when possible .", "label": "", "metadata": {}, "score": "61.048347"}
{"text": "BRIEF DESCRIPTION OF THE DRAWINGS .FIG .1 is a block diagram of a system for building a translation lexicon according to an embodiment .FIG .2 is a flowchart describing a method for building a translation lexicon from non - parallel corpora .", "label": "", "metadata": {}, "score": "61.419872"}
{"text": "Our approach , however , does have two clear advantages which complement those of traditional lexica : first , this method allows us to include statistics about actual word usage in the corpus we derive it from .And since we can run our word alignment at any time , we are always in a position to update the lexicon with the addition of new texts .", "label": "", "metadata": {}, "score": "61.544144"}
{"text": "SRILM is not used for representing language models during decoding ( and in fact is not supported , having been supplanted by KenLM and BerkeleyLM ) .The pipeline uses the Thrax grammar extractor , which is built on Hadoop .If you have a Hadoop installation , simply ensure that the $ HADOOP environment variable is defined , and the pipeline will use it automatically at the grammar extraction step .", "label": "", "metadata": {}, "score": "61.68091"}
{"text": "In this paper we present a novel approach for inducing word alignments from sentence aligned data .We use a Conditional Random Field ( CRF ) , a discriminative model , which is estimated on a small supervised training set .The CRF is conditioned on both the source and target texts , and thus allows for t ... \" .", "label": "", "metadata": {}, "score": "61.693104"}
{"text": "For example , in an implementation , an English - German translation lexicon was generated from a 1990 - 1992 Wall Street Journal corpus on the English side and a 1995 - 1996 German news wire ( DPA ) on the German side .", "label": "", "metadata": {}, "score": "62.080566"}
{"text": "The parser uses considerable amounts of memory .If you see a java.lang.OutOfMemoryError , you either need to give the parser more memory or to take steps to reduce the memory needed .( You give java more memory at the command line by using the -mx flag , for example -mx500 m . )", "label": "", "metadata": {}, "score": "62.35561"}
{"text": "By default , DocumentPreprocessor uses PTBTokenizer for tokenization .If you need to change that , either because you have a better Tokenizer for your domain or because you have already tokenized your text , you can do that by passing in a TokenizerFactory such as a WhitespaceTokenizerFactory for no tokenization beyond splitting on whitespace .", "label": "", "metadata": {}, "score": "62.37506"}
{"text": "You may also use Giza++ to create the alignments , although that program is a little unwieldy to install .To run the Berkeley aligner you first need to set up a configuration file , which defines the models that are used to align the data , how the program runs , and which files are to be aligned .", "label": "", "metadata": {}, "score": "62.422707"}
{"text": "If POS - tagging sentences prior to parsing is an option , that speeds things up ( less possibilities to search ) .The main tool remaining is to run multiple parsers at once in parallel .If you have a machine with enough memory and multiple cores , you can very usefully run several parsing threads at once .", "label": "", "metadata": {}, "score": "62.70549"}
{"text": "You will need a collection of syntactically annotated data such as the Penn Treebank to train the parser .If they are not in the same format as currently supported Treebanks , you may need to write classes to read in the trees , etc .", "label": "", "metadata": {}, "score": "62.731728"}
{"text": "Rayson , P. , and Wilson , A. ( 1996 ) .The ACAMRIT semantic tagging system : progress report .In L. J. Evett , and T. G. Rose ( eds ) Language Engineering for Document Analysis and Recognition , LEDAR , AISB96 Workshop proceedings , pp 13 - 20 .", "label": "", "metadata": {}, "score": "63.047916"}
{"text": "All models are used during search , i.e. they are incorporated directly into the log - linear model combination of the decoder .Phrase table smoothing with triplet lexicon models and with discriminative word lexicons are novel contributions .We also propose a new regularization technique for IBM model 1 by means of the Kullback - Leibler divergence with the empirical unigram distribution as regularization term .", "label": "", "metadata": {}, "score": "63.160774"}
{"text": "You can use it as follows : .There are several options , including one for batch - processing lots of files ; see the Javadoc documentation of the main method of PTBTokenizer .Parsing speed depends strongly on the distribution of sentence lengths - and on your machine , etc .", "label": "", "metadata": {}, "score": "63.173634"}
{"text": "We present a new generative alignment model which avoids these structural limitations , and show that it is effective when trained using both unsupervised and semi - supervised training methods . ... lar to work using discriminative log - linear models for alignment , which is similar to discriminative log - linear models used for the SMT decoding ( translation ) problem ( Och and Ney , 2002 ; Och , 2003 ) .", "label": "", "metadata": {}, "score": "63.30953"}
{"text": "The next step is to identify unknown target language words that are surrounded by aligned substrings .The source language word that corresponds to the \" well - aligned \" unknown is considered to be a possible translation .A suffix tree stores in linear space all suffixes of a given string .", "label": "", "metadata": {}, "score": "63.527977"}
{"text": "A brief demo program included with the download will demonstrate how to load the tool and start processing text .When using this demo program , be sure to include all of the appropriate jar files in the classpath .For part - of - speech tags and phrasal categories , this depends on the language and treebank on which the parser was trained ( and was decided by the treebank producers not us ) .", "label": "", "metadata": {}, "score": "63.55391"}
{"text": "If you successfully installed srilm in Step 1 , then you should be able to train a language model with the following command : . mkdir -p model / lm $ SRILM / bin / macosx64/ngram - count \\ -order 3 \\ -unk \\ -kndiscount1 -kndiscount2 -kndiscount3 \\ -text training / training .", "label": "", "metadata": {}, "score": "63.655476"}
{"text": "In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .We explore sourceto - target models with phrase - level as well as sentence - level scoring and target - to - source models with scoring on phrase level only .", "label": "", "metadata": {}, "score": "63.81067"}
{"text": "See especially the sample invocation in the parser.lexparser package documentation .The included file makeSerialized.csh effectively documents how the included grammars were made .The included file ParserDemo.java gives a good first example of how to call the parser programmatically , including getting Tree and typedDependencies output .", "label": "", "metadata": {}, "score": "63.865788"}
{"text": "Training the RNN parser is a two step process .First , because the RNN parser uses the parsings of a simpler PCFG parser to train , it is useful to precache the results of that parser before training the RNN parser .", "label": "", "metadata": {}, "score": "63.973305"}
{"text": "N ( 1 ) .This enables parallel operation over a cluster using the qsub command .This feature is not well - documented at this point , but you will likely want to edit the file $ JOSHUA / scripts / training / parallelize / LocalConfig .", "label": "", "metadata": {}, "score": "64.0015"}
{"text": "(Poster : ) .The Chinese semantic tagger has been developed by incorporating the Stanford Chinese word segmenter and the Chinese POS tagger into the USAS Java framework .The Chinese semantic lexicons have been automatically generated by translating the English semantic lexicons entries using a Chinese - English Dictionary ( Xiao et al . , 2010 ) and a LDC ( Linguistic Data Consortium ) English - Chinese Wordlist .", "label": "", "metadata": {}, "score": "64.04321"}
{"text": "It first runs a ( simpler ) PCFG parser and then an untyped dependency parser , and then runs a third parser which finds the parse with the best joint score across the two other parsers via a product model .This is described in the NIPS Fast Exact Inference paper .", "label": "", "metadata": {}, "score": "64.239426"}
{"text": "Yes , you can .However , for good results , you should make sure that you provide correctly tokenized input and use exactly the correct tag names .( That is , the input must be tokenized and normalized exactly as the material in the treebank underlying the grammar is . )", "label": "", "metadata": {}, "score": "64.25433"}
{"text": "We introduce a semi - supervised approach to training for statistical machine translation that alternates the traditional Expectation Maximization step that is applied on a large training corpus with a discriminative step aimed at increasing word - alignment quality on a small , manually word - aligned sub ... \" .", "label": "", "metadata": {}, "score": "64.32136"}
{"text": "You pass to the parse method a List .If the items in this list implement HasTag , such as being of type TaggedWord or CoreLabel , and the tag value is not null , then the parser will use the tags that you provide .", "label": "", "metadata": {}, "score": "64.4989"}
{"text": "When the grammar is extracted , it is compressed and placed at RUNDIR / grammar .gz .Language model .Before tuning can take place , a language model is needed .A language model is always built from the target side of the training corpus unless --no - corpus - lm is specified .", "label": "", "metadata": {}, "score": "64.76276"}
{"text": "ukWaC : a 2 billion word corpus constructed from the Web limiting the crawl to the .uk domain and using medium - frequency words from the BNC as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger .", "label": "", "metadata": {}, "score": "65.05274"}
{"text": "If you want a language model built from the target side of your training data , you 'll also need to pass in the training corpus ( --corpus ) .You can also specify an arbitrary number of additional language models with one or more --lmfile flags .", "label": "", "metadata": {}, "score": "65.30371"}
{"text": "4.2 Collocation Correction with Phrase - based SMT We implement our approach in the fram ... . \" ...We report on efforts to build large - scale translation systems for eight European language pairs .We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation .", "label": "", "metadata": {}, "score": "65.3198"}
{"text": "and then rerun the pipeline with the same invocation .Memory usage is a major consideration in decoding with Joshua and hierarchical grammars .In particular , SAMT grammars often require a large amount of memory .Many steps have been taken to reduce memory usage , including beam settings and test - set- and sentence - level filtering of grammars .", "label": "", "metadata": {}, "score": "65.48195"}
{"text": "2 shows a flowchart describing a method 200 for building a translation lexicon from non - parallel corpora .A word comparator 120 may be used to collect pairs of identical words ( block 205 ) .In the English German implementation described above , 977 identical words were found .", "label": "", "metadata": {}, "score": "65.634766"}
{"text": "We achieve most gains from the use of larger training corpora and basic modeling , but also show promising results from integrating more linguistic annotation . \" ...In this paper , we investigate lexicon models for hierarchical phrase - based statistical machine translation .", "label": "", "metadata": {}, "score": "65.79947"}
{"text": "Systems based on synchronous grammars and tree transducers promise to improve the quality of statistical machine translation output , but are often very computationally intensive .The complexity is exponential in the size of individual grammar rules due to arbitrary re - orderings between the two langu ... \" .", "label": "", "metadata": {}, "score": "65.81773"}
{"text": "ACL Workshop on SMT , 2007 . \" ...We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "65.85469"}
{"text": "( This mode is triggered when $ HADOOP is undefined ) .Theoretically , any grammar extractable on a full Hadoop cluster should be extractable in standalone mode , if you are patient enough ; in practice , you probably are not patient enough , and will be limited to smaller datasets .", "label": "", "metadata": {}, "score": "65.97366"}
{"text": "This indication would become stronger if , for example , the sequences following d and y in the two corpora would also be aligned .One way to verify this is to reverse both strings , build a BST for the reversed corpora ( a reverse BST ) , and look for a common path that diverges at the same d and y. .", "label": "", "metadata": {}, "score": "66.09655"}
{"text": "3 is a table showing results of an experiment utilizing the system of .FIG .1 .FIG .4 is a block diagram of a system for building a translation lexicon according to another embodiment .FIG .5 is a suffix tree .", "label": "", "metadata": {}, "score": "66.13945"}
{"text": "WaCkypedia_EN : a 2009 dump of the English Wikipedia ( about 800 million tokens ) , in the same format as PukWaC , including POS / lemma information , as well as a full dependency parse ( parsing performed with the MaltParser ) .", "label": "", "metadata": {}, "score": "66.23724"}
{"text": "In particular , you can now download a version of our CRF - based word segmenter ( similar to the system we used in the Second Sighan Bakeoff ) from our software page .However , for convenience , we also provide an ability for the parser to do word segmentation .", "label": "", "metadata": {}, "score": "66.350105"}
{"text": "Piao , S. , Rayson , P. , Archer , D. , McEnery , T. ( 2005 ) Comparing and combining a semantic tagger and a statistical tool for MWE extraction .Computer Speech and Language , ( Special issue on Multiword expressions ) , Volume 19 , issue 4 , pp .", "label": "", "metadata": {}, "score": "66.45351"}
{"text": "Memory usage expands roughly with the square of the sentence length .You may wish to set a -maxLength and to skip long sentences .The factored parser requires several times as much memory as just running the PCFG parser , since it runs 3 parsers .", "label": "", "metadata": {}, "score": "66.4648"}
{"text": "Extras includeExtras , boolean threadSafe , Predicate . filter , . boolean originalDependencies ) UCREL Semantic Analysis System ( USAS ) .The UCREL semantic analysis system is a framework for undertaking the automatic semantic analysis of text .The framework has been designed and used across a number of research projects and this page collects together various pointers to those projects and publications produced since 1990 .", "label": "", "metadata": {}, "score": "66.50032"}
{"text": "At the tuning step , an LM is built from the target side of the training data ( unless --no - corpus - lm is specified ) .This controls which code is used to build it .The default is a BerkeleyLM java class that computes a Kneser - Ney LM with a constant discounting and no count thresholding .", "label": "", "metadata": {}, "score": "66.58871"}
{"text": "Often , that works out okay , but , overall , results wo n't be quite as good .The default character encoding depends on the language that you are parsing .It is defined in the appropriate TreebankLanguagePack class .That is , it will never default to your platform default character encoding .", "label": "", "metadata": {}, "score": "66.66444"}
{"text": "We conclude with an overview of evaluation and notes on future directions . by Michel Simard , Nicola Ueffing , Pierre Isabelle - In Proceedings of the ACL-2007 Workshop on Statistical Machine Translation ( WMT-07 , 2007 . \" ...This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "66.74672"}
{"text": "With this small model , there are many untranslated words , and the quality of the translations is very low .In the next steps , we 'll show you how to train a model for a new language pair , using a larger training corpus that will result in higher quality translations .", "label": "", "metadata": {}, "score": "66.77627"}
{"text": "For those of you who are n't very familiar with Java , the arguments are the following : . -Xmx1 g -- this tells Java to use 1 GB of memory . -cp$ JOSHUA / bin -- this specifies the directory that contains the Java class files .", "label": "", "metadata": {}, "score": "66.86746"}
{"text": "The lexicon builder 125 extracts potential translation word pairs based on one or more clues .These clues may include similar spelling , similar context , preserving word similarity , and word frequency .When words are adopted into another language , their spelling might change slightly in a manner that can not be simply generalized in a rule , e.g. , \" website \" and \" Webseite .", "label": "", "metadata": {}, "score": "66.88225"}
{"text": "en.trigram.lm .( Note : the above assumes that you are on a 64-bit machine running Mac OS X. If that 's not the case , your path to ngram - count will be slightly different . )This will train a trigram language model on the English side of the parallel corpus .", "label": "", "metadata": {}, "score": "66.9167"}
{"text": "Automatic parsing generally requires the presence of a treebank - a large collection of manually annotated sentences - and a treebank 's size directly correlates with parsing accuracy : the larger the treebank , the better the automatic analysis .We are currently in the process of creating a treebank for Latin , and have just begun work on a one - million - word treebank of Ancient Greek .", "label": "", "metadata": {}, "score": "67.0851"}
{"text": "If you have an already - computed alignment , you can pass that to the script using this flag .Note that , in this case , you will want to skip data preparation and alignment using --first - step thrax ( the first step after alignment ) and also to specify --no - prepare - data so as not to retokenize the data and mess with your alignments .", "label": "", "metadata": {}, "score": "67.18292"}
{"text": "The extractions of all parallel phrases and of the translations took about 2 hours each .The experiments were run on a Linux \u00ae system 400 with an Intel \u00ae Pentium \u00ae 3 processor of 866 Mhz .A number of embodiments have been described .", "label": "", "metadata": {}, "score": "67.33035"}
{"text": "We can do recasing using SRILM , and can do detokenization with a perl script .To build a recasing model first train a language model on true cased English text : .$ SRILM / bin / macosx64/ngram - count \\ -unk \\ -order 5 \\ -kndiscount1 -kndiscount2 -kndiscount3 -kndiscount4 -kndiscount5 \\ -text training / training .", "label": "", "metadata": {}, "score": "67.3517"}
{"text": "FIG .5 shows the suffix tree 500 of string xyzyxzy .Note that if a suffix of a string is also a prefix of another suffix ( as would be the case for suffix zy of string xyzyxzy ) , a proper suffix tree can not be built for the string .", "label": "", "metadata": {}, "score": "67.387596"}
{"text": "Around 2009 , we parsed large volumes of text at a rate of about 1,000,000 sentences a day by distributing the work over 6 dual core / dual processor machines .Sure ! !These instructions concentrate on parsing from the command line , since you need to use that to be able to set most options .", "label": "", "metadata": {}, "score": "67.41202"}
{"text": "One effective way to do that is to post - edit the translations produced by a vanilla RBMT system using a specially - trained statistical machine translation ( SMT ) system .Our experiments indicate that this method is just as effective as manual customization of system dictionaries in reducing the need for manual post - editing . ... entence - length feature .", "label": "", "metadata": {}, "score": "67.43758"}
{"text": "Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .This paper presents a method which alleviates this problem by exploiting multiple translations of the same source phrase .", "label": "", "metadata": {}, "score": "67.59468"}
{"text": "This process is described in the several papers on the topic by Marie - Catherine de Marneffe .Confusingly , the current code to generate Stanford Dependencies requires a phrase structure ( CFG ) parse .It does n't require or use a dependency parse .", "label": "", "metadata": {}, "score": "67.64936"}
{"text": "These chunked files are all created in a subdirectory of RUNDIR / data / train / splits , named corpus .LANG.0 , corpus .LANG.1 , and so on .The pipeline parameters affecting alignment are : .Which aligner to use .", "label": "", "metadata": {}, "score": "67.706375"}
{"text": "To find the foreign phrases in the test set , we first create an easily searchable index , called a suffix array , for the training data .java -Xmx500 m -cp $ JOSHUA / bin/ \\ joshua.corpus.suffix_array.Compile \\ training / subsampled / subsample .", "label": "", "metadata": {}, "score": "67.73476"}
{"text": "If the word alignment process maps the source word be to both of these lemmas in a given sentence ( as in Figure 3 ) , the translation probability is divided evenly between them .The weighted list of translation equivalents we identify using this technique can provide the foundation for our further lexical work .", "label": "", "metadata": {}, "score": "67.857254"}
{"text": "There is considerable Javadoc documentation included in the javadoc/ directory of the distribution .You should start by looking at the javadoc for the parser.lexparser package and the LexicalizedParser class .( The documentation appearing on the nlp.stanford.edu website refers to code under development and is not necessarily consistent with the released version of the parser . )", "label": "", "metadata": {}, "score": "67.89483"}
{"text": "To each of these prefixes , a \" .\" is appended , followed by each of SOURCE ( --source ) and TARGET ( --target ) , which are file extensions identifying the languages .The SOURCE and TARGET files must have the same number of lines .", "label": "", "metadata": {}, "score": "67.905075"}
{"text": "We obtain gains in ... \" .Word alignments that violate syntactic correspondences interfere with the extraction of string - to - tree transducer rules for syntaxbased machine translation .We present an algorithm for identifying and deleting incorrect word alignment links , using features of the extracted rules .", "label": "", "metadata": {}, "score": "67.94243"}
{"text": "A MERT configuration file .A separate file with the list of the feature functions used in your model , along with their possible ranges .Create a MERT configuration file .In this example we name the file mert / mert . config .", "label": "", "metadata": {}, "score": "67.98535"}
{"text": "If you do n't have a Hadoop installation , there are still no worries .The pipeline will unroll a standalone installation and use it to extract your grammar .This behavior will be triggered if $ HADOOP is undefined .Make sure that the environment variable $ JOSHUA is defined , and you should be all set .", "label": "", "metadata": {}, "score": "68.04524"}
{"text": "We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation .The focus of this description is on improvements which were incorporated into the system over the last year .", "label": "", "metadata": {}, "score": "68.2318"}
{"text": "In general , though , you should not use this part of the feature and simply use \" .Yes , for the PCFG parser ( only ) .With a PCFG parser , you can give the option -printPCFGkBest n and it will print the n highest - scoring parses for a sentence .", "label": "", "metadata": {}, "score": "68.3833"}
{"text": "Building on this work , we demonstrate substan ... \" .For many years , statistical machine translation relied on generative models to provide bilingual word alignments .In 2005 , several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach .", "label": "", "metadata": {}, "score": "68.42524"}
{"text": "FIG .7 shows two corpora 705 , 710 , a bilingual lexicon 715 , and the corresponding BST 720 .Edges drawn with dotted lines mark ends of alignment paths through the tree .Their labels are ( unaligned ) continuations of the source language substrings from the respective paths .", "label": "", "metadata": {}, "score": "68.45226"}
{"text": "We present a novel translation model based on tree - to - string alignment template ( TAT ) which describes the alignment be - tween a source parse tree and a target string .A TAT is capable of generating both terminals and non - terminals and per - forming reordering at both low and high levels .", "label": "", "metadata": {}, "score": "68.46182"}
{"text": "We present a novel translation model based on tree - to - string alignment template ( TAT ) which describes the alignment be - tween a source parse tree and a target string .A TAT is capable of generating both terminals and non - terminals and per - forming reordering at both low and high levels .", "label": "", "metadata": {}, "score": "68.46182"}
{"text": "With clustering techniques , we can establish the semantic similarity between two words based on their appearance in similar contexts .In creating a lexicon with these features , we are exploring two strengths of automated methods : they can analyze not only very large bodies of data but also provide customized analysis for particular texts or collections .", "label": "", "metadata": {}, "score": "68.69018"}
{"text": "Note the correspondences with the files defined in the first step above .The prefixes can be either absolute or relative pathnames .This particular invocation assumes that a subdirectory input/ exists in the current directory , that you are translating from a language identified \" ur \" extension to a language identified by the \" en \" extension , that the training data can be found at input / train .", "label": "", "metadata": {}, "score": "68.791336"}
{"text": "In recent distributions , the models are included in a jar file inside the parser distribution .For example , in the 2012 - 11 - 12 distribution , the models are included in stanford - parser-2.0.4-models.jarThe easiest way to access these models is to include this file in your classpath .", "label": "", "metadata": {}, "score": "68.93927"}
{"text": "This alters the amount of memory available to Hadoop mappers ( passed via the mapred.child.java.opts options ) .--thrax - conf FILE .Use the provided Thrax configuration file instead of the ( grammar - specific ) default .The Thrax templates are located at $ JOSHUA / scripts / training / templates / thrax - TYPE .", "label": "", "metadata": {}, "score": "69.29569"}
{"text": "Based predominantly on the guidelines used for the Prague Dependency Treebank , our annotation style is also influenced by the Latin grammar of Pinkster ( 1990 ) , and is founded on the principles of dependency grammar [ Mel'\u010duk 1988 ] .", "label": "", "metadata": {}, "score": "69.52528"}
{"text": "Yes .The parser treats a filename as - as meaning to read from stdin and by default writes to stdout ( this can be changed with the -writeOutputFiles option ) .Note : the tokenizer uses lookahead , so you will either need to close the input to get the last sentence parsed , or use another option like -sentences newline .", "label": "", "metadata": {}, "score": "69.646255"}
{"text": "For example , .Also , should Thrax fail , it might be due to a memory restriction .By default , Thrax requests 2 GB from the Hadoop server .If more memory is needed , set the memory requirement with the --hadoop - mem in the same way as the --joshua - mem option is used .", "label": "", "metadata": {}, "score": "69.65674"}
{"text": "We show that our algorithm leads not only to improved alignments but also to machine translation outputs of higher quality . ... evious work on discriminative training for wordalignment differed most strongly from our approach in that it generally views word - alignment as a supervised task .", "label": "", "metadata": {}, "score": "69.7142"}
{"text": "This file contains the n - best translations , under the model .The first 10 lines that you see above are 10 best translations of the first sentence .Each line contains 4 fields .To get the 1-best translations for each sentence in the test set without all of the extra information , you can run the following command : . nbest.srilm.out \\ example / example .", "label": "", "metadata": {}, "score": "69.73878"}
{"text": "5 ] and applied in their respective translation systems for phrase table smoothing .Chiang et al .[ 15 ] suggested morphology - based and provenance - based improvements to the Koehn - Och - Marcu method recently ...The Joshua Pipeline .", "label": "", "metadata": {}, "score": "69.82234"}
{"text": "An example of this command line is java -mx12 g edu.stanford.nlp.parser.dvparser.DVParser -cachedTrees /path / to / cached .wsj.ser.gz -train -testTreebank /path / to / wsj 2200 - 2219 -debugOutputFrequency 500 -trainingThreads 8 -parser /path / to / pcfg / pcfg.ser.gz -dvIterations 40 -dvBatchSize 25 -wordVectorFile /path / to / embedding -model /scr / nlp / data / dvparser / wsj / train / averaged / averaged . ser.gz .", "label": "", "metadata": {}, "score": "69.91472"}
{"text": "# phrasemodel mono 0 0.5 .# wordpenalty weight . wordpenalty -2.721711092619053 .Finally , run the command to start MERT : . nohup java -cp $ JOSHUA / bin \\ joshua.zmert.ZMERT \\ -maxMem 1500 mert / mert . config & .", "label": "", "metadata": {}, "score": "70.00995"}
{"text": "From these , some high - quality lexical entries can be learned , but there will always be many words that are missing .These may be learned using the described methods .FIG .4 shows a system 400 for building a translation lexicon according to another embodiment .", "label": "", "metadata": {}, "score": "70.03114"}
{"text": "More detail can be found in Chiang ( 2007 ) [ PDF ] .SAMT grammars make use of a source- or target - side parse tree on the training data , projecting constituent labels down on the phrasal alignments in a variety of configurations .", "label": "", "metadata": {}, "score": "70.03136"}
{"text": "You 'll also need to set a JAVA_HOME environment variable .For Mac OS X this usually is done by typing : .These variables will need to be set every time you use Joshua , so it 's useful to add them to your . bashrc , .", "label": "", "metadata": {}, "score": "70.045784"}
{"text": "The availability of monolingual corpora has been enhanced greatly due to the digital revolution and widespread use of the World Wide Web .Methods for processing such resources can therefore greatly benefit the field .SUMMARY .In an embodiment , a system may be able to build a translation lexicon from comparable , non - parallel corpora .", "label": "", "metadata": {}, "score": "70.05069"}
{"text": "After you successfully compile SRILM , Joshua will need to know what directory it is in .You can type pwd to get the absolute path to the sirlm/ directory that you created .Once you 've figured out the path , set an SRILM environment variable by typing : .", "label": "", "metadata": {}, "score": "70.15457"}
{"text": "The fact that n has outgoing edge e indicates there is a mismatch on the subsequent words of those two sequences .Thus , in order to extract all aligned substrings , the system 400 traverses the BST on edges labeled with word pairs , and extract all paths that end either at the leaves or at nodes that have outgoing edges labeled only with source language words .", "label": "", "metadata": {}, "score": "70.28503"}
{"text": "For example , blocks in the flowcharts may be skipped or performed out of order and still produce desirable results .Also , the heuristics described herein may be combined with the alignment method described herein .Accordingly , other embodiments are within the scope of the following claims .", "label": "", "metadata": {}, "score": "70.42738"}
{"text": "The translated vector can be compared to other vectors in the second language .The lexicon builder 125 may perform a greedy search for the best matching similarity vectors and add the corresponding words to the lexicon .Another clue is based on the assumption that in comparable corpora , the same concepts should occur with similar frequencies .", "label": "", "metadata": {}, "score": "70.55765"}
{"text": "Genia ( biomedical English ) .Originally we used the treebank beta version reformatted by Andrew Clegg , his training split , but more recently ( 1.6.5 + ? ) we 've used the official Treebank , and David McClosky 's splits .", "label": "", "metadata": {}, "score": "70.71387"}
{"text": "In order to filter out such cases , the system 400 uses two simple heuristics : length and word content .The translation candidate must also be an open - class word .The algorithm 1000 for learning translations of unknown words is summarized in .", "label": "", "metadata": {}, "score": "70.74232"}
{"text": "Longman , London , pp .53 - 65 .Garside , R. , and Rayson , P. ( 1997 ) .Higher - level annotation tools .In . R. Garside , G. Leech , and A. McEnery ( eds . )", "label": "", "metadata": {}, "score": "70.75609"}
{"text": "conf ): .# # word - align .conf # # ---------------------- # # This is an example training script for the Berkeley # # word aligner .In this configuration it uses two HMM # # alignment models trained jointly and then decoded # # using the competitive thresholding heuristic .", "label": "", "metadata": {}, "score": "70.7906"}
{"text": "We also discuss the more general , and computationally more difficult , problem of finding good parsing strategies for non - binarizable rules , and present an approximate polynomial - time algorithm for this problem . \" ...For many years , statistical machine translation relied on generative models to provide bilingual word alignments .", "label": "", "metadata": {}, "score": "70.84558"}
{"text": "doi:10.1016/j.csl.2004.11.002 .Mudraya , O. , Babych , B. , Piao , S. , Rayson , P. , Wilson , A. ( 2006 ) .Developing a Russian semantic tagger for automatic semantic annotation .In proceedings of Corpus Linguistics 2006 , St. Petersburg , from 10 - 14 October 2006 .", "label": "", "metadata": {}, "score": "70.888214"}
{"text": "I. . ... andard word alignment and translation models described in Section IV - A1 .The system also used two large language models , both trained on the same data : a 4-gram language model using modified Kneser - Ney smoothing , and a suffix array language model with arbitrary history l .. \" ...", "label": "", "metadata": {}, "score": "70.8889"}
{"text": "Run the example model .To test to make sure that the decoder is installed properly , we 'll translate 5 sentences using a small translation model that loads quickly .The sentences that we will translate are contained in example / example .", "label": "", "metadata": {}, "score": "70.894714"}
{"text": "Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data . ...e probabalistic models developed at IBM by Brown et al .( 1993 ) , sometimes augmented by an HMMbased model or Och and Ney 's \" Model 6 \" ( Och and Ney , 2003 ) . \" ...", "label": "", "metadata": {}, "score": "70.9139"}
{"text": "This will enable us to include these later texts in our statistics on a word 's usage , and link these passages to the definition as well .Parsing .Two of the features we would like to incorporate into a dynamic lexicon are based on a word 's role in syntax : subcategorization and selectional preference .", "label": "", "metadata": {}, "score": "70.94739"}
{"text": "es.tok.lc : . /model\\ mert / news - dev2009 .es.tok.lc.grammar.raw \\ dev / news - dev2009 .es.tok.lc & .Next , sort the grammar rules and remove the redundancies with the following Unix command : . sort -u mert / news - dev2009 .", "label": "", "metadata": {}, "score": "71.04965"}
{"text": "S. Sharoff , P. Rayson , O. Mudraya , A. Wilson and T. McEnery ( 2004 ) .A tool for assisting translators using automatic semantic annotation .Presented at Corpus Use and Learning to Translate ( CULT - BCN ) Barcelona , January 22nd-24th 2004 .", "label": "", "metadata": {}, "score": "71.15796"}
{"text": "profile file .Download and Install Joshua .Running ant will compile the Java classes and link in srilm .If everything works properly , you should see the message BUILD SUCCESSFUL .If you get a BUILD FAILED message , it may be because you have not properly set the paths to SRILM and JAVA_HOME , or because srilm was not compiled properly , as described above .", "label": "", "metadata": {}, "score": "71.25325"}
{"text": "Verbs , adjectives , adverbs and other part of speech may be handled in a similar way .They might also provide useful context information that is beneficial to building a noun lexicon .These methods may be also useful given a different starting point .", "label": "", "metadata": {}, "score": "71.31487"}
{"text": "The parser is supplied with 5 Chinese grammars ( and , with access to suitable training data , you could train other versions ) .You can find them inside the supplied stanford - parser- YYYY - MM - DD -models.jar file ( in the GUI , select this file and then navigate inside it ; at the command line , use jar -tf to see its contents ) .", "label": "", "metadata": {}, "score": "71.32675"}
{"text": "The system 400 may use a suffix tree data structure in order to identify the alignments .The suffix tree of a string uniquely encodes all the suffixes of that string ( and thus , implicitly , all its substrings too ) .", "label": "", "metadata": {}, "score": "71.32684"}
{"text": "In addition we developed a Finnish Semantic Tagging ( FST ) tool .In the ASSIST project , we extended the existing USAS framework to construct a Russian Semantic Tagger ( RST ) .In 2013 , the UCREL research centre funded initial development of the Italian , Dutch and Chinese lexicons .", "label": "", "metadata": {}, "score": "71.421906"}
{"text": "In one approach , the context vector for each word in the lexicon may consist of co - occurrence counts in respect to a number of peripheral tokens ( basically , the most frequent words ) .These counts may be collected for each position in an n - word window around the word in focus .", "label": "", "metadata": {}, "score": "71.45732"}
{"text": "For instance : .Parsing file : chinese - onesent with 1 sentences .Parsing [ sent .1 len .Parsed 10 words in 1 sentences ( 7.10 wds / sec ; 0.71 sents / sec ) .There are many kinds of ' vanilla ' , but , providing your treebank is in Penn Treebank format , then , yes , this is easy to do .", "label": "", "metadata": {}, "score": "71.47853"}
{"text": "Munich : K. G. Saur , 2006 .Tufis et al .2004 Tufis , Dan , Radu Ion , and Nancy Ide . \" Fine - Grained Word Sense Disambiguation Based on Parallel Corpora , Word Alignment , Word Clustering and Aligned Wordnets \" , Proceedings of the 20th International Conference on Computational Linguistics ( 2004 ) .", "label": "", "metadata": {}, "score": "71.547134"}
{"text": "es.tok.lc \\ test / newstest2009 .output.nbest .After the decoder has finished , you can extract the 1-best translations from the n - best list using the following command : . output.nbest \\ test / newstest2009 .output.1best .", "label": "", "metadata": {}, "score": "71.568665"}
{"text": "The pipeline script needs to be told where to find the raw training , tuning , and test data .A good convention is to place these files in an input/ subdirectory of your run 's working directory ( NOTE : do not use data/ , since a directory of that name is created and used by the pipeline itself ) .", "label": "", "metadata": {}, "score": "71.620834"}
{"text": "Figure 1 presents a mock - up of what a dictionary entry could look like in such a dynamic reference work .The first section ( \" Translation equivalents \" ) presents items 1 and 2 from the list , and is reminiscent of traditional lexica for classical languages : a list of possible definitions is provided along with examples of use .", "label": "", "metadata": {}, "score": "71.62566"}
{"text": "These steps are discussed below , after a few intervening sections about high - level details of the pipeline .Grammar options .Joshua can extract two types of grammars : Hiero - style grammars and SAMT grammars .As described on the file formats page , both of them are encoded into the same file format , but they differ in terms of the richness of their nonterminal sets .", "label": "", "metadata": {}, "score": "71.63096"}
{"text": "If you wish to use SRILM instead , you need to do the following : .Install SRILM and set the $ SRILM environment variable to point to its installed location .Add the --lm - gen srilm flag to your pipeline invocation .", "label": "", "metadata": {}, "score": "71.65947"}
{"text": "This article describes a machine translation system based on an automatic post - editing strategy : initially translate the input text into the target - language using a rule - based MT system , then automatically post - edit the output using a statistical phrase - based system .", "label": "", "metadata": {}, "score": "71.66617"}
{"text": "FIG .3 shows results of the English - German implementation .\" Entries \" indicate the number of correct lexicon entries that were added to a seed lexicon of 1337 identically spelled words , and \" Corpus \" indicates how well the resulting translation lexicon performs compared to the actual word - level translations in a parallel corpus .", "label": "", "metadata": {}, "score": "71.8699"}
{"text": "At the API level , with the factored parser , if you ask for getBestDependencyParse ( ) , then you will get the best untyped dependency parse .If you call that method with englishPCFG.ser.gz , it will return null , as there is no dependency parse .", "label": "", "metadata": {}, "score": "71.93082"}
{"text": "If the program finishes right away , then it probably terminated with an error .You can read the nohup.out file to see what went wrong .Common problems include a missing example / test directory , or a file not found exception .", "label": "", "metadata": {}, "score": "72.0172"}
{"text": "The results of this automatic processing go far beyond the construction of a single lexicon .I noted earlier that all scholarly dictionaries include a list of citations illustrating a word 's exemplary use .As Figure 1 shows , each entry in this new , dynamic lexicon ultimately ends with a list of canonical citations to fixed passages in the text .", "label": "", "metadata": {}, "score": "72.021454"}
{"text": "With this code , you should be able to parse the sentence in a file with a command like this ( details depending on your shell , OS , etc . ) : . java -mx200 m -cp \" stanford - parser .", "label": "", "metadata": {}, "score": "72.0393"}
{"text": "/usr / bin / perl # # truecase - map .cat training / training . map .Finally , recase the lowercased 1-best translation by running the SRILM disambig program , which takes the map of alternative capitalizations , creates a confusion network , and uses truecased LM to find the best path through it : .", "label": "", "metadata": {}, "score": "72.05992"}
{"text": "As automatically generated lexicons , they contain errors .The Chinese lexicons can be accessed here ( download as UTF-8 ) : ( a ) Single word lexicon ( b ) MWE lexicon .The Dutch semantic tagger has been developed using a similar process to that of the Italian semantic tagger , using the Dutch version of TreeTagger .", "label": "", "metadata": {}, "score": "72.064766"}
{"text": "en.tok.lc 1411589 41042110 training / training .es.tok.lc 671429 16721564 training / subsampled / subsample .en.tok.lc671429 17670846 training / subsampled / subsample .es.tok.lc .Step 3 : Create word alignments .Before extracting a translation grammar , we first need to create word alignments for our parallel corpus .", "label": "", "metadata": {}, "score": "72.12508"}
{"text": "10 .An advantage of the algorithm over previous approaches is that we do not provide as input to the algorithm a list of unknown words .Instead , the system automatically learns from the corpus both the unknown words and their translation , upon discovery of appropriate context alignments .", "label": "", "metadata": {}, "score": "72.21364"}
{"text": "Longman , London , pp 92 - 109 .Rayson , P. , Garside , R. , and Sawyer , P. ( 1999 ) .Recovering Legacy Requirements .In Proceedings of REFSQ'99 Fifth International Workshop on Requirements Engineering : Foundations of Software Quality , June 14 - 15 1999 , Heidelberg , Germany .", "label": "", "metadata": {}, "score": "72.244"}
{"text": "In their ability to include statistical information about a word 's actual use , these contemporary projects are exploiting advances in computational linguistics that have been made over the past thirty years .Before turning , however , to how we can adapt these technologies in the creation of a new and complementary reference work , we must first address the use of such lexica .", "label": "", "metadata": {}, "score": "72.32356"}
{"text": "HOW - TO GUIDE :Installing and running the Joshua Decoder .Note : these instructions are several years out of date .This document gives instructions on how to install and use the Joshua decoder .Joshua is an open - source decoder for parsing - based machine translation .", "label": "", "metadata": {}, "score": "72.32797"}
{"text": "If your file is extremely large , splitting it into multiple files and parsing them sequentially will reduce memory usage .A 64-bit application requires more memory than a 32-bit application ( Java uses lots of pointers ) .", "label": "", "metadata": {}, "score": "72.37098"}
{"text": "An alternative system may use a small bilingual lexicon in addition to non - parallel corpora to learn translations of unknown words and to generate a parallel corpus .A method for building a translation lexicon from non - parallel corpora by a machine translation system , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}, "score": "72.40862"}
{"text": "Either form of list will pass the tags to the parser .Or you can do this with code that you write .Here 's an example that very manually makes the List in question : . ; List .There are other constraints which can be added , but they have to be added programmatically .", "label": "", "metadata": {}, "score": "72.547005"}
{"text": "Each run can be found in a directory RUNDIR / tune / N .When tuning is finished , each final configuration file can be found at either .RUNDIR / tune / N / joshua.config .ZMERT.final RUNDIR / tune / N / joshua.config . PRO.final .", "label": "", "metadata": {}, "score": "72.60949"}
{"text": "This value is required if you start at the grammar extraction step .When alignment is complete , the alignment file can be found at RUNDIR / alignments / training . align .It is parallel to the training corpora .There are many files in the alignments/ subdirectory that contain the output of intermediate steps .", "label": "", "metadata": {}, "score": "72.67093"}
{"text": "This can be accomplished with the --first - step and --last - step flags , which take as argument a case - insensitive version of the following steps : .FIRST :Data preparation .Everything begins with data preparation .This is the default first step , so there is no need to be explicit about it .", "label": "", "metadata": {}, "score": "72.688385"}
{"text": "The workshop will be centered around the problem of building and using .parallel corpora , which are vital resources for efficiently deriving .multi - lingual text processing tools .In addition to regular papers , .the workshop also includes a shared task that will result in a . comparative evaluation of word alignment techniques .", "label": "", "metadata": {}, "score": "72.99092"}
{"text": "Without the temporal annotation , some simple temporals like today will still be recognized , but a bare temporal like last week in I left last week will be tagged as an object ( dobj ) .With the Stanford parser , you can get marking of temporal NPs in the tree output by giving the option -retainTmpSubcategories , either on the command line or by passing it to the setOptionFlags(String [ ] ) method of the parser .", "label": "", "metadata": {}, "score": "73.40639"}
{"text": "For English , the parser by default now produces Universal Dependencies , which are extensively documented on that page .You can also have it produce the prior Stanford Dependencies representation .For this , and for the Chinese dependencies , you can find links to documentation on the Stanford Dependencies page .", "label": "", "metadata": {}, "score": "73.44084"}
{"text": "The relevant options are -sentences ( see above ) , -tokenized , -tokenizerFactory , -tokenizerMethod , and -tagSeparator .If , for example , you want to denote a POS tag by appending /POS on a word , you would include the options -tokenized -tagSeparator / -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory in your invocation of LexicalizedParser .", "label": "", "metadata": {}, "score": "73.46204"}
{"text": "# lm config .# tm config .# pruning config .# nbest config .# remote lm server config , we should first prepare remote_symbol_tbl before starting any jobs . /voc.remote.sym ./remote.lm.server.list .# parallel deocoder : it can not be used together with remote lm .", "label": "", "metadata": {}, "score": "73.48969"}
{"text": "Sentence alignment .In this exercise , we 'll start with an existing sentence - aligned parallel corpus .Download this tarball , which contains a Spanish - Engish parallel corpus , along with a dev and a test set : data.tar.gz .", "label": "", "metadata": {}, "score": "73.48977"}
{"text": "6 is a Generalized Suffix Tree ( GST ) .FIG .7 is a Bilingual Suffix Tree ( BST ) .FIG .8 is a portion of a BST showing example alignments .FIG .9 are portions of a BST describing left and right alignments .", "label": "", "metadata": {}, "score": "73.70003"}
{"text": "The pipeline will notice that it is parsed and will not reparse it .Parsing is affected by both the --threads N and --jobs N options .The former runs the parser in multithreaded mode , while the latter distributes the runs across as cluster ( and requires some configuration , not yet documented ) .", "label": "", "metadata": {}, "score": "73.75291"}
{"text": "Before decoding the test set , you 'll need to extract a translation grammar for the foreign phrases in the test set test / newstest2009 .es.tok.lc : . /model\\ test / newstest2009 .es.tok.lc.grammar.raw \\ test / newstest2009 .es.tok.lc & .", "label": "", "metadata": {}, "score": "73.961365"}
{"text": "To accommodate this kind of variation , the pipeline script allows you to specify both ( a ) the amount of memory used by the Joshua decoder instance and ( b ) the amount of memory required of nodes obtained by the qsub command .", "label": "", "metadata": {}, "score": "74.03682"}
{"text": "Beyond the lexicon .These technologies , borrowed from computational linguistics , will give us the grounding to create a new kind of lexicon , one that presents information about a word 's actual usage .This lexicon resembles its more traditional print counterparts in that it is a work designed to be browsed : one looks up an individual headword and then reads its lexical entry .", "label": "", "metadata": {}, "score": "74.07939"}
{"text": "Most translation models also make use of an n - gram language model as a way of assigning higher probability to hypothesis translations that look like fluent examples of the target language .Joshua provides support for n - gram language models , either through a built in data structure , or through external calls to the SRI language modeling toolkit ( srilm ) .", "label": "", "metadata": {}, "score": "74.10072"}
{"text": "The English version of the USAS framework can be accessed in the web - based corpus tool Wmatrix and directly online .The Russian lexicon and MWE list are available for download here .Future updates will also be released here .", "label": "", "metadata": {}, "score": "74.18588"}
{"text": "In general , with appropriate grammars loaded , you can parse with and ask for output of the PCFG , ( untyped ) dependency , or factored parsers .For English , although the grammars and parsing methods differ , the average quality of englishPCFG.ser.gz and englishFactored.ser.gz is similar , and so many people opt for the faster englishPCFG.ser.gz , though englishFactored.ser.gz sometimes does better because it does include lexicalization .", "label": "", "metadata": {}, "score": "74.20519"}
{"text": "If you already have a grammar and wish to skip this step , you can do so passing the grammar with the --grammar GRAMMAR flag .The main variable in grammar extraction is Hadoop .If you have a Hadoop installation , simply ensure that the environment variable $ HADOOP is defined , and Thrax will seamlessly use it .", "label": "", "metadata": {}, "score": "74.22249"}
{"text": "10 is psuedocode describing an algorithm for learning translations of unknown words .DETAILED DESCRIPTION .FIG .1 shows a system 100 for building a translation lexicon 105 according to an embodiment .The system may use non - parallel monolingual corpora 110 , 115 in two languages to automatically generate one - to - one mapping of words in the two languages .", "label": "", "metadata": {}, "score": "74.242035"}
{"text": "Nat .6.90 , etc .For a more comprehensive ( but not exhaustive ) comparison , I can consult the TLL .This is what we might consider a manual form of \" lemmatized searching .\" The search results are thus significantly diluted by a large number of false positives .", "label": "", "metadata": {}, "score": "74.26082"}
{"text": "SMT has made tremendous strides in less than two decades , and many popular tec ... \" .Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .By examining many samples of human - produced translation , SMT algorithms automatically learn how to translate .", "label": "", "metadata": {}, "score": "74.30854"}
{"text": "The Berkeley Aligner - this software is used to align words across sentence pairs in a bilingual parallel corpus .Word alignment takes place before extracting an SCFG .After you have downloaded the srilm tar file , type the following commands to install it : . mkdir srilm mv srilm.tgz srilm/ cd srilm/ tar xfz srilm.tgz make .", "label": "", "metadata": {}, "score": "74.42322"}
{"text": "You can see a list of the other parameters available in our MERT implementation by running this command : . java -cp $ JOSHUA / bin joshua.zmert.ZMERT -h .Next , create a file called mert / params .txt that specifies what feature functions you are using in your mode .", "label": "", "metadata": {}, "score": "74.42572"}
{"text": "Grammar extraction with Thrax .If you jump to this step , you 'll need to provide an aligned corpus ( --alignment ) along with your parallel data .TUNE : Tuning .With this option , you need to specify a grammar ( --grammar ) or separate tune ( --tune - grammar ) and test ( --test - grammar ) grammars .", "label": "", "metadata": {}, "score": "74.44136"}
{"text": "For example , this command ( with appropriate paths ) will convert a Penn Treebank file to uncollapsed typed dependencies : . java -cp stanford-parser.jar edu.stanford.nlp.trees.EnglishGrammaticalStructure -treeFile wsj/02/wsj_0201 .mrg -basic .Also , here is a sample Java class that you can download that converts from an input file of trees to typed dependencies .", "label": "", "metadata": {}, "score": "74.46956"}
{"text": "Note that the parallel corpus belongs to a different domain than the comparable corpus .Also the parallel corpus is extremely small .For low density languages , such a corpus can be built manually .When given as input the comparable corpora described above and the bilingual lexicon of 6,900 entries , the algorithm 1000 found 33,926 parallel sequences , with length between three and seven words .", "label": "", "metadata": {}, "score": "74.53381"}
{"text": "Gale et al .1992a Gale , William , Kenneth W. Church and David Yarowsky . \"Using bilingual materials to develop word sense disambiguation methods \" , Proceedings of the 4th International Conference on Theoretical and Methodological Issues in Machine Translation ( 1992 ) .", "label": "", "metadata": {}, "score": "74.63124"}
{"text": "For some sentences the parse tree output by the standalone parser and the tree output by the CoreNLP pipeline can be different .The reason for this is that if you run the CoreNLP pipeline with the default annotators , it will run a part - of - speech ( POS ) tagger before running the parser .", "label": "", "metadata": {}, "score": "74.708115"}
{"text": "Building the suffix tree of a string takes time and space linear in the length of the string .Building a GST for a set of strings takes time and space linear in the sum of the lengths of all strings in the set .", "label": "", "metadata": {}, "score": "74.739395"}
{"text": "For best results , we recommend that you first segment input text with a high quality word segmentation system which provides word segmentation according to Penn Chinese Treebank conventions ( note that there are many different conventions for Chinese word segmentation ... ) .", "label": "", "metadata": {}, "score": "74.750626"}
{"text": "The default is \" 2 g \" , but you will want to increase it for larger language models .If SRILM is used , it is called with the following arguments : .$ SRILM / bin / i686-m64/ngram - count -interpolate SMOOTHING -order 5 -text TRAINING - DATA -unk -lm lm.gz .", "label": "", "metadata": {}, "score": "74.84895"}
{"text": "Joshua uses whitespace to delineate words .For many languages , tokenization can be as simple as separating punctation off as its own token .For languages like Chinese , which do n't put spaces around words , tokenization can be more tricky .", "label": "", "metadata": {}, "score": "74.854645"}
{"text": "Our key assumption is that collocation errors are often caused by semantic similarity in the first language ( L1language ) of the writer .An analysis ... \" .We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora .", "label": "", "metadata": {}, "score": "74.925705"}
{"text": "If you want to display the output in a command window , you separately also need to work out what character set your computer supports for display .If that is different to the encoding of the file , you will need to convert the encoding for display .", "label": "", "metadata": {}, "score": "75.04017"}
{"text": "While multiple LexicalizedparserQuery threads share the same grammar ( LexicalizedParser ) , the memory space savings are n't huge , as most of the memory goes to the transient data structures used in chart parsing .So , if you are running lots of parsing threads concurrently , you will need to give a lot of memory to the JVM .", "label": "", "metadata": {}, "score": "75.07526"}
{"text": "Starting with version 1.6.2 of the parser , there is a fairly flexible scheme for options in tokenization style .You can give options such as this one to turn off Americanization of spelling : .Or this one to change several options : .", "label": "", "metadata": {}, "score": "75.07945"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ , 160 - 167 .Taskar , B. , et al . , \" A Discriminative Matching Approach to Word Alignment , \" In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing ( Vancouver , BC , Canada , Oct. 6 - 8 , 2005 ) .", "label": "", "metadata": {}, "score": "75.20301"}
{"text": "9 ( ii ) shows the reverse BST 910 , and in bold , the path we are interested in .When d and y are surrounded by aligned sequences , we hypothesize that they are translations of each other .For a pair of words from the two corpora , we use the terms \" right alignment \" and \" left alignment \" to refer to the aligned sequences that precede and respectively succeed the two words in each corpus .", "label": "", "metadata": {}, "score": "75.35422"}
{"text": "Due to cultural exchange , a large number of words that originate in one language may be adopted by others .Recently , this phenomenon can be seen with words such as \" Internet U or \" Aids u. \" These terms may be adopted verbatim or changed by well - established rules .", "label": "", "metadata": {}, "score": "75.37633"}
{"text": "The grammar extraction step takes three pieces of data : ( 1 ) the source - language training corpus , ( 2 ) the target - language training corpus ( parsed , if an SAMT grammar is being extracted ) , and ( 3 ) the alignment file .", "label": "", "metadata": {}, "score": "75.4382"}
{"text": "Vector comparison is done by adding all absolute differences of all components .Alternatively , the lexicon builder 125 may count how often another word occurs in the same sentence as the target word .The counts may then be normalized by a using the tf / idf method , which is often used in information retrieval .", "label": "", "metadata": {}, "score": "75.488205"}
{"text": "Computational Linguistics Meets Metadata , or the Automatic Extraction of Key Words from Full Text Content .RLG Diginews , Vol . 8 , No . 2 .ISSN 1093 - 5371 .Jones , M. , Rayson , P. and Leech , G. ( 2004 )", "label": "", "metadata": {}, "score": "75.56308"}
{"text": "We 'll use the word alignments to create a translation grammar similar to the Chinese one shown in Step 1 .The translation grammar is created by looking for where the foreign language phrases from the test set occur in the training set , and then using the word alignments to figure out which foreign phrases are aligned .", "label": "", "metadata": {}, "score": "75.713036"}
{"text": "McDonald et al .2005 McDonald , Ryan , Fernando Pereira , Kiril Ribarov , and Jan Haji\u010d .\" Non - projective Dependency Parsing using Spanning Tree Algorithms \" , Proceedings of HLT / EMNLP ( 2005 ) .Mel'\u010duk 1988 Mel'\u010duk , Igor A. Dependency Syntax : Theory and Practice .", "label": "", "metadata": {}, "score": "75.73557"}
{"text": "Experimental results on the test data of the previous campaign are presented . ... to Canadian universities for research and education purposes . \" ...Current phrase - based SMT systems perform poorly when using small training sets .This is a consequence of unreliable translation estimates and low coverage over source and target phrases .", "label": "", "metadata": {}, "score": "75.77029"}
{"text": "In a GST of a set of strings , each path from the root to a leaf represents a suffix in one or more strings from the set .FIG .6 shows the GST 600 for a corpus of two sentences .", "label": "", "metadata": {}, "score": "75.911705"}
{"text": "1363 - 1371 .ISBN 2 - 905450 - 07-X .Rayson , P. , Emmet , L. , Garside , R. , and Sawyer , P. ( 2000 ) .The REVERE Project : Experiments with the application of probabilistic NLP to Systems Engineering .", "label": "", "metadata": {}, "score": "75.9727"}
{"text": "applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar spelling .", "label": "", "metadata": {}, "score": "76.05147"}
{"text": "This is because the Berkeley aligner generally expects to test against a set of manually word - aligned data : . cd es - en / full - training/ mkdir -p example / test .After you 've created the word - align .", "label": "", "metadata": {}, "score": "76.05613"}
{"text": "The three numbers listed after each translation rules are negative log probabilities that signify , in order : .You can use the grammar to translate the test set by running .config.srilm \\ example / example .test.in \\ example / example .", "label": "", "metadata": {}, "score": "76.216156"}
{"text": "This example contains 1,000 sentences of Urdu - English data ( the full dataset is available as part of the Indian languages parallel corpora with 100-sentence tuning and test sets with four references each .Running the pipeline requires two main steps : data preparation and invocation .", "label": "", "metadata": {}, "score": "76.21791"}
{"text": "Here are example commands for parsing two of the test files , one in UTF-8 and one in GB18030 .The ( Linux ) computer that this is being run on is set up to work with UTF-8 ( and this webpage is also in UTF-8 ) , so for the case of GB18030 , the output is piped through the Unix iconv utility for display .", "label": "", "metadata": {}, "score": "76.21968"}
{"text": "The option -smoothTagsThresh 0 stops any probability mass being reserved for unknown words .Naturally , such a parser will be unable to parse any sentence with unknown words in it .The 2003 unlexicalized parsing paper lists several modifications that gradually improve the performance of the Stanford parser for English .", "label": "", "metadata": {}, "score": "76.22368"}
{"text": "Piao , S. , Bianchi , F. , Dayrell , C. , D'Egidio , A. and Rayson , P. ( 2015 ) .Development of the multilingual semantic annotation system .In proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies ( NAACL HLT 2015 ) , Denver , Colorado , United States , pp .", "label": "", "metadata": {}, "score": "76.369965"}
{"text": "input/ train .SOURCE train .TARGET tune .SOURCE tune .TARGET test .SOURCE test .TARGET .These files should be parallel at the sentence level ( with one sentence per line ) , should be in UTF-8 , and should be untokenized ( tokenization occurs in the pipeline ) .", "label": "", "metadata": {}, "score": "76.3767"}
{"text": "FIG .9 .FIG .9 ( i ) shows a branch 905 of the BST corresponding to the comparable corpus in the same figure .The path defined by the bold edges shows that sequences xyz and abc are aligned , and diverge ( i.e. , have a mismatch ) at characters y and d , respectively .", "label": "", "metadata": {}, "score": "76.453766"}
{"text": "An apparatus comprising : . a lexicon builder operative to be executed to expand the seed lexicon by identifying possible translations of words in the first and second corpora using one or more clues .The apparatus of .claim 24 , wherein the lexicon builder is configured to use the identically spelled words in the seed lexicon as accurate translations .", "label": "", "metadata": {}, "score": "76.59965"}
{"text": "issues : . -Construction of parallel corpora , including the automatic . identification and harvesting of parallel corpora from the Web . -Methods to evaluate the quality of parallel corpora and word . alignments . -Tools for processing parallel corpora , including automatic sentence . alignment , word alignment , phrase alignment , detection of omissions .", "label": "", "metadata": {}, "score": "76.68678"}
{"text": "To avoid this , the system 400 appends an end - of - string marker \" $ \" that appears nowhere else in the string .For clarity , the drawings only show the $ marker when necessary .Each monolingual corpus given as input to the system 400 may be divided into a set of sentences .", "label": "", "metadata": {}, "score": "76.719696"}
{"text": "Sinclair 1987 Sinclair , John M. ( ed . )Looking Up : an account of the COBUILD project in lexical computing .Collins , 1987 .Singh and Husain 2005 Singh , Anil Kumar and Samar Husain .\" Comparison , Selection and Use of Sentence Alignment Algorithms for New Language Pairs \" , Proceedings of the ACL Workshop on Building and Using Parallel Texts ( 2005 ) .", "label": "", "metadata": {}, "score": "76.736855"}
{"text": "The lexicon builder 125 may combine different clues by adding up the matching scores .The scores can be weighted .For example , when using the spelling clue in combination with others , it may be useful to define a cutoff .", "label": "", "metadata": {}, "score": "76.77614"}
{"text": "And in the current directory , you will see the following files ( among other intermediate files generated by the individual sub - steps ) .data/ train/ corpus.ur corpus.en thrax - input - file tune/ tune.tok.lc.ur tune.tok.lc.en grammar.filtered.gz grammar.glue test/ test.tok.lc.ur test.tok.lc.en grammar.filtered.gz grammar.glue alignments/ 0/ [ berkeley aligner output files ] training.align thrax - hiero . conf thrax.log grammar.gz lm.gz tune/ 1/ decoder_command joshua.config params.txt joshua.log mert.log joshua.config.ZMERT.final final - bleu .", "label": "", "metadata": {}, "score": "76.81937"}
{"text": "COMMON USE CASES AND PITFALLS .If the pipeline dies at the \" thrax - run \" stage with an error like the following : .JOB FAILED ( return code 1 ) hadoop / bin / hadoop : line 47 : /some / path / to / a / directory / hadoop / bin / hadoop - config .", "label": "", "metadata": {}, "score": "76.83353"}
{"text": "ur , and so on .Assuming no problems arise , this command will run the complete pipeline in about 20 minutes , producing BLEU scores at the end .As it runs , you will see output that looks like the following : .", "label": "", "metadata": {}, "score": "76.87742"}
{"text": "Of course , parsing will suffer unless your tokenization accurately matches the tokenization of the underlying treebank , for instance Penn Treebank tokenization .A common occurrence is that your text is already correctly tokenized but does not escape characters the way the Penn Treebank does ( turning parentheses into -LRB- and -RRB- , and putting a backslash in front of forward slashes and asterisks - presumably a holdover from Lisp ) .", "label": "", "metadata": {}, "score": "76.979355"}
{"text": "The ACASD and ACAMRIT projects led to the initial design and implementation of the tools and applied them in the area of interview transcripts .The REVERE project applied the tools in the domain of software engineering documentation using a web front end called Wmatrix .", "label": "", "metadata": {}, "score": "77.026886"}
{"text": "en.tok.lc \\ training / subsampled / training .en.tok.lc-es .tok.lc.align \\ model .This compiles the index that Joshua will use for its rule extraction , and puts it into a directory named model .Extract grammar rules for the dev set .", "label": "", "metadata": {}, "score": "77.068726"}
{"text": "There is n't a separate included tagger ; the parser does POS tagging as part of parsing .Yes , you can .You can use the main method of EnglishGrammaticalStructure ( for English , or the corresponding class for Chinese ) .", "label": "", "metadata": {}, "score": "77.13855"}
{"text": "That 's the end of the pipeline !Joshua also supports decoding further test sets .This is enabled by rerunning the pipeline with a number of arguments : . --first - step TEST .This tells the decoder to start at the test step . --name", "label": "", "metadata": {}, "score": "77.143906"}
{"text": "It does not attempt to determine grammaticality , though it will normally prefer a \" grammatical \" parse for a sentence if one exists .This is appropriate in many circumstances , such as when wanting to interpret user input , or dealing with conversational speech , web pages , non - native speakers , etc . .", "label": "", "metadata": {}, "score": "77.18962"}
{"text": "The pipeline takes a set of inputs ( training , tuning , and test data ) , and creates a set of intermediate files in the run directory .By default , the run directory is the current directory , but it can be changed with the --rundir parameter .", "label": "", "metadata": {}, "score": "77.27659"}
{"text": "en.tok.lc -rps 1 # references per sentence -p mert / params .txt # parameter file -m BLEU 4 closest # evaluation metric and its options -maxIt 10 # maximum MERT iterations -ipi 20 # number of intermediate initial points per iteration -cmd mert / decoder_command # file containing commands to run decoder -decOut mert / news - dev2009 .", "label": "", "metadata": {}, "score": "77.277084"}
{"text": "[ train - copy - ur ] cached , skipping ... ... .This indicates that the caching module has discovered that the step was already computed and thus did not need to be rerun .This feature is quite useful for restarting pipeline runs that have crashed due to bugs , memory limitations , hardware failures , and the myriad other problems that plague MT researchers across the world .", "label": "", "metadata": {}, "score": "77.29062"}
{"text": "Yes , you can .Various tokenizers are included .The one used for English is called PTBTokenizer .It is a hand - written rule - based ( FSM ) tokenizer , but is quite accurate over newswire - style text .", "label": "", "metadata": {}, "score": "77.29654"}
{"text": "PTBEscapingProcessor .If calling the parser within your own program , the main parse methods take a List of words which should already be correctly tokenized and escaped before calling the parser .You do n't need to and can not give the -tokenized option .", "label": "", "metadata": {}, "score": "77.34304"}
{"text": "Bannard , C. and Callison - Burch , C. , \" Paraphrasing with Bilingual Parallel Corpora , \" In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics ( Ann Arbor , MI , Jun. 25 - 30 , 2005 ) .", "label": "", "metadata": {}, "score": "77.35065"}
{"text": "A language model built from the target side of the training data is placed at RUNDIR / lm .gz .Interlude : decoder arguments .Running the decoder is done in both the tuning stage and the testing stage .A critical point is that you have to give the decoder enough memory to run .", "label": "", "metadata": {}, "score": "77.36311"}
{"text": "By default , our Chinese parser uses GB18030 ( the native character encoding of the Penn Chinese Treebank and the national encoding of China ) for input and output .However , it is very easy to parse text in another character encoding : you simply give the flag -encoding encoding to the parser , where encoding is a character set encoding name recognized within Java , such as : UTF-8 , Big5-HKSCS , or GB18030 .", "label": "", "metadata": {}, "score": "77.41238"}
{"text": "The current versions ( 14th December 2006 ) contain 13,153 single words , 4,444 proper names , and 713 MWE templates .For each word in the files , we include a part - of - speech tag based on the mystem tagset for Russian and a list of possible semantic tags , all of which have been manually checked .", "label": "", "metadata": {}, "score": "77.456566"}
{"text": "Testing .For each of the tuner runs , Joshua takes the tuner output file and decodes the test set .Afterwards , by default , minimum Bayes - risk decoding is run on the 300-best output .This step usually yields about 0.3 - 0.5 BLEU points but is time - consuming , and can be turned off with the --no - mbr flag .", "label": "", "metadata": {}, "score": "77.50788"}
{"text": "To turn these off , you can use the following options : .At present , we do n't have any documentation beyond what you get in the download and what 's on this page .If you would like to help by producing better documentation , feel free to write to parser-support@lists.stanford.edu .", "label": "", "metadata": {}, "score": "77.55827"}
{"text": "GIZA++ is the default aligner .It is included with Joshua , and should compile successfully when you typed ant all from the Joshua root directory .It is not required because you can use the ( included ) Berkeley aligner ( --aligner berkeley ) .", "label": "", "metadata": {}, "score": "77.562706"}
{"text": "The system also found translations for thirty unknown French words .Of these , nine were correct , which means a precision of 30 % .For each of the two corpora , building the monolingual GST took only 1.5 minutes .", "label": "", "metadata": {}, "score": "77.61369"}
{"text": "Parsing file : chinese-onesent-unseg.txt with 1 sentences .Parsing [ sent .1 len .falling back to PCFG parse .Parsed 5 words in 1 sentences ( 6.08 wds / sec ; 1.22 sents / sec ) .1 sentences were parsed by fallback to PCFG .", "label": "", "metadata": {}, "score": "77.621414"}
{"text": "If the lexicon is probabilistic , each matching between two words will be weighted by the corresponding translation probability .The paths in the resulting bilingual tree will also have weights associated with them , defined as the product of the matching probabilities of the words along the path .", "label": "", "metadata": {}, "score": "77.82315"}
{"text": "Bilingual word alignment forms the foundation of most approaches to statistical machine translation .Current word alignment methods are predominantly based on generative models .In this paper , we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used .", "label": "", "metadata": {}, "score": "77.85267"}
{"text": "( slides ) .Scott S.L. Piao , Dawn Archer , Olga Mudraya , Paul Rayson , Roger Garside , Tony McEnery , Andrew Wilson ( 2005 )A Large Semantic Lexicon for Corpus Annotation .In proceedings of the Corpus Linguistics 2005 conference , July 14 - 17 , Birmingham , UK .", "label": "", "metadata": {}, "score": "77.962555"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "78.023796"}
{"text": "It is generally acknowledged that the performance of rule - based machine translation ( RMBT ) systems can be greatly improved through domain - specic system adaptation .To that end , RBMT users often choose to invest signicant re - sources into the development of ad hoc MT dictionaries .", "label": "", "metadata": {}, "score": "78.023796"}
{"text": "In order to accomplish this , we need to consider the role that automatic methods can play within our emerging cyberinfrastructure .[ 7 ] We need to provide traditional scholars with the apparatus necessary to facilitate their own textual research .", "label": "", "metadata": {}, "score": "78.08379"}
{"text": "Another useful flag is the --rundir DIR flag , which chdir()s to the specified directory before running the pipeline .By default the rundir is the current directory .Changing it can be useful for organizing related pipeline runs .Relative paths specified to other flags ( e.g. , to --corpus or --lmfile ) are relative to the directory the pipeline was called from , not the rundir itself ( unless they happen to be the same , of course ) .", "label": "", "metadata": {}, "score": "78.132034"}
{"text": "Miller et al .1993 Miller , George , Claudia Leacock , Randee Tengi , and Ross Bunker . \"A Semantic Concordance \" , Proceedings of the ARPA Workshop on Human Language Technology ( 1993 ) .Moore 2002 Moore , Robert C. \" Fast and Accurate Sentence Alignment of Bilingual Corpora \" , AMTA ' 02 : Proceedings of the 5th Conference of the Association for Machine Translation in the Americas on Machine Translation ( 2002 ) .", "label": "", "metadata": {}, "score": "78.18544"}
{"text": "Lowercasing .This creates a series of intermediate files which are saved for posterity but compressed .For example , you might see .data/ train/ train.en.gz train.tok.en.gz train.tok.50.en.gz train.tok.50 .lc.en .The file \" corpus .LANG \" is a symbolic link to the last file in the chain .", "label": "", "metadata": {}, "score": "78.21462"}
{"text": "Below are some statistics for 32-bit operation with the supplied englishPCFG and englishFactoredGrammars .We have parsed sentences as long as 234 words , but you need lots of RAM and patience .You can use the -outputFormat wordsAndTags option .", "label": "", "metadata": {}, "score": "78.32031"}
{"text": "For example , perhaps the decoder ran out of memory .This allows you to adjust the parameter ( e.g. , --joshua - mem ) and rerun the script .Of course , if you change one of the parameters a step depends on , it will trigger a rerun , which in turn might trigger further downstream reruns .", "label": "", "metadata": {}, "score": "78.439606"}
{"text": "How do we get there ?We have already begun work on a dynamic lexicon like that shown in Figure 1 [ Bamman and Crane 2008 ] .Our approach is to use already established methods in natural language processing ; as such , our methodology involves the application of three core technologies : . identifying word senses from parallel texts ; . locating the correct sense for a word using contextual information ; and .", "label": "", "metadata": {}, "score": "78.443695"}
{"text": "( Since these parsers were written , direct typed dependency parsers have been increasingly explored .Both us and others have now built parsers that directly parse to Stanford Dependencies .See the Stanford Dependencies page for more information . )For Chinese ( and Arabic , German , and \" WSJ \" ) , you can look at the included file makeSerialized.csh , and easily see exactly what files the models are trained on , in terms of LDC or Negra file numbers .", "label": "", "metadata": {}, "score": "78.452286"}
{"text": "Automatic Content Analysis of Spoken Discourse .In : C. Souter and E. Atwell ( eds ) , Corpus Based Computational Linguistics .Amsterdam : Rodopi .pp215 - 226 ( text ) .Wilson , A. ( 1993 ) .Towards an Integration of Content Analysis and Discourse Analysis : The Automatic Linkage of Key Relations in Text .", "label": "", "metadata": {}, "score": "78.46678"}
{"text": "Marcu , Daniel , \" Building Up Rhetorical Structure Trees , \" 1996 , Proc . of the National Conference on Artificial Intelligence and Innovative Applications of Artificial Intelligence Conference , vol .2 , pp .1069 - 1074 .Och , F. , \" Minimum Error Rate Training in Statistical Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "78.65391"}
{"text": "For each occurrence of a target word , the counts may be collected over how often certain context words occur in the two positions directly ahead of the target word and the two following positions .The counts may be collected separately for each position and then entered into a context vector with a dimension for each context word in each position .", "label": "", "metadata": {}, "score": "78.72046"}
{"text": "We therefore must concentrate on two problems .First , how much can we automatically learn from a large textual collection using machine learning techniques that thrive on large corpora ?And second , how can the vast labor already invested in handcrafted lexica help those techniques to learn ?", "label": "", "metadata": {}, "score": "78.72774"}
{"text": "The context of a whole corpus of automatic translations rather than a single sentence is taken into account in order to achieve high alignment quality .The confusion network is rescored with a special language model , and the consensus translation is extracted as the best path .", "label": "", "metadata": {}, "score": "78.86548"}
{"text": "Feature function weights in the log - linear model are set using Och 's minimum ... . \" ...Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .", "label": "", "metadata": {}, "score": "78.95384"}
{"text": "Tuning is run till convergence in the RUNDIR / tune directory .By default , tuning is run just once , but the pipeline supports running the optimizer an arbitrary number of times due to recent work pointing out the variance of tuning procedures in machine translation , in particular MERT .", "label": "", "metadata": {}, "score": "79.01312"}
{"text": "Multiple --corpora files are concatenated in the order they are specified .Multiple --tune and --test flags are not currently allowed .Normalizing punctuation and text ( e.g. , removing extra spaces , converting special quotations ) .There are a few language - specific options that depend on the file extension matching the two - letter ISO 639 - 1 designation .", "label": "", "metadata": {}, "score": "79.16159"}
{"text": "TrueCase.5gram.lm \\ -keep - unk \\ -order 5 \\ -map model / lm / true - case . map \\ -text test / mt09 .output.1best.recased .Where strip - sent - tags .perl is : .Step 9 : Score the translations .", "label": "", "metadata": {}, "score": "79.181656"}
{"text": "For German to English , this includes replacing the letters k and z by c and changing the ending -t\u00e4t to -ty .Both these rules can be observed in the word pair Elektrizitat and electricity .The lexicon builder 125 may utilize these rules to expand the seed lexicon .", "label": "", "metadata": {}, "score": "79.18743"}
{"text": "FIG .8 shows some alignments we can extract from the BST in .FIG .7 , a portion of which is shown in .FIG .8 .As can be seen in .For alignment extraction , we are interested in edges of the third type , because they mark ends of alignments .", "label": "", "metadata": {}, "score": "79.24872"}
{"text": "There are several techniques available for deciding which sense is most appropriate given the context , and several different measures for what definition of \" context \" is most appropriate itself .One technique that we have experimented with is a naive Bayesian classifier ( following Gale et al .", "label": "", "metadata": {}, "score": "79.27864"}
{"text": "Or it may be because the parser made a mistake .While our goal is to improve the parser when we can , we ca n't fix individual examples .The parser is just choosing the highest probability analysis according to its grammar .", "label": "", "metadata": {}, "score": "79.33241"}
{"text": "49 - 54 .ISBN 2 87037 307 4 .Rayson , P. , Garside , R. , and Sawyer , P. ( 2000 ) .Assisting requirements engineering with semantic document analysis .In Proceedings of Content - based multimedia information access RIAO 2000 ( Recherche d'Informations Assistie par Ordinateur , Computer - Assisted Information Retrieval ) International Conference , College de France , Paris , France , April 12 - 14 , 2000 .", "label": "", "metadata": {}, "score": "79.563614"}
{"text": "Busa 2004 Busa , Roberto .\" Foreword : Perspectives on the Digital Humanities \" , Blackwell Companion to Digital Humanities .Oxford : Blackwell , 2004 .Charniak 2000 Charniak , Eugene .\" A Maximum - Entropy - Inspired Parser \" , Proceedings of NAACL ( 2000 ) .", "label": "", "metadata": {}, "score": "79.62608"}
{"text": "French .frWaC : a 1.6 billion word corpus constructed from the Web limiting the crawl to the .fr domain and using medium - frequency words from the Le Monde Diplomatique corpus and basic French vocabulary lists as seeds .The corpus was POS - tagged and lemmatized with the TreeTagger , more information available here .", "label": "", "metadata": {}, "score": "79.6812"}
{"text": "Run the pipeline .The following is the minimal invocation to run the complete pipeline : .$ JOSHUA / scripts / training / pipeline.pl \\ --corpus input / train \\ --tune input / tune \\ --test input / devtest \\ --source SOURCE \\ --target TARGET .", "label": "", "metadata": {}, "score": "79.73446"}
{"text": "Two strings ( i.e. , sequences of words ) match if the corresponding words are translations of each other according to a bilingual lexicon .In order to perform the matching operation , all paths that correspond to an exhaustive traversal of one of the trees ( the source tree ) are traversed in the other ( the target tree ) , until a mismatch occurs .", "label": "", "metadata": {}, "score": "79.77283"}
{"text": "It 's located in the tarball under the scripts directory .To use it type the following commands : . en.tok .Normalization .After tokenization , we recommend that you normalize your data by lowercasing it .The system treats words with variant capitalization as distinct , which can lead to worse probability estimates for their translation , since the counts are fragmented .", "label": "", "metadata": {}, "score": "79.82966"}
{"text": "If the monolingual corpora are somewhat comparable , it can be assumed that a word that occurs in a certain context should have a translation that occurs in a similar context .The context may be defined by the frequencies of context words in surrounding positions .", "label": "", "metadata": {}, "score": "79.84851"}
{"text": "This is a powerful resource that can give us much more information about a text than simple search engines currently allow .Conclusion .In this a dynamic lexicon fills a gap left by traditional reference works .By creating a lexicon directly from a corpus of texts and then situating it within that corpus itself , we can let the two interact in ways that traditional lexica can not .", "label": "", "metadata": {}, "score": "79.919785"}
{"text": "Since the XML files of both the source text and its translations are marked up with the same reference points , \" chapter 1 , section 1 \" of Tacitus ' Annales is automatically aligned with its English translation ( step 1 ) .", "label": "", "metadata": {}, "score": "79.94319"}
{"text": "Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search .Because the features are handled so easily , ... . \" ...Word alignments that violate syntactic correspondences interfere with the extraction of string - to - tree transducer rules for syntaxbased machine translation .", "label": "", "metadata": {}, "score": "79.954254"}
{"text": "This gives a reasonable , but not excellent , Chinese word segmentation system .( It 's performance is n't as good as the Stanford CRF word segmenter mentioned above . )To use it , you use the -segmentMarkov option or a grammar trained with this option .", "label": "", "metadata": {}, "score": "80.065384"}
{"text": "JoshuaDecoder -- This is the class that is run .If you want to look at the the source code for this class , you can find it in src / joshua / decoder / JoshuaDecoder.java .example / example .config.srilm -- This is the configuration file used by Joshua .", "label": "", "metadata": {}, "score": "80.15068"}
{"text": "es.tok.lc.grammar.raw \\ -o test / newstest2009 .es.tok.lc.grammar .Once the grammar extraction has completed , you can edit the joshua.config file for the test set .cp mert / joshua .config .ZMERT.final test / joshua . config .es.tok.lc.grammar .", "label": "", "metadata": {}, "score": "80.35416"}
{"text": "NoClassDefFoundError: org / apache / hadoop / fs / FsShell Caused by : java.lang.ClassNotFoundException : org.apache.hadoop.fs.FsShell .This occurs if the $ HADOOP environment variable is set but does not point to a working Hadoop installation .", "label": "", "metadata": {}, "score": "80.44711"}
{"text": "Because the POS tagger and the dictionary employ different POS tagset , these tagsets are mapped into a simplified common POS tagset to be used by the software .The Spanish lexicon is being created in collaboration with Ricardo - Mar\u00eda Jim\u00e9nez , faculty member of UICbarcelona ( Universitat Internacional de Catalu\u00f1a , Barcelona , Spain ) .", "label": "", "metadata": {}, "score": "80.542984"}
{"text": "First , the lexicon builder 125 searches for the highest score for any word pair .This is added to the lexicon ( block 230 ) , and word pairs that include either the German and English word are dropped from further search .", "label": "", "metadata": {}, "score": "80.54741"}
{"text": "Alignments are between the parallel corpora at RUNDIR / data / train / corpus .To prevent the alignment tables from getting too big , the parallel corpora are grouped into files of no more than ALIGNER_CHUNK_SIZE blocks ( controlled with a parameter below ) .", "label": "", "metadata": {}, "score": "80.59982"}
{"text": "Brants and Franz 2006 Brants , Thorsten and Alex Franz .Web 1 T 5-gram Version 1 .Philadelphia : Linguistic Data Consortium , 2006 .Brown et al .1991c Brown , Peter F. , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer .", "label": "", "metadata": {}, "score": "80.67076"}
{"text": "817 - 827 .ISBN 2 - 9522 - 4570 - 3 .Paul Rayson , Scott Piao , Dawn Archer ( 2004 ) .Modern and Historical Aspects of the UCREL Semantic Analysis System .Invited talk at the University of Sheffield , UK , 16th November 2004 .", "label": "", "metadata": {}, "score": "80.74542"}
{"text": "The Joshua pipeline script is designed in the spirit of Moses ' train-model.pl , and shares many of its features .It is not as extensive , however , as Moses ' Experiment Management System .Installation .The pipeline has no required external dependencies .", "label": "", "metadata": {}, "score": "80.81956"}
{"text": "Fox , H. , \" Phrasal Cohesion and Statistical Machine Translation \" Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , Jul. 2002 , pp .304 - 311 .Association for Computational Linguistics .Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :", "label": "", "metadata": {}, "score": "80.92032"}
{"text": "Next , create a file called mert / decoder_command that contains the following command : . config \\ dev / news - dev2009 .es.tok.lc \\ mert / news - dev2009 .output.nbest .Next , create a configuration file for joshua at mert / joshua .", "label": "", "metadata": {}, "score": "80.95009"}
{"text": "However , this hard constraint can also rule out correct alignments , and its utility decreases as alignment models become more ... \" .Word alignment methods can gain valuable guidance by ensuring that their alignments maintain cohesion with respect to the phrases specified by a monolingual dependency tree .", "label": "", "metadata": {}, "score": "80.960915"}
{"text": "You can see how much the subsampling step reduces the training data , by yping wc -lw es - en / full - training / training . tok.lc es - en / full - training / subsampled / subsample . tok.lc : .", "label": "", "metadata": {}, "score": "81.026184"}
{"text": "Archer , D. and Rayson , P. ( 2004 )Using an historical semantic tagger as a diagnostic tool for variation in spelling .Presented at Thirteenth International Conference on English Historical Linguistics ( ICEHL 13 ) University of Vienna , Austria 23 - 29 August , 2004 .", "label": "", "metadata": {}, "score": "81.19085"}
{"text": "2006 Klosa , Annette , Ulrich Schn\u00f6rch , and Petra Storjohann .\" ELEXIKO - A Lexical and Lexicological , Corpus - based Hypertext Information System at the Institut f\u00fcr deutsche Sprache , Mannheim \" , Proceedings of the 12th Euralex International Congress ( 2006 ) .", "label": "", "metadata": {}, "score": "81.198425"}
{"text": "Head - Driven Statistical Models for Natural Language Parsing \" , Ph.D. thesis .Philadelphia : University of Pennsylvania , 1999 .Freund 1840 Freund , Wilhelm ( ed . )W\u00f6rterbuch der lateinischen Sprache : nach historisch - genetischen Principien , mit steter Ber\u00fccksichtigung der Grammatik , Synonymik und Alterthumskunde .", "label": "", "metadata": {}, "score": "81.28204"}
{"text": "The correctness of word mappings acquired in this fashion may depend highly on word length .While identical three - letter words were only translations of each other 60 % of the time , this was true for 98 % of ten - letter words .", "label": "", "metadata": {}, "score": "81.32922"}
{"text": "The small translation grammar contains 15,939 rules -- you can get the count of the number of rules by running gunzip -c example / example .The first part of the rule is the left - hand side non - terminal .", "label": "", "metadata": {}, "score": "81.419846"}
{"text": "For part of speech and phrasal categories , here are relevant links : .Please read the documentation for each of these corpora to learn about their tagsets and phrasal categories .You can often also find additional documentation resources by doing web searches .", "label": "", "metadata": {}, "score": "81.446594"}
{"text": "Digital methods also let us deal well with scale .For instance , while the OLD focused on a canon of Classical authors that ends around the second century CE , Latin continued to be a productive language for the ensuing two millennia , with prolific writers in the Middle Ages , Renaissance and beyond .", "label": "", "metadata": {}, "score": "81.456985"}
{"text": "Previous generative word alignment models have made structural assumptions such as the 1-to-1 , 1-to - N , or phrase - based consecutive word assumptions , while previous discriminative models have either made such ... \" .Word alignment is the problem of annotating parallel text with translational correspondence .", "label": "", "metadata": {}, "score": "81.91458"}
{"text": "Accordingly , the word comparator 120 may restrict the word length to be able to increase the accuracy of the collected word pairs .For instance , by relying only on words at least of length six , 622 word pairs were collected with 96 % accuracy .", "label": "", "metadata": {}, "score": "81.95679"}
{"text": "You must preprocess your dev and test sets in the same way you preprocess your training data .Run the following commands on the data that you downloaded : . cat es - en / dev / news - dev2009 .es.tok.lc cat es - en / dev / news - dev2009 .", "label": "", "metadata": {}, "score": "82.04018"}
{"text": "Mediae Latinitatis Lexicon Minus .Leiden : Brill , 1976 .Nivre et al .2006 Nivre , Joakim , Johan Hall , and Jens Nilsson . \"MaltParser : A Data - Driven Parser - Generator for Dependency Parsing \" , Proceedings of the Fifth International Conference on Language Resources and Evaluation ( 2006 ) .", "label": "", "metadata": {}, "score": "82.09381"}
{"text": "The method of .claim 1 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The method of .claim 1 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}, "score": "82.10387"}
{"text": "You can give the options -outputFormat typedDependencies or -outputFormat typedDependenciesCollapsed to get typed dependencies ( or grammatical relations ) output ( for English and Chinese only , currently ) .You can print out lexicalized trees ( head words and tags at each phrasal node with the -outputFormatOptions lexicalize option .", "label": "", "metadata": {}, "score": "82.13831"}
{"text": ".. a top - scoring MT system in the Chinese newswire track of the 2008 NIST evaluation .However , except for ( Fraser and Marcu , 2007b ) , none of these advances in alignment quality has improv ... . \" ...", "label": "", "metadata": {}, "score": "82.29901"}
{"text": "Corpus Linguistics around the world .Rodopi , Amsterdam .For more recent applications of the English Semantic Tagger , see the list on the Wmatrix website Tools . \" ...Statistical machine translation ( SMT ) treats the translation of natural language as a machine learning problem .", "label": "", "metadata": {}, "score": "82.50609"}
{"text": "An analysis of a large corpus of annotated learner English confirms this assumption .We evaluate our approach on real - world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance , homophones , and WordNet synonyms . ... where f denotes a foreign phrase in the L1 language .", "label": "", "metadata": {}, "score": "82.568245"}
{"text": "When using the Berkeley aligner , you 'll want to pay attention to how much memory you allocate to it with --aligner - mem ( the default is 10 g ) .aligner - chunk - size SIZE ( 1,000,000 ) .", "label": "", "metadata": {}, "score": "82.57297"}
{"text": "# Choose the training sources , which can either be directories or files that list files / directories trainSourcessubsampled/ .sentencesMAX .# 1-best output .competitiveThresholding .To run the Berkeley aligner , first set an environment variable saying where the aligner 's jar file is located ( this environment variable is just used for convenience in this document , and is not necessary for running the aligner in general : .", "label": "", "metadata": {}, "score": "82.66513"}
{"text": "These Russian lexical resources are licensed under a Creative Commons Attribution - NonCommercial - ShareAlike 2.5 License .The Italian , Chinese and Dutch resources are available and linked above .Publications describing the system ( or extensions of the system ) .", "label": "", "metadata": {}, "score": "82.77186"}
{"text": "Springer - Verlag , London , pp .251 - 263 .ISBN 1 - 85233 - 2220 .Barbara Lewandowska - Tomaszczyk , Michael Oakes & Paul Rayson ( 2001 ) .Annotated Corpora for Assistance with English - Polish Translation .", "label": "", "metadata": {}, "score": "82.93265"}
{"text": "All researchers who have a word alignment system available are invited . to participate in the shared task , individually or as part of a team .Participants in the shared task will be provided with common sets of .training data , consisting of Romanian - English and French - English . parallel texts .", "label": "", "metadata": {}, "score": "82.93483"}
{"text": "When you are aligning tens of millions of words worth of data , the word alignment process will take several hours to complete .While it is running , you can skip ahead and complete step 4 , but not step 5 .", "label": "", "metadata": {}, "score": "83.01371"}
{"text": "A name is needed to distinguish this test set from the previous ones .Output for this test run will be stored at RUNDIR / test / NAME . --joshua - config CONFIG .A tuned parameter file is required .This file will be the output of some prior tuning run .", "label": "", "metadata": {}, "score": "83.11214"}
{"text": "Step 7 : Decode a test set .When MERT finishes , it will output a file mert / joshua .config .ZMERT.final that contains the news weights for the different feature functions .You can copy this config file and use it to decode the test set .", "label": "", "metadata": {}, "score": "83.16994"}
{"text": "Based on these alignments , the system 400 may generate a parallel corpus 420 and identify translations 425 of words from the source language which are not in the lexicon .For example , consider the following two sentences where the only unknown French word is \" raison \" : .", "label": "", "metadata": {}, "score": "83.21284"}
{"text": "Restarting failed runs .If the pipeline dies , you can restart it with the same command you used the first time .If you rerun the pipeline with the exact same invocation as the previous run ( or an overlapping configuration - one that causes the same set of behaviors ) , you will see slightly different output compared to what we saw above : .", "label": "", "metadata": {}, "score": "83.336845"}
{"text": "[ 9 ] These are syntactic qualities since each of these arguments bears a direct syntactic relation to their head as much as they hold a semantic place within the underlying argument structure .In order to extract this kind of subcategorization and selectional information from unstructured text , we first need to impose syntactic order on it .", "label": "", "metadata": {}, "score": "83.3385"}
{"text": "In our experience , this works fine , but you should note the following caveats : .It is of crucial importance that you have enough physical disks .We have found that having too few , or too slow of disks , results in a whole host of seemingly unrelated issues that are hard to resolve , such as timeouts .", "label": "", "metadata": {}, "score": "83.38992"}
{"text": "MERT is a method for setting the weights of the different feature functions the translation model to maximize the translation quality on the dev set .Translation quality is calculated according to an automatic metric , such as Bleu .Our implementation of MERT allows you to easily implement some other metric , and optimize your paramters to that .", "label": "", "metadata": {}, "score": "83.41225"}
{"text": "In the recent literature the alignment task has frequently been decoupled from the translation task , and assumptions have been made about measuring alignment quality for machine translation which , it turns out , are not justified .In particular , none of the tens of papers published over the last five years has shown that significant decreases in Alignment Error Rate , AER ( Och and Ney , 2003 ) , result in significant increases in translation quality .", "label": "", "metadata": {}, "score": "83.42069"}
{"text": "You cat then look at the 1-best output file by typing cat example / example .nbest.srilm.out.1best : . the goal of gene scientists is to provide diagnostic tools to found of the flawed genes , are still provide a to stop these genes treatments .", "label": "", "metadata": {}, "score": "83.696045"}
{"text": "Description .CROSS - REFERENCE TO RELATED APPLICATIONS .This application is a Continuation of U.S. patent application Ser .No .10/401,124 , filed on Mar. 26 , 2003 , now U.S. Pat .No .7,620,538 , which claims priority to U.S. Provisional Application Ser .", "label": "", "metadata": {}, "score": "83.703156"}
{"text": "The statistical models may be trained on parallel corpora .Parallel corpora contain large amounts of text in one language along with their translation in another .Unfortunately , such corpora are available only in limited amounts and cover only in specific genres ( Canadian politics , Hong Kong laws , etc ) .", "label": "", "metadata": {}, "score": "83.84484"}
{"text": "In proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies ( NAACL HLT 2015 ) , Denver , Colorado , United States , pp .1268 - 1274 .Publications describing applications of the system .", "label": "", "metadata": {}, "score": "84.05443"}
{"text": "You can score your output using the JoshuaEval class , Joshua 's built - in scorer : .en.output \\ -ref dev / dev2006 .en.small \\ -m BLEU 4 closest LINGUIST List 14.662 .Fri Mar 7 2003 .Calls : Celtic Ling / Building and Using Parallel Texts .", "label": "", "metadata": {}, "score": "84.08477"}
{"text": "TrueCase.5gram.lm .Next , you 'll need to create a list of all of the alternative ways that each word can be capitalized .This will be stored in a map file that lists a lowercased word as the key and associates it with all of the variant capitalization of that word .", "label": "", "metadata": {}, "score": "84.1206"}
{"text": "Using parallel corpora for data driven Machine Translation . -Using parallel corpora for the derivation of language processing . tools in new languages . -Using parallel corpora for automatic corpora annotation .- Language learning applied to parallel corpora . - Translation memory systems as a source of aligned corpora .", "label": "", "metadata": {}, "score": "84.13031"}
{"text": "The Chinese lexicon also employs a set of extended kinship semantic tags designed by Qian and Piao ( 2009 ) .The full tagset is available in Chinese ( txt , docx , pdf ) .We are grateful for the assistance of Dr Richard Xiao ( Lancaster University , UK ) and Qian Yufang ( Zhejiang University of Media and Communications , China ) with this research .", "label": "", "metadata": {}, "score": "84.17463"}
{"text": "Mihalcea and Edmonds 2004 Mihalcea , Rada and Philip Edmonds ( eds . )Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) .Miller 1995 Miller , George .", "label": "", "metadata": {}, "score": "84.18538"}
{"text": "Qian , Yufang and Scott Piao ( 2009 ) .The Development of A Semantic Annotation Scheme for Chinese Kinship .Corpora , Vol . 4 ( 2 ) , Edinburgh University Press . pp .189 - 208 .Piao , S. , Bianchi , F. , Dayrell , C. , D'Egidio , A. and Rayson , P. ( 2015 ) .", "label": "", "metadata": {}, "score": "84.28788"}
{"text": "When SAMT grammars are being built ( --type samt ) , the target side of the training data must be parsed .The pipeline assumes your target side will be English , and will parse it for you using the Berkeley parser , which is included .", "label": "", "metadata": {}, "score": "84.32216"}
{"text": "The k best parses are extracted efficiently using the algorithm of Huang and Chiang ( 2005 ) .This may be because the parser chose an incorrect structure for your sentence , or because the phrase structure annotation conventions used for training the parser do n't match your expectations .", "label": "", "metadata": {}, "score": "84.43671"}
{"text": "Franz Josef Och , Hermann Ney : \" Improved Statistical Alignment Models \" ACLOO :Proc . of the 38th Annual Meeting of the Association for Computational Lingustics , ' Online !Oct. 2 - 6 , 2000 , pp .440 - 447 , XP002279144", "label": "", "metadata": {}, "score": "84.52632"}
{"text": "Bayesian classification is most commonly found in spam filtering .By counting each word and the class ( spam / not spam ) it appears in , we can assign it a probability that it falls into one class or the other .", "label": "", "metadata": {}, "score": "84.64928"}
{"text": "For example , the left alignment xyzabc , the right alignment xzy - acb and the words y and d in .FIG .9 ( iii ) make up a context alignment 915 .Given a comparable corpus , this procedure will yield many context alignments which correspond to incorrect translations , such as that between the words \" canadien \" and \" previous \" : . tout canadien serieux .", "label": "", "metadata": {}, "score": "84.66843"}
{"text": "The Perseus Digital Library contains at least one English translation for most of its Latin and Greek prose and poetry source texts .Many of these translations are encoded under the same canonical citation scheme as their source , but must further be aligned at the sentence and word level before individual word translation probabilities can be calculated .", "label": "", "metadata": {}, "score": "84.807434"}
{"text": "Once the parsing is complete , there will be two parsed files : .RUNDIR / data / train / corpus.en.parsed : this is the mixed - case file that was parsed .RUNDIR / data / train / corpus.parsed.en : this is a leaf - lowercased version of the above file used for grammar extraction .", "label": "", "metadata": {}, "score": "84.876945"}
{"text": "In a labeled dependency grammar , we can express a verb 's subcategorization as a combination of syntactic roles ( e.g. , OBJ OBJ ) .A predicate 's selectional preference specifies the type of argument it generally appears with .The verb to eat , for example , typically requires its object to be a thing that can be eaten and its subject to have animacy , unless used metaphorically .", "label": "", "metadata": {}, "score": "84.91641"}
{"text": "Since the Perseus Digital Library contains two large monolingual corpora ( the canon of Greek and Latin classical texts ) and sizable parallel corpora as well , we have investigated using parallel texts for word sense disambiguation .This method uses the same techniques we used to create a sense inventory to disambiguate words in context .", "label": "", "metadata": {}, "score": "84.92871"}
{"text": "Four of the parsers assume input that has already been word segmented , while the fifth does word segmentation internal to the parser .This is discussed further below .The parser also comes with 3 Chinese example sentences , in files whose names all begin with chinese .", "label": "", "metadata": {}, "score": "85.049995"}
{"text": "Searching by selectional preference .The ability to search by a predicate 's selectional preference is also a step toward semantic searching - the ability to search a text based on what it \" means . \"In building the lexicon , we automatically assign an argument structure to all of the verbs .", "label": "", "metadata": {}, "score": "85.058914"}
{"text": "es.tok.lc cat es - en / test / newstest2009 .en.tok.lc .Subsampling ( optional ) .Sometimes the amount of training data is so large that it makes creating word alignments extremely time - consuming and memory - intesive .We therefore provide a facility for subsampling the training corpus to select sentences that are relevant for a test set .", "label": "", "metadata": {}, "score": "85.109116"}
{"text": "Submissions should consist of regular full papers of max .7 pages , .formatted following the NAACL 2003 guidelines .In addition , teams . participating in the word alignment shared task are invited to submit .short papers ( max .", "label": "", "metadata": {}, "score": "85.11488"}
{"text": "By default , the tokenizer used by the English parser ( PTBTokenizer ) performs various normalizations so as to make the input closer to the normalized form of English found in the Penn Treebank .One of these normalizations is the Americanization of spelling variants ( such as changing colour to color ) .", "label": "", "metadata": {}, "score": "85.170555"}
{"text": "The last step .This is the default target of --last - step .We now discuss these steps in more detail .DATA PREPARATION .Data prepare involves doing the following to each of the training data ( --corpus ) , tuning data ( --tune ) , and testing data ( --test ) .", "label": "", "metadata": {}, "score": "85.39522"}
{"text": "1993 Marcus , Mitchell P. , Beatrice Santorini , and Mary Ann Marcinkiewicz .\" Building a Large Annotated Corpus of English : The Penn Treebank \" , Computational Linguistics 19.2 ( 1993 ) .McCarthy et al .2004 McCarthy , Diana , Rob Koeling , Julie Weeds and John Carroll .", "label": "", "metadata": {}, "score": "85.485115"}
{"text": "However , they span different time periods and have a different orientation : the World Street Journal covers mostly business news , the German news wire mostly German politics .The system 100 may use clues to find translations of words in the monolingual corpora .", "label": "", "metadata": {}, "score": "85.50469"}
{"text": "As the Dictionary and TreeTagger use different POS tagsets , they are both mapped into a simplified common tagset to be used by the software system .Currently Dutch semantic single word lexicon contains 4,203 entries .We are grateful for the assistance of Dr. Carole Tiberius ( INL , Netherlands ) with this research .", "label": "", "metadata": {}, "score": "85.64712"}
{"text": "This answer is specific to English .It mostly applies to other languages although some components are missing in some languages .The file englishPCFG.ser.gz comprises just an unlexicalized PCFG grammar .It is basically the parser described in the ACL 2003 Accurate Unlexicalized Parsing paper .", "label": "", "metadata": {}, "score": "85.66291"}
{"text": "I strongly recommend staring with the smaller set , and building an end - to - end system with it , since many steps take a very long time on the full data set .You should debug on the smaller set to avoid wasting time .", "label": "", "metadata": {}, "score": "85.6633"}
{"text": "This allows us to provide translation equivalents for idioms and common phrases such as res publica ( republic ) or gratias ago ( to give thanks ) .Corpus methods ( especially supervised methods ) generally perform best in the SENSEVAL competitions - at SENSEVAL-3 , the best system achieved an accuracy of 72.9 % in the English lexical sample task and 65.1 % in the English all - words task .", "label": "", "metadata": {}, "score": "85.75521"}
{"text": "Again using January 2014 version 3.3.1 as an example , you would not make your classpath -cp stanford - parser-3.3.1.jar Instead , you would make it Windows : -cp stanford - parser-3.3.1 . jar : stanford - parser-3.3.1-models.jar .In order to see exactly which models are available , you can use jar tvf stanford - parser-3.3.1-models.jar This will show you that to access the Arabic Factored model , for example , you would use the path edu / stanford / nlp / models / lexparser / arabicFactored . ser.gz .", "label": "", "metadata": {}, "score": "85.875595"}
{"text": "Alignment .You might want to start here if you want to skip data preprocessing .PARSE : Parsing .This is only relevant for building SAMT grammars ( --type samt ) , in which case the target side ( --target ) of the training data ( --corpus ) is parsed before building a grammar .", "label": "", "metadata": {}, "score": "85.91881"}
{"text": "1 len .2 len .Parsed 14 words in 2 sentences ( 6.55 wds / sec ; 0.94 sents / sec ) .Parsing file : chinese - onesent with 1 sentences .Parsing [ sent .1 len .Parsed 10 words in 1 sentences ( 10.78 wds / sec ; 1.08 sents / sec ) .", "label": "", "metadata": {}, "score": "85.972404"}
{"text": "i .b .i . )n .n .The result is a matrix with similarity scores between all German words , and a second matrix with similarity scores between all English words .For a new word , the lexicon builder 125 may look up its similarity scores to seed words , thus creating a similarity vector .", "label": "", "metadata": {}, "score": "85.975845"}
{"text": "Hebrew : UTF-8 .However , the parser is able to parse text in any encoding , providing you pass the correct encoding option on the command line , for example : .-encoding ISO_8859 - 15 .( Or , when used within a program , it is your job to open files with the right kind of Reader / Writer . ) caseless.ser.gz - Loading parser from serialized file edu / stanford / nlp / models / lexparser / englishPCFG .", "label": "", "metadata": {}, "score": "86.023285"}
{"text": "We feel that this is more useful for most semantic interpretation applications , because it directly connects the main predicate with its arguments , while the auxiliary is rendered as modifying the verb ( aux(singing , is ) ) .Most people seem to agree .", "label": "", "metadata": {}, "score": "86.236855"}
{"text": "data will be released .Participants will run their alignment system on .this test data and submit their results , which will be evaluated using .a common set of metrics .See the workshop website for details .regarding the shared task .", "label": "", "metadata": {}, "score": "86.39935"}
{"text": "For each position , the tokens are compared in frequency and the frequency count is replaced by the frequency rank , e.g. , the most frequent token count is replaced with 1 and the least frequent by n. R . a .", "label": "", "metadata": {}, "score": "86.75322"}
{"text": "Still , these words , often called \" cognates , \" maintain a very similar spelling .This can be defined as differing in very few letters .This measurement can be formalized as the number of letters common in sequence between the two words , divided by the length of the longer word .", "label": "", "metadata": {}, "score": "86.78417"}
{"text": "Currently , there are two Italian semantic lexicons : single word lexicon ( over 20,400 entries ) and multi - word lexicon ( over 4,100 entries which were manually checked ) .The Italian lexicons can be accessed here ( download as UTF-8 ) : ( a ) Single word lexicon ( b ) MWE lexicon .", "label": "", "metadata": {}, "score": "86.88895"}
{"text": "You can invoke it with the -escaper flag , by using a command like the following ( which also shows output being sent to a file ) : . stp .Word segmentation : Chinese is not normally written with spaces between words .", "label": "", "metadata": {}, "score": "86.95633"}
{"text": "The TLL , however , is impeccable in precision , while the Perseus and TLG results are dirty .What we need is a resource to combine the best of both .Where do we want to be ?The OLD and TLL are not likely to become obsolete anytime soon ; as the products of highly skilled editors and over a century of labor , the sense distinctions within them are highly precise and well substantiated .", "label": "", "metadata": {}, "score": "87.094246"}
{"text": "To translate a source sentence , we first employ a parser to pro - duce a source parse tree and then ap - ply TATs to transform the tree into a tar - get string .Our experiments show that the TAT - based model significantly outper - forms Pharaoh , a state - of - the - art decoder for phrase - based models . .", "label": "", "metadata": {}, "score": "87.29461"}
{"text": "DVParser with no flags .The memory requirements of the parser is not actually that high , but the more threads added with -trainingThreads , the more memory will be required to train .As the parser is training , it will output intermediate models every 20 minutes ( by default ) .", "label": "", "metadata": {}, "score": "87.36045"}
{"text": "Given an initial bilingual lexicon 405 and two texts 410 , 415 in each of the languages , the system 400 may identify parts of the texts which can be aligned ( i.e. , are mutual translations of each other according to the lexicon ) .", "label": "", "metadata": {}, "score": "87.49559"}
{"text": "LexicalizedParser -encoding utf-8 /u / nlp / data / lexparser / chineseFactored .ser.gz chinese-onesent-utf8.txt Loading parser from serialized file /u / nlp / data / lexparser / chineseFactored . ser.gz ... done [ 20.7 sec].Parsing file : chinese-onesent-utf8.txt with 2 sentences .", "label": "", "metadata": {}, "score": "87.65724"}
{"text": "claim 1 , wherein said identifying comprises : . identifying a plurality of context words ; and . identifying a frequency of context words in an n - word window around a target word .The method of .claim 9 , further comprising generating a context vector .", "label": "", "metadata": {}, "score": "87.69887"}
{"text": "They are : .PCFG .Factored .Factored , segmenting .Xinhua ( mainland , newswire ) .xinhuaPCFG.ser.gz . xinhuaFactored.ser.gz .xinhuaFactoredSegmenting.ser.gz .Mixed Chinese .chinesePCFG.ser.gz . chineseFactored.ser.gz .The PCFG parsers are smaller and faster .But the Factored parser is significantly better for Chinese , and we would generally recommend its use .", "label": "", "metadata": {}, "score": "87.70468"}
{"text": "obvious self - references in their abstracts .ABSTRACT SUBMISSION DEADLINE : . 1 April 2003 .NOTIFICATION OF ACCEPTANCE : . 1 May 2003 .PROGRAM COMMITTEE : .David Willis .Bob Borsley .Ian Roberts .Louisa Sadler .", "label": "", "metadata": {}, "score": "87.741165"}
{"text": "Acknowledgement This work is supported by National High Technology Research and Development Program contract \" Generally Technical Research and Basic Database Establishment of Chinese Platform\"(Subj ... . by Er Fraser Daniel Marcu - In Technical Report ISI - TR-616 . html , ISI / University of Southern California , 2006 . \" ...", "label": "", "metadata": {}, "score": "87.80472"}
{"text": "claim 1 , wherein said identifying comprises identifying frequencies of occurrence of words in the first and second first corpora .The method of .claim 1 , further comprising : generating matching scores for each of a plurality of clues .", "label": "", "metadata": {}, "score": "88.0011"}
{"text": "This is an element of the dependency analysis we adopted .It 's not uncontroversial , and it could have been done differently , but we 'll try to explain briefly why we did things the way we did .The general philosophy of the grammatical relations design is that main predicates should be heads and auxiliaries should not .", "label": "", "metadata": {}, "score": "88.02216"}
{"text": "The -order 3 tells srilm to produce a trigram language model .You can set this to a higher value , and srilm will happily output 4-gram , 5-gram or even higher order language models .The -kndiscount tells SRILM to use modified Kneser - Ney discounting as its smoothing scheme .", "label": "", "metadata": {}, "score": "88.05391"}
{"text": "You 'll notice that your output is all lowercased and has the punctuation split off .In order to make the output more readable to human beings ( remember us ? ) , it 'd be good to fix these problems and use proper punctuation and spacing .", "label": "", "metadata": {}, "score": "88.05628"}
{"text": "claim 12 , further comprising adding the matching scores .The method of . claim 13 , further comprising weighting the matching scores .A non - transitory computer readable medium having embodied thereon a program , the program being executable by a processor for performing a method for building a translation lexicon from non - parallel corpora , the method comprising : . generating a seed lexicon by the machine translation system , the seed lexicon including identically spelled words ; and .", "label": "", "metadata": {}, "score": "88.36198"}
{"text": "The pipeline eases the pain of two related tasks in statistical machine translation ( SMT ) research : .Training SMT systems involves a complicated process of interacting steps that are time - consuming and prone to failure .Developing and testing new techniques requires varying parameters at different points in the pipeline .", "label": "", "metadata": {}, "score": "88.48912"}
{"text": "Rayson , P. ( 2005 )Right from the word go : identifying multi - word - expressions for semantic tagging .Invited talk at BAAL Corpus Linguistics SIG / OTA Workshop : Identifying and Researching Multi - Word Units .Thursday 21st", "label": "", "metadata": {}, "score": "88.50667"}
{"text": "The English semantic lexicon entries have been automatically translated into Italian counterparts using FreeLang and other English - Italian Dictionaries with the help of Italian native speakers .Although some lexicon entries were manually checked , most of the entries were automatically generated and therefore they inevitably contain errors , which need to be cleared manually in future .", "label": "", "metadata": {}, "score": "88.56773"}
{"text": "The ability to search a Latin or Greek text by an English translation equivalent is a close approximation to real cross - language information retrieval .By searching for word sense , however , a scholar can simply search for slave and automatically be presented with all of the passages for which this translation equivalent applies .", "label": "", "metadata": {}, "score": "88.58768"}
{"text": "You should really try to install physical disks that are dedicated to Hadoop scratch space .Here are some flags relevant to Hadoop and grammar extraction with Thrax : . --hadoop /path / to / hadoop .This sets the location of Hadoop ( overriding the environment variable $ HADOOP ) .", "label": "", "metadata": {}, "score": "88.817505"}
{"text": "Unfortunately the relationship between alignment quality and statistical machine translation performance has not been well understood .In the recent literature the alignment task has frequently been decoupled from the ... \" .Automatic word alignment plays a critical role in statistical machine translation .", "label": "", "metadata": {}, "score": "88.85068"}
{"text": "Currently , there are two Portuguese semantic lexicons : single word lexicon ( over 13,900 entries ) and multi - word lexicon ( over 1,780 entries ) .The Portuguese lexicons are being created with the help of Carmen Dayrell ( CASS , Lancaster University , UK ) .", "label": "", "metadata": {}, "score": "88.91403"}
{"text": "7sISI - University of Southern California ISI - TR-616 Acknowledgments This work was supporte ... . \" ...Bilingual word alignment forms the foundation of most approaches to statistical machine translation .Current word alignment methods are predominantly based on generative models .", "label": "", "metadata": {}, "score": "89.01013"}
{"text": "Studies in Honour of Jarmila Panevov\u00e1 .Prague : Charles University Press , 1999 .Kilgarriff et al .2004 Kilgarriff , Adam , Pavel Rychly , Pavel Smrz , and David Tugwell . \"The Sketch Engine \" , Proceedings of EURALEX ( 2004 ) .", "label": "", "metadata": {}, "score": "89.02055"}
{"text": "; \" and .\" It is for this reason that the party has proposed . . . \" .Since \" Ce est pour cette \" can be aligned with \" It is for this \" and \" que le \" with \" that the , \" it is a reasonable assumption that \" raison \" can be translated by \" reason .", "label": "", "metadata": {}, "score": "89.04576"}
{"text": "[ 1 ] The Biblioteca Teubneriana BTL-1 collection , for instance , contains 6.6 million words , covering Latin literature up to the second century CE .For a recent overview of the Index Thomisticus , including the corpus size and composition , see Busa ( 2004 ) .", "label": "", "metadata": {}, "score": "89.179214"}
{"text": "A Systematic Comparison of Various Statistical Alignment Models \" , Computational Linguistics 29.1 ( 2003 ) .Pinkster 1990 Pinkster , Harm .Latin Syntax and Semantics .London : Routledge , 1990 .Sch\u00fctz 1895 Sch\u00fctz , Ludwig .Thomas - Lexikon .", "label": "", "metadata": {}, "score": "89.23801"}
{"text": "The USAS framework is now being extended to cover six more languages : Chinese , Dutch , Italian , Portuguese , Spanish and Malay .The Java software framework developed in the Benedict and ASSIST projects has been modified to accommodate these languages , and semantic lexicons are compiled for them by automatically \" translating \" the English semantic lexicon entries , with some manual improvement where possible .", "label": "", "metadata": {}, "score": "89.2963"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "89.42091"}
{"text": "Gildea , D. , \" Loosely Tree - based Alignment for Machine Translation , \" In Proceedings of the 41st Annual Meeting on Assoc . for Computational Linguistics - vol . 1 ( Sapporo , Japan , Jul. 7 - 12 , 2003 ) .", "label": "", "metadata": {}, "score": "89.42091"}
{"text": "The method of .claim 1 , wherein said identifying comprises identifying cognates .The method of .claim 1 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The method of .claim 1 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}, "score": "89.47974"}
{"text": "claim 3 , wherein said identifying substantially identical words comprises .applying transformation rules to words in the first corpora to form transformed words ; and . comparing said transformed words to words in the second corpora .The method of .", "label": "", "metadata": {}, "score": "89.66886"}
{"text": "It has a multi - tier structure with 21 major discourse fields ( shown here on the right ) , subdivided , and with the possibility of further fine - grained subdivision in certain cases .We have written an introduction to the USAS category system ( PDF file ) with examples of prototypical words and multi - word units in each semantic field .", "label": "", "metadata": {}, "score": "89.82697"}
{"text": "The Dutch multiword lexicon is not available yet .The Italian semantic tagger is being developed in collaboration with Dr Francesca Bianchi ( Dip . di Studi Umanistici , Universita del Salento , Italy ) and Prof. Elena Semino ( Dept . of Linguistics and English Language , Lancaster University , UK ) .", "label": "", "metadata": {}, "score": "90.15631"}
{"text": "Searching by word sense also allows us to investigate problems of changing orthography - both across authors and time : as Latin passes through the Middle Ages , for instance , the spelling of words changes dramatically even while meaning remains the same .", "label": "", "metadata": {}, "score": "90.25164"}
{"text": "Up to six state - of - the - art statistical phrase - based translation systems from different project partners were combined in the experiments .Significant improvements in translation quality from Spanish to English and from English to Spanish in comparison with the best of the individual MT systems were achieved under official evaluation conditions .", "label": "", "metadata": {}, "score": "90.536835"}
{"text": "Busa 1974 - 1980 Busa , Roberto .Index Thomisticus : sancti Thomae Aquinatis operum omnium indices et concordantiae , in quibus verborum omnium et singulorum formae et lemmata cum suis frequentiis et contextibus variis modis referuntur quaeque / consociata plurium opera atque electronico IBM automato usus digessit Robertus Busa SI .", "label": "", "metadata": {}, "score": "90.54364"}
{"text": "Again , there are language - specific tokenizations for a few languages ( English , German , and Greek ) .( Training only ) Removing all parallel sentences with more than --maxlen tokens on either side .By default , MAXLEN is 50 .", "label": "", "metadata": {}, "score": "90.607796"}
{"text": "In the following I will detail how we can leverage them all to uncover large - scale usage patterns in a text .Word Sense Induction .So , for example , the Greek word arch\u00ea may be translated in one context as beginning and in another as empire , corresponding respectively to LSJ definitions I.1 and II.2 .", "label": "", "metadata": {}, "score": "90.648544"}
{"text": "The default amount of memory is 3100 m , which is likely not enough ( especially if you are decoding with SAMT grammar ) .You can alter the amount of memory for Joshua using the --joshua - mem MEM argument , where MEM is a Java memory specification ( passed to its -Xmx flag ) .", "label": "", "metadata": {}, "score": "90.91411"}
{"text": "BACKGROUND .Machine translation ( MT ) concerns the automatic translation of natural language sentences from a first language ( e.g. , French ) into another language ( e.g. , English ) .Systems that perform MT techniques are said to \" decode \" the source language into the target language .", "label": "", "metadata": {}, "score": "90.97496"}
{"text": "Versailles , France , June 28 - 30th , 2000 .Rayson , P. , Garside , R. , and Sawyer , P. ( 2000 ) .Assisting Requirements Recovery from Legacy Documents .In Henderson , P. ( ed . )", "label": "", "metadata": {}, "score": "91.17526"}
{"text": "Send your submission ( a ps or pdf file ) , prepared for anonymous .review , to both : .Rada Mihalcea , University of North Texas , rada cs.unt.edu and Ted .Pedersen , University of Minnesota , Duluth , tpederse d.umn.edu .", "label": "", "metadata": {}, "score": "91.20502"}
{"text": "test.in -- This is the input file containing the sentences to translate . example / example .nbest.srilm.out -- This is the output file that the n - best translations will be written to .You can inspect the output file by typing head example / example .", "label": "", "metadata": {}, "score": "91.26795"}
{"text": "Faculty of Engineering and Computing , Nottingham Trent University , UK .ISBN 0 905 488628 .Wilson , A. and Thomas , J.A. ( 1997 )Semantic annotation , in Garside , R. , Leech , G. , and McEnery , A. ( eds . )", "label": "", "metadata": {}, "score": "91.68669"}
{"text": "In the past thirty years , computers have allowed this process to be significantly expedited , even in such simple ways as textual searching .This approach has been exploited most recently by the Greek Lexicon Project [ 2 ] at the University of Cambridge , which has been developing a New Greek Lexicon since 1998 using a large database of electronically compiled slips ( with a target completion date of 2010 ) .", "label": "", "metadata": {}, "score": "91.720245"}
{"text": "A single reference will have the format TUNE.TARGET , while multiple references will have the format TUNE.TARGET.NUM , where NUM starts at 0 and increments for as many references as there are .The following processing steps are applied to each file .", "label": "", "metadata": {}, "score": "91.97568"}
{"text": "This frequently seems to confuse people , because the main predicate of the clause is now not a verb .But we believe that this is the best thing to do for several reasons : .Consistency of treatment of auxiliary / copula between English periphrastic verb forms and adjectival / nominal predications .", "label": "", "metadata": {}, "score": "92.2119"}
{"text": "Automatic Sense Disambiguation Using Machine Readable Dictionaries : How to Tell a Pine Cone from an Ice Cream Cone \" , Proceedings of the ACM - SIGDOC Conference ( 1986 ) .Lewis and Short 1879 Lewis , Charles T. and Charles Short ( eds . )", "label": "", "metadata": {}, "score": "92.38133"}
{"text": "corpora addressing languages with scarce resources .We expect to make arrangements with a journal in Natural Language .Processing or Computational Linguistics for a special issue that will . include selected papers from this workshop .Invited Speaker : .Elliot Macklovitch , University of Montreal .", "label": "", "metadata": {}, "score": "92.39631"}
{"text": "The Portuguese lexicons have been compiled by translating the English semantic lexicon entries using a Portuguese - English dictionary developed by Davies and Preto - Bay ( 2007 ) and FreeLang Portuguese - English bilingual lexicon .A small section of the lexicons were manually checked , but most of the lexicon entries were automatically generated and therefore contain errors , which need to be cleared manually in the future .", "label": "", "metadata": {}, "score": "92.51466"}
{"text": "The Spanish semantic tagger is in an early stage of development , following a similar process to that of the Italian semantic tagger , using the Spanish TreeTagger .Currently Spanish tagger has only a single - word semantic lexicon compiled by translating the English semantic lexicon entries using a Spanish - English dictionary compiled by Mark Davies ( 2006 ) .", "label": "", "metadata": {}, "score": "92.76376"}
{"text": "ASSIST : Automated Semantic Assistance for Translators .In companion proceedings to the 11th Conference of the European Chapter of the Association for Computational Linguistics ( EACL 2006 ) , Trento , Italy , April 3 - 7 , 2006 , pp .", "label": "", "metadata": {}, "score": "92.976906"}
{"text": "The default HeadFinder is written specifically for the PTB .If you train a parser on trees that use a different set of productions , the default HeadFinder will not know how to handle this and will throw this exception .The easiest way to get around this problem is to use LeftHeadFinder instead .", "label": "", "metadata": {}, "score": "93.02576"}
{"text": "Oxford Latin Dictionary .Oxford : Oxford University Press , 1968 - 1982 .Grozea 2004 Grozea , Christian .\" Finding Optimal Parameter Settings for High Performance Word Sense Disambiguation \" , Proceedings of Senseval-3 : Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text ( 2004 ) .", "label": "", "metadata": {}, "score": "93.10495"}
{"text": "Great advances have been made in the sciences on which lexicography depends .Minute research in manuscript authorities has largely restored the texts of the classical writers , and even their orthography .Philology has traced the growth and history of thousands of words , and revealed meanings and shades of meaning which were long unknown .", "label": "", "metadata": {}, "score": "93.22975"}
{"text": "The Hansard Corpus includes parallel texts in English and Canadian French , drawn from official records of the proceedings of the Canadian Parliament .A small bilingual lexicon of 6,900 entries was built using 5,000 sentences pairs ( 150,000 words for each language ) .", "label": "", "metadata": {}, "score": "93.4892"}
{"text": "Abstract - This paper describes an approach for computing a consensus translation from the outputs of multiple machine translation ( MT ) systems .The consensus translation is computed by weighted majority voting on a confusion network , similarly to the well - established ROVER approach of Fiscus for combining speech recognition hypotheses .", "label": "", "metadata": {}, "score": "93.500435"}
{"text": "Classics has arguably the most well - curated collection of texts in the world , and the uses its scholars demand from that collection are unique .In the following I will document the technologies available to us in creating a new kind of reference work for the future - one that complements the traditional lexicography exemplified by the OLD and the TLL and lets scholars interact with their texts in new and exciting ways .", "label": "", "metadata": {}, "score": "93.551575"}
{"text": "The \" scholarship of thirty years ago \" that Lewis and Short here distance themselves from is Andrews ' 1850 Latin - English lexicon , itself largely a translation of Freund 's German W\u00f6rterbuch published only a decade before .Founded on the same lexicographic principles that produced the juggernaut Oxford English Dictionary , the OLD is a testament to the extraordinary results that rigorous manual labor can provide .", "label": "", "metadata": {}, "score": "93.83328"}
{"text": "A website interface is provided for users to test the semantic taggers .This is a beta release of the tools , which will be improved in future .Please get in touch with Paul Rayson if you would like to be involved in further improvements of the tools .", "label": "", "metadata": {}, "score": "94.0626"}
{"text": "The non - transitory computer readable medium of .claim 15 wherein said expanding comprises using the identically spelled words in the seed lexicon as accurate translations .The non - transitory computer readable medium of . claim 15 , further comprising : . identifying substantially identical words in the first and second corpora ; and . adding said substantially identical words to the seed lexicon .", "label": "", "metadata": {}, "score": "94.652985"}
{"text": "60/368,070 , filed on Mar. 26 , 2002 , and U.S. Provisional Application Ser .No .60/368,447 , filed on Mar. 27 , 2002 , the disclosures of which are incorporated by reference .ORIGIN OF INVENTION .The research and development described in this application were supported by Defense Advanced Research Project Agency ( DARPA ) under grant number N66001 - 00 - 1 - 8914 .", "label": "", "metadata": {}, "score": "94.87439"}
{"text": "In Williams G. and Vessier S. ( eds . )Proceedings of the 11th EURALEX ( European Association for Lexicography ) International Congress ( Euralex 2004 ) , Lorient , France , 6 - 10 July 2004 .Universit\u00e9 de Bretagne Sud .", "label": "", "metadata": {}, "score": "95.02493"}
{"text": "Clare Voss , Army Research Lab .Dan Tufis , RACAI Romania .Yorick Wilks , University of Sheffield Sidebar .Table of Contents .Corpora .The resources below are large corpora build by downloading text from the web .See the Publications section for further details , and the Free web interfaces section for information on how to get them : .", "label": "", "metadata": {}, "score": "95.6093"}
{"text": "Automatic Content Analysis and the Stylistic Analysis of Prose Literature .Revue : Informatique et Statistique dans les Sciences Humaines 29 : 219 - 234 .Thomas , J. , and Wilson , A. ( 1996 ) .Methodologies for studying a corpus of doctor - patient interaction .", "label": "", "metadata": {}, "score": "96.21612"}
{"text": "Banerjee and Pedersen 2002 Banerjee , Sid and Ted Pedersen . \"An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet \" , Proceedings of the Conference on Computational Linguistics and Intelligent Text Processing ( 2002 ) .Bourne 1916 Bourne , Ella . \"", "label": "", "metadata": {}, "score": "96.333626"}
{"text": "# lm order weight .lm 1.0 .# phrasemodel owner column(0-indexed ) weight . phrasemodel pt 0 1.4037585111897322 . phrasemodel pt 1 0.38379188013385945 . phrasemodel pt 2 0.47752204361625605 .# arityphrasepenalty owner start_arity end_arity weight .# arityphrasepenalty pt 0 0 1.0 .", "label": "", "metadata": {}, "score": "96.35423"}
{"text": "A lexicon builder 125 may expand the seed lexicon into the larger translation lexicon 105 by applying rules based on clues which indicate probable translations .The lexicon builder 125 may use seed lexicon to bootstrap these methods , using the word pairs in the seed lexicon as correct translations .", "label": "", "metadata": {}, "score": "96.401695"}
{"text": "There 's not much in the way of secret sauce to speed that up ( partly by the design of the parsers as guaranteed to find model optimal solutions ) .If you 're not using englishPCFG.ser.gz for English , then you should be - it 's much faster than the Factored parser .", "label": "", "metadata": {}, "score": "96.64844"}
{"text": "Just as a spam filter is trained by a user explicitly labeling a message as spam , this classifier can be trained simply by the presence of an aligned translation .For instance , the Latin word spiritus has several senses , including spirit and wind .", "label": "", "metadata": {}, "score": "96.65831"}
{"text": "Even within a given time frame , spelling can vary , especially from poetry to prose .By allowing users to search for a sense rather than a specific word form , we can return all passages containing saeculum , saeclum , seculum and seclum - all valid forms for era .", "label": "", "metadata": {}, "score": "96.692444"}
{"text": "The / DT quick / JJ brown / JJ fox / NN jumped / VBD over / IN the / DT lazy / JJ dog / NN .with the command : . ser.gz fox.txt .Partially - tagged input ( only indicating the POS of some words ) is also OK .", "label": "", "metadata": {}, "score": "96.75882"}
{"text": "\" The lexicon builder 125 may measure the spelling similarity between every German and English word , and sort possible word pairs accordingly .This may be done in a greedy fashion , i.e. , once a word is assigned to a word pair , the lexicon builder 125 does not look for another match .", "label": "", "metadata": {}, "score": "97.377975"}
{"text": "This is an especially appropriate manner of representation for languages with a free word order ( such as Latin and Czech ) , where the linear order of constituents is broken up with elements of other constituents .A dependency grammar representation , for example , of ista meam norit gloria canitiem Propertius I.8.46 - \" that glory would know my old age \" - would look like the following : .", "label": "", "metadata": {}, "score": "97.432976"}
{"text": "The history of ancient nations , the private life of the citizens , the thoughts and beliefs of their writers have been closely scrutinized in the light of accumulating information .Thus the student of to - day may justly demand of his Dictionary far more than the scholarship of thirty years ago could furnish .", "label": "", "metadata": {}, "score": "97.53337"}
{"text": "We apply this alignment model to both French - English and Romanian - English language pairs .An exception is Taskar et al .( 2005 ) who presented a word matching model for discriminative alignment which they they were able to solve optimally .", "label": "", "metadata": {}, "score": "98.326324"}
{"text": "Bamman and Crane 2007 Bamman , David and Gregory Crane . \"The Latin Dependency Treebank in a Cultural Heritage Digital Library \" , Proceedings of the ACL Workshop on Language Technology for Cultural Heritage Data ( 2007 ) .Bamman and Crane 2008 Bamman , David and Gregory Crane .", "label": "", "metadata": {}, "score": "99.033295"}
{"text": "The apparatus of 26 , the apparatus comprising : . an alignment module operative to be executed to align text segments in two nonparallel corpora , the corpora including a source language corpus and a target language corpus .The apparatus of .", "label": "", "metadata": {}, "score": "99.08386"}
{"text": "Ulrich Germann , Information Sciences Institute .Daniel Gildea , University of Pennsylvania .Maria das Gracas Volpe Nunes , University of Sao Paulo .Nancy Ide , Vassar College .Lucia Helena Machado Rino , Federal University of Sao Carlos .", "label": "", "metadata": {}, "score": "99.780556"}
{"text": "We would recommend their use for parsing material from mainland China .The chinese grammars also include some training material from Hong Kong SAR and Taiwan .We 'd recommend their use if parsing material from these areas or a mixture of text types .", "label": "", "metadata": {}, "score": "100.08937"}
{"text": "You can lowercase your tokenized data with the following script : . cat es - en / full - training / training .en.tok.lc cat es - en / full - training / training .es.tok.lc .Resumption of the session I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "100.31445"}
{"text": "The non - transitory computer readable medium of . claim 15 , wherein said identifying comprises identifying word pairs having a minimum longest common subsequence ratio .The non - transitory computer readable medium of . claim 15 , wherein said one or more clues includes similar context .", "label": "", "metadata": {}, "score": "100.82698"}
{"text": "That is , they will just say Jill busy .Connection to logical representations : If you were to translate these sentences into a simple predicate logic form , you would presumably use busy(jill ) and teacher(jill ) .The treatment of the adjective or noun as the predicate in a predicate logic form parallels what we do in our grammatical relations representation .", "label": "", "metadata": {}, "score": "100.86599"}
{"text": "Classics , however , is only one field among many concerned with the technologies underlying lexicography , and by relying on the techniques of other disciplines like computational linguistics and computer science , we can count on the future progress of disciplines far outside our own .", "label": "", "metadata": {}, "score": "101.3896"}
{"text": "Each of the clues provides a matching score between two words ( block 220 ) , e.g. , a German word and an English word .The likelihood of these two words being actual translations of each other may correlate to these scores .", "label": "", "metadata": {}, "score": "101.83369"}
{"text": "Grace Ngai , Hong Kong Polytechnic University .Hermann Ney , RWTH Aachen .Franz Och , Information Sciences Institute .Kemal Oflazer , Sabanci University .Kishore Papineni , IBM .Jessie Pinkham , Microsoft Research .Andrei Popescu - Belis , ISSCO / TIM / ETI University of Geneva .", "label": "", "metadata": {}, "score": "101.887436"}
{"text": "In Dawn Archer , Paul Rayson , Andrew Wilson and Tony McEnery ( eds . )Proceedings of the Corpus Linguistics 2003 conference .UCREL technical paper number 16 .UCREL , Lancaster University , pp .457 - 464 .Archer , D. , Rayson , P. , Piao , S. , McEnery , T. ( 2004 ) .", "label": "", "metadata": {}, "score": "102.52965"}
{"text": "Of necessity , these citations are usually only exemplary selections , though the TLL provides comprehensive listings by Classical authors for many of its lemmata .These citations essentially function as an index into the textual collection .If I am interested in the places in Classical literature where the verb libero means to acquit , I can consult the OLD and then turn to the source texts it cites : Cic .", "label": "", "metadata": {}, "score": "102.88068"}
{"text": "Deadline for regular paper submissions : March 10 .Deadline for results submissions : March 25 ( shared task ) .Deadline for short paper submissions : April 1 ( shared task ) .Notification of acceptance for regular papers : April 1 .", "label": "", "metadata": {}, "score": "102.90825"}
{"text": "Institute .Philippe Langlais , University of Montreal .Elliot Macklovitch , University of Montreal .Daniel Marcu , University of Southern California / Information Sciences .Institute .Dan Melamed , New York University .Magnus Merkel , Linkoping University .", "label": "", "metadata": {}, "score": "103.91316"}
{"text": "The original , pre - lemmatized Latin is salvum tu me esse cupisti ( Cicero , Pro Plancio , chapter 33 ) .The original English is you wished me to be safe .As a result of the lemmatization process , many source words are mapped to multiple words in the target - most often to lemmas which share a common inflection .", "label": "", "metadata": {}, "score": "104.95227"}
{"text": "Longman , London .pp 179 - 193 .Dawn Archer , Andrew Wilson , Paul Rayson ( 2002 ) .Introduction to the USAS category system .Benedict project report , October 2002 .Dawn Archer , Tony McEnery , Paul Rayson , Andrew Hardie ( 2003 ) .", "label": "", "metadata": {}, "score": "104.99595"}
{"text": "Given that the English side of the parallel corpus is a relatively small amount of data in terms of language modeling , it only takes a few minutes a few minutes to output the LM .The uncompressed LM is 144 megabytes large ( du -h europarl.en.trigram.lm ) .", "label": "", "metadata": {}, "score": "106.025604"}
{"text": "E - mail submissions should be sent to : David Willis ( dwew2 cam.ac.uk ) .Hard - copy submissions should be sent to : .David Willis .Dept . of Linguistics .University of Cambridge .Sidgwick Avenue , Cambridge .", "label": "", "metadata": {}, "score": "106.05737"}
{"text": "Assoc . for Computational Linguistics , Morristown , NJ .Computational Linguistics and Classical Lexicography .Abstract .Manual lexicography has produced extraordinary results for Greek and Latin , but it can not in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials .", "label": "", "metadata": {}, "score": "106.08323"}
{"text": "Organisation Committee : .Rada Mihalcea , University of North Texas .Ted Pedersen , University of Minnesota , Duluth .Program Committee : .Lars Ahrenberg , Linkoping University .Nicoletta Calzolari , University of Pisa .Tim Chklovski , Massachusetts Institute of Technology .", "label": "", "metadata": {}, "score": "106.09229"}
{"text": "In deciding how we want to design a cyberinfrastructure for Classics over the next ten years , there is an important question that lurks between \" where are we now ? \" and \" where do we want to be ?\" : where are our colleagues already ?", "label": "", "metadata": {}, "score": "107.340034"}
{"text": "Andrews 1850 Andrews , E. A. ( ed . )A Copious and Critical Latin - English Lexicon , Founded on the Larger Latin - German Lexicon of Dr. William Freund ; With Additions and Corrections from the Lexicons of Gesner , Facciolati , Scheller , Georges , etc . .", "label": "", "metadata": {}, "score": "108.507614"}
{"text": "ISBN 1 - 932432 - 60 - 4 .Andrew Wilson , Olga Moudraia ( 2006 ) Quantitative or Qualitative Content Analysis ?Experiences from a cross - cultural comparison of female students ' attitudes to shoe fashions in Germany , Poland and Russia .", "label": "", "metadata": {}, "score": "108.69527"}
{"text": "Philip Resnik , University of Maryland .Antonio Ribeiro , Joint Research Centre , Ispra , Italy .Michel Simard , University of Montreal .Harold Somers , University of Manchester Institute of Science and .Technology .Arturo Trujillo , Canon Research Centre Europe .", "label": "", "metadata": {}, "score": "108.84178"}
{"text": "That is , sentences like Jill is busy or Jill is a teacher .We continue to regard the adjective or noun as the predicate of which the subject is the argument , rather than changing and now regarding the copular verb is as the head and busy / teacher as a complement .", "label": "", "metadata": {}, "score": "109.490814"}
{"text": "The grant holders and supervisors were Roger Garside ( Computing ) , Geoff Leech ( Linguistics ) and Jenny Thomas ( Linguistics , now at Bangor ) .Tony McEnery was the principal investigator for Benedict .For ASSIST , Roger Garside , Tony McEnery , Andrew Wilson and Paul Rayson were the grant holders .", "label": "", "metadata": {}, "score": "109.56085"}
{"text": "When it is translated as spirit , its context has ( more naturally ) a religious tone , including words such as sanctus ( holy ) and omnipotens ( all - powerful ) .If we are confronted with an instance of spiritus in a sentence for which we have no translation , we can disambiguate it as either spirit or wind by looking at its context in the original Latin .", "label": "", "metadata": {}, "score": "109.70395"}
{"text": "In Dawn Archer , Paul Rayson , Andrew Wilson and Tony McEnery ( eds . )Proceedings of the Corpus Linguistics 2003 conference .UCREL technical paper number 16 .UCREL , Lancaster University , pp .22 - 31 .Laura L\u00f6fberg , Dawn Archer , Scott Piao , Paul Rayson , Tony McEnery , Krista Varantola , Jukka - Pekka Juntunen ( 2003 ) .", "label": "", "metadata": {}, "score": "110.22616"}
{"text": "After tokenization and lowercasing , the file looks like this ( head -3 es - en / full - training / training .en.tok.lc ): . resumption of the session i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .", "label": "", "metadata": {}, "score": "111.1575"}
{"text": "People .Andrew Wilson was the RA in Linguistics on the first two projects and Paul Rayson was the RA in Computing on all five projects .Scott Piao and Dawn Archer were the RAs on the Benedict project .Olga Mudraya was the RA in Linguistics and Scott Piao was the Computing RA on the Assist project .", "label": "", "metadata": {}, "score": "111.584366"}
{"text": "Department of Language and Linguistics .University of Essex .Wivenhoe Park .COLCHESTER CO4 3SQ , UK . rborsley essex.ac.uk .tel : +44 1206 873762 .Notice the March 10 Deadline for regular papers submissions !The goal of this workshop is to provide a forum for researchers . working on problems related to the creation and use of parallel . text .", "label": "", "metadata": {}, "score": "111.76142"}
{"text": "As a matter of policy , LINGUIST discourages the use of abbreviations or acronyms in conference announcements unless they are explained in the text .Directory .FOURTH CELTIC LINGUISTICS CONFERENCE Selwyn College University of Cambridge , UK 1 - 3 September , 2003 SECOND CALL FOR PAPERS The 4th Celtic Linguistics Conference will take place on 1 - 3 September 2003 at Selwyn College , University of Cambridge , UK .", "label": "", "metadata": {}, "score": "114.63929"}
{"text": "The best word matches may be collected in a greedy fashion .Another clue is based on the assumption that pairs of words that are similar in one language should have translations that are similar in the other language .For instance , Wednesday is similar to Thursday as Mittwoch is similar to Donnerstag .", "label": "", "metadata": {}, "score": "121.53885"}
{"text": "Parsing file : - i ca n't believe @mistamau does n't know who channing tatum is ... # loser Parsing [ sent .1 len .Parsed 14 words in 1 sentences ( 4.29 wds / sec ; 0.31 sents / sec ) .", "label": "", "metadata": {}, "score": "122.416084"}
{"text": "FEEDBACK .", "label": "", "metadata": {}, "score": "126.14415"}
{"text": "Oxford : Clarendon Press , 1879 .Liddell and Scott 1940 Liddell , Henry George and Robert Scott ( eds . )A Greek - English Lexicon , revised and augmented throughout by Sir Henry Stuart Jones .Oxford : Clarendon Press , 1940 .", "label": "", "metadata": {}, "score": "136.95105"}
