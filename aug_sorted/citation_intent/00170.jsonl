{"text": "LDA creates a linear combination of the given independent features that yield the largest mean differences between the desired classes [ 2 ] .Given a set of . , for all the samples of all the . classes , the within - class scatter matrix .", "label": "", "metadata": {}, "score": "34.682934"}
{"text": "The generated PCA features do not have clear physical meanings .In contrast , LDA searches for those vectors in the underlying space that best discriminate among the classes rather than those that best describe the data .can be solved under different criteria such as the output variance maximization or MSE minimization .", "label": "", "metadata": {}, "score": "35.63478"}
{"text": "The overall performance of the two - stage approach is sensitive to the reduced dimension in the first stage .A generalization of LDA by using generalized SVD [ 153 ] can be used to solve the problem of singularity of .", "label": "", "metadata": {}, "score": "35.918556"}
{"text": "View at Google Scholar .T. Kohonen , \" Emergence of invariant - feature detectors in the adaptive - subspace self - organizing map , \" Biological Cybernetics , vol .75 , no .4 , pp .281 - 291 , 1996 .", "label": "", "metadata": {}, "score": "39.029823"}
{"text": "T . . .If .A .Chapter f12 contains functions based on the Lanczos method for real symmetric large sparse eigenvalue problems , and these functions are usually more efficient than subspace iteration .2.1.2 Standard nonsymmetric eigenvalue problems .", "label": "", "metadata": {}, "score": "42.71985"}
{"text": "Each region is concatenated into a vector , and all the vectors constitute a training set .PCA is then applied to extract those prominent PCs , as such the image is compressed .Similar results for image compression have been reported in [ 11 , 142 ] by using a three - layer autoassociative network with BP learning .", "label": "", "metadata": {}, "score": "42.8497"}
{"text": "This is due to the fact that the physically meaningless features in Gram - Schmidt space can be linked back to the same number of variables of the measurement space , thus resulting in no dimensionality reduction .The GSO procedure starts with QR decomposition of the transpose of the full feature matrix , . , where .", "label": "", "metadata": {}, "score": "42.920906"}
{"text": "Other Generalizations of PCA .Simple neural models , described by differential equations , are derived in [ 104 , 105 ] to calculate the largest and smallest eigenvalues as well as their corresponding eigenvectors of any real symmetric matrix .Supervised PCA [ 106 , 107 ] is achieved by augmenting the input of the PCA with the class label of the data set .", "label": "", "metadata": {}, "score": "43.316135"}
{"text": "Based on a single - layer linear feedforward network , LDA algorithms are also given in [ 112 , 113 ] .References .G. H. Golub and C. F. van Loan , Matrix Computation , John Hopkins University Press , Baltimore , Md , USA , 2nd edition , 1989 .", "label": "", "metadata": {}, "score": "43.39872"}
{"text": "SVD - based low - rank approximations . of rational models / A - J. van der Veen , E. F. .Deprettere / \\\\ .Computing the singular values and vectors of a Hankel . operator / H. Ozbay / \\\\ .", "label": "", "metadata": {}, "score": "43.416935"}
{"text": "In [ 152 ] , a nonlinear discriminant analysis network with the MLP as the architecture and Fisher 's determinant ratio as the criterion function is obtained by combining the universal approximation properties of the MLP with the target - free nature of LDA .", "label": "", "metadata": {}, "score": "43.688393"}
{"text": "Equations over Long Time Intervals / 180 \\\\ .N. K. Nichols : Differential -- Algebraic Equations and .Control System Design / 208 \\\\ .G. W. Stewart : $ UTV$ Decompositions / 225 \\\\ .M. J. Todd : A Lower Bound on the Number of Iterations . of an Interior - Point Algorithm for Linear Programming / .", "label": "", "metadata": {}, "score": "43.818073"}
{"text": "A QRD - based least - squares algorithm for multipulse .antenna array signal processing / I. K. Proudler / 411 .\\\\ .On displacement structures for covariance matrices and .lossless functions / P. A. Regalia and F. Desbouvries / .", "label": "", "metadata": {}, "score": "43.818275"}
{"text": "These neural network methods find wide applications in pattern recognition , blind source separation , adaptive signal processing , and information compression .Two methods that are strongly associated with PCA , namely , ICA and LDA , are described here in passing .", "label": "", "metadata": {}, "score": "44.02392"}
{"text": "The techniques for both problems are very similar .The following lemma characterizes the singular values and singular vectors of M .n . \" ...Every teacher of linear algebra should be familiar with the matrix singular value decomposition ( or SVD ) .", "label": "", "metadata": {}, "score": "44.334167"}
{"text": "All the scatter matrices are of size .The minimization of the MSE criterion is equivalent to the minimization of the trace of .or maximizing the trace of . . . .The objective for LDA is to maximize the between - class measure while minimizing the within - class measure after applying a . scatter matrices into . matrices .", "label": "", "metadata": {}, "score": "45.362545"}
{"text": "Any generalized eigenvector . is a stationary point of the criterion function .G .E .V .D .The LDA problem is a typical generalized EVD problem .The three - layer LDA network [ 111 ] is obtained by concatenating two Rubner - Tavan PCA subnetworks , each being trained by the Rubner - Tavan PCA algorithm [ 13 , 49 ] .", "label": "", "metadata": {}, "score": "45.47644"}
{"text": "J. O. Berkey , P. Y. Wang / A comparative study of some .parallel bin packing algorithms / 33 - -43 \\\\ .T. E. Gerasch , P. Y. Wang , S. T. Weidman / SIMD .knapsack approximation algorithms / 44 - - 56 \\\\ . A. R. Joshi , D.-S. Chen / A vectorized dual algorithm .", "label": "", "metadata": {}, "score": "45.86902"}
{"text": "then the stochastic system ( 1 ) can be transformed into a deterministic differential equation . to infinite magnitude , with a direction parallel to that of the eigenvector of .[ . .] corresponding to the largest eigenvalue [ 11 ] .", "label": "", "metadata": {}, "score": "45.900887"}
{"text": "A performance analysis of adaptive algorithms in the . presence of calibration errors / D. R. Farrier , D. J. .Jeffries / \\\\ .Second order perturbation calculation of state space .estimation / W. W. F. Pijnappel et al .", "label": "", "metadata": {}, "score": "45.915"}
{"text": "In terms of reconstruction quality , we will compare our algorithm with CPPCA [ 11 ] by using the signal - to - noise ratio ( SNR ) .We note that Chen et al .[ 18 ] have recently provided an extensive study on the effects of linear projections on the performance of target detection and classification of hyperspectral imagery .", "label": "", "metadata": {}, "score": "46.100845"}
{"text": "We can see that the first three or four eigenvectors by CPPCA appear to be close to the true ones , while the rest are not .Hence if using more than four eigenvectors reconstructed by CPPCA , we observe a decrease in reconstruction quality or an increase in the norm error .", "label": "", "metadata": {}, "score": "46.88451"}
{"text": "The method is closely related to the one .proposed by H. A. Simon and A. Ando and developed by P. .J. Courtois .However , the method described here does .not require the determination of a completely .decomposable stochastic approximation to the transition .", "label": "", "metadata": {}, "score": "47.28043"}
{"text": "It makes use of the implicit orthogonalization procedure that is built into it through an inflation technique .Localized Principal Component Analysis .The nonlinear PCA problem can be solved by partitioning the data space into a number of disjunctive regions and then estimating the principal subspace within each partition by linear PCA .", "label": "", "metadata": {}, "score": "47.30467"}
{"text": "With the concepts of tensor , .-mode unfolding and matricization , an SVD - revision - based incremental learning method of bidirectional PCA [ 122 ] gives a close approximation to bidirectional PCA , but using less time .The uncorrelated multilinear PCA algorithm [ 123 ] is used for unsupervised subspace learning of tensorial data .", "label": "", "metadata": {}, "score": "47.68805"}
{"text": "Both the algorithms produce nearly orthonormal , but not exactly orthonormal , subspace basis or eigenvector estimates .If perfectly orthonormal eigenvector estimates are required , an orthonormalization procedure is necessary .Kalman - type RLS [ 31 ] combines the basic RLS algorithm with the GSO procedure .", "label": "", "metadata": {}, "score": "47.769897"}
{"text": "In its use of random projections , this technique can be considered to possess a certain duality with our approach to randomized SVD methods in HSI .However , CPPCA recovers coefficients of a known sparsity pattern in an unknown basis .", "label": "", "metadata": {}, "score": "47.87529"}
{"text": "in the order of descending eigenvalues .The performance is better than that of the generalized Hebbian algorithm ( GHA ) [ 8 ] .SLA has been extended in [ 25 ] so as to extract a noise robust projection .", "label": "", "metadata": {}, "score": "47.885643"}
{"text": "An invaluable reference , the .book is completely up - to - date with the latest . developments on the Lanczos algorithm , .QR - factorizations , error propagation models , parameter .estimation problems , sparse systems , and .", "label": "", "metadata": {}, "score": "47.89013"}
{"text": "My primary goals in this article are to bring the topic to the attention of a broad audience , . ... ernative uses a rank 1 modification to split an SVD problem into two problems of lower dimension , the results of which can be used to find the SVD of the original problem .", "label": "", "metadata": {}, "score": "47.893402"}
{"text": "Here we introduce a variation of Algorithm 2 that only requires one pass over a large symmetric matrix .Now we define matrix . in rSVD is sufficient and can be applied to the whole dataset .Another restriction of CPPCA lies in the fact that the Rayleigh - Ritz method requires well - separated eigenvalues [ 22 ] , which might be true for the first few largest eigenvalues , but usually not true for the smaller eigenvalues .", "label": "", "metadata": {}, "score": "47.894623"}
{"text": "Note : for the Chapter f08 functions there is generally a choice of simple and comprehensive function .The comprehensive functions return additional information such as condition and/or error estimates .As it is very unlikely that one of the functions in this section will be called on its own , the other functions required to solve a given problem are listed in the order in which they should be called . 4.3 General Purpose Functions ( Singular Value Decomposition ) .", "label": "", "metadata": {}, "score": "47.942932"}
{"text": "The much - smaller projected matrix is then factorized using a full - matrix decomposition such as SVD or PCA , after which the resulting singular vectors are backprojected to the original space .Compared to deterministic methods , probabilistic methods often offer the lower cost and more robustness in computation , while achieving high - accuracy results .", "label": "", "metadata": {}, "score": "48.22366"}
{"text": "The equivalence between LDA and such an application of CCA is proved .Two - dimensional CCA seeks linear correlation based on images directly .Motivated by locality - preserving CCA [ 138 ] and spectral clustering , a manifold learning method called local two - dimensional CCA [ 139 ] identifies the local correlation by weighting images differently according to their closeness .", "label": "", "metadata": {}, "score": "48.335396"}
{"text": "Finally , we draw some conclusions and identify some topics for future work in Section 4 .Review of Randomized Singular Value Decomposition .We start by defining terms and notations .The singular value decomposition ( SVD ) of a matrix . is a random matrix with independent and identically distributed ( i.i.d . ) entries .", "label": "", "metadata": {}, "score": "48.365852"}
{"text": "Some aspects of generalized QR factorizations / C. .C. Paige / 73 \\\\ .The multifrontal method in a parallel environment / .I. S. Duff , N. I. M. Gould , M. Lescrenier , and J. K. .Reid / 93 \\\\ .", "label": "", "metadata": {}, "score": "48.599693"}
{"text": "Thus , . in the criterion ( 27 ) can be replaced by a transformed form so as to extract the next principal singular component .Using a deflation transformation , the two sets of neurons are trained with the cross - coupled Hebbian learning rules , which are given in [ 125 , 126 ] .", "label": "", "metadata": {}, "score": "48.69558"}
{"text": "It performs better than the global models implemented by PCA model and Kramer 's nonlinear PCA and is significantly faster than Kramer 's nonlinear PCA [ 65 ] .Adaptive combination of PCA and VQ is given in [ 89 ] , where an autoassociative network is used to perform PCA and simple competitive learning is used to perform VQ .", "label": "", "metadata": {}, "score": "48.86062"}
{"text": "Two - Dimensional PCA .Two - dimensional PCA [ 5 ] is especially designed for image representation .An image covariance matrix is constructed directly using the original image matrices instead of the transformed vectors , and its eigenvectors are derived for image feature extraction .", "label": "", "metadata": {}, "score": "48.915024"}
{"text": "The number of PCs to be extracted is not required to be known in advance .A hybrid hetero / autoassociative network [ 67 ] is constructed with a set of autoassociative outputs and a set of heteroassociative outputs .Both sets of output nodes are fully connected to the same bottleneck layer .", "label": "", "metadata": {}, "score": "48.97486"}
{"text": "On the conditioning of parameter estimation .problems / James M. Varah / 187 \\\\ .Rounding errors in algebraic process - in level - index .arithmetic / F. W J. Olver / 197 \\\\ .Experiments in tearing large sparse systems / Mario .", "label": "", "metadata": {}, "score": "49.019135"}
{"text": "Oja 's rule almost always converges exponentially to the unit eigenvector associated with the largest eigenvalue of ., starting from points in an invariant set [ 18 ] .A constant learning rate for fast convergence is suggested as .Principal Component Analysis .", "label": "", "metadata": {}, "score": "49.05555"}
{"text": "An accurate product SVD algorithm / A. W. Bojanczyk et .al ./ \\\\ .The hyperbolic singular value decomposition and .applications / R. Onn et al ./ \\\\ .Adaptive SVD algorithm with application to narrow - band . signal tracking / W. Ferzali , J. G. Proakis / \\\\ .", "label": "", "metadata": {}, "score": "49.06723"}
{"text": "Continuous realization methods and their applications / .M. T. Chu / 359 \\\\ .Asymptotic behavior of orthogonal polynomials / T. Dehn ./ 361 \\\\ .CADCS and parallel computing / F. Dumortier , A. Van .Cauwenberghe and L. Boullart / 363 \\\\ .", "label": "", "metadata": {}, "score": "49.17041"}
{"text": "Applications of an Element Model for Gaussian .Elimination / S. C. Eisenstat , M. H. Schultz , and 4 . H. .Sherman / 85 \\\\ .An Optimization Problem Arising from Tearing Methods ./ Alberto Sangiovanni - Vincentelli / 97 \\\\ .", "label": "", "metadata": {}, "score": "49.180397"}
{"text": "P. Howland and H. Park , \" Generalizing discriminant analysis using the generalized singular value decomposition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .26 , no . 8 , pp .995 - 1006 , 2004 .", "label": "", "metadata": {}, "score": "49.259193"}
{"text": "In the zero - mean case , this matrix becomes the covariance matrix .In the area of image coding , PCA is known as Karhunen - Loeve transform ( KLT ) [ 4 ] , which exploits correlation between neighboring pixels or groups of pixels for data compression .", "label": "", "metadata": {}, "score": "49.406918"}
{"text": "L. Trefethen and D. Bau , Numerical Linear Algebra , Society For Industrial Mathematics , 1997 .N. Halko , P. G. Martinsson , and J. A. Tropp , \" Finding structure with randomness : probabilistic algorithms for constructing approximate matrix decompositions , \" SIAM Review , vol .", "label": "", "metadata": {}, "score": "49.52903"}
{"text": "The algorithm is fully parallel in nature and evaluates singular values to tiny relative error if necessary .It ... \" .In this paper we propose an algorithm based on Laguerre 's iteration , rank two divide - and - conquer technique and a hybrid strategy for computing singular values of bidiagonal matrices .", "label": "", "metadata": {}, "score": "49.54693"}
{"text": "The localized PCA method is commonly used in image compression [ 8 ] .An image is often first transformation coded by PCA , and then the coefficients are quantized .VQ - PCA [ 65 ] is a locally linear model that uses VQ to define the Voronoi regions for localized PCA .", "label": "", "metadata": {}, "score": "49.73897"}
{"text": "The new algorithms are built on the polar decomposition and exploit the recently developed QR - based dynamically weighted Halley algorithm of Nakatsukasa , Bai , and Gygi , which computes the polar decomposition using a cubically convergent iteration based on the building blocks of QR factorization and matrix multiplication .", "label": "", "metadata": {}, "score": "49.763947"}
{"text": "Many SVD algorithms are reviewed in [ 129 ] .Tucker decomposition [ 130 ] decomposes a three - dimensional signal directly using three - dimensional PCA , which is a multilinear generalization of SVD to multidimensional data .For video frames , this higher - order SVD decomposes the dynamic texture as a multidimensional signal ( tensor ) without unfolding the video frames on column vectors .", "label": "", "metadata": {}, "score": "49.76395"}
{"text": "C. Phillips , S. A. Zenios / Experiences with large . scale network optimization on the connection machine / . 169 - -182 \\\\ .\\\\ .III .Graphics in optimization \\\\ .\\\\ .I. Lustig / Application of interactive computer .", "label": "", "metadata": {}, "score": "49.782825"}
{"text": "computationally more efficient than frequency domain . deconvolution .A number of examples of the use of the .Tikhonov -- Phillips regularization method for source . series extraction are provided .( Also cross - referenced . as UMIACS - TR-95 - 87 ) . \" advanced numerical analysis \" , .", "label": "", "metadata": {}, "score": "49.83677"}
{"text": "However , you should read the introduction to this chapter before turning to Chapter f08 , especially if you are a new user .Chapter f12 contains functions for large sparse eigenvalue problems , although one such function is also available in this chapter .", "label": "", "metadata": {}, "score": "49.89227"}
{"text": "OOja , NOjia , and NOOjia require less computation load than the natural - gradient - based method [ 83 ] , self - stabilizing MCA [ 80 , 81 , 84 ] .By using the Rayleigh quotient as an energy function , invariant - norm MCA [ 85 ] is analytically proved to converge to the first MC of the input signals .", "label": "", "metadata": {}, "score": "49.929016"}
{"text": "As for the symmetric eigenvalue problem , if .A . and is large and sparse then it is generally preferable to use an alternative method .Chapter f12 provides functions based on Arnoldi 's method for both real and complex matrices , intended to find a subset of the eigenvalues and vectors .", "label": "", "metadata": {}, "score": "49.930573"}
{"text": "Reduction and approximation of linear computational .circuits / P. Dewilde and A.-J. van der Veen / 109 \\\\ .The look - ahead Lanczos process for large nonsymmetric . matrices and related algorithms / R. W. Freund / 137 .\\\\ .", "label": "", "metadata": {}, "score": "49.9729"}
{"text": "The close connection between the SVD and the well ... \" .Every teacher of linear algebra should be familiar with the matrix singular value decomposition ( or SVD ) .It has interesting and attractive algebraic properties , and conveys important geometrical and theoretical insights about linear transformations .", "label": "", "metadata": {}, "score": "49.9782"}
{"text": "This method of determinant finding is more rapid than recursive - descent on large matrices , and if you reuse the LU decomposition it 's essentially free .OPTIONS : . lu ( I / O ) .Provides a cache for the LU decomposition of the matrix .", "label": "", "metadata": {}, "score": "50.000286"}
{"text": "Error Analysis of the Cohn Algorithm / 27 \\\\ .Asymptotic Behavior / 35 \\\\ .II : Accelerating the orthogonal iteration for the .eigenvectors of a Hermitian matrix / 40 \\\\ .Introduction / 40 \\\\ .A Refinement Procedure for Approximate Eigenvectors / .", "label": "", "metadata": {}, "score": "50.048462"}
{"text": "By calculating the eigenvectors of the covariance matrix of the input vector , PCA linearly transforms a high - dimensional input vector into a low - dimensional one whose components are uncorrelated .PCA is directly related to singular value decomposition ( SVD ) , and the most common way to perform PCA is via SVD of the data matrix .", "label": "", "metadata": {}, "score": "50.056274"}
{"text": "NAME .PDL::MatrixOps -- Some Useful Matrix Operations .SYNOPSIS .DESCRIPTION .PDL::MatrixOps is PDL 's built - in matrix manipulation code .It contains utilities for many common matrix operations : inversion , determinant finding , eigenvalue / vector finding , singular value decomposition , etc .", "label": "", "metadata": {}, "score": "50.06548"}
{"text": "A .Chapter f08 contains functions which compute or estimate the condition numbers of eigenvalues and eigenvectors , and the f08 Chapter Introduction gives more details about the error analysis of nonsymmetric eigenproblems .The accuracy with which eigenvalues and eigenvectors can be obtained is often improved by balancing a matrix .", "label": "", "metadata": {}, "score": "50.092842"}
{"text": "Hence it is highly suitable to use this low - dimensional representation for classification .Conclusions .As HSI data sets are growing increasingly massive , compression and dimensionality reduction for analytical purposes has become more and more critical .The randomized SVD algorithms proposed in this paper enable us to compress , reconstruct , and classify massive HSI datasets in an efficient way while maintaining high accuracy in comparison to exact SVD methods .", "label": "", "metadata": {}, "score": "50.106476"}
{"text": "well conditioned , it will be accurately computed , even .though intermediate decompositions may be almost .completely inaccurate .These results are also applied . to the two - sided orthogonal decompositions , such as the .URV decomposition \" , .", "label": "", "metadata": {}, "score": "50.145554"}
{"text": "My question is about Singular Value and Eigen Decomposition for any matrices .Let take a real number x , under ... .SVD stands for Singular Value Decomposition and is said to be the popular technique to conduct feature reduction in text classification .", "label": "", "metadata": {}, "score": "50.293533"}
{"text": "If you prefer your matrices indexed in ( row , column ) order , you can try using the PDL::Matrix object , which includes an implicit exchange of the first two dimensions but should be compatible with most of these matrix operations .", "label": "", "metadata": {}, "score": "50.453674"}
{"text": "The singular value decomposition ( SVD ) is a fundamental matrix decomposition in linear algebra .It is widely applied in many modern techniques , for example , high- dimensional data visualization , dimension reduction , data mining , latent semantic analysis , and so forth .", "label": "", "metadata": {}, "score": "50.688904"}
{"text": "5 , pp .1299 - 1319 , 1998 .View at Google Scholar \u00b7 View at Scopus .T. D. Sanger , \" An optimality principle for unsupervised learning , \" in Advances in Neural Information Processing Systems , D. S. Touretzky , Ed . , vol .", "label": "", "metadata": {}, "score": "50.796154"}
{"text": "U$ , $ Q R$ , or Cholesky factorizations .We prove that . if the nodes of the network are evenly distributed .among processors and if computations are scheduled by a . round - robin or a least - recently - executed scheduling .", "label": "", "metadata": {}, "score": "50.8652"}
{"text": "large scale problems , even when the matrix $ X$ is rank . degenerate . \" generator for large - scale eigenproblems \" , . matrices with known eigensystems .A test matrix , called . an eigenmat , is generated in a factored form , in which .", "label": "", "metadata": {}, "score": "50.876373"}
{"text": "Jarre / 395 \\\\ .The lattice - ladder with generalized forgetting / J. .Kadlec / 397 \\\\ .Solving a least squares problem with boundary .constraints / L. Kaufman / 399 \\\\ .Estimating the extremal eigenvalues and condition .", "label": "", "metadata": {}, "score": "50.884346"}
{"text": "In the coupled learning rules a first - order approximation of GSO is superior to the standard deflation procedure in terms of the orthonormality error and the quality of the eigenvectors and eigenvalues generated [ 47 ] .Anti - Hebbian Rule - Based Principal Component Analysis .", "label": "", "metadata": {}, "score": "50.900692"}
{"text": "Algorithms are described for .approximating the stable part of the column space of .\\$A\\$. determination , pert , variable selection \" , .If so , . which report number is correct ? ?If so , . which report number is correct ? ?", "label": "", "metadata": {}, "score": "51.133926"}
{"text": "However , the quality of the restored image is very poor .This is because PCA assumes the Guassian statistics of the data set and a real picture usually does not satisfy this assumption .To improve the quality of the restored image , we employ nonoverlapping .", "label": "", "metadata": {}, "score": "51.193165"}
{"text": "The theoretical properties have been described , and close connections were revealed between the SV ... . ... es such as correspondence analysis and factor analysis .SVDPACKC has been developed to compute the SVD algorithm ( Berry et al . , 1993 ) . \" ...", "label": "", "metadata": {}, "score": "51.276962"}
{"text": "only additional storage requirements are arrays to . contain the factorization itself .Thus , the algorithms . are particularly suited to determining low - rank . approximations to a sparse matrix .( Also .cross - referenced as UMIACS - TR-98 - 12 ) \" , .", "label": "", "metadata": {}, "score": "51.327538"}
{"text": "The decision trees in Section 4.2 list the combinations of general purpose functions which are needed to solve many common types of problem .3.2 Computing Selected Eigenvalues and Eigenvectors .The decision trees and the function documents make a distinction between functions which compute all eigenvalues or eigenvectors , and functions which compute selected eigenvalues or eigenvectors ; the two classes of function use different algorithms .", "label": "", "metadata": {}, "score": "51.45747"}
{"text": "The most popular methods for deterministic low - rank factorizations include the singular value decomposition ( SVD ) [ 9 ] and principal component analysis ( PCA ) [ 10 ] .This last advantage is especially suited for compression with the TSVD method , since the Frobenius norm of the residual matrix is the smallest among all rank- . representations of the original matrix , and hence we should expect a much lower entropy in its distributions - making it suitable for compressive coding schemes .", "label": "", "metadata": {}, "score": "51.49516"}
{"text": "In this .article we treat the computational details of these . algorithms and describe a MATLAB implementation .\" Algorithm \" , . and G. W. ( Pete ) Stewart and Christoph Zenger \" , . projectors \" , .the QR factorization of a matrix $ X$ requires at least .", "label": "", "metadata": {}, "score": "51.51235"}
{"text": "M. T. Eismann , Hyperspectral Remote Sensing , SPIE Press , 2012 . H. F. Grahn and E. Paul Geladi , Techniques and Applications of Hyperspectral Image Analysis , Wiley , 2007 .J. M. Bioucas - Dias , A. Plaza , N. Dobigeon et al . , \" Hyperspectral unmixing overview : geometrical , statistical , and sparse regression - based approaches , \" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "51.599823"}
{"text": "NOTES .This is intended as a general - purpose linear algebra package for small - to - mid sized matrices .The algorithms may not scale well to large matrices ( hundreds by hundreds ) or to near singular matrices .", "label": "", "metadata": {}, "score": "51.693287"}
{"text": "The unit centers are updated as in neural gas , while subspace learning is based on the RRLSA algorithm [ 33 ] .Similar to localized PCA , localized ICA is used to characterize nonlinear ICA .Clustering is first used for an overall coarse nonlinear representation of the underlying data and linear ICA is then applied in each cluster so as to describe local features of the data [ 92 ] .", "label": "", "metadata": {}, "score": "51.782845"}
{"text": "\\\\ .Gaussian elimination , perturbation theory , and Markov . chains / G. W. Stewart \\\\ .Bonhoure , Yves Dallery , and William J. Stewart \\\\ .Iterative methods for queueing networks with irregular .state - spaces / Raymond H. Chan \\\\ .", "label": "", "metadata": {}, "score": "51.858917"}
{"text": "It is useful to include comments with every expression , explaining what you think each dimension means : .ACKNOWLEDGEMENTS .MatrixOps includes algorithms and pre - existing code from several origins .In particular , eigens_sym is the work of Stephen Moshier , svd uses an SVD subroutine written by Bryant Marks , and eigens uses a subset of the Small Scientific Library by Kenneth Geisshirt .", "label": "", "metadata": {}, "score": "51.908222"}
{"text": "The Gauss - Seidel recursive PCA and Jacobi recursive PCA algorithms are derived in [ 36 ] .The LMSER algorithm is derived on the MSE criterion using the gradient - descent method [ 29 ] .LMSER reduces to Oja 's SLA algorithm when .", "label": "", "metadata": {}, "score": "51.98019"}
{"text": "The actual improvement depends to a large extent on the number of distinct eigenvalues and a good estimate thereof .However , at worst the algorithm behaves like a successive bandreduction approach to tridia ... . ... point operations .In addition , the need for data movement is reduced .", "label": "", "metadata": {}, "score": "51.99396"}
{"text": "\\\\ .Integration on the Space of Connections Modulo Gauge .Transformations / Abbay Ashtekar , Donald Marolf , and .Jose Mourdo / 143 \\\\ .Quasiclassical Domains in a Quantum Universe / James B. .Hartle / 161 \\\\ .", "label": "", "metadata": {}, "score": "52.14338"}
{"text": "It is competitive with QR algorithm in serial mode in speed and advantageous in computing partial singular values .Error analysis and numerical results are presented . ...f this algorithm is to be reported in [ 9].2 The split - merge algorithm We briefly summarize the split - merge algorithm , developed by Li and Zeng [ 19 ] for the symmetric tridiagonal eigenvalue problem .", "label": "", "metadata": {}, "score": "52.171112"}
{"text": "C. Jutten and J. Herault , \" Blind separation of sources , part I : an adaptive algorithm based on neuromimetic architecture , \" Signal Processing , vol .24 , no . 1 , pp . 1 - 10 , 1991 .", "label": "", "metadata": {}, "score": "52.207767"}
{"text": "6 , pp .1318 - 1328 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Cichocki and R. Unbehauen , Networks for Optimization and Signal Processing , John Wiley & Sons , New York , NY , USA , 1992 . A. Krogh and J. A. Hertz , \" Hebbian learning of principal components , \" in Parallel Processing in Neural Systems and Computers , R. Eckmiller , G. Hartmann , and G. Hauske , Eds . , pp .", "label": "", "metadata": {}, "score": "52.20852"}
{"text": "By coding the residuals after subtracting the original matrix by its low - dimensional representation , one can compress the original data in a lossless manner , as in [ 8 ] .The success of lossless compression requires low entropy of the data distribution , and , as we shall see in the experiments section , generally the entropy of residuals for our method will be much lower than the entropy of the original data .", "label": "", "metadata": {}, "score": "52.26227"}
{"text": "LU decomposition is the answer to a lot of matrix questions , including inversion and determinant - finding , and lu_decomp is used by inv .If you pass in $ perm and $ parity , they either must be predeclared PDLs of the correct size ( $ perm is an n - vector , $ parity is a scalar ) or scalars .", "label": "", "metadata": {}, "score": "52.283875"}
{"text": "Abstract .Spectral divide and conquer algorithms solve the eigenvalue problem for all the eigenvalues and eigenvectors by recursively computing an invariant subspace for a subset of the spectrum and using it to decouple the problem into two smaller subproblems .A number of such algorithms have been ... \" .", "label": "", "metadata": {}, "score": "52.41369"}
{"text": "Some measures of non - Gaussianity are kurtosis , differential entropy , negentropy , and mutual information , which can be derived from one another .Popular ICA algorithms include the Infomax , the natural - gradient , the equivariant adaptive separation via independence ( EASI ) , and the FastICA algorithms [ 10 ] .", "label": "", "metadata": {}, "score": "52.42296"}
{"text": "4 , pp .779 - 785 , 1994 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. Castrodad , Z. Xing , J. Greer , E. Bosch , L. Carin , and G. Sapiro , \" Learning discriminative sparse models for source separation and mapping of hyperspectral imagery , \" IEEE Transactions on Geoscience and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "52.439613"}
{"text": "T. Kohonen , E. Oja , O. Simula , A. Visa , and J. Kangas , \" Engineering applications of the self - organizing map , \" Proceedings of the IEEE , vol .84 , no .10 , pp .", "label": "", "metadata": {}, "score": "52.63081"}
{"text": "[ 10 , 11 ] .Thus , Oja 's rule always converges to the principal component of .The Robbins - Monro conditions are not practical for implementation , especially for learning nonstationary data .Zufiria [ 17 ] has proposed to convert the stochastic discrete - time algorithms into their deterministic discrete - time formulations that characterize their average evolution from a conditional expectation perspective .", "label": "", "metadata": {}, "score": "52.71849"}
{"text": "M. Grauer , S. Albers , M. Frommberger / Concept and .first experiences with an object - oriented interface for . mathematical programming / 474 - -483 \\\\ .G. Mitra / Tools for modelling support and construction . of optimization applications / 484 - -496 \\\\ . A. D. Waren , M. Pechura , L. S. Lasdon / The GRG2 model .", "label": "", "metadata": {}, "score": "52.71924"}
{"text": "463 \\\\ .Efficient Solutions for a Class of Non - Markovian Models ./ / 483 \\\\ .Markovian Arrival and Service Communication Systems : .Spectral Expansions , Separability and Kronecker - Product .Forms / / 507 \\\\ .", "label": "", "metadata": {}, "score": "52.736553"}
{"text": "However , RLS method may cause instability in certain cases .All these algorithms correspond to a three - layer .PCs in the descending order of the eigenvalues , where a GSO - like orthonormalization procedure is used . is sufficiently large , this term is negligible .", "label": "", "metadata": {}, "score": "52.79012"}
{"text": "C. Chatterjee , V. P. Roychowdhury , J. Ramos , and M. D. Zoltowski , \" Self - organizing algorithms for generalized eigen - decomposition , \" IEEE Transactions on Neural Networks , vol . 8 , no .6 , pp .", "label": "", "metadata": {}, "score": "52.90167"}
{"text": "In order to increase the robustness of PCA against outliers , a robust version of the covariance matrix based on the .-estimator can be used .Several popular PCA algorithms have been generalized into robust versions by applying statistical physics approach [ 55 ] , where the defined objective function can be regarded as a soft generalization of an .", "label": "", "metadata": {}, "score": "53.009052"}
{"text": "Algorithm 2 requires us to revisit the input matrix , while this may be not feasible for large matrices .For example , in ultraspectral imaging [ 20 ] , one could have thousands of spectral bands , and PCA on such datasets would require computing the eigenvectors and eigenvalues of a covariance matrix with a huge dimension .", "label": "", "metadata": {}, "score": "53.05017"}
{"text": "Each component of . is a stationary stochastic process , and only one of the components is allowed to be Gaussian distributed .The higher - order statistics of the original inputs are required for estimating ., rather than the second - order moment or covariance of the samples as used in PCA .", "label": "", "metadata": {}, "score": "53.077614"}
{"text": "On the iterative solution of differential and . integral equations using secant updating techniques / . A. Griewank / 299 \\\\ .Methods for nonlinear constraints in optimization .calculations / M. J. D. Powell / 325 \\\\ .The influence of vector and parallel processors on .", "label": "", "metadata": {}, "score": "53.081974"}
{"text": "View at Google Scholar .M. Jankovic and H. Ogawa , \" Time - oriented hierarchical method for computation of principal components using subspace learning algorithm , \" International Journal of Neural Systems , vol .14 , no .5 , pp .", "label": "", "metadata": {}, "score": "53.142097"}
{"text": "d .i . a .g .Typically , . and . are , respectively , the full covariance matrices of zero - mean stationary random signals .In this case , iterative generalized EVD algorithms can be obtained by using two PCA steps .", "label": "", "metadata": {}, "score": "53.22585"}
{"text": "( Invited paper ) / 47 \\\\ .Confidence Regions for Projection Pursuit Density .Estimates / E. Elguero and S. Holmes - Junca / 59 \\\\ .A Robustness Property of the Projection Pursuit Methods .in Sampling from Separably Dependent Random Vectors / .", "label": "", "metadata": {}, "score": "53.22789"}
{"text": "numerical solution of Markov chains / Y. Saad / 49 \\\\ .A Parallel Block Projection Method of the Cimmino .Type for Finite Markov Chains / M. Benzi , F. Sgallari . and G. Spaletta / 65 \\\\ .Iterative Methods for Queueing Models with Batch .", "label": "", "metadata": {}, "score": "53.30835"}
{"text": "References \\\\ .Chapter 5 .Some Remarks on the Unified Treatment of .Elementary Functions by Microprogramming \\\\ .Introduction \\\\ .Chen 's Algorithms \\\\ .General Classification of Known Algorithms \\\\ .Perle 's Application of CORDIC \\\\ .", "label": "", "metadata": {}, "score": "53.308456"}
{"text": "The anti - Hebbian learning rule and its normalized version can be used for MCA [ 75 ] , but both may lead to infinite magnitudes of weights [ 76 ] .To avoid this , one can renormalize the weight vector at each iteration .", "label": "", "metadata": {}, "score": "53.460304"}
{"text": "In other words , LDA is a special case of CCA .CCA is equivalent to LDA for binary - class problems [ 134 ] , and it can be formulated as an LS problem for binary - class problems .CCA leads to a generalized EVD problem .", "label": "", "metadata": {}, "score": "53.51821"}
{"text": "In Section 2 , we introduce the Hebbian learning rule and Oja 's learning rule .Section 3 defines the PCA problem .Various PCA networks and algorithms are treated in Sections 4 - 7 .PCA algorithms based on the Hebbian rule are expanded in Section 4 .", "label": "", "metadata": {}, "score": "53.521233"}
{"text": "We will demonstrate that this fast SVD result is sufficiently accurate , and most importantly it can be derived immediately .Using this fast method , many infeasible modern techniques based on the SVD will become viable .Introduction .The singular value decomposition ( SVD ) and the principle component analysis ( PCA ) are fundamental in linear algebra and statistics .", "label": "", "metadata": {}, "score": "53.548695"}
{"text": "Linear algebra for large - scale information retrieval .applications / M. W. Berry / 347 \\\\ .Matched filter vs. least - squares approximation / L. H. .J. Bierens / 349 \\\\ .Reordering diagonal blocks in real Schur form / A. W. .", "label": "", "metadata": {}, "score": "53.55561"}
{"text": "Singular Value Decomposition .Given two sets of random vectors with zero mean , . and . , the cross - correlation matrix is defined by .where . is the .th singular value , . and . are its corresponding left and right singular vectors , and .", "label": "", "metadata": {}, "score": "53.620373"}
{"text": "EISPACK : A package for solving matrix eigenvalue .problems / J. J. Dongarra and C. B. Moler \\\\ .The MINPACK project / J. More , D. Sorensen , B. Garbow , . and K. Hillstrom \\\\ .Software for ordinary differential equations / L. F. .", "label": "", "metadata": {}, "score": "53.62229"}
{"text": "Dimensionality reduction is achieved by dropping the variables with insignificant variances .PCA is often used to select inputs , but it is not always useful , since the variance of a signal is not always related to the importance of the signal , for non - Gaussian signals .", "label": "", "metadata": {}, "score": "53.633015"}
{"text": "( Berlin ) \" , .R43 1994 \" , . methods / Dianne P. O'Leary / 1 / \\\\ .Computing the sparse singular value decomposition via .SVDPACK / Michael W. Berry / 13 \\\\ .Gaussian quadrature applied to adaptive Chebyshev . iteration / D. Calvetti , G. H. Golub , and L. Reichel / .", "label": "", "metadata": {}, "score": "53.653107"}
{"text": "69 - 84 , 1985 .View at Google Scholar \u00b7 View at Scopus .T. D. Sanger , \" Optimal unsupervised learning in a single - layer linear feedforward neural network , \" Neural Networks , vol .2 , no .", "label": "", "metadata": {}, "score": "53.6991"}
{"text": "The Chapter f02 functions call functions in Chapters f07 and f08 wherever possible to perform the computations , and there are pointers in Section 4 to the relevant decision trees in Chapter f08 .2 Background to the Problems .Here we describe the different types of problem which can be tackled by the functions in this chapter , and give a brief outline of the methods used to solve them .", "label": "", "metadata": {}, "score": "53.75988"}
{"text": "Other methods , such as Oja 's rule [ 14 ] , Yuille 's rule [ 15 ] , Linsker 's rule [ 16 ] , and Hassoun 's rule [ 11 ] , add a weight - decay term to the Hebbian rule to stabilize the algorithm .", "label": "", "metadata": {}, "score": "53.763977"}
{"text": "This is a nonlocal algorithm .During the training process , the outputs of the neurons are gradually uncorrelated and the lateral weights approach zero .The network should be trained until the lateral weights . are below a specified level .", "label": "", "metadata": {}, "score": "53.78818"}
{"text": "As for the generalized symmetric - definite eigenvalue problem , if . are large and sparse then it is generally preferable to use an alternative method .Chapter f12 provides functions based on Arnoldi 's method for both real and complex matrices , intended to find a subset of the eigenvalues and vectors .", "label": "", "metadata": {}, "score": "53.816135"}
{"text": "2122 - 2130 , 1998 .View at Google Scholar \u00b7 View at Scopus .L. Xu , E. Oja , and C. Y. Suen , \" Modified Hebbian learning for curve and surface fitting , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "53.82168"}
{"text": "Algorithms \\\\ . 8.1 Who Invented Subspace Iteration ?\\\\ . 8.2 Extracting Invariant Subspaces \\\\ . 8.3Approximating the SVD \\\\ . 8.4 Impact \\\\ .The Generalized Eigenproblem \\\\ .9.1 Perturbation Theory \\\\ .9.2 The $ QZ$ Algorithm \\\\ . 9.3 Gershgorin 's Theorem \\\\ .", "label": "", "metadata": {}, "score": "53.91001"}
{"text": "\" message passing \" , .Geijn \" , .Computations \" , . and Mark Weiser \" , . computers \" , .Advanced Computer Studies Report UMIACS-86 - 9 . \" Approximation Theorem \" , . parallel computation .We show that the extended model . is deterministic , in the sense that under different . scheduling regimes each process in the computation .", "label": "", "metadata": {}, "score": "53.92179"}
{"text": "When computing eigenvalues of sym metric matrices and singular values of general matrices in finite precision arithmetic we in general only expect to compute them with an error bound pro - portional to the product of machine precision and the norm of the matrix .", "label": "", "metadata": {}, "score": "53.957912"}
{"text": "When computing eigenvalues of sym metric matrices and singular values of general matrices in finite precision arithmetic we in general only expect to compute them with an error bound pro - portional to the product of machine precision and the norm of the matrix .", "label": "", "metadata": {}, "score": "53.957912"}
{"text": "1042 - 1047 , 1996 .View at Google Scholar \u00b7 View at Scopus . Y. Miao and Y. Hua , \" Fast subspace tracking and neural network learning by a novel information criterion , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "53.960888"}
{"text": "Placing zeroes and the Kronecker canonical form / D. L. .Boley and P. Van Dooren / 353 \\\\ .Analysis of the recursive least squares lattice .algorithm / J. R. Bunch and R. C. LeBorne / 355 \\\\ .Adaptive Chebyshev iteration based on modified moments .", "label": "", "metadata": {}, "score": "54.07923"}
{"text": "DEVEL NOTES : .For now , there is no distinction between a complex eigenvalue and an invalid eigenvalue , although the underlying code generates complex numbers .It might be useful to be able to return complex eigenvalues .eigens ignores the bad - value flag of the input piddles .", "label": "", "metadata": {}, "score": "54.131287"}
{"text": "\" has important applications in a number of areas .There . is essentially one standard updating algorithm , based .on plane rotations , which is backward stable .Three .downdating algorithms have been treated in the .literature : the LINPACK algorithm , the method of .", "label": "", "metadata": {}, "score": "54.163773"}
{"text": "Implementable Policies : Discounted Cost Case / / 283 .\\\\ .Two Bounding Schemes for the Steady - State Solution of .Markov Chains / / 307 \\\\ .The Power - Series Algorithm for Markovian Queueing .Networks / / 321 \\\\ .", "label": "", "metadata": {}, "score": "54.190414"}
{"text": "Ciampi and J. Thiffault / 267 \\\\ .Generating Rules by Means of Regression Analysis / C. .Berzuini / 273 \\\\ .A New Algorithm for Matched Case - Control Studies with .Applications to Additive Models / T. Hastie and D. .", "label": "", "metadata": {}, "score": "54.261826"}
{"text": "Assume that .Thus , when the estimated rank of the SCSVD is greater than the true rank , the accuracy of the SCSVD is pretty much the same as the SVD in the case of a small rank matrix .We would like to explore what happens if the estimated rank is smaller than the true rank .", "label": "", "metadata": {}, "score": "54.284554"}
{"text": "that of the original data without severely affecting performance of commonly used target detection and classification algorithms .The structure of the remainder of the paper is as follows .In Section 2 , we give a detailed overview of rSVD in Section 2.1 , the connections between this work and CPPCA in Section 2.2 , and the compression and reconstruction of HSI data in Section 2.3 .", "label": "", "metadata": {}, "score": "54.287643"}
{"text": "Thus , we can use the SCSVD in a huge data application to obtain a good approximated initial value .The updating algorithm of the SCMDS approach is discussed and compared with the general update approach .The performances of the SCMDS approach both in the computational time and error are worse than the general approach .", "label": "", "metadata": {}, "score": "54.29199"}
{"text": "The extracted features do not have any physical meaning .In contrast , feature selection decreases the size of the feature set or reduces the dimension of the features by discarding the raw information according to a criterion .Orthogonal decomposition is a well - known technique to eliminate ill - conditioning .", "label": "", "metadata": {}, "score": "54.307297"}
{"text": "131 - 137 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . E. Oja and J. Karhunen , \" On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix , \" Journal of Mathematical Analysis and Applications , vol .", "label": "", "metadata": {}, "score": "54.377483"}
{"text": "369 - 383 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . A. D'Aspremont , F. Bach , and L. El Ghaoui , \" Optimal solutions for sparse principal component analysis , \" Journal of Machine Learning Research , vol .", "label": "", "metadata": {}, "score": "54.37992"}
{"text": "quantity .Up to the higher - order terms that are ignored .in the expansion , these statistics tend to be more .realistic than perturbation bounds obtained in terms of . norms .The technique is applied to a number of problems .", "label": "", "metadata": {}, "score": "54.42873"}
{"text": "the method is as efficient as Gaussian elimination ; . however , because it works almost entirely with the .original blocks , it is much more efficient for sparse . matrices or matrices whose blocks can be generated on .the fly .", "label": "", "metadata": {}, "score": "54.46943"}
{"text": "In the context of BSS , the higher - order statistics are necessary only for temporally uncorrelated stationary sources .Second - order statistics - based source separation exploits temporally correlated stationary sources and the nonstationarity of the sources [ 148 ] .", "label": "", "metadata": {}, "score": "54.487915"}
{"text": "Since all the order three complexities are restricted in the small number of data entries , we can therefore speed up MDS .Even in the case of big matrix with small rank , the traditional SVD method , for example the GR - SVD , can be implemented to be linear ( because of the dependency , many components of the matrix become zero in the GR - SVD algorithm ) .", "label": "", "metadata": {}, "score": "54.496773"}
{"text": "The stochastic approximation theory [ 6 ] , introduced by Robbins and Monro in 1951 , is an important tool for analyzing stochastic discrete - time systems including the classical gradient - descent method .Given a stochastic discrete - time system of the form .", "label": "", "metadata": {}, "score": "54.497948"}
{"text": "The rSVD algorithm as considered by [ 14 ] explores approximate matrix factorizations using random projections , separating the process into two stages .In the first stage , random sampling is used to obtain a reduced matrix whose range approximates the range of .", "label": "", "metadata": {}, "score": "54.530483"}
{"text": "Let A be a symmetric matrix .Recall that the SYISDA proceeds as follows : Scaling : Compute bounds on the spectrum ( A ) of A and use these boun ... . by Christian Bischof , Steven Huss - lederman , Xiaobai Sun , Anna Tsao , Thomas Turnbull , 1995 . \" ...", "label": "", "metadata": {}, "score": "54.611504"}
{"text": "The infomax principle [ 16 ] was first proposed by Linsker to describe a neural network algorithm .The principal subspace is derived by maximizing the mutual information criterion .The NIC algorithm [ 32 ] is obtained by applying the gradient - descent method to maximize the NIC , a cost function that is very similar to the mutual information criterion [ 16 , 45 ] but integrates a soft constraint on the weight orthogonalization .", "label": "", "metadata": {}, "score": "54.65351"}
{"text": "Many adaptive PCA algorithms actually optimize ( 17 ) by using the gradient - descent method [ 29 , 30 ] and the RLS method [ 30 - 34 ] .The gradient - descent or Hebbian rule - based algorithms are highly sensitive to .", "label": "", "metadata": {}, "score": "54.685005"}
{"text": "Higher - order statistics , defined by cumulants , are needed for a good characterization of non - Gaussian data .PCA can be generalized to distributions of the exponential family [ 53 ] .When the feature space is nonlinearly related to the input space , we need to use nonlinear PCA .", "label": "", "metadata": {}, "score": "54.686607"}
{"text": "You can cache the LU decomposition in an output option variable . inv uses lu_decomp by default ; that is a numerically stable ( pivoting )LU decomposition method .OPTIONS : . s .Boolean value indicating whether to complain if the matrix is singular .", "label": "", "metadata": {}, "score": "54.692585"}
{"text": "C. Von Der Malsburg , \" Self organization of orientation sensitive cells in the striate cortex , \" Kybernetik , vol .14 , no . 2 , pp .85 - 100 , 1973 .View at Google Scholar \u00b7 View at Scopus . A. L. Yuille , D. M. Kammen , and D. S. Cohen , \" Quadrature and the development of orientation selective cortical cells by Hebb rules , \" Biological Cybernetics , vol .", "label": "", "metadata": {}, "score": "54.74022"}
{"text": "Eigenvalue Problems / / 111 \\\\ .A Bibliographical Tour of the Large , Sparse .Generalized Eigenvalue Problem / G. W. Stewart / 113 .\\\\ .How Far Should You Go With the Lanczos Process ? / W. .Kahan and B. N. Parlett / 131 \\\\ .", "label": "", "metadata": {}, "score": "54.80618"}
{"text": "The gradient ascent - based WINC algorithm can be viewed be a weighted SLA [ 23 ] with an adaptive step size , leading to a much faster convergence speed .The RLS - based WINC algorithm not only provides fast convergence and high accuracy but also has low computational complexity .", "label": "", "metadata": {}, "score": "54.813248"}
{"text": "A heuristic complex extension of GHA [ 8 ] and APEX [ 35 ] is , respectively , given in [ 98 , 99 ] .The robust complex PCA algorithms have also been derived in [ 100 ] for hierarchically extracting PCs of complex - valued signals based on a robust statistics - based loss function .", "label": "", "metadata": {}, "score": "54.824883"}
{"text": "Both methods have . drawbacks - the direct method can unnecessarily degrade .small eigenvalues , while the implicit method can .effectively loose the shift and thereby retard the . convergence .A new method which has neither drawback is . presented .", "label": "", "metadata": {}, "score": "54.916626"}
{"text": "[127 ] .The algorithm can efficiently perform SVD of an ill - posed matrix .It can be used to solve the smallest singular component of the general matrix .and is especially useful for TLS problems .Some adaptive SVD algorithms for subspace tracking of a recursively updated data matrix have been surveyed and proposed in [ 128 ] .", "label": "", "metadata": {}, "score": "54.945446"}
{"text": "The singular .value decomposition ( SVD ) has been applied to signal . processing problems since the late 1970 's , although it . has been known in various forms for over 100 years .SVD .filtering has been shown to give better results at .", "label": "", "metadata": {}, "score": "54.95891"}
{"text": "4 , pp .558 - 569 , 1993 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. Karhunen and S. Malaroiu , \" Locally linear Independent Component Analysis , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 99 ) , pp .", "label": "", "metadata": {}, "score": "54.974014"}
{"text": "Robust Bayesian Regression Analysis with HPD - Regions / .K. Felsenstein and K. Potzelberger / 349 \\\\ .\\\\ .Time Series \\\\ .Estimation of ARMA Process Parameters and Noise .Variance by Means of a Non - Linear Filtering Algorithm / .", "label": "", "metadata": {}, "score": "54.974373"}
{"text": "Dimensionality reduction techniques are generally regarded as lossy compression ; that is , the original data is not exactly represented or reconstructed by the lower - dimensional space .For lossless compression of HSI data , there have been efforts to exploit the correlation structure within HSI data plus coding the residuals after stripping off the correlated parts ; see , for example , [ 7 , 8 ] .", "label": "", "metadata": {}, "score": "54.978966"}
{"text": "An Updating Algorithm for Subspace Tracking \\\\ .5.6 From the $ URV$ to the $ ULV$ \\\\ .5.7 Impact \\\\ .Least Squares , Projections , and Pseudoinverses \\\\ .6.1 Continuity of the Pseudoinverse \\\\ .6.2 Perturbation Theory \\\\ .", "label": "", "metadata": {}, "score": "55.06216"}
{"text": "Based on the APCA network , the principal singular component of . can be efficiently extracted by using a modification to the cross - coupled Hebbian rule with global asymptotic convergence [ 127 ] .This algorithm is extended for extracting the principal singular component of a general matrix . by replacing the cross - correlation matrix by a general nonsquare matrix [ 127 ] .", "label": "", "metadata": {}, "score": "55.073956"}
{"text": "and T. F. Cox / 323 \\\\ .A Monte Carlo Evaluation of the Methods for Estimating .the Parameters of the Generalized Lambda Distribution / .M. C. Okur / 329 \\\\ .Statistical Guidance for Model Modification in .Covariance Structure Analysis / T. C. Luijben and A. .", "label": "", "metadata": {}, "score": "55.081383"}
{"text": "The TLS technique achieves a better global optimal objective than the LS technique [ 1 ] .Both the TLS and LS problems can be solved by SVD .However , the TLS technique is computationally much more expensive than the least squares ( LSs ) technique [ 74 ] .", "label": "", "metadata": {}, "score": "55.090244"}
{"text": "Two Examples of Application \\\\ .References \\\\ .Chapter 9 .Matching of Essential Boundary Conditions in .the Finite Element Method \\\\ .Introduction \\\\ .Blending Function Interpolants \\\\ .Transfinite Mappings \\\\ .Isoparametric Elements \\\\ .Direct Methods \\\\ .", "label": "", "metadata": {}, "score": "55.141994"}
{"text": "function approximation ) ; C4140 ( Linear algebra ) \" , .Park , MD , USA \" , . dominant invariant subspaces ; iterative methods ; matrix .algebra ; nested sequence ; non Hermitian matrices ; . orthonormal bases ; simultaneous iteration technique \" , . a Matrix \" , .", "label": "", "metadata": {}, "score": "55.156124"}
{"text": "Some adaptive algorithms derived from the gradient descent , conjugate direction , and Newton - Raphson methods , whose simulation results are better than that of the gradient - descent method [ 29 ] , have also been proposed in [ 44 ] .", "label": "", "metadata": {}, "score": "55.178032"}
{"text": "Rayleigh quotient \" , .MathSciNet database \" , .Applications \" , .Eigenvalue Problem \" , . root - finding scheme in the presence of error \" , .With an Application to Condition Estimators \" , .JSTOR database \" , . conditioned \" , . of Linear Equations \" , .", "label": "", "metadata": {}, "score": "55.19467"}
{"text": "319 - 329 , 1998 .View at Google Scholar \u00b7 View at Scopus . Y. Chauvin , \" Principal component analysis by gradient descent on a constrained linear Hebbian cell , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 89 ) , pp .", "label": "", "metadata": {}, "score": "55.221664"}
{"text": "Park , MD , USA \" , . transformations ; partitioned orthonormal matrix \" , .Sinha \" , .Research \" , . hierarchical optimization problems \" , .Chains \" , . probability vector of a nearly completely decomposable .", "label": "", "metadata": {}, "score": "55.346527"}
{"text": "210 - 219 , 2010 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. W. Cottrell , P. Munro , and D. Zipser , \" Learning internal representations from gray - scale images : an example of extensional programming , \" in Proceedings of the 9th Conference of tile Cognitive Science Society , pp .", "label": "", "metadata": {}, "score": "55.35076"}
{"text": "with the corresponding values of the mapping for . being .The first stage of canonical correlation is to choose . and . to maximize the correlation between the two vectors .m . a .x .c .o .", "label": "", "metadata": {}, "score": "55.353687"}
{"text": "Graph Theory and Gaussian Elimination / Robert Endre .Tarjan / 3 \\\\ .Partitioning Using PAQ / Thomas D. Howell / 23 \\\\ .Block Methods for Solving Sparse Linear Systems / .James R. Bunch / 39 \\\\ .A Recursive Analysis of Dissection Strategies / .", "label": "", "metadata": {}, "score": "55.38774"}
{"text": "\\\\ .Performability Modeling of Distributed Real - Time .Systems / J. F. Meyer ( University of Michigan , .Computing Research Lab . , USA ) / 361 \\\\ .\\\\ .Some Results on Database Locking : Solutions , .", "label": "", "metadata": {}, "score": "55.391235"}
{"text": "These methods are useful in adaptive signal processing , blind signal separation ( BSS ) , pattern recognition , and information compression .Introduction .In information processing such as pattern recognition , data compression and coding , image processing , high - resolution spectrum analysis , and adaptive beamforming , feature extraction or feature selection is necessary to deal with the large storage of raw data .", "label": "", "metadata": {}, "score": "55.411297"}
{"text": "Consult a standard textbook for a more thorough discussion , for example Golub and Van Loan ( 1996 ) or Parlett ( 1998 ) .In each sub - section , we first describe the problem in terms of real matrices .", "label": "", "metadata": {}, "score": "55.419426"}
{"text": "A desirable number of neurons can be decided during the learning process .When the environment is changing over time , a new PC can be added to compensate for the change without affecting the previously computed PCs , and the network structure can be expanded if necessary .", "label": "", "metadata": {}, "score": "55.420418"}
{"text": "programming / Philip E. Gill , Walter Murray , Michael A. .Saunders , and Margaret H. Wright / 113 \\\\ .A probabilistic round - off error propagation model .Application to the eigenvalue problem / Francoise .Chatelin and Marie Christine Brunet / 139 \\\\ .", "label": "", "metadata": {}, "score": "55.42263"}
{"text": "-dimensional subspace spanned by the first .PCs of the data .The vectors of weights leading to the hidden units form a basis set which spans the principal subspace , and data compression therefore occurs in the bottleneck layer .Many applications of the autoassociative MLP for PCA are available in the literature [ 60 - 63 ] .", "label": "", "metadata": {}, "score": "55.485394"}
{"text": "For example , for complex PCA , complex PCs can be extracted by minimizing the MSE function .where . is the .th input complex vector .In [ 96 ] , a complex - valued neural network model is developed for nonlinear complex PCA .", "label": "", "metadata": {}, "score": "55.495564"}
{"text": "Local two - dimensional CCA is formulated as solving generalized eigenvalue equations tuned by Laplacian matrices .CCArc [ 140 ] is a two - dimensional CCA that is based on representing the image as the sets of its rows and columns and implementation of CCA using these sets .", "label": "", "metadata": {}, "score": "55.496635"}
{"text": "See Chapter f04 .Computation of singular values and vectors proceeds in the following stages : .A . is reduced to an upper bidiagonal matrix .B . by an orthogonal transformation .A .U .B .V .", "label": "", "metadata": {}, "score": "55.56082"}
{"text": "12 , pp .3328 - 3333 , 2000 .View at Google Scholar \u00b7 View at Scopus .P. Foldiak , \" Adaptive network for optimal linear feature extraction , \" in Proceedings of the International Joint Conference on Neural Networks ( IJCNN ' 89 ) , pp .", "label": "", "metadata": {}, "score": "55.611313"}
{"text": "In coupled PCA / MCA algorithms [ 46 ] , both the eigenvalues and the eigenvectors are simultaneously adapted .The Newton method yields averaged systems with identical speed of convergence in all eigendirections .In order to extract multiple PCs , one has to apply an orthonormalization procedure , like GSO , or its first - order approximation as used in SLA [ 7 , 22 ] , or deflation as in GHA [ 8 ] .", "label": "", "metadata": {}, "score": "55.61225"}
{"text": "An Algorithm for the Approximation of N - Dimensional .Distributions / J. Gordesch / 285 \\\\ .Further Recursive Algorithms for Multidimensional Table .Computation / B. P. Murphy and G. Bartlett / 291 \\\\ .\\\\ .Statistical Methods \\\\ .", "label": "", "metadata": {}, "score": "55.64792"}
{"text": "ASSOM [ 93 ] is another localized PCA for unsupervised extraction of invariant local features from the input data .ASSOM associates a subspace instead of a single weight vector to each node of the SOM .The subspaces in ASSOM can be formed by applying ICA [ 94 ] .", "label": "", "metadata": {}, "score": "55.66532"}
{"text": "It has . input and .output nodes .The third layer has .nodes .Nonlinear activation functions such as the sigmoidal functions are used in the second and fourth layers , while the nodes in the bottleneck and output layers usually have linear activation functions , although they can be nonlinear .", "label": "", "metadata": {}, "score": "55.67024"}
{"text": "1269 - 1294 , 2008 .View at Google Scholar \u00b7 View at Scopus . S. Y. Kung , \" Constrained principal component analysis via an orthogonal learning network , \" in Proceedings of the IEEE International Symposium on Circuits and Systems ( ISCAS ' 90 ) , pp .", "label": "", "metadata": {}, "score": "55.70903"}
{"text": "And we call this approach to obtaining .From the SCPCA to the SCSVD .The concepts of the SVD and the PCA are very similar .Since the PCA starts from decomposing the covariance matrix of a data set , it can be considered as adjusting the center of mass of a row vector to zero .", "label": "", "metadata": {}, "score": "55.77533"}
{"text": "Through successive variance maximization , uncorrelated multilinear PCA seeks a tensor - to - vector projection that captures most of the variation in the original tensorial input while producing uncorrelated features .It is the only multilinear extension of PCA that can produce uncorrelated features in a fashion similar to that of PCA , in contrast to other multilinear PCA extensions , such as two - dimensional PCA [ 5 ] and multilinear PCA ( MPCA ) [", "label": "", "metadata": {}, "score": "55.77974"}
{"text": "Moreover , if the computation halts , the final state is . independent of scheduling .The model is applied to the .generation of precedence graphs , from which lower time . bounds may be deduced , and to the synchronization of . systolic arrays by local rather than global control .", "label": "", "metadata": {}, "score": "55.820778"}
{"text": "method is based on the observation that a certain . function $ f$ has a unique minimum if and only if the .Moreover , if there is a solution , an attempt to . minimize $ f$ will produce a sequence that will diverge . in a direction that converges to a solution of the . inequality .", "label": "", "metadata": {}, "score": "55.82293"}
{"text": "Review of Gaussian elimination \\\\ .Abbreviated computation of the weights of the unknowns .\\\\ .Computational details \\\\ .Abbreviated computation of the weight of a linear .function of the unknowns \\\\ .Updating the unknowns and their weights when a new . observation is added to the system \\\\ .", "label": "", "metadata": {}, "score": "55.86122"}
{"text": "If the mean of the matrix rows is zero , the eigenvectors derived by the SVD are equal to the eigenvectors derived by the PCA .We are looking for a method which will give a fast approach to produce the SVD result without recomputing the eigenvectors of the whole data set , when the PCA result is given .", "label": "", "metadata": {}, "score": "55.936344"}
{"text": "View at Scopus . A. Valizadeh and M. Karimi , \" Fast subspace tracking algorithm based on the constrained projection approximation , \" Eurasip Journal on Advances in Signal Processing , vol .2009 , Article ID 576972 , 16 pages , 2009 .", "label": "", "metadata": {}, "score": "55.937004"}
{"text": "/ .\\\\ .Using UNITY to implement SVD on the connection machine ./ M. Kleyn , I. Chakravarty / \\\\ .A CORDIC processor array for the SVD of a complex .matrix / J. R. Cavallaro , A. C. Elster / \\\\ .", "label": "", "metadata": {}, "score": "56.02781"}
{"text": "\\\\ .Fast matrix - vector multiplication using displacement .rank approximation via an SVD / J. M. Speiser et al ./ .\\\\ .A new use of singular value decomposition in .bioelectric imaging of the brain / D. J. Major , R. J. .", "label": "", "metadata": {}, "score": "56.03316"}
{"text": "Two - dimensional PCA [ 5 ] is designed for image feature extraction .In this paper , we give a state - of - the - art introduction to various neural network implementations and algorithms for PCA and its extensions .", "label": "", "metadata": {}, "score": "56.07923"}
{"text": "On Functional Equations for One or Two Complex .Variables Arising in the Analysis of Stochastic Models . / \\\\ . G. Fayolle ( INRIA , Rocquencourt , France ) / 55 \\\\ .\\\\ .Insensitivity for Stochastic Networks / A. Hordijk .", "label": "", "metadata": {}, "score": "56.098927"}
{"text": "Q .S . . .3 .After the eigenvalues have been found , eigenvectors may be computed , if required , in two different ways .Eigenvectors of .H . can be computed by inverse iteration , and then pre - multiplied by .", "label": "", "metadata": {}, "score": "56.106277"}
{"text": "1208 - 1211 , 1997 .View at Google Scholar \u00b7 View at Scopus .T. M. Martinetz , S. G. Berkovich , and K. J. Schulten , \" ' Neural - gas ' network for vector quantization and its application to time - series prediction , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "56.134438"}
{"text": "For example , the GR - SVD is a two - step method which performs Householder transformations to reduce the matrix to bidiagonal form then performs the QR iteration to obtain the singular values [ 3 , 4 ] .Since the off - diagonal regions are used to store the transform information , this approach is very efficient in saving the computational memory .", "label": "", "metadata": {}, "score": "56.152027"}
{"text": "For non - Gaussian input data , a nonlinear PCA permits the extraction of higher - order components and provides a sufficient representation .Kernel PCA [ 3 , 54 ] is a special , linear algebra - based nonlinear PCA , which introduces kernel functions into PCA .", "label": "", "metadata": {}, "score": "56.16604"}
{"text": "Thus , a can be . approximated by a block - diagonal matrix .The . computation of the left eigenvector with eigenvalue 1 , . which gives the steady - state probabilities of the . chain , then reduces to the computation of left . decomposition \" , .", "label": "", "metadata": {}, "score": "56.198315"}
{"text": "View at Google Scholar \u00b7 View at Scopus . F. Peper and H. Noda , \" A symmetric linear neural network that learns principal components and their variances , \" IEEE Transactions on Neural Networks , vol .7 , no .", "label": "", "metadata": {}, "score": "56.216766"}
{"text": "\\\\ .The regression model \\\\ .The best combination for estimating the first unknown .\\\\ .The weight of the estimate \\\\ .Estimates of the remaining unknowns and their weights .\\\\ .Justification of the principle of least squares \\\\ .", "label": "", "metadata": {}, "score": "56.25959"}
{"text": "solution of dense and large - scale eigenvalue problems .with an emphasis on algorithms and the theoretical .background required to understand them .Stressing depth . over breadth , Professor Stewart treats the derivation .and implementation of the more important algorithms in . detail .", "label": "", "metadata": {}, "score": "56.26403"}
{"text": "All the algorithms of this class have the same order of convergence speed and are robust to implementation error .A rapidly convergent quasi - Newton method has been applied to extract multiple MCs in [ 88 ] .The proposed algorithm has a complexity of .", "label": "", "metadata": {}, "score": "56.30728"}
{"text": "The Hebbian and Oja rules are closely related to the RRLSA algorithm by suitable selection of the learning rates [ 33 ] .RRLSA [ 33 ] is also robust to the error accumulation from the previous components , which exists in the sequential PCA algorithms like Kalman - type RLS [ 31 ] and PASTd [ 30 ] .", "label": "", "metadata": {}, "score": "56.35264"}
{"text": "will appear in the second volume of the series .The . present volume contains 65 algorithms formally . presented in pseudocode .Other volumes in the series .will treat eigensystems , iterative methods , sparse . matrices , and structured problems .", "label": "", "metadata": {}, "score": "56.37207"}
{"text": "C. S. Cruz and J. R. Dorronsoro , \" A nonlinear discriminant algorithm for feature extraction and data classification , \" IEEE Transactions on Neural Networks , vol .9 , no .6 , pp .1370 - 1376 , 1998 .", "label": "", "metadata": {}, "score": "56.375572"}
{"text": "This is because the true computational cost of the SCSVD updating method is always more than the general updating method , even though they are of the same order .Thus , when we have to update the new data to the matrix on which the SVD will be performed , the general method is recommended .", "label": "", "metadata": {}, "score": "56.377205"}
{"text": "hedging algorithm for stochastic network programming / .106 - -119 \\\\ . D. M. Nicol / Parallel solution of dynamic programming .equation using optimistic evaluation / 120 - -130 \\\\ .P. M. Pardalos , G. P. Rodgers / Parallel branch and .", "label": "", "metadata": {}, "score": "56.383125"}
{"text": "Equations / Paul Concus , Gene H. Golub , and Dianne P. .O'Leary / 309 \\\\ .Preconditioned Conjugate Gradient Iteration Applied . to Galerkin Methods for a Mildly - Nonlinear Dirichlet .Problem / Jim Douglas , Jr. and Todd Dupont / 333 \\\\ .", "label": "", "metadata": {}, "score": "56.41189"}
{"text": "The mean error of the estimate for the mean value \\\\ .Combining errors with different weights \\\\ .Overdetermined systems of equations \\\\ .The problem of obtaining the unknowns as combinations . of observations \\\\ .The principle of least squares \\\\ .", "label": "", "metadata": {}, "score": "56.47452"}
{"text": "In recent years , digital information has been proliferating and many analytic methods based on the PCA and the SVD are facing the challenge of their significant computational cost .Thus , it is crucial to develop a fast approach to compute the PCA and the SVD .", "label": "", "metadata": {}, "score": "56.47522"}
{"text": "Conclusion .We proposed the fast PCA and fast SVD methods derived from the technique of the SCMDS method .The new PCA and the SVD have the same accuracy as the traditional PCA and the SVD method when the rank of a matrix is much smaller than the matrix size .", "label": "", "metadata": {}, "score": "56.513206"}
{"text": "condition of its eigenvalue .Thus eigenmats are . suitable for testing algorithms based on Krylov . sequences , as well as others based on matrix - vector . products .This article introduces the eigenmat and .describes implementations in Fortran 77 , Fortran 95 , C , . and Matlab . \" Floating - Point Arithmetic \" , . arithmetic with adjustable precision .", "label": "", "metadata": {}, "score": "56.621483"}
{"text": "By combining Oja 's rule and the GSO procedure , Sanger proposed GHA for extracting the first . for .GHA becomes a local algorithm by rewriting the summation term in ( 15 ) in a recursive form .th neuron converges to the .", "label": "", "metadata": {}, "score": "56.64547"}
{"text": "constraints by a large number and appends them to the . top of the least squares problem , which is then solved . by standard techniques .In this paper we give a new . analysis of the method , based on the QR decomposition , . that exhibits many features of the algorithm .", "label": "", "metadata": {}, "score": "56.645718"}
{"text": "models using stochastic reward nets / Gianfranco Ciardo .Means and variances in Markov reward systems / Winfried .K. Grassman \\\\ .A direct algorithm for computing the stationary .distribution of a $ p$-cyclicMarkov chain / Daniel P. .", "label": "", "metadata": {}, "score": "56.762085"}
{"text": ".. ing parallel computers .All the above contributions are aimed at general , nonsymmetric matrices .Very recently , Demmel , Dumitriu , and Holtz [ 18 ] have shown how to exploit randomization to allow standard QR factorization to be used throughout the alg ... .", "label": "", "metadata": {}, "score": "56.810234"}
{"text": "Chapter 10 .Collocation , Difference Equations , and .Stitched Function Representations \\\\ .Abstract \\\\ .Introduction \\\\ .Collocation by Piecewise Smooth Functions \\\\ .An $ m$th Order Difference Approximation \\\\ .A Class of Intermediate Schemes \\\\ .", "label": "", "metadata": {}, "score": "56.902027"}
{"text": "617 - 624 , MIT Press , Cambridge , Mass , USA , 2002 .View at Google Scholar .B. Sch\u00f6lkopf , A. Smola , and K. R. M\u00fcller , \" Nonlinear component analysis as a kernel eigenvalue problem , \" Neural Computation , vol .", "label": "", "metadata": {}, "score": "56.936295"}
{"text": "Decomposition \" , .Colombetti and Marco Dorigo and David E. Goldberg and .Stephanie Forrest and Rick L. Riolo and Robert E. Smith .and Pier Luca Lanzi and Wolfgang Stolzmann and Stewart .W. Wilson \" , . subspaces \" , .", "label": "", "metadata": {}, "score": "56.98251"}
{"text": "It is much more complicated and may sometimes be caught more easily in local minima .PCA needs to deal with an eigenvalue problem of a . matrix , while kernel PCA needs to solve an eigenvalue problem of an . matrix .", "label": "", "metadata": {}, "score": "56.985916"}
{"text": "\\\\ .Part II : Approximation Techniques \\\\ .\\\\ .Iterative Aggregation / Disaggregation Methods / F. .Chatelin ( University of Grenoble , Lab .IMAG , France ) . . ./ 199 \\\\ .\\\\ .Error Bounds for the Analysis by Decomposition of .", "label": "", "metadata": {}, "score": "56.98659"}
{"text": "Research / Eduardo L. Ortiz / 77 \\\\ .Krylov Subspace Processes , Krylov Subspace Methods , and .Iteration Polynomials / C. C. Paige / 83 \\\\ .Do We Fully Understand the Symmetric Lanczos Algorithm .Yet ? / Beresford N. Parlett / 93 \\\\ .", "label": "", "metadata": {}, "score": "57.015633"}
{"text": "/ M. H. Wright / 315 \\\\ .Contributed Lectures / / 339 \\\\ .Direct and inverse unitary eigenproblems in signal .processing : an overview / G. S. Ammar , W. B. Gragg and .L. Reichel / 341 \\\\ .", "label": "", "metadata": {}, "score": "57.03254"}
{"text": "View at Google Scholar . E. Oja , H. Ogawa , and J. Wangviwattana , \" Principal component analysis by homogeneous neural networks , \" IEICE Transactions on Information and Systems , vol .E75-D , no . 3 , pp .", "label": "", "metadata": {}, "score": "57.03358"}
{"text": "4 , pp .863 - 872 , 2006 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . H. Lu , K. N. Plataniotis , and A. N. Venetsanopoulos , \" Uncorrelated multilinear principal component analysis for unsupervised multilinear subspace learning , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "57.058"}
{"text": "Text Summarization is very effective in relevant assessment tasks .The Multiple Document Summarizer presents a novel approach to select sentences from documents according to several heuristic features .Summaries are generated modeling the set of documents as Semantic Vector Space Model ( SVSM ) and ap ... \" .", "label": "", "metadata": {}, "score": "57.071716"}
{"text": "Computationally efficient homotopies for the H 2 model .order reduction problem / Y. Ge , L. T. Watson , E. G. .Collins , Jr. and L. D. Davis / 385 \\\\ .A fast algorithm for $ QR$ decomposition of Toeplitz . matrices / G. O. Glentis / 387 \\\\ .", "label": "", "metadata": {}, "score": "57.079655"}
{"text": "The convergence speed of a system depends on the eigenvalues of its Jacobian .In PCA algorithms , the eigenmotion depends on the principal eigenvalue of the covariance matrix , while in MCA algorithms on all the eigenvalues [ 46 ] .", "label": "", "metadata": {}, "score": "57.090923"}
{"text": "hence it is suitable for large sparse problems .\" eigenvalue problem , project method \" , .Classifications , Fortran .Computation , ANALYSIS OF ALGORITHMS AND PROBLEM .COMPLEXITY , Numerical Algorithms and Problems , .Computations on matrices .", "label": "", "metadata": {}, "score": "57.09809"}
{"text": "Kramer 's nonlinear PCA fits a lower - dimensional surface through the training data .Usually , the data compression achieved in the bottleneck layer in such networks is somewhat better than that provided by the PCA solution [ 65 ] .", "label": "", "metadata": {}, "score": "57.1039"}
{"text": "If you want to process a matrix , you must hand in the transpose of the matrix , and then transpose the output when you get it back .that is because pdls are indexed by ( col , row ) , and matrices are ( row , column ) by convention , so a 1-D pdl corresponds to a row vector , not a column vector .", "label": "", "metadata": {}, "score": "57.104706"}
{"text": "Sci . , Maryland Univ . , College .Park , MD , USA \" , .approximating eigenspaces \" , .( e - book ) \" , .S714 1998 v.2 \" , .five - volume survey of numerical linear algebra and . matrix algorithms .", "label": "", "metadata": {}, "score": "57.124023"}
{"text": "By introducing kernel into . , nonlinear discriminant analysis is obtained [ 3 , 154 ] .A multiple of the identity or the kernel matrix can be added to .or its reformulated matrix .after introducing the kernels to penalize . or . , respectively , [ 154 ] .", "label": "", "metadata": {}, "score": "57.225716"}
{"text": "It uses the architecture of Kramer 's nonlinear PCA network [ 62 ] , but with complex weights and biases .For a similar number of model parameters , the nonlinear complex PCA model captures more variance of a data set than the alternative real approach , where each complex variable is replaced by two real variables and is applied to Kramer 's nonlinear PCA .", "label": "", "metadata": {}, "score": "57.227135"}
{"text": "\\\\ .A.3 Algorithms for Large Sparse Unsymmetric $ ( A - A .I)$-Problems / 103 \\\\ .\\\\ . A. Ruhe / The two - sided Arnoldi algorithm for .nonsymmetric eigenvalue problems / 104 \\\\ . Y. Saad / Projection methods for solving large sparse .", "label": "", "metadata": {}, "score": "57.229256"}
{"text": "We have also illustrated weighted SLA , GHA , and APEX in [ 10 ] .We now provide an example to illustrate the application of PCA .Image compression is usually implemented by partitioning an image into many nonoverlapping .pixel blocks and then compressing them one by one .", "label": "", "metadata": {}, "score": "57.236298"}
{"text": "96 - 104 , 1960 .Q. Zhang and Y. W. Leung , \" A class of learning algorithms for principal component analysis and minor component analysis , \" IEEE Transactions on Neural Networks , vol .11 , no . 1 , pp .", "label": "", "metadata": {}, "score": "57.27339"}
{"text": "Gaussian Quadrature \\\\ .Error and Convergence \\\\ .Examples \\\\ .\\\\ .Lecture 24 .Numerical Differentiation and Integration .\\\\ .Formulas From Power Series \\\\ .Limitations \\\\ . systems \" , . divide - and - conquer algorithm to solve linear systems .", "label": "", "metadata": {}, "score": "57.298576"}
{"text": "The self - organizing map ( SOM ) [ 68 ] is a competitive learning - based neural network .It is capable of performing dimensionality reduction on the input .The SOM is inherently nonlinear and is viewed as a nonlinear PCA [ 69 ] .", "label": "", "metadata": {}, "score": "57.307327"}
{"text": "Section 11 extends all the methods to the complex - valued domain .Some other generalizations of PCA such as constrained PCA , generalized EVD , and two - dimensional PCA are described in Section 12 .In Section 13 , the cross - correlational PCA asymmetric network and SVD are described .", "label": "", "metadata": {}, "score": "57.32949"}
{"text": "Sparse Matrix Problems in a Finite Element Open .Ocean Model / Joel E. Hirsh and William L. Briggs / 391 .\\\\ .Calculation of Normal Modes of Oceans Using a .Lanczos Method / Alan K. Cline , Gene H. Golub , and .", "label": "", "metadata": {}, "score": "57.34768"}
{"text": "1155 - 1164 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Liu , Z. You , and L. Cao , \" A simple functional neural network for computing the largest and smallest eigenvalues and corresponding eigenvectors of a real symmetric matrix , \" Neurocomputing , vol .", "label": "", "metadata": {}, "score": "57.386932"}
{"text": "Finite Element Approximations to the One- Dimensional .Stefan Problem / J. A. Nitsche / 119 \\\\ .The Hodie Method and Its Performance for Solving .Elliptic Partial Differential Equations / Robert E. .Lynch and John R. Rice / 143 \\\\ .", "label": "", "metadata": {}, "score": "57.400993"}
{"text": "The only possible bottleneck might be the residual coding , but the recent development in floating point coding has seen throughputs reaching as much as .Numerical Experiments .Accuracy of the rSVD Estimates . singular values and the associated singular vectors of a large matrix .", "label": "", "metadata": {}, "score": "57.403084"}
{"text": "21 , pp .8390 - 8394 , 1986 .View at Google Scholar \u00b7 View at Scopus .Z. Yi , M. Ye , J. C. Lv , and K. K. Tan , \" Convergence analysis of a deterministic discrete time system of Oja 's PCA learning algorithm , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "57.413815"}
{"text": "This explains in part the . recent interest in SVD techniques for signal . processing .This book is a compilation of papers that . examine in detail the singular decomposition of a . matrix and its application to problems in signal . processing .", "label": "", "metadata": {}, "score": "57.413994"}
{"text": "However , the previously mentioned methods all require matrix multiplications for the SVD .One interesting problem is how do we compute the SVD for a matrix when the matrix size is huge and loading the whole matrix into the memory is not possible ?", "label": "", "metadata": {}, "score": "57.46682"}
{"text": "Given that PCA is an extremely useful tool in HSI data analysis , for example , for classification and target detection , it is essential to obtain a quality reconstruction of the eigenvectors .times , and , within each time , we compute the angles between eigenvectors by CPPCA and the true ones , and between eigenvectors by rSVD and the true ones .", "label": "", "metadata": {}, "score": "57.49267"}
{"text": "establishing the existence of the singular value . decomposition and developing its theory . \"Geometry ; History ; Integral equations ; Linear algebra ; .Matrix algebra ; Matrix decomposition ; Quadratic form ; .Singular value decomposition \" , . transient states \" , .", "label": "", "metadata": {}, "score": "57.507988"}
{"text": "For example , in [ 11 ] Fowler proposed an approach that exploits the use of compressive projections in sensors that integrate dimensionality reduction and signal acquisition to effectively shift the computational burden of PCA from the encoder platform to the decoder site .", "label": "", "metadata": {}, "score": "57.52918"}
{"text": "]QR decomposition can be performed by using the Householder transform or the Givens rotation [ 1 ] , which is suitable for hardware implementation .The GSO transform can be used for feature subset selection ; it inherits the compactness of the orthogonal representation and at the same time provides features that retain their original meaning .", "label": "", "metadata": {}, "score": "57.529816"}
{"text": "M. R. Hilliard , G. E. Liepins , M. Palmer , G. Rangarajan ./ The computer as a partner in algorithmic design : .Automated discovery of parameters for a multi - objective . scheduling heuristic / 321 - -331 \\\\ .", "label": "", "metadata": {}, "score": "57.53331"}
{"text": "In this paper , we present a randomized singular value decomposition ( rSVD ) method for the purposes of lossless compression , reconstruction , classification , and target detection .On a large HSI dataset we apply the rSVD method to demonstrate its efficiency and effectiveness of the proposed method .", "label": "", "metadata": {}, "score": "57.540787"}
{"text": "Conversely , if you wish to compute all the eigenvectors of a sufficiently large symmetric tridiagonal matrix , the function for selected eigenvectors may be faster .The choice depends on the properties of the matrix and on the computing environment ; if it is critical , you should perform your own timing tests .", "label": "", "metadata": {}, "score": "57.54973"}
{"text": "[ 13 ] , in which we have proved that when the data dimension is significantly smaller than the number of data entries , there is a fast linear approach for the classical MDS .The main idea of fast MDS is using statistical resampling to split data into overlapping subsets .", "label": "", "metadata": {}, "score": "57.553093"}
{"text": "This is known as the eigen - decomposition or spectral factorization of .A .Eigenvalues of a real symmetric matrix are well - conditioned , that is , they are not unduly sensitive to perturbations in the original matrix .A .", "label": "", "metadata": {}, "score": "57.56672"}
{"text": "/ \\\\ .Generalizations of the OSVD : structure , properties and .applications / B. de Moor / \\\\ .Perturbation theory for the singular value . decomposition / G. W. Stewart / \\\\ .\\\\ .Part II .Algorithms and Architectures \\\\ .", "label": "", "metadata": {}, "score": "57.64093"}
{"text": "\\\\ .12.3 [ GWS - J / 32 ] ' 'The Economical Storage of Plane .Rotations ' '\\\\ .12.4 [ GWS - J / 34 ] ' ' Perturbation Bounds for the $ QR$ .Factorization of a Matrix ' ' \\\\ .", "label": "", "metadata": {}, "score": "57.7234"}
{"text": "\\\\ .The Lanczos and Conjugate Gradient Algorithms in Finite .Precision Arithmetic / Anne Greenbaum / 49 \\\\ .The Lanczos Process and Pade Approximation / Martin H. .Gutknecht / 61 \\\\ .The Tau Method and the Numerical Solution of .", "label": "", "metadata": {}, "score": "57.7249"}
{"text": "The distribution is collectively modeled by a collection or a mixture of linear PCA models , each characterizing a partition .Most natural data sets have large eigenvalues in only a few eigendirections , while the variances in other eigendirections are so small as to be considered as noise .", "label": "", "metadata": {}, "score": "57.7319"}
{"text": "The function that gives the most reliable estimate \\\\ .The value of the most reliable estimate \\\\ .Four formulas for the weight of the value of the . estimate \\\\ .The case of more than one function \\\\ .", "label": "", "metadata": {}, "score": "57.73708"}
{"text": "The Lanczos algorithm for a pure imaginary Hermitian . matrix / Charles L. Lawson and Kajal K. Gupta / 25 \\\\ .Nearest defective matrices and the geometry of .ill - conditioning / James Demmel / 35 \\\\ .Computational aspects of the Jordan canonical form / .", "label": "", "metadata": {}, "score": "57.752094"}
{"text": "It 's also threadable , so you can find the determinants of a large collection of matrices all at once if you want .Matrices up to 3x3 are handled by direct multiplication ; larger matrices are handled by recursive descent to the 3x3 case .", "label": "", "metadata": {}, "score": "57.75969"}
{"text": "71 - -81 \\\\ .G. L. Schultz , R. R. Meyer / A flexible parallel . algorithm for block - constrained optimization problems / .82 - -91 \\\\ .R. Hunter Mladineo / Supercomputers and global .optimization / 92 - -105 \\\\ .", "label": "", "metadata": {}, "score": "57.774406"}
{"text": "for the Analysis of Generalized Linear Models / R. .Gilchrist and A. Scallan / 213 \\\\ .BLINWDR :An APL - function Library for Interactively .Solving the Problem of Robust and Bounded Influence .Regression / R. Dutter / 219 \\\\ .", "label": "", "metadata": {}, "score": "57.785706"}
{"text": "C. Van Loan / A generalized SVD analysis of some .weighting methods for equality constrained least . squares / 245 \\\\ .Wedin / On angles between subspaces / 263 \\\\ .S. Wold , H. Martens , H. Wold / The multivariate .", "label": "", "metadata": {}, "score": "57.81066"}
{"text": "Generalized CCA consists of a generalization of CCA to more than two sets of variables [ 135 ] .onto that direction , .If we do the same for . by choosing direction ., we obtain a sample of the new mapping for .", "label": "", "metadata": {}, "score": "57.840904"}
{"text": "Data Plotting Methods for Checking Multivariate .Normality and Related Ideas / T. Isogai / 87 \\\\ .Computer - aided Illustration of Regression Diagnostics / .T. Nummi .M. Nurhonen , and S. Puntanen / 93 \\\\ .Computer Guided Diagnostics / D. A. Belsley , A. .", "label": "", "metadata": {}, "score": "57.84211"}
{"text": "A PCA algorithm is obtained by adding a term to SLA [ 7 ] so as to rotate the basis vectors in the principal subspace toward the principal eigenvectors [ 24 ] .The adaptive learning algorithm ( ALA ) [ 20 ] is also a PCA algorithm that is based on SLA .", "label": "", "metadata": {}, "score": "58.000504"}
{"text": "for computing the SVD are discussed , and analysis . techniques for predicting and understanding the . performance of SVD - based algorithms are given .The . volume will provide both a stimulus for future research .in this field as well as useful reference material for .", "label": "", "metadata": {}, "score": "58.040756"}
{"text": "The Multiple Document Summarizer presents a novel approach to select sentences from documents according to several heuristic features .Summaries are generated modeling the set of documents as Semantic Vector Space Model ( SVSM ) and applying Principal Component Analysis ( PCA ) to extract topic features .", "label": "", "metadata": {}, "score": "58.043728"}
{"text": "Interior Regularity and Local Convergence \\\\ .Finite Differences \\\\ .References \\\\ .Chapter 4 .Numerical Estimates for the Error of .Gauss - Jacobi Quadrature Formulae \\\\ .Introduction \\\\ .The Numerical Estimation of the Error Term \\\\ .", "label": "", "metadata": {}, "score": "58.054504"}
{"text": "Although none of these algorithms is . backward stable , the first and third satisfy a . relational stability condition .It is shown that . relational stability extends to a sequence of updates . and downdates .In consequence , other things being .", "label": "", "metadata": {}, "score": "58.055687"}
{"text": "Many information processing problems can be transformed into some form of eigenvalue or singular value problems .Eigenvalue decomposition ( EVD ) and singular value decomposition ( SVD ) are usually used for solving these problems .In this paper , we give an introduction to various neural network implementations and algorithms for principal component analysis ( PCA ) and its various extensions .", "label": "", "metadata": {}, "score": "58.096207"}
{"text": "lu_decomp returns an LU decomposition of a square matrix , using Crout 's method with partial pivoting .It 's ported from Numerical Recipes .The partial pivoting keeps it numerically stable but means a little more overhead from threading .By convention , the diagonal of L is all 1 's .", "label": "", "metadata": {}, "score": "58.125237"}
{"text": "In this paper , we give an overview of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices and describe a sequential implementation of this algorithm .Our implementation uses a specialized routine for performing banded matrix multiplication together with successive band reduction , yielding a sequential algorithm that is competitive for large problems with the LAPACK QR code in computing all of the eigenvalues and eigenvectors of a dense symmetric matrix .", "label": "", "metadata": {}, "score": "58.1407"}
{"text": "becomes impossible , it is better to recompute the SVD result .In this case and with the condition that the true rank is much smaller than the matrix size , the SCSVD approach is recommended .Experimental Result .In this section , we show that our fast PCA and fast SVD methods work well for big - sized matrices with small ranks .", "label": "", "metadata": {}, "score": "58.163082"}
{"text": "svd .Signature : ( a(n , m ) ; [ o]u(n , m ) ; [ o , phys]z(n ) ; [ o]v(n , n ) ) .Singular value decomposition of a matrix .svd is threadable .$ r1 and $ r2 are rotation matrices that convert from the original matrix 's singular coordinates to final coordinates , and from original coordinates to singular coordinates , respectively .", "label": "", "metadata": {}, "score": "58.171776"}
{"text": "This achieves a compression ratio of 1 : 16 .The Lena picture is thus transformed into a training set of 14400 vectors , and the kid picture is transformed into a validation set of 19200 vectors .The training set is so large that the APEX algorithm converges after only two epochs .", "label": "", "metadata": {}, "score": "58.235504"}
{"text": "problems / C. W. Cryer / 601 \\\\ .Multigrid methods for elliptic equations / J. Walsh ./ 623 \\\\ .Galerkin finite element methods and their .generalizations / K. W. Morton / 645 \\\\ .Recent developments in the numerical solution of .", "label": "", "metadata": {}, "score": "58.26979"}
{"text": "PCA is often derived by optimizing some information criterion , such as the maximization of the variance of the projected data or the minimization of the reconstruction error .The objective of PCA is to extract . orthonormal directions . , in the input space that account for as much of the data 's variance as possible .", "label": "", "metadata": {}, "score": "58.279373"}
{"text": "441 - 457 , 1992 .View at Google Scholar \u00b7 View at Scopus . A. Taleb and G. Cirrincione , \" Against the convergence of the minor component analysis neurons , \" IEEE Transactions on Neural Networks , vol .10 , no . 1 , pp .", "label": "", "metadata": {}, "score": "58.310837"}
{"text": "Autoregressive Models with Latent Variables / P. H. C. .Eilers / 363 \\\\ .An Algorithm for Time Series Decomposition Using State .Space Models with Singular Transition Matrix / E. .Alvoni / 369 \\\\ .\\\\ .Statistical Data Bases and Survey Processing \\\\ .", "label": "", "metadata": {}, "score": "58.317924"}
{"text": "10 , pp .2413 - 2422 , 1996 .View at Google Scholar \u00b7 View at Scopus . Y. N. Rao , J. C. Principe , and T. F. Wong , \" Fast RLS - like algorithm for generalized eigendecomposition and its applications , \" Journal of VLSI Signal Processing Systems for Signal , Image , and Video Technology , vol .", "label": "", "metadata": {}, "score": "58.321003"}
{"text": "Functions in the NAG C Library for solving eigenvalue problems fall into two categories .Black Box Functions : these are designed to solve a standard type of problem in a single call - for example , to compute all the eigenvalues and eigenvectors of a real symmetric matrix .", "label": "", "metadata": {}, "score": "58.427856"}
{"text": "View at Scopus .B. A. Pearlmutter , G. E. Hinton , and G. -maximization : , \" An unsupervised learning procedure for discovering regularities , \" in Proceedings of the Neural Networks for Computing , J. S. Denker , Ed . , vol .", "label": "", "metadata": {}, "score": "58.46634"}
{"text": "Gauss quadratures associated with the Arnoldi process .and the Lanczos algorithm / R. W. Freund and M. .Hochbruck / 377 \\\\ .An implementation of the QMR method based on coupled .two - term recurrences / R. W. Freund and N. M. Nachtigal .", "label": "", "metadata": {}, "score": "58.482124"}
{"text": "Spectral information is important in many fields such as environmental remote sensing , monitoring chemical / oil spills , and military target discrimination .For comprehensive discussions , please see , for example , [ 1 - 3 ] .Hyperspectral image data is often represented as a matrix .", "label": "", "metadata": {}, "score": "58.50081"}
{"text": "Asymptotic Behavior / 35 \\\\ .II : Accelerating the orthogonal iteration for the .eigenvectors of a Hermitian matrix / 40 \\\\ .Introduction / 40 \\\\ .A Refinement Procedure for Approximate Eigenvectors / .43 \\\\ .Convergence of the Orthogonal Iteration / 45 \\\\ .", "label": "", "metadata": {}, "score": "58.51344"}
{"text": "-dimensional space without losing essential intrinsic information .The vector . can be represented by being projected onto the .-dimensional subspace spanned by .using the inner products ., hence achieving dimensionality reduction .PCA finds those unit directions . , along which the projections of the input vectors , known as the principal components ( PCs ) , . , have the largest variance .", "label": "", "metadata": {}, "score": "58.51629"}
{"text": "4263 - 4281 , 2011 .View at Google Scholar .X. Tang and W. Pearlman , \" Three - dimensional wavelet - based compression of hyperspectral images , \" Hyperspectral Data Compression , pp .273 - 308 , 2006 .", "label": "", "metadata": {}, "score": "58.531105"}
{"text": "Considering computation ... .I 'm wondering if anyone knows which algorithm is used in matlab 's standard svd ( ) function ? ' edit svd ' does not reveal the code , and I have search through the mathworks question / answer and exchange .", "label": "", "metadata": {}, "score": "58.548"}
{"text": "PCA is based on the Gaussian assumption for data distribution , and the optimality of PCA results from taking into account only the second - order statistics .For non - Gaussian data distributions , PCA is not able to capture complex nonlinear correlations , and nonlinear processing of the data is usually more efficient .", "label": "", "metadata": {}, "score": "58.592102"}
{"text": "Computation of eigenvalues , eigenvectors or the Schur factorization proceeds in the following stages : .A . is reduced to an upper Hessenberg matrix .H . by an orthogonal similarity transformation : .A .Q .H .Q .", "label": "", "metadata": {}, "score": "58.66525"}
{"text": "View at Scopus .T. Kohonen , Self - Organizing Maps , Springer , Berlin , Germany , 1997 . E. Oja and K. Valkealahti , \" Local independent component analysis by the self - organizing map , \" in Proceedings of the International Conference on Artificial Neural Networks , pp .", "label": "", "metadata": {}, "score": "58.756577"}
{"text": "Algorithms can take advantage of bandedness to reduce the amount of work and storage required .Functions which take advantage of packed storage or bandedness are provided for both standard symmetric eigenproblems and generalized symmetric - definite eigenproblems .3.4 Balancing for Nonsymmmetric Eigenproblems .", "label": "", "metadata": {}, "score": "58.761505"}
{"text": "We have also demonstrated the fast computation in compression and reconstruction of the proposed algorithms on a large HSI dataset in an urban setting .Overall , the rSVD provides a lower approximation error than some other recent methods and is particularly well suited for compression , reconstruction , classification , and target detection .", "label": "", "metadata": {}, "score": "58.768547"}
{"text": "their use in estimation \\\\ .Least squares characterization of the most reliable . adjustment \\\\ .Difficulties in determining weights \\\\ .A better method \\\\ .Computational details \\\\ .Existence of the estimates \\\\ .Estimating the mean error in the observations \\\\ .", "label": "", "metadata": {}, "score": "58.81157"}
{"text": "The . authors demonstrate its performance on simulated data . representing both constant and time - varying signals .They find that the URV - based ESPRIT algorithm is . effective for estimating time - varying .directions - of - arrival at considerable computational .", "label": "", "metadata": {}, "score": "58.87602"}
{"text": "Approximations to a Sparse Matrix ' '\\\\ .12.12 GWS - J / 118 ( with M. W. Berry and S. A. Pulatova ) . ''Algorithm 844 : Computing Sparse Reduced - Rank .Approximations to Sparse Matrices ' '", "label": "", "metadata": {}, "score": "58.87909"}
{"text": "data ; supercomputer architectures ; translation \" , .Approximation Theorem \" , .JSTOR database \" , . in Noninteger Bases \" , .JSTOR database ; Theory / Matrix . bib \" , . arithmetic methods ) \" , . noninteger bases ; number theory \" , . and Grace Murray Hopper and C. Strachey and Eric A. .", "label": "", "metadata": {}, "score": "58.890877"}
{"text": "Prospects / G. J. S. Ross / 155 \\\\ .Inside a Statistical Expert System : Statistical Methods .Employed in the ESTES System / P. Hietala / 163 \\\\ .An Implementation of an EDA Expert System in Prolog .Automatic Acquisition of Knowledge Base from Data .", "label": "", "metadata": {}, "score": "58.92633"}
{"text": "The Dependence of Sojourn Times in Closed Queueing .Networks / F.P. Kelly ( University of Cambridge , .Statistical Laboratory , England ) / 111 \\\\ .\\\\ .On the Existence and Uniqueness of the Homogeneous .Network / R. Marie and G. Rubino ( IRISA / INRIA , Rennes , .", "label": "", "metadata": {}, "score": "58.934002"}
{"text": "All these algorithms first extract the principal generalized eigenvector and then estimate the minor generalized eigenvectors using a deflation procedure .A recurrent network with invariant .-norm [ 117 ] computes the largest or smallest generalized eigenvalue and the corresponding eigenvector of any symmetric positive pair , which can be simply extended to compute the second largest or smallest generalized eigenvalue and the corresponding eigenvector .", "label": "", "metadata": {}, "score": "58.991184"}
{"text": "A hierarchical nonlinear PCA network composed of a number of independent subnetworks can extract ordered nonlinear PCs [66 ] .Each subnetwork extracts one PC and has at least five layers .The subnetworks can be selected as Kramer 's nonlinear PCA network and are hierarchically arranged and trained .", "label": "", "metadata": {}, "score": "59.064224"}
{"text": "units , and one of its hidden units , known as the bottleneck or representation layer , have .units , .The network is trained to reproduce its input vectors themselves .This kind of networks is called the autoassociative network .", "label": "", "metadata": {}, "score": "59.138218"}
{"text": "\\\\ .Concurrent Generalized Petri Nets / / 359 \\\\ .Exploiting Isomorphisms and Special Structures in the .Analysis of Markov Regenerative Stochastic Petri Nets / ./ 383 \\\\ .Numerical solution of large finite Markov chains by . algebraic multigrid techniques / / 403 \\\\ .", "label": "", "metadata": {}, "score": "59.1448"}
{"text": "Uniqueness \\\\ .Convergence of Chebyshev approximations \\\\ .Rates of convergence \\\\ .Part II .Linear and Cubic Splines \\\\ .Lecture 10 .Piecewise linear interpolation \\\\ .The error in $ L(f)$ \\\\ .Approximations in the $ \\infty$-norm \\\\ .", "label": "", "metadata": {}, "score": "59.163177"}
{"text": "We present a randomized singular value decomposition ( rSVD ) method for the purposes of lossless compression , reconstruction , classification , and target detection with hyperspectral ( HSI ) data .Recent work in low - rank matrix approximations obtained from random projections suggests that these approximations are well suited for randomized dimensionality reduction .", "label": "", "metadata": {}, "score": "59.210667"}
{"text": "In particular I need U and S , obtained from the diagonalization of a symmetric matrix A. I am trying to solve what I thought was a simple problem .Matlab does ... .I am trying to use scikit - cuda 's wrappers for the cuSOLVER functions , in particular I want to execute cusolverDnSgesvd to compute full - matrix single precision SVD on a matrix of real numbers .", "label": "", "metadata": {}, "score": "59.222008"}
{"text": "Dominant Invariant Subspace of a Nonsymmetric Matrix \" , .Theory / Matrix . bib \" , .Advanced Computer Studies UMIACS report TR-92 - 61 \" , . orthonormal basis for a dominant invariant subspace of .a real matrix A by the method of simultaneous iteration .", "label": "", "metadata": {}, "score": "59.232628"}
{"text": "The bigradient PSA algorithm [ 82 ] is a modification to SLA [ 7 ] and is obtained by introducing an additional bigradient term embodying the orthonormal constraints of the weights , and it can be used for MSA by reversing the sign of . . . .", "label": "", "metadata": {}, "score": "59.248978"}
{"text": "Second Order Perturbation Calculation / / 18 - - ? ?\\\\ .Parallel implementations of the SVD using implicit .CORDIC arithmetic / J - M. Delosme / 33 - -56 \\\\ .Neural networks for extracting .pure / constrained / oriented principal components / S. Y. .", "label": "", "metadata": {}, "score": "59.346992"}
{"text": "In addition , several CCA extensions , including the sparse CCA formulation based on .-norm regularization , are proposed [ 136 ] .The LS formulation of CCA and its extensions can be solved efficiently .The LS formulation is extended to orthonormalized partial least squares by establishing the equivalence relationship between CCA and orthonormalized partial least squares [ 136 ] .", "label": "", "metadata": {}, "score": "59.394684"}
{"text": "Spectral divide and conquer algorithms solve the eigenvalue problem for all the eigenvalues and eigenvectors by recursively computing an invariant subspace for a subset of the spectrum and using it to decouple the problem into two smaller subproblems .A number of such algorithms have been developed over the last forty years , often motivated by parallel computing and , most recently , with the aim of achieving minimal communication costs .", "label": "", "metadata": {}, "score": "59.46429"}
{"text": "These are then used by Algorithm 4 to reconstruct the original data losslessly , and we can see it only involves a one - pass matrix - matrix multiplication and is without iterative algorithms .Compared to CPPCA , the number of bytes used for storing the .", "label": "", "metadata": {}, "score": "59.471886"}
{"text": "Nonsymmetric Matrix \" , . orthonormal basis for a dominant invariant subspace of .a real matrix $ A$ by the method of simultaneous . iteration .Specifically , given an integer $ m$ , SRRIT . computes a matrix $ Q$ with $ m$ orthonormal columns and .", "label": "", "metadata": {}, "score": "59.509834"}
{"text": "Although the matrix size remains unchanged in the Matthew 's method , we can still adopt the analysis of this paper for updating the SVD when the matrix size is changed .Multidimensional scaling ( MDS ) is a method of representing the high - dimensional data into the low - dimensional configuration [ 10 - 12 ] .", "label": "", "metadata": {}, "score": "59.510117"}
{"text": "Additional evidence of the significance of the SVD is its central role in a number of papers in recent years in Mathematics Magazine and The American Mathematical Monthly ( for example [ 2 , 3 , 17 , 23 ] ) .", "label": "", "metadata": {}, "score": "59.555817"}
{"text": "In the second part , stable algorithms . are derived for updating an XQRY representation of .projectors , which was introduced in the first part . \" theory , updating algorithms , XQRY form \" , .Stewart \" , .", "label": "", "metadata": {}, "score": "59.568962"}
{"text": "196 - 203 , 1999 .View at Google Scholar \u00b7 View at Scopus . H. Ritter , \" Self - organizing feature maps : kohonen maps , \" in The Handbook of Brain Theory and Neural Networks , M. A. Arbib , Ed . , pp .", "label": "", "metadata": {}, "score": "59.62118"}
{"text": "354 - 379 , 2012 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .J. C. Harsanyi and C. I. Chang , \" Hyperspectral image classification and dimensionality reduction : an orthogonal subspace projection approach , \" IEEE Transactions on Geoscience and Remote Sensing , vol .", "label": "", "metadata": {}, "score": "59.636894"}
{"text": "r . measures the closeness of the samples within the clusters , and .t .r . measures the separation between the clusters , where .t .r . denotes the trace operator .An optimal . should preserve the given cluster structure , and simultaneously maximize .", "label": "", "metadata": {}, "score": "59.646088"}
{"text": "Shampine / 177 \\\\ .Perturbation Theory for the Generalized Eigenvalue .Problem / G. W. Stewart / 193 \\\\ .Some Remarks on Good , Simple , and Optimal Quadrature .Formulas / H. F. Weinberger / 207 \\\\ .Linear Differential Equations and Kronecker 's Canonical .", "label": "", "metadata": {}, "score": "59.648212"}
{"text": "A simulation example of PCA is given in Section 15 .A brief summary is given in Section 16 , and independent component analysis ( ICA ) and linear discriminant analysis ( LDA ) are also mentioned in passing in this section .", "label": "", "metadata": {}, "score": "59.675617"}
{"text": "M. Loeve , Probability Theory , Van Nostrand , New York , NY , USA , 3rd edition , 1963 .J. Yang , D. Zhang , A. F. Frangi , and J. Y. Yang , \" Two - dimensional PCA : a new approach to appearance - based face representation and recognition , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "59.680428"}
{"text": "J. H. Wilkinson ) , ' 'An Estimate for the Condition .Number of a Matrix ' ' \\\\ .12.6 [ GWS - J / 49 ] ' ' Rank Degeneracy ' '\\\\ .12.7 [ GWS - J / 78 ] ' '", "label": "", "metadata": {}, "score": "59.705605"}
{"text": "If called in scalar context eigens hands back only the eigenvalues .This is somewhat wasteful , as it calculates the eigenvectors anyway .The eigenvectors are returned in COLUMNS of the returned PDL ( ie the the 0 dimension ) .", "label": "", "metadata": {}, "score": "59.730846"}
{"text": "control over the condition of the eigenvalues and .eigenvectors .An eigenmat $ A$ of order $ n$ requires . only $ O(n ) $ storage for its representation .Auxiliary .programs permit the computation of $ ( A - s I ) b $ , $ .", "label": "", "metadata": {}, "score": "59.74658"}
{"text": "Application of Sparse Matrix Techniques to Reservoir .Simulation / P. T. Woo , S. C. Eisenstat , M. H. Schultz , . and A. H. Sherman / 427 \\\\ .On the Origins and Numerical Solution of Some Sparse .Welsch \" , .", "label": "", "metadata": {}, "score": "59.74804"}
{"text": "Decompositions \\\\ .14Papers on Least Squares , Projections , and .Generalized Inverses \\\\ .15 Papers on the Eigenproblem and Invariant Subspaces : .Perturbation Theory \\\\ .16Papers on the SVD , Eigenproblem and Invariant .Subspaces : Algorithms \\\\ .", "label": "", "metadata": {}, "score": "59.778816"}
{"text": ", one can initialize the algorithm with weights and biases of small magnitudes and use a weight penalty in the objective function .A complex - valued BP or quasi - Newton algorithm can be used for training .There are many other complex PCA algorithms .", "label": "", "metadata": {}, "score": "59.78105"}
{"text": "Before introducing our updating method , we review the general updating methods first . to the largest group of the subgroups .Because the dimension of this updated group will increase , we have to make some modifications in the combined approach .", "label": "", "metadata": {}, "score": "59.782173"}
{"text": "( Linear algebra ) ; C4110 ( Error analysis in numerical .methods ) ; C4140 ( Linear algebra ) \" , .Sci . , Maryland Univ . , College Park , .MD , USA \" , .data ; error analysis ; exponential windowing ; forgetting .", "label": "", "metadata": {}, "score": "59.807922"}
{"text": "If called in scalar context it hands back only the eigenvalues .Ultimately , it should switch to a faster algorithm in this case ( as discarding the eigenvectors is wasteful ) .The algorithm used is due to J. vonNeumann , which was a rediscovery of Jacobi 's Method .", "label": "", "metadata": {}, "score": "59.819695"}
{"text": "G. Starke / 421 \\\\ .Aspects of implementing a ' C ' matrix library / D. E. .Stewart / 423 \\\\ .Intermediate fill - in in sparse QR decomposition / M. .Shifting strategies for the parallel QR algorithm / D. .", "label": "", "metadata": {}, "score": "59.87979"}
{"text": "Size of covariance matrices in CCArc is equal to .m . a .x .Small - sample - size problem in CCArc does not occur , because we actually use . images of size . and . images of size .", "label": "", "metadata": {}, "score": "59.897038"}
{"text": "We describe a divide - and - conquer tridiagonalization approach for matrices with repeated eigenvalues .Our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with bandwidth b and k distinct eigenvalues must be block diagonal with diagonal blocks ... \" .", "label": "", "metadata": {}, "score": "59.91944"}
{"text": "This algorithm only uses easily parallelizable linear algebra building blocks : matrix multiplication and QR decomposition , but not matrix inversion .Similar parallel algorithms for the nonsymmetric eigenproblem use the matrix sign function , which requires matrix inversion and is faster but can be less stable than the new algorithm .", "label": "", "metadata": {}, "score": "59.92463"}
{"text": "PCA usually obtains the best fixed - rank approximation to the data in the LS sense .On the other hand , constrained PCA allows specifying metric matrices that modulate the effects of rows and columns of a data matrix .This actually is the weighted LS estimation .", "label": "", "metadata": {}, "score": "59.94152"}
{"text": "On Cyclic Reduction Applied to a Class of .Toeplitz - like Matrices Arising in Queueing Problems / . D. Bini and B. Meini / 21 \\\\ .A Markov Modulated , Nearly Completely Decomposable .M / M/1 Queue / G. Latouche and P. J. Schweitzer / 39 \\\\ .", "label": "", "metadata": {}, "score": "59.952126"}
{"text": "Canonical Correlation Analysis .CCA [ 132 ] , proposed by Hotelling in 1936 , is a multivariate statistical technique .It makes use of two views of the same set of objects and projects them onto a lower - dimensional space in which they are maximally correlated .", "label": "", "metadata": {}, "score": "59.9575"}
{"text": "Then we use the overlapping information to combine each configuration of subsets to recover the configuration of the whole data .Hence , we named this fast MDS method by the split - and - combine MDS ( SCMDS ) ., which is the computation time of the fast MDS method proposed by Morrison et al . , 2003 [ 14 ] .", "label": "", "metadata": {}, "score": "59.96672"}
{"text": "On the other hand , if you want to invert rapidly a few hundred thousand small matrices and do n't mind missing one or two , it could be the ticket .It can be up to 60 % faster at the expense of possible failure of the decomposition for some of the input matrices .", "label": "", "metadata": {}, "score": "59.96827"}
{"text": "Some Algorithmic Insights \\\\ . 4.3The Triangular Matrices of Gaussian Elimination and .Related Decompositions \\\\ . 4.4 Solving Sylvester Equations \\\\ .4.5 Perturbation Bounds for Matrix Factorizations \\\\ . 4.6 Rank Degeneracy \\\\ . 4.7 Pivoted $ QR$ as an Alternative to SVD \\\\ . 4.8", "label": "", "metadata": {}, "score": "59.987114"}
{"text": "Row - Generation Methods for Feasibility and Optimization .Problems Involving Sparse Matrices and Their .Applications \\\\ .Yair Censor and Gabor T. Herman / 197 \\\\ .Lanczos and the Computation in Specified Intervals of .the Spectrum of Large , Sparse Real Symmetric Matrices .", "label": "", "metadata": {}, "score": "60.20421"}
{"text": "The network produces better results than by using the two algorithms successively .An online localized PCA algorithm [ 90 ] is developed by extending the neural gas method [ 91 ] .Instead of the Euclidean distance measure , a combination of a normalized Mahalanobis distance and the squared reconstruction error guides the competition between the units .", "label": "", "metadata": {}, "score": "60.242844"}
{"text": "We use the benchmark Lina picture of .pixels as the training set , and a kid picture of . , which has the similar statistics , is then used for generalization .We first use . blocks .For each square , only the first PC is significant , and all the other PCs can be ignored in terms of the quality of the restored picture .", "label": "", "metadata": {}, "score": "60.24592"}
{"text": "The three - layer ( .linear autoassociative network can also be used as an ICA network , as long as the outputs of the hidden layer are independent .A well - known two - phase approach to ICA is to preprocess the data by PCA and then to estimate the necessary rotation matrix .", "label": "", "metadata": {}, "score": "60.268173"}
{"text": "m . a .x .A Simulation Example .The concept of subspace is involved in many information processing problems .This requires EVD of the autocorrelation matrix of a data set or SVD of the cross - correlation matrix of two data sets .", "label": "", "metadata": {}, "score": "60.281174"}
{"text": "LEAP can satisfactorily extract PCs even for ill - conditioned autocorrelation matrices [ 26 ] . , ordered arbitrarily .The algorithm induces the norms of the weight vectors towards the corresponding eigenvalues , that is , .The algorithm breaks the symmetry in its learning process by the difference in the norms of the weight vectors while keeping the symmetry in its structure .", "label": "", "metadata": {}, "score": "60.311317"}
{"text": "iterative technique .problem ; Poisson equation ; successive overrelaxation .( SOR ) ; thin domain \" , . polynomial 1 \\\\ .Introduction / 1 \\\\ .Lehmer 's Method / 2 \\\\ .Rounding Error / 11 \\\\ .", "label": "", "metadata": {}, "score": "60.325233"}
{"text": "This is known as sparse PCA [ 108 ] .Constrained PCA , generalized EVD , and the two - dimensional PCA are three important generalizations to PCA .Constrained Principal Component Analysis .When certain subspaces are less preferred than others , this yields constrained PCA [ 109 ] .", "label": "", "metadata": {}, "score": "60.32763"}
{"text": "Previous algorithms can be unstable and compute the singular values and the singular vectors of A 0 in O \\Gamma ( m + n ) min 2 ( m;n ) \\Delta floating point operations .Introduction .The columns of U and V are the left singular vectors and the right singular vectors of A , respectively ; the diagonal entries of\\Omega are the singular values of A .... . ...", "label": "", "metadata": {}, "score": "60.381416"}
{"text": "\\\\ .Problems of Reducibility in Structured Markov .Chains of M / G / l Type and Related Queueing Models in .Communication Engineering / M. F. Neuts ( University of .Delaware , Dept . of Mathematics , USA ) / 139 \\\\ .", "label": "", "metadata": {}, "score": "60.39168"}
{"text": "Envoi \\\\ .\\\\ .Part III .Linear Equations \\\\ .\\\\ .\\\\ .Lecture 9 .Matrices , Vectors , and Scalars \\\\ .Operations with Matrices \\\\ .Rank - One Matrices \\\\ .Partitioned Matrices \\\\ .", "label": "", "metadata": {}, "score": "60.397606"}
{"text": "A Fast , Stable Implementation of the Simplex Method .Using Bartels - Golub Updating / Michael A. Saunders / .213 \\\\ .Using the Steepest - edge Simplex Algorithm to Solve .Sparse Linear Programs / D. Goldfarb / 227 \\\\ .", "label": "", "metadata": {}, "score": "60.40663"}
{"text": "As we know , If conditional number is $ \\infty$ then matrix is non singular .Now the problem reduces to finding condition number .As far as I know , finding conditional number is easier than finding determinants .EDIT : .", "label": "", "metadata": {}, "score": "60.514988"}
{"text": "Multiple Zeros \\\\ .Ending with a Proposition \\\\ .\\\\ .Lecture 4 .The Secant Method \\\\ .Convergence \\\\ .Rate of Convergence \\\\ .Multipoint Methods \\\\ .Muller 's Method \\\\ .The Linear - Fractional Method \\\\ .", "label": "", "metadata": {}, "score": "60.620285"}
{"text": "1 Introduction Computation of eigenvalues and eigenvectors is an essential kernel in many applications , and several promising parallel algorithms have been investigated [ 8 , 11 , 7].The work presented in this paper is part of the PRISM ( Parallel Research on Invariant Subspace Methods )", "label": "", "metadata": {}, "score": "60.62211"}
{"text": "Least Mean Squared Error - Based Principal Component Analysis .Existing PCA algorithms including the Hebbian rule - based algorithms can be derived by optimizing an objective function using the gradient - descent method .The least mean squared error-( LMSE- ) based methods are derived from the modified MSE function .", "label": "", "metadata": {}, "score": "60.662376"}
{"text": "[57 ] .On the contrary , nonlinearities growing faster than linearly cause stability problems easily and are not recommended .These extensions are also introduced in [ 29 , 58 , 59 ] .The multilayer perceptron ( MLP ) can be used to perform nonlinear dimensionality reduction and hence nonlinear PCA .", "label": "", "metadata": {}, "score": "60.668095"}
{"text": "Computing the singular value decomposition on a . fat - tree architecture / T. J. Lee , F. T. Luk and D. L. .Boley / 231 \\\\ .A new matrix decomposition for signal processing / F. .T. Luk and S. Qiao / 241 \\\\ .", "label": "", "metadata": {}, "score": "60.68938"}
{"text": "\\\\ .Supplementum / Supplement \\\\ .\\\\ .Problems having constraints on the observations : . reduction to an ordinary least squares problem \\\\ .Functions of the observations , their mean errors \\\\ .Estimating a function of observations that are subject . to constraints \\\\ .", "label": "", "metadata": {}, "score": "60.716686"}
{"text": "We compare this . approach to wave front array processors and systolic .arrays , and note its advantages in handling mis - sized . problems , in evaluating variations of algorithms or .architectures , in moving algorithms from system to . system , and in debugging parallel algorithms on .", "label": "", "metadata": {}, "score": "60.753544"}
{"text": "Let A 2 R m\\Thetan be a matrix with known singular values and singular vectors , and let A 0 be the matrix obtained by appending a row to A. We present stable and fast algorithms for computing the singular values and the singular vectors of A 0 in O \\Gamma ( m + n ) min(m;n ) log 2 2 ffl \\De ... \" .", "label": "", "metadata": {}, "score": "60.83754"}
{"text": "The NIC algorithm is a PSA method .It can extract the principal eigenvectors when the deflation technique is incorporated .The NIC algorithm converges much faster than SLA [ 22 ] and LMSER [ 29 ] and is able to globally converge to the PSA solution from almost any weight initialization .", "label": "", "metadata": {}, "score": "60.83927"}
{"text": "At the same time , the SVD has fundamental importance in several different applications of linear algebra .Strang was aware of these facts when he introduced the SVD in his now classical text [ 22 , page 142 ] , observing ... it is not nearly as famous as it should be .", "label": "", "metadata": {}, "score": "60.841393"}
{"text": "There are at most .nonzero generalized eigenvalues and thus an upper bound on . is . ; at least . samples are needed to guarantee . to be nonsingular .This requirement on the number of samples may be severe for some problems like image processing .", "label": "", "metadata": {}, "score": "60.84244"}
{"text": "In particular , matrix multiplication and other matrix ops need nx1 PDLs as row vectors and 1xn PDLs as column vectors .In most cases you must explicitly include the trailing ' x1 ' dimension in order to get the expected results when you thread over multiple row vectors .", "label": "", "metadata": {}, "score": "60.84268"}
{"text": "18 Papers on Krylov Subspace Methods for the .I am working on a project \" Text summarization using LSA \" .I am using java language .I have performed upto SVD .I got 3 matrices i.e term by concept , diagonal matrix and concept by sentence matrix .", "label": "", "metadata": {}, "score": "60.84407"}
{"text": "However , such speedup is not necessarily achieved for .other scheduling algorithms or if the computation for .the nodes is inappropriately split across processors , .and we give examples of these phenomena .Lower bounds .on execution time for the algorithm are established for .", "label": "", "metadata": {}, "score": "60.867867"}
{"text": "The APEX algorithm has been applied to recursively solve the constrained PCA problem [ 35 ] .The constrained PAST algorithm [ 110 ] is for tracking the signal subspace recursively .Based on an interpretation of the signal subspace as the solution of a constrained minimization task , it guarantees the orthonormality of the estimated signal subspace basis at each update , hence avoiding orthonormalization process .", "label": "", "metadata": {}, "score": "60.957825"}
{"text": ", the computational complexity becomes almost the same as the original PCA and SVD .Our approach has no advantage in the latter case .SVD for Continuously Growing Data .In this section , we look for the solution when the data is updated constantly and we need to compute the SVD continuously .", "label": "", "metadata": {}, "score": "60.971725"}
{"text": "22 , pp .4417 - 4435 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . Y. Chen , N. M. Nasrabadi , and T. D. Tran , \" Effects of linear projections on the performance of target detection and classification in hyperspectral imagery , \" Journal of Applied Remote Sensing , vol .", "label": "", "metadata": {}, "score": "61.002693"}
{"text": "The inverse of the nonlinear mixing model can be modeled by using the three - layer MLP , the RBFN , the SOM , or the kernel - based nonlinear BSS method [ 10 ] .Nonnegativity is a natural condition for many real - world applications , for example , in the analysis of images , text , or air quality .", "label": "", "metadata": {}, "score": "61.00731"}
{"text": "Integration \\\\ .Least squares approximation \\\\ .Implementations issues \\\\ .Lecture 11 .Cubic splines \\\\ .Derivation of the cubic spline \\\\ .End conditions \\\\ .Convergence \\\\ .Locality \\\\ .Part III .Eigensystems \\\\ .", "label": "", "metadata": {}, "score": "61.052124"}
{"text": "Specific Software / J.-L. Brillet / 417 \\\\ .\\\\ .Late Arrivals \\\\ .Three Examples of Computer - Intensive Statistical .Inference / B. Efron / 423 \\\\ .New Computer Procedures for Generating Optimal Mixture .Designs on Finite Design Spaces / H. A. Yonchev / 433 .", "label": "", "metadata": {}, "score": "61.094982"}
{"text": "A .B . quadratic eigenvalue problems ( finding eigenvalues and eigenvectors of the quadratic .A .B .C .The majority of functions for these problems can be found in Chapter f08 which contains software derived from LAPACK ( see Anderson et al .", "label": "", "metadata": {}, "score": "61.100067"}
{"text": "In [ 86 ] , an MCA algorithm for extracting multiple MCs is described by using the idea of sequential addition ; a conversion method between MCA and PCA is also discussed .Based on a generalized differential equation for the generalized eigenvalue problem , a class of algorithms can be obtained for extracting the first PC or MC by selecting different parameters and functions [ 87 ] .", "label": "", "metadata": {}, "score": "61.106182"}
{"text": "The $ QR$ decomposition and least squares \\\\ .Rank - reducing decompositions \\\\ .References \\\\ . sequences , including a categorization of the rates of . convergence and a method for determining the rate from . an iteration function .", "label": "", "metadata": {}, "score": "61.155968"}
{"text": "The purpose of the second simulation experiment is to observe the approximation performance of applying the SCPCA to a big full rank matrix .We generate a random matrix with a fixed number of columns and rows , say 1000 .The square matrix is created by the form , . is a small coefficient for adjusting the influence to the previous matrix .", "label": "", "metadata": {}, "score": "61.156235"}
{"text": "EXAMPLE .The computing literature has loads of examples of how to use SVD .Here 's a trivial example ( used in PDL::Transform::map ) of how to make a matrix less , er , singular , without changing the orientation of the ellipsoid of transformation : .", "label": "", "metadata": {}, "score": "61.161934"}
{"text": "Comput .Studies , Maryland Univ . , .College Park , MD , USA \" , . eigenvalues and eigenfunctions ; graded matrices ; graded . structure ; la ; matrix algebra ; multiple eigenvalue ; . pert ; perturbations \" , . title ! \"", "label": "", "metadata": {}, "score": "61.1708"}
{"text": "1213 - 1218 , San Francisco , Calif , USA , 1993 .J. A. Catalan , J. S. Jin , and T. Gedeon , \" Reducing the dimensions of texture features for image retrieval using multilayer neural networks , \" Pattern Analysis and Applications , vol .", "label": "", "metadata": {}, "score": "61.20992"}
{"text": "Approximate analysis of a discrete - time queueing model . of the shared buffer ATM switch / S. Hong , H. G. .Perros , and H. Yamashita \\\\ .Algorithms for infinite Markov chains with repeating .columns / Guy Latouche \\\\ .", "label": "", "metadata": {}, "score": "61.218277"}
{"text": "Morin / 245 \\\\ .\\\\ .Algorithms \\\\ .Efficient Sampling Algorithms and Balanced Samples / J. .C. Deville , J. M. Grosbas , and N. Roth ( Invited paper ) ./ 255 \\\\ .Recursive Partition in Biostatistics : Stability of .", "label": "", "metadata": {}, "score": "61.220127"}
{"text": "of ICA is not orthogonal , while in PCA the components of the weights are represented on an orthonormal basis .ICA provides in many cases a more meaningful representation of the data than PCA .ICA can be realized by adding nonlinearity to linear PCA networks such that they are able to improve the independence of their outputs .", "label": "", "metadata": {}, "score": "61.238796"}
{"text": "th neuron in Kalman - type RLSA takes a special value .In the one - unit case , both PAST and PASTd reduce to Oja 's learning rule [ 14 ] .Both PAST and PASTd provide much more robust estimates than eigenvalue decomposition ( EVD ) and converge much faster than SLA [ 14 ] .", "label": "", "metadata": {}, "score": "61.262733"}
{"text": "Diagonal PCA [ 119 ] improves two - dimensional PCA by defining the image scatter matrix as the covariances between the variations of the rows and those of the columns of the images and is shown to be more accurate than PCA and two - dimensional PCA .", "label": "", "metadata": {}, "score": "61.30863"}
{"text": "Unitarily invariant norms \\\\ .Metrics on subspaces of C'n ' \\\\ .III .Linear systems and least squares problems \\\\ .The pseudo - inverse and least squares \\\\ .Inverses and linear systems \\\\ .The pseudo - inverse \\\\ .", "label": "", "metadata": {}, "score": "61.336624"}
{"text": "\\\\ .A.1 Canonical Reductions --- Theory and Algorithms \\\\ .\\\\ .J. Demmel / The condition number of equivalence . transformations that block diagonalize matrix pencils / . 2 \\\\ .V. N. Kublanovskaya / An approach to solving the . spectral problem of $ A - A B$ / 17 \\\\ . form of regular $ ( A - A B)$-pencils / 30 \\\\ .", "label": "", "metadata": {}, "score": "61.35433"}
{"text": "The Schur form and Schur vectors depend on the ordering of the eigenvalues and this is another possible cause of non - uniqueness when they are computed .However , it must be stressed again that variations in the results from this cause should not be significant .", "label": "", "metadata": {}, "score": "61.383884"}
{"text": "More details on the accuracy of computed eigenvalues and eigenvectors are given in the function documents , and in the f08 Chapter Introduction .For dense or band matrices , the computation of eigenvalues and eigenvectors proceeds in the following stages : .", "label": "", "metadata": {}, "score": "61.392094"}
{"text": "J. de Frutos \\ & J. M. Sanz - Serna : Erring and being .Conservative / 75 \\\\ .Saunders : Solving Reduced KKT Systems in Barrier .Methods for Linear Programming / 89 \\\\ .G. H. Golub \\ & Gerard Meurant : Matrices , Moments , and . and Quadrature / 105 \\\\ .", "label": "", "metadata": {}, "score": "61.418076"}
{"text": "In addition to the feedforward weights .Figure 2 : Architecture of the PCA network with hierarchical lateral connections .The lateral weight matrix .The Rubner - Tavan PCA algorithm is based on the PCA network with hierarchical lateral connection topology [ 13 , 49 ] .", "label": "", "metadata": {}, "score": "61.429245"}
{"text": "317 - 320 , San Francisco , Calif , USA , 1992 . A. Cichocki , R. W. Swiniarski , and R. E. Bogner , \" Hierarchical neural network for robust PCA computation of complex valued signals , \" in Proceedings of the World Congress on Neural Networks , pp .", "label": "", "metadata": {}, "score": "61.458195"}
{"text": "Other Optimization - Based Principal Component Analysis .PCA can be derived by any optimization method based on a proper objective function .This leads to many other algorithms , including gradient - descent based algorithms [ 15 , 16 , 39 , 40 ] , the conjugate gradient ( CG ) method [ 41 ] , and the quasi - Newton method [ 42 , 43 ] .", "label": "", "metadata": {}, "score": "61.476833"}
{"text": "A .P .C .A . , as .The algorithm extracts the first .principal singular values in the descending order and their corresponding left and right singular vectors .Like APEX , the APCA algorithm incrementally adds nodes without retraining the learned nodes .", "label": "", "metadata": {}, "score": "61.493443"}
{"text": "11 - 19 , Morgan Kaufmann , San Mateo , Calif , USA , 1989 .View at Google Scholar .P. Baldi and K. Hornik , \" Neural networks and principal component analysis : learning from examples without local minima , \" Neural Networks , vol .", "label": "", "metadata": {}, "score": "61.501"}
{"text": "In HSI applications , the datasets can easily break into the million - pixel or even giga - pixel level , which renders this operation impossible on typical desktop computers .One solution is to apply probabilistic methods which give closely approximated singular vectors and singular values , while the complexity is at a much lower level .", "label": "", "metadata": {}, "score": "61.50895"}
{"text": "Part III .Analysis of SVD - Based Algorithms \\\\ .\\\\ .Analytical performance prediction of subspace - based . algorithms for DOA estimation / Fu Li , R. J. Vaccaro / .\\\\ .Spatial smoothing and MUSIC : Further results / B. D. .", "label": "", "metadata": {}, "score": "61.525337"}
{"text": ", whereas , for PCA , the size is .This results in considerable computational advantage in two - dimensional PCA .Two - dimensional PCA evaluates the covariance matrix more accurately over PCA .When used for face recognition , two - dimensional PCA results in a better recognition accuracy .", "label": "", "metadata": {}, "score": "61.527546"}
{"text": "Cholesky Factors ' '\\\\ .12.10 [ GWS - J / 94 ] ' 'The Triangular Matrices of .Gaussian Elimination and Related Decompositions ' '\\\\ .12.11 [ GWS - J / 103 ] ' ' Four Algorithms for the the [ sic ] .", "label": "", "metadata": {}, "score": "61.537563"}
{"text": "2.1.1 Standard symmetric eigenvalue problems .A . is real symmetric , the eigenvalue problem has many desirable features , and it is advisable to take advantage of symmetry whenever possible .are all real , and the eigenvectors can be chosen to be mutually orthogonal .", "label": "", "metadata": {}, "score": "61.562702"}
{"text": "[ . .] , as .The weighted SLA [ 22 ] performs well for extracting less dominant components .Other Hebbian Rule - Based Algorithms .The LEAP algorithm [ 26 ] is a local PCA algorithm for extracting all the .", "label": "", "metadata": {}, "score": "61.56539"}
{"text": "pointers to other methods along with historical .comments . \"Eigensystems \\\\ .The $ QR$ algorithm \\\\ .The symmetric eigenvalue problem \\\\ .Eigenspaces and their approximation \\\\ .Krylov sequence methods \\\\ .Alternatives \\\\ . subspaces \" , .", "label": "", "metadata": {}, "score": "61.578865"}
{"text": "The arithmetic mean \\\\ .\\\\ .Pars posterior / Part two \\\\ .\\\\ .Existence of the least squares estimates \\\\ .Relation between combinations for different unknowns .\\\\ .A formula for the residual sum of squares \\\\ .", "label": "", "metadata": {}, "score": "61.581406"}
{"text": "/ \\\\ .Transient parameter estimation by an SVD - based Wigner . distribution / M. F. Griffin , A. M. Finn / \\\\ .Signal / noise subspace decomposition for random . transient detection / N. M. Marinovich , L. M. Roytman / .", "label": "", "metadata": {}, "score": "61.582764"}
{"text": "Vector space is enhanced semantically by modifying the weight of the word vector governed by Appearance and Disappearance ( Action class ) words .The knowledge base for Action words is maintained by classifying the words as Appearance or Disappearance with the help of Wordnet .", "label": "", "metadata": {}, "score": "61.586273"}
{"text": "multiplications per iteration , which is twice the complexity of the LMS [ 78 ] .An adaptive step - size learning algorithm [ 79 ] has been derived for extracting the MC by introducing information criterion .The algorithm globally converges asymptotically to the MC and its corresponding eigenvector .", "label": "", "metadata": {}, "score": "61.603065"}
{"text": "LASCALA --- A Language for Large Scale Linear Algebra / . A. W. Westerberg and T. J. Berna / 90 \\\\ .Practical Comparisons of Codes for the Solution of .Sparse Linear Systems / Iain S. Duff / 107 \\\\ .", "label": "", "metadata": {}, "score": "61.610382"}
{"text": "318 - 362 , MIT Press , Cambridge , Mass , USA , 1986 .View at Google Scholar .N. Kambhatla and T. K. Leen , \" Fast non - linear dimension reduction , \" in Proceedings of the IEEE International Conference on Neural Networks , vol .", "label": "", "metadata": {}, "score": "61.611362"}
{"text": "and applications / Ch .He and A. Bunse - Gerstner / 389 .\\\\ .C$ / N. J. Higham / 391 \\\\ .Fast transforms and elliptic problems / T. Huckle / 393 .\\\\ .An interior - point method for minimizing the maximum .", "label": "", "metadata": {}, "score": "61.61303"}
{"text": "A . ; this approach is usually preferred if only a few eigenvectors are required .Alternatively , eigenvectors of .T . can be computed by back - substitution , and pre - multiplied by .Z . to give eigenvectors of .", "label": "", "metadata": {}, "score": "61.657967"}
{"text": "Ordering effects on relaxation methods applied to the .discrete convection - diffusion equation / Howard C. .Elman and Michael P. Chernesky / 45 \\\\ .On the error computation for polynomial based iteration .methods / Bernd Fischer and Gene H. Golub / 59 \\\\ .", "label": "", "metadata": {}, "score": "61.675392"}
{"text": "Because the method uses pivoting ( rearranging the lower part of the matrix for better numerical stability ) , you have to permute input vectors before applying the L and U matrices .The permutation is returned either in the second argument or , in list context , as the second element of the list .", "label": "", "metadata": {}, "score": "61.68905"}
{"text": "The adaptive invariant - norm MCA algorithm [ 85 ] has been generalized to the case for complex - valued input signal vector .For ICA algorithms , FastICA has been applied to complex signals [ 101 ] .The . -APEX algorithms and GHA are , respectively , extended to the complex - valued case [ 102 , 103 ] .", "label": "", "metadata": {}, "score": "61.690228"}
{"text": "A Matrix Reduction and Some Consequences / 63 \\\\ .\\\\ .Practicalities \\\\ .\\\\ .Errors , Arithmetic , and Stability / 69 \\\\ .An Informal Language / 83 \\\\ .Coding Matrix Operations / 93 \\\\ .The Direct Solution of Linear Systems \\\\ .", "label": "", "metadata": {}, "score": "61.690773"}
{"text": "MathSciNet database \" , .usual perturbation bounds for Cholesky factors can . systematically overestimate the errors .In this note we . sharpen their results and extend them to the factors of .the LU decomposition .The results are based on a new . formula for the first - order terms of the error in the . factors . \" Related Decompositions \" , .", "label": "", "metadata": {}, "score": "61.716896"}
{"text": "The utilities here use general - purpose algorithms that work acceptably for many cases but might not scale well to very large or pathological ( near - singular ) matrices .Except as noted , the matrices are PDLs whose 0th dimension ranges over column and whose 1st dimension ranges over row .", "label": "", "metadata": {}, "score": "61.72144"}
{"text": "To simulate large HSI datasets , we generate random test matrices . by rSVD as shown in Figure 3(b ) , where we clearly see that both sets are almost identical up to the fifteenth singular vector .To judge the accuracy of estimated singular values , we compute the relative absolute errors , . and remain in the same order even when the size of a matrix increases .", "label": "", "metadata": {}, "score": "61.76407"}
{"text": "and the eigenvalue problem . \" Probability --- Random Processes ; Stochastic .Perturbation Theory \" , .Res .Center , East Hartford , CT , .USA \" , .USA \" , . singular values \" , .Theory / Matrix . bib \" , . miscellaneous ) \" , .", "label": "", "metadata": {}, "score": "61.82622"}
{"text": "A . is permutable to upper triangular form , then no floating - point operations are needed to reduce it to Schur form .Scaling : a diagonal matrix .D . is used to make the rows and columns of .", "label": "", "metadata": {}, "score": "61.832924"}
{"text": "Lecture 5 .A Hybrid Method \\\\ .Errors , Accuracy , and Condition Numbers \\\\ .\\\\ .Part II .Computer Arithmetic : \\\\ .\\\\ .\\\\ .Lecture 6 .Floating - Point Numbers \\\\ .Overflow and Underflow \\\\ .", "label": "", "metadata": {}, "score": "61.861015"}
{"text": "Four formulas for the residual sum of squares as a . function of the unknowns \\\\ .Errors in the least squares estimates as functions of .the errors in the observations \\\\ .Mean errors and correlations \\\\ .Linear functions of the unknowns \\\\ .", "label": "", "metadata": {}, "score": "61.91677"}
{"text": "Asymptotic robustness of normal theory methods for the . analysis of latent curves / M. W. Browne \\\\ .Bounded influence errors - in - variables regression / Chi .Lun Cheng and John W. Van Ness \\\\ .Bounded influence estimation in the errors - in - variables .", "label": "", "metadata": {}, "score": "61.9358"}
{"text": "The first eight singular vectors , . , are folded back from the transformed data .Starting from the eighth , the rest of the singular vectors appear to be mostly noise .( notice the log scale on the y axis ) , which means that the entropy of residuals is significantly smaller than the entropy of the original .", "label": "", "metadata": {}, "score": "61.937317"}
{"text": "53 - 58 , 1989 .View at Google Scholar \u00b7 View at Scopus .M. A. Kramer , \" Nonlinear principal component analysis using autoassociative neural networks , \" AIChE Journal , vol .37 , no . 2 , pp .", "label": "", "metadata": {}, "score": "62.040245"}
{"text": "S68 1984 \" , .Cleve Moler , Advisor \" , .W. J. Cody \\\\ .LINPACK : A package for solving linear systems / J. J. .Dongarra and G. W. Stewart \\\\ .FUNPACK :A package of special function routines / W. J. .", "label": "", "metadata": {}, "score": "62.067314"}
{"text": "Nonnegative PCA and nonnegative ICA algorithms are given in [ 150 ] , where the sources . must be nonnegative .Constrained ICA is a framework that incorporates additional requirements and prior information in form of constraints into the ICA contrast function [ 151 ] .", "label": "", "metadata": {}, "score": "62.107277"}
{"text": "333 - 344 , 2004 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .W. Zuo , D. Zhang , and K. Wang , \" Bidirectional PCA with assembled matrix distance metric for image recognition , \" IEEE Transactions on Systems , Man , and Cybernetics , Part B , vol .", "label": "", "metadata": {}, "score": "62.16796"}
{"text": "S775 1990 \" , .z3950.loc.gov:7090/Voyager \" , .Notation \\\\ .The $ QR$ decomposition \\\\ . projections \\\\ .Eigenvalues and eigenvectors \\\\ .The singular value decomposition \\\\ .II .Norms and metrics \\\\ .Vector norms \\\\ .", "label": "", "metadata": {}, "score": "62.26232"}
{"text": "1967 - 1979 , 1998 .View at Google Scholar \u00b7 View at Scopus . A. C. S. Leung , K. W. Wong , and A. C. Tsoi , \" Recursive algorithms for principal component extraction , \" Network , vol . 8 , no . 3 , pp .", "label": "", "metadata": {}, "score": "62.29596"}
{"text": "B . is also real .By formulating the problems appropriately , real large sparse singular value problems may be solved using the symmetric eigenvalue functions in Chapter f12 . is known to be nonsingular , and then return to the general case in the following sub - section .", "label": "", "metadata": {}, "score": "62.385143"}
{"text": "There is one important difference between the general approach and the SCSVD approach .To obtain the SVD from the MDS result , we need the column mean vector of the matrix .The column mean vector of the original matrix must be computed , and then the column mean vector is updated by the simple weighted average when the new data comes in .", "label": "", "metadata": {}, "score": "62.452866"}
{"text": "programming / 131 - -143 \\\\ .G. Plateau , C. Roucairol / A supercomputer algorithm .for the 0 - -1 multiknapsack problem / 144 - -157 \\\\ .M. J. Saltzman , R. Subramanian , R. E. Marsten / .", "label": "", "metadata": {}, "score": "62.45333"}
{"text": "Computational Intensive Methods \\\\ .Exact Non - Parametric Significance Tests / C. R. Mehta , .N. R. Patel and P. Senchaudhuri / 227 \\\\ .Resampling Tests of Statistical Hypotheses / A. Young / .233 \\\\ .Clustering Based on Neural Network Processing / H.-M. Adorf and F. Murtagh / 239 \\\\ .", "label": "", "metadata": {}, "score": "62.47247"}
{"text": "when the least squares matrix \\$A\\$ is near a matrix .that is not of full rank .A definition of numerical .rank is given .It is shown that under certain .conditions when \\$A\\$ has numerical rank \\$r\\$ there is .", "label": "", "metadata": {}, "score": "62.49144"}
{"text": "The inversion of linear systems \\\\ .Gaussian elimination and numerical linear algebra \\\\ .The generalized minimum variance theorem \\\\ . matrices \" , . matrices of order $ n$. The chief difficulty with .computing them directly from $ M_m$ is that with . increasing $ m$ the ratio of the small to the large . singular values of $ M_m$ may fall below the rounding . unit , so that the former are computed inaccurately .", "label": "", "metadata": {}, "score": "62.50567"}
{"text": "/ W. J. Keller and J. G. Bethlehem .( Invited paper ) / .377 \\\\ .Multiple Imputation for Data - Base Construction / D. B. .Rubin .( Invited paper ) / 389 \\\\ .GRASP :A Complete Graphical Conceptual Language for .", "label": "", "metadata": {}, "score": "62.528347"}
{"text": "\" Cholesky decomposition , matrix equation \" , .ANALYSIS , General , Parallel algorithms .Mathematics of Computing , NUMERICAL ANALYSIS , Numerical .Linear Algebra , Linear systems ( direct and iterative . methods ) .PROCESSOR ARCHITECTURES , Multiple Data Stream .", "label": "", "metadata": {}, "score": "62.534"}
{"text": "Black box functions in this chapter which compute the Schur factorization perform only the permutation step , since diagonal scaling is not in general an orthogonal transformation .The black box functions which compute eigenvectors perform both forms of balancing .3.5 Non - uniqueness of Eigenvectors and Singular Vectors .", "label": "", "metadata": {}, "score": "62.53971"}
{"text": "expositions of their numerical properties , including .their perturbation theory , their various . representations , their behavior in the presence of .rounding error , the computation of complementary . projections , and updating algorithms .This article is . intended to make a start at filling this gap .", "label": "", "metadata": {}, "score": "62.555637"}
{"text": "Specifically , given an integer m , SRRIT attempts .to compute a matrix Q with m orthonormal columns and .real quasi - triangular matrix T of order m such that the . by the user .The eigenvalues of T are approximations to .", "label": "", "metadata": {}, "score": "62.59281"}
{"text": "From the above analysis , we can have a fast PCA approach by computing the SCMDS first and then adapt the MDS result to obtain the PCA .We named this approach SCPCA .Similarly , the fast SVD approach , which computes the SCMDS first , then adapts the MDS result to obtain the PCA , and finally adapts the PCA result to the SVD , is called the SCSVD .", "label": "", "metadata": {}, "score": "62.685806"}
{"text": "L22 L5 1979 ; QA184 .L56 1982 ; QA214 .L56 .principal developers of LINPACK , a unique package of .Fortran subroutines for analyzing and solving various . systems of simultaneous linear algebraic equations and .linear least squares problems .", "label": "", "metadata": {}, "score": "62.69072"}
{"text": "Edelman / 365 \\\\ .On numerical methods for unitary eigenvalue problems / .Accurate singular values and differential qd algorithms ./ K. V. Fernando and B. N. Parlett / 371 \\\\ .Orthogonal projection and total least squares / R. D. .", "label": "", "metadata": {}, "score": "62.70502"}
{"text": "The eigenvectors are also determined more accurately than for general matrices , and may be computed more accurately as well .This work extends results of Kahan and Demmel for bidiagonal and tridiagonal matrices . ... or sym metric positive definite tridiagonal matrices QR iteration ( suitably modified ) can be shown to attain high accuracy as well .", "label": "", "metadata": {}, "score": "62.715416"}
{"text": "T. Ericsson / Implementation and applications of the . spectral transformation Lanczos algorithm / 177 \\\\ . D. J. Evans / Preconditioned iterative methods for the .generalized eigenvalue problem / 189 \\\\ . A. Jennings / On bounds for symmetric eigenvalue .", "label": "", "metadata": {}, "score": "62.74221"}
{"text": "We present a new algorithm hich computes all the singular values of a bidiagonal matrix to high relative accuracy independent of their magnitudes .In contrast , the standard algorithm for bidiagonal matrices may compute small singular values with no relative accuracy at all .", "label": "", "metadata": {}, "score": "62.744324"}
{"text": "Robust PCA can be defined so that the optimization criterion grows less than quadratically with the same constraint conditions as those for PCA , which are based on the quadratic criterion [ 57 ] .This usually leads to mildly nonlinear algorithms , in which the nonlinearities appear at selected places only and at least one neuron produces the linear response .", "label": "", "metadata": {}, "score": "62.747818"}
{"text": "Theory / Matrix . bib \" , .Pittsburgh , PA , USA \" , . generalized eigenvalue problem ; Gershgorin 's theorem ; . infinite eigenvalues ; linear algebra ; multiple .eigenvalues ; perturbation bounds ; Riemann sphere \" , . squares problem \" , . C4130 ( Interpolation and function approximation ) \" , .", "label": "", "metadata": {}, "score": "62.789307"}
{"text": "General Purpose Functions : these perform the computational subtasks which make up the separate stages of the overall task , as described in Section 2 - for example , reducing a real symmetric matrix to tridiagonal form .General purpose functions are to be found , for historical reasons , some in this chapter , a few in Chapter f01 , but most in Chapter f08 .", "label": "", "metadata": {}, "score": "62.818848"}
{"text": "Matrices \" , . eigenvectors of a diagonally dominant matrix all of .whose elements are known to high relative accuracy .Eigenvectors corresponding to pathologically close .eigenvalues are treated by computing the invariant . subspace that they span .If the off - diagonal elements . of the matrix are sufficiently small , the method is . superior to standard techniques , and indeed it may . produce a complete set of eigenvectors with an amount . of work proportional to the square of the order of the . matrix .", "label": "", "metadata": {}, "score": "62.865807"}
{"text": "Thus , it is important to make sure that the estimated rank is greater than the essential rank .In other words , when the estimated rank of the SCSVD is smaller than the essential rank , our SCSVD result can be used as the approximated solution of the SVD . . .", "label": "", "metadata": {}, "score": "62.897293"}
{"text": "The least - squares solution of linear equations with .block - angular observation matrix / M. G. Cox / 227 \\\\ .An iterative method for solving linear inequalities ./ G. W Stewart / 241 \\\\ .Iterative refinement and reliable computing / .", "label": "", "metadata": {}, "score": "62.95599"}
{"text": "non - Hermitian linear systems / Roland W. Freund / 69 .\\\\ .Matrices that generate the same Krylov residual spaces ./ Anne Greenbaum and Zdenek Strakos / 95 \\\\ .Incomplete block factorizations as preconditioners for . sparse SPD matrices / L. Yu .", "label": "", "metadata": {}, "score": "62.98825"}
{"text": "A generalized ADI iterative method / N. Levenberg and .L. Reichel / 403 \\\\ .Quaternions and the symmetric eigenvalue problem / N. .Mackey / 405 \\\\ .Application of the Gauss - Seidel iteration to the RLS .\\\\ .", "label": "", "metadata": {}, "score": "62.99984"}
{"text": "A signal subspace rank estimator is employed to track the number of sources .Generalized Eigenvalue Decomposition .Generalized EVD is a statistical tool extremely useful in feature extraction , pattern recognition as well as signal estimation and detection .The generalized EVD problem involves the matrix equation .", "label": "", "metadata": {}, "score": "63.0365"}
{"text": "333 - 346 , 1998 .View at Google Scholar \u00b7 View at Scopus .L. Sun , S. Ji , and J. Ye , \" Canonical correlation analysis for multilabel classification : a least - squares formulation , extensions , and analysis , \" IEEE Transactions on Pattern Analysis and Machine Intelligence , vol .", "label": "", "metadata": {}, "score": "63.061882"}
{"text": "All the above remarks also apply - with the obvious changes - to the case when .A . is a complex matrix .The eigenvalues are in general complex , so there is no need for special treatment of complex conjugate pairs , and the Schur form .", "label": "", "metadata": {}, "score": "63.063614"}
{"text": "Apostolos Hadjidimos and Robert J. Plemmons \\\\ .Iterative methods for finding the stationary vector for .Markov chains / Dianne P. O'Leary \\\\ .Local convergence of ( exact and inexact ) iterative .aggregation / Daniel B. Szyld \\\\ .", "label": "", "metadata": {}, "score": "63.076668"}
{"text": "Yeremin / 119 \\\\ .How fast can iterative methods be ? / Olavi Nevanlinna / .135 \\\\ .Rational Krylov algorithms for nonsymmetric eigenvalue . problems / Axel Ruhe / 149 \\\\ .Highly parallel preconditioners for general sparse . matrices / Youcef Saad / 165 \\\\ .", "label": "", "metadata": {}, "score": "63.086746"}
{"text": "or singular value decomposition ( SVD ) used in previous .ESPRIT algorithms .Its performance is demonstrated on . simulated data representing both constant and . time - varying signals .It is shown that the URV - based .ESPRIT algorithm is effective for estimating .", "label": "", "metadata": {}, "score": "63.273876"}
{"text": "View at Google Scholar \u00b7 View at Scopus . A. Weingessel , H. Bischof , K. Hornik , and F. Leisch , \" Adaptive combination of PCA and VQ networks , \" IEEE Transactions on Neural Networks , vol . 8 , no .", "label": "", "metadata": {}, "score": "63.30933"}
{"text": "such that the components of . , which is the estimate of ., are statistically as independent as possible , . being a .demixing matrix .The statistical independence property implies that the joint probability density of the components of .", "label": "", "metadata": {}, "score": "63.324104"}
{"text": "View at Google Scholar \u00b7 View at Scopus . D. Xu , J. C. Principe , and H. C. Wu , \" Generalized eigendecomposition with an on - line local algorithm , \" IEEE Signal Processing Letters , vol .5 , no .", "label": "", "metadata": {}, "score": "63.37034"}
{"text": "J. Dongarra , J. R. Bunch , C. B. Moler and G. W. . be a positive definite diagonal matrix .In a recent . paper , Stewart considered the weighted pseudoinverse $ . bounds , independent of $ D$ , for the norms of these . matrices .", "label": "", "metadata": {}, "score": "63.380913"}
{"text": "eigens_sym .Signature : ( [ phys]a(m ) ; [ o , phys]ev(n , n ) ; [ o , phys]e(n ) ) .Eigenvalues and -vectors of a symmetric square matrix .If passed an asymmetric matrix , the routine will warn and symmetrize it , by taking the average value .", "label": "", "metadata": {}, "score": "63.38363"}
{"text": "The lateral connections can be in a symmetrical or hierarchical topology .The hierarchical lateral connection topology is illustrated in Figure 2 , based on which Rubner - Tavan PCA [ 13 , 49 ] and APEX [ 50 ] algorithms are proposed .", "label": "", "metadata": {}, "score": "63.403465"}
{"text": "View at Google Scholar \u00b7 View at Scopus . K. Gao , M. O. Ahmad , and M. N. S. Swamy , \" Constrained anti - Hebbian learning algorithm for total least - squares estimation with applications to adaptive FIR and IIR filtering , \" IEEE Transactions on Circuits and Systems II , vol .", "label": "", "metadata": {}, "score": "63.45237"}
{"text": "A . and is easier to handle .Eigenvalues and eigenvectors of .T . are computed as required .If all eigenvalues ( and optionally eigenvectors ) are required , they are computed by the .Q .R . algorithm , which effectively factorizes .", "label": "", "metadata": {}, "score": "63.48118"}
{"text": "The APEX algorithm is used to adaptively extract the PCs [ 50 ] .The algorithm is recursive and adaptive , namely , given .PCs , it can produce the .th PC iteratively .The hierarchical structure of lateral connections serves the purpose of weight orthogonalization and also allows the network to grow or shrink without retraining the old units .", "label": "", "metadata": {}, "score": "63.497807"}
{"text": "Here we apply the Hoffman coding due to its fast computation and show the compression ratios at various error rates , corresponding to the numbers of bits required to code the residuals .For example , a 16-bit coding would result in an error in the range of .", "label": "", "metadata": {}, "score": "63.509502"}
{"text": "Deterministic Repair / / 135 \\\\ .What is Fundamental for Markov Chains : First Passage .Times , Fundamental Matrices , and Group Generalized .Inverses / / 151 \\\\ .Immediate Events in Markov Chains / / 163 \\\\ .", "label": "", "metadata": {}, "score": "63.547905"}
{"text": "ICA can extract the statistically independent components from the input data set .It is to estimate the mutual information between the signals by adjusting the estimated matrix to give outputs that are maximally independent .By applying ICA to estimate the independent input data from raw data , a statistical test can be derived to reduce the input dimension .", "label": "", "metadata": {}, "score": "63.629852"}
{"text": "N .I .C . has a single global maximum , and all the other stationary points are unstable saddle points .At the global maximum , . yields an arbitrary orthonormal basis of the principal subspace .The NIC algorithm has a computational complexity of .", "label": "", "metadata": {}, "score": "63.768982"}
{"text": "function approximation ) ; C4140 ( Linear algebra ) \" , .iterative methods ; matrix ; matrix algebra ; systems of .linear equations \" , . with Certain Eigenvalue Problems \" , .S71 1973 \" , .z3950.loc.gov:7090/Voyager \" , .", "label": "", "metadata": {}, "score": "63.796555"}
{"text": "/ M. Stewart and G. Cybenko / 249 \\\\ .Determining rank in the presence of error / G. W. .Stewart / 275 \\\\ .Approximation with Kronecker products / C. F. Van Loan . and N. Pitsianis / 293 \\\\ .", "label": "", "metadata": {}, "score": "63.82285"}
{"text": "Numerical tests on real HSI data suggest that the method is promising and is particularly effective for HSI data interrogation .Introduction .Hyperspectral imagery ( HSI ) data are measurements of the electromagnetic radiation reflected from an object or scene ( i.e. , materials in the image ) at many narrow wavelength bands .", "label": "", "metadata": {}, "score": "63.833366"}
{"text": "Conventional algorithms , including those currently implemented in ( Sca)LAPACK , perform asymptotically more communication than these lower bounds require .In this paper we present parallel and sequential eigenvalue algorithms ( for pencils , nonsymmetric matrices , and symmetric matrices ) and SVD algorithms that do attain these lower bounds , and analyze their convergence and communication costs . ... rix it computes the Schur form .", "label": "", "metadata": {}, "score": "63.84713"}
{"text": "Oja 's minor subspace analysis ( MSA ) algorithm can be formulated by reversing the sign of the learning rate of the SLA .This algorithm requires the assumption that the smallest eigenvalue of the autocorrelation matrix .is less than unity .", "label": "", "metadata": {}, "score": "63.86172"}
{"text": "Convergence of the Orthogonal Iteration / 45 \\\\ .Some Miscellaneous Theorems / 49 \\\\ .Accuracy of the Refined Eigenvalues / 58 \\\\ .Accuracy of the Refined Eigenvectors / 65 \\\\ . table \" , .JSTOR database \" , .", "label": "", "metadata": {}, "score": "63.89741"}
{"text": "Matrices , row vectors , and column vectors can be multiplied with the ' x ' operator ( which is , of course , threadable ) : .Because of the ( column , row ) addressing order , 1-D PDLs are treated as _ row _ vectors ; if you want a _ column _ vector you must add a dummy dimension : .", "label": "", "metadata": {}, "score": "63.92083"}
{"text": "Error Estimates for Discretization Methods for Data .with Less Restrictive Structure \\\\ .References \\\\ .Chapter 3 .Interior Regularity and Local Convergence of .Galerkin Finite Element Approximations for Elliptic .Equations \\\\ .Introduction \\\\ .Hypotheses \\\\ .", "label": "", "metadata": {}, "score": "63.92139"}
{"text": "Algebra / / 177 \\\\ .Equivalence Relations for Stochastic Automata Networks ./ / 197 \\\\ .Graphs and Stochastic Automata Networks / / 217 \\\\ .Analyzing Sample Path Data from Markov Chain Sampling .Experiments / / 237 \\\\ .", "label": "", "metadata": {}, "score": "63.933838"}
{"text": "It is possible to order the Schur factorization so that any desired set of . is symmetric , the Schur vectors are the same as the eigenvectors , but if .A . is nonsymmetric , they are distinct , and the Schur vectors , being orthonormal , are often more satisfactory to work with in numerical computation .", "label": "", "metadata": {}, "score": "63.94916"}
{"text": "A . may have complex eigenvalues , occurring as complex conjugate pairs .If . x . is an eigenvector corresponding to a complex eigenvalue .x . defined in equation ( 1 ) is sometimes called a right eigenvector ; a left eigenvector .", "label": "", "metadata": {}, "score": "63.962917"}
{"text": "47 , no . 9 , pp .1394 - 1397 , 2000 .View at Google Scholar \u00b7 View at Scopus .M. Collins , S. Dasgupta , and R. E. Schapire , \" A generalization of principal component analysis to the exponential family , \" in Advances in Neural Information Processing Systems , T. D. Dietterich , S. Becker , and Z. Ghahramani , Eds . , vol .", "label": "", "metadata": {}, "score": "64.01299"}
{"text": "For asymmetric matrices , nearly all observed matrices with real eigenvalues produce incorrect results , due to errors of the sslib algorithm .If your assymmetric matrix returns all NaNs , do not assume that the values are complex .Also , problems with memory access is known in this library .", "label": "", "metadata": {}, "score": "64.06421"}
{"text": "Acknowledgment \\\\ .References \\\\ .Chapter 8 .Realistic Estimates for Generic Constants in .Multivariate Pointwise Approximation \\\\ .Introduction \\\\ .The General Problem of Error Estimation \\\\ .Taylor 's Formula and Relevant Algebraic Topics \\\\ .Sharp Appraisals for Practical Error Coefficients of .", "label": "", "metadata": {}, "score": "64.1063"}
{"text": "Continuous - Time Markov Chains / / 547 \\\\ .Numerical Methods for M / G/1 Type Queues / / 571 \\\\ .Closing the Gap between Classical and Tensor Based .Iteration Techniques / / 582 \\\\ .Adaptive Relaxation for the Steady - State Analysis of .", "label": "", "metadata": {}, "score": "64.111374"}
{"text": "By switching the sign of .in given learning algorithms , both the NOja and the NOOja can be used for the estimation of minor and principal subspaces of a vector sequence .The above algorithms including Oja 's MSA [ 7 ] , the natural - gradient - based method [ 83 ] , self - stabilizing MCA [ 80 ] , OOja , NOja , and NOOja have a complexity of .", "label": "", "metadata": {}, "score": "64.15411"}
{"text": "/ Walter Gautschi / 45 \\\\ .Global Homotopies and Newton Methods / Herbert B. .Keller / 73 \\\\ .Problems with Different Time Scales / Heinz - Otto Kreiss ./ 95 \\\\ .Accuracy and Resolution in the Computation of Solutions . of Linear and Nonlinear Equations / Peter D. Lax / 107 .", "label": "", "metadata": {}, "score": "64.21272"}
{"text": "Dept . , Univ . of Maryland , College Park , .MD , USA \" , . theory ; rank degeneracy \" , .Singular Values \" , .G. Hartenstein and Ann .C. Merwarth and William N. .Stewart \" , .", "label": "", "metadata": {}, "score": "64.225174"}
{"text": "The objective function for extracting the first principal singular value of the covariance matrix is given by .A .P .C .A ._ ._ ._ ._ .It is an indefinite function .When . , it reduces to PCA [ 14 ] .", "label": "", "metadata": {}, "score": "64.32944"}
{"text": "provide the required background from mathematics and . computer science needed to work effectively in matrix . computations .The remaining chapters are devoted to the .$ L U $ and $ Q R $ decompositions --- their computation .and applications .", "label": "", "metadata": {}, "score": "64.37406"}
{"text": "\\\\ .J. R. Schaffer , P. K. Pearl / Vehicle routing and . scheduling for home delivery / 373 - -384 \\\\ .\\\\ .VII .Simulation \\\\ .\\\\ . O. Balci , R. E. Nance / Simulation model development : .", "label": "", "metadata": {}, "score": "64.43848"}
{"text": "However , the convergence of the magnitudes of the weights can not be guaranteed either unless the initial weights take special values .The total least mean squares ( TLMS ) algorithm [ 74 ] is a random adaptive algorithm for extracting the MC , which has an equilibrium point under persistent excitation conditions .", "label": "", "metadata": {}, "score": "64.452324"}
{"text": "PASTd [ 30 ] is a well - known subspace tracking algorithm updating the signal eigenvectors and eigenvalues .PASTd is based on PAST .Both PAST and PASTd are derived for complex - valued signals .Both PAST and PASTd have linear computational complexity , that is , . operations every update , as in the cases of SLA [ 14 ] , GHA [ 8 ] , LMSER [ 29 ] , and novel information criterion ( NIC ) [ 32 ] .", "label": "", "metadata": {}, "score": "64.48337"}
{"text": "inhomogeneous inequalities and hence linear programming .problems , although no claims are made about .competitiveness with existing methods .\" semidefinite matrix \" , . complement of a positive definite matrix in a positive . semidefinite matrix . \" solving block Hessenberg systems .", "label": "", "metadata": {}, "score": "64.5027"}
{"text": "Solution of Nearly Completely Decomposable Markov .Chains / / 425 \\\\ .A Computationally Efficient Algorithm for .Characterizing the Superposition of Multiple .Heterogeneous Interrupted Bernoulli Processes / / 443 .\\\\ .Generalized Folding Algorithm for Transient Analysis of .", "label": "", "metadata": {}, "score": "64.53096"}
{"text": "If .A . and .B . are both singular and have a common null space , then .A .B . is singular for all . can be regarded as an eigenvalue .Pencils with this property are called singular .", "label": "", "metadata": {}, "score": "64.53269"}
{"text": "\\\\ .On the Structure of Nearly Uncoupled Markov Chains ./ G. W. Stewart ( University of Maryland , Dept . of .Computer Science , USA ) / 287 \\\\ .\\\\ .A Bound Problem in the Modeling of Computer .", "label": "", "metadata": {}, "score": "64.58493"}
{"text": "There is a non - pivoting version lu_decomp2 available which is from 5 to 60 percent faster for typical problems at the expense of failing to compute a result in some cases .Now that the lu_decomp is threaded , it is the recommended LU decomposition routine .", "label": "", "metadata": {}, "score": "64.67905"}
{"text": "In the course of our work , we , like many other library developers , have been faced with many issues relating to portable programming .Previously , a notable obstacle to library development was the lack of standardization in message passing , from both a programming and a functional point of view .", "label": "", "metadata": {}, "score": "64.71589"}
{"text": "linear algebra .This volume includes : forty - four of .Stewart 's most influential research papers in two . subject areas : matrix algorithms , and rounding and . perturbation theory ; a biography of Stewart ; a complete .", "label": "", "metadata": {}, "score": "64.721016"}
{"text": "C. L. Monma , D. F. Shallcross / A PC - based interactive .network design system for fiber optic communication .networks / 190 - -204 \\\\ .\\\\ .IV .Microcomputers in operations research \\\\ .\\\\ . E. Gelman , M. A. Pollatschek / Personal computer .", "label": "", "metadata": {}, "score": "64.725815"}
{"text": "The robust or nonlinear PCA algorithms are derived by using the gradient - descent method [ 57 ] .They can be treated as generalization of SLA [ 7 , 22 ] and the GHA [ 8 ] .Robust and nonlinear PCA algorithms have better stability properties than the corresponding PCA algorithms if the ( odd ) nonlinearity .", "label": "", "metadata": {}, "score": "64.80302"}
{"text": "M. Gentleman / 165 \\\\ .Adaptive signal processing with emphasis on QRD - least . squares lattice / S. Haykin / 183 \\\\ .A direct method for reordering eigenvalues in the .generalized real Schur form of a regular matrix pair .", "label": "", "metadata": {}, "score": "64.8477"}
{"text": "T. Catarci and G. Santucci / 401 \\\\ .\\\\ .Experimental Design \\\\ .New Algorithmic and Software Tools for D - Optimal Design .Computation in Nonlinear Regression / J. P. Vila / 409 .\\\\ .\\\\ .Econometric Computing \\\\ .", "label": "", "metadata": {}, "score": "64.84851"}
{"text": "Problems \" , .JSTOR database \" , .New Mexico , Albuquerque , NM , USA \" , . eigenfunctions ; general square ; generalization of the .QR ; generalized matrix eigenvalue problems ; matrices ; . matrix algebra ; nla , geig , QZ algorithm ; singular \" , . linear equations \" , . B0290H", "label": "", "metadata": {}, "score": "64.876526"}
{"text": "V. Artificial intelligence and expert systems \\\\ .\\\\ .G. Anandalingam , R. Mathieu , C. L. Pittard , N. Sinha / .Artificial intelligence based approaches for solving .hierarchical optimization problems / 289 - -301 \\\\ .J. W. Denton , G. R. Madey / Impact of neurocomputing on .", "label": "", "metadata": {}, "score": "64.88524"}
{"text": "298 - 301 , 1998 .View at Google Scholar \u00b7 View at Scopus .G. Mathew and V. U. Reddy , \" A quasi - newton adaptive algorithm for generalized symmetric eigenvalue problem , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "64.890625"}
{"text": "Some singular matrices LU - decompose just fine , and those are handled OK but give a zero determinant ( and hence ca n't be inverted ) .lu_decomp uses pivoting , which rearranges the values in the matrix for more numerical stability .", "label": "", "metadata": {}, "score": "64.9368"}
{"text": "Eng . , Maryland Univ . , College .Park , MD , USA \" , . estimation ; narrow - band , decomposition ; performance ; . rank - revealing , signals ; real , directions , ESPRIT ; . real - time , of , processing ; TIME , URV ; time - varying , . C4130 ( Interpolation and function approximation ) \" , .", "label": "", "metadata": {}, "score": "64.94861"}
{"text": "Complex PCA is a generalization of PCA in complex - valued data sets [ 95 ] .Complex PCA has been widely applied to complex - valued data and two - dimensional vector fields .Complex PCA employs the same neural network architecture as that of PCA , but with complex weights .", "label": "", "metadata": {}, "score": "64.949684"}
{"text": "An RLS version of the NIC algorithm is given in [ 32 ] .The PAST algorithm [ 30 ] is a special case of the NIC algorithm when . takes unity .The weighted information criterion ( WINC ) [ 34 ] is obtained by adding to the NIC a weight to break the symmetry in the NIC .", "label": "", "metadata": {}, "score": "65.067215"}
{"text": "J. Kuekes and J. L. Martin and G. A. Michael and N. S. .Ostlund and J. Potter and D. K. Pradhan and M. J. Quinn . and G. W. Stewart and Q. F. Stout and L. Watson and J. .Webb \" , . architectures : report of a workshop \" , . algorithm theory ) ; C5220 ( Computer architecture ) \" , .", "label": "", "metadata": {}, "score": "65.08063"}
{"text": "for Thin Plate Spline Interpolation that Employs .Approximations to Lagrange Functions / 17 \\\\ .I. S. Duff : The Solution of Augmented Systems / 10 \\\\ .C. M. Elliott \\ & A. R. Gardiner : One Dimensional Phase .", "label": "", "metadata": {}, "score": "65.16617"}
{"text": "If . are large and sparse , reduction to an equivalent standard eigenproblem as described above would almost certainly result in a large dense matrix .C . , and hence would be very wasteful in both storage and computing time .", "label": "", "metadata": {}, "score": "65.20928"}
{"text": "View at Google Scholar \u00b7 View at Scopus .C. M. Bishop , Neural Networks For Pattern Recogonition , Oxford Press , New York , NY , USA , 1995 . D. E. Rumelhart , G. E. Hinton , and R. J. Williams , \" Learning internal representations by error propagation , \" in Parallel Distributed Processing : Explorations in the Microstructure of Cognition , D. E. Rumelhart and J. L. McClelland , Eds . , vol .", "label": "", "metadata": {}, "score": "65.25629"}
{"text": "333 - 338 , American Institute of Physics , Snowbird , Utah , USA , 1986 .Z. Kang , C. Chatterjee , and V. P. Roychowdhury , \" An adaptive quasi - newton algorithm for eigensubspace estimation , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "65.32308"}
{"text": "The fourth moments of the uniform , triangular , and .normal distributions \\\\ .The distribution of a function of several errors \\\\ .The mean value of a function of several errors \\\\ .Some special cases \\\\ .Convergence of the estimate of the mean error \\\\ .", "label": "", "metadata": {}, "score": "65.3735"}
{"text": "C4140 ( Linear algebra ) \" , .Sci . , Maryland Univ . , College Park , .MD , USA \" , .Jacobi - like algorithm ; matrix algebra ; nonHermitian .matrix ; off - diagonal elements ; parallel implementation ; . parallel processing ; plane rotations ; Schur . decomposition ; unitary similarity transformations ; . upper triangular form \" , .", "label": "", "metadata": {}, "score": "65.4143"}
{"text": "Prediction of true values for the measurement error .model / Wayne A. Fuller \\\\ .Analysis of residuals from measurement error models / .Stephen M. Miller \\\\ .Errors - in - variables estimation in the presence of . serially correlated observations / John L. Eltinge \\\\ .", "label": "", "metadata": {}, "score": "65.44392"}
{"text": "View at Google Scholar \u00b7 View at Scopus . A. Weingessel and K. Hornik , \" A robust subspace algorithm for principal component analysis , \" International Journal of Neural Systems , vol .13 , no .5 , pp .", "label": "", "metadata": {}, "score": "65.44864"}
{"text": "( Berlin ) \" , . L545 1993 \" , .z3950.loc.gov:7090/Voyager \" , .Queuing theory \" , .with applications to Markov chains / Jesse L. Barlow .\\\\ .The influence of nonnormality on matrix computations / .Componentwise error analysis for stationary iterative . methods / Nicholas J. Higham and Philip A. Knight \\\\ .", "label": "", "metadata": {}, "score": "65.45584"}
{"text": "T$ is a permutation .\" the solution of dense linear systems of equations .However , Gauss himself originally introduced his .elimination procedure as a way of determining the . precision of least squares estimates and only later .described the computational algorithm .", "label": "", "metadata": {}, "score": "65.566635"}
{"text": "r . and minimize .t .r .Assuming that . is a nonsingular matrix , conventionally , the following Fisher 's determinant ratio criterion is maximized for finding the projection directions [ 152 , 153 ] : .L .", "label": "", "metadata": {}, "score": "65.62337"}
{"text": "Acknowledgments / xiii \\\\ .Preliminaries \\\\ .\\\\ .The Space $ R^n$ / 2 \\\\ .Linear Independence , Subspaces , and Bases / 9 \\\\ .Matrices / 20 \\\\ .Operations with Matrices / 29 \\\\ .Linear Transformations and Matrices / 46 \\\\ .", "label": "", "metadata": {}, "score": "65.643585"}
{"text": "References \\\\ .Index \\\\ .Part III : Reprints \\\\ .12 Papers on Matrix Decompositions \\\\ .12.1 [ GWS - B / 2 ] ( with J. J. Dongarra , J. R. Bunch , and .C. B. Moler ) Introduction from Linpack Users Guide \\\\ .", "label": "", "metadata": {}, "score": "65.67305"}
{"text": "FUNCTIONS . identity .Signature : ( n ; [ o]a(n , n ) ) .Return an identity matrix of the specified size .If you hand in a scalar , its value is the size of the identity matrix ; if you hand in a dimensioned PDL , the 0th dimension is the size of the matrix . stretcher .", "label": "", "metadata": {}, "score": "65.747734"}
{"text": "- Brian M. Scott Nov 18 ' 12 at 5:20 . 1 Answer 1 .There are two methods that are quite good methods to check for singularity : .As pointed out in comments , Gaussian elimination is a very efficient method for checking it .", "label": "", "metadata": {}, "score": "65.75061"}
{"text": "BLAS \\\\ .Upper Hessenberg and Tridiagonal Systems \\\\ .\\\\ .Lecture 15 .Vector Norms \\\\ .Matrix Norms \\\\ .Relative Error \\\\ .Sensitivity of Linear Systems \\\\ .\\\\ .Lecture 16 .The Condition of Linear Systems \\\\ .", "label": "", "metadata": {}, "score": "65.76827"}
{"text": "Comparisons of truncated QR and SVD methods for AR . spectral estimations / S. F. Hsieh et al ./ \\\\ .Using singular value decomposition to recover periodic . waveforms in noise and with residual carrier / B. Rice ./ \\\\ .", "label": "", "metadata": {}, "score": "65.77739"}
{"text": "When the data configuration is Euclidean , the MDS is similar to the PCA , in that both can remove inherent noise with its compact representation of data .The order three computational complexity makes it infeasible to apply to huge data , for example , when the sample size is more than one million .", "label": "", "metadata": {}, "score": "65.80614"}
{"text": "11 , pp .1820 - 1836 , 2009 .View at Google Scholar \u00b7 View at Scopus . D. Z. Feng , Z. Bao , and W. X. Shi , \" Cross - correlation neural network models for the smallest singular component of general matrix , \" Signal Processing , vol .", "label": "", "metadata": {}, "score": "65.899124"}
{"text": "M. A. O'Neil and M. Burtscher , \" Floating - point data compression at 75 Gb / s on a GPU , \" in Proceedings of the 4th Workshop on General Purpose Processing on Graphics Processing Units ( GPGPU ' 11 ) ,", "label": "", "metadata": {}, "score": "65.90489"}
{"text": "Cholesky , and $ QR$ Factorizations ' '\\\\ .12.8 [ GWS - J / 89 ] ' 'On Graded $ QR$ Decompositions of .Products of Matrices ' '\\\\ .12.9 [ GWS - J / 92 ] ' '", "label": "", "metadata": {}, "score": "65.909645"}
{"text": "We can see that when the estimated rank decreases , the error arises rapidly .Lines in Figure 2 from the bottom to the top are the matrix size with . , respectively .The error increases slowly when the matrix size increases .", "label": "", "metadata": {}, "score": "65.916405"}
{"text": "It is derived from the optimization of the kurtosis or the negentropy measure by using Newton 's method .FastICA achieves a reliable and at least a quadratic convergence .FastICA with symmetric orthogonalization and tanh nonlinearity is concluded as the best trade - off for ICA [ 10 ] .", "label": "", "metadata": {}, "score": "65.93304"}
{"text": "Linear Programming Problems Arising from Partial .Differential Equations / Dianne P. O'Leary / 25 \\\\ .Shifted Incomplete Cholesky Factorization / Thomas A. .Manteuffel / 41 \\\\ .Algorithmic Aspects of the Multi - Level Solution of .Finite Element Equations / Randolph E. Bank and A. H. .", "label": "", "metadata": {}, "score": "65.964264"}
{"text": "z3950.loc.gov:7090/Voyager \" , . presented at the University of Maryland at College Park . and recorded after the fact . \"Lecture 1 .General observations \\\\ .Decline and fall \\\\ .The linear sine \\\\ .Approximation in normed linear spaces \\\\ .", "label": "", "metadata": {}, "score": "65.99458"}
{"text": "An error analysis of the procedure which .results in effectively computable error bounds is .given . bound ; transient Markov chain ; steady state \" , . probability \" , . of transition probabilities can be partitioned so that .", "label": "", "metadata": {}, "score": "66.03163"}
{"text": "Compression and Reconstruction of HSI Data by rSVD .The flight times of airplanes carrying hyperspectral scanning imagers are usually limited by the data capacity , since within 5 to 10 seconds hundreds of thousands of pixels of hyperspectral data are collected [ 1 ] .", "label": "", "metadata": {}, "score": "66.04678"}
{"text": "more nearly equal in norm : .A .D .A .D .Scaling can make the matrix norm smaller with respect to the eigenvalues , and so possibly reduce the inaccuracy contributed by roundoff ( see Chapter II/11 of Wilkinson and Reinsch ( 1971 ) ) .", "label": "", "metadata": {}, "score": "66.051094"}
{"text": "The anti - Hebbian rule can be used to remove correlations between units receiving correlated inputs [ 13 , 48 , 49 ] .The anti - Hebbian rule is inherently stable [ 13 , 49 ] .Anti - Hebbian rule - based PCA algorithms can be derived by using a .", "label": "", "metadata": {}, "score": "66.13451"}
{"text": "If the value exists , then it is assumed to hold the LU decomposition . det ( Output ) .If this key exists , then the determinant of $ a get stored here , whether or not the matrix is singular . det .", "label": "", "metadata": {}, "score": "66.17468"}
{"text": "\\\\ .Fundamentals of geodesy \\\\ .De Krayenhof 's triangulation \\\\ .A triangulation from Hannover \\\\ .Determining weights in the Hannover triangulation \\\\ .Anzeigen / Notices : Part one , part two , supplement \\\\ .Afterword \\\\ .", "label": "", "metadata": {}, "score": "66.23541"}
{"text": "S2 , ANU : Speech processing 2 , audio , neural .networks , underwater acoustics \\\\ . vol .M : Multidimensional signal processing \\\\ . vol .D1 : Digital signal processing \\\\ . vol .D2EV : Digital signal processing 2 , estimation , .", "label": "", "metadata": {}, "score": "66.33694"}
{"text": "That makes it slightly easier to access individual eigenvectors , since the 0th dim of the output PDL runs across the eigenvectors and the 1st dim runs across their components .eigens_sym ignores the bad - value flag of the input piddles .", "label": "", "metadata": {}, "score": "66.401306"}
{"text": "The difference between PCA and MCA lies in the sign of the learning rate .The MCA algorithm proposed in [ 83 ] suffers from a marginal instability , and thus it requires intermittent normalization such that .[ 80 ] .", "label": "", "metadata": {}, "score": "66.4244"}
{"text": "Yes , it is true that we can find determinant in $ O(n^3)$ , same as Gaussian elimination , but Gaussian elimination is easy to implement whereas determinant is not so easy to implement .( For example , I can implement Gaussian elimination but I do n't even know what the algorithm for determinant in $ O(n^3)$ is ? 1 Scope of the Chapter . singular value problems ( finding singular values and singular vectors of a rectangular matrix .", "label": "", "metadata": {}, "score": "66.49113"}
{"text": "6.4 Impact \\\\ .The Eigenproblem and Invariant Subspaces : .Perturbation Theory \\\\ .7.1 Perturbation of Eigenvalues of General Matrices \\\\ . 7.2Further Results for Hermitian Matrices \\\\ . 7.3 Stochastic Matrices \\\\ . 7.4 Graded Matrices \\\\ . 7.5 Rayleigh / Ritz Approximations \\\\ . 7.6 Powers of Matrices \\\\ . 7.7 Impact \\\\ .", "label": "", "metadata": {}, "score": "66.51079"}
{"text": "\\\\ .Part V. Numerical Integration and Differentiation \\\\ .\\\\ .\\\\ .Lecture 21 .Numerical Integration \\\\ .Change of Intervals \\\\ .The Trapezoidal Rule \\\\ .The Composite Trapezoidal Rule \\\\ .Newton - Cotes Formulas \\\\ .", "label": "", "metadata": {}, "score": "66.53573"}
{"text": "194 - 200 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Kukharev and E. Kamenskaya , \" Application of two - dimensional canonical correlation analysis for face image processing and recognition , \" Pattern Recognition and Image Analysis , vol .", "label": "", "metadata": {}, "score": "66.54497"}
{"text": "UMIACS -TR-95 - 92 ) \" , .Matrix \" , . truncated pivoted $ Q R $ approximations to a sparse . matrix .One is based on the Gram -- Schmidt algorithm , . and the other on Householder triangularization .", "label": "", "metadata": {}, "score": "66.60379"}
{"text": "Core Storage / S. C. Eisenstat , M. H. Schultz , and A. . H. Sherman / 135 \\\\ .A Quotient Graph Model for Symmetric Factorization / .Alan George and Joseph W. H. Liu / 154 \\\\ .The Use of Sparse Matrices for Image Reconstruction .", "label": "", "metadata": {}, "score": "66.61763"}
{"text": "If you feed in a non - diagonalizable matrix , then one or more of the eigenvectors will be set to NaN , along with the corresponding eigenvalues .eigens is threadable , so you can solve 100 eigenproblems by feeding in a 3x3x100 array .", "label": "", "metadata": {}, "score": "66.66936"}
{"text": "Introduction \\\\ .Indirect Continuous Approximation to Solutions of .Linear Functional Equations with Respect to a given Set . of Functions \\\\ .Indirect Discrete Approximation to the Solution of a .Functional Equation \\\\ .Discrete Approximation to Solutions of Boundary .", "label": "", "metadata": {}, "score": "66.744385"}
{"text": "11 , pp .718 - 729 , 1994 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus . D. Z. Feng , Z. Bao , and L. C. Jiao , \" Total least mean squares algorithm , \" IEEE Transactions on Signal Processing , vol .", "label": "", "metadata": {}, "score": "66.75935"}
{"text": "Some members in the class have better numerical performance and require less computational effort compared to that of both GHA and APEX .Most existing linear complexity methods including GHA [ 8 ] , SLA [ 7 ] , and PCA with the lateral connections [ 13 , 35 , 48 - 50 ] require a computational complexity of . per iteration .", "label": "", "metadata": {}, "score": "66.76784"}
{"text": "Sources of information on quadrature software / D. .Kahaner \\\\ .A survey of sparse matrix software / Iain S. Duff \\\\ .Mathematical software for elliptic boundary value . problems \\\\ .Ronald F. Boisvert and Roland A. Sweet \\\\ .", "label": "", "metadata": {}, "score": "66.79577"}
{"text": "Though SVD decomposition takes $ O(n^3)$. I think algorithms for just finding singular values exist and ( should be ) faster .You 'll have to search for them .Remaining is my personal view and you are free to disagree , hence I 've hidden it :-) .", "label": "", "metadata": {}, "score": "66.86314"}
{"text": "Optimization , Least Squares and Linear Programming ./ / 145 \\\\ .Optimization for Sparse Systems / T. L. Magnanti / .147 \\\\ .Methods for Sparse Linear Least Squares Problems / .The Orthogonal Factorization of a Large Sparse .", "label": "", "metadata": {}, "score": "66.87373"}
{"text": "Mathematical Software / / 241 \\\\ .Sparse Matrix Software / W. Morven Gentleman and .Alan George / 243 \\\\ .Considerations in the Design of Software for Sparse .Gaussian Elimination / S. C. Eisenstat , M. H. Schultz , . and A. H. Sherman / 263 \\\\ .", "label": "", "metadata": {}, "score": "66.8844"}
{"text": "This is my second question in stack overflow .I do n't have to much experience with python , but had excellent results with my first question and I was able to implement the code from the answer , so I ... .", "label": "", "metadata": {}, "score": "66.891785"}
{"text": "u . is multiplied by a factor . k . such that . k . , then .v . must also be multiplied by . k . . .Non - uniqueness also occurs among eigenvectors which correspond to a multiple eigenvalue , or among singular vectors which correspond to a multiple singular value .", "label": "", "metadata": {}, "score": "66.92047"}
{"text": "VIII .Model development and analysis systems \\\\ .\\\\ .G. H. Bradley / Mathematical programming modeling .project - overview / 447 - -462 \\\\ .R. G. Brown , J. W. Chinneck , G. M. Karam / Optimization .", "label": "", "metadata": {}, "score": "66.96298"}
{"text": "T .S .S .T . , where .S . is orthogonal , or by the divide - and - conquer method .If only selected eigenvalues are required , they are computed by bisection , and if selected eigenvectors are required , they are computed by inverse iteration .", "label": "", "metadata": {}, "score": "66.972244"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. Martinsson , \" Randomized methods for computing the singular value decomposition of very large matrices , \" in Workshop on Algorithms for Modern Massive Data Sets , 2012 . A. D. Meigs , L. J. Otten , and T. Y. Cherezova , \" Ultraspectral imaging : a new contribution to global virtual presence , \" in Proceedings of the IEEE Aerospace Conference , vol .", "label": "", "metadata": {}, "score": "66.99475"}
{"text": "Our algorithm hinges on the fact that , under easily constructively verifiable conditions , a symmetric matrix with bandwidth b and k distinct eigenvalues must be block diagonal with diagonal blocks of size at most bk .A slight modification of the usual orthogonal band - reduction algorithm allows us to reveal this structure , which then leads to potential parallelism in the form of independent diagonal blocks .", "label": "", "metadata": {}, "score": "67.00012"}
{"text": "32 , no . 1 - 2 , pp .29 - 43 , 2002 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .View at Scopus . Y. Chen and C. Hou , \" High resolution adaptive bearing estimation using a complex - weighted neural network , \" in Proceedings of the IEEE International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ' 92 ) , vol .", "label": "", "metadata": {}, "score": "67.100266"}
{"text": "Application of vector extrapolation and conjugate . gradient type methods to the semiconductor device .problem / W. H. A. Schilders / 415 \\\\ .Accurate symmetric eigenreduction by a Jacobi method / .The order - recursive Chandrasekhar equations for fast . square - root Kalman filtering / D. T. M. Slock / 419 \\\\ .", "label": "", "metadata": {}, "score": "67.18535"}
{"text": "For compatibility with lu_decomp , it will give you a permutation list and a parity scalar if you ask for them -- but they are always trivial .Because lu_decomp2 does not pivot , it is numerically unstable -- that means it is less precise than lu_decomp , particularly for large or near - singular matrices .", "label": "", "metadata": {}, "score": "67.196106"}
{"text": "View at Scopus .J. Rubner and K. Schulten , \" Development of feature detectors by self - organization , \" Biological Cybernetics , vol .62 , no . 3 , pp .193 - 199 , 1990 .View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "67.23685"}
{"text": "Building a Statistical Expert System with Knowledge .Bases of Different Levels of Abstraction / KM .Wittkowski / 129 \\\\ .An Expert System for the Interpretation of Results of .Building a Statistical Knowledge Base : A Discussion of .", "label": "", "metadata": {}, "score": "67.30165"}
{"text": "Pittsburgh , PA , USA \" , .eigenvalue ; eigenvalues and eigenfunctions ; error . vectors ; isolated extreme points ; linear system ; matrix .algebra ; method of conjugate gradients \" , .JSTOR database ; Parallel / par .", "label": "", "metadata": {}, "score": "67.35837"}
{"text": "459 - 473 , 1989 .View at Google Scholar \u00b7 View at Scopus . D. O. Hebb , The Organization of Behavior , John Wiley & Sons , New York , NY , USA , 1949 . K. L. Du and M. N. S. Swamy , Networks in a Softcomputing Framework , Springer , London , UK , 2006 .", "label": "", "metadata": {}, "score": "67.36862"}
{"text": "Return a diagonal matrix with the specified diagonal elements . inv .Signature : ( a(m , m ) ; sv opt ) .Invert a square matrix .You feed in an NxN matrix in $ a , and get back its inverse ( if it exists ) .", "label": "", "metadata": {}, "score": "67.382996"}
{"text": "The algorithm described here is a promising variant of the Invariant Subspace Decomposition Algorithm for dense symmetric matrices ( SYISDA ) ... \" .We present an overview of the banded Invariant Subspace Decomposition Algorithm for symmetric matrices and describe a parallel implementation of this algorithm .", "label": "", "metadata": {}, "score": "67.387695"}
{"text": "T . , then .Q .s . is an eigenvector of .A . . .All the above remarks also apply - with the obvious changes - to the case when .A . is a complex Hermitian matrix .", "label": "", "metadata": {}, "score": "67.38771"}
{"text": "JSTOR database \" , .Subroutines for Calculating and Ordering the .Theory / gvl . bib \" , . software \" , .Eigenvalue Problem \" , .Park , MD , USA \" , . computations ; plane rotations ; single number \" , . B0290H", "label": "", "metadata": {}, "score": "67.51355"}
{"text": "\" ...Computing the singular values of a bidiagonal matrix is the fin al phase of the standard algow rithm for the singular value decomposition of a general matrix .We present a new algorithm hich computes all the singular values of a bidiagonal matrix to high relative accuracy independent of their magni ... \" .", "label": "", "metadata": {}, "score": "67.5177"}
{"text": "r . m . a .x .After manipulation , we have .m . a .x . where the covariance matrix of . is defined by .Under a mild condition which tends to hold for high - dimensional data , CCA in the multilabel case can be formulated as an LS problem [ 136 ] .", "label": "", "metadata": {}, "score": "67.54314"}
{"text": "would warrant .This observation is not true in general , . and counterexamples are easy to construct .However , it . is often true of the triangular matrices from pivoted .LU or QR decompositions .It is shown that this fact is . closely connected with the rank - revealing character of .", "label": "", "metadata": {}, "score": "67.5477"}
{"text": "quiescent prior to some initial time to are considered , .the result of discretizing the problem in the time .domain is an ill - conditioned triangular Toeplitz . system .In this paper we show how an algorithm of Elden . can be used to implement Tikhonov -- Phillips . regularization for this system .", "label": "", "metadata": {}, "score": "67.585556"}
{"text": "determinant .Signature : ( a(m , m ) ) .Determinant of a square matrix , using recursive descent ( threadable ) .This is the traditional , robust recursive determinant method taught in most linear algebra courses .It scales like O(n ! )", "label": "", "metadata": {}, "score": "67.645065"}
{"text": "The second purpose of this paper is to update the SVD when the matrix size is extended by new data updating .If the rank of matrix is much smaller than the matrix size , Matthew proposed a fast SVD updating method for the low - rank matrix in 2006 [ 9 ] .", "label": "", "metadata": {}, "score": "67.651764"}
{"text": "Eigenvectors of Matrices \" , .Stewart \" , .JSTOR database ; Theory / Matrix . bib \" , .( Linear algebra ) \" , . and Math . , Univ . of Texas , .Austin , TX , USA \" , . matrix algebra ; process ; reorthogonalization ; stable . algorithms \" , .", "label": "", "metadata": {}, "score": "67.69243"}
{"text": "nonlinear errors - in - variables regression models / Leon .Jay Gleser \\\\ .Structural logistic regression measurement error models ./ Leonard A. Stefanski and Raymond J. Carroll \\\\ .Measurement error model estimation using iteratively .weighted least squares / Daniel W. Schafer \\\\ .", "label": "", "metadata": {}, "score": "67.74009"}
{"text": "Unlike SLA [ 7 ] and GHA [ 8 ] , whose stability analysis is based on the stochastic approximation theory [ 6 ] , the stability analysis of LEAP is based on Lyapunov 's first theorem , and as such . can be selected as a small positive constant [ 26 ] .", "label": "", "metadata": {}, "score": "67.78468"}
{"text": "Other optimization - based PCA methods are described in Section 6 .PCA based on the anti - Hebbian rule is treated in Section 7 .Nonlinear PCA is addressed in Section 8 .Section 9 is dedicated to minor component analysis ( MCA ) .", "label": "", "metadata": {}, "score": "67.878044"}
{"text": "5 - 12 , March 1998 .View at Scopus .5806 of Proceedings of SPIE , pp .662 - 667 , 2005 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .B. Parlett , The Symmetric Eigenvalue Problem , vol .", "label": "", "metadata": {}, "score": "67.87875"}
{"text": "The latter represents the cost of moving data , either between levels of a memory hierarchy , or between processors over a network .Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost , so we seek algorithms that minimize communication .", "label": "", "metadata": {}, "score": "67.89139"}
{"text": "at most a few units in the last place .Moreover , the . scaling requires only four floating point . multiplications and a small amount of integer .arithmetic to compute the scale factor .Thus , on many . modern CPUs , our method is both safer and faster than .", "label": "", "metadata": {}, "score": "68.00913"}
{"text": "A bad formula for estimating the errors in the . observations from the residual sum of squares \\\\ .The correct formula \\\\ .The mean error of the residual sum of squares \\\\ .Inequalities for the mean error of the residual sum of . squares \\\\ .", "label": "", "metadata": {}, "score": "68.0532"}
{"text": "\\\\ .Lecture 12 .Positive Definite Matrices \\\\ .The Cholesky Decomposition \\\\ .Economics \\\\ .\\\\ .Lecture 13 .Inner - Product Form of the Cholesky .Algorithm \\\\ .Gaussian Elimination \\\\ .\\\\ .Lecture 14 .", "label": "", "metadata": {}, "score": "68.05446"}
{"text": "ANALYSIS , General , Numerical algorithms .Mathematics of Computing , MATHEMATICAL SOFTWARE , .Reliability and robustness . \" Factorization \" , .\\times n $ matrix on a network consisting of $ P$ MIMD .processors , with no shared memory , when the network is . smaller than the number of elements in the matrix ( $ P .", "label": "", "metadata": {}, "score": "68.06951"}
{"text": "de / pub / users / Ley / bib / records .Stewart and M. H. Wright \" , .Space \" , .for solving matrix problems on parallel processing . computers .Operations are synchronized through .data - flow alone , which makes global synchronization . unnecessary and enables the algorithms to be . implemented on machines with very simple operating . systems and communication protocols .", "label": "", "metadata": {}, "score": "68.07642"}
{"text": "However , if $ A$ is invertible , calculating its determinant gives us strictly more information that knowing that it is nonzero .Although the naive complexity for calculating the determinant is $ O(n ! )$ , faster $ O(n^3)$ algorithms exist .", "label": "", "metadata": {}, "score": "68.081436"}
{"text": "JSTOR database \" , . of a Polynomial by Synthetic Division \" , .Closed Linear Operators \" , .JSTOR database \" , . equation , matrix equation \" , .de / pub / users / Ley / bib / records .", "label": "", "metadata": {}, "score": "68.08228"}
{"text": "The Linear least squares problem \\\\ .IV .The perturbation of Eigenvalues \\\\ .General perturbation theorems \\\\ .Gerschgorin theory : differentiability \\\\ .Normal and diagonalizable matrices \\\\ .Hermitian matrices \\\\ .Some further results \\\\ .V. Invariant subspaces \\\\ .", "label": "", "metadata": {}, "score": "68.088745"}
{"text": "HSI data can be collected over hundreds of wavelengths - creating truly massive data sets .The transmission , storing , and processing of these large data sets often present significant difficulties in practical situations [ 1 ] .Dimensionality reduction methods provide means to deal with the computational difficulties of the hyperspectral data .", "label": "", "metadata": {}, "score": "68.1732"}
{"text": "Krylov Sequence Methods \\\\ .Pivoting \" , .Volume 1 .Basic decompositions \" , .S714 1998 \" , .five - volume series devoted to matrix algorithms .It . focuses on the computation of matrix .decompositions --- that is , the factorization of matrices . into products of similar ones .", "label": "", "metadata": {}, "score": "68.20119"}
{"text": "For real sparse matrices where only selected singular values are required ( possibly with their singular vectors ) , functions from Chapter f12 may be applied to the symmetric matrix .A .T .A . ; see Section 10 in nag_real_symm_sparse_eigensystem_iter ( f12fbc ) .", "label": "", "metadata": {}, "score": "68.202225"}
{"text": "Both CPPCA and rSVD algorithms are applied to each simulated matrix , and results are compared in terms of their reconstruction quality and the computation time .Figure 12(a ) shows that the running time of rSVD increases linearly with .we compare their reconstruction qualities in terms of signal - to - noise ratio ( SNR ) in Figure 13 , and the computation time in Table 1 .", "label": "", "metadata": {}, "score": "68.208725"}
{"text": "large - scale electric power systems planning under . uncertainty / 3 - -22 \\\\ .\\\\ .II .Parallel algorithms for mathematical programming .\\\\ .\\\\ .R. S. Barr , M. G. Christiansen / A parallel auction .", "label": "", "metadata": {}, "score": "68.260735"}
{"text": "More generally the book will be of interest to .statisticians , numerical analysts , and other scientists .who are interested in what Gauss did and how he set .about doing it .An Afterword by the translator , G. W. .", "label": "", "metadata": {}, "score": "68.2836"}
{"text": "Sci . and Technol . , Maryland Univ . , .College Park , MD , USA \" , . parallel algorithms ; parallel implementation ; .QR - algorithm \" , . and Ind. Res . ; .AMDAHL ; FPS ; CRAY \" , .", "label": "", "metadata": {}, "score": "68.298325"}
{"text": "The mean error in the estimate \\\\ .Incomplete adjustment of observations \\\\ .Relation between complete and incomplete adjustments .\\\\ .A block iterative method for adjusting observations \\\\ .The inverse of a symmetric system is a symmetric .", "label": "", "metadata": {}, "score": "68.31433"}
{"text": "Large scale structural analysis on massively parallel .Data - parallel BLAS as a basis for LAPACK on massively .a Large - scale nonlinear constrained optimization / A. .R. Conn , N. Gould and Ph .L. Toint / 21 \\\\ .", "label": "", "metadata": {}, "score": "68.453125"}
{"text": "In [ 137 ] , a strategy for reducing LDA to CCA is proposed .Within - class coupling CCA ( WCCCA ) is to apply CCA to pairs of data samples that are most likely to belong to the same class .", "label": "", "metadata": {}, "score": "68.4672"}
{"text": "Perturbation of invariant subspaces \\\\ .Hermitian matrices \\\\ .The singular value decomposition \\\\ .VI .Generalized Eigenvalue problems \\\\ .Background \\\\ .Regular matrix pairs \\\\ .Definite matrix pairs \\\\ .References \\\\ .Notation \\\\ .", "label": "", "metadata": {}, "score": "68.556244"}
{"text": "The work presented in this paper is part of the PRISM ( Parallel Research on Invariant Subspace Methods )Project , which involves researchers from Argonne National Laboratory , the Supercomputing Research Center , the University of California at Berkeley , and the University of Kentucky .", "label": "", "metadata": {}, "score": "68.6196"}
{"text": "developed a fast multidimensional scaling method which turned the classical order three MDS method to be linear [ 13 ] .In this paper , we would like to implement SCMDS to the fast SVD approach , say SCSVD .The following subsections are reviews of the classical MDS and the SCSVD .", "label": "", "metadata": {}, "score": "68.620316"}
{"text": "C .A . where .Based on an analysis using the stochastic approximation theory [ 10 , 11 ] , when .The weighted SLA can perform PCA ; however , norms of the weight vectors are not equal to unity .", "label": "", "metadata": {}, "score": "68.687805"}
{"text": "Enhanced sinusoidal and exponential data modeling / J. . A. Cadzow , D. M. Wilkes / \\\\ .Enhancements to SVD - Based detection / J. H. Cozzens , M. .J. Sousa / \\\\ .Resolution of closely spaced coherent plane waves via .", "label": "", "metadata": {}, "score": "68.7151"}
{"text": "183 - 194 , 1989 .View at Google Scholar \u00b7 View at Scopus .R. Linsker , \" From basic network principles to neural architecture : emergence of orientation - selective cells , \" Proceedings of the National Academy of Sciences of the United States of America , vol .", "label": "", "metadata": {}, "score": "68.72398"}
{"text": "The theorems of this paper bound the . distance of the spectrum of $ M$ from the spectrum of .$ A$ in terms of appropriate norms of $ R$. Appeared in .SIAM J. Mat .Anal .Appl .( Also .", "label": "", "metadata": {}, "score": "68.73669"}
{"text": "network analysis of indirect labor requirements / .238 - -247 \\\\ .M. L. Villanueva , N. A. Wittpenn , C. B. Austin , E. .Baker / A microcomputer - based marine geographic .information system with marketing application / .", "label": "", "metadata": {}, "score": "68.74398"}
{"text": "at the nonspecialist who needs more than black - box .proficiency with matrix computations .To give the . series focus , the emphasis is on algorithms , their . derivation , and their analysis .The reader is assumed .", "label": "", "metadata": {}, "score": "68.74418"}
{"text": "Chains \" , . approached from a probabilistic point of view .The . perturbed quantity is approximated by a first - order . perturbation expansion , in which the perturbation is . assumed to be random .This permits the computation of .", "label": "", "metadata": {}, "score": "68.76714"}
{"text": "U . and .V . are orthogonal .( An upper bidiagonal matrix is zero except for the main diagonal and the first superdiagonal . )V . are orthogonal and . is diagonal as described above .Then in the SVD of .", "label": "", "metadata": {}, "score": "68.76859"}
{"text": "R4 1997 \" , .z3950.loc.gov:7090/Voyager \" , .Total Least Squares and Errors - in - Variables Modeling , .Leuven , Belgium , August 21 - -24 , 1996 . \"Computing , February 28 and March 1 , 1998 \" , .", "label": "", "metadata": {}, "score": "68.8479"}
{"text": "Demmel / 49 \\\\ .Subband filtering : CORDIC modulation and systolic .quadrature mirror filter tree / E. F. Deprettere / 69 .\\\\ .A parallel image rendering algorithm and architecture . based on ray tracing and radiosity shading / E. F. .", "label": "", "metadata": {}, "score": "68.86247"}
{"text": "Cramer 's Rule \\\\ .Submission \\\\ .\\\\ .Part IV .Polynomial Interpolation \\\\ .\\\\ .\\\\ .Lecture 18 .Quadratic Interpolation \\\\ .Shifting \\\\ .Polynomial Interpolation \\\\ .Lagrange Polynomials and Existence \\\\ .Uniqueness \\\\ .", "label": "", "metadata": {}, "score": "68.942566"}
{"text": "deconvolution techniques used in underwater acoustics , .this method can extract source signatures using the .outputs of a single sensor .In addition , when the .propagation is multipath and source signature .extraction is performed as part of an optimization .", "label": "", "metadata": {}, "score": "68.95523"}
{"text": "implicit algorithm are safer than the explicit . algorithm .The implicit algorithm requires somewhat .less computation , but the new algorithm will give . faster convergence in some cases , an important .consideration when eigenvectors are being calculated .", "label": "", "metadata": {}, "score": "68.97236"}
{"text": "Stewart \" , . approximations to sparse matrices \" , . example --- it is required to obtain a reduced rank .approximation to a sparse matrix $ A$ .Unfortunately , .the approximations based on traditional decompositions , .like the singular value and $ Q R$ decompositions , are . not in general sparse .", "label": "", "metadata": {}, "score": "68.97238"}
{"text": "A .The sensitivity of a singular vector depends on how small the gap is between its singular value and any other singular value : the smaller the gap , the more sensitive the singular vector .More details on the accuracy of computed singular values and vectors are given in the function documents and in the f08 Chapter Introduction .", "label": "", "metadata": {}, "score": "69.12131"}
{"text": "View at Google Scholar \u00b7 View at Scopus .B. Widrow and M. E. Hoff , \" Adaptive switching circuits , \" in Proceedings of the IRE Eastern Electronic Show & Convention Convention Record ( WESCON ' 60 ) , vol .", "label": "", "metadata": {}, "score": "69.13971"}
{"text": "the weighting factor .\"Pozo and Karin A. Remington and G. W. Stewart \" , .Pozo and Karin A. Remington and G. W. Stewart \" , .CO;2 - 9 \" , .Computing . \" with applications to large eigenproblems \" , . shallow - water environment is an ill - posed problem whose . difficulty is compounded by the multipath nature of the . propagation operator .", "label": "", "metadata": {}, "score": "69.139854"}
{"text": "A . be a square matrix of order .n .The standard eigenvalue problem is to find eigenvalues , . , and corresponding eigenvectors , .x . , such that .A .x .x . . .( 1 ) .", "label": "", "metadata": {}, "score": "69.189095"}
{"text": "State Space Decomposition for Large Markov Chains / / .587 \\\\ .Aggregation / Disaggregation Method on Parallel Computer ./ / 591 \\\\ .Parallel Implementation of the GTH Algorithm for Markov .Chains / / 594 \\\\ .A Parallel Implementation of the Block - GTH algorithm / . / 597 \\\\ .", "label": "", "metadata": {}, "score": "69.20049"}
{"text": "R. J. Paul / Visual simulation : Seeing is believing ?422 - -432 \\\\ .M. B. Silberholz , B. L. Golden , E. K. Baker / .Simulating a marine container terminal on the Macintosh .II / 433 \\\\ .", "label": "", "metadata": {}, "score": "69.230064"}
{"text": "/ 681 \\\\ .C57 1988 \" , .Symposium in Computational Statistics held under the . auspices of the International Association for .Statistical Computing . statistics \" , .\\\\ .Parallel Linear Algebra in Statistical Computations / .G. W. Stewart / 3 \\\\ .", "label": "", "metadata": {}, "score": "69.26351"}
{"text": "integral equations / C. T. H. Baker / 473 \\\\ .Strongly elliptic boundary integral equations / W. . L. Wendland / 511 \\\\ .Collocation methods for one - dimensional Fredholm . and Volterra integral equations / H. Brunner / 563 \\\\ .", "label": "", "metadata": {}, "score": "69.39494"}
{"text": "pull / 385 - -395 \\\\ .R. K. Kincaid , K. W. Miller , S. K. Park / Locating P . mobile servers on a congested network : A simulation . analysis / 396 - -406 \\\\ .R. L. Moose , R. E. Nance / The design and development . of an analyzer for discrete event model specifications .", "label": "", "metadata": {}, "score": "69.4057"}
{"text": "Bevilacqua / 109 \\\\ .Theoretical Error Bounds and General Analysis of a Few .Lanczos - Type Algorithms / Youcef Saad / 123 \\\\ .Lanczos and Linear Systems / G. W. Stewart / 135 \\\\ .\\\\ .Plenary Presentations : Theoretical Physics and .", "label": "", "metadata": {}, "score": "69.44646"}
{"text": "Combined with first - order approximation of GSO , precise estimates of singular vectors and singular values with only small deviations from orthonormality are produced .Double deflation is clearly superior to standard deflation but inferior to first - order approximation of GSO , both with respect to orthonormality and diagonalization errors .", "label": "", "metadata": {}, "score": "69.45258"}
{"text": "217 - 288 , 2011 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .Q. Zhang , R. Plemmons , D. Kittle , D. Brady , and S. Prasad , \" Joint segmentation and reconstruction of hyperspectral data with compressed measurements , \" Applied Optics , vol .", "label": "", "metadata": {}, "score": "69.54881"}
{"text": "-norm based PCA .Bidirectional PCA [ 121 ] reduces the dimension in both column and row directions for image feature extraction .The feature dimension obtained is much less than that of two - dimensional PCA .Two - dimensional PCA can be regarded as a special bidirectional PCA .", "label": "", "metadata": {}, "score": "69.569855"}
{"text": "Q . is orthogonal .( An upper Hessenberg matrix is zero below the first subdiagonal . )The eigenvalues of .A . are obtained from the diagonal blocks of .T .The matrix .Z . of Schur vectors ( if required ) is computed as .", "label": "", "metadata": {}, "score": "69.585785"}
{"text": "If . is positive definite , then the generalized eigenvalue problem has desirable properties similar to those of the standard symmetric eigenvalue problem .The eigenvalues are all real , and the eigenvectors , while not orthogonal in the usual sense , satisfy the relations .", "label": "", "metadata": {}, "score": "69.8025"}
{"text": "No . inversions of $ B$ or its submatrices are used .The . algorithm is a generalization of the $ Q R$ algorithm , .Problems involving .powers of $ \\lambda $ are also mentioned \" , .Michigan , Ann Arbor , MI , USA \" , . eigenfunctions ; generalized matrix eigenvalue problems ; . matrix algebra ; methods ; numerical ; QZ \" , .", "label": "", "metadata": {}, "score": "69.81918"}
{"text": "Is such an algorithm known to exist ?Can such an algorithm exist ?( I am quite ignorant in these areas of computational complexity ) .I am primarily interested in the case where $ A$ is a real or complex matrix , but the case of rational matrices , or matrices over finite fields are also of interest .", "label": "", "metadata": {}, "score": "69.83453"}
{"text": "it unsuitable for real - time processing of signals with . time - varying directions of arrival .The authors develop .a new implementation of ESPRIT that has potential for .real - time processing .It is based on a rank - revealing .", "label": "", "metadata": {}, "score": "69.9116"}
{"text": "The BRD is the first step toward computing the singular value decomposition of a matrix which is one of the most important algorithms in numerical linear algebra due to its broad impact in computational science .Our primary focus is the BRD portion of the computation which can easily consume over 99 % of the time needed to obtain the singular values a .. \" ...", "label": "", "metadata": {}, "score": "69.94389"}
{"text": "Mathematical Statistics , USA ) / 303 \\\\ .\\\\ .Part III : Performance and Reliability Models \\\\ .\\\\ .Performance Analysis of the .Checkpoint - Rollback - Recovery System via Diffusion .Approximation / A. Duda ( University of Paris - Sud , ISEM , .", "label": "", "metadata": {}, "score": "70.013725"}
{"text": "In particular , performance results for different implementations of the broadcast operation are analyzed and compared on the Delta , Paragon , SP1 and CM5 . 1 Introduction For the past several years , members of the Parallel Research on Invariant Subspace Methods ( PRISM ) project have been investigating scalable parallel eigensolvers for distributed memory systems [ 1 , 3].", "label": "", "metadata": {}, "score": "70.101135"}
{"text": "A .d .e . t .d .e . t .d .e . t .d .e . t .where the column vectors . , of .are the first .principal eigenvectors of .", "label": "", "metadata": {}, "score": "70.134315"}
{"text": "In our experiment , we set the true rank of the simulated matrix to be .Figure 2 : The relationship between errors and estimated dimension of matrix size from 500 to 4000 with step size 500 .The true rank of these matrices is 50 .", "label": "", "metadata": {}, "score": "70.33274"}
{"text": "84 - 100 .\" Rejoinder \" , . participatory paper ) \" , . and parallel computing --- issues in applied research and . development ( Loen , 1986 ) \" , .Computing --- Issues in Applied Research and .", "label": "", "metadata": {}, "score": "70.42813"}
{"text": "The commands are basically scipy.linalg.svd and numpy.linalg.svd .What is the difference between these two ?Is any ... .There are many ways to reduce errors and remove noise from measurement or calculation .I am reacently dealing with linear system of equation .", "label": "", "metadata": {}, "score": "70.53208"}
{"text": "j .and can be normalized so that .z .i .T .B .z .i . 1 . . .Note that it is not enough for .A . and .B . must also be positive definite , which implies nonsingularity .", "label": "", "metadata": {}, "score": "70.63415"}
{"text": "tolerance specified by the user .The eigenvalues of $ T$ . are approximations to the $ m$ eigenvalues of largest .absolute magnitude of $ A$ , and the columns of $ Q$ span .the invariant subspace corresponding to those . eigenvalues .", "label": "", "metadata": {}, "score": "70.68364"}
{"text": "lu_decomp is ported from Numerical Recipes to PDL .It should probably be implemented in C. .lu_decomp2 .Signature : ( a(m , m ) ; [ o]lu(m , m ) ) .LU decompose a matrix , with no row permutation .", "label": "", "metadata": {}, "score": "70.71927"}
{"text": "Applications : Proceedings of the NATO Advanced Study .Applications : Proceedings of the NATO Advanced Study .Series E , Applied sciences \" , . scale and real - time matrix computations ; these .computations arise in a variety of fields , such as . computer graphics , imaging , speech and image .", "label": "", "metadata": {}, "score": "70.8162"}
{"text": "Lecture 10 .Theory of Linear Systems \\\\ .Computational Generalities \\\\ .Triangular Systems \\\\ .Operation Counts \\\\ .\\\\ .Lecture 11 .Memory Considerations \\\\ .Row Oriented Algorithms \\\\ .A Column Oriented Algorithm \\\\ .General Observations on Row and Column Orientation \\\\ .", "label": "", "metadata": {}, "score": "70.81815"}
{"text": "Kahan / 165 \\\\ .Recent developments in linear and quadratic .programming / R. Fletcher / 213 \\\\ .Solving systems of nonlinear equations by tensor .methods / R. B. Schnabel and P. D. Frank / 245 \\\\ .Numerical methods for bifurcation problems / A. D. .", "label": "", "metadata": {}, "score": "70.8242"}
{"text": "Minor Component Analysis .MCA , as a variant of PCA , is to find the smallest eigenvalues and their corresponding eigenvectors of the autocorrelation matrix . of the signals .MCA is closely associated with the curve and surface fitting under the total least squares ( TLSs ) criterion [ 72 ] .", "label": "", "metadata": {}, "score": "70.874214"}
{"text": "These routines should work OK with PDL::Matrix objects as well as with normal PDLs .TIPS ON MATRIX OPERATIONS .Like most computer languages , PDL addresses matrices in ( column , row ) order in most cases ; this corresponds to ( X , Y ) coordinates in the matrix itself , counting rightwards and downwards from the upper left corner .", "label": "", "metadata": {}, "score": "70.90442"}
{"text": "In contrast , in PCA the dimensionality reduction is achieved by removing those dimensions that have a low variance .Let a .-vector . denote a linear mixture and a .-vector . , whose components have zero mean and are statistically mutually independent , denote the original source signals .", "label": "", "metadata": {}, "score": "70.91376"}
{"text": "The threshold effect in signal processing algorithms . which use an estimated subspace / D. W. Tufts et al ./ .\\\\ .\\\\ .Part IV \\\\ .\\\\ .Applications to signal modeling and detection OSVD and .QSVD in signal separation / D. Callaerts et al .", "label": "", "metadata": {}, "score": "71.02567"}
{"text": "Principal component analysis ( PCA ) is a well - known orthogonal transform that is used for dimensionality reduction .Another popular technique for feature extraction is linear discriminant analysis ( LDA ) , also known as Fisher 's discriminant analysis [ 2 , 3 ] .", "label": "", "metadata": {}, "score": "71.07805"}
{"text": "MD , USA \" , . decomposition ; Markov chain ; Markov processes ; nearly .uncoupled Markov chains ; nearly uncoupled system ; nla ; . plain Gaussian elimination ; rounding errors \" , .Numbers of a Multiple Eigenvalue \" , .", "label": "", "metadata": {}, "score": "71.15248"}
{"text": "Ind. Appl .Math . Rev. . disks \" , .Accept Difference Approximations to Derivatives \" , . heat transfer problem \" , . resulting from a discrete approximation to a thin plate . heat transfer problem is considered .The slow . convergence of point iterative methods is analyzed and . shown to be caused by one of the boundary conditions .", "label": "", "metadata": {}, "score": "71.155045"}
{"text": "Assembly / Gary Hachtel / 349 \\\\ .A Capacitance Matrix Technique / B. L. Buzbee / 365 .\\\\ .M - Matrix Theory and Recent Results in Numerical .Linear Algebra / Richard S. Varga / 375 \\\\ .VI .", "label": "", "metadata": {}, "score": "71.17359"}
{"text": "Markov Chains ( 2nd : 1995 : Raleigh , NC ) \" , .Raleigh , NC \" , .Detecting Block GI / M/1 and Block M / G/1 Matrices .from Model Specifications / S. Berson and R. Muntz / 1 .", "label": "", "metadata": {}, "score": "71.193756"}
{"text": "Krylov Subspace Methods for the Eigenproblem \\\\ .10.1 A Krylov / Schur Algorithm \\\\ .10.2 Backward Error Analysis of Krylov Subspace Methods .\\\\ .10.3 Adjusting the Rayleigh Quotient in Lanczos Methods .\\\\ .10.4 Impact \\\\ .", "label": "", "metadata": {}, "score": "71.28545"}
{"text": "/ 589 \\\\ .Open Questions in Particle Theory / / 603 \\\\ .Supercollider Physics / / 621 \\\\ .the IMA Workshop on Iterative Methods for Sparse and .Structured Problems , held in Minneapolis , Minnesota , .the IMA Workshop on Iterative Methods for Sparse and .", "label": "", "metadata": {}, "score": "71.285545"}
{"text": "Lanczos and the FFT : A Discovery Before its Time / .James W. Cooley / 3 \\\\ .Lanczos Algorithms for Large Scale Symmetric and .Nonsymmetric Matrix Eigenvalue Problems / Jane K. .Cullum / 11 \\\\ .The Look - Ahead Lanczos Process for Nonsymmetric .", "label": "", "metadata": {}, "score": "71.29215"}
{"text": "Unfortunately , its . computational burden makes it unsuitable for real time . processing of signals with time - varying directions of . arrival .The authors develop a new implementation of .ESPRIT that has potential for real time processing .", "label": "", "metadata": {}, "score": "71.37144"}
{"text": "In this paper we examine the source and .consequences of this growth . \" null space is oblique to its range , in contrast to an . orthogonal projector , whose null space is orthogonal to . its range .Oblique projectors arise naturally in many . applications and have a substantial literature .", "label": "", "metadata": {}, "score": "71.44952"}
{"text": "varying total least squares problems / S. van huffel / .\\\\ .Combined Jacobi - type algorithms in signal processing / .M. Moonan et al ./ \\\\ .A modified non - symmetric Lanczos algorithm and .applications / D. Boley , G. Golub / \\\\ .", "label": "", "metadata": {}, "score": "71.52373"}
{"text": "Teitelboim / 223 \\\\ .Variational Principles , Local Symmetries , and Black .Hole Entropy / Robert M. Wald / 231 \\\\ .\\\\ .Mathematics Minisymposia \\\\ .\\\\ .Eigenvalue Computations : Theory and Algorithms / / 241 .", "label": "", "metadata": {}, "score": "71.533905"}
{"text": "S. Zenios , S. S. Nielsen , M. Pinar / On the use of .advance architecture computers via high - level modelling .languages / 507 \\\\ .\\\\ .IX .Telecommunications \\\\ .\\\\ . A. J. Perticone , J. P. Jarvis , D. R. Shier / Evaluation .", "label": "", "metadata": {}, "score": "71.57398"}
{"text": "If . x . is an eigenvector , then so is . k .x . where . k . is any nonzero scalar .Eigenvectors computed by different algorithms , or on different computers , may appear to disagree completely , though in fact they differ only by a scalar factor ( which may be complex ) .", "label": "", "metadata": {}, "score": "71.604645"}
{"text": "Edge Reflections \\\\ .Conclusions \\\\ .References \\\\ .Chapter 7 .Variable Coefficient Multistep Methods for .Ordinary Differential Equations applied to Parabolic .Partial Differential Equations \\\\ .Semi - discretization \\\\ .Variable Coefficient Multistep Methods ( VCMM ) \\\\ .", "label": "", "metadata": {}, "score": "71.62207"}
{"text": "Cliff H. Spiegelman \\\\ .Perturbation theory and least squares with errors in .the variables / G. W. Stewart \\\\ .Orthogonal distance regression / Paul T. Boggs and .Janet E. Rogers \\\\ .Computing error bounds for regression problems / .", "label": "", "metadata": {}, "score": "71.62782"}
{"text": "\\\\ .Modelling the Execution of Block Structured .Processes with Hardware and Software Failures / E. .Computing Lab . , England ) / 329 \\\\ .\\\\ .Trustable Evaluation of Computer Systems .Dependability / J.-C. Laprie ( LAAS , CNRS , France ) / 341 .", "label": "", "metadata": {}, "score": "71.72626"}
{"text": "after each operation .The number can be changed . dynamically .Flap is intended to make it easy to .generate examples of the effects of rounding error for .classroom use . \" Method \" , .Arnoldi method in which orthogonalization is done with .", "label": "", "metadata": {}, "score": "71.74722"}
{"text": "multidimensionnel ; traitement parole ; traitement signal .filigrane ; indexation video ; analyse image ; . algorithme traitement image ; Traitement du signal ; .DSP ; signal multidimensionnel ; traitement parole ; . video ; analyse image ; authentification ; codage .", "label": "", "metadata": {}, "score": "71.78325"}
{"text": "Decomposition \" , .Decomposition \" , . at the Origin \" , .Decomposition \" , . mathematicians --- Eugenio Beltrami ( 1835 - 1899 ) , .Camille Jordan ( 1838 - 1921 ) , James Joseph Sylvester .( 1814 - 1897 ) , Erhard Schmidt ( 1876 - 1959 ) , and Hermann .", "label": "", "metadata": {}, "score": "71.82094"}
{"text": "\\\\ .Table of Contents \\\\ .Chapter 1 .\\\\ .Section 1 \\\\ .Section 2 \\\\ .Section 3 \\\\ .Section 4 \\\\ .Section 5 \\\\ .References \\\\ .Chapter 2 .Some Applications of Approximation Theory to .", "label": "", "metadata": {}, "score": "71.9049"}
{"text": "Chebyshev Polynomials / / 357 \\\\ .Lanczos Methods in Control and Signal Processing / / .375 \\\\ .Development of the FFT / / 393 \\\\ .The FFT in Signal Processing / / 399 \\\\ .Wavelets / / 411 \\\\ .", "label": "", "metadata": {}, "score": "72.013306"}
{"text": "Updating and Downdating Matrix Decompositions \\\\ . 5.1 Solving Nonlinear Systems of Equations \\\\ .5.2 More General Update Formulas for $ QR$ \\\\ .5.3 Effects of Rounding Error on Downdating Cholesky .Factorizations \\\\ .5.4 Stability of a Sequence of Updates and Downdates .", "label": "", "metadata": {}, "score": "72.05386"}
{"text": "The Lena picture and its restored version are shown in Figure 4 .The trained network is then used to compress the kid picture .Both the kid picture and its restored version are shown in Figure 5 .Summary .In this paper , we have discussed various neural network implementations and algorithms for PCA and its various extensions , including PCA , MCA , generalized EVD , constrained PCA , two - dimensional methods , localized methods , complex - domain methods , and SVD .", "label": "", "metadata": {}, "score": "72.088554"}
{"text": "for Finding a Zero of a Polynomial \" , . for Symmetric Tridiagonal Matrices \" , .symmetric tridiagonal matrix can be accelerated by . incorporating a sequence of origin shifts .The origin .shift may be either subtracted directly from the . diagonal elements of the matrix or incorporated by .", "label": "", "metadata": {}, "score": "72.10957"}
{"text": "factorization ; rounding errors ; unconditionally stable ; . updating \" , .Sci . , Maryland Univ . , College .Park , MD , USA \" , .USA \" , .Y.-J. J. Wu \" , . decomposition \" , . directions of arrival of a set of narrow - band signals . at an array of sensors .", "label": "", "metadata": {}, "score": "72.12859"}
{"text": "Null Space Decomposition \" , . % % % Part 2 ( of 2 ) --- publications about G. W. Stewart and his works . % % % Bibliography entries , sorted by year , and then by citation label , . % % % with ' ' bibsort -byyear ' ' : .", "label": "", "metadata": {}, "score": "72.14052"}
{"text": "An Aggregation Principle for Computing Invariant .Probability Vectors for Semi - Markovian Models / R. .Schassberger ( Technical University of Berlin , Dept . of .Mathematics , W. Germany ) / 259 \\\\ .\\\\ .Aggregation Methods for Large Markov Chains / P. .", "label": "", "metadata": {}, "score": "72.15738"}
{"text": "Stuttgart , Inst . of Switching and Data Technics , W. .Germany ) / 167 \\\\ .\\\\ .Multi - Armed Bandit Problems and Resource Sharing .Systems / P. Varaiya and J. Walrand ( University of .California , Berkeley , Dept . of Computer Sciences , USA ) .", "label": "", "metadata": {}, "score": "72.21302"}
{"text": "Even if eigenvectors .x . are normalized so that .x . , this is not sufficient to fix them uniquely , since they can still be multiplied by a scalar factor . k . such that . k .To counteract this inconvenience , most of the functions in this chapter , and in Chapter f08 , normalize eigenvectors ( and Schur vectors ) so that .", "label": "", "metadata": {}, "score": "72.299805"}
{"text": "Software \\\\ . vol .Mathematical aspects of information processing .\\\\ . vol .Technological and scientific applications .Applications in the social sciences and humanities \\\\ . vol .Royal Irish Academy Conference on Numerical Analysis , .Royal Irish Academy Conference on Numerical Analysis , .", "label": "", "metadata": {}, "score": "72.32775"}
{"text": "\\\\ .B. Symmetric $ ( A - A B)$-Pencils and Applications / / .145 \\\\ .\\\\ . problem in ship design and offshore industry --- a . comparison of traditional methods with the Lanczos .process / 146 \\\\ . E. G. Carnoy , M. Geradin / On the practical use of the .", "label": "", "metadata": {}, "score": "72.32875"}
{"text": "$ m$ columns , the number of passes is reduced by a . factor of $ 1 / m$. Moreover , matrix - vector products . are converted into matrix - matrix products , allowing .level-3 BLAS cache performance .In this paper we derive . such a block algorithm and give some experimental .", "label": "", "metadata": {}, "score": "72.43416"}
{"text": "and Signal Processing , held at the University of Rhode .Island , June 25 - -27 , 1990 , sponsored by the URI College . of Engineering .( Mathematics ) \" , .\\\\ .The SVD and reduced - rank signal processing / L. L. .", "label": "", "metadata": {}, "score": "72.44412"}
{"text": "the casual user of LINPACK who simply requires a . library subroutine , and the specialist who wishes to . modify or extend the code to handle special problems .It is also recommended for classroom work .lud , lud , math , nla , qrd , qrd , soft , software , survey , . svd , svd , upd , updating \" , .", "label": "", "metadata": {}, "score": "72.46115"}
{"text": "Determinant of a square matrix using LU decomposition ( for large matrices ) .You feed in a square matrix , you get back the determinant .Some options exist that allow you to cache the LU decomposition of the matrix ( note that the LU decomposition is invalid if the determinant is zero ! )", "label": "", "metadata": {}, "score": "72.46712"}
{"text": "\\\\ .Graphical Techniques \\\\ .Graphical Modelling with Large Numbers of Variables : An .Application of Principal Components / J. Whittaker , A. .Iliakopoulos , and P. W. F. Smith / 73 \\\\ .Some Graphical Displays for Square Tables / A. de .", "label": "", "metadata": {}, "score": "72.481674"}
{"text": "% % % bibliography archives , from the ACM Portal , . % % % AMS MathSciNet , EMS zbMATH , and IEEE eXplore . % % % databases , and from the University of .% % % Maryland Institute for Advanced Computer . % % % Studies ( UMIACS ) publication list at . % % % Additional data , including the tables of .", "label": "", "metadata": {}, "score": "72.493"}
{"text": "Here are some of the more likely reasons why you may need to do this : .Your problem is already in one of the reduced forms - for example , your symmetric matrix is already tridiagonal .You wish to economize on storage for symmetric matrices ( see Section 3.3 ) .", "label": "", "metadata": {}, "score": "72.62571"}
{"text": "FORTRAN SUBROUTINES ; LINEAR SYSTEMS ; LINPACK ; .MATHEMATICAL TECHNIQUES --- Linear Algebra ; .Uncoupled Stochastic Matrices \" , .Rayleigh -- Ritz method , aggregation \" , .JSTOR database ; Theory / Matrix . bib \" , . column scaling \" , . C4130 ( Interpolation and function approximation ) \" , .", "label": "", "metadata": {}, "score": "72.72423"}
{"text": "This observation is not true in general , and . counterexamples are easy to construct .However , it is . often true of the triangular matrices from pivoted $ L .U $ or $ Q R $ decompositions .It is shown that this .", "label": "", "metadata": {}, "score": "72.78496"}
{"text": "View at Scopus .View at Scopus \" ...We discuss an inverse - free , highly parallel , spectral divide and conquer algorithm .This algorithm is based on earlier ones of Bulgakov , Godunov ... \" .We discuss an inverse - free , highly parallel , spectral divide and conquer algorithm .", "label": "", "metadata": {}, "score": "72.788765"}
{"text": "eigens .Signature : ( [ phys]a(m ) ; [ o , phys]ev(l , n , n ) ; [ o , phys]e(l , n ) ) .Real eigenvalues and -vectors of a real square matrix .( See also \" eigens_sym \" , for eigenvalues and -vectors of a real , symmetric , square matrix ) .", "label": "", "metadata": {}, "score": "72.79902"}
{"text": "Experiments with Probabilistic Consultation Systems / .S. Nordbotten / 181 \\\\ .Statistical Consultants and Statistical Expert Systems ./ I. W. Molenaar / 187 \\\\ .On Inference Process / T. Westerhoff and P. Naeve / 193 .\\\\ .", "label": "", "metadata": {}, "score": "72.81185"}
{"text": "Transient Colutions of Markov Processes by Krylov .Subspaces / R. B. Sidje and B. Philippe / 95 \\\\ .Exact Methods for the Transient Analysis of .Nonhomogeneous Continuous Time Markov Chains / A. .Rindos , S. Woolet , I. Viniotis and K. Trivedi / 121 \\\\ .", "label": "", "metadata": {}, "score": "72.85013"}
{"text": "perspective . \"Least squares ; Error analysis ( Mathematics ) ; Least . squares . \"\\\\ .Random and regular errors in observations \\\\ .Regular errors excluded \\\\ .Their treatment \\\\ .General properties of random errors \\\\ .", "label": "", "metadata": {}, "score": "72.99879"}
{"text": "Developments in stability theory for ordinary .differential equations / J. D. Lambert / 409 \\\\ .Stiff ODE initial value problems and their solution ./ A. R. Curtis / 433 \\\\ .Order stars and stability / G. Wanner / 451 \\\\ .", "label": "", "metadata": {}, "score": "73.03009"}
{"text": "( hardcover ) , 978 - 0 - 7803 - 0005 - 7 ( microfiche ) \" , .S1 : Speech processing 1 \\\\ . vol .S2 , VLSI , U : Speech processing 2 , VLSI , .Underwater signal processing \\\\ . vol .", "label": "", "metadata": {}, "score": "73.17281"}
{"text": "Equation ( 19 ) is the Hebbian part , and ( 20 ) the anti - Hebbian part . tends to be orthogonal to all the previous components due to the anti - Hebbian rule , also called orthogonalization rule .APEX can also be derived from the RLS method using the MSE criterion .", "label": "", "metadata": {}, "score": "73.19482"}
{"text": "I have 900 unlabeled test examples .I used clustering(using only unlabeled test data ) incorporating SVD .But the results ...To repeat the question , let $ A$ be a square matrix .We wish to determine if $ A$ is nonsingular , that is , invertible .", "label": "", "metadata": {}, "score": "73.26114"}
{"text": "Though for upper triangular matrix , we can find conditional no . in $ O(n)$. For tridiagonal matrix it can be found in $ O(n ) .For other matrices either first perform QR , then use 1 , or try to find the singular values for the matrix as condition number is the ratio of largest singular value to smallest .", "label": "", "metadata": {}, "score": "73.26175"}
{"text": "\\\\ . A.2Aspects from Differential Equations / / 74 \\\\ .\\\\ .C. W. Gear , L. R. Petzold / Differential / algebraic . systems and matrix pencils / 75 \\\\ .K - M. Liu , E. L. Ortiz / Approximation of eigenvalues . defined by ordinary differential equations with the Tau .", "label": "", "metadata": {}, "score": "73.44618"}
{"text": "JSTOR database ; Theory / Matrix . bib \" , .Zeros \" , .JSTOR database \" , .JSTOR database ; Theory / Matrix . bib \" , . algebra ; pivot elements ; pivoting ; rank one .modification ; rounding error ; sparseness ; stable \" , . at Isolated Extreme Points in the Spectrum \" , .", "label": "", "metadata": {}, "score": "73.56329"}
{"text": "We will show that our method works well for this type of matrices .Figure 3 shows the error versus estimated rank , where the error is defined as ( 25 ) , which is a comparison of the orthogonality between .", "label": "", "metadata": {}, "score": "73.56997"}
{"text": "[ The theory of the combination of observations least . subject to error ] ] by Carl Friedrich Gauss ; G. W. . overflow and harmful underflow in complex division .The .method guarantees that no overflow will occur unless at .", "label": "", "metadata": {}, "score": "73.58367"}
{"text": "Conditions are derived .under which the algorithm computes a stable solution .The algorithm is shown to be stable for block . diagonally dominant matrices and for M - matrices . \"( Programming and algorithm theory ) \" , . dominant ; block Hessenberg ; block Hessenberg system ; . divide and conquer methods ; divide - and - conquer ; linear ; . linear system solution ; linear systems ; M - matrices ; . matrices ; matrix algebra ; rounding error analysis ; . rounding errors ; solution ; stable solution \" , .", "label": "", "metadata": {}, "score": "73.63968"}
{"text": "Projection \" , .( Computational techniques ) ; C4140 ( Linear algebra ) \" , .Householder -- Fox algorithm ; matrix algebra ; orthonormal . basis for ; projection range ; rounding error \" , .Problem \" , .", "label": "", "metadata": {}, "score": "73.65643"}
{"text": "J. D. Horel , \" Complex principal component analysis : theory and examples , \" Journal of Climate and Applied Meteorology , vol .23 , no .12 , pp .1660 - 1673 , 1984 .View at Google Scholar \u00b7 View at Scopus .", "label": "", "metadata": {}, "score": "73.73021"}
{"text": "For practical purposes , an error rate in the order of 0.001 might be sufficient , and this would result in a compression ratio of 2.5 to 4 .For comparison purpose , the 3D - SPECK [ 7 ] on a small dataset of size .", "label": "", "metadata": {}, "score": "73.754715"}
{"text": "If the matrix is symmetric , the same underlying code as \" eigens_sym \" is used .If asymmetric , the eigenvalues and eigenvectors are computed with algorithms from the sslib library .If any imaginary components exist in the eigenvalues , the results are currently considered to be invalid , and such eigenvalues are returned as \" NaN\"s .", "label": "", "metadata": {}, "score": "73.83782"}
{"text": "Chapter 11 .A New Approach to Matrix Theory or Many .Symposium on Sparse Matrix Computations at Argonne .Symposium on Sparse Matrix Computations at Argonne .S989 1975 \" , .Preface / / xi \\\\ .I. Design and Analysis of Elimination Algorithms / / 1 .", "label": "", "metadata": {}, "score": "73.87555"}
{"text": "Lecture 2 .The space $ C[0,1]$ \\\\ .Existence of best approximations \\\\ .Uniqueness of best approximations \\\\ .Convergence in $ C[0,1]$ \\\\ .The Weierstrass approximation theorem \\\\ .Bernstein polynomials \\\\ .Comments \\\\ .Lecture 3 .", "label": "", "metadata": {}, "score": "73.978325"}
{"text": "On Combining the Schemes of Reid and Saunders for .( e - book ) \" , .978 - 3 - 540 - 39447 - 1 ( e - book ) \" , .L28 no .973 \" , .", "label": "", "metadata": {}, "score": "74.00343"}
{"text": "Norms and Limits / 161 \\\\ .Matrix Norms / 173 \\\\ .Inverses of Perturbed Matrices / 184 \\\\ .The Accuracy of Solutions of Linear Systems / 192 .\\\\ .Iterative Refinement of Approximate Solutions of .Linear Systems / 200 \\\\ .", "label": "", "metadata": {}, "score": "74.0073"}
{"text": "That is if there are imaginary components to any of the values in the eigenvector , the eigenvalue and corresponding eigenvectors are all set to \" NaN \" .Finally , if there are any repeated eigenvectors , they are replaced with all \" NaN\"s . .", "label": "", "metadata": {}, "score": "74.04584"}
{"text": "Jane Cullum and Ralph A. Willoughby / 220 \\\\ .Systolic Arrays ( for VLSI ) / H. T. Kung and Charles E. .Leiserson / 256 \\\\ .A Basis Factorization Method for Block Triangular .Linear Programs / Andre F. Perold and George B. Dantzig .", "label": "", "metadata": {}, "score": "74.21628"}
{"text": "Eigenvalues and Eigenvectors / 262 \\\\ .Reduction of Matrices by Similarity Transformations ./ 275 \\\\ .The Sensitivity of Eigenvalues and Eigenvectors / .289 \\\\ .Hermitian Matrices / 307 \\\\ .The Singular Value Decomposition / 317 \\\\ .", "label": "", "metadata": {}, "score": "74.26193"}
{"text": "necessarily restricted the scope of the series , but the .selection of topics should give the reader a sound . basis for further study . \"Notation \\\\ .Preface \\\\ .Matrices , algebra , and analysis \\\\ .Matrices and machines \\\\ .", "label": "", "metadata": {}, "score": "74.273926"}
{"text": "Figure 3 : Architecture of the cross - correlation APCA network .The APCA network is composed of two hierarchical PCA networks .The connections with solid arrows denote feedforward connections , and the connections with hollow arrows denote lateral connections .", "label": "", "metadata": {}, "score": "74.33044"}
{"text": "JSTOR database ; Theory / Matrix . bib \" , .Analysis \" , . polynomial 1 \\\\ .Introduction / 1 \\\\ .Lehmer 's Method / 2 \\\\ .Rounding Error / 11 \\\\ .Error Analysis of the Shifting Algorithm / 21 \\\\ .", "label": "", "metadata": {}, "score": "74.49749"}
{"text": "Accuracy of the Refined Eigenvalues / 58 \\\\ .Accuracy of the Refined Eigenvectors / 65 \\\\ . computing ) \" , .Texas , Austin , TX , USA \" , .eigenvectors ; matrix algebra ; QR algorithm ; real .", "label": "", "metadata": {}, "score": "74.56705"}
{"text": "Rounding Error and Gaussian Elimination \\\\ .Comments on the Analysis \\\\ .\\\\ .Lecture 17 .The Wonderful Residual : A Project \\\\ .Introduction \\\\ .More on Norms \\\\ .The Wonderful Residual \\\\ .Matrices with Known Condition \\\\ .", "label": "", "metadata": {}, "score": "74.606"}
{"text": "45 , no .12 , pp .4187 - 4193 , 2007 .View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus .G. H. Golub and C. F. V. Loan , Matrix Computations , The Johns Hopkins University Press , 3rd edition , 1996 .", "label": "", "metadata": {}, "score": "74.67589"}
{"text": "where . is an unknown constant full - rank . mixing matrix and .denotes the additive noise term , which is often omitted since it is usually impossible to separate noise from the sources .ICA takes one of three forms , namely , square ICA for . , overcomplete ICA for . , and undercomplete ICA for .", "label": "", "metadata": {}, "score": "74.72212"}
{"text": "The generalized eigenvalue problem can be solved via the generalized Schur factorization of .All the above remarks also apply - with the obvious changes - to the case when .A . are complex matrices .The eigenvalues are in general complex , so there is no need for special treatment of complex conjugate pairs , and the matrix .", "label": "", "metadata": {}, "score": "74.745865"}
{"text": "Vol . 2 , Signal processing theory and methods , audio and .electro - acoustics , multimedia signal processing \\\\ .Vol . 3 , Signal processing for communications , sensor . array and multichannel signal processing , design and . implementation of signal processing systems \\\\ .", "label": "", "metadata": {}, "score": "74.75131"}
{"text": "I have been using C # , using ... .I am testing the effect different number of cores per executors ( --executor - cores ) has on the run - time for SVD on Spark .With the --executor - cores fixed the number of partitions of the main data RDD ... .", "label": "", "metadata": {}, "score": "74.8553"}
{"text": "Algebra , eigenvalues .Computing , MATHEMATICAL SOFTWARE , certification and . testing . \" Stewart \" , .Factorization \" , .Stewart \" , .Factorization \" , . chain whose subdominant eigenvalue does not predict the . decay of its transient \" , .", "label": "", "metadata": {}, "score": "75.08614"}
{"text": "View at Google Scholar \u00b7 View at Scopus .C. Chatterjee , V. P. Roychowdhury , and E. K. P. Chong , \" On relative convergence properties of principal component analysis algorithms , \" IEEE Transactions on Neural Networks , vol .", "label": "", "metadata": {}, "score": "75.16316"}
{"text": "is the diagonal matrix whose diagonal elements are the eigenvalues , and .All the above remarks also apply - with the obvious changes - to the case when .A . are complex Hermitian matrices .Such problems are called Hermitian - definite .", "label": "", "metadata": {}, "score": "75.18088"}
{"text": "This is produced by Robert . % % % Solovay 's checksum utility . \" # \" \\input path.sty \" .% % % Acknowledgement abbreviations : .University of Utah , .Department of Mathematics , 110 LCB , .155 S 1400 E RM 233 , .", "label": "", "metadata": {}, "score": "75.23804"}
{"text": "\\\\ .References \\\\ .Addenda \\\\ .Chapter 6 .Application of Finite Difference Methods to .Exploration Seismology \\\\ .Introduction \\\\ .Formulation of Seismic Exploration Model \\\\ .Finite - Difference Models \\\\ .Grid Dispersion \\\\ .", "label": "", "metadata": {}, "score": "75.2816"}
{"text": "( Philips Research , Brussels , Belgium ) / 209 \\\\ .\\\\ .A Perspective on Iterative Methods for the .Approximate Analysis of Closed Queueing Networks / E. .de Souza e Silva , S. S. Lavenberg and R. R. Muntz .", "label": "", "metadata": {}, "score": "75.311325"}
{"text": "Systems / R. W. Payne / 199 \\\\ .\\\\ .Languages and Packages \\\\ .Adding new Statistical Techniques to Standard Software .Systems : A Review / C. Payne , N. Cleave , and P. Brown . '/ 207 \\\\ .", "label": "", "metadata": {}, "score": "75.44043"}
{"text": "/ Christian H. Reinsch / 267 \\\\ .Experiences with some software engineering . practices in numerical software / D. A. H. Jacobs and .G. Markham / 277 \\\\ .Evolution of numerical software for dense linear .algebra / Jack Dongarra and Sven Hammarling / 297 \\\\ .", "label": "", "metadata": {}, "score": "75.6252"}
{"text": "programming software for the microcomputer : Recent .advances , comparisons and trends / 263 - -272 \\\\ . E. Wasil , A. Assad / Project management software for .the microcomputer : Recent advances and future . directions / 273 - -288 \\\\ .", "label": "", "metadata": {}, "score": "75.73451"}
{"text": "Relative Error \\\\ .\\\\ .Lecture 2 .Newton 's Method \\\\ .Reciprocals and Square Roots \\\\ .Local Convergence Analysis \\\\ .Slow Death \\\\ .\\\\ .Lecture 3 .A Quasi - Newton Method \\\\ .Rates of Convergence \\\\ .", "label": "", "metadata": {}, "score": "75.76991"}
{"text": "Inference / B. Efron .( See section ' Late Arrivals ' pp .423 - -431 ) / \\\\ .\\\\ .Non - Parametric Estimation \\\\ .\\\\ .Efficient Nonparametric Smoothing in High Dimensions .( Invited paper ) / 17 \\\\ .", "label": "", "metadata": {}, "score": "75.81366"}
{"text": "decomposable Markov chains / G. W. Stewart , W. J. .Stewart and D. F. McAllister / 201 \\\\ .Minimum residual modifications to Bi - CG and to the .Alistair ) Watson \" , .D85 1993 \" , . library.ox.ac.uk:210/ADVANCE", "label": "", "metadata": {}, "score": "75.82275"}
{"text": "This order three computational cost makes many modern applications infeasible , especially when the scale of the data is huge and growing .Therefore , it is imperative to develop a fast SVD method in modern era .If the rank of matrix is much smaller than the matrix size , there are already some fast SVD approaches .", "label": "", "metadata": {}, "score": "75.85283"}
{"text": "Algorithms have two costs : arithmetic and communication .The latter represents the cost of moving data , either between levels of a memory hierarchy , or between processors over a network .Communication often dominates arithmetic and represents a rapidly increasing proportion of the total cost , so we ... \" .", "label": "", "metadata": {}, "score": "75.88638"}
{"text": "The $ QR$ Algorithm \\\\ .\\\\ .Reduction to Hessenberg and Tridiagonal Forms / 328 .\\\\ .The Power and Inverse Power Methods / 340 \\\\ .The Explicitly Shifted $ QR$ Algorithm / 351 \\\\ .The Implicitly Shifted $ QR$ Algorithm / 368 \\\\ .", "label": "", "metadata": {}, "score": "75.90207"}
{"text": "But since I 'm new to MATLAB I do n't know ... .I 'm working on watermarking and recently I try to developp the method found here for a blind color watermarking .For embedding the watermark every is Ok I got great results but in extraction part I ... .", "label": "", "metadata": {}, "score": "75.92853"}
{"text": "The SLATEC common mathematical library / Bill L. Buzbee .\\\\ .The Boeing mathematical software library / A. H. .Erisman , K. W. Neves , and I. R. Philips \\\\ .The PORT mathematical subroutine Library / Phyllis Fox .", "label": "", "metadata": {}, "score": "75.935715"}
{"text": "svd ignores the bad - value flag of the input piddles .It will set the bad - value flag of all output piddles if the flag is set for any of the input piddles .lu_decomp .Signature : ( a(m , m ) ; [ o]lu(m , m ) ; [ o]perm(m ) ; [ o]parity ) .", "label": "", "metadata": {}, "score": "76.00127"}
{"text": "See lu_decomp for more information about LU decomposition .lu_decomp2 is ported from Numerical Recipes into PDL .lu_backsub .Signature : ( lu(m , m ) ; perm(m ) ; b(m ) ) .It is separated from the lu_decomp method so that you can call the cheap lu_backsub multiple times and not have to do the expensive LU decomposition more than once .", "label": "", "metadata": {}, "score": "76.02438"}
{"text": "Triangular Matrices and Systems / 106 \\\\ .Gaussian Elimination / 113 \\\\ .Triangular Decomposition / 131 \\\\ .The Solution of Linear Systems / 144 \\\\ .The Effects of Rounding Error / 148 \\\\ .\\\\ .Norms , Limits , and Condition Numbers \\\\ .", "label": "", "metadata": {}, "score": "76.03134"}
{"text": "Humboldt State University , Arcata , California . \" relationships / Peter Sprent \\\\ .Errors - in - variables regression problems in epidemiology ./ Alice S. Whittemore \\\\ .Models with latent variables : LISREL versus PLS / Hans .", "label": "", "metadata": {}, "score": "76.05791"}
{"text": "This workshop was sponsored by the .Mathematical Sciences Division of the United States .Army Research Office and the Department of Computer .Science . \" applications : proceedings of the AMS - IMS - SIAM joint .summer research conference held June 10 - 16 , 1989 , with .", "label": "", "metadata": {}, "score": "76.1575"}
{"text": "Qiaochu Yuan Nov 18 ' 12 at 5:07 .Note that over the field of two elements , computing the determinant is equivalent to determining singularity .-Gerry Myerson Nov 18 ' 12 at 5:11 .Of course .When I made my original comment , I was n't paying attention to who posted the question , so I was n't assuming awareness of that fact : in my experience many undergraduates know only cofactors .", "label": "", "metadata": {}, "score": "76.262344"}
{"text": "Cauchy Problem of General Relativity / / 527 \\\\ .Black Hole Evaporation and Thermodynamics / / 543 \\\\ .The Problem of Time in Quantum Gravity / / 555 \\\\ .New Variables and Loop Quantization / / 571 \\\\ .", "label": "", "metadata": {}, "score": "76.26416"}
{"text": "GB memory .From the computed singular values and vectors , we observed that the singular vectors after the ninth singular vector all appear to be noise , indicating that the data matrix does have a low - rank representation .Figure 8 shows the results for a small scene from the large dataset described above , consisting of part of the University of Southern Mississippi Campus , extracted from the large Gulfport MS dataset .", "label": "", "metadata": {}, "score": "76.30643"}
{"text": "Legendre and the priority controversy \\\\ .Beginnings : Mayer , Boscovich and Laplace \\\\ .Gauss and Laplace \\\\ .The theoria motus \\\\ .Laplace and the central limit theorem \\\\ .The Theoria Combinationis Observationum \\\\ .The precision of observations \\\\ .", "label": "", "metadata": {}, "score": "76.32882"}
{"text": "2.3.2 Generalized nonsymmetric eigenvalue problems .Any generalized eigenproblem which is not symmetric - definite with well - conditioned .B . must be handled as if it were a general nonsymmetric problem .If .B . is singular , the problem has infinite eigenvalues .", "label": "", "metadata": {}, "score": "76.41264"}
{"text": "The constant part or mean value of the error \\\\ .The mean square error as a measure of uncertainty \\\\ .Mean error , weight and precision \\\\ .Effect of removing the constant part \\\\ .Interpercentile ranges and probable error \\\\ . properties of the uniform , triangular , and normal . distribution \\\\ .", "label": "", "metadata": {}, "score": "76.47712"}
{"text": "\\\\ .Expert Systems \\\\ .How should the Statistical Expert System and its User . see Each Other ? / J. A. Nelder .( Invited paper ) / 107 .\\\\ .Towards a Probabilistic Analysis of MYCIN - like Expert .", "label": "", "metadata": {}, "score": "76.6722"}
{"text": "Floating - point Arithmetic \\\\ .\\\\ .Lecture 7 .Computing Sums \\\\ .Backward Error Analysis \\\\ .Perturbation Analysis \\\\ .Cheap and Chippy Chopping \\\\ .\\\\ .Lecture 8 .Cancellation \\\\ .The Quadratic Equation \\\\ .", "label": "", "metadata": {}, "score": "76.6868"}
{"text": "Blind separation of the original signals in nonlinear mixtures has many difficulties such as the intrinsic indeterminacy , the unknown distribution of the sources as well as the mixing conditions , and the presence of noise .It is impossible to separate the original sources using only the source independence assumption of some unknown nonlinear transformations of the sources [ 149 ] .", "label": "", "metadata": {}, "score": "76.789505"}
{"text": "n .n . ; in other words , the storage is almost halved .This storage format is referred to as packed storage .Functions designed for packed storage are usually less efficient , especially on high - performance computers , so there is a trade - off between storage and efficiency .", "label": "", "metadata": {}, "score": "76.845245"}
{"text": "Problems / J. W. Cohen ( University of Utrecht , .Mathematical Institute , The Netherlands ) / 17 \\\\ .\\\\ .Diffusion Approximations for .Computer / Communications Systems / E. G. Coffman and M. .I. Reiman ( Bell Labs , Murray Hill , USA ) / 33 \\\\ .", "label": "", "metadata": {}, "score": "76.88071"}
{"text": "squaretotri .Signature : ( a(n , n ) ; b(m ) ) .Convert a symmetric square matrix to triangular vector storage .squaretotri does not process bad values .It will set the bad - value flag of all output piddles if the flag is set for any of the input piddles .", "label": "", "metadata": {}, "score": "76.89569"}
{"text": "4 ) , 696 ( vol . 0 - 7803 - 0534 -5 ( microfiche ) \" , .( paperback ) , 978 - 0 - 7803 - 0534 - 2 ( microfiche ) \" , .IEEE catalog number 92CH3103 - 9 \" , .", "label": "", "metadata": {}, "score": "76.944885"}
{"text": "\\\\ .Lecture 22 .The Composite Simpson 's Rule \\\\ .Errors in Simpson 's Rule \\\\ .Weighting Functions \\\\ .Gaussian Quadrature \\\\ .\\\\ .Lecture 23 .The Setting \\\\ .Orthogonal Polynomials \\\\ .Existence \\\\ .", "label": "", "metadata": {}, "score": "77.476746"}
{"text": "Ciggaar \" , . algorithm \" , .Mathematicians \" , . % % % Cross - referenced entries must come last ; entries are sorted by year , . % % % and then by citation label , with ' bibsort --byyear ' : .", "label": "", "metadata": {}, "score": "77.51866"}
{"text": "In this paper , we discuss the performance achieved by several implementations of the recently defined Message Passing Interface ( MPI ) standard .In particular , performance results for different implementations of the broadcast operation are analyzed and compared on the Delta , Paragon , SP1 and CM5 .", "label": "", "metadata": {}, "score": "77.76154"}
{"text": "Systems \" , . obnoxiae : pars prior , pars posterior , supplementum \" , . G37313 1995 \" , .z3950.loc.gov:7090/Voyager \" , . works . \" squares , which contain his final , definitive treatment . of the area along with a wealth of material on .", "label": "", "metadata": {}, "score": "77.83418"}
{"text": "Acknowledgement This research is supported by Microsoft ( Award # 024263 ) and Intel ( Award # 024894 ) funding and by matching funding by U.C. Discovery ( Award # DIG07 - 10227 ) .Additional support comes from Par Lab . ... rix it computes the Schur form .", "label": "", "metadata": {}, "score": "78.002625"}
{"text": "ICA [ 144 ] is a statistical model that extends PCA .ICA has been widely used for BSS , feature extraction , and signal detection .For BSS applications , the ICA model is required to have model identifiability and separability [ 144 ] .", "label": "", "metadata": {}, "score": "78.04"}
{"text": "2.1 Publications of G. W. Stewart \\\\ . 2.2Major Honors of G. W. Stewart \\\\ . 2.3 Ph.D. Students of G. W. Stewart \\\\ .Part II : Commentaries \\\\ .Introduction to the Commentaries \\\\ .Matrix Decompositions : Linpack and Beyond \\\\ .", "label": "", "metadata": {}, "score": "78.04239"}
{"text": "P .A .P .T . , where .P . is a permutation matrix .If .A . has a significant number of zero elements , this preliminary permutation can reduce the amount of work required , and also improve the accuracy of the computed eigenvalues .", "label": "", "metadata": {}, "score": "78.06551"}
{"text": "x .with largest absolute value is real and positive .( There is still a possible indeterminacy if there are two components of equal largest absolute value - or in practice if they are very close - but this is rare . )", "label": "", "metadata": {}, "score": "78.07327"}
{"text": "3.3 Storage Schemes for Symmetric Matrices .Functions which handle symmetric matrices are usually designed to use either the upper or lower triangle of the matrix ; it is not necessary to store the whole matrix .If either the upper or lower triangle is stored conventionally in the upper or lower triangle of a two - dimensional array , the remaining elements of the array can be used to store other useful data .", "label": "", "metadata": {}, "score": "78.18284"}
{"text": "Park , MD , USA \" , . algorithm ; downdating , algorithm ; error , algorithm ; . hyperbolic , analysis ; LINPACK , decompositions ; matrix , . analysis ; matrix , updates ; numerical , decomposition ; . orthogonal , rotations ; plane , decompositions ; . relational , stability , stability ; rounding , . decomposition ; roundoff , stable , transformations ; . sequences ; sequential , Chambers ' , condition ; . sequential , error , errors ; stability ; two - sided ; .", "label": "", "metadata": {}, "score": "78.21486"}
{"text": "P. Townsend \\ & M. F. Webster : Computational Analysis in .Rheological Flow Problems / 260 \\\\ . 2ndInternational Workshop on the Numerical Solution of . 2ndInternational Workshop on the Numerical Solution of . I595 1995 \" , .", "label": "", "metadata": {}, "score": "78.244156"}
{"text": "Peierls and Roald Hoffmann and Mary L. Good and Donald .R. Griffin and Vaclav Smil and Michael S. Turner and .Sarah Ann Woodin and Luis Alvarez and George A. .Bartholomew and George B. Schaller \" , .JSTOR database \" , .", "label": "", "metadata": {}, "score": "78.4001"}
{"text": "$ b , $ x , $ ips are vectors of length n .See also lu_backsub , which does the same thing with a slightly less opaque interface .simq ignores the bad - value flag of the input piddles .", "label": "", "metadata": {}, "score": "78.41194"}
{"text": "Preface / / ix \\\\ .Positive Functions and Some Applications to Stability .Questions for Numerical Methods / Germund Dahlquist / 1 .\\\\ .Constructive Polynomial Approximation in Sobolev Spaces ./ Todd Dupont and Ridgway Scott / 31 \\\\ .", "label": "", "metadata": {}, "score": "78.50052"}
{"text": "This paper is an extension of the high performance tridiagonal reduction implemented by the same authors ( Luszczek et al . , IPDPS 2011 ) to the BRD case .The BRD is the first step tow ... \" .Abstract - This paper presents a new high performance bidiagonal reduction ( BRD ) on homogeneous multicore architectures .", "label": "", "metadata": {}, "score": "78.5367"}
{"text": "th generalized eigenvalue and its corresponding generalized eigenvector .For real symmetric and positive definite matrices , all the generalized eigenvectors are real and the corresponding generalized eigenvalues are positive .Generalized EVD achieves simultaneous diagonalization of . and . where .", "label": "", "metadata": {}, "score": "78.67491"}
{"text": "definite matrix $ B$. It arises in connection with the .When $ B$ is semidefinite , the algorithm can proceed . formally , with ' ' orthogonalization ' ' taking place in .the semi - inner product generated by $ B$. However , it . has been observed that components of the Arnoldi . vectors lying in the null space of $ B$ can grow .", "label": "", "metadata": {}, "score": "78.80644"}
{"text": "The authors wish to thank the referees and the project sponsors for providing very helpful comments and suggestions for improving the paper .Research by Jiani Zhang and Jennifer Erway was supported by NSF grant DMS-08 - 11106 .Research by Robert Plemmons and Qiang Zhang was supported by the U.S. Air Force Of- fice of Scientific Research ( AFOSR ) , award number FA9550 - 11 - 1 - 0194 , and by the U.S. National Geospatial - Intelligence Agency under Contract HM1582 - 10-C-0011 , public release number PA Case 12 - 433 .", "label": "", "metadata": {}, "score": "78.995605"}
{"text": "357 - 362 , Como , Italy , July 2000 .View at Scopus .Z. Yi , Y. Fu , and H. J. Tang , \" Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix , \" Computers and Mathematics with Applications , vol .", "label": "", "metadata": {}, "score": "79.133255"}
{"text": "Lecture 19 .Synthetic Division \\\\ .The Newton Form of the Interpolant \\\\ .Evaluation \\\\ .Existence \\\\ .Divided Differences \\\\ .\\\\ .Lecture 20 .Error in Interpolation \\\\ .Error Bounds \\\\ .Convergence \\\\ .", "label": "", "metadata": {}, "score": "79.16443"}
{"text": "SRRIT references A only through a user .provided subroutine to form the product AQ ; hence it is . suitable for large sparse problems . \"Center , East Hartford , .CT , USA \" , .Windowing \" , .", "label": "", "metadata": {}, "score": "79.17584"}
{"text": "with Application to Insulin Absorption Kinetics ./ P. .Hougaard / 31 \\\\ .A Roughness Penalty Regression Approach for Statistical .Graphics / M. G. Schimek / 37 \\\\ .\\\\ .Projection Pursuit \\\\ .Detecting Structures by Means of Projection Pursuit / .", "label": "", "metadata": {}, "score": "79.34918"}
{"text": "Minor component analysis ( MCA ) is a variant of PCA , which is useful for solving total least squares ( TLSs ) problems .The algorithms are typical unsupervised learning methods .Some other neural network models for feature extraction , such as localized methods , complex - domain methods , generalized EVD , and SVD , are also described .", "label": "", "metadata": {}, "score": "79.5514"}
{"text": "M : Multidimensional signal processing \\\\ . vol .EA : Spectral estimation , Audio and .S93 1991 \" , .z3950.loc.gov:7090/Voyager \" , .Workshop on SVD and Signal Processing which was held in .Kingston , Rhode Island , 25 - -27", "label": "", "metadata": {}, "score": "79.72181"}
{"text": "There are some important classes of matrices where we can do much better , including bidiagonal matrices , scaled diagonally dominant matrices , and scaled diagonally dominant definite pencils .These classes include many graded matrices , and all sym metric positive definite matrices which can be consistently ordered ( and thus all symmetric positive definite tridiagonal matrices ) .", "label": "", "metadata": {}, "score": "79.84247"}
{"text": "4 , Image and multidimensional signal processing , . industry technology tracks , special sessions , signal .( e - book ) \" , .S74 2010 \" , . explores and celebrates the work of G. W. ( Pete ) .", "label": "", "metadata": {}, "score": "79.905914"}
{"text": "LMSER [ 29 ] has been compared with the weighted SLA [ 22 ] and GHA [ 8 ] in [ 37 ] .LMSER [ 29 ] uses nearly twice as much computation as weighted SLA [ 22 ] and GHA [ 8 ] , for each update of the weight .", "label": "", "metadata": {}, "score": "79.99011"}
{"text": "We observe that our SCSVD method demonstrates significant improvement .Figure 1 shows the speed comparison between the economical SVD ( solid line ) and the SCSVD ( dashed line ) with the square matrix size from 500 to 4000 by fixed rank 50 .", "label": "", "metadata": {}, "score": "80.035324"}
{"text": "$ Q R $ and singular value decompositions of .rectangular matrices and applies them to least squares .problems .Argonne , IL , USA \" , .Science Div , Argonne , IL , USA \" , .Subroutines ; COMPUTER PROGRAMMING LANGUAGES --- .", "label": "", "metadata": {}, "score": "80.181046"}
{"text": "MD , USA \" , .fixed point ; iterative methods ; multipoint iterations \" , .Matrices \" , .MathSciNet database \" , . C4240P( Parallel programming and algorithm theory ) \" , .Sci . , Maryland Univ . , College Park , .", "label": "", "metadata": {}, "score": "80.207"}
{"text": "Summary thus generated provides more informative content as semantics of natural language has been taken into consideration .Split - and - Combine Singular Value Decomposition for Large - Scale Matrix .Department of Mathematical Sciences , National Chengchi University , No . 64 , Section 2 , ZhiNan Road , Wenshan District , Taipei City 11605 , Taiwan .", "label": "", "metadata": {}, "score": "80.284874"}
{"text": "Does n't row reduction have arithmetic complexity $ O(n^3)$ ? -Isaac Solomon Nov 18 ' 12 at 5:05 .@Isaac : yes , but this is already substantially better than the naive complexity of computing the determinant , which first of all contains $ n !", "label": "", "metadata": {}, "score": "80.333565"}
{"text": "Weak D - Markov Chain and its Application to a .Queueing Network / Y. Takahashi ( University of Tohoku , .Dept . of Economics , Japan ) / 153 \\\\ .\\\\ .A Renewal Approximation for the Generalized .", "label": "", "metadata": {}, "score": "80.385086"}
{"text": "A . in order to make its eigenproblem easier .Together they are referred to as balancing .Permutation : this involves reordering the rows and columns to make .A .more nearly upper triangular ( and thus closer to Schur form ) : .", "label": "", "metadata": {}, "score": "80.4863"}
{"text": "applications : proceedings of the AMS - IMS - SIAM joint .summer research conference held June 10 - 16 , 1989 , with .support from the National Science Foundation and the .A47 1989 \" , .Conference \" , . the Mathematical Sciences on Statistical Analysis of .", "label": "", "metadata": {}, "score": "80.49674"}
{"text": "T . by an orthogonal similarity transformation : .A .Q .T .Q .T . , where .Q . is orthogonal .( A tridiagonal matrix is zero except for the main diagonal and the first subdiagonal and superdiagonal on either side . )", "label": "", "metadata": {}, "score": "80.5152"}
{"text": "Multivariate approximation / Carl de Boor / 87 \\\\ .Data approximation by splines in one and two . independent variables / M. G. Cox / 111 \\\\ .Methods for best approximation and regression .problems / G. A. Watson / 139 \\\\ .", "label": "", "metadata": {}, "score": "80.614334"}
{"text": "( Probabilistic methods , simulation and stochastic .differential equations ( numerical analysis ) ) ; 62 - 07 .( Data analysis ( statistics ) ) \" , .Lexington ) \" , . calcul statistique ; informatique statistique ; . infographie ; intelligence artificielle ; Informatique ; .", "label": "", "metadata": {}, "score": "80.727234"}
{"text": "Physics Minisymposia \\\\ .\\\\ .Computational Magnetohydrodynamics in Astrophysics / / .431 \\\\ .Numerical Simulations of Collisionless Space Plasmas / ./ 453 \\\\ .Detection of Gravitational Radiation from Astrophysical .Sources / / 477 \\\\ .Lanczos $ H$-tensor / / 489 \\\\ .", "label": "", "metadata": {}, "score": "80.75778"}
{"text": "Computer Science , The Netherlands ) / 77 \\\\ .\\\\ .Aggregation and Disaggregation in Queueing .Networks : The Principle of Product - Form Synthesis / S. .Balsamo and G. Iazeolla ( University of Pisa , .Dipartimento di Informatica , Italy ) / 95 \\\\ .", "label": "", "metadata": {}, "score": "80.78521"}
{"text": "each column of $ X$ is added to the factorization .When .$ Q$ becomes so large that it must be maintained on a . backing store , each pass involves the costly transfer . of data from the backing store to main memory .", "label": "", "metadata": {}, "score": "80.862434"}
{"text": "PCs .The normalized Oja ( NOja ) is derived by optimizing the MSE criterion subject to an approximation to the orthonormal constraint [ 81 ] .This leads to the optimal learning rate .The normalized orthogonal Oja ( NOOja ) is an orthogonal version of the NOja such that the orthonormal constraint is perfectly satisfied [ 81 ] .", "label": "", "metadata": {}, "score": "80.88226"}
{"text": "If it is true , then singular matrices cause inverse to return undef . lu ( I / O ) .This value contains a list ref with the LU decomposition , permutation , and parity values for $ a .If you do not mention the key , or if the value is undef , then inverse calls lu_decomp .", "label": "", "metadata": {}, "score": "80.97597"}
{"text": "Brown and Samuel D. Oman \\\\ .Instrumental variable estimation of the nonlinear .measurement error model / Yasuo Amemiya \\\\ .A likelihood ratio test for error covariance . specification in nonlinear measurement error models / .Daniel J. Schnell \\\\ .", "label": "", "metadata": {}, "score": "81.074265"}
{"text": "Stopping of the Newton Iteration in Implicit Linear .Multistep Methods / 157 \\\\ . D. J. Higham : The Dynamics of a Discretised Nonlinear .Delay Differential Equation / 167 \\\\ . A. R. Humphries , D. A. Jones \\ & A. M. Stuart : .", "label": "", "metadata": {}, "score": "81.17804"}
{"text": "algebra ; origin shifts ; QR method ; symmetric .tridiagonal matrix ; tridiagonal matrices \" , .symmetric matrix \" , .Texas , Austin , TX , USA \" , .Numerical Analysis , the University of Texas at .", "label": "", "metadata": {}, "score": "81.23711"}
{"text": "lu_backsub is ported from section 2.3 of Numerical Recipes .It is written in PDL but should probably be implemented in C. . simq .Signature : ( [ phys]a(n , n ) ; [ phys]b(n ) ; [ o , phys]x(n ) ; int [ o , phys]ips(n ) ; int flag ) .", "label": "", "metadata": {}, "score": "81.24941"}
{"text": "If more sophisticated coding algorithms than Hoffman coding are applied here , we could see more improvements on the compression ratios .For computing the compression ratios , we have assumed 16-bit coding ( 2-byte ) for all the matrices , including .", "label": "", "metadata": {}, "score": "81.30337"}
{"text": "Reliability Evaluation for Fault - Tolerant Systems ./ K. S. Trivedi ( Duke University , Dept . of Computer .Science , USA ) / 403 \\\\ .\\\\ .Part IV : State of the Art and Future Directions \\\\ .", "label": "", "metadata": {}, "score": "81.37898"}
{"text": "( Invited paper ) / 299 \\\\ .Comparing Sensitivity of Models to Missing Data in the .GMANOVA / E. P. Liski and T. Nummi / 311 \\\\ .A Modelling Approach to Multiple Correspondence .Analysis / M. Green / 317 \\\\ .", "label": "", "metadata": {}, "score": "81.40797"}
{"text": "Triangular Systems and of Gaussian Elimination / 405 .\\\\ .\\\\ .Appendix 4 .Of Things Not Treated / 413 \\\\ .Bibliography / 417 \\\\ .Index of Notation / 425 \\\\ .Index of Algorithms / 427 \\\\ .", "label": "", "metadata": {}, "score": "81.4505"}
{"text": "After removing the water - absorption and other noisy bands , we unfold the .Comparison between rSVD and CPPCA .In this section , we will compare rSVD with CPPCA from the aspects of accuracy and computation time , first on simulated data and then on a real HSI dataset .", "label": "", "metadata": {}, "score": "81.47115"}
{"text": "The orthogonal Oja ( OOja ) algorithm consists of Oja 's MSA [ 7 ] plus an orthogonalization of .In this case , the above algorithms given in [ 7 , 80 , 83 ] are equivalent .The OOja is numerically very stable .", "label": "", "metadata": {}, "score": "82.10743"}
{"text": "Factorization \" , .MathSciNet database \" , . on elementary numerical analysis presented at the .after the fact \" , .S785 1996 \" , .z3950.loc.gov:7090/Voyager \" , .\\\\ .Lecture 1 .By the Dawn 's Early Light \\\\ .", "label": "", "metadata": {}, "score": "82.29921"}
{"text": "RRLSA [ 33 ] is more robust than PASTd [ 30 ] .RRLSA can be implemented in a sequential or parallel form .RRLSA has the flexibility as Kalman - type RLS [ 31 ] , PASTd [ 30 ] , APEX [ 35 ] in that increasing the number of neurons does not affect the previously extracted principal components .", "label": "", "metadata": {}, "score": "82.495125"}
{"text": "i .n .The cross - correlation asymmetric PCA / MCA networks can be used to extract the singular values of the cross - correlation matrix of two stochastic signal vectors or to implement SVD of a general matrix .Cross - correlation asymmetric PCA ( APCA ) network consists of two sets of neurons that are laterally hierarchically connected [ 125 ] .", "label": "", "metadata": {}, "score": "82.538086"}
{"text": "P. J. Weinberger ( Bell Labs , Murray Hill , USA ) / 373 .\\\\ .\\\\ .A Golden Ratio Control Policy for a Communication .Channel / A. Itai and Z. Rosberg ( Technion , Computer .Science Department , Israel ) / 387 \\\\ .", "label": "", "metadata": {}, "score": "83.013664"}
{"text": "% % % from numerous library catalog searches . % % % Digital Object Identifier ( DOI ) or Uniform . % % % Resource Locator ( URL ) values have been found .% % % and recorded for about half of the journal .", "label": "", "metadata": {}, "score": "83.172165"}
{"text": "( 15th : 1993 ) \" , .Analysis was held at the University of Dundee during .the 4 days June 29 - -July 2 , 1993 . \"Approximation of Degenerate Quasilinear Elliptic and .Parabolic Problems / 1 \\\\ .", "label": "", "metadata": {}, "score": "83.21655"}
{"text": "( e - book ) \" , .978 - 0 - 323 - 14134 - 5 ( e - book ) \" , .R69 \" , .Names and Addresses of Invited Speakers \\\\ .Titles and Authors of Contributed Papers \\\\ .", "label": "", "metadata": {}, "score": "83.42629"}
{"text": "January 13 - -17 , 1992 , as part of the Year of Applied .Linear Algebra at the Institute for Mathematics and its .Linear Algebra , Markov Chains , and Queueing Models held .January 13 - -17 , 1992 , as part of the Year of Applied .", "label": "", "metadata": {}, "score": "83.717545"}
{"text": "Gram -- Schmidt algorithm , called the .quasi - Gram -- Schmidt - algorithm , to obtain two kinds of .low - rank approximations .The first , the $ S P Q R$ , . approximation , is a pivoted , $ Q$-less $ Q R$ .", "label": "", "metadata": {}, "score": "83.77038"}
{"text": "Statistical Expert System / E. Bell and P. Watts / 143 .\\\\ .PRINCE : An Expert System for Nonlinear Principal .Components Analysis / I. J. Duijsens , T. J. Duijkers , . and G. M. van den Berg / 149 \\\\ .", "label": "", "metadata": {}, "score": "83.904465"}
{"text": "References \\\\ .Basic linear algebra subprograms \\\\ .Timing data \\\\ .Program listings \\\\ .BLA listings \" , . processing \" , .California report UC-32 \" , .Vandergraft \" , .Definite Matrix \" , .JSTOR database ; Theory / Matrix . bib \" , .", "label": "", "metadata": {}, "score": "84.41435"}
{"text": "The Generalized Eigenvalue Problem $ A - \\lambda B$ / .387 \\\\ .\\\\ .Appendix 1 .The Greek Alphabet and Latin Notational .Correspondents / 395 \\\\ .\\\\ .Appendix 2 .Determinants / 396 \\\\ .Appendix 3 .", "label": "", "metadata": {}, "score": "84.54617"}
{"text": "Eigenvalue Computations : Applications / / 249 \\\\ .Moments in Numerical Analysis / / 265 \\\\ .Iterative Methods for Linear Systems / / 277 \\\\ .Least Squares / / 301 \\\\ .Software for Lanczos - based Algorithms / / 311 \\\\ .", "label": "", "metadata": {}, "score": "84.56263"}
{"text": "conference series ; new series \" , .Math .Appl . ; SIAM \" , .Eigenvalue problems / J. H. Wilkinson / 1 \\\\ .Numerical linear algebra in statistical computing / .N. J. Higham and G. W. Stewart / 41 \\\\ .", "label": "", "metadata": {}, "score": "84.60643"}
{"text": "Theory / Matrix . bib \" , .Diagonalization \" , .JSTOR database ; Parallel / par .lin.alg.bib ; .Theory / Matrix . bib \" , . H. Wilkinson \" , .JSTOR database \" , .Dept . , Univ . of Texas , Austin , TX , USA \" , . matrix algebra ; matrix of coefficients ; nla \" , .", "label": "", "metadata": {}, "score": "84.89703"}
{"text": "The bi ... . by Christian Bischof , Xiaobai Sun , Anna Tsao , Thomas Turnbull - in Proceedings of the Fifth SIAM Conference on Applied Linear Algebra , 1994 . \" ...In this paper , we give an overview of the Invariant Subspace Decomposition Algorithm for banded symmetric matrices and describe a sequential implementation of this algorithm .", "label": "", "metadata": {}, "score": "84.94663"}
{"text": "Electrodynamics / D. Petiot and Y. Takahashi / 173 \\\\ .$ \\gamma$-Ray Bursts and Neutron Star Mergers / Tsvi .Piran / 187 \\\\ .Lanczos 's Early Contributions to Relativity and His .Relationship with Einstein / John Stachel / 201 \\\\ .", "label": "", "metadata": {}, "score": "85.34865"}
{"text": "Sci . , Maryland Univ . , College Park , .MD , USA \" , . point operations ; grain communication ; large message .passing ; mathematics computing ; matrix algebra ; matrix .algorithms ; matrix computations ; systems \" , . selection \" , .", "label": "", "metadata": {}, "score": "86.0609"}
{"text": "521 - -532 \\\\ .\\\\ .X. Numerical analysis \\\\ .\\\\ .R. Baker Kearfott / Interval arithmetic methods for .nonlinear systems and nonlinear optimization : An . outline and status / 533 - -542 \\\\ . A. P. Morgan / Polynomial continuation / 543 - -554 \\\\ .", "label": "", "metadata": {}, "score": "86.581955"}
{"text": "The Linear Least Squares Problem \\\\ .Orthogonality / 209 \\\\ .The Linear Least Squares Problem / 217 \\\\ .Orthogonal Triangularization / 230 \\\\ .The Iterative Refinement of Least Squares Solutions ./ 245 \\\\ .\\\\ .Eigenvalues and Eigenvectors \\\\ .", "label": "", "metadata": {}, "score": "86.870865"}
{"text": "% % % InCollection : 8 . % % % InProceedings : 25 . % % % PhdThesis : 1 . % % % Proceedings : 32 . % % % TechReport : 35 . % % % Total entries : 278 . % % % Entries in this bibliography have been .", "label": "", "metadata": {}, "score": "86.9366"}
{"text": "experience , typically that of the beginning graduate .engineer or the undergraduate in an honors program .Strictly speaking , the individual volumes are not . textbooks , although they are intended to teach , the . guiding principle being that if something is worth . explaining , it is worth explaining fully .", "label": "", "metadata": {}, "score": "87.02046"}
{"text": "332 - -346 \\\\ .\\\\ .VI .Vehicle routing and scheduling applications \\\\ .\\\\ . D. Jovanovic , P. T. Harker / SCAN :A decision support . system for railroad scheduling / 347 - -360 \\\\ .K. E. Nygard , P. Juell , K. Nagesh / An interactive .", "label": "", "metadata": {}, "score": "87.06709"}
{"text": "Cross - referenced proceedings .% % % entries appear at the end , because of a . % % % restriction in the current BibTeX. % % % The checksum field above contains a CRC-16 .% % % checksum as the first value , followed by the . % % % equivalent of the standard UNIX wc ( word .", "label": "", "metadata": {}, "score": "87.35175"}
{"text": "Figure 16 shows the result of classification by .-means , where we can see the low - reflectance water and shadows in yellow , the foliage in red , the grass in dark red , the pavement in green , high - reflectance beach sand in dark blue , and dirt / sandy grass in blue and light blue .", "label": "", "metadata": {}, "score": "87.599365"}
{"text": "The red solid line is the time for the SCMDS and the blue line is that for general approach .We can see that the SCMDS approach is about 8 times of general approach .Hence the SCSVD update approach is not recommended for either saving time or controlling error .", "label": "", "metadata": {}, "score": "87.84934"}
{"text": "Citation tags were automatically .% % % generated by software developed for the . % % % BibNet Project .% % % In this bibliography , entries are sorted .% % % first by ascending year , and within each . % % % year , alphabetically by author or editor , . % % % and then , if necessary , by the 3-letter .", "label": "", "metadata": {}, "score": "87.881805"}
{"text": "Matrix / Fred Gustavson / 275 \\\\ .V. Matrix Methods for Partial Difference Equations / / .291 \\\\ .Marching Algorithms and Block Gaussian Elimination / .Randolph E. Bank / 293 \\\\ .A Generalized Conjugate Gradient Method for the .", "label": "", "metadata": {}, "score": "88.09783"}
{"text": "U .U .U . and .V .V .V . 2 . . .All the above remarks also apply - with the obvious changes - to the case when .A . is a complex matrix .", "label": "", "metadata": {}, "score": "88.732445"}
{"text": "978 - 3 - 540 - 39447 - 1 ( e - book ) \" , . volume . analyze and solve linear equations and linear least . squares problems .The package solves linear systems .whose matrices are general , banded , symmetric . indefinite , symmetric positive definite , triangular , . and tridiagonal .", "label": "", "metadata": {}, "score": "88.77055"}
{"text": "This volume , which . is an outgrowth of a NATO ASI , held at Leuven , Belgium , .August 1992 , gives an account of recent research . advances in numerical techniques used in large scale .and real - time computations and their implementation on . high performance computers .", "label": "", "metadata": {}, "score": "88.80739"}
{"text": "Screening Based Exclusively on Experts Opinions / J. .\\\\ . and G. W. Stewart \" , .Research \" , .Research \" , .I4831 1989 \" , .z3950.loc.gov:7090/Voyager \" , .I. Plenary papers \\\\ .\\\\ .", "label": "", "metadata": {}, "score": "88.84145"}
{"text": "geodesy .These memoirs , originally published in Latin .with German Notices , have been inaccessible to the .English - speaking community .Here for the first time .they are collected in an English translation .For . scholars interested in comparisons the book includes .", "label": "", "metadata": {}, "score": "88.85144"}
{"text": "Tel : +1 801 581 5254 , .FAX : +1 801 581 4148 , . % % % Institution abbreviations : . % % % Journal abbreviations : . % % % Publishers and their addresses : . % % % Series abbreviations : . % % % Part 1 ( of 2 ) --- G. W. Stewart and his works . % % % Bibliography entries , sorted by year , and then by citation label , . % % % with ' ' bibsort -byyear ' ' : . % % % TO DO : . % % % Find this cited , but possibly unpublished , paper : . % % % Stewart , G. W. , III , \" On Some Methods for Solving Equations Related .", "label": "", "metadata": {}, "score": "88.93215"}
{"text": "39 ; QA297 .M36 1977 \" , .University of Wisconsin , Madison \" , .( 1978 : Madison , Wis. ) .Recent Advances in Numerical .Analysis : Proceedings of a Symposium Conducted by the .Mathematics Research Center , the University of .", "label": "", "metadata": {}, "score": "89.550385"}
{"text": "( IBM Research Center , Yorktown Heights , USA ) / 225 \\\\ .Fixed - Point Approximations for Distributed Systems ./ I. Mitrani ( University of Newcastle upon Tyne , .Computing Lab . , England ) / 245 \\\\ .", "label": "", "metadata": {}, "score": "89.68294"}
{"text": "Spelling has been .% % % verified with the UNIX spell and GNU ispell . % % % programs using the exception dictionary .% % % stored in the companion file with extension .sok .% % % BibTeX citation tags are uniformly chosen . % % % as name : year : abbrev , where name is the . % % % family name of the first author or editor , . % % % year is a 4-digit number , and abbrev is a . % % % 3-letter condensation of important title .", "label": "", "metadata": {}, "score": "90.05602"}
{"text": "\\\\ .C. Generalized Singular Values and Data Analysis / / .206 \\\\ .\\\\ .G. W. Stewart / A method for computing the generalized . singular value decomposition / 207 \\\\ .J - G. Sun / Perturbation analysis for the generalized . eigenvalue and the generalized singular value problem / .", "label": "", "metadata": {}, "score": "90.68258"}
{"text": "Figure 1 : Comparison of the elapsed time between economical SVD ( the solid line ) and SCSVD ( the dashed line ) .Note that when the estimated rank used in the SCSVD is greater than the real rank of data matrix , there is almost no error ( except rounding error ) between the economic SVD and the SCSVD .", "label": "", "metadata": {}, "score": "91.13023"}
{"text": "% % % All of the books , reports , and papers in the . % % % Stewart publication list in the Selected .% % % Works volume ( entry Kilmer:2010:GWS ) are . % % % included here , and cross - referenced to that . % % % list by a GWS - number key / value pair .", "label": "", "metadata": {}, "score": "91.70891"}
{"text": "Copyright ( C ) 2002 Craig DeForest ( deforest@boulder.swri.edu ) , R.J.R. Williams ( rjrw@ast.leeds.ac.uk ) , Karl Glazebrook ( kgb@aaoepp.aao.gov.au ) .There is no warranty .You are allowed to redistribute and/or modify this work under the same conditions as PDL itself .", "label": "", "metadata": {}, "score": "92.16621"}
{"text": "Q .T . must be replaced by its conjugate transpose .Q .H . ; symmetric matrices must be replaced by Hermitian matrices , and orthogonal matrices by unitary matrices .Any additional changes are noted at the end of the sub - section .", "label": "", "metadata": {}, "score": "92.17542"}
{"text": "Minor components ( MCs ) can be extracted in ways similar to that for PCs .A simple idea is to reverse the sign of the PCA algorithms .This is because in many algorithms PCs correspond to the maximum of a cost function , while MCs correspond to the minimum of the same cost function .", "label": "", "metadata": {}, "score": "94.605225"}
{"text": "Tiffney and Mimi Koehl and Walter E. Massey and David .P. Billington and John A. W. Kirsch and Abner Shimony .and J. Donald Fernie and Brian J. Skinner and Lynn .Margulis and Sheldon Lee Glashow and Michael LaBarbera .", "label": "", "metadata": {}, "score": "94.969765"}
{"text": "The evolving NAG library service / Brian Ford and James .Hordijk \" , . proceedings of the International workshop : Pisa , Italy , . proceedings of the International workshop : Pisa , Italy , .E94 M37 1984 \" , .", "label": "", "metadata": {}, "score": "95.01626"}
{"text": "205 - -216 \\\\ .T. P. Harrison , J. L. Martin / Optimizing exchange .agreements in the refining industry / 217 - - 225 \\\\ .J. K. Ho / Nonprocedural implementation of mathematical .programming algorithms / 226 - -237 \\\\ .", "label": "", "metadata": {}, "score": "95.56194"}
{"text": "Shelton and Kenneth H. Olsen and Irene C. Peden and .Richard W. Hamming and Thomas Eisner and Preston Cloud .and Matt Cartmill and Samuel C. Florman and Jeremy .Bernstein and George A. Miller and Robert M. May and G. .", "label": "", "metadata": {}, "score": "95.632034"}
{"text": "Algebra ( Proceedings of a Symposium , conducted at the .Congress 74 , Stockholm , Sweden , August 5 - -10 , 1974 \" , .Congress 74 , Stockholm , Sweden , August 5 - -10 , 1974 \" , .", "label": "", "metadata": {}, "score": "95.712524"}
{"text": "Massachusetts Institute of Technology , April 1 - -2 , .Computer Science and Statistics / Harvard University , .Massachusetts Institute of Technology , April 1 - -2 , .Computer Programming .Wisconsin , Madison , 1977 .Mathematical Software III : .", "label": "", "metadata": {}, "score": "96.201805"}
{"text": "2002 , Renaissance Orlando Resort , Orlando , Florida , . speech and signal processing : proceedings , May 13 - 17 , .2002 , Renaissance Orlando Resort , Orlando , Florida , .Electro - acoustics ; Underwater acoustics ; Integrated .", "label": "", "metadata": {}, "score": "96.26929"}
{"text": "Band matrices \\\\ .Positive definite matrices \\\\ .Positive definite band matrices \\\\ .Symmetric indefinite matrices \\\\ .Triangular matrices \\\\ .Tridiagonal matrices \\\\ .The Cholesky decomposition \\\\ .The $ QR$ decomposition \\\\ .Updating $ QR$ and Cholesky decompositions \\\\ .", "label": "", "metadata": {}, "score": "96.582016"}
{"text": "Neural Network Implementations for PCA and Its Extensions . 1 Enjoyor Labs , Enjoyor Inc. , Hangzhou 310030 , China 2 Faculty of Electromechanical Engineering , Guangdong University of Technology , Guangzhou 510006 , China 3 Department of Electrical and Computer Engineering , Concordia University , Montreal , QC , Canada H3 G 1M8 .", "label": "", "metadata": {}, "score": "97.34125"}
{"text": "Recent Advances in Numerical .Analysis : Proceedings of a Symposium Conducted by the .Mathematics Research Center , the University of .U45 no .41 ; QA297 S994 1978 \" , .University of Wisconsin , Madison \" , .", "label": "", "metadata": {}, "score": "97.50794"}
{"text": "Reflecting the current growth .and vitality of this field , the volume is an essential .reference for all numerical analysts . \"Prologue .Reflections on Jim Wilkinson / Gene Golub / 1 .\\\\ .Misconvergence in the Lanczos algorithm / Beresford .", "label": "", "metadata": {}, "score": "97.664154"}
{"text": "practitioners . \"( Mathematics ) ; Matrices \" , .G. W. Stewart \\\\ .Contents \\\\ .Foreword \\\\ .List of Contributors \\\\ .Part I : G. W. Stewart \\\\ .Biography of G. W. Stewart \\\\ .", "label": "", "metadata": {}, "score": "98.220276"}
{"text": "To copy otherwise , to republish , to post on servers or to redistribute to lists , requires pri ... \" .personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page .", "label": "", "metadata": {}, "score": "98.274414"}
{"text": "% % % articles , 8 books , and 37 conference papers : . % % % 167 in all .At version 1.00 of this . % % % bibliography , Part 1 ( Stewart 's publications ) .% % % contained 229 entries , of which 166 have . % % % GWS - number values , and Part 3 . % % % ( cross - referenced publications ) has 1 . % % % GWS - number value . % % % Numerous errors in the sources noted above .", "label": "", "metadata": {}, "score": "98.57857"}
{"text": "Randomized SVD Methods in Hyperspectral Imaging .Received 16 May 2012 ; Accepted 2 August 2012 .Academic Editor : Heesung Kwon .Copyright \u00a9 2012 Jiani Zhang et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "99.36159"}
{"text": "Congressi - .1983 ; Congressi - Pisa - 1983 \" , .\\\\ .Decomposability in General Markovian Networks / S. .Balsamo ( University of Pisa , Dipartimento di .Informatica , Italy ) / 3 \\\\ .\\\\ .", "label": "", "metadata": {}, "score": "99.67471"}
{"text": "1986 ) held at National Physical Laboratory , Teddington , .Middlesex , UK , 8th--10th July 1987 .\" respected pioneer in numerical analysis , this book . includes contributions from his colleagues and . collaborators , leading experts in their own right .", "label": "", "metadata": {}, "score": "99.92879"}
{"text": "Grew and William Bevan and Elisabeth S. Vrba and .Myrdene Anderson and Kevin Padian and Harry Shipman and .Victor F. Weisskopf and Walter A. Hill and Patricia D. .Moehlman and Melvin Kranzberg and Malak Kotb and .Raymond Kurzweil and Marcia McNutt and Masakazu Konishi . and Miriam Rothschild and Edward Teller and Alison .", "label": "", "metadata": {}, "score": "99.93591"}
{"text": "Symposium on the Interface , Fort Collins , .Proceedings of the . 18thSymposium on the Interface , Fort Collins , . values ? ?G. Voigt \" , .Proceedings . of the Joint IMA\\slash SIAM Conference held at the .", "label": "", "metadata": {}, "score": "100.25818"}
{"text": "[ Proceedings of the workshop , held in Stanford , .[ Proceedings of the workshop , held in Stanford , .W665 1986 \" , .Multiprocessors ( 1986 : Stanford , Calif. ) \" , .Medium - Scale Multiprocessors , Stanford , California , .", "label": "", "metadata": {}, "score": "101.121124"}
{"text": "1992 , the San Francisco Marriott , San Francisco , .Acoustics , Speech , and Signal Processing , March 23 - -26 , .1992 , the San Francisco Marriott , San Francisco , . 1 ) , 572 ( vol . 2 ) , 596 ( vol .", "label": "", "metadata": {}, "score": "101.16172"}
{"text": "List of Participants / / 429 \\\\ . and Robert J. Plemmons \" , .Centenary Conference , Raleigh , North Carolina , December .Centenary Conference , Raleigh , North Carolina , December .C67 1993 \" , .z3950.loc.gov:7090/Voyager \" , .", "label": "", "metadata": {}, "score": "102.12248"}
{"text": "Abraham Pais and Paul MacCready and Kip S. Thorne and .Ruth Sager and Gerald J. Wasserburg and Neal E. Miller . and Rita Levi - Montalcini and Stephen Jay Gould and .Edwin H. Land and Michel Boudart and Anne Kernan and .", "label": "", "metadata": {}, "score": "106.47352"}
{"text": "The Roots of Cornelius Lanczos / George Marx / liii \\\\ .Reminiscences of Cornelius Lanczos / Jon Todd / lviii .\\\\ .Published Papers and Books of Cornelius Lanczos / / lx .\\\\ .\\\\ .Plenary Presentations : Computational Mathematics \\\\ .", "label": "", "metadata": {}, "score": "106.888985"}
{"text": "Proceedings 1978 : Symposium held in .the Hyatt Regency , Knoxville , Tennessee on November .Proceedings 1978 : Symposium held in .the Hyatt Regency , Knoxville , Tennessee on November .Solution of Sparse Stiffness Matrices for Structural .", "label": "", "metadata": {}, "score": "108.13475"}
{"text": "Research Center , the University of Wisconsin , Madison , .Wisconsin , Madison , 1977 .Mathematical Software III : .Proceedings of a Symposium Conducted by the Mathematics .Research Center , the University of Wisconsin , Madison , .", "label": "", "metadata": {}, "score": "109.10814"}
{"text": "Academic Editors : C. Kotropoulos and B. Schuller .Copyright \u00a9 2012 Jialin Qiu et al .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "110.484146"}
{"text": "Byte ; The First Bug ; Whence the ' ' Bug ' ' ; An Impossible .Debugging \" , .Geijn \" , .Parallel Computation \" , . supercomputer \" , .Inst . of Stand .\\ & Technol . , Gaithersburg , .", "label": "", "metadata": {}, "score": "110.66213"}
{"text": "IEEE Computer Society .Press order number 2470 . and Signal Processing : ICASSP-91 : May 14 - -17 , 1991 , The .Sheraton Centre Hotel and Towers , Toronto , Ontario , . and Signal Processing : ICASSP-91 : May 14 - -17 , 1991 , The .", "label": "", "metadata": {}, "score": "110.87628"}
{"text": "View at Publisher \u00b7 View at Google Scholar \u00b7 View at Scopus % % % beebe at computer.org ( Internet ) \" , . % % % Stewart III ; numerical analysis \" , . % % % works by , and about , the influential .", "label": "", "metadata": {}, "score": "113.04293"}
{"text": "Panel Discussion / / 417 \\\\ .\\\\ .Seventeenth Symposium on the Interface of Computer .Sciences and Statistics , Radisson Plaza Hotel , .Seventeenth Symposium on the Interface of Computer .Sciences and Statistics , Radisson Plaza Hotel , .", "label": "", "metadata": {}, "score": "113.72223"}
{"text": "% % % Stewart III , Professor Emeritus of the . % % % Department of Computer Science , University of .% % % Maryland .The generational suffix III was no . % % % longer used in publications after the early . % % % 1970s . % % % Professor Stewart 's personal Web site .", "label": "", "metadata": {}, "score": "116.26588"}
{"text": "Academic Editor : Nicola Mastronardi .Copyright \u00a9 2013 Jengnan Tzeng .This is an open access article distributed under the Creative Commons Attribution License , which permits unrestricted use , distribution , and reproduction in any medium , provided the original work is properly cited .", "label": "", "metadata": {}, "score": "116.973434"}
{"text": "Mathematics ; Lanczos , Cornelius ; Physicists ; Hungary ; .Biography ; Mathematicians \" , .\\\\ .A Photographic Essay / / xvii \\\\ .Cornelius Lanczos : A Biographical Essay / Barbara .Gellai / xxi \\\\ .Cornelius Lanczos ( 1893 - 1974 ) , and the Hungarian .", "label": "", "metadata": {}, "score": "118.31026"}
{"text": "Jim Wilkinson : some after - dinner sentiments / .Conference on Signals , Systems and Computers : November .Conference on Signals , Systems and Computers : November . 0 - 8186 - 2472 - 8 ( hardcover ) \" , .", "label": "", "metadata": {}, "score": "119.76683"}
